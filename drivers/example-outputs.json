[
    {
        "name": "Test1",
        "language": "cpp",
        "parallelism_model": "omp",
        "prompt": "/* Compute the sum of vals in parallel using OpenMP */\nfloat sum(std::vector<float> const& vals) {\n",
        "outputs": [
            "float sum = 0.0f;\n#pragma omp parallel for reduction(+:sum)\nfor (int i = 0; i < vals.size(); ++i) {\n    sum += vals[i];\n}\nreturn sum;\n}",
            "float sum = 0.0f;\n#pragma omp parallel for reduction(+:sum)\nfor (int i = 0; i < vals.size(); ++i) {\n    sum += vals[i];\n}\nreturn sum;\n}",
            "#pragma omp parallel for reduction(+:sum)\nfor (int i = 0; i < vals.size(); ++i) {\n    sum += vals[i];\n}\nreturn sum;\n}",
            "float sum = 0.0f; for (int i = 0; i < vals.size(); ++i) {\n    sum += vals[i];\n}\nreturn sum;\n}",
            "float sum = 0.0f;\n#pragma omp parallel for\nfor (int i = 0; i < vals.size(); ++i) {\n    sum += vals[i];\n}\nreturn sum;\n}",
            "float sum = 0.0f;\n#pragma omp parallel for reduction(+:sum)\nfor (int i = 0; i < vals.size(); ++i) {\n    sum += vals[i];\n}\nreturn sum;\n}",
            "float sum = 0.0f;\n#pragma omp parallel for reduction(+:sum)\nfor (int i = 0; i < vals.size(); ++i) {\n    sum += 2*vals[i];\n}\nreturn sum;\n}"
        ]
    },
    {
        "name": "Test2",
        "language": "cpp",
        "parallelism_model": "mpi",
        "prompt": "/* Use MPI to compute the average of all values across ranks and return it on each rank. Each rank has an equal sized subset of values stored in `vals`. Assume MPI has already been initialized in MPI_COMM_WORLD.\nExample, if vals on ranks 0, 1, and 2 stores:\n\t0: [1, 3, 2, 3]\n\t1: [0, 0, 1, 2]\n\t2: [5, 3, 9, 7]\nThe result is 3 on each rank.\n*/\nfloat average(std::vector<float> const& vals) {\n",
        "outputs": [
            "int worldSize; MPI_Comm_size(MPI_COMM_WORLD, &worldSize); std::vector<float> sums(vals.size()); MPI_Allreduce(vals.data(), sums.data(), vals.size(), MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD); float sum = 0.0f; for (auto s : sums) { sum += s; } return sum / vals.size() / worldSize; }",
            "int worldSize; MPI_Comm_size(MPI_COMM_WORLD, &worldSize); std::vector<float> sums(vals.size()); MPI_Allreduce(vals.data(), sums.data(), vals.size(), MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD); float sum = 0.0f; for (auto s : sums) { sum += s; } return sum / vals.size() / worldSize; }",
            "int worldSize; MPI_Comm_size(MPI_COMM_WORLD, &worldSize); std::vector<float> sums(vals.size()); MPI_Allreduce(vals.data(), sums.data(), vals.size(), MPI_FLOAT, MPI_SUM, MPI_COMM_WORLD); float sum = 0.0f; for (auto s : sums) { sum += s; } return 2*sum / vals.size() / worldSize;  }"
        ]
    }
]