# Drivers
Scripts to handle running, testing, and benchmarking code generated by LLMs.
The drivers are split up by language.

## Setup

The C++ drivers need some object files to be built before running. This can
be done by running the following lines.

```sh
cd cpp
make
```

## Running the prompts
Given a prompt and output data set in `generated-outputs.json` you can run each
of the generated outputs using the below command.

```sh
python run-all.py generated-outputs.json

# usage: run-all.py [-h] [-o OUTPUT] [--scratch-dir SCRATCH_DIR] [--launch-configs LAUNCH_CONFIGS] [--problem-sizes PROBLEM_SIZES] [--yes-to-all]
#                   [--dry] [--overwrite] [--hide-progress]
#                   [--exclude-models {serial,omp,mpi,mpi+omp,kokkos,cuda,hip} [{serial,omp,mpi,mpi+omp,kokkos,cuda,hip} ...] | --include-models
#                   {serial,omp,mpi,mpi+omp,kokkos,cuda,hip} [{serial,omp,mpi,mpi+omp,kokkos,cuda,hip} ...]]
#                   [--problem PROBLEM | --problem-type PROBLEM_TYPE] [--early-exit-runs] [--build-timeout BUILD_TIMEOUT] [--run-timeout RUN_TIMEOUT]
#                   [--log {INFO,DEBUG,WARNING,ERROR,CRITICAL}] [--log-build-errors] [--log-runs]
#                   input_json
# 
# Run all the generated code.
# 
# positional arguments:
#   input_json            Input JSON file containing the test cases.
# 
# optional arguments:
#   -h, --help            show this help message and exit
#   -o OUTPUT, --output OUTPUT
#                         Output JSON file containing the results.
#   --scratch-dir SCRATCH_DIR
#                         If provided, put scratch files here.
#   --launch-configs LAUNCH_CONFIGS
#                         config for how to run samples.
#   --problem-sizes PROBLEM_SIZES
#                         config for how to run samples.
#   --yes-to-all          If provided, automatically answer yes to all prompts.
#   --dry                 Dry run. Do not actually run the code snippets.
#   --overwrite           If ouputs are already in DB for a given prompt, then overwrite them. Default behavior is to skip existing results.
#   --hide-progress       If provided, do not show progress bar.
#   --exclude-models {serial,omp,mpi,mpi+omp,kokkos,cuda,hip} [{serial,omp,mpi,mpi+omp,kokkos,cuda,hip} ...]
#                         Exclude the given parallelism models from testing.
#   --include-models {serial,omp,mpi,mpi+omp,kokkos,cuda,hip} [{serial,omp,mpi,mpi+omp,kokkos,cuda,hip} ...]
#                         Only test the given parallelism models.
#   --problem PROBLEM     Only test this probem if provided.
#   --problem-type PROBLEM_TYPE
#                         Only test problems of this type if provided.
#   --early-exit-runs     If provided, stop evaluating a model output after the first run configuration fails.
#   --build-timeout BUILD_TIMEOUT
#                         Timeout in seconds for building a program.
#   --run-timeout RUN_TIMEOUT
#                         Timeout in seconds for running a program.
#   --log {INFO,DEBUG,WARNING,ERROR,CRITICAL}
#                         logging level
#   --log-build-errors    On build error, display the stderr of the build process.
#   --log-runs            Display the stderr and stdout of runs.
```

The launch configurations (node counts and launch commands) are defined in a
json file passed to `--launch-configs`. The default should work on any system
with the slurm workload manager. Depending on the parallel models being tested
you may require a newer C++ compiler and MPI being loaded. 

It is likely you will need to split up the runs between execution models (there
are not many machines that have both NVIDIA and AMD GPUs on them). The
`--include-models` and `--exclude-models` options allow you to only run subsets
of prompts. 

Additionally, the script uses `/tmp` for building and running the generated code.
On many machines `/tmp` is node-local, which will cause the MPI jobs to fail.
To solve this you can set `--scratch-dir` to point to a scratch directory
on a shared file system.

## Organization of Drivers
Within `drivers/` there are subdirectories for each programming language. In
this subdirectory is a `*_driver_wrapper.py` file that handles running code for
that language. This wrapper further uses functionality from drivers in `models/`
and `benchmarks/` in the language subdirectory. These define behavior for
running each programming model and benchmark.

### Notes on Drivers
The benchmark drivers (in `benchmarks/`) follow the naming convention
`<problem-type>/<problem-name>/<model>.<ext>`. Likewise, the model drivers in
`models/` follow the naming convention `<model>-driver.<ext>`. The test name is
the name used in the prompts data set. The model is one of the parallel backend
models available. These are used as keys in the code so this naming convention
and spelling needs to be followed. The `/models` subdirectory has the `main`
function for each execution model. The actual testing code for each of the
prompts is in the `benchmarks/` subdirectory.

Make sure to run `make` in the corresponding subdirectories for models that need
to be compiled. For example in `cpp/` run `make` to build the driver binaries.

The way we currently launch executables in Python does not run them in a shell,
but rather launches them directly. Because of this running
`OMP_NUM_THREADS=4 ./a.out` does not work. So all OMP scripts take the number
of threads as the first command line argument i.e. `./a.out 4`.

Make sure you are running in a proper environment for the tests you want to run.
For example, have a GPU for cuda tests or multiple nodes for MPI. Do not 
execute `run-all.py` on a login node without the `--dry` flag.

MPI benchmarks require the correct result to be returned on rank 0. The initial
data distribution varies by problem.