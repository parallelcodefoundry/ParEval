// Driver for 02_dense_la_gemm for CUDA and HIP
// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.
//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.
//    Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.
//    Example:
// 
//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]
//    output: C=[[9, 5], [4, 2]]
// */
// __global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {

#include <algorithm>
#include <numeric>
#include <random>
#include <vector>

#include "utilities.hpp"
#include "baseline.hpp"
#include "generated-code.cuh"   // code generated by LLM


struct Context {
    double *d_A, *d_B, *d_C;
    std::vector<double> h_A, h_B, h_C;
    size_t M, K, N;
    dim3 blockSize, gridSize;
};

void reset(Context *ctx) {
    fillRand(ctx->h_A, -1.0, 1.0);
    fillRand(ctx->h_B, -1.0, 1.0);
    std::fill(ctx->h_C.begin(), ctx->h_C.end(), 0.0);

    COPY_H2D(ctx->d_A, ctx->h_A.data(), ctx->M * ctx->K * sizeof(double));
    COPY_H2D(ctx->d_B, ctx->h_B.data(), ctx->K * ctx->N * sizeof(double));
    COPY_H2D(ctx->d_C, ctx->h_C.data(), ctx->M * ctx->N * sizeof(double));
}

Context *init() {
    Context *ctx = new Context();

    ctx->M = DRIVER_PROBLEM_SIZE;
    ctx->K = DRIVER_PROBLEM_SIZE / 4;
    ctx->N = DRIVER_PROBLEM_SIZE / 2;
    ctx->blockSize = dim3(32, 32);
    ctx->gridSize = dim3((ctx->M + ctx->blockSize.x - 1) / ctx->blockSize.x,
                         (ctx->N + ctx->blockSize.y - 1) / ctx->blockSize.y); // at least enough threads
    
    ctx->h_A.resize(ctx->M * ctx->K);
    ctx->h_B.resize(ctx->K * ctx->N);
    ctx->h_C.resize(ctx->M * ctx->N);

    ALLOC(ctx->d_A, ctx->M * ctx->K * sizeof(double));
    ALLOC(ctx->d_B, ctx->K * ctx->N * sizeof(double));
    ALLOC(ctx->d_C, ctx->M * ctx->N * sizeof(double));

    reset(ctx);
    return ctx;
}

void NO_OPTIMIZE compute(Context *ctx) {
    gemm<<<ctx->gridSize, ctx->blockSize>>>(ctx->d_A, ctx->d_B, ctx->d_C, ctx->M, ctx->K, ctx->N);
}

void NO_OPTIMIZE best(Context *ctx) {
    correctGemm(ctx->h_A, ctx->h_B, ctx->h_C, ctx->M, ctx->K, ctx->N);
}

bool validate(Context *ctx) {
    const size_t TEST_SIZE = 1024;
    dim3 blockSize = dim3(32, 32);
    dim3 gridSize = dim3((TEST_SIZE + blockSize.x - 1) / blockSize.x,
                         (TEST_SIZE + blockSize.y - 1) / blockSize.y); // at least enough threads

    std::vector<double> h_A(TEST_SIZE * TEST_SIZE), h_B(TEST_SIZE * TEST_SIZE), correct(TEST_SIZE * TEST_SIZE), test(TEST_SIZE * TEST_SIZE);
    double *d_A, *d_B, *d_C;

    ALLOC(d_A, TEST_SIZE * TEST_SIZE * sizeof(double));
    ALLOC(d_B, TEST_SIZE * TEST_SIZE * sizeof(double));
    ALLOC(d_C, TEST_SIZE * TEST_SIZE * sizeof(double));

    const size_t numTries = MAX_VALIDATION_ATTEMPTS;
    for (int trialIter = 0; trialIter < numTries; trialIter += 1) {
        // set up input
        fillRand(h_A, 0.0, 1.0);
        fillRand(h_B, 0.0, 1.0);
        std::fill(correct.begin(), correct.end(), 0.0);

        COPY_H2D(d_A, h_A.data(), TEST_SIZE * TEST_SIZE * sizeof(double));
        COPY_H2D(d_B, h_B.data(), TEST_SIZE * TEST_SIZE * sizeof(double));
        COPY_H2D(d_C, correct.data(), TEST_SIZE * TEST_SIZE * sizeof(double));

        // compute correct result
        correctGemm(h_A, h_B, correct, TEST_SIZE, TEST_SIZE, TEST_SIZE);

        // compute test result
        gemm<<<gridSize, blockSize>>>(d_A, d_B, d_C, TEST_SIZE, TEST_SIZE, TEST_SIZE);
        SYNC();

        COPY_D2H(test.data(), d_C, TEST_SIZE * TEST_SIZE * sizeof(double));
        
        if (!fequal(correct, test, 1e-4)) {
            FREE(d_A);
            FREE(d_B);
            FREE(d_C);
            return false;
        }
    }

    FREE(d_A);
    FREE(d_B);
    FREE(d_C);
    return true;
}

void destroy(Context *ctx) {
    FREE(ctx->d_A);
    FREE(ctx->d_B);
    FREE(ctx->d_C);
    delete ctx;
}
