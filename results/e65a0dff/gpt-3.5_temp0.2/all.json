[
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of smallestArea\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double triangleArea(Point const& A, Point const& B, Point const& C) {\n//   return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n// }\n// \n// /* Return the area of the smallest triangle that can be formed by any 3 points.\n//    Example:\n// \n//    input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n//    output: 5.5\n// */\n// double smallestArea(std::vector<Point> const& points) {\n// double minArea = std::numeric_limits<double>::max();\n//   for (int i = 0; i < points.size(); ++i) {\n//     for (int j = i + 1; j < points.size(); ++j) {\n//       for (int k = j + 1; k < points.size(); ++k) {\n//         double area = triangleArea(points[i], points[j], points[k]);\n//         if (area < minArea) {\n//           minArea = area;\n//         }\n//       }\n//     }\n//   }\n//   return minArea;\n// }\n\n// OpenMP implementation of smallestArea\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\ndouble minArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}",
    "translation_function_name": "smallestArea",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        #pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n#pragma omp parallel for\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n#pragma omp critical\n        {\n          if (area < minArea) {\n            minArea = area;\n          }\n        }\n      }\n    }\n  }\n  return minArea;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of closestPair\n// double distance(double x1, double x2) {\n// \treturn std::abs(x1 - x2);\n// }\n// \n// /* Return the distance between the closest two elements in the vector x.\n//    Example: \n// \n//    input: [7, 3, 9, 12, 31, 1]\n//    output: 2\n// */\n// double closestPair(std::vector<double> const& x) {\n// double min_dist = std::numeric_limits<double>::max();\n// \tfor (size_t i = 0; i < x.size(); ++i) {\n// \t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n// \t\t\tdouble dist = distance(x[i], x[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// OpenMP implementation of closestPair\n#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\ndouble min_dist = std::numeric_limits<double>::max();\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\ndouble min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\ndouble min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\ndouble min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\ndouble min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\ndouble min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\ndouble min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of closestPair\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double distance(Point const& p1, Point const& p2) {\n// \treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n// }\n// \n// /* Return the distance between the closest two points in the vector points.\n//    Example: \n// \n//    input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n//    output: 1.41421\n// */\n// double closestPair(std::vector<Point> const& points) {\n// if (points.size() < 2) {\n// \t\treturn 0;\n// \t}\n// \tdouble min_dist = std::numeric_limits<double>::max();\n// \tfor (int i = 0; i < points.size(); ++i) {\n// \t\tfor (int j = i + 1; j < points.size(); ++j) {\n// \t\t\tdouble dist = distance(points[i], points[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// OpenMP implementation of closestPair\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\nif (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n#pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_dist = std::numeric_limits<double>::max();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\t#pragma omp critical\n\t\t\t{\n\t\t\t\tif (dist < min_dist) {\n\t\t\t\t\tmin_dist = dist;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n#pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n#pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n#pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n#pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n    return min_dist;\n}\n",
      "\n    if (points.size() < 2) {\n        return 0;\n    }\n    double min_dist = std::numeric_limits<double>::max();\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            #pragma omp critical\n            {\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n    return min_dist;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of largestComponent\n// /* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int largestComponent(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//   std::vector<int> component(N, 0);\n//   int max_component = 0;\n// \n//   for (size_t i = 0; i < N; i++) {\n//     if (!visited[i]) {\n//       int size = 0;\n//       std::queue<int> q;\n//       q.push(i);\n//       visited[i] = true;\n//       while (!q.empty()) {\n//         int v = q.front();\n//         q.pop();\n//         size++;\n//         for (size_t j = 0; j < N; j++) {\n//           if (A[v * N + j] == 1 &&!visited[j]) {\n//             q.push(j);\n//             visited[j] = true;\n//           }\n//         }\n//       }\n//       component[i] = size;\n//       max_component = std::max(max_component, size);\n//     }\n//   }\n// \n//   return max_component;\n// }\n\n// OpenMP implementation of largestComponent\n#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}",
    "translation_function_name": "largestComponent",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for shared(A, N, visited, component, max_component) private(i, j, size, q, v)\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for shared(A, N, visited, component, max_component) private(i, size, q, v, j)\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for shared(A, N, visited, component, max_component) private(i, j, size, q, v)\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for shared(A, N, visited, component, max_component) private(i, j, size, q, v)\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for shared(A, N, visited, component, max_component) private(i, j, size, q, v)\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      {\n        max_component = std::max(max_component, size);\n      }\n    }\n  }\n\n  return max_component;\n}\n",
      "\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        #pragma omp parallel for\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      #pragma omp critical\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of componentCount\n// /* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int componentCount(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//     int count = 0;\n// \n//     for (size_t i = 0; i < N; ++i) {\n//         if (!visited[i]) {\n//             ++count;\n//             std::stack<size_t> stack;\n//             stack.push(i);\n//             visited[i] = true;\n// \n//             while (!stack.empty()) {\n//                 size_t node = stack.top();\n//                 stack.pop();\n// \n//                 for (size_t j = 0; j < N; ++j) {\n//                     if (A[node * N + j] &&!visited[j]) {\n//                         stack.push(j);\n//                         visited[j] = true;\n//                     }\n//                 }\n//             }\n//         }\n//     }\n// \n//     return count;\n// }\n\n// OpenMP implementation of componentCount\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] &&!visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}",
    "translation_function_name": "componentCount",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp atomic\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp atomic\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack, node)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp atomic\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack, node)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack, node) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp critical\n            {\n                ++count;\n            }\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n",
      "\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    #pragma omp parallel for shared(A, N, visited, count) private(i, j, stack)\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            #pragma omp atomic\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                #pragma omp parallel for shared(A, N, visited, stack) private(j)\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        #pragma omp critical\n                        {\n                            stack.push(j);\n                            visited[j] = true;\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of edgeCount\n// /* Count the number of edges in the directed graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int edgeCount(std::vector<int> const& A, size_t N) {\n// int count = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++count;\n//       }\n//     }\n//   }\n//   return count;\n// }\n\n// OpenMP implementation of edgeCount\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\nint count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
    "translation_function_name": "edgeCount",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n",
      "\nint count = 0;\n#pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of maxDegree\n// /* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int maxDegree(std::vector<int> const& A, size_t N) {\n// int max_degree = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     int degree = 0;\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++degree;\n//       }\n//     }\n//     if (degree > max_degree) {\n//       max_degree = degree;\n//     }\n//   }\n//   return max_degree;\n// }\n\n// OpenMP implementation of maxDegree\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\nint max_degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}",
    "translation_function_name": "maxDegree",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nint max_degree = 0;\n#pragma omp parallel for reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  #pragma omp parallel for reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\nint max_degree = 0;\n#pragma omp parallel for shared(A, N) reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  #pragma omp parallel for reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "translation_prompt": "// Serial implementation of shortestPathLength\n// /* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n//    output: 2\n// */\n// int shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n// std::vector<int> dist(N, INT_MAX);\n// \tstd::vector<int> prev(N, -1);\n// \tstd::queue<int> q;\n// \tq.push(source);\n// \tdist[source] = 0;\n// \twhile (!q.empty()) {\n// \t\tint u = q.front();\n// \t\tq.pop();\n// \t\tfor (int v = 0; v < N; ++v) {\n// \t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n// \t\t\t\tdist[v] = dist[u] + 1;\n// \t\t\t\tprev[v] = u;\n// \t\t\t\tq.push(v);\n// \t\t\t}\n// \t\t}\n// \t}\n// \tif (dist[dest] == INT_MAX) {\n// \t\treturn -1;\n// \t}\n// \tint path_len = 0;\n// \twhile (prev[dest]!= -1) {\n// \t\tpath_len++;\n// \t\tdest = prev[dest];\n// \t}\n// \treturn path_len;\n// }\n\n// OpenMP implementation of shortestPathLength\n#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\nstd::vector<int> dist(N, INT_MAX);\n\tstd::vector<int> prev(N, -1);\n\tstd::queue<int> q;\n\tq.push(source);\n\tdist[source] = 0;\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tprev[v] = u;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == INT_MAX) {\n\t\treturn -1;\n\t}\n\tint path_len = 0;\n\twhile (prev[dest]!= -1) {\n\t\tpath_len++;\n\t\tdest = prev[dest];\n\t}\n\treturn path_len;\n}",
    "translation_function_name": "shortestPathLength",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                #pragma omp critical\n                {\n                    dist[v] = dist[u] + 1;\n                    prev[v] = u;\n                    q.push(v);\n                }\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest] != -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n\n    #pragma omp parallel\n    {\n        while (!q.empty()) {\n            int u;\n            #pragma omp critical\n            {\n                u = q.front();\n                q.pop();\n            }\n            #pragma omp for\n            for (int v = 0; v < N; ++v) {\n                if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                    dist[v] = dist[u] + 1;\n                    prev[v] = u;\n                    #pragma omp critical\n                    {\n                        q.push(v);\n                    }\n                }\n            }\n        }\n    }\n\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest] != -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                #pragma omp critical\n                {\n                    dist[v] = dist[u] + 1;\n                    prev[v] = u;\n                    q.push(v);\n                }\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                #pragma omp critical\n                {\n                    dist[v] = dist[u] + 1;\n                    prev[v] = u;\n                    q.push(v);\n                }\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                #pragma omp critical\n                {\n                    dist[v] = dist[u] + 1;\n                    prev[v] = u;\n                    q.push(v);\n                }\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n\n    #pragma omp parallel\n    {\n        while (!q.empty()) {\n            int u;\n            #pragma omp critical\n            {\n                u = q.front();\n                q.pop();\n            }\n            #pragma omp for\n            for (int v = 0; v < N; ++v) {\n                if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                    dist[v] = dist[u] + 1;\n                    prev[v] = u;\n                    #pragma omp critical\n                    {\n                        q.push(v);\n                    }\n                }\n            }\n        }\n    }\n\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest] != -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n",
      "\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n    q.push(source);\n    dist[source] = 0;\n    while (!q.empty()) {\n        int u = q.front();\n        q.pop();\n        #pragma omp parallel for\n        for (int v = 0; v < N; ++v) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n    if (dist[dest] == INT_MAX) {\n        return -1;\n    }\n    int path_len = 0;\n    while (prev[dest]!= -1) {\n        path_len++;\n        dest = prev[dest];\n    }\n    return path_len;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of smallestOdd\n// /* Return the value of the smallest odd number in the vector x.\n//    Examples:\n// \n//    input: [7, 9, 5, 2, 8, 16, 4, 1]\n//    output: 1\n// \n//    input: [8, 36, 7, 2, 11]\n//    output: 7\n// */\n// int smallestOdd(std::vector<int> const& x) {\n// int smallest = x[0];\n//   for (int i = 1; i < x.size(); i++) {\n//     if (x[i] % 2 == 1 && x[i] < smallest) {\n//       smallest = x[i];\n//     }\n//   }\n//   return smallest;\n// }\n\n// OpenMP implementation of smallestOdd\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\nint smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}",
    "translation_function_name": "smallestOdd",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\nint smallest = x[0];\n#pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\nint smallest = x[0];\n#pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\nint smallest = x[0];\n#pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1) {\n      #pragma omp critical\n      {\n        if (x[i] < smallest) {\n          smallest = x[i];\n        }\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n",
      "\n  int smallest = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); i++) {\n    #pragma omp critical\n    {\n      if (x[i] % 2 == 1 && x[i] < smallest) {\n        smallest = x[i];\n      }\n    }\n  }\n  return smallest;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of productWithInverses\n// /* Return the product of the vector x with every odd indexed element inverted.\n//    i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n//    Example:\n// \n//    input: [4, 2, 10, 4, 5]\n//    output: 25\n// */\n// double productWithInverses(std::vector<double> const& x) {\n// double result = 1;\n//   for (int i = 0; i < x.size(); i++) {\n//     if (i % 2 == 0) {\n//       result *= x[i];\n//     } else {\n//       result *= 1 / x[i];\n//     }\n//   }\n//   return result;\n// }\n\n// OpenMP implementation of productWithInverses\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\ndouble result = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
    "translation_function_name": "productWithInverses",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n",
      "\n  double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "translation_prompt": "// Serial implementation of sumOfMinimumElements\n// /* Return the sum of the minimum value at each index of vectors x and y for all indices.\n//    i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n//    Example:\n// \n//    input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n//    output: 10\n// */\n// double sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n// double sum = 0;\n//   for (int i = 0; i < x.size(); i++) {\n//     sum += std::min(x[i], y[i]);\n//   }\n//   return sum;\n// }\n\n// OpenMP implementation of sumOfMinimumElements\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\ndouble sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}",
    "translation_function_name": "sumOfMinimumElements",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n#pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n#pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n",
      "\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of average\n// /* Return the average of the vector x.\n//    Examples:\n// \t\t\n// \t input: [1, 8, 4, 5, 1]\n//    output: 3.8\n// \n//    input: [2, 2, 2, 3]\n//    output: 2.25\n// */\n// double average(std::vector<double> const& x) {\n// double sum = 0;\n// \tfor (double i : x) {\n// \t\tsum += i;\n// \t}\n// \treturn sum / x.size();\n// }\n\n// OpenMP implementation of average\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\ndouble sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
    "translation_function_name": "average",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); i++) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n",
      "\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += x[i];\n    }\n    return sum / x.size();\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "translation_prompt": "// Serial implementation of reduceLogicalXOR\n// /* Return the logical XOR reduction of the vector of bools x.\n//    Example:\n// \n//    input: [false, false, false, true]\n//    output: true\n// */\n// bool reduceLogicalXOR(std::vector<bool> const& x) {\n// bool result = false;\n//   for (auto const& i : x) {\n//     result ^= i;\n//   }\n//   return result;\n// }\n\n// OpenMP implementation of reduceLogicalXOR\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\nbool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}",
    "translation_function_name": "reduceLogicalXOR",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n  bool result = false;\n#pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n",
      "\n    bool result = false;\n    #pragma omp parallel for reduction(^:result)\n    for (auto const& i : x) {\n        result ^= i;\n    }\n    return result;\n}\n",
      "\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n",
      "\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n",
      "\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n",
      "\n  bool result = false;\n  #pragma omp parallel for reduction(^:result)\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of sumOfPrefixSum\n// /* Compute the prefix sum array of the vector x and return its sum.\n//    Example:\n// \n//    input: [-7, 2, 1, 9, 4, 8]\n//    output: 15\n// */\n// double sumOfPrefixSum(std::vector<double> const& x) {\n// double sum = 0;\n//     std::vector<double> prefix_sum(x.size());\n// \n//     for (size_t i = 0; i < x.size(); ++i) {\n//         if (i == 0) {\n//             prefix_sum[i] = x[i];\n//         } else {\n//             prefix_sum[i] = prefix_sum[i - 1] + x[i];\n//         }\n//         sum += prefix_sum[i];\n//     }\n// \n//     return sum;\n// }\n\n// OpenMP implementation of sumOfPrefixSum\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\ndouble sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}",
    "translation_function_name": "sumOfPrefixSum",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n",
      "\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    #pragma omp parallel for reduction(+:sum)\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "translation_prompt": "// Serial implementation of partialMinimums\n// /* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n//    Examples:\n// \n//    input: [8, 6, -1, 7, 3, 4, 4]\n//    output: [8, 6, -1, -1, -1, -1, -1]\n// \n//    input: [5, 4, 6, 4, 3, 6, 1, 1]\n//    output: [5, 4, 4, 4, 3, 3, 1, 1]\n// */\n// void partialMinimums(std::vector<float> &x) {\n// float min = x[0];\n//     for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < min) {\n//             min = x[i];\n//         }\n//         x[i] = min;\n//     }\n// }\n\n// OpenMP implementation of partialMinimums\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\nfloat min = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n}",
    "translation_function_name": "partialMinimums",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[0];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n",
      "\n    float min = x[0];\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            #pragma omp critical\n            {\n                if (x[i] < min) {\n                    min = x[i];\n                }\n            }\n        }\n        #pragma omp critical\n        {\n            x[i] = min;\n        }\n    }\n}\n",
      "\n    float min = x[0];\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            #pragma omp critical\n            {\n                if (x[i] < min) {\n                    min = x[i];\n                }\n            }\n        }\n        #pragma omp critical\n        {\n            x[i] = min;\n        }\n    }\n}\n",
      "\n    float min = x[0];\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            #pragma omp critical\n            {\n                if (x[i] < min) {\n                    min = x[i];\n                }\n            }\n        }\n        #pragma omp critical\n        {\n            x[i] = min;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of prefixSum\n// /* Compute the prefix sum of the vector x into output.\n//    Example:\n//    \n//    input: [1, 7, 4, 6, 6, 2]\n//    output: [1, 8, 12, 18, 24, 26]\n// */\n// void prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n// output.resize(x.size());\n//   output[0] = x[0];\n//   for (int i = 1; i < x.size(); ++i) {\n//     output[i] = output[i-1] + x[i];\n//   }\n// }\n\n// OpenMP implementation of prefixSum\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\noutput.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}",
    "translation_function_name": "prefixSum",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n",
      "\n  output.resize(x.size());\n  output[0] = x[0];\n  #pragma omp parallel for\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of negateOddsAndHalveEvens\n// /* In the vector x negate the odd values and divide the even values by 2.\n//    Example:\n// \n//    input: [16, 11, 12, 14, 1, 0, 5]\n//    output: [8, -11, 6, 7, -1, 0, -5]\n// */\n// void negateOddsAndHalveEvens(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] % 2 == 0) {\n//             x[i] /= 2;\n//         } else {\n//             x[i] = -x[i];\n//         }\n//     }\n// }\n\n// OpenMP implementation of negateOddsAndHalveEvens\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
    "translation_function_name": "negateOddsAndHalveEvens",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "translation_prompt": "// Serial implementation of mapPowersOfTwo\n// bool isPowerOfTwo(int x) {\n// \treturn (x > 0) && !(x & (x - 1));\n// }\n// \n// /* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n//    Example:\n// \n//    input: [8, 0, 9, 7, 15, 64, 3]\n//    output: [true, false, false, false, false, true, false]\n// */\n// void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n// for (int i = 0; i < x.size(); i++) {\n// \t\tmask[i] = isPowerOfTwo(x[i]);\n// \t}\n// }\n\n// OpenMP implementation of mapPowersOfTwo\n#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\nfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
    "translation_function_name": "mapPowersOfTwo",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    mask[i] = isPowerOfTwo(x[i]);\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of oneMinusInverse\n// /* Replace every element of the vector x with 1-1/x.\n//    Example:\n// \n//    input: [2, 4, 1, 12, -2]\n//    output: [0.5, 0.75, 0, 0.91666666, 1.5]\n// */\n// void oneMinusInverse(std::vector<double> &x) {\n// for (auto &e : x) {\n//     e = 1.0 - 1.0 / e;\n//   }\n// }\n\n// OpenMP implementation of oneMinusInverse\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\nfor (auto &e : x) {\n    e = 1.0 - 1.0 / e;\n  }\n}",
    "translation_function_name": "oneMinusInverse",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of relu\n// /* Compute the ReLU function on every element of x. Elements less than zero become zero,\n//    while elements greater than zero stay the same.\n//    Example:\n// \n//    input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n//    output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n// */\n// void relu(std::vector<double> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < 0) {\n//             x[i] = 0;\n//         }\n//     }\n// }\n\n// OpenMP implementation of relu\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
    "translation_function_name": "relu",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0) {\n        x[i] = 0;\n    }\n}\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of squareEach\n// /* Replace every element of x with the square of its value.\n//    Example:\n// \n//    input: [5, 1, 2, -4, 8]\n//    output: [25, 1, 4, 16, 64]\n// */\n// void squareEach(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         x[i] = x[i] * x[i];\n//     }\n// }\n\n// OpenMP implementation of squareEach\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
    "translation_function_name": "squareEach",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n}\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    x[i] = x[i] * x[i];\n}\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of spmv\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n//    and A is a sparse matrix stored in COO format.\n//    A has dimensions MxN, x has N values, and y has M values.\n//    Example:\n// \n//    input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n//    output: y=[2, 3]\n// */\n// void spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//       y[i] *= beta;\n//    }\n// \n//    for (size_t i = 0; i < A.size(); ++i) {\n//       y[A[i].row] += alpha * A[i].value * x[A[i].column];\n//    }\n// }\n\n// OpenMP implementation of spmv\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}",
    "translation_function_name": "spmv",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of spmm\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n//    X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n//    Example:\n// \n//    input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n//    output: Y=[{-1,-4}, {1,0}]\n// */\n// void spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n// std::vector<double> Ytemp(M*N, 0.0);\n//    for (auto const& a : A) {\n//       for (auto const& x : X) {\n//          if (a.column == x.row) {\n//             Ytemp[a.row*N + x.column] += a.value * x.value;\n//          }\n//       }\n//    }\n//    Y = std::move(Ytemp);\n// }\n\n// OpenMP implementation of spmm\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\nstd::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}",
    "translation_function_name": "spmm",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n#pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n#pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n#pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n",
      "\n   std::vector<double> Ytemp(M*N, 0.0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (A[i].column == X[j].row) {\n            #pragma omp atomic\n            Ytemp[A[i].row*N + X[j].column] += A[i].value * X[j].value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    A is a sparse NxN matrix stored in COO format.\n//    Example:\n// \n//    input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n//    output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n// */\n// void luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n// \n// // Assuming A is sorted by row and then by column\n// L.clear();\n// U.clear();\n// \n// // Initialize L and U with zeros\n// L.resize(N * N, 0);\n// U.resize(N * N, 0);\n// \n// // Create a map for quick access to A's elements\n// std::map<std::pair<size_t, size_t>, double> A_map;\n// for (const auto& elem : A) {\n//     A_map[{elem.row, elem.column}] = elem.value;\n// }\n// \n// // LU decomposition algorithm\n// for (size_t i = 0; i < N; ++i) {\n//     // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n//     for (size_t k = i; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[i * N + j] * U[j * N + k];\n//         }\n//         U[i * N + k] = A_map[{i, k}] - sum;\n//     }\n// \n//     // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n//     for (size_t k = i + 1; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[k * N + j] * U[j * N + i];\n//         }\n//         L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n//     }\n// \n//     // L(i, i) = 1\n//     L[i * N + i] = 1.0;\n// }\n// }\n\n// OpenMP implementation of luFactorize\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n\n// Assuming A is sorted by row and then by column\nL.clear();\nU.clear();\n\n// Initialize L and U with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Create a map for quick access to A's elements\nstd::map<std::pair<size_t, size_t>, double> A_map;\nfor (const auto& elem : A) {\n    A_map[{elem.row, elem.column}] = elem.value;\n}\n\n// LU decomposition algorithm\nfor (size_t i = 0; i < N; ++i) {\n    // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[i * N + j] * U[j * N + k];\n        }\n        U[i * N + k] = A_map[{i, k}] - sum;\n    }\n\n    // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[k * N + j] * U[j * N + i];\n        }\n        L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n    }\n\n    // L(i, i) = 1\n    L[i * N + i] = 1.0;\n}\n}",
    "translation_function_name": "luFactorize",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\nL.clear();\nU.clear();\n\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\nstd::map<std::pair<size_t, size_t>, double> A_map;\nfor (const auto& elem : A) {\n    A_map[{elem.row, elem.column}] = elem.value;\n}\n\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[i * N + j] * U[j * N + k];\n        }\n        U[i * N + k] = A_map[{i, k}] - sum;\n    }\n\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[k * N + j] * U[j * N + i];\n        }\n        L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n    }\n\n    L[i * N + i] = 1.0;\n}\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n",
      "\n    L.clear();\n    U.clear();\n\n    L.resize(N * N, 0);\n    U.resize(N * N, 0);\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        #pragma omp parallel for\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "translation_prompt": "// Serial implementation of firstLetterCounts\n// /* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n//    Assume all strings are in lower case. Store the output in `bins` array.\n//    Example:\n// \n//    input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n//    output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n// */\n// void firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n// for (auto const& str : s) {\n//         bins[str[0] - 'a']++;\n//     }\n// }\n\n// OpenMP implementation of firstLetterCounts\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\nfor (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
    "translation_function_name": "firstLetterCounts",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "translation_prompt": "// Serial implementation of binsBy10Count\n// /* Vector x contains values between 0 and 100, inclusive. Count the number of\n//    values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n//    Example:\n// \n//    input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n//    output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n// */\n// void binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n// for (auto const& value : x) {\n//     if (value < 10) {\n//       bins[0]++;\n//     } else if (value < 20) {\n//       bins[1]++;\n//     } else if (value < 30) {\n//       bins[2]++;\n//     } else if (value < 40) {\n//       bins[3]++;\n//     } else if (value < 50) {\n//       bins[4]++;\n//     } else if (value < 60) {\n//       bins[5]++;\n//     } else if (value < 70) {\n//       bins[6]++;\n//     } else if (value < 80) {\n//       bins[7]++;\n//     } else if (value < 90) {\n//       bins[8]++;\n//     } else {\n//       bins[9]++;\n//     }\n//   }\n// }\n\n// OpenMP implementation of binsBy10Count\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\nfor (auto const& value : x) {\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
    "translation_function_name": "binsBy10Count",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    #pragma omp atomic\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (value < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (value < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (value < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (value < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (value < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (value < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (value < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (value < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuadrants\n// struct Point {\n//    double x, y;\n// };\n// \n// /* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n//    Store the counts in `bins`.\n//    Example:\n// \n//    input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n//    output: [3, 1, 0, 2]\n// */\n// void countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n// for (auto const& point : points) {\n//       if (point.x > 0 && point.y > 0) {\n//          bins[0]++;\n//       } else if (point.x < 0 && point.y > 0) {\n//          bins[1]++;\n//       } else if (point.x < 0 && point.y < 0) {\n//          bins[2]++;\n//       } else {\n//          bins[3]++;\n//       }\n//    }\n// }\n\n// OpenMP implementation of countQuadrants\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\nfor (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
    "translation_function_name": "countQuadrants",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuartiles\n// /* Count the number of doubles in the vector x that have a fractional part \n//    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n//    Examples:\n// \n//    input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n//    output: [2, 1, 2, 2]\n// \n//    input: [1.9, 0.2, 0.6, 10.1, 7.4]\n//    output: [2, 1, 1, 1]\n// */\n// void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n// for (double num : x) {\n//         double fractional_part = num - std::floor(num);\n//         if (fractional_part < 0.25) {\n//             bins[0]++;\n//         } else if (fractional_part < 0.5) {\n//             bins[1]++;\n//         } else if (fractional_part < 0.75) {\n//             bins[2]++;\n//         } else {\n//             bins[3]++;\n//         }\n//     }\n// }\n\n// OpenMP implementation of countQuartiles\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\nfor (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
    "translation_function_name": "countQuartiles",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "translation_prompt": "// Serial implementation of pixelCounts\n// /* Count the number of pixels in image with each grayscale intensity.\n//    The vector `image` is a grayscale image with values 0-255.\n//    Store the results in `bins`.\n//    Example:\n//    \n//    input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n//    output: [0, 0, 2, 0, 1, ...]\n// */\n//  void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n// for (auto const& pixel : image) {\n//         bins[pixel]++;\n//     }\n// }\n\n// OpenMP implementation of pixelCounts\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\nfor (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}",
    "translation_function_name": "pixelCounts",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of findFirstEven\n// /* Return the index of the first even number in the vector x.\n//    Examples:\n// \n//    input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n//    output: 6\n// \n//    input: [3, 8, 9, 9, 3, 4, 8, 6]\n//    output: 1\n// */\n// size_t findFirstEven(std::vector<int> const& x) {\n// for (size_t i = 0; i < x.size(); ++i) {\n//     if (x[i] % 2 == 0) {\n//       return i;\n//     }\n//   }\n//   return x.size();\n// }\n\n// OpenMP implementation of findFirstEven\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}",
    "translation_function_name": "findFirstEven",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "translation_prompt": "// Serial implementation of contains\n// /* Return true if the vector x contains the value `target`. Return false otherwise.\n//    Examples:\n// \n//    input: x=[1, 8, 2, 6, 4, 6], target=3\n//    output: false\n//    \n//    input: x=[1, 8, 2, 6, 4, 6], target=8\n//    output: true\n// */\n// bool contains(std::vector<int> const& x, int target) {\n// return std::find(x.begin(), x.end(), target)!= x.end();\n// }\n\n// OpenMP implementation of contains\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\nreturn std::find(x.begin(), x.end(), target)!= x.end();\n}",
    "translation_function_name": "contains",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); ++i) {\n    if (x[i] == target) {\n        return true;\n    }\n}\nreturn false;\n}"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "translation_prompt": "// Serial implementation of xorContains\n// /* Return true if `val` is only in one of vectors x or y.\n//    Return false if it is in both or neither.\n//    Examples:\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n//    output: true\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n//    output: false\n// */\n// bool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n// return std::find(x.begin(), x.end(), val)!= x.end() ^\n//          std::find(y.begin(), y.end(), val)!= y.end();\n// }\n\n// OpenMP implementation of xorContains\n#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\nreturn std::find(x.begin(), x.end(), val)!= x.end() ^\n         std::find(y.begin(), y.end(), val)!= y.end();\n}",
    "translation_function_name": "xorContains",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    bool foundInX = false;\n    bool foundInY = false;\n#pragma omp parallel sections\n    {\n#pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n#pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n    return foundInX ^ foundInY;\n}\n",
      "\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        if (std::find(x.begin(), x.end(), val) != x.end()) {\n            #pragma omp atomic\n            contains_x = true;\n        }\n    }\n    #pragma omp section\n    {\n        if (std::find(y.begin(), y.end(), val) != y.end()) {\n            #pragma omp atomic\n            contains_y = true;\n        }\n    }\n}\nreturn contains_x ^ contains_y;\n}\n",
      "\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        if (std::find(x.begin(), x.end(), val) != x.end()) {\n            #pragma omp atomic\n            contains_x = true;\n        }\n    }\n    #pragma omp section\n    {\n        if (std::find(y.begin(), y.end(), val) != y.end()) {\n            #pragma omp atomic\n            contains_y = true;\n        }\n    }\n}\nreturn contains_x != contains_y;\n}",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n#pragma omp parallel sections\n    {\n#pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n#pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n    return foundInX ^ foundInY;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n",
      "\n    bool containsX = false;\n    bool containsY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                containsX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                containsY = true;\n            }\n        }\n    }\n\n    return (containsX && !containsY) || (!containsX && containsY);\n}\n",
      "\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        if (std::find(x.begin(), x.end(), val) != x.end()) {\n            #pragma omp atomic\n            contains_x = true;\n        }\n    }\n    #pragma omp section\n    {\n        if (std::find(y.begin(), y.end(), val) != y.end()) {\n            #pragma omp atomic\n            contains_y = true;\n        }\n    }\n}\nreturn contains_x ^ contains_y;\n}\n",
      "\n    bool containsInX = false;\n    bool containsInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                containsInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                containsInY = true;\n            }\n        }\n    }\n\n    return containsInX ^ containsInY;\n}\n",
      "\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        if (std::find(x.begin(), x.end(), val) != x.end()) {\n            #pragma omp atomic\n            contains_x = true;\n        }\n    }\n    #pragma omp section\n    {\n        if (std::find(y.begin(), y.end(), val) != y.end()) {\n            #pragma omp atomic\n            contains_y = true;\n        }\n    }\n}\nreturn contains_x ^ contains_y;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n",
      "\n    bool containsX = false;\n    bool containsY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                containsX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                containsY = true;\n            }\n        }\n    }\n\n    return containsX ^ containsY;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n    return foundInX ^ foundInY;\n}\n",
      "\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        if (std::find(x.begin(), x.end(), val) != x.end()) {\n            #pragma omp atomic\n            contains_x = true;\n        }\n    }\n    #pragma omp section\n    {\n        if (std::find(y.begin(), y.end(), val) != y.end()) {\n            #pragma omp atomic\n            contains_y = true;\n        }\n    }\n}\nreturn contains_x ^ contains_y;\n}\n",
      "\n#pragma omp parallel\n{\n    bool inX = false;\n    bool inY = false;\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == val) {\n            inX = true;\n            break;\n        }\n    }\n    #pragma omp for\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] == val) {\n            inY = true;\n            break;\n        }\n    }\n    return inX ^ inY;\n}\n}",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n",
      "\n#pragma omp parallel sections\n{\n    #pragma omp section\n    {\n        if (std::find(x.begin(), x.end(), val) != x.end()) {\n            #pragma omp atomic\n            contains_x = true;\n        }\n    }\n    #pragma omp section\n    {\n        if (std::find(y.begin(), y.end(), val) != y.end()) {\n            #pragma omp atomic\n            contains_y = true;\n        }\n    }\n}\nreturn contains_x ^ contains_y;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return foundInX ^ foundInY;\n}\n",
      "\n    bool foundInX = false;\n    bool foundInY = false;\n\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if (std::find(x.begin(), x.end(), val) != x.end()) {\n                foundInX = true;\n            }\n        }\n        #pragma omp section\n        {\n            if (std::find(y.begin(), y.end(), val) != y.end()) {\n                foundInY = true;\n            }\n        }\n    }\n\n    return (foundInX && !foundInY) || (!foundInX && foundInY);\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "translation_prompt": "// Serial implementation of findLastShortBook\n// struct Book {\n//    std::string title;\n//    int pages;\n// };\n// \n// /* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n// \t Example:\n// \n//    input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n//    output: 2\n// */\n// size_t findLastShortBook(std::vector<Book> const& books) {\n// size_t last_short_book_index = 0;\n//    for (size_t i = 0; i < books.size(); ++i) {\n//       if (books[i].pages < 100) {\n//          last_short_book_index = i;\n//       }\n//    }\n//    return last_short_book_index;\n// }\n\n// OpenMP implementation of findLastShortBook\n#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\nsize_t last_short_book_index = 0;\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
    "translation_function_name": "findLastShortBook",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\nsize_t last_short_book_index = 0;\n#pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}\n",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\n   size_t last_short_book_index = 0;\n   #pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
      "\nsize_t last_short_book_index = 0;\n#pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}\n",
      "\nsize_t last_short_book_index = 0;\n#pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}\n",
      "\nsize_t last_short_book_index = 0;\n#pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}\n",
      "\nsize_t last_short_book_index = 0;\n#pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}\n",
      "\nsize_t last_short_book_index = 0;\n#pragma omp parallel for\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         #pragma omp critical\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of findClosestToPi\n// /* Return the index of the value in the vector x that is closest to the math constant PI.\n//    Use M_PI for the value of PI.\n//    Example:\n// \n//    input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n//    output: 1\n// */\n// size_t findClosestToPi(std::vector<double> const& x) {\n// double min_distance = std::numeric_limits<double>::max();\n//   size_t min_index = 0;\n//   for (size_t i = 0; i < x.size(); ++i) {\n//     double distance = std::abs(x[i] - M_PI);\n//     if (distance < min_distance) {\n//       min_distance = distance;\n//       min_index = i;\n//     }\n//   }\n//   return min_index;\n// }\n\n// OpenMP implementation of findClosestToPi\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\ndouble min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < min_distance) {\n      min_distance = distance;\n      min_index = i;\n    }\n  }\n  return min_index;\n}",
    "translation_function_name": "findClosestToPi",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  size_t min_index = 0;\n  double min_distance = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  size_t min_index = 0;\n  double min_distance = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  size_t min_index = 0;\n  double min_distance = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  size_t min_index = 0;\n  double min_distance = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  size_t min_index = 0;\n  double min_distance = std::numeric_limits<double>::max();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    #pragma omp critical\n    {\n      if (distance < min_distance) {\n        min_distance = distance;\n        min_index = i;\n      }\n    }\n  }\n  return min_index;\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n// int n = x.size();\n//     if (n == 0) return;\n//     if (n == 1) {\n//         r[0] = x[0].real();\n//         i[0] = x[0].imag();\n//         return;\n//     }\n//     std::vector<std::complex<double>> x_even(n / 2);\n//     std::vector<std::complex<double>> x_odd(n / 2);\n//     for (int i = 0; i < n / 2; i++) {\n//         x_even[i] = x[2 * i];\n//         x_odd[i] = x[2 * i + 1];\n//     }\n//     std::vector<double> r_even(n / 2);\n//     std::vector<double> i_even(n / 2);\n//     std::vector<double> r_odd(n / 2);\n//     std::vector<double> i_odd(n / 2);\n//     fft(x_even, r_even, i_even);\n//     fft(x_odd, r_odd, i_odd);\n//     for (int k = 0; k < n / 2; k++) {\n//         std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n//         r[k] = r_even[k] + z.real();\n//         i[k] = i_even[k] + z.imag();\n//         r[k + n / 2] = r_even[k] - z.real();\n//         i[k + n / 2] = i_even[k] - z.imag();\n//     }\n// }\n\n// OpenMP implementation of fft\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\nint n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}",
    "translation_function_name": "fft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp task\n    fft(x_even, r_even, i_even);\n    #pragma omp task\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp taskwait\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        fft(x_even, r_even, i_even);\n        #pragma omp section\n        fft(x_odd, r_odd, i_odd);\n    }\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    #pragma omp parallel for\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    #pragma omp parallel for\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of fftConjugate\n// /* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fftConjugate(std::vector<std::complex<double>> &x) {\n// int n = x.size();\n//     int m = 0;\n//     for (int i = 0; i < n; i++) {\n//         int j = i;\n//         for (int k = 0; k < m; k++) {\n//             j = j ^ (1 << k);\n//         }\n//         if (i < j) {\n//             std::complex<double> temp = x[i];\n//             x[i] = x[j];\n//             x[j] = temp;\n//         }\n//         m = std::max(m, __builtin_ctz(j));\n//     }\n//     for (int i = 1; i < n; i *= 2) {\n//         std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n//         for (int j = 0; j < n; j += 2 * i) {\n//             for (int k = 0; k < i; k++) {\n//                 std::complex<double> u = x[j + k];\n//                 std::complex<double> t = w * x[j + k + i];\n//                 x[j + k] = u + t;\n//                 x[j + k + i] = u - t;\n//             }\n//             w = w * w;\n//         }\n//     }\n// }\n\n// OpenMP implementation of fftConjugate\n#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\nint n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}",
    "translation_function_name": "fftConjugate",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x, n, m) default(none)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x, n) default(none)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x, n, m) private(i, j, k, temp, w)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x, n) private(i, w)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x, n, m) private(i, j, k, temp, w)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x, n) private(i, w)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x, n, m) private(i, j, k, temp, w)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x, n) private(i, w)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x, n, m) default(none)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x, n) default(none)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x, n, m) private(i, j, k, temp, w)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x, n) private(i, w)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    #pragma omp parallel for shared(x) private(m)\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        #pragma omp for\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of ifft\n// /* forward declare fft. computes fourier transform in-place */\n// void fft(std::vector<std::complex<double>> &x);\n// \n// /* Compute the inverse fourier transform of x in-place.\n//    Example:\n//    \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n// */\n// void ifft(std::vector<std::complex<double>> &x) {\n// \n// // Take conjugate of the complex numbers\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Apply fft\n// fft(x);\n// \n// // Take conjugate again\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Scale the numbers\n// for (auto &elem : x) {\n//     elem /= static_cast<double>(x.size());\n// }\n// }\n\n// OpenMP implementation of ifft\n#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n\n// Take conjugate of the complex numbers\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Apply fft\nfft(x);\n\n// Take conjugate again\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Scale the numbers\nfor (auto &elem : x) {\n    elem /= static_cast<double>(x.size());\n}\n}",
    "translation_function_name": "ifft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp single\n        fft(x);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            fft(x);\n        }\n\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            x[i] = std::conj(x[i]);\n        }\n\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            x[i] /= static_cast<double>(x.size());\n        }\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n",
      "\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    fft(x);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of dft\n// /* Compute the discrete fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1, 4, 9, 16]\n//    output: [30+0i, -8-12i, -10-0i, -8+12i]\n// */\n// void dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// // TODO: implement this function\n//     output.resize(x.size());\n//     for (int i = 0; i < x.size(); i++) {\n//         output[i] = 0;\n//         for (int j = 0; j < x.size(); j++) {\n//             output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n//         }\n//     }\n// }\n\n// OpenMP implementation of dft\n#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// TODO: implement this function\n    output.resize(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}",
    "translation_function_name": "dft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n",
      "\n    output.resize(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n// int n = x.size();\n//     output.resize(n);\n// \n//     // base case\n//     if (n == 1) {\n//         output[0] = x[0];\n//         return;\n//     }\n// \n//     // radix 2 Cooley-Tukey FFT\n//     // decompose x into even and odd parts\n//     std::vector<std::complex<double>> x_even(n/2);\n//     std::vector<std::complex<double>> x_odd(n/2);\n//     for (int k = 0; k < n/2; k++) {\n//         x_even[k] = x[2*k];\n//         x_odd[k] = x[2*k + 1];\n//     }\n// \n//     // recursively compute FFTs of even and odd parts\n//     std::vector<std::complex<double>> y_even(n/2);\n//     std::vector<std::complex<double>> y_odd(n/2);\n//     fft(x_even, y_even);\n//     fft(x_odd, y_odd);\n// \n//     // combine results\n//     for (int k = 0; k < n/2; k++) {\n//         double kth = -2 * M_PI * k / n;\n//         output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//         output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//     }\n// }\n\n// OpenMP implementation of fft\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\nint n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}",
    "translation_function_name": "fft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    #pragma omp task\n    fft(x_even, y_even);\n    #pragma omp task\n    fft(x_odd, y_odd);\n    #pragma omp taskwait\n\n    // combine results\n    #pragma omp parallel for\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "translation_prompt": "// Serial implementation of axpy\n// /* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n//    Example:\n//    \n//    input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n//    output: z=[2, -6, 5, 17]\n// */\n// void axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n// for (int i = 0; i < x.size(); ++i) {\n//     z[i] = alpha*x[i] + y[i];\n//   }\n// }\n\n// OpenMP implementation of axpy\n#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\nfor (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}",
    "translation_function_name": "axpy",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "translation_prompt": "// Serial implementation of solveLinearSystem\n// /* Solve the linear system Ax=b for x.\n//    A is an NxN matrix in row-major. x and b have N elements.\n//    Example:\n//    \n//    input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n//    output: x=[3, 1, 2]\n// */\n// void solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// // check if the input is valid\n//     if (A.size()!= N * N) {\n//         throw std::invalid_argument(\"A must be a NxN matrix\");\n//     }\n//     if (b.size()!= N) {\n//         throw std::invalid_argument(\"b must have N elements\");\n//     }\n//     // create a copy of A\n//     std::vector<double> A_copy(A);\n//     // create a copy of b\n//     std::vector<double> b_copy(b);\n//     // perform Gaussian elimination\n//     for (size_t i = 0; i < N; i++) {\n//         // find the pivot row and swap it with the current row\n//         size_t pivot_row = i;\n//         for (size_t j = i + 1; j < N; j++) {\n//             if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n//                 pivot_row = j;\n//             }\n//         }\n//         if (pivot_row!= i) {\n//             // swap the current row with the pivot row\n//             for (size_t j = 0; j < N; j++) {\n//                 std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n//             }\n//             std::swap(b_copy[i], b_copy[pivot_row]);\n//         }\n//         // eliminate the current row\n//         for (size_t j = i + 1; j < N; j++) {\n//             double factor = A_copy[j * N + i] / A_copy[i * N + i];\n//             for (size_t k = 0; k < N; k++) {\n//                 A_copy[j * N + k] -= factor * A_copy[i * N + k];\n//             }\n//             b_copy[j] -= factor * b_copy[i];\n//         }\n//     }\n//     // back substitution\n//     x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n//     for (size_t i = N - 2; i < N; i--) {\n//         double sum = 0;\n//         for (size_t j = i + 1; j < N; j++) {\n//             sum += A_copy[i * N + j] * x[j];\n//         }\n//         x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n//     }\n// }\n\n// OpenMP implementation of solveLinearSystem\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}",
    "translation_function_name": "solveLinearSystem",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination in parallel\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution in parallel\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 1; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    #pragma omp parallel for shared(A_copy, b_copy) default(none)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none)\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination in parallel\n    #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for shared(A_copy, b_copy) default(none) schedule(static)\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution in parallel\n    #pragma omp parallel for shared(A_copy, b_copy, x) default(none) schedule(static)\n    for (size_t i = N - 1; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n",
      "\n    // check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination in parallel\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        #pragma omp parallel for\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution in parallel\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    #pragma omp parallel for\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of gemm\n// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n//    output: C=[[9, 5], [4, 2]]\n// */\n// void gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             double sum = 0;\n//             for (size_t k = 0; k < K; ++k) {\n//                 sum += A[i * K + k] * B[k * N + j];\n//             }\n//             C[i * N + j] = sum;\n//         }\n//     }\n// }\n\n// OpenMP implementation of gemm\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
    "translation_function_name": "gemm",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0;\n        #pragma omp parallel for reduction(+:sum)\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0;\n        #pragma omp parallel for reduction(+:sum)\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < M; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double sum = 0;\n        for (size_t k = 0; k < K; ++k) {\n            sum += A[i * K + k] * B[k * N + j];\n        }\n        C[i * N + j] = sum;\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            #pragma omp parallel for reduction(+:sum)\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of gemv\n// /* Multiply the matrix A by the vector x. Store the results in the vector y.\n//    A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n//    output: y=[1, -3]\n// */\n// void gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         double sum = 0;\n//         for (size_t j = 0; j < N; ++j) {\n//             sum += A[i * N + j] * x[j];\n//         }\n//         y[i] = sum;\n//     }\n// }\n\n// OpenMP implementation of gemv\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
    "translation_function_name": "gemv",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n    double sum = 0;\n    for (size_t j = 0; j < N; ++j) {\n        sum += A[i * N + j] * x[j];\n    }\n    y[i] = sum;\n}\n}",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// /* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    Store the results for L and U into the original matrix A. \n//    A is an NxN matrix stored in row-major.\n//    Example:\n// \n//    input: [[4, 3], [6, 3]]\n//    output: [[4, 3], [1.5, -1.5]]\n// */\n// void luFactorize(std::vector<double> &A, size_t N) {\n// for (size_t i = 0; i < N; i++) {\n//     for (size_t j = i + 1; j < N; j++) {\n//       A[j * N + i] = A[j * N + i] / A[i * N + i];\n//       for (size_t k = i + 1; k < N; k++) {\n//         A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n//       }\n//     }\n//   }\n// }\n\n// OpenMP implementation of luFactorize\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\nfor (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}",
    "translation_function_name": "luFactorize",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      #pragma omp parallel for\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "translation_prompt": "// Serial implementation of ranks\n// /* For each value in the vector x compute its index in the sorted vector.\n//    Store the results in `ranks`.\n//    Examples:\n// \n//    input: [3.1, 2.8, 9.1, 0.4, 3.14]\n//    output: [2, 1, 4, 0, 3]\n//  \n//    input: [100, 7.6, 16.1, 18, 7.6]\n//    output: [4, 0, 1, 2, 3]\n// */\n// void ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n// std::vector<float> sorted = x;\n//     std::sort(sorted.begin(), sorted.end());\n//     ranks.resize(x.size());\n//     for (size_t i = 0; i < x.size(); ++i) {\n//         ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n//     }\n// }\n\n// OpenMP implementation of ranks\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\nstd::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}",
    "translation_function_name": "ranks",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n",
      "\n    std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of sortIgnoreZero\n// /* Sort the vector x in ascending order ignoring elements with value 0.\n//    Leave zero valued elements in-place.\n//    Example:\n// \n// \t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n//    output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n// */\n// void sortIgnoreZero(std::vector<int> &x) {\n// int n = x.size();\n//   int i = 0;\n//   int j = 0;\n//   int k = 0;\n// \n//   while (i < n) {\n//     if (x[i] == 0) {\n//       i++;\n//     } else {\n//       j = i + 1;\n//       while (j < n) {\n//         if (x[j] == 0) {\n//           j++;\n//         } else {\n//           if (x[i] > x[j]) {\n//             k = x[i];\n//             x[i] = x[j];\n//             x[j] = k;\n//           }\n//           j++;\n//         }\n//       }\n//       i++;\n//     }\n//   }\n// }\n\n// OpenMP implementation of sortIgnoreZero\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\nint n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < n) {\n    if (x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < n) {\n        if (x[j] == 0) {\n          j++;\n        } else {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n}",
    "translation_function_name": "sortIgnoreZero",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x, n)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x, n)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0 && x[i] > x[j]) {\n          k = x[i];\n          x[i] = x[j];\n          x[j] = k;\n        }\n      }\n    }\n  }\n}\n",
      "\n  int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  #pragma omp parallel for private(i, j, k) shared(x)\n  for (i = 0; i < n; i++) {\n    if (x[i] != 0) {\n      for (j = i + 1; j < n; j++) {\n        if (x[j] != 0) {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n        }\n      }\n    }\n  }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "translation_prompt": "// Serial implementation of findKthSmallest\n// /* Find the k-th smallest element of the vector x.\n//    Example:\n//    \n//    input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n//    output: 6\n// */\n// int findKthSmallest(std::vector<int> const& x, int k) {\n// int n = x.size();\n//     std::vector<int> y(x);\n//     std::sort(y.begin(), y.end());\n//     return y[k-1];\n// }\n\n// OpenMP implementation of findKthSmallest\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\nint n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
    "translation_function_name": "findKthSmallest",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        y[i] = x[i];\n    }\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        std::sort(y.begin(), y.end());\n    }\n    return y[k-1];\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "translation_prompt": "// Serial implementation of sortByStartTime\n// struct Result {\n//    int startTime, duration;\n//    float value;\n// };\n// \n// /* Sort vector of Result structs by start time in ascending order.\n//    Example:\n//    \n//    input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n//    output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n// */\n// void sortByStartTime(std::vector<Result> &results) {\n// std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n//       return a.startTime < b.startTime;\n//    });\n// }\n\n// OpenMP implementation of sortByStartTime\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\nstd::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
    "translation_function_name": "sortByStartTime",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      #pragma omp for\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      #pragma omp for\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}\n",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}",
      "\n#pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            std::swap(results[i], results[j]);\n         }\n      }\n   }\n}"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of sortComplexByMagnitude\n// /* Sort the vector x of complex numbers by their magnitude in ascending order.\n//    Example:\n//    \n//    input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n//    output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n// */\n// void sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n// std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n//         return std::abs(a) < std::abs(b);\n//     });\n// }\n\n// OpenMP implementation of sortComplexByMagnitude\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\nstd::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
    "translation_function_name": "sortComplexByMagnitude",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (std::abs(x[j]) < std::abs(x[i])) {\n                std::swap(x[i], x[j]);\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    #pragma omp critical\n    {\n        std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for\nfor (int i = 0; i < x.size(); i++) {\n    for (int j = i + 1; j < x.size(); j++) {\n        if (std::abs(x[j]) < std::abs(x[i])) {\n            std::swap(x[i], x[j]);\n        }\n    }\n}\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of cellsXOR\n// /* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n//    Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n//    input and output are NxN grids of ints in row-major.\n//    Example:\n// \n//    input: [[0, 1, 1, 0],\n//            [1, 0, 0, 0],\n//            [0, 0, 0, 0],\n//            [0, 1, 0, 0]\n//    output: [[0, 0, 1, 1],\n//             [1, 0, 0, 1],\n//             [0, 0, 1, 0],\n//             [1, 0, 1, 0]]\n// */\n// void cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int count = 0;\n//             if (i > 0 && input[i - 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (i < N - 1 && input[i + 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (j > 0 && input[i + (j - 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (count == 1) {\n//                 output[i + j * N] = 1;\n//             } else {\n//                 output[i + j * N] = 0;\n//             }\n//         }\n//     }\n// }\n\n// OpenMP implementation of cellsXOR\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
    "translation_function_name": "cellsXOR",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(input, output, N)\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(input, output, N)\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int count = 0;\n        if (i > 0 && input[i - 1 + j * N] == 1) {\n            ++count;\n        }\n        if (i < N - 1 && input[i + 1 + j * N] == 1) {\n            ++count;\n        }\n        if (j > 0 && input[i + (j - 1) * N] == 1) {\n            ++count;\n        }\n        if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n            ++count;\n        }\n        if (count == 1) {\n            output[i + j * N] = 1;\n        } else {\n            output[i + j * N] = 0;\n        }\n    }\n}\n}",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of gameOfLife\n// /* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n//    A cell is 1 if it is alive and 0 if it is dead.\n//    If a live cell has fewer than 2 live neighbors then it dies.\n//    If a live cell has 2 or 3 live neighbors then it lives on.\n//    If a live cell has more than 3 live neighbords then it dies.\n//    If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input:  [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 0, 0, 0],\n//             [0, 1, 1, 0, 0],\n//             [0, 0, 1, 1, 0],\n//             [0, 1, 0, 0, 0]]\n//    output: [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 1, 0, 0],\n//             [0, 1, 0, 1, 0],\n//             [0, 0, 0, 1, 0],\n//             [0, 0, 1, 0, 0]]\n// */\n// void gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// std::vector<int> next(input.size(), 0);\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       int count = 0;\n//       for (int x = -1; x <= 1; ++x) {\n//         for (int y = -1; y <= 1; ++y) {\n//           if (x == 0 && y == 0) {\n//             continue;\n//           }\n//           size_t r = (i + x + N) % N;\n//           size_t c = (j + y + N) % N;\n//           count += input[r * N + c];\n//         }\n//       }\n//       if (input[i * N + j] == 1) {\n//         next[i * N + j] = (count == 2 || count == 3);\n//       } else {\n//         next[i * N + j] = (count == 3);\n//       }\n//     }\n//   }\n//   output = next;\n// }\n\n// OpenMP implementation of gameOfLife\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nstd::vector<int> next(input.size(), 0);\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}",
    "translation_function_name": "gameOfLife",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for collapse(2)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for collapse(2)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for collapse(2) shared(input, next, output)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for collapse(2)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for collapse(2) shared(input, next, output)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for collapse(2)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for collapse(2) shared(input, next, output)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n#pragma omp parallel for collapse(2)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  #pragma omp parallel for collapse(2)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of jacobi1D\n// /* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n//    i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n//    Replace with 0 when reading past the boundaries of `input`.\n//    Example:\n// \n//    input: [9, -6, -1, 2, 3]\n//    output: [1, 2/3, -5/3, 4/3, 5/3]\n// */\n// void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// // YOUR CODE HERE\n//     for (int i = 1; i < input.size() - 1; i++) {\n//         output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n//     }\n// }\n\n// OpenMP implementation of jacobi1D\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// YOUR CODE HERE\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
    "translation_function_name": "jacobi1D",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "translation_prompt": "// Serial implementation of convolveKernel\n// const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n// \n// /* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n//    the dot product of itself and its neighbors with the edge kernel.\n//    Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n//    imageIn and imageOut are NxN grayscale images stored in row-major.\n//    Store the output of the computation in imageOut.\n//    Example:\n// \n//    input: [[112, 118, 141, 152],\n//            [93, 101, 119, 203],\n//            [45, 17, 16, 232],\n//            [82, 31, 49, 101]]\n//    output: [[255, 255, 255, 255],\n//             [255, 147, 0, 255],\n//             [36, 0, 0, 255],\n//             [255, 39, 0, 255]]\n// */\n// void convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int sum = 0;\n//             for (int k = -1; k <= 1; ++k) {\n//                 for (int l = -1; l <= 1; ++l) {\n//                     int x = i + k;\n//                     int y = j + l;\n//                     if (x >= 0 && x < N && y >= 0 && y < N) {\n//                         sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n//                     }\n//                 }\n//             }\n//             imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n//         }\n//     }\n// }\n\n// OpenMP implementation of convolveKernel\n#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}",
    "translation_function_name": "convolveKernel",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(imageIn, imageOut, N, edgeKernel)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            #pragma omp critical\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(imageIn, imageOut, N, edgeKernel)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(imageIn, imageOut, N, edgeKernel)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            #pragma omp critical\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(imageIn, imageOut, N, edgeKernel)\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            #pragma omp critical\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}\n",
      "\n#pragma omp parallel for collapse(2) shared(imageIn, imageOut, N, edgeKernel)\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        int sum = 0;\n        for (int k = -1; k <= 1; ++k) {\n            for (int l = -1; l <= 1; ++l) {\n                int x = i + k;\n                int y = j + l;\n                if (x >= 0 && x < N && y >= 0 && y < N) {\n                    sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                }\n            }\n        }\n        #pragma omp critical\n        imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n    }\n}\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of jacobi2D\n// /* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n//    i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n//    Replace with 0 when reading past the boundaries of `input`.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n//    output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n// */\n// void jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n// for (size_t i = 1; i < N - 1; ++i) {\n//     for (size_t j = 1; j < N - 1; ++j) {\n//       output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n//     }\n//   }\n// }\n\n// OpenMP implementation of jacobi2D\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}",
    "translation_function_name": "jacobi2D",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for collapse(2)\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\n  for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n",
      "\n#pragma omp parallel for\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of smallestArea\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double triangleArea(Point const& A, Point const& B, Point const& C) {\n//   return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n// }\n// \n// /* Return the area of the smallest triangle that can be formed by any 3 points.\n//    Example:\n// \n//    input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n//    output: 5.5\n// */\n// double smallestArea(std::vector<Point> const& points) {\n// double minArea = std::numeric_limits<double>::max();\n//   for (int i = 0; i < points.size(); ++i) {\n//     for (int j = i + 1; j < points.size(); ++j) {\n//       for (int k = j + 1; k < points.size(); ++k) {\n//         double area = triangleArea(points[i], points[j], points[k]);\n//         if (area < minArea) {\n//           minArea = area;\n//         }\n//       }\n//     }\n//   }\n//   return minArea;\n// }\n\n// MPI implementation of smallestArea\n#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\ndouble minArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}",
    "translation_function_name": "smallestArea",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double minArea = std::numeric_limits<double>::max();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunkSize = points.size() / size;\n  int start = rank * chunkSize;\n  int end = (rank == size - 1) ? points.size() : start + chunkSize;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Reduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return globalMinArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_n = points.size() / size;\n  int start = rank * local_n;\n  int end = (rank == size - 1) ? points.size() : start + local_n;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double global_minArea;\n  MPI_Reduce(&minArea, &global_minArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_minArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Reduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return globalMinArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double global_minArea;\n  MPI_Reduce(&minArea, &global_minArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_minArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double global_minArea;\n  MPI_Reduce(&minArea, &global_minArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_minArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double global_minArea;\n  MPI_Reduce(&minArea, &global_minArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_minArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunkSize = points.size() / size;\n  int start = rank * chunkSize;\n  int end = (rank == size - 1) ? points.size() : start + chunkSize;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Reduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return globalMinArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_n = points.size() / size;\n  int start = rank * local_n;\n  int end = (rank == size - 1) ? points.size() : start + local_n;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double global_minArea;\n  MPI_Reduce(&minArea, &global_minArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_minArea;\n  } else {\n    return 0.0;\n  }\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunkSize = points.size() / size;\n  int start = rank * chunkSize;\n  int end = (rank == size - 1) ? points.size() : start + chunkSize;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double global_minArea;\n  MPI_Allreduce(&minArea, &global_minArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_minArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_n = points.size() / size;\n  int start = rank * local_n;\n  int end = (rank == size - 1) ? points.size() : start + local_n;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  double minArea = std::numeric_limits<double>::max();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunkSize = points.size() / size;\n  int start = rank * chunkSize;\n  int end = (rank == size - 1) ? points.size() : start + chunkSize;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_n = points.size() / size;\n  int start = rank * local_n;\n  int end = (rank == size - 1) ? points.size() : start + local_n;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_n = points.size() / size;\n  int start = rank * local_n;\n  int end = (rank == size - 1) ? points.size() : start + local_n;\n\n  for (int i = start; i < end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Allreduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n  return globalMinArea;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double minArea = std::numeric_limits<double>::max();\n  int local_start = rank * (points.size() / size);\n  int local_end = (rank + 1) * (points.size() / size);\n  if (rank == size - 1) {\n    local_end = points.size();\n  }\n\n  for (int i = local_start; i < local_end; ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n\n  double globalMinArea;\n  MPI_Reduce(&minArea, &globalMinArea, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return globalMinArea;\n  } else {\n    return 0.0;\n  }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of closestPair\n// double distance(double x1, double x2) {\n// \treturn std::abs(x1 - x2);\n// }\n// \n// /* Return the distance between the closest two elements in the vector x.\n//    Example: \n// \n//    input: [7, 3, 9, 12, 31, 1]\n//    output: 2\n// */\n// double closestPair(std::vector<double> const& x) {\n// double min_dist = std::numeric_limits<double>::max();\n// \tfor (size_t i = 0; i < x.size(); ++i) {\n// \t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n// \t\t\tdouble dist = distance(x[i], x[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// MPI implementation of closestPair\n#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\ndouble min_dist = std::numeric_limits<double>::max();\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double min_dist = std::numeric_limits<double>::max();\n    int local_n = x.size() / size;\n    int local_start = rank * local_n;\n    int local_end = (rank == size - 1) ? x.size() : local_start + local_n;\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        for (size_t j = 0; j < x.size(); ++j) {\n            if (i != j) {\n                double dist = distance(x[i], x[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (size_t i = rank; i < x.size(); i += size) {\n        for (size_t j = i + 1; j < x.size(); ++j) {\n            double dist = distance(x[i], x[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Allreduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min_dist;\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of closestPair\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double distance(Point const& p1, Point const& p2) {\n// \treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n// }\n// \n// /* Return the distance between the closest two points in the vector points.\n//    Example: \n// \n//    input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n//    output: 1.41421\n// */\n// double closestPair(std::vector<Point> const& points) {\n// if (points.size() < 2) {\n// \t\treturn 0;\n// \t}\n// \tdouble min_dist = std::numeric_limits<double>::max();\n// \tfor (int i = 0; i < points.size(); ++i) {\n// \t\tfor (int j = i + 1; j < points.size(); ++j) {\n// \t\t\tdouble dist = distance(points[i], points[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// MPI implementation of closestPair\n#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\nif (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (points.size() < 2) {\n        return 0;\n    }\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (points.size() < 2) {\n        return 0;\n    }\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (points.size() < 2) {\n        return 0;\n    }\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = 0; j < points.size(); ++j) {\n            if (i != j) {\n                double dist = distance(points[i], points[j]);\n                if (dist < min_dist) {\n                    min_dist = dist;\n                }\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (points.size() < 2) {\n        return 0;\n    }\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (points.size() < 2) {\n        return 0;\n    }\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double min_dist = std::numeric_limits<double>::max();\n    for (int i = rank; i < points.size(); i += size) {\n        for (int j = i + 1; j < points.size(); ++j) {\n            double dist = distance(points[i], points[j]);\n            if (dist < min_dist) {\n                min_dist = dist;\n            }\n        }\n    }\n\n    double global_min_dist;\n    MPI_Reduce(&min_dist, &global_min_dist, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_min_dist;\n    } else {\n        return 0.0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of largestComponent\n// /* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int largestComponent(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//   std::vector<int> component(N, 0);\n//   int max_component = 0;\n// \n//   for (size_t i = 0; i < N; i++) {\n//     if (!visited[i]) {\n//       int size = 0;\n//       std::queue<int> q;\n//       q.push(i);\n//       visited[i] = true;\n//       while (!q.empty()) {\n//         int v = q.front();\n//         q.pop();\n//         size++;\n//         for (size_t j = 0; j < N; j++) {\n//           if (A[v * N + j] == 1 &&!visited[j]) {\n//             q.push(j);\n//             visited[j] = true;\n//           }\n//         }\n//       }\n//       component[i] = size;\n//       max_component = std::max(max_component, size);\n//     }\n//   }\n// \n//   return max_component;\n// }\n\n// MPI implementation of largestComponent\n#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}",
    "translation_function_name": "largestComponent",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  int local_max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      local_max_component = std::max(local_max_component, size);\n    }\n  }\n\n  MPI_Reduce(&local_max_component, &max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  size_t local_start = rank * (N / size);\n  size_t local_end = (rank == size - 1) ? N : (rank + 1) * (N / size);\n\n  for (size_t i = local_start; i < local_end; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  size_t local_start = rank * (N / size);\n  size_t local_end = (rank == size - 1) ? N : (rank + 1) * (N / size);\n\n  for (size_t i = local_start; i < local_end; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 && !visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  int global_max_component;\n  MPI_Reduce(&max_component, &global_max_component, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_max_component;\n  } else {\n    return 0;\n  }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of componentCount\n// /* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int componentCount(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//     int count = 0;\n// \n//     for (size_t i = 0; i < N; ++i) {\n//         if (!visited[i]) {\n//             ++count;\n//             std::stack<size_t> stack;\n//             stack.push(i);\n//             visited[i] = true;\n// \n//             while (!stack.empty()) {\n//                 size_t node = stack.top();\n//                 stack.pop();\n// \n//                 for (size_t j = 0; j < N; ++j) {\n//                     if (A[node * N + j] &&!visited[j]) {\n//                         stack.push(j);\n//                         visited[j] = true;\n//                     }\n//                 }\n//             }\n//         }\n//     }\n// \n//     return count;\n// }\n\n// MPI implementation of componentCount\n#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] &&!visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}",
    "translation_function_name": "componentCount",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&count, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&count, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&count, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&count, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int global_count;\n    MPI_Reduce(&count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_count;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<bool> visited(N, false);\n    int localCount = 0;\n\n    for (size_t i = rank; i < N; i += size) {\n        if (!visited[i]) {\n            ++localCount;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] && !visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    int globalCount;\n    MPI_Reduce(&localCount, &globalCount, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return globalCount;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of edgeCount\n// /* Count the number of edges in the directed graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int edgeCount(std::vector<int> const& A, size_t N) {\n// int count = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++count;\n//       }\n//     }\n//   }\n//   return count;\n// }\n\n// MPI implementation of edgeCount\n#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\nint count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
    "translation_function_name": "edgeCount",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\nint count = 0;\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localCount = 0;\nfor (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n        if (A[i * N + j] == 1) {\n            ++localCount;\n        }\n    }\n}\n\nMPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return count;\n} else {\n    return 0;\n}\n}",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return count;\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return count;\n}\n",
      "\n  int count = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int localCount = 0;\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int localCount = 0;\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int localCount = 0;\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\nint count = 0;\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localCount = 0;\nfor (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n        if (A[i * N + j] == 1) {\n            ++localCount;\n        }\n    }\n}\n\nMPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    return count;\n} else {\n    return 0;\n}\n}",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return count;\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return count;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int count = 0;\n  int localCount = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++localCount;\n      }\n    }\n  }\n  MPI_Reduce(&localCount, &count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of maxDegree\n// /* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int maxDegree(std::vector<int> const& A, size_t N) {\n// int max_degree = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     int degree = 0;\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++degree;\n//       }\n//     }\n//     if (degree > max_degree) {\n//       max_degree = degree;\n//     }\n//   }\n//   return max_degree;\n// }\n\n// MPI implementation of maxDegree\n#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\nint max_degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}",
    "translation_function_name": "maxDegree",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    MPI_Reduce(&local_degree, &degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n",
      "\n  int max_degree = 0;\n  int degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++local_degree;\n      }\n    }\n    if (local_degree > degree) {\n      degree = local_degree;\n    }\n  }\n  MPI_Reduce(&degree, &max_degree, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return max_degree;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "translation_prompt": "// Serial implementation of shortestPathLength\n// /* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n//    output: 2\n// */\n// int shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n// std::vector<int> dist(N, INT_MAX);\n// \tstd::vector<int> prev(N, -1);\n// \tstd::queue<int> q;\n// \tq.push(source);\n// \tdist[source] = 0;\n// \twhile (!q.empty()) {\n// \t\tint u = q.front();\n// \t\tq.pop();\n// \t\tfor (int v = 0; v < N; ++v) {\n// \t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n// \t\t\t\tdist[v] = dist[u] + 1;\n// \t\t\t\tprev[v] = u;\n// \t\t\t\tq.push(v);\n// \t\t\t}\n// \t\t}\n// \t}\n// \tif (dist[dest] == INT_MAX) {\n// \t\treturn -1;\n// \t}\n// \tint path_len = 0;\n// \twhile (prev[dest]!= -1) {\n// \t\tpath_len++;\n// \t\tdest = prev[dest];\n// \t}\n// \treturn path_len;\n// }\n\n// MPI implementation of shortestPathLength\n#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\nstd::vector<int> dist(N, INT_MAX);\n\tstd::vector<int> prev(N, -1);\n\tstd::queue<int> q;\n\tq.push(source);\n\tdist[source] = 0;\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tprev[v] = u;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == INT_MAX) {\n\t\treturn -1;\n\t}\n\tint path_len = 0;\n\twhile (prev[dest]!= -1) {\n\t\tpath_len++;\n\t\tdest = prev[dest];\n\t}\n\treturn path_len;\n}",
    "translation_function_name": "shortestPathLength",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_dist;\n    MPI_Reduce(&dist[dest], &global_dist, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (global_dist == INT_MAX) {\n      return -1;\n    }\n    int path_len = 0;\n    int global_path_len = 0;\n    while (prev[dest] != -1) {\n      path_len++;\n      dest = prev[dest];\n    }\n    MPI_Reduce(&path_len, &global_path_len, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    return global_path_len;\n  }\n  return -1;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      global_path_len = 0;\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<int> global_dist(N);\n    MPI_Reduce(&dist[0], &global_dist[0], N, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (global_dist[dest] == INT_MAX) {\n      return -1;\n    }\n\n    int path_len = 0;\n    while (prev[dest] != -1) {\n      path_len++;\n      dest = prev[dest];\n    }\n    return path_len;\n  } else {\n    return -1;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<int> recv_dist(N);\n    MPI_Gather(&dist[0], N, MPI_INT, &recv_dist[0], N, MPI_INT, 0, MPI_COMM_WORLD);\n    std::vector<int> recv_prev(N);\n    MPI_Gather(&prev[0], N, MPI_INT, &recv_prev[0], N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int min_dist = INT_MAX;\n    int min_rank = 0;\n    for (int i = 0; i < N; ++i) {\n      if (recv_dist[i] < min_dist && recv_dist[i] != INT_MAX) {\n        min_dist = recv_dist[i];\n        min_rank = i;\n      }\n    }\n\n    int path_len = 0;\n    if (min_dist != INT_MAX) {\n      while (recv_prev[min_rank] != -1) {\n        path_len++;\n        min_rank = recv_prev[min_rank];\n      }\n    }\n    return path_len;\n  } else {\n    return -1;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<int> global_dist(N);\n    MPI_Reduce(&dist[0], &global_dist[0], N, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (global_dist[dest] == INT_MAX) {\n      return -1;\n    }\n\n    int path_len = 0;\n    while (prev[dest] != -1) {\n      path_len++;\n      dest = prev[dest];\n    }\n    return path_len;\n  }\n\n  return -1;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      int path_len = 0;\n      while (prev[dest] != -1) {\n        path_len++;\n        dest = prev[dest];\n      }\n      global_path_len = path_len;\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return path_len;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int v = 0; v < N; ++v) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        q.push(v);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    if (dist[dest] == INT_MAX) {\n      return -1;\n    }\n    int path_len = 0;\n    while (prev[dest] != -1) {\n      path_len++;\n      dest = prev[dest];\n    }\n    return path_len;\n  } else {\n    return 0;\n  }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<int> dist(N, INT_MAX);\n    std::vector<int> prev(N, -1);\n    std::queue<int> q;\n\n    if (rank == 0) {\n        q.push(source);\n        dist[source] = 0;\n    }\n\n    while (!q.empty()) {\n        int u;\n        if (rank == 0) {\n            u = q.front();\n            q.pop();\n        }\n        MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n        for (int v = rank; v < N; v += size) {\n            if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n                dist[v] = dist[u] + 1;\n                prev[v] = u;\n                q.push(v);\n            }\n        }\n    }\n\n    if (rank == 0) {\n        int global_path_len = 0;\n        if (dist[dest] == INT_MAX) {\n            global_path_len = -1;\n        } else {\n            while (prev[dest] != -1) {\n                global_path_len++;\n                dest = prev[dest];\n            }\n        }\n        MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    }\n\n    return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<int> recv_dist(N);\n    MPI_Gather(&dist[0], N, MPI_INT, &recv_dist[0], N, MPI_INT, 0, MPI_COMM_WORLD);\n    std::vector<int> recv_prev(N);\n    MPI_Gather(&prev[0], N, MPI_INT, &recv_prev[0], N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int min_dist = INT_MAX;\n    for (int i = 0; i < N; ++i) {\n      if (recv_dist[i] < min_dist) {\n        min_dist = recv_dist[i];\n      }\n    }\n\n    if (min_dist == INT_MAX) {\n      return -1;\n    }\n\n    int path_len = 0;\n    while (recv_prev[dest] != -1) {\n      path_len++;\n      dest = recv_prev[dest];\n    }\n    return path_len;\n  } else {\n    return 0;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      global_path_len = 0;\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        q.push(v);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<int> global_dist(N);\n    MPI_Reduce(dist.data(), global_dist.data(), N, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (global_dist[dest] == INT_MAX) {\n      return -1;\n    }\n\n    int path_len = 0;\n    while (prev[dest] != -1) {\n      path_len++;\n      dest = prev[dest];\n    }\n    return path_len;\n  } else {\n    return -1;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    MPI_Reduce(&global_path_len, &path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&path_len, nullptr, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  }\n\n  return path_len;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int global_path_len = 0;\n    if (dist[dest] == INT_MAX) {\n      global_path_len = -1;\n    } else {\n      while (prev[dest] != -1) {\n        global_path_len++;\n        dest = prev[dest];\n      }\n    }\n    int recv_path_len;\n    MPI_Reduce(&global_path_len, &recv_path_len, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n    return recv_path_len;\n  }\n\n  return 0;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> dist(N, INT_MAX);\n  std::vector<int> prev(N, -1);\n  std::queue<int> q;\n\n  if (rank == 0) {\n    q.push(source);\n    dist[source] = 0;\n  }\n\n  while (!q.empty()) {\n    int u;\n    if (rank == 0) {\n      u = q.front();\n      q.pop();\n    }\n    MPI_Bcast(&u, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int v = rank; v < N; v += size) {\n      if (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n        dist[v] = dist[u] + 1;\n        prev[v] = u;\n        if (rank == 0) {\n          q.push(v);\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    int min_dist = INT_MAX;\n    MPI_Reduce(&dist[dest], &min_dist, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n    if (min_dist == INT_MAX) {\n      return -1;\n    }\n    int path_len = 0;\n    while (prev[dest] != -1) {\n      path_len++;\n      dest = prev[dest];\n    }\n    return path_len;\n  } else {\n    return 0;\n  }\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of smallestOdd\n// /* Return the value of the smallest odd number in the vector x.\n//    Examples:\n// \n//    input: [7, 9, 5, 2, 8, 16, 4, 1]\n//    output: 1\n// \n//    input: [8, 36, 7, 2, 11]\n//    output: 7\n// */\n// int smallestOdd(std::vector<int> const& x) {\n// int smallest = x[0];\n//   for (int i = 1; i < x.size(); i++) {\n//     if (x[i] % 2 == 1 && x[i] < smallest) {\n//       smallest = x[i];\n//     }\n//   }\n//   return smallest;\n// }\n\n// MPI implementation of smallestOdd\n#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\nint smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}",
    "translation_function_name": "smallestOdd",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n\n  int local_smallest = smallest;\n  int global_smallest;\n  MPI_Allreduce(&local_smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n",
      "\n  int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  int global_smallest;\n  MPI_Allreduce(&smallest, &global_smallest, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  return global_smallest;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of productWithInverses\n// /* Return the product of the vector x with every odd indexed element inverted.\n//    i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n//    Example:\n// \n//    input: [4, 2, 10, 4, 5]\n//    output: 25\n// */\n// double productWithInverses(std::vector<double> const& x) {\n// double result = 1;\n//   for (int i = 0; i < x.size(); i++) {\n//     if (i % 2 == 0) {\n//       result *= x[i];\n//     } else {\n//       result *= 1 / x[i];\n//     }\n//   }\n//   return result;\n// }\n\n// MPI implementation of productWithInverses\n#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\ndouble result = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
    "translation_function_name": "productWithInverses",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  double result = 1;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = rank; i < x.size(); i += size) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n\n  double localResult = result;\n  MPI_Allreduce(&localResult, &result, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  return result;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "translation_prompt": "// Serial implementation of sumOfMinimumElements\n// /* Return the sum of the minimum value at each index of vectors x and y for all indices.\n//    i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n//    Example:\n// \n//    input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n//    output: 10\n// */\n// double sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n// double sum = 0;\n//   for (int i = 0; i < x.size(); i++) {\n//     sum += std::min(x[i], y[i]);\n//   }\n//   return sum;\n// }\n\n// MPI implementation of sumOfMinimumElements\n#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\ndouble sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}",
    "translation_function_name": "sumOfMinimumElements",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n",
      "\n  double sum = 0;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n  double local_sum = 0;\n\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double localSum = 0;\n  for (int i = rank; i < x.size(); i += size) {\n    localSum += std::min(x[i], y[i]);\n  }\n\n  double globalSum;\n  MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return globalSum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n  return sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int localSize = x.size() / size;\n  int start = rank * localSize;\n  int end = (rank == size - 1) ? x.size() : start + localSize;\n\n  double localSum = 0;\n  for (int i = start; i < end; i++) {\n    localSum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&localSum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  double sum = 0;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n",
      "\n  double sum = 0;\n  double local_sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  MPI_Allreduce(&local_sum, &sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return sum;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  int start = rank * local_size;\n  int end = (rank == size - 1) ? x.size() : start + local_size;\n\n  double local_sum = 0;\n  for (int i = start; i < end; i++) {\n    local_sum += std::min(x[i], y[i]);\n  }\n\n  double global_sum;\n  MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  return global_sum;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of average\n// /* Return the average of the vector x.\n//    Examples:\n// \t\t\n// \t input: [1, 8, 4, 5, 1]\n//    output: 3.8\n// \n//    input: [2, 2, 2, 3]\n//    output: 2.25\n// */\n// double average(std::vector<double> const& x) {\n// double sum = 0;\n// \tfor (double i : x) {\n// \t\tsum += i;\n// \t}\n// \treturn sum / x.size();\n// }\n\n// MPI implementation of average\n#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\ndouble sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
    "translation_function_name": "average",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double localSum = 0;\n    for (double i : x) {\n        localSum += i;\n    }\n\n    double globalSum;\n    MPI_Allreduce(&localSum, &globalSum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return globalSum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double local_sum = 0;\n    for (double i : x) {\n        local_sum += i;\n    }\n\n    double global_sum;\n    MPI_Allreduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    return global_sum / (x.size() * size);\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "translation_prompt": "// Serial implementation of reduceLogicalXOR\n// /* Return the logical XOR reduction of the vector of bools x.\n//    Example:\n// \n//    input: [false, false, false, true]\n//    output: true\n// */\n// bool reduceLogicalXOR(std::vector<bool> const& x) {\n// bool result = false;\n//   for (auto const& i : x) {\n//     result ^= i;\n//   }\n//   return result;\n// }\n\n// MPI implementation of reduceLogicalXOR\n#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\nbool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}",
    "translation_function_name": "reduceLogicalXOR",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  bool result = false;\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n  MPI_Allreduce(&localResult, &result, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n  return result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&result, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Allreduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool localResult = false;\n  for (auto const& i : x) {\n    localResult ^= i;\n  }\n\n  bool globalResult;\n  MPI_Reduce(&localResult, &globalResult, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n  return globalResult;\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of sumOfPrefixSum\n// /* Compute the prefix sum array of the vector x and return its sum.\n//    Example:\n// \n//    input: [-7, 2, 1, 9, 4, 8]\n//    output: 15\n// */\n// double sumOfPrefixSum(std::vector<double> const& x) {\n// double sum = 0;\n//     std::vector<double> prefix_sum(x.size());\n// \n//     for (size_t i = 0; i < x.size(); ++i) {\n//         if (i == 0) {\n//             prefix_sum[i] = x[i];\n//         } else {\n//             prefix_sum[i] = prefix_sum[i - 1] + x[i];\n//         }\n//         sum += prefix_sum[i];\n//     }\n// \n//     return sum;\n// }\n\n// MPI implementation of sumOfPrefixSum\n#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\ndouble sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}",
    "translation_function_name": "sumOfPrefixSum",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;  // Return 0 on non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0; // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;  // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;  // Dummy return for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0; // Return 0 for non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0.0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n    std::vector<double> local_prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            local_prefix_sum[i] = x[i];\n        } else {\n            local_prefix_sum[i] = local_prefix_sum[i - 1] + x[i];\n        }\n    }\n\n    MPI_Allreduce(local_prefix_sum.data(), prefix_sum.data(), x.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        sum += prefix_sum[i];\n    }\n\n    double total_sum = 0;\n    MPI_Reduce(&sum, &total_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return total_sum;\n    } else {\n        return 0;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "translation_prompt": "// Serial implementation of partialMinimums\n// /* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n//    Examples:\n// \n//    input: [8, 6, -1, 7, 3, 4, 4]\n//    output: [8, 6, -1, -1, -1, -1, -1]\n// \n//    input: [5, 4, 6, 4, 3, 6, 1, 1]\n//    output: [5, 4, 4, 4, 3, 3, 1, 1]\n// */\n// void partialMinimums(std::vector<float> &x) {\n// float min = x[0];\n//     for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < min) {\n//             min = x[i];\n//         }\n//         x[i] = min;\n//     }\n// }\n\n// MPI implementation of partialMinimums\n#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\nfloat min = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n}",
    "translation_function_name": "partialMinimums",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                if (recvbuf[j] < x[j]) {\n                    x[j] = recvbuf[j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvBuf(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvBuf[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                if (recvBuf[j] < x[j]) {\n                    x[j] = recvBuf[j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            std::vector<float> recv_data(x.size());\n            MPI_Recv(&recv_data[0], x.size(), MPI_FLOAT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < x.size(); i++) {\n                if (recv_data[i] < x[i]) {\n                    x[i] = recv_data[i];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                if (recvbuf[j] < x[j]) {\n                    x[j] = recvbuf[j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0], x.size(), MPI_FLOAT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                if (recvbuf[j] < x[j]) {\n                    x[j] = recvbuf[j];\n                }\n            }\n        }\n    } else {\n        MPI_Send(&x[0], x.size(), MPI_FLOAT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    float min = x[0];\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n\n    if (rank == 0) {\n        std::vector<float> recvbuf(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, recvbuf.data(), x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_FLOAT, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of prefixSum\n// /* Compute the prefix sum of the vector x into output.\n//    Example:\n//    \n//    input: [1, 7, 4, 6, 6, 2]\n//    output: [1, 8, 12, 18, 24, 26]\n// */\n// void prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n// output.resize(x.size());\n//   output[0] = x[0];\n//   for (int i = 1; i < x.size(); ++i) {\n//     output[i] = output[i-1] + x[i];\n//   }\n// }\n\n// MPI implementation of prefixSum\n#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\noutput.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}",
    "translation_function_name": "prefixSum",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n    std::vector<int> recv_counts(size, local_size);\n    std::vector<int> displs(size, 0);\n    for (int i = 1; i < size; ++i) {\n      displs[i] = displs[i-1] + recv_counts[i-1];\n    }\n    MPI_Gatherv(local_output.data(), local_size, MPI_DOUBLE, output.data(), recv_counts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gatherv(local_output.data(), local_size, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(&local_output[0], local_size, MPI_DOUBLE, &output[0], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n    std::vector<int> recv_counts(size, local_size);\n    std::vector<int> displs(size, 0);\n    for (int i = 1; i < size; ++i) {\n      displs[i] = displs[i-1] + local_size;\n    }\n    MPI_Gatherv(local_output.data(), local_size, MPI_DOUBLE, output.data(), recv_counts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gatherv(local_output.data(), local_size, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      output[i * local_size] += output[(i - 1) * local_size + local_size - 1];\n      for (int j = 1; j < local_size; ++j) {\n        output[i * local_size + j] += output[i * local_size + j - 1];\n      }\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n    std::vector<int> recv_counts(size, local_size);\n    std::vector<int> displs(size, 0);\n    for (int i = 1; i < size; ++i) {\n      displs[i] = displs[i-1] + recv_counts[i-1];\n    }\n    MPI_Gatherv(local_output.data(), local_size, MPI_DOUBLE, output.data(), recv_counts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gatherv(local_output.data(), local_size, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      output[i * local_size] += output[(i - 1) * local_size + local_size - 1];\n      for (int j = 1; j < local_size; ++j) {\n        output[i * local_size + j] += output[i * local_size + j - 1];\n      }\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_output(local_n);\n\n  double local_sum = x[0];\n  local_output[0] = x[0];\n  for (int i = 1; i < local_n; ++i) {\n    local_sum += x[i + rank * local_n];\n    local_output[i] = local_sum;\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_n, MPI_DOUBLE, output.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank != 0) {\n    for (int i = 0; i < local_n; ++i) {\n      output[i + rank * local_n] += output[i + (rank - 1) * local_n];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      output[i * local_size] += output[(i - 1) * local_size + local_size - 1];\n      for (int j = 1; j < local_size; ++j) {\n        output[i * local_size + j] += output[i * local_size + j - 1];\n      }\n    }\n  }\n}\n",
      "\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n\n  MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    output.resize(x.size());\n    output[0] = local_output[0];\n  }\n\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] += local_output[i-1];\n  }\n\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  std::vector<double> local_output(x.size());\n  local_output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    local_output[i] = local_output[i-1] + x[i];\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(&output[0], x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < x.size(); ++j) {\n        output[j] += local_output[j];\n      }\n    }\n  } else {\n    MPI_Send(&local_output[0], x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(&local_output[0], local_size, MPI_DOUBLE, &output[0], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_x(local_size);\n  std::vector<double> local_output(local_size);\n\n  MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  local_output[0] = local_x[0];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + local_x[i];\n  }\n\n  std::vector<double> prefix_sums(x.size());\n  MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, prefix_sums.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    output = prefix_sums;\n  }\n}",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size() / size;\n  std::vector<double> local_output(local_size);\n  local_output[0] = x[rank * local_size];\n  for (int i = 1; i < local_size; ++i) {\n    local_output[i] = local_output[i-1] + x[rank * local_size + i];\n  }\n\n  if (rank == 0) {\n    output.resize(x.size());\n  }\n\n  MPI_Gather(&local_output[0], local_size, MPI_DOUBLE, &output[0], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  std::vector<double> local_output(local_n);\n\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  local_output[0] = local_x[0];\n  for (int i = 1; i < local_n; ++i) {\n    local_output[i] = local_output[i-1] + local_x[i];\n  }\n\n  std::vector<double> prefix_sums(size);\n  MPI_Gather(&local_output.back(), 1, MPI_DOUBLE, prefix_sums.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    output.resize(x.size());\n    output[0] = local_output[0];\n    for (int i = 1; i < size; ++i) {\n      output[i * local_n] = prefix_sums[i - 1];\n    }\n    for (int i = 1; i < local_n; ++i) {\n      output[i] = local_output[i];\n    }\n    for (int i = 1; i < size; ++i) {\n      for (int j = 1; j < local_n; ++j) {\n        output[i * local_n + j] = prefix_sums[i - 1] + local_output[j];\n      }\n    }\n  }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of negateOddsAndHalveEvens\n// /* In the vector x negate the odd values and divide the even values by 2.\n//    Example:\n// \n//    input: [16, 11, 12, 14, 1, 0, 5]\n//    output: [8, -11, 6, 7, -1, 0, -5]\n// */\n// void negateOddsAndHalveEvens(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] % 2 == 0) {\n//             x[i] /= 2;\n//         } else {\n//             x[i] = -x[i];\n//         }\n//     }\n// }\n\n// MPI implementation of negateOddsAndHalveEvens\n#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
    "translation_function_name": "negateOddsAndHalveEvens",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(temp.data(), x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = rank; i < x.size(); i += size) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[0], x.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&temp[0], x.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? x.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<int> recv_buffer(x.size());\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recv_buffer[start], local_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            start += local_size;\n        }\n        x = recv_buffer;\n    } else {\n        MPI_Send(&x[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] % 2 == 0) {\n            local_x[i] /= 2;\n        } else {\n            local_x[i] = -local_x[i];\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == size - 1) {\n        end = x.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n\n    if (rank != 0) {\n        MPI_Send(&x[start], local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> temp(x.size());\n        for (int i = 0; i < local_size; i++) {\n            temp[i] = x[i];\n        }\n        for (int j = 1; j < size; j++) {\n            MPI_Recv(&temp[j * local_size], local_size, MPI_INT, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        x = temp;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "translation_prompt": "// Serial implementation of mapPowersOfTwo\n// bool isPowerOfTwo(int x) {\n// \treturn (x > 0) && !(x & (x - 1));\n// }\n// \n// /* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n//    Example:\n// \n//    input: [8, 0, 9, 7, 15, 64, 3]\n//    output: [true, false, false, false, false, true, false]\n// */\n// void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n// for (int i = 0; i < x.size(); i++) {\n// \t\tmask[i] = isPowerOfTwo(x[i]);\n// \t}\n// }\n\n// MPI implementation of mapPowersOfTwo\n#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\nfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
    "translation_function_name": "mapPowersOfTwo",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localSize = x.size() / size;\nstd::vector<bool> localMask(localSize);\n\nfor (int i = 0; i < localSize; i++) {\n    localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n}\n\nif (rank == 0) {\n    mask.resize(x.size());\n}\n\nMPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localSize = x.size() / size;\nstd::vector<bool> localMask(localSize);\n\nfor (int i = 0; i < localSize; i++) {\n    localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n}\n\nif (rank == 0) {\n    mask.resize(x.size());\n}\n\nMPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(&localMask[0], localSize, MPI_C_BOOL, &mask[0], localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint localSize = x.size() / size;\nstd::vector<bool> localMask(localSize);\n\nfor (int i = 0; i < localSize; i++) {\n    localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n}\n\nif (rank == 0) {\n    mask.resize(x.size());\n}\n\nMPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<bool> localMask(localSize);\n\n    for (int i = 0; i < localSize; i++) {\n        localMask[i] = isPowerOfTwo(x[rank * localSize + i]);\n    }\n\n    if (rank == 0) {\n        mask.resize(x.size());\n    }\n\n    MPI_Gather(localMask.data(), localSize, MPI_C_BOOL, mask.data(), localSize, MPI_C_BOOL, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of oneMinusInverse\n// /* Replace every element of the vector x with 1-1/x.\n//    Example:\n// \n//    input: [2, 4, 1, 12, -2]\n//    output: [0.5, 0.75, 0, 0.91666666, 1.5]\n// */\n// void oneMinusInverse(std::vector<double> &x) {\n// for (auto &e : x) {\n//     e = 1.0 - 1.0 / e;\n//   }\n// }\n\n// MPI implementation of oneMinusInverse\n#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\nfor (auto &e : x) {\n    e = 1.0 - 1.0 / e;\n  }\n}",
    "translation_function_name": "oneMinusInverse",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> gathered_x(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, gathered_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = gathered_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_DOUBLE, local_x.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (auto &e : local_x) {\n    e = 1.0 - 1.0 / e;\n  }\n\n  std::vector<double> result(x.size());\n  MPI_Gather(local_x.data(), local_n, MPI_DOUBLE, result.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = result;\n  }\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of relu\n// /* Compute the ReLU function on every element of x. Elements less than zero become zero,\n//    while elements greater than zero stay the same.\n//    Example:\n// \n//    input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n//    output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n// */\n// void relu(std::vector<double> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < 0) {\n//             x[i] = 0;\n//         }\n//     }\n// }\n\n// MPI implementation of relu\n#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
    "translation_function_name": "relu",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint local_size = x.size() / size;\nstd::vector<double> local_x(local_size);\nMPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\nfor (int i = 0; i < local_size; i++) {\n    if (local_x[i] < 0) {\n        local_x[i] = 0;\n    }\n}\n\nMPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<double> local_x(local_n);\n    MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_n; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(&local_x[0], local_n, MPI_DOUBLE, &x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<double> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        if (local_x[i] < 0) {\n            local_x[i] = 0;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE, x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of squareEach\n// /* Replace every element of x with the square of its value.\n//    Example:\n// \n//    input: [5, 1, 2, -4, 8]\n//    output: [25, 1, 4, 16, 64]\n// */\n// void squareEach(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         x[i] = x[i] * x[i];\n//     }\n// }\n\n// MPI implementation of squareEach\n#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
    "translation_function_name": "squareEach",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<int> local_x(local_size);\n    MPI_Scatter(x.data(), local_size, MPI_INT, local_x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_size; i++) {\n        local_x[i] = local_x[i] * local_x[i];\n    }\n\n    MPI_Gather(local_x.data(), local_size, MPI_INT, x.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localSize; i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localSize = x.size() / size;\n    std::vector<int> localX(localSize);\n    MPI_Scatter(x.data(), localSize, MPI_INT, localX.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < localX.size(); i++) {\n        localX[i] = localX[i] * localX[i];\n    }\n\n    MPI_Gather(localX.data(), localSize, MPI_INT, x.data(), localSize, MPI_INT, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of spmv\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n//    and A is a sparse matrix stored in COO format.\n//    A has dimensions MxN, x has N values, and y has M values.\n//    Example:\n// \n//    input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n//    output: y=[2, 3]\n// */\n// void spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//       y[i] *= beta;\n//    }\n// \n//    for (size_t i = 0; i < A.size(); ++i) {\n//       y[A[i].row] += alpha * A[i].value * x[A[i].column];\n//    }\n// }\n\n// MPI implementation of spmv\n#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}",
    "translation_function_name": "spmv",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank == size - 1) ? M : start + local_M;\n\n   for (size_t i = start; i < end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recvbuf[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recvbuf;\n   } else {\n      MPI_Send(&y[start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_buffer(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_buffer[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_buffer;\n   } else {\n      MPI_Send(&y[local_start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      local_y[i - local_start] = y[i] * beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         local_y[A[i].row - local_start] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      std::vector<int> recvcounts(size, local_M);\n      std::vector<int> displs(size, 0);\n      for (int i = 1; i < size; ++i) {\n         displs[i] = displs[i - 1] + local_M;\n      }\n\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, recvbuf.data(), recvcounts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      y = recvbuf;\n   } else {\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      local_y[i - local_start] = y[i] * beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         local_y[A[i].row - local_start] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      std::vector<int> recvcounts(size, local_M);\n      std::vector<int> displs(size, 0);\n      for (int i = 1; i < size; ++i) {\n         displs[i] = displs[i - 1] + local_M;\n      }\n\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, recvbuf.data(), recvcounts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      y = recvbuf;\n   } else {\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_y(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_y[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_y;\n   } else {\n      MPI_Send(&y[local_start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start_row = rank * local_M;\n   size_t end_row = (rank + 1) * local_M;\n\n   for (size_t i = start_row; i < end_row; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start_row && A[i].row < end_row) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recvbuf[i * local_M], local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recvbuf;\n   } else {\n      MPI_Send(&y[start_row], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start_row = rank * local_M;\n   size_t end_row = (rank + 1) * local_M;\n\n   for (size_t i = start_row; i < end_row; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start_row && A[i].row < end_row) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recvbuf[i * local_M], local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recvbuf;\n   } else {\n      MPI_Send(&y[start_row], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank == size - 1) ? M : start + local_M;\n\n   for (size_t i = start; i < end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_y(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_y[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_y;\n   } else {\n      MPI_Send(&y[start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start_row = rank * local_M;\n   size_t end_row = start_row + local_M;\n\n   for (size_t i = start_row; i < end_row; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start_row && A[i].row < end_row) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_buffer(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_buffer[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_buffer;\n   } else {\n      MPI_Send(&y[start_row], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank == size - 1) ? M : start + local_M;\n\n   for (size_t i = start; i < end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recvbuf[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recvbuf;\n   } else {\n      MPI_Send(&y[start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start_row = rank * local_M;\n   size_t end_row = (rank + 1) * local_M;\n   if (rank == size - 1) {\n      end_row = M;\n      local_M = end_row - start_row;\n   }\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = start_row; i < end_row; ++i) {\n      local_y[i - start_row] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start_row && A[i].row < end_row) {\n         local_y[A[i].row - start_row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      y = local_y;\n      std::vector<double> recv_buffer(local_M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(recv_buffer.data(), local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         y.insert(y.end(), recv_buffer.begin(), recv_buffer.end());\n      }\n   } else {\n      MPI_Send(local_y.data(), local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_buffer(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_buffer[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_buffer;\n   } else {\n      MPI_Send(&y[local_start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = start + local_M;\n\n   for (size_t i = start; i < end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recv(local_M);\n         MPI_Recv(&recv[0], local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < local_M; ++j) {\n            y[i * local_M + j] = recv[j];\n         }\n      }\n   } else {\n      MPI_Send(&y[start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_buffer(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_buffer[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_buffer;\n   } else {\n      MPI_Send(&y[local_start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      local_y[i - local_start] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         local_y[A[i].row - local_start] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      std::vector<int> recvcounts(size, local_M);\n      std::vector<int> displs(size, 0);\n\n      MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, recvbuf.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n      y = recvbuf;\n   } else {\n      MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t local_start = rank * local_M;\n   size_t local_end = (rank == size - 1) ? M : (rank + 1) * local_M;\n\n   for (size_t i = local_start; i < local_end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= local_start && A[i].row < local_end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recv_buffer(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_buffer[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_buffer;\n   } else {\n      MPI_Send(&y[local_start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank == size - 1) ? M : start + local_M;\n\n   for (size_t i = start; i < end; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         y[A[i].row] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recvbuf[0] + i * local_M, local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recvbuf;\n   } else {\n      MPI_Send(&y[start], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank == size - 1) ? M : start + local_M;\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = start; i < end; ++i) {\n      local_y[i - start] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         local_y[A[i].row - start] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      std::vector<int> recvcounts(size, local_M);\n      std::vector<int> displs(size, 0);\n      for (int i = 1; i < size; ++i) {\n         displs[i] = displs[i - 1] + local_M;\n      }\n\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, recvbuf.data(), recvcounts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      y = recvbuf;\n   } else {\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank == size - 1) ? M : start + local_M;\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = start; i < end; ++i) {\n      local_y[i - start] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         local_y[A[i].row - start] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M, 0.0);\n      std::vector<int> recvcounts(size, local_M);\n      std::vector<int> displs(size, 0);\n      for (int i = 1; i < size; ++i) {\n         displs[i] = displs[i - 1] + local_M;\n      }\n\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, recvbuf.data(), recvcounts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      y = recvbuf;\n   } else {\n      MPI_Gatherv(local_y.data(), local_M, MPI_DOUBLE, nullptr, nullptr, nullptr, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_M = M / size;\n   size_t start = rank * local_M;\n   size_t end = (rank + 1) * local_M;\n   if (rank == size - 1) {\n      end = M;\n   }\n\n   std::vector<double> local_y(local_M, 0.0);\n\n   for (size_t i = start; i < end; ++i) {\n      local_y[i - start] = beta * y[i];\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      if (A[i].row >= start && A[i].row < end) {\n         local_y[A[i].row - start] += alpha * A[i].value * x[A[i].column];\n      }\n   }\n\n   if (rank == 0) {\n      y = local_y;\n      std::vector<double> recv_y(M, 0.0);\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(&recv_y[i * local_M], local_M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n      y = recv_y;\n   } else {\n      MPI_Send(&local_y[0], local_M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of spmm\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n//    X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n//    Example:\n// \n//    input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n//    output: Y=[{-1,-4}, {1,0}]\n// */\n// void spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n// std::vector<double> Ytemp(M*N, 0.0);\n//    for (auto const& a : A) {\n//       for (auto const& x : X) {\n//          if (a.column == x.row) {\n//             Ytemp[a.row*N + x.column] += a.value * x.value;\n//          }\n//       }\n//    }\n//    Y = std::move(Ytemp);\n// }\n\n// MPI implementation of spmm\n#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\nstd::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}",
    "translation_function_name": "spmm",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvY(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvY.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvY);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> temp(M*N);\n         MPI_Recv(temp.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += temp[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N, 0.0);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvbuf(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvbuf.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvbuf);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N, 0.0);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      std::vector<double> recvY(M*N, 0.0);\n      MPI_Reduce(Ytemp.data(), recvY.data(), M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n      Y = std::move(recvY);\n   } else {\n      MPI_Reduce(Ytemp.data(), nullptr, M*N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            if (rank == 0) {\n               Ytemp[a.row*N + x.column] += a.value * x.value;\n            }\n         }\n      }\n   }\n\n   if (rank == 0) {\n       Y = std::move(Ytemp);\n       for (int i = 1; i < size; ++i) {\n           std::vector<double> recvY(M*N);\n           MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n           for (size_t j = 0; j < M*N; ++j) {\n               Y[j] += recvY[j];\n           }\n       }\n   } else {\n       MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      Y = std::move(Ytemp);\n      for (int i = 1; i < size; ++i) {\n         std::vector<double> recvY(M*N);\n         MPI_Recv(recvY.data(), M*N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < M*N; ++j) {\n            Y[j] += recvY[j];\n         }\n      }\n   } else {\n      MPI_Send(Ytemp.data(), M*N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    A is a sparse NxN matrix stored in COO format.\n//    Example:\n// \n//    input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n//    output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n// */\n// void luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n// \n// // Assuming A is sorted by row and then by column\n// L.clear();\n// U.clear();\n// \n// // Initialize L and U with zeros\n// L.resize(N * N, 0);\n// U.resize(N * N, 0);\n// \n// // Create a map for quick access to A's elements\n// std::map<std::pair<size_t, size_t>, double> A_map;\n// for (const auto& elem : A) {\n//     A_map[{elem.row, elem.column}] = elem.value;\n// }\n// \n// // LU decomposition algorithm\n// for (size_t i = 0; i < N; ++i) {\n//     // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n//     for (size_t k = i; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[i * N + j] * U[j * N + k];\n//         }\n//         U[i * N + k] = A_map[{i, k}] - sum;\n//     }\n// \n//     // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n//     for (size_t k = i + 1; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[k * N + j] * U[j * N + i];\n//         }\n//         L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n//     }\n// \n//     // L(i, i) = 1\n//     L[i * N + i] = 1.0;\n// }\n// }\n\n// MPI implementation of luFactorize\n#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n\n// Assuming A is sorted by row and then by column\nL.clear();\nU.clear();\n\n// Initialize L and U with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Create a map for quick access to A's elements\nstd::map<std::pair<size_t, size_t>, double> A_map;\nfor (const auto& elem : A) {\n    A_map[{elem.row, elem.column}] = elem.value;\n}\n\n// LU decomposition algorithm\nfor (size_t i = 0; i < N; ++i) {\n    // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[i * N + j] * U[j * N + k];\n        }\n        U[i * N + k] = A_map[{i, k}] - sum;\n    }\n\n    // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[k * N + j] * U[j * N + i];\n        }\n        L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n    }\n\n    // L(i, i) = 1\n    L[i * N + i] = 1.0;\n}\n}",
    "translation_function_name": "luFactorize",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(const_cast<std::vector<COOElement>*>(&A), A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    if (rank == 0) {\n        for (const auto& elem : A) {\n            A_map[{elem.row, elem.column}] = elem.value;\n        }\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather the results to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : L.data(), L.size(), MPI_DOUBLE, L.data(), L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : U.data(), U.size(), MPI_DOUBLE, U.data(), U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_L(N * N);\n            std::vector<double> recv_U(N * N);\n            MPI_Recv(recv_L.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(recv_U.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N * N; ++j) {\n                L[j] += recv_L[j];\n                U[j] += recv_U[j];\n            }\n        }\n    } else {\n        MPI_Send(L.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(U.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n\n    if (rank == 0) {\n        // Gather results from all ranks to rank 0\n        std::vector<double> recv_L(N * N, 0);\n        std::vector<double> recv_U(N * N, 0);\n        MPI_Gather(L.data(), N * N, MPI_DOUBLE, recv_L.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Gather(U.data(), N * N, MPI_DOUBLE, recv_U.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        L = recv_L;\n        U = recv_U;\n    } else {\n        // Send results from other ranks to rank 0\n        MPI_Gather(L.data(), N * N, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n        MPI_Gather(U.data(), N * N, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather results on rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    if (rank == 0) {\n        // Gather results from all ranks to rank 0\n        std::vector<double> local_L = L;\n        std::vector<double> local_U = U;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&local_L[0], N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&local_U[0], N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N * N; ++j) {\n                L[j] += local_L[j];\n                U[j] += local_U[j];\n            }\n        }\n    } else {\n        // Send local results to rank 0\n        MPI_Send(&L[0], N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&U[0], N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        if (i % size == rank) {\n            for (size_t k = i; k < N; ++k) {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += L[i * N + j] * U[j * N + k];\n                }\n                U[i * N + k] = A_map[{i, k}] - sum;\n            }\n\n            for (size_t k = i + 1; k < N; ++k) {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += L[k * N + j] * U[j * N + i];\n                }\n                L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n            }\n            L[i * N + i] = 1.0;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<double> recvL(N * N, 0);\n        std::vector<double> recvU(N * N, 0);\n        MPI_Gather(L.data(), N * N, MPI_DOUBLE, recvL.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Gather(U.data(), N * N, MPI_DOUBLE, recvU.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        L = recvL;\n        U = recvU;\n    } else {\n        MPI_Gather(L.data(), N * N, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Gather(U.data(), N * N, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    L.clear();\n    U.clear();\n\n    if (rank == 0) {\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        if (i % size == rank) {\n            for (size_t k = i; k < N; ++k) {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += L[i * N + j] * U[j * N + k];\n                }\n                U[i * N + k] = A_map[{i, k}] - sum;\n            }\n\n            for (size_t k = i + 1; k < N; ++k) {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += L[k * N + j] * U[j * N + i];\n                }\n                L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n            }\n\n            L[i * N + i] = 1.0;\n        }\n\n        if (size > 1) {\n            MPI_Bcast(&U[i * N], N, MPI_DOUBLE, i % size, MPI_COMM_WORLD);\n            MPI_Bcast(&L[i * N], N, MPI_DOUBLE, i % size, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_L(N * N);\n            std::vector<double> recv_U(N * N);\n            MPI_Recv(recv_L.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(recv_U.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N * N; ++j) {\n                L[j] += recv_L[j];\n                U[j] += recv_U[j];\n            }\n        }\n    } else {\n        MPI_Send(L.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(U.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_L(N * N);\n            std::vector<double> recv_U(N * N);\n            MPI_Recv(recv_L.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(recv_U.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N * N; ++j) {\n                L[j] += recv_L[j];\n                U[j] += recv_U[j];\n            }\n        }\n    } else {\n        MPI_Send(L.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(U.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n\n    if (rank == 0) {\n        // Gather results from all ranks to rank 0\n        std::vector<double> L_recv(N * N, 0);\n        std::vector<double> U_recv(N * N, 0);\n        MPI_Gather(L.data(), N * N, MPI_DOUBLE, L_recv.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Gather(U.data(), N * N, MPI_DOUBLE, U_recv.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        L = L_recv;\n        U = U_recv;\n    } else {\n        MPI_Gather(L.data(), N * N, MPI_DOUBLE, nullptr, 0, MPI_COMM_WORLD);\n        MPI_Gather(U.data(), N * N, MPI_DOUBLE, nullptr, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        L[i * N + i] = 1.0;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            std::vector<double> recv_L(N * N);\n            std::vector<double> recv_U(N * N);\n            MPI_Recv(recv_L.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(recv_U.data(), N * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < N * N; ++j) {\n                L[j] += recv_L[j];\n                U[j] += recv_U[j];\n            }\n        }\n    } else {\n        MPI_Send(L.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(U.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather L and U to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather the results to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather results to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    if (rank == 0) {\n        for (const auto& elem : A) {\n            A_map[{elem.row, elem.column}] = elem.value;\n        }\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather L and U to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(const_cast<std::vector<COOElement>*>(&A)[0].data(), A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather the results to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : L.data(), L.size(), MPI_DOUBLE, L.data(), L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : U.data(), U.size(), MPI_DOUBLE, U.data(), U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather the results on rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(const_cast<COOElement*>(A.data()), A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    if (rank == 0) {\n        for (const auto& elem : A) {\n            A_map[{elem.row, elem.column}] = elem.value;\n        }\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather results on rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : L.data(), N * N, MPI_DOUBLE, L.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : U.data(), N * N, MPI_DOUBLE, U.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    L.clear();\n    U.clear();\n\n    if (rank == 0) {\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    for (size_t i = 0; i < N; ++i) {\n        if (i % size == rank) {\n            for (size_t k = i; k < N; ++k) {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += L[i * N + j] * U[j * N + k];\n                }\n                U[i * N + k] = A_map[{i, k}] - sum;\n            }\n\n            for (size_t k = i + 1; k < N; ++k) {\n                double sum = 0.0;\n                for (size_t j = 0; j < i; ++j) {\n                    sum += L[k * N + j] * U[j * N + i];\n                }\n                L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n            }\n\n            L[i * N + i] = 1.0;\n        }\n\n        if (size > 1) {\n            MPI_Bcast(&U[i * N], N, MPI_DOUBLE, i % size, MPI_COMM_WORLD);\n            MPI_Bcast(&L[i * N], N, MPI_DOUBLE, i % size, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            std::vector<double> recv_L(N * N);\n            std::vector<double> recv_U(N * N);\n            MPI_Recv(recv_L.data(), N * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(recv_U.data(), N * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < N * N; ++i) {\n                L[i] += recv_L[i];\n                U[i] += recv_U[i];\n            }\n        }\n    } else {\n        MPI_Send(L.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(U.data(), N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(const_cast<COOElement*>(A.data()), A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    if (rank == 0) {\n        for (const auto& elem : A) {\n            A_map[{elem.row, elem.column}] = elem.value;\n        }\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather results on rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : L.data(), N * N, MPI_DOUBLE, L.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : U.data(), N * N, MPI_DOUBLE, U.data(), N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        L.clear();\n        U.clear();\n        L.resize(N * N, 0);\n        U.resize(N * N, 0);\n    }\n\n    // Broadcast A to all ranks\n    MPI_Bcast(&A[0], A.size() * sizeof(COOElement), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    // Create a map for quick access to A's elements\n    std::map<std::pair<size_t, size_t>, double> A_map;\n    for (const auto& elem : A) {\n        A_map[{elem.row, elem.column}] = elem.value;\n    }\n\n    // LU decomposition algorithm\n    for (size_t i = 0; i < N; ++i) {\n        // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n        for (size_t k = i; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[i * N + j] * U[j * N + k];\n            }\n            U[i * N + k] = A_map[{i, k}] - sum;\n        }\n\n        // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n        for (size_t k = i + 1; k < N; ++k) {\n            double sum = 0.0;\n            for (size_t j = 0; j < i; ++j) {\n                sum += L[k * N + j] * U[j * N + i];\n            }\n            L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n        }\n\n        // L(i, i) = 1\n        L[i * N + i] = 1.0;\n    }\n\n    // Gather results to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &L[0], L.size(), MPI_DOUBLE, &L[0], L.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : &U[0], U.size(), MPI_DOUBLE, &U[0], U.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "translation_prompt": "// Serial implementation of firstLetterCounts\n// /* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n//    Assume all strings are in lower case. Store the output in `bins` array.\n//    Example:\n// \n//    input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n//    output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n// */\n// void firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n// for (auto const& str : s) {\n//         bins[str[0] - 'a']++;\n//     }\n// }\n\n// MPI implementation of firstLetterCounts\n#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\nfor (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
    "translation_function_name": "firstLetterCounts",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> received_bins;\n            MPI_Recv(received_bins.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += received_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(local_bins.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> received_bins;\n            MPI_Recv(received_bins.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += received_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(local_bins.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> recv_bins = {0};\n        MPI_Reduce(local_bins.data(), recv_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = recv_bins;\n    } else {\n        MPI_Reduce(local_bins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> recv_bins = {0};\n        MPI_Reduce(local_bins.data(), recv_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = recv_bins;\n    } else {\n        MPI_Reduce(local_bins.data(), nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            localBins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recvBins;\n            MPI_Recv(&recvBins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recvBins[j];\n            }\n        }\n    } else {\n        MPI_Send(&localBins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recvBins;\n            MPI_Recv(&recvBins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recvBins[j];\n            }\n        }\n    } else {\n        MPI_Send(&localBins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if ((str[0] - 'a') % size == rank) {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(&recv_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = local_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 26> localBins = {0};\n\n    for (auto const& str : s) {\n        if ((str[0] - 'a') % size == rank) {\n            localBins[str[0] - 'a']++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "translation_prompt": "// Serial implementation of binsBy10Count\n// /* Vector x contains values between 0 and 100, inclusive. Count the number of\n//    values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n//    Example:\n// \n//    input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n//    output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n// */\n// void binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n// for (auto const& value : x) {\n//     if (value < 10) {\n//       bins[0]++;\n//     } else if (value < 20) {\n//       bins[1]++;\n//     } else if (value < 30) {\n//       bins[2]++;\n//     } else if (value < 40) {\n//       bins[3]++;\n//     } else if (value < 50) {\n//       bins[4]++;\n//     } else if (value < 60) {\n//       bins[5]++;\n//     } else if (value < 70) {\n//       bins[6]++;\n//     } else if (value < 80) {\n//       bins[7]++;\n//     } else if (value < 90) {\n//       bins[8]++;\n//     } else {\n//       bins[9]++;\n//     }\n//   }\n// }\n\n// MPI implementation of binsBy10Count\n#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\nfor (auto const& value : x) {\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
    "translation_function_name": "binsBy10Count",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> localBins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      localBins[0]++;\n    } else if (value < 20) {\n      localBins[1]++;\n    } else if (value < 30) {\n      localBins[2]++;\n    } else if (value < 40) {\n      localBins[3]++;\n    } else if (value < 50) {\n      localBins[4]++;\n    } else if (value < 60) {\n      localBins[5]++;\n    } else if (value < 70) {\n      localBins[6]++;\n    } else if (value < 80) {\n      localBins[7]++;\n    } else if (value < 90) {\n      localBins[8]++;\n    } else {\n      localBins[9]++;\n    }\n  }\n\n  MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_start = rank * (x.size() / size);\n  int local_end = (rank + 1) * (x.size() / size);\n\n  for (int i = local_start; i < local_end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_start = rank * (x.size() / size);\n  int local_end = (rank == size - 1) ? x.size() : (rank + 1) * (x.size() / size);\n\n  for (int i = local_start; i < local_end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_start = rank * (x.size() / size);\n  int local_end = (rank == size - 1) ? x.size() : (rank + 1) * (x.size() / size);\n\n  for (int i = local_start; i < local_end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      local_bins[0]++;\n    } else if (value < 20) {\n      local_bins[1]++;\n    } else if (value < 30) {\n      local_bins[2]++;\n    } else if (value < 40) {\n      local_bins[3]++;\n    } else if (value < 50) {\n      local_bins[4]++;\n    } else if (value < 60) {\n      local_bins[5]++;\n    } else if (value < 70) {\n      local_bins[6]++;\n    } else if (value < 80) {\n      local_bins[7]++;\n    } else if (value < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> recv_bins;\n      MPI_Recv(&recv_bins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += recv_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(&local_bins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_start = rank * (x.size() / size);\n  int local_end = (rank + 1) * (x.size() / size);\n  if (rank == size - 1) {\n    local_end = x.size();\n  }\n\n  for (int i = local_start; i < local_end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> localBins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      localBins[0]++;\n    } else if (value < 20) {\n      localBins[1]++;\n    } else if (value < 30) {\n      localBins[2]++;\n    } else if (value < 40) {\n      localBins[3]++;\n    } else if (value < 50) {\n      localBins[4]++;\n    } else if (value < 60) {\n      localBins[5]++;\n    } else if (value < 70) {\n      localBins[6]++;\n    } else if (value < 80) {\n      localBins[7]++;\n    } else if (value < 90) {\n      localBins[8]++;\n    } else {\n      localBins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> recvBins;\n      MPI_Recv(&recvBins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += recvBins[j];\n      }\n    }\n  } else {\n    MPI_Send(&localBins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] += localBins[i];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  for (int i = 0; i < 10; i++) {\n    MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      local_bins[0]++;\n    } else if (value < 20) {\n      local_bins[1]++;\n    } else if (value < 30) {\n      local_bins[2]++;\n    } else if (value < 40) {\n      local_bins[3]++;\n    } else if (value < 50) {\n      local_bins[4]++;\n    } else if (value < 60) {\n      local_bins[5]++;\n    } else if (value < 70) {\n      local_bins[6]++;\n    } else if (value < 80) {\n      local_bins[7]++;\n    } else if (value < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> received_bins;\n      MPI_Recv(received_bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += received_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(local_bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      local_bins[0]++;\n    } else if (value < 20) {\n      local_bins[1]++;\n    } else if (value < 30) {\n      local_bins[2]++;\n    } else if (value < 40) {\n      local_bins[3]++;\n    } else if (value < 50) {\n      local_bins[4]++;\n    } else if (value < 60) {\n      local_bins[5]++;\n    } else if (value < 70) {\n      local_bins[6]++;\n    } else if (value < 80) {\n      local_bins[7]++;\n    } else if (value < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> recv_bins;\n      MPI_Recv(&recv_bins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += recv_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(&local_bins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      local_bins[0]++;\n    } else if (value < 20) {\n      local_bins[1]++;\n    } else if (value < 30) {\n      local_bins[2]++;\n    } else if (value < 40) {\n      local_bins[3]++;\n    } else if (value < 50) {\n      local_bins[4]++;\n    } else if (value < 60) {\n      local_bins[5]++;\n    } else if (value < 70) {\n      local_bins[6]++;\n    } else if (value < 80) {\n      local_bins[7]++;\n    } else if (value < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> recv_bins;\n      MPI_Recv(recv_bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += recv_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(local_bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      local_bins[0]++;\n    } else if (value < 20) {\n      local_bins[1]++;\n    } else if (value < 30) {\n      local_bins[2]++;\n    } else if (value < 40) {\n      local_bins[3]++;\n    } else if (value < 50) {\n      local_bins[4]++;\n    } else if (value < 60) {\n      local_bins[5]++;\n    } else if (value < 70) {\n      local_bins[6]++;\n    } else if (value < 80) {\n      local_bins[7]++;\n    } else if (value < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> received_bins;\n      MPI_Recv(received_bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += received_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(local_bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    bins = local_bins;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  for (int i = 0; i < 10; i++) {\n    MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      local_bins[0]++;\n    } else if (value < 20) {\n      local_bins[1]++;\n    } else if (value < 30) {\n      local_bins[2]++;\n    } else if (value < 40) {\n      local_bins[3]++;\n    } else if (value < 50) {\n      local_bins[4]++;\n    } else if (value < 60) {\n      local_bins[5]++;\n    } else if (value < 70) {\n      local_bins[6]++;\n    } else if (value < 80) {\n      local_bins[7]++;\n    } else if (value < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 10> recv_bins;\n      MPI_Recv(&recv_bins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += recv_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(&local_bins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> localBins = {0};\n  size_t localCount = x.size() / size;\n  size_t remainder = x.size() % size;\n\n  size_t start = rank * localCount + std::min(rank, remainder);\n  size_t end = start + localCount + (rank < remainder ? 1 : 0);\n\n  for (size_t i = start; i < end; ++i) {\n    double value = x[i];\n    if (value < 10) {\n      localBins[0]++;\n    } else if (value < 20) {\n      localBins[1]++;\n    } else if (value < 30) {\n      localBins[2]++;\n    } else if (value < 40) {\n      localBins[3]++;\n    } else if (value < 50) {\n      localBins[4]++;\n    } else if (value < 60) {\n      localBins[5]++;\n    } else if (value < 70) {\n      localBins[6]++;\n    } else if (value < 80) {\n      localBins[7]++;\n    } else if (value < 90) {\n      localBins[8]++;\n    } else {\n      localBins[9]++;\n    }\n  }\n\n  MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> localBins = {0};\n\n  for (auto const& value : x) {\n    if (value < 10) {\n      localBins[0]++;\n    } else if (value < 20) {\n      localBins[1]++;\n    } else if (value < 30) {\n      localBins[2]++;\n    } else if (value < 40) {\n      localBins[3]++;\n    } else if (value < 50) {\n      localBins[4]++;\n    } else if (value < 60) {\n      localBins[5]++;\n    } else if (value < 70) {\n      localBins[6]++;\n    } else if (value < 80) {\n      localBins[7]++;\n    } else if (value < 90) {\n      localBins[8]++;\n    } else {\n      localBins[9]++;\n    }\n  }\n\n  for (int i = 0; i < 10; i++) {\n    MPI_Reduce(&localBins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_count = x.size() / size;\n  int remainder = x.size() % size;\n\n  int start = rank * local_count + std::min(rank, remainder);\n  int end = start + local_count + (rank < remainder ? 1 : 0);\n\n  for (int i = start; i < end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_count = x.size() / size;\n  int remainder = x.size() % size;\n\n  int start = rank * local_count + std::min(rank, remainder);\n  int end = start + local_count + (rank < remainder ? 1 : 0);\n\n  for (int i = start; i < end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  int local_count = x.size() / size;\n  int remainder = x.size() % size;\n\n  int start = rank * local_count + std::min(rank, remainder);\n  int end = start + local_count + (rank < remainder ? 1 : 0);\n\n  for (int i = start; i < end; i++) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 10> local_bins = {0};\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] < 10) {\n      local_bins[0]++;\n    } else if (x[i] < 20) {\n      local_bins[1]++;\n    } else if (x[i] < 30) {\n      local_bins[2]++;\n    } else if (x[i] < 40) {\n      local_bins[3]++;\n    } else if (x[i] < 50) {\n      local_bins[4]++;\n    } else if (x[i] < 60) {\n      local_bins[5]++;\n    } else if (x[i] < 70) {\n      local_bins[6]++;\n    } else if (x[i] < 80) {\n      local_bins[7]++;\n    } else if (x[i] < 90) {\n      local_bins[8]++;\n    } else {\n      local_bins[9]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuadrants\n// struct Point {\n//    double x, y;\n// };\n// \n// /* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n//    Store the counts in `bins`.\n//    Example:\n// \n//    input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n//    output: [3, 1, 0, 2]\n// */\n// void countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n// for (auto const& point : points) {\n//       if (point.x > 0 && point.y > 0) {\n//          bins[0]++;\n//       } else if (point.x < 0 && point.y > 0) {\n//          bins[1]++;\n//       } else if (point.x < 0 && point.y < 0) {\n//          bins[2]++;\n//       } else {\n//          bins[3]++;\n//       }\n//    }\n// }\n\n// MPI implementation of countQuadrants\n#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\nfor (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
    "translation_function_name": "countQuadrants",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t local_bins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         local_bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         local_bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         local_bins[2]++;\n      } else {\n         local_bins[3]++;\n      }\n   }\n\n   MPI_Reduce(local_bins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t localBins[4] = {0, 0, 0, 0};\n   for (size_t i = rank; i < points.size(); i += size) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuartiles\n// /* Count the number of doubles in the vector x that have a fractional part \n//    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n//    Examples:\n// \n//    input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n//    output: [2, 1, 2, 2]\n// \n//    input: [1.9, 0.2, 0.6, 10.1, 7.4]\n//    output: [2, 1, 1, 1]\n// */\n// void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n// for (double num : x) {\n//         double fractional_part = num - std::floor(num);\n//         if (fractional_part < 0.25) {\n//             bins[0]++;\n//         } else if (fractional_part < 0.5) {\n//             bins[1]++;\n//         } else if (fractional_part < 0.75) {\n//             bins[2]++;\n//         } else {\n//             bins[3]++;\n//         }\n//     }\n// }\n\n// MPI implementation of countQuartiles\n#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\nfor (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
    "translation_function_name": "countQuartiles",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    for (int i = 0; i < 4; i++) {\n        size_t local_count = local_bins[i];\n        size_t global_count;\n        MPI_Reduce(&local_count, &global_count, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        if (rank == 0) {\n            bins[i] = global_count;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    for (int i = 0; i < 4; i++) {\n        size_t local_count = local_bins[i];\n        size_t global_count;\n        MPI_Reduce(&local_count, &global_count, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        if (rank == 0) {\n            bins[i] = global_count;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    int local_count = x.size() / size;\n    int remainder = x.size() % size;\n    int start = rank * local_count + std::min(rank, remainder);\n    int end = start + local_count + (rank < remainder ? 1 : 0);\n\n    for (int i = start; i < end; i++) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            std::array<size_t, 4> recv_bins;\n            MPI_Recv(&recv_bins, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&local_bins, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    for (size_t i = rank; i < x.size(); i += size) {\n        double num = x[i];\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractionintal_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            local_bins[0]++;\n        } else if (fractional_part < 0.5) {\n            local_bins[1]++;\n        } else if (fractional_part < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "translation_prompt": "// Serial implementation of pixelCounts\n// /* Count the number of pixels in image with each grayscale intensity.\n//    The vector `image` is a grayscale image with values 0-255.\n//    Store the results in `bins`.\n//    Example:\n//    \n//    input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n//    output: [0, 0, 2, 0, 1, ...]\n// */\n//  void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n// for (auto const& pixel : image) {\n//         bins[pixel]++;\n//     }\n// }\n\n// MPI implementation of pixelCounts\n#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\nfor (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}",
    "translation_function_name": "pixelCounts",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    for (size_t i = rank; i < image.size(); i += size) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n    for (auto const& pixel : image) {\n        local_bins[pixel]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n    for (auto const& pixel : image) {\n        localBins[pixel]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 256> recvBins;\n            MPI_Recv(recvBins.data(), 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += recvBins[j];\n            }\n        }\n    } else {\n        MPI_Send(localBins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = localBins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    for (size_t i = rank; i < image.size(); i += size) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localImageSize = image.size() / size;\n    std::vector<int> localImage(localImageSize);\n    MPI_Scatter(image.data(), localImageSize, MPI_INT, localImage.data(), localImageSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : localImage) {\n        localBins[pixel]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    for (auto const& pixel : image) {\n        localBins[pixel]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 256> recvBins;\n            MPI_Recv(recvBins.data(), 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += recvBins[j];\n            }\n        }\n    } else {\n        MPI_Send(localBins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = localBins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_image_size = image.size() / size;\n    std::vector<int> local_image(local_image_size);\n    MPI_Scatter(image.data(), local_image_size, MPI_INT, local_image.data(), local_image_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : local_image) {\n        local_bins[pixel]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_image_size = image.size() / size;\n    std::vector<int> local_image(local_image_size);\n    MPI_Scatter(image.data(), local_image_size, MPI_INT, local_image.data(), local_image_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : local_image) {\n        local_bins[pixel]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_image_size = image.size() / size;\n    std::vector<int> local_image(local_image_size);\n    MPI_Scatter(image.data(), local_image_size, MPI_INT, local_image.data(), local_image_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : local_image) {\n        local_bins[pixel]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localImageSize = image.size() / size;\n    int remainder = image.size() % size;\n\n    int localStart = rank * localImageSize;\n    int localEnd = localStart + localImageSize;\n    if (rank == size - 1) {\n        localEnd += remainder;\n    }\n\n    for (int i = localStart; i < localEnd; i++) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_start = rank * (image.size() / size);\n    int local_end = (rank == size - 1) ? image.size() : (rank + 1) * (image.size() / size);\n\n    for (int i = local_start; i < local_end; i++) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localImageSize = image.size() / size;\n    std::vector<int> localImage(localImageSize);\n    MPI_Scatter(image.data(), localImageSize, MPI_INT, localImage.data(), localImageSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : localImage) {\n        localBins[pixel]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_image_size = image.size() / size;\n    std::vector<int> local_image(local_image_size);\n    MPI_Scatter(image.data(), local_image_size, MPI_INT, local_image.data(), local_image_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : local_image) {\n        local_bins[pixel]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    for (auto const& pixel : image) {\n        localBins[pixel]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    for (auto const& pixel : image) {\n        localBins[pixel]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> local_bins = {0};\n\n    int local_image_size = image.size() / size;\n    std::vector<int> local_image(local_image_size);\n    MPI_Scatter(image.data(), local_image_size, MPI_INT, local_image.data(), local_image_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : local_image) {\n        local_bins[pixel]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localImageSize = image.size() / size;\n    std::vector<int> localImage(localImageSize);\n    MPI_Scatter(image.data(), localImageSize, MPI_INT, localImage.data(), localImageSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : localImage) {\n        localBins[pixel]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n    for (auto const& pixel : image) {\n        localBins[pixel]++;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 256> recvBins;\n            MPI_Recv(recvBins.data(), 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += recvBins[j];\n            }\n        }\n    } else {\n        MPI_Send(localBins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = localBins;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localImageSize = image.size() / size;\n    std::vector<int> localImage(localImageSize);\n    MPI_Scatter(image.data(), localImageSize, MPI_INT, localImage.data(), localImageSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (auto const& pixel : localImage) {\n        localBins[pixel]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 256> localBins = {0};\n\n    int localImageSize = image.size() / size;\n    int remainder = image.size() % size;\n\n    int localStart = rank * localImageSize;\n    int localEnd = localStart + localImageSize;\n    if (rank == size - 1) {\n        localEnd += remainder;\n    }\n\n    for (int i = localStart; i < localEnd; i++) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of findFirstEven\n// /* Return the index of the first even number in the vector x.\n//    Examples:\n// \n//    input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n//    output: 6\n// \n//    input: [3, 8, 9, 9, 3, 4, 8, 6]\n//    output: 1\n// */\n// size_t findFirstEven(std::vector<int> const& x) {\n// for (size_t i = 0; i < x.size(); ++i) {\n//     if (x[i] % 2 == 0) {\n//       return i;\n//     }\n//   }\n//   return x.size();\n// }\n\n// MPI implementation of findFirstEven\n#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}",
    "translation_function_name": "findFirstEven",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Allreduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Allreduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Allreduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return global_result;\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_result = x.size();\n  size_t global_result;\n\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] % 2 == 0) {\n      local_result = i;\n      break;\n    }\n  }\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return x.size();\n  }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "translation_prompt": "// Serial implementation of contains\n// /* Return true if the vector x contains the value `target`. Return false otherwise.\n//    Examples:\n// \n//    input: x=[1, 8, 2, 6, 4, 6], target=3\n//    output: false\n//    \n//    input: x=[1, 8, 2, 6, 4, 6], target=8\n//    output: true\n// */\n// bool contains(std::vector<int> const& x, int target) {\n// return std::find(x.begin(), x.end(), target)!= x.end();\n// }\n\n// MPI implementation of contains\n#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\nreturn std::find(x.begin(), x.end(), target)!= x.end();\n}",
    "translation_function_name": "contains",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = std::find(x.begin(), x.end(), target) != x.end();\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_result = std::find(x.begin(), x.end(), target) != x.end();\n    int global_result;\n    MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = std::find(x.begin(), x.end(), target) != x.end();\n  bool global_result;\n\n  MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return false;\n  }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = std::find(x.begin(), x.end(), target) != x.end();\n  bool global_result;\n\n  MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return false;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n  bool global_result;\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return false;\n  }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), target) != x.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool local_result = std::find(x.begin(), x.end(), target) != x.end();\n  bool global_result;\n\n  MPI_Reduce(&local_result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return false;\n  }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "translation_prompt": "// Serial implementation of xorContains\n// /* Return true if `val` is only in one of vectors x or y.\n//    Return false if it is in both or neither.\n//    Examples:\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n//    output: true\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n//    output: false\n// */\n// bool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n// return std::find(x.begin(), x.end(), val)!= x.end() ^\n//          std::find(y.begin(), y.end(), val)!= y.end();\n// }\n\n// MPI implementation of xorContains\n#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\nreturn std::find(x.begin(), x.end(), val)!= x.end() ^\n         std::find(y.begin(), y.end(), val)!= y.end();\n}",
    "translation_function_name": "xorContains",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n\n  if (rank == 0) {\n    bool global_result;\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n    return global_result;\n  } else {\n    MPI_Reduce(&result, nullptr, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n    return false;\n  }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n\n    if (rank == 0) {\n        bool global_result;\n        MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n        return global_result;\n    } else {\n        MPI_Reduce(&result, nullptr, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result_local = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool result_global;\n\n    MPI_Reduce(&result_local, &result_global, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result_global;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result_local = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool result_global;\n\n    MPI_Reduce(&result_local, &result_global, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result_global;\n    } else {\n        return false;\n    }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n  bool global_result;\n\n  MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    return global_result;\n  } else {\n    return false;\n  }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result_local = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n\n    bool result_global;\n    MPI_Reduce(&result_local, &result_global, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result_global;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool global_result;\n\n    MPI_Reduce(&result, &global_result, 1, MPI_CXX_BOOL, MPI_LXOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_result;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result_local = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool result_global;\n\n    MPI_Reduce(&result_local, &result_global, 1, MPI_CXX_BOOL, MPI_LAND, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result_global;\n    } else {\n        return false;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    bool result_local = std::find(x.begin(), x.end(), val) != x.end() ^ std::find(y.begin(), y.end(), val) != y.end();\n    bool result_global;\n\n    MPI_Reduce(&result_local, &result_global, 1, MPI_CXX_BOOL, MPI_LOR, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return result_global;\n    } else {\n        return false;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "translation_prompt": "// Serial implementation of findLastShortBook\n// struct Book {\n//    std::string title;\n//    int pages;\n// };\n// \n// /* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n// \t Example:\n// \n//    input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n//    output: 2\n// */\n// size_t findLastShortBook(std::vector<Book> const& books) {\n// size_t last_short_book_index = 0;\n//    for (size_t i = 0; i < books.size(); ++i) {\n//       if (books[i].pages < 100) {\n//          last_short_book_index = i;\n//       }\n//    }\n//    return last_short_book_index;\n// }\n\n// MPI implementation of findLastShortBook\n#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\nsize_t last_short_book_index = 0;\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
    "translation_function_name": "findLastShortBook",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or any other value, as only rank 0 returns the result\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // Return 0 on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // Return a default value on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or any other value, as only rank 0 returns the result\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // Placeholder return for non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n\n   size_t global_last_short_book_index;\n   MPI_Reduce(&last_short_book_index, &global_last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return global_last_short_book_index;\n   } else {\n      return 0; // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // or some other value to indicate no result\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // Return a dummy value on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // Return 0 for non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0;  // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n\n   size_t global_last_short_book_index;\n   MPI_Reduce(&last_short_book_index, &global_last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return global_last_short_book_index;\n   } else {\n      return 0;\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   size_t local_last_short_book_index = 0;\n\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         local_last_short_book_index = i;\n      }\n   }\n\n   MPI_Reduce(&local_last_short_book_index, &last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return last_short_book_index;\n   } else {\n      return 0; // or some other value to indicate no result on non-root ranks\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t last_short_book_index = 0;\n   for (size_t i = rank; i < books.size(); i += size) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n\n   size_t global_last_short_book_index;\n   MPI_Reduce(&last_short_book_index, &global_last_short_book_index, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      return global_last_short_book_index;\n   } else {\n      return 0;\n   }\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of findClosestToPi\n// /* Return the index of the value in the vector x that is closest to the math constant PI.\n//    Use M_PI for the value of PI.\n//    Example:\n// \n//    input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n//    output: 1\n// */\n// size_t findClosestToPi(std::vector<double> const& x) {\n// double min_distance = std::numeric_limits<double>::max();\n//   size_t min_index = 0;\n//   for (size_t i = 0; i < x.size(); ++i) {\n//     double distance = std::abs(x[i] - M_PI);\n//     if (distance < min_distance) {\n//       min_distance = distance;\n//       min_index = i;\n//     }\n//   }\n//   return min_index;\n// }\n\n// MPI implementation of findClosestToPi\n#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\ndouble min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < min_distance) {\n      min_distance = distance;\n      min_index = i;\n    }\n  }\n  return min_index;\n}",
    "translation_function_name": "findClosestToPi",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Reduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  size_t local_min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Allreduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, MPI_COMM_WORLD);\n\n  return min_index;\n}\n",
      "\n  double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  double local_min_distance = std::numeric_limits<double>::max();\n  size_t local_min_index = 0;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < local_min_distance) {\n      local_min_distance = distance;\n      local_min_index = i;\n    }\n  }\n\n  MPI_Reduce(&local_min_distance, &min_distance, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&local_min_index, &min_index, 1, MPI_UNSIGNED_LONG, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return min_index;\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n// int n = x.size();\n//     if (n == 0) return;\n//     if (n == 1) {\n//         r[0] = x[0].real();\n//         i[0] = x[0].imag();\n//         return;\n//     }\n//     std::vector<std::complex<double>> x_even(n / 2);\n//     std::vector<std::complex<double>> x_odd(n / 2);\n//     for (int i = 0; i < n / 2; i++) {\n//         x_even[i] = x[2 * i];\n//         x_odd[i] = x[2 * i + 1];\n//     }\n//     std::vector<double> r_even(n / 2);\n//     std::vector<double> i_even(n / 2);\n//     std::vector<double> r_odd(n / 2);\n//     std::vector<double> i_odd(n / 2);\n//     fft(x_even, r_even, i_even);\n//     fft(x_odd, r_odd, i_odd);\n//     for (int k = 0; k < n / 2; k++) {\n//         std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n//         r[k] = r_even[k] + z.real();\n//         i[k] = i_even[k] + z.imag();\n//         r[k + n / 2] = r_even[k] - z.real();\n//         i[k + n / 2] = i_even[k] - z.imag();\n//     }\n// }\n\n// MPI implementation of fft\n#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\nint n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}",
    "translation_function_name": "fft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    std::vector<double> r_local(n);\n    std::vector<double> i_local(n);\n\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r_local[0] = x_local[0].real();\n        i_local[0] = x_local[0].imag();\n    } else {\n        std::vector<std::complex<double>> x_even(n / 2);\n        std::vector<std::complex<double>> x_odd(n / 2);\n        for (int i = 0; i < n / 2; i++) {\n            x_even[i] = x_local[2 * i];\n            x_odd[i] = x_local[2 * i + 1];\n        }\n        std::vector<double> r_even(n / 2);\n        std::vector<double> i_even(n / 2);\n        std::vector<double> r_odd(n / 2);\n        std::vector<double> i_odd(n / 2);\n\n        fft(x_even, r_even, i_even);\n        fft(x_odd, r_odd, i_odd);\n\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r_local[k] = r_even[k] + z.real();\n            i_local[k] = i_even[k] + z.imag();\n            r_local[k + n / 2] = r_even[k] - z.real();\n            i_local[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n\n    MPI_Gather(r_local.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r[0] = x_local[0].real();\n        i[0] = x_local[0].imag();\n        MPI_Gather(r.data(), 1, MPI_DOUBLE, r.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Gather(i.data(), 1, MPI_DOUBLE, i.data(), 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        return;\n    }\n\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x_local[2 * i];\n        x_odd[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n\n    MPI_Gather(r.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n / 2);\n    std::vector<std::complex<double>> x_odd(local_n / 2);\n    for (int i = 0; i < local_n / 2; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n / 2);\n    std::vector<double> i_even(local_n / 2);\n    std::vector<double> r_odd(local_n / 2);\n    std::vector<double> i_odd(local_n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    for (int k = 0; k < local_n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * (rank * local_n / n + k) / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        local_r[k] = r_even[k] + z.real();\n        local_i[k] = i_even[k] + z.imag();\n        local_r[k + local_n / 2] = r_even[k] - z.real();\n        local_i[k + local_n / 2] = i_even[k] - z.imag();\n    }\n\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> x_local(local_n);\n    std::vector<double> r_local(local_n);\n    std::vector<double> i_local(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, x_local.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n / 2);\n    std::vector<std::complex<double>> x_odd(local_n / 2);\n    for (int i = 0; i < local_n / 2; i++) {\n        x_even[i] = x_local[2 * i];\n        x_odd[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n / 2);\n    std::vector<double> i_even(local_n / 2);\n    std::vector<double> r_odd(local_n / 2);\n    std::vector<double> i_odd(local_n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    MPI_Gather(r_local.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + n / 2] = r_even[k] - z.real();\n            i[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n);\n    std::vector<std::complex<double>> x_odd(local_n);\n    for (int i = 0; i < local_n; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n);\n    std::vector<double> i_even(local_n);\n    std::vector<double> r_odd(local_n);\n    std::vector<double> i_odd(local_n);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    MPI_Gather(r_even.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_even.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + n / 2] = r_even[k] - z.real();\n            i[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    std::vector<std::complex<double>> x_local(n);\n    std::vector<double> r_local(n);\n    std::vector<double> i_local(n);\n\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x_local[2 * i];\n        x_odd[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r_local[k] = r_even[k] + z.real();\n        i_local[k] = i_even[k] + z.imag();\n        r_local[k + n / 2] = r_even[k] - z.real();\n        i_local[k + n / 2] = i_even[k] - z.imag();\n    }\n\n    MPI_Gather(r_local.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n);\n    std::vector<std::complex<double>> x_odd(local_n);\n    for (int i = 0; i < local_n; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n);\n    std::vector<double> i_even(local_n);\n    std::vector<double> r_odd(local_n);\n    std::vector<double> i_odd(local_n);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    MPI_Gather(r_even.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_even.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + n / 2] = r_even[k] - z.real();\n            i[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n);\n    std::vector<std::complex<double>> x_odd(local_n);\n    for (int i = 0; i < local_n; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n);\n    std::vector<double> i_even(local_n);\n    std::vector<double> r_odd(local_n);\n    std::vector<double> i_odd(local_n);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    MPI_Gather(r_even.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_even.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + n / 2] = r_even[k] - z.real();\n            i[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    std::vector<double> r_local(n);\n    std::vector<double> i_local(n);\n\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r_local[0] = x_local[0].real();\n        i_local[0] = x_local[0].imag();\n    } else {\n        std::vector<std::complex<double>> x_even(n / 2);\n        std::vector<std::complex<double>> x_odd(n / 2);\n        for (int i = 0; i < n / 2; i++) {\n            x_even[i] = x_local[2 * i];\n            x_odd[i] = x_local[2 * i + 1];\n        }\n        std::vector<double> r_even(n / 2);\n        std::vector<double> i_even(n / 2);\n        std::vector<double> r_odd(n / 2);\n        std::vector<double> i_odd(n / 2);\n\n        fft(x_even, r_even, i_even);\n        fft(x_odd, r_odd, i_odd);\n\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r_local[k] = r_even[k] + z.real();\n            i_local[k] = i_even[k] + z.imag();\n            r_local[k + n / 2] = r_even[k] - z.real();\n            i_local[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n\n    MPI_Gather(r_local.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r[0] = x_local[0].real();\n        i[0] = x_local[0].imag();\n        return;\n    }\n\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x_local[2 * i];\n        x_odd[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    std::vector<double> r_combined(n);\n    std::vector<double> i_combined(n);\n    MPI_Gather(r_even.data(), n / 2, MPI_DOUBLE, r_combined.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_even.data(), n / 2, MPI_DOUBLE, i_combined.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(r_odd.data(), n / 2, MPI_DOUBLE, r_combined.data() + n / 2, n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_odd.data(), n / 2, MPI_DOUBLE, i_combined.data() + n / 2, n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_combined[k + n / 2], i_combined[k + n / 2]);\n            r[k] = r_combined[k] + z.real();\n            i[k] = i_combined[k] + z.imag();\n            r[k + n / 2] = r_combined[k] - z.real();\n            i[k + n / 2] = i_combined[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n / 2);\n    std::vector<std::complex<double>> x_odd(local_n / 2);\n    for (int i = 0; i < local_n / 2; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n / 2);\n    std::vector<double> i_even(local_n / 2);\n    std::vector<double> r_odd(local_n / 2);\n    std::vector<double> i_odd(local_n / 2);\n\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    for (int k = 0; k < local_n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * (rank * local_n / n + k) / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        local_r[k] = r_even[k] + z.real();\n        local_i[k] = i_even[k] + z.imag();\n        local_r[k + local_n / 2] = r_even[k] - z.real();\n        local_i[k + local_n / 2] = i_even[k] - z.imag();\n    }\n\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    fft(local_x, local_r, local_i);\n    std::vector<double> all_r(n);\n    std::vector<double> all_i(n);\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, all_r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, all_i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int k = 0; k < n; k++) {\n            r[k] = all_r[k];\n            i[k] = all_i[k];\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    std::vector<double> r_local(n);\n    std::vector<double> i_local(n);\n\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r_local[0] = x_local[0].real();\n        i_local[0] = x_local[0].imag();\n    } else {\n        std::vector<std::complex<double>> x_even(n / 2);\n        std::vector<std::complex<double>> x_odd(n / 2);\n        for (int i = 0; i < n / 2; i++) {\n            x_even[i] = x_local[2 * i];\n            x_odd[i] = x_local[2 * i + 1];\n        }\n        std::vector<double> r_even(n / 2);\n        std::vector<double> i_even(n / 2);\n        std::vector<double> r_odd(n / 2);\n        std::vector<double> i_odd(n / 2);\n\n        fft(x_even, r_even, i_even);\n        fft(x_odd, r_odd, i_odd);\n\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r_local[k] = r_even[k] + z.real();\n            i_local[k] = i_even[k] + z.imag();\n            r_local[k + n / 2] = r_even[k] - z.real();\n            i_local[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n\n    MPI_Gather(r_local.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    std::vector<double> r_local(n);\n    std::vector<double> i_local(n);\n\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r_local[0] = x_local[0].real();\n        i_local[0] = x_local[0].imag();\n    } else {\n        std::vector<std::complex<double>> x_even(n / 2);\n        std::vector<std::complex<double>> x_odd(n / 2);\n        for (int i = 0; i < n / 2; i++) {\n            x_even[i] = x_local[2 * i];\n            x_odd[i] = x_local[2 * i + 1];\n        }\n        std::vector<double> r_even(n / 2);\n        std::vector<double> i_even(n / 2);\n        std::vector<double> r_odd(n / 2);\n        std::vector<double> i_odd(n / 2);\n\n        fft(x_even, r_even, i_even);\n        fft(x_odd, r_odd, i_odd);\n\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r_local[k] = r_even[k] + z.real();\n            i_local[k] = i_even[k] + z.imag();\n            r_local[k + n / 2] = r_even[k] - z.real();\n            i_local[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n\n    MPI_Gather(r_local.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n);\n    std::vector<std::complex<double>> x_odd(local_n);\n    for (int i = 0; i < local_n; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n);\n    std::vector<double> i_even(local_n);\n    std::vector<double> r_odd(local_n);\n    std::vector<double> i_odd(local_n);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    MPI_Gather(r_even.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_even.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + n / 2] = r_even[k] - z.real();\n            i[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> x_local(local_n);\n    std::vector<double> r_local(local_n);\n    std::vector<double> i_local(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, x_local.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n / 2);\n    std::vector<std::complex<double>> x_odd(local_n / 2);\n    for (int i = 0; i < local_n / 2; i++) {\n        x_even[i] = x_local[2 * i];\n        x_odd[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n / 2);\n    std::vector<double> i_even(local_n / 2);\n    std::vector<double> r_odd(local_n / 2);\n    std::vector<double> i_odd(local_n / 2);\n\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    MPI_Gather(r_local.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r[k] = r_even[k] + z.real();\n            i[k] = i_even[k] + z.imag();\n            r[k + n / 2] = r_even[k] - z.real();\n            i[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    std::vector<double> r_local(n);\n    std::vector<double> i_local(n);\n\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (n == 1) {\n        r_local[0] = x_local[0].real();\n        i_local[0] = x_local[0].imag();\n    } else {\n        std::vector<std::complex<double>> x_even(n / 2);\n        std::vector<std::complex<double>> x_odd(n / 2);\n        for (int i = 0; i < n / 2; i++) {\n            x_even[i] = x_local[2 * i];\n            x_odd[i] = x_local[2 * i + 1];\n        }\n        std::vector<double> r_even(n / 2);\n        std::vector<double> i_even(n / 2);\n        std::vector<double> r_odd(n / 2);\n        std::vector<double> i_odd(n / 2);\n\n        fft(x_even, r_even, i_even);\n        fft(x_odd, r_odd, i_odd);\n\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n            r_local[k] = r_even[k] + z.real();\n            i_local[k] = i_even[k] + z.imag();\n            r_local[k + n / 2] = r_even[k] - z.real();\n            i_local[k + n / 2] = i_even[k] - z.imag();\n        }\n    }\n\n    MPI_Gather(r_local.data(), n, MPI_DOUBLE, r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n, MPI_DOUBLE, i.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<double> local_r(local_n);\n    std::vector<double> local_i(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even(local_n);\n    std::vector<std::complex<double>> x_odd(local_n);\n    for (int i = 0; i < local_n; i++) {\n        x_even[i] = local_x[2 * i];\n        x_odd[i] = local_x[2 * i + 1];\n    }\n\n    std::vector<double> r_even(local_n);\n    std::vector<double> i_even(local_n);\n    std::vector<double> r_odd(local_n);\n    std::vector<double> i_odd(local_n);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n\n    for (int k = 0; k < local_n; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * (rank * local_n + k) / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        local_r[k] = r_even[k] + z.real();\n        local_i[k] = i_even[k] + z.imag();\n        local_r[k + local_n] = r_even[k] - z.real();\n        local_i[k + local_n] = i_even[k] - z.imag();\n    }\n\n    MPI_Gather(local_r.data(), local_n, MPI_DOUBLE, r.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_i.data(), local_n, MPI_DOUBLE, i.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even_local(n / 2);\n    std::vector<std::complex<double>> x_odd_local(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even_local[i] = x_local[2 * i];\n        x_odd_local[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_local(n / 2);\n    std::vector<double> i_local(n / 2);\n    fft(x_even_local, r_local, i_local);\n\n    std::vector<double> r_even_local(n / 2);\n    std::vector<double> i_even_local(n / 2);\n    std::vector<double> r_odd_local(n / 2);\n    std::vector<double> i_odd_local(n / 2);\n\n    if (rank == 0) {\n        r_even_local.resize(n / 2 * size);\n        i_even_local.resize(n / 2 * size);\n        r_odd_local.resize(n / 2 * size);\n        i_odd_local.resize(n / 2 * size);\n    }\n\n    MPI_Gather(r_local.data(), n / 2, MPI_DOUBLE, r_even_local.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n / 2, MPI_DOUBLE, i_even_local.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            for (int j = 1; j < size; j++) {\n                std::complex<double> z = std::polar(1.0, -2 * M_PI * (k + j * n / 2) / n) * std::complex<double>(r_odd_local[k + j * n / 2], i_odd_local[k + j * n / 2]);\n                r_even_local[k + j * n / 2] = r_even_local[k] + z.real();\n                i_even_local[k + j * n / 2] = i_even_local[k] + z.imag();\n                r_even_local[k] = r_even_local[k] - z.real();\n                i_even_local[k] = i_even_local[k] - z.imag();\n            }\n        }\n    }\n\n    if (rank == 0) {\n        r.resize(n);\n        i.resize(n);\n    }\n\n    MPI_Gather(r_even_local.data(), n / 2, MPI_DOUBLE, r.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_even_local.data(), n / 2, MPI_DOUBLE, i.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (n == 0) return;\n\n    std::vector<std::complex<double>> x_local(n);\n    MPI_Scatter(x.data(), n, MPI_DOUBLE_COMPLEX, x_local.data(), n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::vector<std::complex<double>> x_even_local(n / 2);\n    std::vector<std::complex<double>> x_odd_local(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even_local[i] = x_local[2 * i];\n        x_odd_local[i] = x_local[2 * i + 1];\n    }\n\n    std::vector<double> r_local(n / 2);\n    std::vector<double> i_local(n / 2);\n    fft(x_even_local, r_local, i_local);\n\n    std::vector<double> r_odd_local(n / 2);\n    std::vector<double> i_odd_local(n / 2);\n    MPI_Gather(r_local.data(), n / 2, MPI_DOUBLE, r.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(i_local.data(), n / 2, MPI_DOUBLE, i.data(), n / 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 0; k < n / 2; k++) {\n            std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(i.data()[k], r.data()[k]);\n            r.data()[k] = r.data()[k + n / 2] + z.real();\n            i.data()[k] = i.data()[k + n / 2] + z.imag();\n            r.data()[k + n / 2] = r.data()[k + n / 2] - z.real();\n            i.data()[k + n / 2] = i.data()[k + n / 2] - z.imag();\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of fftConjugate\n// /* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fftConjugate(std::vector<std::complex<double>> &x) {\n// int n = x.size();\n//     int m = 0;\n//     for (int i = 0; i < n; i++) {\n//         int j = i;\n//         for (int k = 0; k < m; k++) {\n//             j = j ^ (1 << k);\n//         }\n//         if (i < j) {\n//             std::complex<double> temp = x[i];\n//             x[i] = x[j];\n//             x[j] = temp;\n//         }\n//         m = std::max(m, __builtin_ctz(j));\n//     }\n//     for (int i = 1; i < n; i *= 2) {\n//         std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n//         for (int j = 0; j < n; j += 2 * i) {\n//             for (int k = 0; k < i; k++) {\n//                 std::complex<double> u = x[j + k];\n//                 std::complex<double> t = w * x[j + k + i];\n//                 x[j + k] = u + t;\n//                 x[j + k + i] = u - t;\n//             }\n//             w = w * w;\n//         }\n//     }\n// }\n\n// MPI implementation of fftConjugate\n#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\nint n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}",
    "translation_function_name": "fftConjugate",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, result.data(), n * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0] + i * (n / size), n / size, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Send(&x[rank * (n / size)], n / size, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * 2 * i; j < n; j += 2 * i * size) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, x.data(), n, MPI_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n        for (int k = 0; k < i; k++) {\n            w = w * w;\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n / size, MPI_DOUBLE_COMPLEX, result.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n / size, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank; j < n; j += size) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Barrier(MPI_COMM_WORLD);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n\n    if (rank != 0) {\n        MPI_Send(x.data(), n * sizeof(std::complex<double>), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<std::complex<double>> temp(n);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(temp.data(), n * sizeof(std::complex<double>), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < n; j++) {\n                x[j] += temp[j];\n            }\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Allgather(MPI_IN_PLACE, (n / size), MPI_DOUBLE_COMPLEX, &x[0], (n / size), MPI_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n        w = w * w;\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Allgather(MPI_IN_PLACE, (n / size), MPI_COMPLEX, &x[0], (n / size), MPI_COMPLEX, MPI_COMM_WORLD);\n        w = w * w;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, result.data(), n * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0] + i * (n / size), n / size, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Send(&x[rank * (n / size)], n / size, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, result.data(), n * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, nullptr, 0, MPI_DATATYPE_NULL, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(n);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recvbuf[0] + i * (n / size), n / size, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Send(&x[rank * (n / size)], n / size, MPI_DOUBLE_COMPLEX, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank; j < n; j += size) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Barrier(MPI_COMM_WORLD);\n        if (rank == 0) {\n            for (int p = 1; p < size; p++) {\n                std::vector<std::complex<double>> recv_buf(n);\n                MPI_Recv(&recv_buf[0], n * 2, MPI_DOUBLE, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                for (int j = p; j < n; j += size) {\n                    x[j] = recv_buf[j];\n                }\n            }\n        } else {\n            MPI_Send(&x[0], n * 2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        }\n        MPI_Barrier(MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int p = 1; p < size; p++) {\n            std::vector<std::complex<double>> recv_buf(n);\n            MPI_Recv(&recv_buf[0], n * 2, MPI_DOUBLE, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = p; j < n; j += size) {\n                x[j] = recv_buf[j];\n            }\n        }\n    } else {\n        MPI_Send(&x[0], n * 2, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n / size, MPI_DOUBLE_COMPLEX, result.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n / size, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n / size, MPI_DOUBLE_COMPLEX, result.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n / size, MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // ... (rest of the code remains the same, but applied to local_x instead of x)\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank * (n / size); j < (rank + 1) * (n / size); j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Allgather(MPI_IN_PLACE, (n / size), MPI_DOUBLE_COMPLEX, &x[0], (n / size), MPI_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n        w = w * w;\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = rank; j < n; j += size) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n        }\n        MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, x.data(), n, MPI_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n        w = w * w;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            x[i] = std::conj(x[i]);\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(n);\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, result.data(), n * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), n * 2, MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int m = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    int m = 0;\n    for (int i = 0; i < local_n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < local_n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = local_x[j + k];\n                std::complex<double> t = w * local_x[j + k + i];\n                local_x[j + k] = u + t;\n                local_x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of ifft\n// /* forward declare fft. computes fourier transform in-place */\n// void fft(std::vector<std::complex<double>> &x);\n// \n// /* Compute the inverse fourier transform of x in-place.\n//    Example:\n//    \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n// */\n// void ifft(std::vector<std::complex<double>> &x) {\n// \n// // Take conjugate of the complex numbers\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Apply fft\n// fft(x);\n// \n// // Take conjugate again\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Scale the numbers\n// for (auto &elem : x) {\n//     elem /= static_cast<double>(x.size());\n// }\n// }\n\n// MPI implementation of ifft\n#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n\n// Take conjugate of the complex numbers\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Apply fft\nfft(x);\n\n// Take conjugate again\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Scale the numbers\nfor (auto &elem : x) {\n    elem /= static_cast<double>(x.size());\n}\n}",
    "translation_function_name": "ifft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers in parallel\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Reduce(x.data(), result.data(), x.size() * 2, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Reduce(x.data(), nullptr, x.size() * 2, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers in parallel\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    // Gather results on rank 0\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    // Gather results to rank 0\n    if (rank == 0) {\n        std::vector<std::vector<std::complex<double>>> all_results(size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, all_results.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        for (int i = 1; i < size; ++i) {\n            x.insert(x.end(), all_results[i].begin(), all_results[i].end());\n        }\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    // Gather the results on rank 0\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers in parallel\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    // Gather results to rank 0\n    if (rank == 0) {\n        std::vector<std::vector<std::complex<double>>> results(size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, results.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        // Combine results from all ranks\n        for (int i = 1; i < size; i++) {\n            for (int j = 0; j < x.size(); j++) {\n                x[j] += results[i][j];\n            }\n        }\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers in parallel\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Take conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Apply fft in parallel\n    fft(x);\n\n    // Take conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Scale the numbers\n    for (auto &elem : x) {\n        elem /= static_cast<double>(x.size());\n    }\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> result(x.size());\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, result.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        x = result;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of dft\n// /* Compute the discrete fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1, 4, 9, 16]\n//    output: [30+0i, -8-12i, -10-0i, -8+12i]\n// */\n// void dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// // TODO: implement this function\n//     output.resize(x.size());\n//     for (int i = 0; i < x.size(); i++) {\n//         output[i] = 0;\n//         for (int j = 0; j < x.size(); j++) {\n//             output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n//         }\n//     }\n// }\n\n// MPI implementation of dft\n#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// TODO: implement this function\n    output.resize(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}",
    "translation_function_name": "dft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = 0; i < local_n; i++) {\n        int global_i = rank * local_n + i;\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * global_i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(&local_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, &gathered_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = 0; i < local_n; i++) {\n        int global_i = rank * local_n + i;\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * global_i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(&local_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, &gathered_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(&local_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, &gathered_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = 0; i < local_n; i++) {\n        int global_i = rank * local_n + i;\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * global_i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Gather(&local_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, &gathered_output[0], local_n, MPI_CXX_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = 0; i < local_n; i++) {\n        int global_i = rank * local_n + i;\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * global_i * j / x.size()));\n        }\n    }\n\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n}",
      "\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = rank * local_n; i < (rank + 1) * local_n; i++) {\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i - rank * local_n] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = 0; i < local_n; i++) {\n        int global_i = rank * local_n + i;\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * global_i * j / x.size()));\n        }\n    }\n\n    std::vector<std::complex<double>> gathered_output(x.size());\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, gathered_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n\n    output = gathered_output;\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_output(local_n, 0);\n\n    for (int i = 0; i < local_n; i++) {\n        int global_i = rank * local_n + i;\n        for (int j = 0; j < x.size(); j++) {\n            local_output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * global_i * j / x.size()));\n        }\n    }\n\n    MPI_Allgather(local_output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, output.data(), local_n, MPI_CXX_DOUBLE_COMPLEX, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n// int n = x.size();\n//     output.resize(n);\n// \n//     // base case\n//     if (n == 1) {\n//         output[0] = x[0];\n//         return;\n//     }\n// \n//     // radix 2 Cooley-Tukey FFT\n//     // decompose x into even and odd parts\n//     std::vector<std::complex<double>> x_even(n/2);\n//     std::vector<std::complex<double>> x_odd(n/2);\n//     for (int k = 0; k < n/2; k++) {\n//         x_even[k] = x[2*k];\n//         x_odd[k] = x[2*k + 1];\n//     }\n// \n//     // recursively compute FFTs of even and odd parts\n//     std::vector<std::complex<double>> y_even(n/2);\n//     std::vector<std::complex<double>> y_odd(n/2);\n//     fft(x_even, y_even);\n//     fft(x_odd, y_odd);\n// \n//     // combine results\n//     for (int k = 0; k < n/2; k++) {\n//         double kth = -2 * M_PI * k / n;\n//         output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//         output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//     }\n// }\n\n// MPI implementation of fft\n#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\nint n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}",
    "translation_function_name": "fft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x[2 * k];\n        x_odd[k] = x[2 * k + 1];\n    }\n\n    // scatter x to all processes\n    MPI_Scatter(&x[0], n / size, MPI_DOUBLE_COMPLEX, &x_local[0], n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // gather y_even and y_odd from all processes\n    MPI_Gather(&y_even[0], n / (2 * size), MPI_DOUBLE_COMPLEX, &y_even[0], n / (2 * size), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    MPI_Gather(&y_odd[0], n / (2 * size), MPI_DOUBLE_COMPLEX, &y_odd[0], n / (2 * size), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // combine results\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    int local_n = n / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    std::vector<std::complex<double>> local_y(local_n);\n\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    fft(local_x, local_y);\n\n    MPI_Gather(local_y.data(), local_n, MPI_DOUBLE_COMPLEX, y_even.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_y.data(), local_n, MPI_DOUBLE_COMPLEX, y_odd.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / size; i++) {\n        x_local[i] = x[rank + i * size];\n    }\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x_local[2 * k];\n        x_odd[k] = x_local[2 * k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n\n    // Gather results from all processes\n    std::vector<std::complex<double>> gathered_output(n);\n    MPI_Gather(&output[0], n / size, MPI_DOUBLE_COMPLEX, &gathered_output[0], n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Copy gathered results to output\n    if (rank == 0) {\n        output = gathered_output;\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    } else {\n        fft(x_odd, y_odd);\n        fft(x_even, y_even);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    } else {\n        fft(x_odd, y_odd);\n        fft(x_even, y_even);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x[2 * k];\n        x_odd[k] = x[2 * k + 1];\n    }\n\n    // distribute input data to all processes\n    MPI_Scatter(&x[0], n / size, MPI_DOUBLE_COMPLEX, &x_local[0], n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n\n    // gather results from all processes\n    MPI_Gather(&output[0], n / size, MPI_DOUBLE_COMPLEX, &output[0], n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    MPI_Scatter(x.data(), n / size, MPI_DOUBLE_COMPLEX, x_local.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x_local[2 * k];\n        x_odd[k] = x_local[2 * k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    std::vector<std::complex<double>> y_local(n / size);\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        y_local[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        y_local[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n\n    MPI_Gather(y_local.data(), n / size, MPI_DOUBLE_COMPLEX, output.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    MPI_Scatter(x.data(), n / size, MPI_DOUBLE_COMPLEX, x_local.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x_local[2 * k];\n        x_odd[k] = x_local[2 * k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    std::vector<std::complex<double>> y_local(n / size);\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        y_local[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        y_local[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n\n    MPI_Gather(y_local.data(), n / size, MPI_DOUBLE_COMPLEX, output.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    MPI_Scatter(x.data(), n / size, MPI_DOUBLE_COMPLEX, x_local.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x_local[2 * k];\n        x_odd[k] = x_local[2 * k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n\n    MPI_Gather(output.data(), n / size, MPI_DOUBLE_COMPLEX, output.data(), n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    } else {\n        fft(x_odd, y_odd);\n        fft(x_even, y_even);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_local(n / size);\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int k = 0; k < n / 2; k++) {\n        x_even[k] = x[2 * k];\n        x_odd[k] = x[2 * k + 1];\n    }\n\n    // distribute data to all processes\n    MPI_Scatter(&x[0], n / size, MPI_DOUBLE_COMPLEX, &x_local[0], n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n / 2);\n    std::vector<std::complex<double>> y_odd(n / 2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n / 2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n / 2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n\n    // gather results from all processes\n    MPI_Gather(&output[0], n / size, MPI_DOUBLE_COMPLEX, &output[0], n / size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int n = x.size();\n    output.resize(n);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    if (rank < size/2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    } else {\n        fft(x_odd, y_odd);\n        fft(x_even, y_even);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    } else {\n        fft(x_odd, y_odd);\n        fft(x_even, y_even);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        std::complex<double> twiddle = std::complex<double>(cos(kth), sin(kth));\n        if (rank < size / 2) {\n            output[k] = y_even[k] + twiddle * y_odd[k];\n            output[k + n/2] = y_even[k] - twiddle * y_odd[k];\n        } else {\n            output[k] = y_even[k + n/2] + twiddle * y_odd[k + n/2];\n            output[k + n/2] = y_even[k + n/2] - twiddle * y_odd[k + n/2];\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        std::complex<double> twiddle = std::complex<double>(cos(kth), sin(kth));\n        if (rank < size / 2) {\n            output[k] = y_even[k] + twiddle * y_odd[k];\n            output[k + n/2] = y_even[k] - twiddle * y_odd[k];\n        }\n    }\n\n    if (rank >= size / 2) {\n        MPI_Send(output.data(), n, MPI_DOUBLE_COMPLEX, rank - size / 2, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<std::complex<double>> recv_buffer(n);\n        for (int i = 0; i < size / 2; i++) {\n            if (i != rank) {\n                MPI_Recv(recv_buffer.data(), n, MPI_DOUBLE_COMPLEX, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                for (int j = 0; j < n; j++) {\n                    output[j] += recv_buffer[j];\n                }\n            }\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n\n    if (rank < size / 2) {\n        fft(x_even, y_even);\n        fft(x_odd, y_odd);\n    } else {\n        fft(x_odd, y_odd);\n        fft(x_even, y_even);\n    }\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "translation_prompt": "// Serial implementation of axpy\n// /* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n//    Example:\n//    \n//    input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n//    output: z=[2, -6, 5, 17]\n// */\n// void axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n// for (int i = 0; i < x.size(); ++i) {\n//     z[i] = alpha*x[i] + y[i];\n//   }\n// }\n\n// MPI implementation of axpy\n#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\nfor (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}",
    "translation_function_name": "axpy",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank*local_n + i] + y[rank*local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, NULL, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, z.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n    MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, z.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gather(local_z.data(), local_n, MPI_DOUBLE, nullptr, 0, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = x.size() / size;\n  std::vector<double> local_z(local_n);\n\n  for (int i = 0; i < local_n; ++i) {\n    local_z[i] = alpha * x[rank * local_n + i] + y[rank * local_n + i];\n  }\n\n  if (rank == 0) {\n    z.resize(x.size());\n  }\n\n  MPI_Gather(&local_z[0], local_n, MPI_DOUBLE, &z[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "translation_prompt": "// Serial implementation of solveLinearSystem\n// /* Solve the linear system Ax=b for x.\n//    A is an NxN matrix in row-major. x and b have N elements.\n//    Example:\n//    \n//    input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n//    output: x=[3, 1, 2]\n// */\n// void solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// // check if the input is valid\n//     if (A.size()!= N * N) {\n//         throw std::invalid_argument(\"A must be a NxN matrix\");\n//     }\n//     if (b.size()!= N) {\n//         throw std::invalid_argument(\"b must have N elements\");\n//     }\n//     // create a copy of A\n//     std::vector<double> A_copy(A);\n//     // create a copy of b\n//     std::vector<double> b_copy(b);\n//     // perform Gaussian elimination\n//     for (size_t i = 0; i < N; i++) {\n//         // find the pivot row and swap it with the current row\n//         size_t pivot_row = i;\n//         for (size_t j = i + 1; j < N; j++) {\n//             if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n//                 pivot_row = j;\n//             }\n//         }\n//         if (pivot_row!= i) {\n//             // swap the current row with the pivot row\n//             for (size_t j = 0; j < N; j++) {\n//                 std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n//             }\n//             std::swap(b_copy[i], b_copy[pivot_row]);\n//         }\n//         // eliminate the current row\n//         for (size_t j = i + 1; j < N; j++) {\n//             double factor = A_copy[j * N + i] / A_copy[i * N + i];\n//             for (size_t k = 0; k < N; k++) {\n//                 A_copy[j * N + k] -= factor * A_copy[i * N + k];\n//             }\n//             b_copy[j] -= factor * b_copy[i];\n//         }\n//     }\n//     // back substitution\n//     x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n//     for (size_t i = N - 2; i < N; i--) {\n//         double sum = 0;\n//         for (size_t j = i + 1; j < N; j++) {\n//             sum += A_copy[i * N + j] * x[j];\n//         }\n//         x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n//     }\n// }\n\n// MPI implementation of solveLinearSystem\n#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}",
    "translation_function_name": "solveLinearSystem",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n        for (size_t i = N - 2; i < N; i--) {\n            double sum = 0;\n            for (size_t j = i + 1; j < N; j++) {\n                sum += A_copy[i * N + j] * x[j];\n            }\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // perform Gaussian elimination in parallel\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        // broadcast the pivot row and the current row to all ranks\n        MPI_Bcast(&A_copy[i * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&b_copy[i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        // eliminate the current row in parallel\n        for (size_t j = i + 1; j < N; j++) {\n            if (j % size == rank) {\n                double factor = A_copy[j * N + i] / A_copy[i * N + i];\n                for (size_t k = 0; k < N; k++) {\n                    A_copy[j * N + k] -= factor * A_copy[i * N + k];\n                }\n                b_copy[j] -= factor * b_copy[i];\n            }\n        }\n    }\n\n    // back substitution on rank 0\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n        for (size_t i = N - 2; i < N; i--) {\n            double sum = 0;\n            for (size_t j = i + 1; j < N; j++) {\n                sum += A_copy[i * N + j] * x[j];\n            }\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // Perform Gaussian elimination in parallel\n    for (size_t i = 0; i < N; i++) {\n        // Find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        // Broadcast the pivot row and the current row to all ranks\n        MPI_Bcast(&A_copy[i * N], N, MPI_DOUBLE, i % size, MPI_COMM_WORLD);\n        MPI_Bcast(&b_copy[i], 1, MPI_DOUBLE, i % size, MPI_COMM_WORLD);\n\n        // Eliminate the current row in parallel\n        for (size_t j = i + 1; j < N; j++) {\n            if (j % size == rank) {\n                double factor = A_copy[j * N + i] / A_copy[i * N + i];\n                for (size_t k = 0; k < N; k++) {\n                    A_copy[j * N + k] -= factor * A_copy[i * N + k];\n                }\n                b_copy[j] -= factor * b_copy[i];\n            }\n        }\n    }\n\n    // Back substitution on rank 0\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n        for (size_t i = N - 2; i < N; i--) {\n            double sum = 0;\n            for (size_t j = i + 1; j < N; j++) {\n                sum += A_copy[i * N + j] * x[j];\n            }\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // perform Gaussian elimination\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        if (rank == 0) {\n            double sum = 0;\n            for (size_t j = i + 1; j < N; j++) {\n                sum += A_copy[i * N + j] * x[j];\n            }\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // perform Gaussian elimination in parallel\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        // broadcast pivot row and b_copy to all ranks\n        MPI_Bcast(&A_copy[i * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&b_copy[i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        // eliminate the current row in parallel\n        for (size_t j = i + 1; j < N; j++) {\n            if (j % size == rank) {\n                double factor = A_copy[j * N + i] / A_copy[i * N + i];\n                for (size_t k = 0; k < N; k++) {\n                    A_copy[j * N + k] -= factor * A_copy[i * N + k];\n                }\n                b_copy[j] -= factor * b_copy[i];\n            }\n        }\n    }\n\n    // back substitution in parallel\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n\n    // gather results to rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : x.data(), N, MPI_DOUBLE, x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n        for (size_t i = N - 2; i < N; i--) {\n            double sum = 0;\n            for (size_t j = i + 1; j < N; j++) {\n                sum += A_copy[i * N + j] * x[j];\n            }\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // perform Gaussian elimination in parallel\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        // broadcast the pivot row and swap it with the current row on all ranks\n        MPI_Bcast(&A_copy[i * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&b_copy[i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        // eliminate the current row in parallel\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    // back substitution in parallel\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n\n    // gather the results on rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : x.data(), N, MPI_DOUBLE, x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n        for (size_t i = N - 2; i < N; i--) {\n            double sum = 0;\n            for (size_t j = i + 1; j < N; j++) {\n                sum += A_copy[i * N + j] * x[j];\n            }\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // perform Gaussian elimination in parallel\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        // broadcast the pivot row and swap if necessary\n        MPI_Bcast(&A_copy[i * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&b_copy[i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        // eliminate the current row in parallel\n        for (size_t j = i + 1; j < N; j++) {\n            if (j % size == rank) {\n                double factor = A_copy[j * N + i] / A_copy[i * N + i];\n                for (size_t k = 0; k < N; k++) {\n                    A_copy[j * N + k] -= factor * A_copy[i * N + k];\n                }\n                b_copy[j] -= factor * b_copy[i];\n            }\n        }\n    }\n\n    // back substitution in parallel\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n\n    // gather the results on rank 0\n    MPI_Gather(rank == 0 ? MPI_IN_PLACE : x.data(), N, MPI_DOUBLE, x.data(), N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    // perform Gaussian elimination in parallel\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        // broadcast the pivot row to all ranks\n        MPI_Bcast(&A_copy[pivot_row * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&b_copy[pivot_row], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        // eliminate the current row in parallel\n        for (size_t j = i + 1; j < N; j++) {\n            if (j % size == rank) {\n                double factor = A_copy[j * N + i] / A_copy[i * N + i];\n                for (size_t k = 0; k < N; k++) {\n                    A_copy[j * N + k] -= factor * A_copy[i * N + k];\n                }\n                b_copy[j] -= factor * b_copy[i];\n            }\n        }\n    }\n\n    // back substitution in parallel\n    std::vector<double> local_x(N, 0);\n    for (size_t i = N - 1; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * local_x[j];\n        }\n        local_x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n\n    // gather the local_x from all ranks to x on rank 0\n    MPI_Gather(&local_x[0], N, MPI_DOUBLE, &x[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (A.size() != N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size() != N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n\n    std::vector<double> A_copy(A);\n    std::vector<double> b_copy(b);\n\n    for (size_t i = 0; i < N; i++) {\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row != i) {\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n\n    if (rank == 0) {\n        x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    }\n\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        if (rank == 0) {\n            x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of gemm\n// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n//    output: C=[[9, 5], [4, 2]]\n// */\n// void gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             double sum = 0;\n//             for (size_t k = 0; k < K; ++k) {\n//                 sum += A[i * K + k] * B[k * N + j];\n//             }\n//             C[i * N + j] = sum;\n//         }\n//     }\n// }\n\n// MPI implementation of gemm\n#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
    "translation_function_name": "gemm",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                double sum = 0;\n                for (size_t k = 0; k < K; ++k) {\n                    sum += A[i * K + k] * B[k * N + j];\n                }\n                C[i * N + j] = sum;\n            }\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of gemv\n// /* Multiply the matrix A by the vector x. Store the results in the vector y.\n//    A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n//    output: y=[1, -3]\n// */\n// void gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         double sum = 0;\n//         for (size_t j = 0; j < N; ++j) {\n//             sum += A[i * N + j] * x[j];\n//         }\n//         y[i] = sum;\n//     }\n// }\n\n// MPI implementation of gemv\n#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
    "translation_function_name": "gemv",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            double sum = 0;\n            for (size_t j = 0; j < N; ++j) {\n                sum += A[i * N + j] * x[j];\n            }\n            y[i] = sum;\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y = std::vector<double>(M, 0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y = std::vector<double>(M, 0);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0.0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(&local_y[0], local_M, MPI_DOUBLE, &y[0], local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t local_M = M / size;\n    size_t local_start = rank * local_M;\n    size_t local_end = (rank == size - 1) ? M : local_start + local_M;\n\n    std::vector<double> local_y(local_M, 0);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        local_y[i - local_start] = sum;\n    }\n\n    if (rank == 0) {\n        y.resize(M);\n    }\n\n    MPI_Gather(local_y.data(), local_M, MPI_DOUBLE, y.data(), local_M, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// /* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    Store the results for L and U into the original matrix A. \n//    A is an NxN matrix stored in row-major.\n//    Example:\n// \n//    input: [[4, 3], [6, 3]]\n//    output: [[4, 3], [1.5, -1.5]]\n// */\n// void luFactorize(std::vector<double> &A, size_t N) {\n// for (size_t i = 0; i < N; i++) {\n//     for (size_t j = i + 1; j < N; j++) {\n//       A[j * N + i] = A[j * N + i] / A[i * N + i];\n//       for (size_t k = i + 1; k < N; k++) {\n//         A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n//       }\n//     }\n//   }\n// }\n\n// MPI implementation of luFactorize\n#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\nfor (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}",
    "translation_function_name": "luFactorize",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n\n      if (rank == 0) {\n        for (size_t k = i + 1; k < N; k++) {\n          A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n        }\n      }\n    }\n\n    if (size > 1) {\n      MPI_Bcast(&A[i * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1 + rank; k < N; k += size) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n      if (size > 1) {\n        MPI_Bcast(&A[j * N + k], N - k, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n      }\n    }\n    if (size > 1) {\n      MPI_Bcast(&A[j * N + i], N - i, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n    }\n  }\n  if (rank == 0) {\n    for (int r = 1; r < size; r++) {\n      MPI_Recv(&A[0], N * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&A[0], N * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    // Gather results from all ranks to rank 0\n    std::vector<double> recvbuf(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < A.size(); j++) {\n        A[j] = recvbuf[j];\n      }\n    }\n  } else {\n    // Send results from other ranks to rank 0\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    // Gather results from all ranks to rank 0\n    std::vector<double> recvBuffer(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvBuffer[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < A.size(); j++) {\n        A[j] = recvBuffer[j];\n      }\n    }\n  } else {\n    // Send results from other ranks to rank 0\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1 + rank; k < N; k += size) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n      if (rank == 0) {\n        MPI_Bcast(&A[j * N + i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&A[j * N + i + 1], N - i - 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      }\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int r = 1; r < size; r++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < A.size(); i++) {\n        A[i] = recvbuf[i];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    // Collect results from other ranks if necessary\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n\n      if (rank == 0) {\n        for (size_t k = i + 1; k < N; k++) {\n          A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n        }\n      }\n    }\n\n    if (size > 1) {\n      MPI_Bcast(&A[(i + 1) * N + i], N - i - 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvA(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvA[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < N * N; j++) {\n        A[j] = recvA[j];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1 + rank; k < N; k += size) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n      if (rank == 0) {\n        MPI_Bcast(&A[j * N], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      }\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    // Gather results from all ranks to rank 0\n    std::vector<double> recvBuffer(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvBuffer[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < A.size(); j++) {\n        A[j] = recvBuffer[j];\n      }\n    }\n  } else {\n    // Send results from other ranks to rank 0\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int r = 1; r < size; r++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < A.size(); i++) {\n        A[i] = recvbuf[i];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvA(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvA[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < N * N; j++) {\n        A[j] = recvA[j];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int r = 1; r < size; r++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < A.size(); i++) {\n        A[i] = recvbuf[i];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvA(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvA[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < N * N; j++) {\n        A[j] = recvA[j];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n\n      if (rank == 0) {\n        for (size_t k = i + 1; k < N; k++) {\n          A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n        }\n      }\n      MPI_Bcast(&A[j * N + i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&A[j * N + i + 1], N - i - 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < A.size(); j++) {\n        A[j] = recvbuf[j];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int r = 1; r < size; r++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t i = 0; i < A.size(); i++) {\n        A[i] = recvbuf[i];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < A.size(); j++) {\n        A[j] = recvbuf[j];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1 + rank; j < N; j += size) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::vector<double> recvbuf(A.size());\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&recvbuf[0], A.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < A.size(); j++) {\n        A[j] = recvbuf[j];\n      }\n    }\n  } else {\n    MPI_Send(&A[0], A.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "translation_prompt": "// Serial implementation of ranks\n// /* For each value in the vector x compute its index in the sorted vector.\n//    Store the results in `ranks`.\n//    Examples:\n// \n//    input: [3.1, 2.8, 9.1, 0.4, 3.14]\n//    output: [2, 1, 4, 0, 3]\n//  \n//    input: [100, 7.6, 16.1, 18, 7.6]\n//    output: [4, 0, 1, 2, 3]\n// */\n// void ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n// std::vector<float> sorted = x;\n//     std::sort(sorted.begin(), sorted.end());\n//     ranks.resize(x.size());\n//     for (size_t i = 0; i < x.size(); ++i) {\n//         ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n//     }\n// }\n\n// MPI implementation of ranks\n#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\nstd::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}",
    "translation_function_name": "ranks",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<std::vector<size_t>> all_ranks(size);\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, all_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        for (int i = 0; i < size; ++i) {\n            for (size_t j = 0; j < local_x.size(); ++j) {\n                ranks[i * local_x.size() + j] = all_ranks[i][j];\n            }\n        }\n    } else {\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), x.size(), MPI_UNSIGNED_LONG, ranks.data(), x.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<size_t> recv_counts(size);\n        std::vector<size_t> displs(size, 0);\n        for (int i = 0; i < size; ++i) {\n            recv_counts[i] = x.size() / size;\n            if (i < x.size() % size) {\n                recv_counts[i]++;\n            }\n            if (i > 0) {\n                displs[i] = displs[i - 1] + recv_counts[i - 1];\n            }\n        }\n        MPI_Gatherv(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), recv_counts.data(), displs.data(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gatherv(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, nullptr, nullptr, nullptr, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), x.size(), MPI_UNSIGNED_LONG, ranks.data(), x.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<size_t> recv_counts(size);\n        std::vector<size_t> displs(size, 0);\n\n        MPI_Gather(&local_ranks[0], local_ranks.size(), MPI_UNSIGNED_LONG, &ranks[0], local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(&local_ranks[0], local_ranks.size(), MPI_UNSIGNED_LONG, NULL, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<std::vector<size_t>> all_ranks(size);\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, all_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        for (int i = 0; i < size; ++i) {\n            size_t offset = i * (x.size() / size);\n            std::copy(all_ranks[i].begin(), all_ranks[i].end(), ranks.begin() + offset);\n        }\n    } else {\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<size_t> recv_counts(size);\n        std::vector<size_t> displs(size, 0);\n        MPI_Gather(&local_ranks[0], local_ranks.size(), MPI_UNSIGNED_LONG, &ranks[0], local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(&local_ranks[0], local_ranks.size(), MPI_UNSIGNED_LONG, NULL, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<std::vector<size_t>> all_ranks(size);\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, all_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        for (int i = 0; i < size; ++i) {\n            for (size_t j = 0; j < local_x.size(); ++j) {\n                ranks[i * local_x.size() + j] = all_ranks[i][j];\n            }\n        }\n    } else {\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<std::vector<size_t>> all_ranks(size);\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, all_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n        for (int i = 0; i < size; ++i) {\n            for (size_t j = 0; j < local_x.size(); ++j) {\n                ranks[i * local_x.size() + j] = all_ranks[i][j];\n            }\n        }\n    } else {\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::sort(local_x.begin(), local_x.end());\n\n    std::vector<size_t> local_ranks(local_x.size());\n    for (size_t i = 0; i < local_x.size(); ++i) {\n        local_ranks[i] = std::distance(local_x.begin(), std::lower_bound(local_x.begin(), local_x.end(), x[i]));\n    }\n\n    if (rank == 0) {\n        ranks.resize(x.size());\n        std::vector<std::vector<size_t>> all_ranks(size);\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, all_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        for (int i = 0; i < size; ++i) {\n            for (size_t j = 0; j < local_x.size(); ++j) {\n                ranks[i * local_x.size() + j] = all_ranks[i][j];\n            }\n        }\n    } else {\n        MPI_Gather(local_ranks.data(), local_ranks.size(), MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::vector<float> sorted;\n    if (rank == 0) {\n        sorted = x;\n        std::sort(sorted.begin(), sorted.end());\n        ranks.resize(x.size());\n    }\n\n    MPI_Bcast(&sorted[0], sorted.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    size_t local_start = rank * (x.size() / size);\n    size_t local_end = (rank == size - 1) ? x.size() : (rank + 1) * (x.size() / size);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        for (int p = 1; p < size; ++p) {\n            size_t start = p * (x.size() / size);\n            size_t end = (p == size - 1) ? x.size() : (p + 1) * (x.size() / size);\n            MPI_Recv(&ranks[start], end - start, MPI_UNSIGNED_LONG, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&ranks[local_start], local_end - local_start, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::vector<float> sorted;\n    if (rank == 0) {\n        sorted = x;\n        std::sort(sorted.begin(), sorted.end());\n        ranks.resize(x.size());\n    }\n\n    MPI_Bcast(&sorted[0], x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    size_t local_start = rank * (x.size() / size);\n    size_t local_end = (rank + 1) * (x.size() / size);\n    if (rank == size - 1) {\n        local_end = x.size();\n    }\n\n    std::vector<size_t> local_ranks(local_end - local_start);\n    for (size_t i = local_start; i < local_end; ++i) {\n        local_ranks[i - local_start] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        ranks = local_ranks;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&ranks[0] + i * (x.size() / size), x.size() / size, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&local_ranks[0], local_end - local_start, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::vector<float> sorted;\n    if (rank == 0) {\n        sorted = x;\n        std::sort(sorted.begin(), sorted.end());\n        ranks.resize(x.size());\n    }\n\n    MPI_Bcast(&sorted[0], x.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    size_t local_start = rank * (x.size() / size);\n    size_t local_end = (rank + 1) * (x.size() / size);\n    if (rank == size - 1) {\n        local_end = x.size();\n    }\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        for (int p = 1; p < size; ++p) {\n            MPI_Recv(&ranks[p * (x.size() / size)], x.size() / size, MPI_UNSIGNED_LONG, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&ranks[local_start], local_end - local_start, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<float> local_x = x;\n    std::vector<float> sorted;\n    if (rank == 0) {\n        sorted = x;\n        std::sort(sorted.begin(), sorted.end());\n        ranks.resize(x.size());\n    }\n\n    MPI_Bcast(&sorted[0], sorted.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    size_t local_start = rank * (x.size() / size);\n    size_t local_end = (rank == size - 1) ? x.size() : (rank + 1) * (x.size() / size);\n\n    for (size_t i = local_start; i < local_end; ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), local_x[i]));\n    }\n\n    if (rank == 0) {\n        std::vector<size_t> recv_ranks;\n        recv_ranks.resize(x.size());\n        for (int p = 1; p < size; ++p) {\n            MPI_Recv(&recv_ranks[0] + p * (x.size() / size), x.size() / size, MPI_UNSIGNED_LONG, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        ranks = recv_ranks;\n    } else {\n        MPI_Send(&ranks[local_start], local_end - local_start, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of sortIgnoreZero\n// /* Sort the vector x in ascending order ignoring elements with value 0.\n//    Leave zero valued elements in-place.\n//    Example:\n// \n// \t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n//    output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n// */\n// void sortIgnoreZero(std::vector<int> &x) {\n// int n = x.size();\n//   int i = 0;\n//   int j = 0;\n//   int k = 0;\n// \n//   while (i < n) {\n//     if (x[i] == 0) {\n//       i++;\n//     } else {\n//       j = i + 1;\n//       while (j < n) {\n//         if (x[j] == 0) {\n//           j++;\n//         } else {\n//           if (x[i] > x[j]) {\n//             k = x[i];\n//             x[i] = x[j];\n//             x[j] = k;\n//           }\n//           j++;\n//         }\n//       }\n//       i++;\n//     }\n//   }\n// }\n\n// MPI implementation of sortIgnoreZero\n#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\nint n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < n) {\n    if (x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < n) {\n        if (x[j] == 0) {\n          j++;\n        } else {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n}",
    "translation_function_name": "sortIgnoreZero",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_INT, local_x.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(local_x.data(), local_n, MPI_INT, x.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_INT, local_x.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(local_x.data(), local_n, MPI_INT, x.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(x.data(), local_n, MPI_INT, local_x.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  std::sort(local_x.begin(), local_x.end(), [](int a, int b) {\n    if (a == 0) {\n      return false;\n    } else if (b == 0) {\n      return true;\n    } else {\n      return a < b;\n    }\n  });\n\n  MPI_Gather(local_x.data(), local_n, MPI_INT, x.data(), local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < local_n) {\n    if (local_x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < local_n) {\n        if (local_x[j] == 0) {\n          j++;\n        } else {\n          if (local_x[i] > local_x[j]) {\n            k = local_x[i];\n            local_x[i] = local_x[j];\n            local_x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int local_n = n / size;\n  std::vector<int> local_x(local_n);\n  MPI_Scatter(&x[0], local_n, MPI_INT, &local_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_n; i++) {\n    if (local_x[i] == 0) {\n      continue;\n    }\n    for (int j = i + 1; j < local_n; j++) {\n      if (local_x[j] == 0) {\n        continue;\n      }\n      if (local_x[i] > local_x[j]) {\n        int temp = local_x[i];\n        local_x[i] = local_x[j];\n        local_x[j] = temp;\n      }\n    }\n  }\n\n  std::vector<int> sorted_x(n);\n  MPI_Gather(&local_x[0], local_n, MPI_INT, &sorted_x[0], local_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    x = sorted_x;\n  }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "translation_prompt": "// Serial implementation of findKthSmallest\n// /* Find the k-th smallest element of the vector x.\n//    Example:\n//    \n//    input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n//    output: 6\n// */\n// int findKthSmallest(std::vector<int> const& x, int k) {\n// int n = x.size();\n//     std::vector<int> y(x);\n//     std::sort(y.begin(), y.end());\n//     return y[k-1];\n// }\n\n// MPI implementation of findKthSmallest\n#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\nint n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
    "translation_function_name": "findKthSmallest",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return 0;  // Placeholder return value for non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1; // or some other error code to indicate non-zero ranks should not return a value\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate non-zero ranks should not return a value\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1; // or some other error code to indicate that only rank 0 should return the result\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate non-root ranks\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    if (rank == 0) {\n        int result;\n        MPI_Status status;\n        MPI_Recv(&result, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n        return result;\n    } else {\n        if (rank <= n) {\n            int result = y[k-1];\n            MPI_Send(&result, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or any other value, since only rank 0 returns the result\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI::COMM_WORLD.Reduce(&local_kth, &global_kth, 1, MPI::INT, MPI::MIN, 0);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_kth;\n    } else {\n        return 0;  // Placeholder return value for non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI::COMM_WORLD.Reduce(&local_kth, &global_kth, 1, MPI::INT, MPI::MIN, 0);\n\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        return global_kth;\n    } else {\n        return 0;  // Placeholder return value for non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return 0;  // or some other value to indicate no result\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1; // or some other error code to indicate that only rank 0 should return the result\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1; // or some other error code to indicate that only rank 0 should return the result\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1; // or some other error code to indicate non-zero ranks should not return a value\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate non-root ranks\n    }\n}\n",
      "\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    int local_kth;\n    if (rank == 0) {\n        local_kth = y[k-1];\n    }\n\n    int global_kth;\n    MPI_Reduce(&local_kth, &global_kth, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        return global_kth;\n    } else {\n        return -1;  // or some other error code to indicate that only rank 0 should return the result\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    if (rank == 0) {\n        int result;\n        MPI_Status status;\n        MPI_Recv(&result, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n        return result;\n    } else {\n        if (rank <= n) {\n            int local_k = k - 1;\n            int local_result = y[local_k];\n            MPI_Send(&local_result, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n\n    if (rank == 0) {\n        int result;\n        MPI_Status status;\n        MPI_Recv(&result, 1, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n        return result;\n    } else {\n        if (rank <= n) {\n            int local_k = k - 1;\n            int local_result = y[local_k];\n            MPI_Send(&local_result, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "translation_prompt": "// Serial implementation of sortByStartTime\n// struct Result {\n//    int startTime, duration;\n//    float value;\n// };\n// \n// /* Sort vector of Result structs by start time in ascending order.\n//    Example:\n//    \n//    input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n//    output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n// */\n// void sortByStartTime(std::vector<Result> &results) {\n// std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n//       return a.startTime < b.startTime;\n//    });\n// }\n\n// MPI implementation of sortByStartTime\n#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\nstd::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
    "translation_function_name": "sortByStartTime",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() - localSize * (size - 1);\n      std::sort(results.begin(), results.begin() + remaining, [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size();\n   std::vector<Result> localResults = results;\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   if (rank == 0) {\n      results = localResults;\n      std::vector<Result> tempResults;\n      for (int i = 1; i < size; i++) {\n         MPI_Recv(&localSize, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         localResults.resize(localSize);\n         MPI_Recv(localResults.data(), localSize * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         tempResults.insert(tempResults.end(), localResults.begin(), localResults.end());\n      }\n      std::sort(tempResults.begin(), tempResults.end(), [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n      results.insert(results.end(), tempResults.begin(), tempResults.end());\n   } else {\n      MPI_Send(&localSize, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size();\n   std::vector<Result> localResults = results;\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   if (rank == 0) {\n      std::vector<Result> mergedResults;\n      mergedResults.reserve(localSize * size);\n      mergedResults.insert(mergedResults.end(), localResults.begin(), localResults.end());\n\n      for (int i = 1; i < size; i++) {\n         MPI_Recv(&localSize, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         localResults.resize(localSize);\n         MPI_Recv(localResults.data(), localSize * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         mergedResults.insert(mergedResults.end(), localResults.begin(), localResults.end());\n      }\n\n      std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n\n      results = mergedResults;\n   } else {\n      MPI_Send(&localSize, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() - localSize * (size - 1);\n      std::sort(results.begin(), results.begin() + remaining, [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n}\n",
      "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nif (rank == 0) {\n    std::vector<Result> localResults = results;\n    std::vector<Result> mergedResults;\n\n    for (int i = 1; i < size; i++) {\n        MPI_Recv(&localResults[0], localResults.size() * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        mergedResults.insert(mergedResults.end(), localResults.begin(), localResults.end());\n    }\n\n    std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    });\n\n    results = mergedResults;\n} else {\n    MPI_Send(&results[0], results.size() * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n}\n}",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() - localSize * (size - 1);\n      std::sort(results.begin(), results.begin() + remaining, [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size() / size;\n   std::vector<Result> localResults(localSize);\n   MPI_Scatter(results.data(), localSize * sizeof(Result), MPI_BYTE, localResults.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   MPI_Gather(localResults.data(), localSize * sizeof(Result), MPI_BYTE, results.data(), localSize * sizeof(Result), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      int remaining = results.size() % size;\n      if (remaining > 0) {\n         std::sort(results.begin() + localSize * size, results.end(), [](const Result &a, const Result &b) {\n            return a.startTime < b.startTime;\n         });\n      }\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size();\n   std::vector<Result> localResults = results;\n\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   if (rank == 0) {\n      std::vector<int> recvCounts(size);\n      std::vector<int> displs(size, 0);\n\n      for (int i = 0; i < size; i++) {\n         recvCounts[i] = results.size() / size;\n      }\n      recvCounts[size - 1] += results.size() % size;\n\n      for (int i = 1; i < size; i++) {\n         displs[i] = displs[i - 1] + recvCounts[i - 1];\n      }\n\n      results.resize(localSize);\n      MPI_Gatherv(localResults.data(), localSize, MPI_BYTE, results.data(), recvCounts.data(), displs.data(), MPI_BYTE, 0, MPI_COMM_WORLD);\n   } else {\n      MPI_Gatherv(localResults.data(), localSize, MPI_BYTE, nullptr, nullptr, nullptr, MPI_BYTE, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size();\n   std::vector<Result> localResults = results;\n\n   // Scatter the data to all processes\n   std::vector<int> recvCounts(size);\n   std::vector<int> displs(size);\n   for (int i = 0; i < size; i++) {\n      recvCounts[i] = localSize / size;\n      if (i < localSize % size) {\n         recvCounts[i]++;\n      }\n      displs[i] = (i > 0) ? (displs[i - 1] + recvCounts[i - 1]) : 0;\n   }\n\n   std::vector<Result> localResultsRecv(recvCounts[rank]);\n   MPI_Scatterv(localResults.data(), recvCounts.data(), displs.data(), MPI_BYTE, localResultsRecv.data(), recvCounts[rank], MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   // Sort the local data\n   std::sort(localResultsRecv.begin(), localResultsRecv.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   // Gather the sorted data to rank 0\n   std::vector<int> sendCounts(size);\n   for (int i = 0; i < size; i++) {\n      sendCounts[i] = (i < localSize % size) ? (localSize / size + 1) : (localSize / size);\n   }\n\n   std::vector<Result> sortedResults(localSize);\n   MPI_Gatherv(localResultsRecv.data(), recvCounts[rank], MPI_BYTE, sortedResults.data(), sendCounts.data(), displs.data(), MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   // Store the sorted results in results on rank 0\n   if (rank == 0) {\n      results = sortedResults;\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (rank == 0) {\n      std::vector<Result> localResults = results;\n      std::vector<Result> mergedResults;\n\n      for (int i = 1; i < size; i++) {\n         int count;\n         MPI_Recv(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         std::vector<Result> tempResults(count);\n         MPI_Recv(tempResults.data(), count * sizeof(Result), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         mergedResults.insert(mergedResults.end(), tempResults.begin(), tempResults.end());\n      }\n\n      mergedResults.insert(mergedResults.end(), localResults.begin(), localResults.end());\n      std::sort(mergedResults.begin(), mergedResults.end(), [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n\n      results = mergedResults;\n   } else {\n      int count = results.size();\n      MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(results.data(), count * sizeof(Result), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n   }\n}\n",
      "\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int localSize = results.size();\n   std::vector<Result> localResults = results;\n\n   // Scatter the data to all processes\n   MPI_Scatter(results.data(), localSize, MPI_BYTE, localResults.data(), localSize, MPI_BYTE, 0, MPI_COMM_WORLD);\n\n   // Sort the local data\n   std::sort(localResults.begin(), localResults.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n\n   // Gather the sorted data to rank 0\n   MPI_Gather(localResults.data(), localSize, MPI_BYTE, results.data(), localSize, MPI_BYTE, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of sortComplexByMagnitude\n// /* Sort the vector x of complex numbers by their magnitude in ascending order.\n//    Example:\n//    \n//    input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n//    output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n// */\n// void sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n// std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n//         return std::abs(a) < std::abs(b);\n//     });\n// }\n\n// MPI implementation of sortComplexByMagnitude\n#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\nstd::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
    "translation_function_name": "sortComplexByMagnitude",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::copy(recvbuf.begin(), recvbuf.end(), x.begin());\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::sort(local_x.begin(), local_x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    std::vector<std::complex<double>> sorted_x(x.size());\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, sorted_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = sorted_x;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    if (rank == 0) {\n        std::vector<std::complex<double>> recvbuf(x.size() * size);\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, recvbuf.data(), x.size(), MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n        std::sort(recvbuf.begin(), recvbuf.end(), [](std::complex<double> a, std::complex<double> b) {\n            return std::abs(a) < std::abs(b);\n        });\n        x = recvbuf;\n    } else {\n        MPI_Gather(x.data(), x.size(), MPI_DOUBLE_COMPLEX, nullptr, 0, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::sort(local_x.begin(), local_x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    std::vector<std::complex<double>> sorted_x(x.size());\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, sorted_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = sorted_x;\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_n = x.size() / size;\n    std::vector<std::complex<double>> local_x(local_n);\n    MPI_Scatter(x.data(), local_n, MPI_DOUBLE_COMPLEX, local_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    std::sort(local_x.begin(), local_x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n\n    std::vector<std::complex<double>> sorted_x(x.size());\n    MPI_Gather(local_x.data(), local_n, MPI_DOUBLE_COMPLEX, sorted_x.data(), local_n, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        x = sorted_x;\n    }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of cellsXOR\n// /* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n//    Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n//    input and output are NxN grids of ints in row-major.\n//    Example:\n// \n//    input: [[0, 1, 1, 0],\n//            [1, 0, 0, 0],\n//            [0, 0, 0, 0],\n//            [0, 1, 0, 0]\n//    output: [[0, 0, 1, 1],\n//             [1, 0, 0, 1],\n//             [0, 0, 1, 0],\n//             [1, 0, 1, 0]]\n// */\n// void cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int count = 0;\n//             if (i > 0 && input[i - 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (i < N - 1 && input[i + 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (j > 0 && input[i + (j - 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (count == 1) {\n//                 output[i + j * N] = 1;\n//             } else {\n//                 output[i + j * N] = 0;\n//             }\n//         }\n//     }\n// }\n\n// MPI implementation of cellsXOR\n#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
    "translation_function_name": "cellsXOR",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start = rank * local_N;\n    int end = start + local_N;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    std::vector<int> local_input(input.begin() + start * N, input.begin() + end * N);\n    std::vector<int> local_output(local_N * N, 0);\n\n    for (size_t i = start; i < end; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[(i - start) * N + j] = 1;\n            } else {\n                local_output[(i - start) * N + j] = 0;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        output = local_output;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&output[i * local_N * N], local_N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&local_output[0], local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    int start_row = rank * local_N;\n    int end_row = start_row + local_N;\n\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = start_row; i < end_row; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[(i - 1) + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && local_input[(i + 1) + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[(i - 1) * N + j] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[(i + 1) * N + j] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i * N + (j - 1)] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i * N + (j + 1)] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i * N + j] = 1;\n            } else {\n                local_output[i * N + j] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_N = N / size;\n    std::vector<int> local_input(local_N * N);\n    std::vector<int> local_output(local_N * N);\n\n    MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < local_N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && local_input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < local_N - 1 && local_input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && local_input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && local_input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                local_output[i + j * N] = 1;\n            } else {\n                local_output[i + j * N] = 0;\n            }\n        }\n    }\n\n    MPI_Gather(local_output.data(), local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of gameOfLife\n// /* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n//    A cell is 1 if it is alive and 0 if it is dead.\n//    If a live cell has fewer than 2 live neighbors then it dies.\n//    If a live cell has 2 or 3 live neighbors then it lives on.\n//    If a live cell has more than 3 live neighbords then it dies.\n//    If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input:  [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 0, 0, 0],\n//             [0, 1, 1, 0, 0],\n//             [0, 0, 1, 1, 0],\n//             [0, 1, 0, 0, 0]]\n//    output: [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 1, 0, 0],\n//             [0, 1, 0, 1, 0],\n//             [0, 0, 0, 1, 0],\n//             [0, 0, 1, 0, 0]]\n// */\n// void gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// std::vector<int> next(input.size(), 0);\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       int count = 0;\n//       for (int x = -1; x <= 1; ++x) {\n//         for (int y = -1; y <= 1; ++y) {\n//           if (x == 0 && y == 0) {\n//             continue;\n//           }\n//           size_t r = (i + x + N) % N;\n//           size_t c = (j + y + N) % N;\n//           count += input[r * N + c];\n//         }\n//       }\n//       if (input[i * N + j] == 1) {\n//         next[i * N + j] = (count == 2 || count == 3);\n//       } else {\n//         next[i * N + j] = (count == 3);\n//       }\n//     }\n//   }\n//   output = next;\n// }\n\n// MPI implementation of gameOfLife\n#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nstd::vector<int> next(input.size(), 0);\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}",
    "translation_function_name": "gameOfLife",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  std::vector<int> local_input(input.size());\n  std::vector<int> local_output(input.size());\n\n  int local_N = N / size;\n  int start = rank * local_N;\n  int end = start + local_N;\n\n  MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += local_input[r * N + c];\n        }\n      }\n      if (local_input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  MPI_Gather(next.data() + start * N, local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), recv_buffer.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < recv_buffer.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), next.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), input.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < input.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), input.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  std::vector<int> local_input(input.begin() + rank * (N * N / size), input.begin() + (rank + 1) * (N * N / size));\n  std::vector<int> local_output(local_input.size(), 0);\n\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += local_input[r * N + c];\n        }\n      }\n      if (local_input[i * N + j] == 1) {\n        local_output[i * N + j] = (count == 2 || count == 3);\n      } else {\n        local_output[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = local_output;\n    for (int i = 1; i < size; ++i) {\n      std::vector<int> temp(N * N / size, 0);\n      MPI_Recv(temp.data(), temp.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      output.insert(output.end(), temp.begin(), temp.end());\n    }\n  } else {\n    MPI_Send(local_output.data(), local_output.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  std::vector<int> local_input(input.size());\n  std::vector<int> local_output(input.size());\n\n  int local_N = N / size;\n  int start = rank * local_N;\n  int end = start + local_N;\n\n  MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += local_input[r * N + c];\n        }\n      }\n      if (local_input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  MPI_Gather(next.data() + start * N, local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  std::vector<int> local_input(input.size());\n  std::vector<int> local_output(input.size());\n\n  int local_N = N / size;\n  int start = rank * local_N;\n  int end = start + local_N;\n\n  MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += local_input[r * N + c];\n        }\n      }\n      if (local_input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  MPI_Gather(next.data() + start * N, local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> temp(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(temp.data(), temp.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < temp.size(); ++j) {\n        output[j] |= temp[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), next.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), input.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < input.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), input.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size());\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), input.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < input.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), input.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size());\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), recv_buffer.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < N; ++j) {\n        output[i * N + j] = recv_buffer[i * N + j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), next.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), input.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < input.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), input.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  std::vector<int> local_input(input.size());\n  std::vector<int> local_output(input.size());\n\n  int local_N = N / size;\n  int start = rank * local_N;\n  int end = start + local_N;\n\n  MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += local_input[r * N + c];\n        }\n      }\n      if (local_input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  MPI_Gather(next.data() + start * N, local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  std::vector<int> local_input(input.size());\n  std::vector<int> local_output(input.size());\n\n  size_t local_N = N / size;\n  size_t start = rank * local_N;\n  size_t end = start + local_N;\n\n  MPI_Scatter(input.data(), local_N * N, MPI_INT, local_input.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += local_input[r * N + c];\n        }\n      }\n      if (local_input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  MPI_Gather(next.data() + start * N, local_N * N, MPI_INT, output.data(), local_N * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_N = N / size;\n  size_t start = rank * local_N;\n  size_t end = start + local_N;\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(local_N * N, 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(&recv_buffer[0] + i * local_N * N, local_N * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    output.insert(output.end(), recv_buffer.begin(), recv_buffer.end());\n  } else {\n    MPI_Send(&next[start * N], local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size());\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), recv_buffer.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < recv_buffer.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), next.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), input.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < input.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), input.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_N = N / size;\n  size_t start = rank * local_N;\n  size_t end = start + local_N;\n\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> temp(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(temp.data(), temp.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < local_N; ++j) {\n        output[(i * local_N + j) * N] = temp[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data() + start * N, local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_N = N / size;\n  size_t start_row = rank * local_N;\n  size_t end_row = start_row + local_N;\n\n  for (size_t i = start_row; i < end_row; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> temp(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(temp.data(), temp.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < local_N; ++j) {\n        output[(i * local_N + j) * N] = temp[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data() + start_row * N, local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> next(input.size(), 0);\n  for (size_t i = rank; i < N; i += size) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> recv_buffer(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(recv_buffer.data(), recv_buffer.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < recv_buffer.size(); ++j) {\n        output[j] |= recv_buffer[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data(), next.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  std::vector<int> next(input.size(), 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_N = N / size;\n  size_t start_row = rank * local_N;\n  size_t end_row = start_row + local_N;\n\n  for (size_t i = start_row; i < end_row; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n\n  if (rank == 0) {\n    output = next;\n    std::vector<int> temp(input.size(), 0);\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(temp.data(), temp.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < local_N; ++j) {\n        output[(i * local_N + j) * N] = temp[j];\n      }\n    }\n  } else {\n    MPI_Send(next.data() + start_row * N, local_N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of jacobi1D\n// /* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n//    i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n//    Replace with 0 when reading past the boundaries of `input`.\n//    Example:\n// \n//    input: [9, -6, -1, 2, 3]\n//    output: [1, 2/3, -5/3, 4/3, 5/3]\n// */\n// void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// // YOUR CODE HERE\n//     for (int i = 1; i < input.size() - 1; i++) {\n//         output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n//     }\n// }\n\n// MPI implementation of jacobi1D\n#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// YOUR CODE HERE\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
    "translation_function_name": "jacobi1D",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[end - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[end - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start + local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start + local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[start], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == size - 1) {\n        end = input.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size - 1; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        MPI_Recv(&output[(size - 1) * local_size], input.size() - (size - 1) * local_size, MPI_DOUBLE, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = start + local_size;\n\n    if (rank == size - 1) {\n        end = input.size();\n    }\n\n    for (int i = start; i < end; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size - 1; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        MPI_Recv(&output[(size - 1) * local_size], input.size() - (size - 1) * local_size, MPI_DOUBLE, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    std::vector<double> local_input(local_size + 2, 0.0);\n    std::vector<double> local_output(local_size, 0.0);\n\n    MPI_Scatter(input.data(), local_size, MPI_DOUBLE, local_input.data() + 1, local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 1; i <= local_size; i++) {\n        local_output[i - 1] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n    }\n\n    MPI_Gather(local_output.data(), local_size, MPI_DOUBLE, output.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[end - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[input.size() - local_size], local_size, MPI_DOUBLE, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[input.size() - local_size], local_size, MPI_DOUBLE, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[end - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[local_size - 2], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < local_size - 1; i++) {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n        if (size > 1) {\n            MPI_Recv(&output[local_size - 1], 1, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else if (rank == size - 1) {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Send(&output[start + 1], 1, MPI_DOUBLE, rank - 1, 0, MPI_COMM_WORLD);\n        MPI_Recv(&output[end - 1], 1, MPI_DOUBLE, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    std::vector<double> local_input(local_size + 2, 0.0);\n    std::vector<double> local_output(local_size, 0.0);\n\n    MPI_Scatter(&input[0], local_size, MPI_DOUBLE, &local_input[1], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        local_input[0] = 0.0;\n    }\n    if (rank == size - 1) {\n        local_input[local_size + 1] = 0.0;\n    }\n\n    for (int i = 1; i < local_size + 1; i++) {\n        local_output[i - 1] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n    }\n\n    MPI_Gather(&local_output[0], local_size, MPI_DOUBLE, &output[0], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start; i < end; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size - 1; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        MPI_Recv(&output[(size - 1) * local_size], input.size() - (size - 1) * local_size, MPI_DOUBLE, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size - 1; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        MPI_Recv(&output[(size - 1) * local_size], input.size() - (size - 1) * local_size, MPI_DOUBLE, size - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    int start = rank * local_size;\n    int end = (rank == size - 1) ? input.size() : start + local_size;\n\n    for (int i = start + 1; i < end - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n\n    if (rank != 0) {\n        MPI_Send(&output[start], local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&output[i * local_size], local_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = input.size() / size;\n    std::vector<double> local_input(local_size + 2, 0.0);\n    std::vector<double> local_output(local_size, 0.0);\n\n    MPI_Scatter(&input[0], local_size, MPI_DOUBLE, &local_input[1], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 1; i <= local_size; i++) {\n        local_output[i - 1] = (local_input[i - 1] + local_input[i] + local_input[i + 1]) / 3;\n    }\n\n    MPI_Gather(&local_output[0], local_size, MPI_DOUBLE, &output[0], local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "translation_prompt": "// Serial implementation of convolveKernel\n// const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n// \n// /* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n//    the dot product of itself and its neighbors with the edge kernel.\n//    Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n//    imageIn and imageOut are NxN grayscale images stored in row-major.\n//    Store the output of the computation in imageOut.\n//    Example:\n// \n//    input: [[112, 118, 141, 152],\n//            [93, 101, 119, 203],\n//            [45, 17, 16, 232],\n//            [82, 31, 49, 101]]\n//    output: [[255, 255, 255, 255],\n//             [255, 147, 0, 255],\n//             [36, 0, 0, 255],\n//             [255, 39, 0, 255]]\n// */\n// void convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int sum = 0;\n//             for (int k = -1; k <= 1; ++k) {\n//                 for (int l = -1; l <= 1; ++l) {\n//                     int x = i + k;\n//                     int y = j + l;\n//                     if (x >= 0 && x < N && y >= 0 && y < N) {\n//                         sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n//                     }\n//                 }\n//             }\n//             imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n//         }\n//     }\n// }\n\n// MPI implementation of convolveKernel\n#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}",
    "translation_function_name": "convolveKernel",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + rank * localN + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + rank * localN + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int globalI = rank * localN + i;\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = globalI + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + rank * localN + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + rank * localN + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n",
      "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t localN = N / size;\n    std::vector<int> localImageIn(localN * N);\n    std::vector<int> localImageOut(localN * N);\n\n    MPI_Scatter(imageIn.data(), localN * N, MPI_INT, localImageIn.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localN; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int globalI = rank * localN + i;\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = globalI + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += localImageIn[i * N + j] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            localImageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n\n    MPI_Gather(localImageOut.data(), localN * N, MPI_INT, imageOut.data(), localN * N, MPI_INT, 0, MPI_COMM_WORLD);\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of jacobi2D\n// /* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n//    i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n//    Replace with 0 when reading past the boundaries of `input`.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n//    output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n// */\n// void jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n// for (size_t i = 1; i < N - 1; ++i) {\n//     for (size_t j = 1; j < N - 1; ++j) {\n//       output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n//     }\n//   }\n// }\n\n// MPI implementation of jacobi2D\n#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}",
    "translation_function_name": "jacobi2D",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2) / size * rank + 1;\n  size_t local_end = (N-2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2) / size * r + 1;\n      size_t end = (N-2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      size_t start = (N - 2) / size * i + 1;\n      size_t end = (N - 2) / size * (i + 1) + 1;\n      if (i == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2)/size * rank + 1;\n  size_t local_end = (N-2)/size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2)/size * r + 1;\n      size_t end = (N-2)/size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2)/size * rank + 1;\n  size_t local_end = (N-2)/size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2)/size * r + 1;\n      size_t end = (N-2)/size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2) / size * rank + 1;\n  size_t local_end = (N-2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2) / size * r + 1;\n      size_t end = (N-2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2)/size * rank + 1;\n  size_t local_end = (N-2)/size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2)/size * r + 1;\n      size_t end = (N-2)/size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_N = N / size;\n  size_t start_row = rank * local_N;\n  size_t end_row = start_row + local_N;\n\n  if (rank == size - 1) {\n    end_row = N;\n    local_N = end_row - start_row;\n  }\n\n  std::vector<double> local_input(local_N * N);\n  std::vector<double> local_output(local_N * N);\n\n  MPI_Scatter(input.data(), local_N * N, MPI_DOUBLE, local_input.data(), local_N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (size_t i = 1; i < local_N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      local_output[i * N + j] = (local_input[(i - 1) * N + j] + local_input[(i + 1) * N + j] + local_input[i * N + j - 1] + local_input[i * N + j + 1] + local_input[i * N + j]) / 5;\n    }\n  }\n\n  MPI_Gather(local_output.data(), local_N * N, MPI_DOUBLE, output.data(), local_N * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2) / size * rank + 1;\n  size_t local_end = (N-2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2) / size * r + 1;\n      size_t end = (N-2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N - 2) / size * rank + 1;\n  size_t local_end = (N - 2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N - 2) / size * r + 1;\n      size_t end = (N - 2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n",
      "\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t local_start = (N-2) / size * rank + 1;\n  size_t local_end = (N-2) / size * (rank + 1) + 1;\n  if (rank == size - 1) {\n    local_end = N - 1;\n  }\n\n  for (size_t i = local_start; i < local_end; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; ++r) {\n      size_t start = (N-2) / size * r + 1;\n      size_t end = (N-2) / size * (r + 1) + 1;\n      if (r == size - 1) {\n        end = N - 1;\n      }\n      MPI_Recv(&output[start * N], (end - start) * N, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&output[local_start * N], (local_end - local_start) * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}\n"
    ]
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(Kokkos::View<const double*> &x) {",
    "translation_prompt": "// CUDA implementation of closestPair\n// __device__ double distanceBetweenPoints(double x1, double x2) {\n// \treturn fabs(x1 - x2);\n// }\n// \n// /* Compute the distance between the closest two elements in the vector x.\n//    Store the result in distance.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [7, 3, 9, 12, 31, 1]\n//    output: 2\n// */\n// __global__ void closestPair(const double *x, size_t N, double *distance) {\n// \n//     int tid = blockIdx.x * blockDim.x + threadIdx.x;\n//     double minDist = DBL_MAX;\n// \n//     for (int i = 0; i < N; i++) {\n//         for (int j = i + 1; j < N; j++) {\n//             double dist = distanceBetweenPoints(x[i], x[j]);\n//             if (dist < minDist) {\n//                 minDist = dist;\n//             }\n//         }\n//     }\n// \n//     distance[tid] = minDist;\n// }\n\n// Kokkos implementation of closestPair\n#include <Kokkos_Core.hpp>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(Kokkos::View<const double*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "__device__ double distanceBetweenPoints(double x1, double x2) {\n\treturn fabs(x1 - x2);\n}\n\n/* Compute the distance between the closest two elements in the vector x.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\n__global__ void closestPair(const double *x, size_t N, double *distance) {\n\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distanceBetweenPoints(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}",
    "translation_function_name": "closestPair",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(\"closest_pair\", x.size(), KOKKOS_LAMBDA(const int i, double& local_minDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < local_minDist) {\n                local_minDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& localMinDist) {\n        for (int j = i + 1; j < x.extent(0); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(\"closest_pair\", x.size(), KOKKOS_LAMBDA(const int i, double& local_minDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < local_minDist) {\n                local_minDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(\"closest_pair\", x.size(), KOKKOS_LAMBDA(int i, double &local_minDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < local_minDist) {\n                local_minDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(\"closest_pair\", x.size(), KOKKOS_LAMBDA(const int i, double& local_minDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < local_minDist) {\n                local_minDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& localMinDist) {\n        for (int j = i + 1; j < x.extent(0); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double &localMinDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(\"closest_pair\", x.size(), KOKKOS_LAMBDA(const int i, double& local_minDist) {\n        for (int j = i + 1; j < x.size(); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < local_minDist) {\n                local_minDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n",
      "\n    double minDist = DBL_MAX;\n\n    Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(int i, double& localMinDist) {\n        for (int j = i + 1; j < x.extent(0); j++) {\n            double dist = distance(x(i), x(j));\n            if (dist < localMinDist) {\n                localMinDist = dist;\n            }\n        }\n    }, Kokkos::Min<double>(minDist));\n\n    return minDist;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A represents a directed graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {",
    "translation_prompt": "// CUDA implementation of edgeCount\n// /* Count the number of edges in the directed graph defined by the adjacency matrix A.\n//    Store the result in numEdges. A represents a directed graph.\n//    A is an NxN adjacency matrix stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// __global__ void edgeCount(const int *A, size_t N, int *numEdges) {\n// int tid = threadIdx.x;\n// \tint bid = blockIdx.x;\n// \tint numThreads = blockDim.x;\n// \tint numBlocks = gridDim.x;\n// \tint start = bid * numThreads + tid;\n// \tint end = (bid + 1) * numThreads;\n// \tint count = 0;\n// \tfor (int i = start; i < end; i++) {\n// \t\tfor (int j = 0; j < N; j++) {\n// \t\t\tif (A[i * N + j] == 1) {\n// \t\t\t\tcount++;\n// \t\t\t}\n// \t\t}\n// \t}\n// \t__syncthreads();\n// \tif (tid == 0) {\n// \t\tatomicAdd(numEdges, count);\n// \t}\n// }\n\n// Kokkos implementation of edgeCount\n#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A represents a directed graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   Store the result in numEdges. A represents a directed graph.\n   A is an NxN adjacency matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {\nint tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numThreads = blockDim.x;\n\tint numBlocks = gridDim.x;\n\tint start = bid * numThreads + tid;\n\tint end = (bid + 1) * numThreads;\n\tint count = 0;\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\tif (tid == 0) {\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
    "translation_function_name": "edgeCount",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(\"edgeCount\", Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n\n  return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(\"edge_count\", Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int i, int& local_count) {\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(\"edgeCount\", Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(\"edge_count\", Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n    int count = 0;\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n        for (int j = 0; j < N; j++) {\n            if (A(i, j) == 1) {\n                local_count++;\n            }\n        }\n    }, count);\n    return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(\"edgeCount\", Kokkos::RangePolicy<>(0, N), KOKKOS_LAMBDA(int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n",
      "\n  int count = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_count) {\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        local_count++;\n      }\n    }\n  }, count);\n  return count;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {",
    "translation_prompt": "// CUDA implementation of maxDegree\n// /* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph. \n//    Store the result in maxDegree.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// __global__ void maxDegree(const int *A, size_t N, int *maxDegree) {\n// int tid = threadIdx.x;\n// \tint max = 0;\n// \tfor (int i = tid; i < N; i += blockDim.x) {\n// \t\tint degree = 0;\n// \t\tfor (int j = 0; j < N; j++) {\n// \t\t\tif (A[i * N + j] == 1) {\n// \t\t\t\tdegree++;\n// \t\t\t}\n// \t\t}\n// \t\tif (degree > max) {\n// \t\t\tmax = degree;\n// \t\t}\n// \t}\n// \t__syncthreads();\n// \tatomicMax(maxDegree, max);\n// }\n\n// Kokkos implementation of maxDegree\n#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph. \n   Store the result in maxDegree.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {\nint tid = threadIdx.x;\n\tint max = 0;\n\tfor (int i = tid; i < N; i += blockDim.x) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\t__syncthreads();\n\tatomicMax(maxDegree, max);\n}",
    "translation_function_name": "maxDegree",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int& localMax) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > localMax) {\n      localMax = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\nint max = 0;\nKokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n        if (A(i, j) == 1) {\n            degree++;\n        }\n    }\n    if (degree > local_max) {\n        local_max = degree;\n    }\n}, Kokkos::Max<int>(max));\nreturn max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\nint max = 0;\nKokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n        if (A(i, j) == 1) {\n            degree++;\n        }\n    }\n    if (degree > local_max) {\n        local_max = degree;\n    }\n}, Kokkos::Max<int>(max));\nreturn max;\n}\n",
      "\nint max = 0;\nKokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n        if (A(i, j) == 1) {\n            degree++;\n        }\n    }\n    if (degree > local_max) {\n        local_max = degree;\n    }\n}, Kokkos::Max<int>(max));\nreturn max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(int i, int &localMax) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > localMax) {\n      localMax = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n",
      "\n  int maxDegree = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int& local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(maxDegree));\n  return maxDegree;\n}\n",
      "\n  int max = 0;\n  Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, int &local_max) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      if (A(i, j) == 1) {\n        degree++;\n      }\n    }\n    if (degree > local_max) {\n      local_max = degree;\n    }\n  }, Kokkos::Max<int>(max));\n  return max;\n}\n"
    ]
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {",
    "translation_prompt": "// CUDA implementation of shortestPathLength\n// /* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Store the result in pathLength.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n// \t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n//    output: 2\n// */\n// __global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {\n// int row = blockIdx.y * blockDim.y + threadIdx.y;\n//     int col = blockIdx.x * blockDim.x + threadIdx.x;\n// \n//     if (row < N && col < N) {\n//         if (row == col) {\n//             pathLength[row] = 0;\n//         } else if (A[row * N + col] == 1) {\n//             pathLength[row] = 1;\n//         } else {\n//             pathLength[row] = INT_MAX;\n//         }\n// \n//         __syncthreads();\n// \n//         for (int k = 0; k < N; ++k) {\n//             if (pathLength[row] + pathLength[k] < pathLength[col]) {\n//                 pathLength[col] = pathLength[row] + pathLength[k];\n//             }\n//             __syncthreads();\n//         }\n//     }\n// \n//     if (row == dest) {\n//         *pathLength = pathLength[row];\n//     }\n// }\n\n// Kokkos implementation of shortestPathLength\n#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in pathLength.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {\nint row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        if (row == col) {\n            pathLength[row] = 0;\n        } else if (A[row * N + col] == 1) {\n            pathLength[row] = 1;\n        } else {\n            pathLength[row] = INT_MAX;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < N; ++k) {\n            if (pathLength[row] + pathLength[k] < pathLength[col]) {\n                pathLength[col] = pathLength[row] + pathLength[k];\n            }\n            __syncthreads();\n        }\n    }\n\n    if (row == dest) {\n        *pathLength = pathLength[row];\n    }\n}",
    "translation_function_name": "shortestPathLength",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == source) {\n            pathLength(row) = 0;\n        } else if (A(row, source) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + A(row, k) < pathLength(k)) {\n                pathLength(k) = pathLength(row) + A(row, k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(dest);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == source) {\n            pathLength(row) = 0;\n        } else if (A(row, source) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + A(row, k) < pathLength(k)) {\n                pathLength(k) = pathLength(row) + A(row, k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(dest);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == source) {\n            pathLength(row) = 0;\n        } else if (A(row, source) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + A(row, k) < pathLength(k)) {\n                pathLength(k) = pathLength(row) + A(row, k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(dest);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == source) {\n            pathLength(row) = 0;\n        } else if (A(row, source) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + A(row, k) < pathLength(k)) {\n                pathLength(k) = pathLength(row) + A(row, k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(dest);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == source) {\n            pathLength(row) = 0;\n        } else if (A(row, source) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& col) {\n            if (pathLength(col) > pathLength(source) + A(col, source) && pathLength(col) > pathLength(k) + A(col, k)) {\n                pathLength(col) = pathLength(source) + A(col, source);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(dest);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == source) {\n            pathLength(row) = 0;\n        } else if (A(row, source) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + A(row, k) < pathLength(k)) {\n                pathLength(k) = pathLength(row) + A(row, k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(dest);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + A(row, k) < pathLength(k)) {\n                pathLength(k) = pathLength(row) + A(row, k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n",
      "\n    Kokkos::View<int*> pathLength(\"pathLength\", N);\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n        if (row == dest) {\n            pathLength(row) = 0;\n        } else if (A(row, dest) == 1) {\n            pathLength(row) = 1;\n        } else {\n            pathLength(row) = INT_MAX;\n        }\n    });\n\n    Kokkos::fence();\n\n    for (int k = 0; k < N; ++k) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int& row) {\n            if (pathLength(row) + pathLength(k) < pathLength(dest)) {\n                pathLength(dest) = pathLength(row) + pathLength(k);\n            }\n        });\n        Kokkos::fence();\n    }\n\n    return pathLength(source);\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {",
    "translation_prompt": "// CUDA implementation of smallestOdd\n// /* Find the value of the smallest odd number in the vector x. Store it in smallest.\n//    Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n//    Examples:\n// \n//    input: [7, 9, 5, 2, 8, 16, 4, 1]\n//    output: 1\n// \n//    input: [8, 36, 7, 2, 11]\n//    output: 7\n// */\n// __global__ void smallestOdd(const int *x, size_t N, int *smallest) {\n// int thread_id = threadIdx.x;\n//     int block_id = blockIdx.x;\n//     int stride = blockDim.x;\n// \n//     int local_smallest = x[block_id * stride + thread_id];\n//     for (int i = 1; i < stride; i++) {\n//         int value = x[block_id * stride + thread_id + i];\n//         if (value % 2 == 1 && value < local_smallest) {\n//             local_smallest = value;\n//         }\n//     }\n//     if (local_smallest % 2 == 1) {\n//         atomicMin(smallest, local_smallest);\n//     }\n// }\n\n// Kokkos implementation of smallestOdd\n#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {\nint thread_id = threadIdx.x;\n    int block_id = blockIdx.x;\n    int stride = blockDim.x;\n\n    int local_smallest = x[block_id * stride + thread_id];\n    for (int i = 1; i < stride; i++) {\n        int value = x[block_id * stride + thread_id + i];\n        if (value % 2 == 1 && value < local_smallest) {\n            local_smallest = value;\n        }\n    }\n    if (local_smallest % 2 == 1) {\n        atomicMin(smallest, local_smallest);\n    }\n}",
    "translation_function_name": "smallestOdd",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsmall) {\n        if (x(i) % 2 == 1 && x(i) < lsmall) {\n            lsmall = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsm) {\n        if (x(i) % 2 == 1 && x(i) < lsm) {\n            lsm = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsm) {\n        if (x(i) % 2 == 1 && x(i) < lsm) {\n            lsm = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsm) {\n        if (x(i) % 2 == 1 && x(i) < lsm) {\n            lsm = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsm) {\n        if (x(i) % 2 == 1 && x(i) < lsm) {\n            lsm = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsm) {\n        if (x(i) % 2 == 1 && x(i) < lsm) {\n            lsm = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lcl) {\n        if (x(i) % 2 == 1 && x(i) < lcl) {\n            lcl = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsm) {\n        if (x(i) % 2 == 1 && x(i) < lsm) {\n            lsm = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lsmall) {\n        if (x(i) % 2 == 1 && x(i) < lsmall) {\n            lsmall = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lcl) {\n        if (x(i) % 2 == 1 && x(i) < lcl) {\n            lcl = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& local_smallest) {\n        if (x(i) % 2 == 1 && x(i) < local_smallest) {\n            local_smallest = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lcl) {\n        if (x(i) % 2 == 1 && x(i) < lcl) {\n            lcl = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n",
      "\n    int smallest = INT_MAX;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, int& lcl) {\n        if (x(i) % 2 == 1 && x(i) < lcl) {\n            lcl = x(i);\n        }\n    }, Kokkos::Min<int>(smallest));\n    return smallest;\n}\n"
    ]
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {",
    "translation_prompt": "// CUDA implementation of average\n// /* Compute the average of the vector x. Store the result in average.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Examples:\n//    \n// \t input: [1, 8, 4, 5, 1]\n//    output: 3.8\n// \n//    input: [2, 2, 2, 3]\n//    output: 2.25\n// */\n// __global__ void average(const double *x, size_t N, double *average) {\n// \n//     int tid = blockIdx.x * blockDim.x + threadIdx.x;\n//     double sum = 0.0;\n//     if (tid < N) {\n//         sum = x[tid];\n//     }\n//     sum = atomicAdd(average, sum);\n//     __syncthreads();\n//     if (tid == 0) {\n//         *average /= N;\n//     }\n// }\n\n// Kokkos implementation of average\n#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the average of the vector x. Store the result in average.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {\n\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
    "translation_function_name": "average",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(const int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n",
      "\n    double sum = 0.0;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, double& local_sum) {\n        local_sum += x(i);\n    }, sum);\n\n    return sum / x.size();\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {",
    "translation_prompt": "// CUDA implementation of partialMinimums\n// /* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Examples:\n// \n//    input: [8, 6, -1, 7, 3, 4, 4]\n//    output: [8, 6, -1, -1, -1, -1, -1]\n// \n//    input: [5, 4, 6, 4, 3, 6, 1, 1]\n//    output: [5, 4, 4, 4, 3, 3, 1, 1]\n// */\n// __global__ void partialMinimums(float *x, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         float min = x[i];\n//         for (int j = 0; j < i; j++) {\n//             if (x[j] < min) {\n//                 min = x[j];\n//             }\n//         }\n//         x[i] = min;\n//     }\n// }\n\n// Kokkos implementation of partialMinimums\n#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
    "translation_function_name": "partialMinimums",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n        float min = x(i);\n        for (int j = 0; j < i; j++) {\n            if (x(j) < min) {\n                min = x(j);\n            }\n        }\n        x(i) = min;\n    });\n}\n"
    ]
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {",
    "translation_prompt": "// CUDA implementation of prefixSum\n// /* Compute the prefix sum of the vector x into output.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n//    Example:\n//    \n//    input: [1, 7, 4, 6, 6, 2]\n//    output: [1, 8, 12, 18, 24, 26]\n// */\n// __global__ void prefixSum(const double *x, double *output, size_t N) {\n// int tid = threadIdx.x + blockIdx.x * blockDim.x;\n//     int stride = blockDim.x * gridDim.x;\n// \n//     for (size_t i = tid; i < N; i += stride) {\n//         double sum = 0;\n//         for (size_t j = 0; j <= i; ++j) {\n//             sum += x[j];\n//         }\n//         output[i] = sum;\n//     }\n// }\n\n// Kokkos implementation of prefixSum\n#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double *x, double *output, size_t N) {\nint tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        double sum = 0;\n        for (size_t j = 0; j <= i; ++j) {\n            sum += x[j];\n        }\n        output[i] = sum;\n    }\n}",
    "translation_function_name": "prefixSum",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(\"prefixSum\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n      double sum = 0;\n      for (int j = 0; j <= i; ++j) {\n          sum += x(j);\n      }\n      output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n      double sum = 0;\n      for (int j = 0; j <= i; ++j) {\n          sum += x(j);\n      }\n      output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n      double sum = 0;\n      for (int j = 0; j <= i; ++j) {\n          sum += x(j);\n      }\n      output(i) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double sum = 0;\n        for (int j = 0; j <= i; ++j) {\n            sum += x(j);\n        }\n        output(i) = sum;\n    });\n    Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {",
    "translation_prompt": "// CUDA implementation of negateOddsAndHalveEvens\n// /* In the vector x negate the odd values and divide the even values by 2.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n// \n//    input: [16, 11, 12, 14, 1, 0, 5]\n//    output: [8, -11, 6, 7, -1, 0, -5]\n// */\n// __global__ void negateOddsAndHalveEvens(int *x, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         if (x[idx] % 2 == 0) {\n//             x[idx] /= 2;\n//         } else {\n//             x[idx] = -x[idx];\n//         }\n//     }\n// }\n\n// Kokkos implementation of negateOddsAndHalveEvens\n#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}",
    "translation_function_name": "negateOddsAndHalveEvens",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        if (x(idx) % 2 == 0) {\n            x(idx) /= 2;\n        } else {\n            x(idx) = -x(idx);\n        }\n    });\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {",
    "translation_prompt": "// CUDA implementation of mapPowersOfTwo\n// __device__ bool isPowerOfTwo(int x) {\n// \treturn (x > 0) && !(x & (x - 1));\n// }\n// \n// /* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n//    Example:\n// \n//    input: [8, 0, 9, 7, 15, 64, 3]\n//    output: [true, false, false, false, false, true, false]\n// */\n// __global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {\n// int i = threadIdx.x + blockIdx.x * blockDim.x;\n// \tif (i < N) {\n// \t\tmask[i] = isPowerOfTwo(x[i]);\n// \t}\n// }\n\n// Kokkos implementation of mapPowersOfTwo\n#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {\nint i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
    "translation_function_name": "mapPowersOfTwo",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    mask(i) = isPowerOfTwo(x(i));\n  });\n}\n"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {",
    "translation_prompt": "// CUDA implementation of oneMinusInverse\n// /* Replace every element of the vector x with 1-1/x.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n//    Example:\n// \n//    input: [2, 4, 1, 12, -2]\n//    output: [0.5, 0.75, 0, 0.91666666, 1.5]\n// */\n// __global__ void oneMinusInverse(double *x, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         x[idx] = 1.0 - 1.0 / x[idx];\n//     }\n// }\n\n// Kokkos implementation of oneMinusInverse\n#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Replace every element of the vector x with 1-1/x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = 1.0 - 1.0 / x[idx];\n    }\n}",
    "translation_function_name": "oneMinusInverse",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n        x(idx) = 1.0 - 1.0 / x(idx);\n    });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n    x(idx) = 1.0 - 1.0 / x(idx);\n});\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n    x(idx) = 1.0 - 1.0 / x(idx);\n});\n}",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n    x(idx) = 1.0 - 1.0 / x(idx);\n});\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = 1.0 - 1.0 / x(idx);\n  });\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {",
    "translation_prompt": "// CUDA implementation of relu\n// /* Compute the ReLU function on every element of x. Elements less than zero become zero,\n//    while elements greater than zero stay the same.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n// \n//    input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n//    output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n// */\n// __global__ void relu(double *x, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         x[idx] = max(0.0, x[idx]);\n//     }\n// }\n\n// Kokkos implementation of relu\n#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = max(0.0, x[idx]);\n    }\n}",
    "translation_function_name": "relu",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n    x(idx) = max(0.0, x(idx));\n});\n}",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n    x(idx) = max(0.0, x(idx));\n});\n}",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n    x(idx) = max(0.0, x(idx));\n});\n}",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n    x(idx) = max(0.0, x(idx));\n});\n}",
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n        x(idx) = max(0.0, x(idx));\n    });\n}\n",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = max(0.0, x(idx));\n  });\n}\n",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = max(0.0, x(idx));\n  });\n}\n",
      "\nKokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n    x(idx) = max(0.0, x(idx));\n});\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int idx) {\n      x(idx) = max(0.0, x(idx));\n  });\n}"
    ]
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {",
    "translation_prompt": "// CUDA implementation of squareEach\n// /* Replace every element of x with the square of its value.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n// \n//    input: [5, 1, 2, -4, 8]\n//    output: [25, 1, 4, 16, 64]\n// */\n// __global__ void squareEach(int *x, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         x[i] = x[i] * x[i];\n//     }\n// }\n\n// Kokkos implementation of squareEach\n#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Replace every element of x with the square of its value.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
    "translation_function_name": "squareEach",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n        x(i) = x(i) * x(i);\n    });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n    x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n    x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}",
      "\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA (const int i) {\n      x(i) = x(i) * x(i);\n  });\n}"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {",
    "translation_prompt": "// CUDA implementation of spmv\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n//    and A is a sparse matrix stored in COO format with sizeA elements.\n//    A has dimensions MxN, x has N values, and y has M values.\n//    Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n//    Example:\n// \n//    input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n//    output: y=[2, 3]\n// */\n// __global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {\n// size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n//    if (row < M) {\n//       double sum = 0.0;\n//       for (size_t i = 0; i < sizeA; i++) {\n//          if (A[i].row == row) {\n//             sum += A[i].value * x[A[i].column];\n//          }\n//       }\n//       y[row] = alpha * sum + beta * y[row];\n//    }\n// }\n\n// Kokkos implementation of spmv\n#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {\nsize_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row < M) {\n      double sum = 0.0;\n      for (size_t i = 0; i < sizeA; i++) {\n         if (A[i].row == row) {\n            sum += A[i].value * x[A[i].column];\n         }\n      }\n      y[row] = alpha * sum + beta * y[row];\n   }\n}",
    "translation_function_name": "spmv",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA (const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA (const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n   Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const size_t row) {\n      double sum = 0.0;\n      for (size_t i = 0; i < A.extent(0); i++) {\n         if (A(i).row == row) {\n            sum += A(i).value * x(A(i).column);\n         }\n      }\n      y(row) = alpha * sum + beta * y(row);\n   });\n   Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// CUDA implementation of spmm\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n//    X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n//    Example:\n// \n//    input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n//    output: Y=[{-1,-4}, {1,0}]\n// */\n// __global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {\n// \n//     int row = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (row < M) {\n//         for (int i = 0; i < sizeA; i++) {\n//             if (A[i].row == row) {\n//                 int col = A[i].column;\n//                 double valA = A[i].value;\n//                 for (int j = 0; j < sizeX; j++) {\n//                     if (X[j].row == col) {\n//                         int colX = X[j].column;\n//                         double valX = X[j].value;\n//                         atomicAdd(&Y[row * N + colX], valA * valX);\n//                     }\n//                 }\n//             }\n//         }\n//     }\n// }\n\n// Kokkos implementation of spmm\n#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        for (int i = 0; i < sizeA; i++) {\n            if (A[i].row == row) {\n                int col = A[i].column;\n                double valA = A[i].value;\n                for (int j = 0; j < sizeX; j++) {\n                    if (X[j].row == col) {\n                        int colX = X[j].column;\n                        double valX = X[j].value;\n                        atomicAdd(&Y[row * N + colX], valA * valX);\n                    }\n                }\n            }\n        }\n    }\n}\n",
    "translation_function_name": "spmm",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.size(); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.size(); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n        for (int i = 0; i < A.extent(0); i++) {\n            if (A(i).row == row) {\n                int col = A(i).column;\n                double valA = A(i).value;\n                for (int j = 0; j < X.extent(0); j++) {\n                    if (X(j).row == col) {\n                        int colX = X(j).column;\n                        double valX = X(j).value;\n                        Kokkos::atomic_add(&Y(row, colX), valA * valX);\n                    }\n                }\n            }\n        }\n    });\n}\n"
    ]
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<double*> &z) {",
    "translation_prompt": "// CUDA implementation of sparseAxpy\n// struct Element {\n// \tsize_t index;\n//   double value;\n// };\n// \n// /* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n//    Example:\n//    \n//    input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n//    output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n// */\n// __global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {\n// size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n// \tif (i < N) {\n// \t\tdouble sum = 0;\n// \t\tfor (size_t j = 0; j < Nx; j++) {\n// \t\t\tif (x[j].index == i) {\n// \t\t\t\tsum += x[j].value;\n// \t\t\t}\n// \t\t}\n// \t\tfor (size_t j = 0; j < Ny; j++) {\n// \t\t\tif (y[j].index == i) {\n// \t\t\t\tsum += y[j].value;\n// \t\t\t}\n// \t\t}\n// \t\tz[i] = alpha * sum;\n// \t}\n// }\n\n// Kokkos implementation of sparseAxpy\n#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<double*> &z) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {\nsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tdouble sum = 0;\n\t\tfor (size_t j = 0; j < Nx; j++) {\n\t\t\tif (x[j].index == i) {\n\t\t\t\tsum += x[j].value;\n\t\t\t}\n\t\t}\n\t\tfor (size_t j = 0; j < Ny; j++) {\n\t\t\tif (y[j].index == i) {\n\t\t\t\tsum += y[j].value;\n\t\t\t}\n\t\t}\n\t\tz[i] = alpha * sum;\n\t}\n}",
    "translation_function_name": "sparseAxpy",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    double sum = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j).index == i) {\n        sum += x(j).value;\n      }\n    }\n    for (size_t j = 0; j < y.extent(0); j++) {\n      if (y(j).index == i) {\n        sum += y(j).value;\n      }\n    }\n    z(i) = alpha * sum;\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
    "translation_prompt": "// CUDA implementation of firstLetterCounts\n// /* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n//    Assume all strings are in lower case. Store the output in `bins` array.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n//    output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n// */\n// __global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {\n// int tid = threadIdx.x;\n//     int bid = blockIdx.x;\n//     int stride = blockDim.x;\n//     int i = bid * stride + tid;\n//     if (i < N) {\n//         char c = s[i][0];\n//         if (c >= 'a' && c <= 'z') {\n//             atomicAdd(&bins[c - 'a'], 1);\n//         }\n//     }\n// }\n\n// Kokkos implementation of firstLetterCounts\n#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {\nint tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int stride = blockDim.x;\n    int i = bid * stride + tid;\n    if (i < N) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
    "translation_function_name": "firstLetterCounts",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z') {\n            Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n        }\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
    "translation_prompt": "// CUDA implementation of binsBy10Count\n// /* Vector x contains values between 0 and 100, inclusive. Count the number of\n//    values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n//    Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n//    Example:\n// \n//    input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n//    output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n// */\n// __global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {\n// size_t tid = threadIdx.x;\n//     size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n//     size_t stride = blockDim.x * gridDim.x;\n// \n//     for (size_t i = gid; i < N; i += stride) {\n//         size_t bin = (size_t) (x[i] / 10);\n//         atomicAdd(&bins[bin], 1);\n//     }\n// }\n\n// Kokkos implementation of binsBy10Count\n#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {\nsize_t tid = threadIdx.x;\n    size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = gid; i < N; i += stride) {\n        size_t bin = (size_t) (x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
    "translation_function_name": "binsBy10Count",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = (size_t) (x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = (size_t) (x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      size_t bin = static_cast<size_t>(x(i) / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = (size_t) (x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = static_cast<size_t>(x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        size_t bin = (size_t) (x(i) / 10);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
    "translation_prompt": "// CUDA implementation of countQuadrants\n// struct Point {\n//    double x, y;\n// };\n// \n// /* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n//    Store the counts in `bins`.\n//    Use CUDA to count in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n//    output: [3, 1, 0, 2]\n// */\n// __global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {\n// int i = threadIdx.x;\n//     if (i < N) {\n//         double x = points[i].x;\n//         double y = points[i].y;\n//         if (x >= 0 && y >= 0) {\n//             atomicAdd(&bins[0], 1);\n//         } else if (x < 0 && y >= 0) {\n//             atomicAdd(&bins[1], 1);\n//         } else if (x < 0 && y < 0) {\n//             atomicAdd(&bins[2], 1);\n//         } else if (x >= 0 && y < 0) {\n//             atomicAdd(&bins[3], 1);\n//         }\n//     }\n// }\n\n// Kokkos implementation of countQuadrants\n#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {\nint i = threadIdx.x;\n    if (i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x >= 0 && y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (x < 0 && y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (x < 0 && y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (x >= 0 && y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
    "translation_function_name": "countQuadrants",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
    "translation_prompt": "// CUDA implementation of countQuartiles\n// /* Count the number of doubles in the vector x that have a fractional part \n//    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Examples:\n// \n//    input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n//    output: [2, 1, 2, 2]\n// \n//    input: [1.9, 0.2, 0.6, 10.1, 7.4]\n//    output: [2, 1, 1, 1]\n// */\n// __global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {\n// size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         double frac = fmod(x[i], 1.0);\n//         if (frac < 0.25)\n//             atomicAdd(&bins[0], 1);\n//         else if (frac < 0.5)\n//             atomicAdd(&bins[1], 1);\n//         else if (frac < 0.75)\n//             atomicAdd(&bins[2], 1);\n//         else\n//             atomicAdd(&bins[3], 1);\n//     }\n// }\n\n// Kokkos implementation of countQuartiles\n#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {\nsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = fmod(x[i], 1.0);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
    "translation_function_name": "countQuartiles",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n      double frac = fmod(x(i), 1.0);\n      if (frac < 0.25)\n          Kokkos::atomic_fetch_add(&bins(0), 1);\n      else if (frac < 0.5)\n          Kokkos::atomic_fetch_add(&bins(1), 1);\n      else if (frac < 0.75)\n          Kokkos::atomic_fetch_add(&bins(2), 1);\n      else\n          Kokkos::atomic_fetch_add(&bins(3), 1);\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double frac = fmod(x(i), 1.0);\n        if (frac < 0.25)\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        else if (frac < 0.5)\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        else if (frac < 0.75)\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        else\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
    "translation_prompt": "// CUDA implementation of pixelCounts\n// /* Count the number of pixels in image with each grayscale intensity.\n//    The vector `image` is a grayscale image with values 0-255.\n//    Store the results in `bins`.\n//    Use CUDA to count in parallel. The kernel is launched with at least N threads.\n//    Example:\n//    \n//    input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n//    output: [0, 0, 2, 0, 1, ...]\n// */\n// __global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {\n// int tid = threadIdx.x;\n//     int gid = blockIdx.x * blockDim.x + threadIdx.x;\n//     int bin = 0;\n//     for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n//         bin = image[i];\n//         atomicAdd(&bins[bin], 1);\n//     }\n// }\n\n// Kokkos implementation of pixelCounts\n#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {\nint tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int bin = 0;\n    for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n        bin = image[i];\n        atomicAdd(&bins[bin], 1);\n    }\n}",
    "translation_function_name": "pixelCounts",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n      int bin = image(i);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n      int bin = image(i);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const int i) {\n        int bin = 0;\n        bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n      int bin = image(i);\n      Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n      int bin = image(i);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n      int bin = image(i);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(0, bin), 1);\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {",
    "translation_prompt": "// CUDA implementation of contains\n// /* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n//    Use CUDA to search in parallel. The kernel is launched with at least N threads.\n//    Examples:\n// \n//    input: x=[1, 8, 2, 6, 4, 6], target=3\n//    output: false\n//    \n//    input: x=[1, 8, 2, 6, 4, 6], target=8\n//    output: true\n// */\n// __global__ void contains(const int *x, size_t N, int target, bool *found) {\n// // TODO: implement the kernel\n// }\n\n// Kokkos implementation of contains\n#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {\n// TODO: implement the kernel\n}",
    "translation_function_name": "contains",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n    bool found = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n        if (x(i) == target) {\n            local_found = true;\n        }\n    }, found);\n    return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n    bool found = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n        if (x(i) == target) {\n            lfound = true;\n        }\n    }, found);\n    return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n    if (x(i) == target) {\n      lfound = true;\n    }\n  }, found);\n  return found;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, Kokkos::View<const int*> const& y, int val) {",
    "translation_prompt": "// CUDA implementation of xorContains\n// /* Set `found` to true if `val` is only in one of vectors x or y.\n//    Set it to false if it is in both or neither.\n//    Use CUDA to search in parallel. The kernel is launched with at least N threads.\n//    Examples:\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n//    output: true\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n//    output: false\n// */\n// __global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {\n// int tid = threadIdx.x;\n//   int i = blockIdx.x * blockDim.x + tid;\n//   int found_local = 0;\n//   while (i < N) {\n//     found_local ^= (x[i] == val) | (y[i] == val);\n//     i += blockDim.x * gridDim.x;\n//   }\n//   __syncthreads();\n//   if (tid == 0) {\n//     *found = found_local;\n//   }\n// }\n\n// Kokkos implementation of xorContains\n#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, Kokkos::View<const int*> const& y, int val) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {\nint tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + tid;\n  int found_local = 0;\n  while (i < N) {\n    found_local ^= (x[i] == val) | (y[i] == val);\n    i += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  if (tid == 0) {\n    *found = found_local;\n  }\n}",
    "translation_function_name": "xorContains",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, Kokkos::Min<bool>(found));\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, Kokkos::Min<bool>(found));\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, found);\n  return found;\n}\n",
      "\n  bool found = false;\n  Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& local_found) {\n    local_found ^= (x(i) == val) | (y(i) == val);\n  }, Kokkos::Min<bool>(found));\n  return found;\n}\n"
    ]
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {",
    "translation_prompt": "// CUDA implementation of findLastShortBook\n// struct Book {\n//    const char* title;\n//    int pages;\n// };\n// \n// /* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n//    Store the result in lastShortBookIndex.\n//    Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n// \t Example:\n// \n//    input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n//    output: 2\n// */\n// __global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {\n// \n// // Thread index\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n// \n// // Check if the thread index is within the bounds of the books array\n// if (i < N) {\n//     // Check if the book has less than 100 pages\n//     if (books[i].pages < 100) {\n//         // Use atomicMax to safely find the largest index among all threads\n//         atomicMax((int*)lastShortBookIndex, i);\n//     }\n// }\n// }\n\n// Kokkos implementation of findLastShortBook\n#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct Book {\n   const char* title;\n   int pages;\n};\n\n/* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {\n\n// Thread index\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n\n// Check if the thread index is within the bounds of the books array\nif (i < N) {\n    // Check if the book has less than 100 pages\n    if (books[i].pages < 100) {\n        // Use atomicMax to safely find the largest index among all threads\n        atomicMax((int*)lastShortBookIndex, i);\n    }\n}\n}",
    "translation_function_name": "findLastShortBook",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\n\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\n  size_t lastShortBookIndex = 0;\n  Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n      lsb = i;\n    }\n  }, lastShortBookIndex);\n  return lastShortBookIndex;\n}\n",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\nsize_t lastShortBookIndex = 0;\nKokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n    if (books(i).pages < 100) {\n        lsb = i;\n    }\n}, lastShortBookIndex);\nreturn lastShortBookIndex;\n}",
      "\n    size_t lastShortBookIndex = 0;\n    Kokkos::parallel_reduce(books.size(), KOKKOS_LAMBDA(int i, size_t& lsb) {\n        if (books(i).pages < 100) {\n            lsb = i;\n        }\n    }, lastShortBookIndex);\n    return lastShortBookIndex;\n}\n"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(Kokkos::View<const double*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
    "translation_prompt": "// CUDA implementation of dft\n// /* Compute the discrete fourier transform of x. Store the result in output.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [1, 4, 9, 16]\n//    output: [30+0i, -8-12i, -10-0i, -8+12i]\n// */\n// __global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {\n// int tid = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (tid >= N) return;\n// \n//     cuDoubleComplex sum = {0, 0};\n//     for (size_t k = 0; k < N; ++k) {\n//         double angle = -2 * M_PI * tid * k / N;\n//         cuDoubleComplex exp_part = {cos(angle), sin(angle)};\n//         cuDoubleComplex term = cuCmul(exp_part, make_cuDoubleComplex(x[k], 0));\n//         sum = cuCadd(sum, term);\n//     }\n//     output[tid] = sum;\n// }\n\n// Kokkos implementation of dft\n#include <Kokkos_Core.hpp>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(Kokkos::View<const double*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\n__global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {\nint tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex sum = {0, 0};\n    for (size_t k = 0; k < N; ++k) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex exp_part = {cos(angle), sin(angle)};\n        cuDoubleComplex term = cuCmul(exp_part, make_cuDoubleComplex(x[k], 0));\n        sum = cuCadd(sum, term);\n    }\n    output[tid] = sum;\n}",
    "translation_function_name": "dft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n      Kokkos::complex<double> sum = {0, 0};\n      for (size_t k = 0; k < x.extent(0); ++k) {\n          double angle = -2 * M_PI * tid * k / x.extent(0);\n          Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n          Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n          sum += term;\n      }\n      output(tid) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (size_t k = 0; k < x.extent(0); ++k) {\n        double angle = -2 * M_PI * tid * k / x.extent(0);\n        Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n        Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n        sum += term;\n    }\n    output(tid) = sum;\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int tid) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (size_t k = 0; k < x.extent(0); ++k) {\n            double angle = -2 * M_PI * tid * k / x.extent(0);\n            Kokkos::complex<double> exp_part = {cos(angle), sin(angle)};\n            Kokkos::complex<double> term = exp_part * Kokkos::complex<double>(x(k), 0);\n            sum += term;\n        }\n        output(tid) = sum;\n    });\n    Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<const Kokkos::complex<double>*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
    "translation_prompt": "// CUDA implementation of fft\n// /* Compute the fourier transform of x. Store the result in output.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// __global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *output, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         cuDoubleComplex sum = {0, 0};\n//         for (int k = 0; k < N; k++) {\n//             cuDoubleComplex a = x[k];\n//             cuDoubleComplex b = cuCmul(make_cuDoubleComplex(cos(2 * M_PI * idx * k / N), -sin(2 * M_PI * idx * k / N)), a);\n//             sum = cuCadd(sum, b);\n//         }\n//         output[idx] = sum;\n//     }\n// }\n\n// Kokkos implementation of fft\n#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<const Kokkos::complex<double>*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *output, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        cuDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex a = x[k];\n            cuDoubleComplex b = cuCmul(make_cuDoubleComplex(cos(2 * M_PI * idx * k / N), -sin(2 * M_PI * idx * k / N)), a);\n            sum = cuCadd(sum, b);\n        }\n        output[idx] = sum;\n    }\n}",
    "translation_function_name": "fft",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      Kokkos::complex<double> sum = {0, 0};\n      for (int k = 0; k < x.extent(0); k++) {\n          Kokkos::complex<double> a = x(k);\n          Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n          sum += b;\n      }\n      output(idx) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = std::polar(1.0, -2 * M_PI * idx * k / x.extent(0)) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      Kokkos::complex<double> sum = {0, 0};\n      for (int k = 0; k < x.extent(0); k++) {\n          Kokkos::complex<double> a = x(k);\n          Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n          sum += b;\n      }\n      output(idx) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      Kokkos::complex<double> sum = {0, 0};\n      for (int k = 0; k < x.extent(0); k++) {\n          Kokkos::complex<double> a = x(k);\n          Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n          sum += b;\n      }\n      output(idx) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (int k = 0; k < x.extent(0); k++) {\n            Kokkos::complex<double> a = x(k);\n            Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n            sum += b;\n        }\n        output(idx) = sum;\n    });\n    Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\n}",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (int k = 0; k < x.extent(0); k++) {\n            Kokkos::complex<double> a = x(k);\n            Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n            sum += b;\n        }\n        output(idx) = sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      Kokkos::complex<double> sum = {0, 0};\n      for (int k = 0; k < x.extent(0); k++) {\n          Kokkos::complex<double> a = x(k);\n          Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n          sum += b;\n      }\n      output(idx) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n    Kokkos::complex<double> sum = {0, 0};\n    for (int k = 0; k < x.extent(0); k++) {\n        Kokkos::complex<double> a = x(k);\n        Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n        sum += b;\n    }\n    output(idx) = sum;\n});\n}",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      Kokkos::complex<double> sum = {0, 0};\n      for (int k = 0; k < x.extent(0); k++) {\n          Kokkos::complex<double> a = x(k);\n          Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n          sum += b;\n      }\n      output(idx) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (int k = 0; k < x.extent(0); k++) {\n            Kokkos::complex<double> a = x(k);\n            Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n            sum += b;\n        }\n        output(idx) = sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (int k = 0; k < x.extent(0); k++) {\n            Kokkos::complex<double> a = x(k);\n            Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n            sum += b;\n        }\n        output(idx) = sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (int k = 0; k < x.extent(0); k++) {\n            Kokkos::complex<double> a = x(k);\n            Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n            sum += b;\n        }\n        output(idx) = sum;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        Kokkos::complex<double> sum = {0, 0};\n        for (int k = 0; k < x.extent(0); k++) {\n            Kokkos::complex<double> a = x(k);\n            Kokkos::complex<double> b = Kokkos::complex<double>(cos(2 * M_PI * idx * k / x.extent(0)), -sin(2 * M_PI * idx * k / x.extent(0))) * a;\n            sum += b;\n        }\n        output(idx) = sum;\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {",
    "translation_prompt": "// CUDA implementation of axpy\n// /* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n//    \n//    input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n//    output: z=[2, -6, 5, 17]\n// */\n// __global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         z[i] = alpha * x[i] + y[i];\n//     }\n// }\n\n// Kokkos implementation of axpy\n#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
    "translation_function_name": "axpy",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n        z(i) = alpha * x(i) + y(i);\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// CUDA implementation of gemm\n// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n//    output: C=[[9, 5], [4, 2]]\n// */\n// __global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {\n// int row = blockIdx.x * blockDim.x + threadIdx.x;\n//     int col = blockIdx.y * blockDim.y + threadIdx.y;\n//     double sum = 0;\n//     for (int k = 0; k < K; k++) {\n//         sum += A[row * K + k] * B[k * N + col];\n//     }\n//     C[row * N + col] = sum;\n// }\n\n// Kokkos implementation of gemm\n#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {\nint row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A[row * K + k] * B[k * N + col];\n    }\n    C[row * N + col] = sum;\n}",
    "translation_function_name": "gemm",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::parallel_for(\"gemm\", M, KOKKOS_LAMBDA(const int row) {\n    for (int col = 0; col < N; col++) {\n        double sum = 0;\n        for (int k = 0; k < K; k++) {\n            sum += A(row, k) * B(k, col);\n        }\n        C(row, col) = sum;\n    }\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(\"gemm\", M, KOKKOS_LAMBDA(const int row) {\n    for (int col = 0; col < N; col++) {\n        double sum = 0;\n        for (int k = 0; k < K; k++) {\n            sum += A(row, k) * B(k, col);\n        }\n        C(row, col) = sum;\n    }\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A(row, k) * B(k, col);\n    }\n    C(row, col) = sum;\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A(row, k) * B(k, col);\n    }\n    C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A(row, k) * B(k, col);\n    }\n    C(row, col) = sum;\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(\"gemm\", M, KOKKOS_LAMBDA(const int row) {\n    for (size_t col = 0; col < N; col++) {\n        double sum = 0;\n        for (size_t k = 0; k < K; k++) {\n            sum += A(row, k) * B(k, col);\n        }\n        C(row, col) = sum;\n    }\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A(row, k) * B(k, col);\n    }\n    C(row, col) = sum;\n});\nKokkos::fence();\n}",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int& row, const int& col) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(row, k) * B(k, col);\n      }\n      C(row, col) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gemm\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0,0}, {M,N}), KOKKOS_LAMBDA (const int i, const int j) {\n      double sum = 0;\n      for (int k = 0; k < K; k++) {\n          sum += A(i, k) * B(k, j);\n      }\n      C(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {",
    "translation_prompt": "// CUDA implementation of gemv\n// /* Multiply the matrix A by the vector x. Store the results in the vector y.\n//    A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n//    Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n//    output: y=[1, -3]\n// */\n// __global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {\n// int row = blockIdx.x * blockDim.x + threadIdx.x;\n//   if (row < M) {\n//     double sum = 0.0;\n//     for (int col = 0; col < N; ++col) {\n//       sum += A[row * N + col] * x[col];\n//     }\n//     y[row] = sum;\n//   }\n// }\n\n// Kokkos implementation of gemv\n#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {\nint row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}",
    "translation_function_name": "gemv",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA (const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA (const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA (const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(M, KOKKOS_LAMBDA(const int row) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A(row, col) * x(col);\n    }\n    y(row) = sum;\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {",
    "translation_prompt": "// CUDA implementation of ranks\n// /* For each value in the vector x compute its index in the sorted vector.\n//    Store the results in `ranks`.\n//    Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n//    Examples:\n// \n//    input: [3.1, 2.8, 9.1, 0.4, 3.14]\n//    output: [2, 1, 4, 0, 3]\n//  \n//    input: [100, 7.6, 16.1, 18, 7.6]\n//    output: [4, 0, 1, 2, 3]\n// */\n// __global__ void ranks(const float *x, size_t *ranks, size_t N) {\n// size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         float val = x[idx];\n//         size_t rank = 0;\n//         for (size_t i = 0; i < N; ++i) {\n//             if (x[i] <= val) {\n//                 ++rank;\n//             }\n//         }\n//         ranks[idx] = rank - 1;\n//     }\n// }\n\n// Kokkos implementation of ranks\n#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {\nsize_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        float val = x[idx];\n        size_t rank = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[i] <= val) {\n                ++rank;\n            }\n        }\n        ranks[idx] = rank - 1;\n    }\n}",
    "translation_function_name": "ranks",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      float val = x(idx);\n      size_t rank = 0;\n      for (size_t i = 0; i < x.extent(0); ++i) {\n          if (x(i) <= val) {\n              ++rank;\n          }\n      }\n      ranks(idx) = rank - 1;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      float val = x(idx);\n      size_t rank = 0;\n      for (size_t i = 0; i < x.extent(0); ++i) {\n          if (x(i) <= val) {\n              ++rank;\n          }\n      }\n      ranks(idx) = rank - 1;\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      float val = x(idx);\n      size_t rank = 0;\n      for (size_t i = 0; i < x.extent(0); ++i) {\n          if (x(i) <= val) {\n              ++rank;\n          }\n      }\n      ranks(idx) = rank - 1;\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      float val = x(idx);\n      size_t rank = 0;\n      for (size_t i = 0; i < x.extent(0); ++i) {\n          if (x(i) <= val) {\n              ++rank;\n          }\n      }\n      ranks(idx) = rank - 1;\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n      float val = x(idx);\n      size_t rank = 0;\n      for (size_t i = 0; i < x.extent(0); ++i) {\n          if (x(i) <= val) {\n              ++rank;\n          }\n      }\n      ranks(idx) = rank - 1;\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int idx) {\n        float val = x(idx);\n        size_t rank = 0;\n        for (size_t i = 0; i < x.extent(0); ++i) {\n            if (x(i) <= val) {\n                ++rank;\n            }\n        }\n        ranks(idx) = rank - 1;\n    });\n    Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
    "translation_prompt": "// CUDA implementation of cellsXOR\n// /* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n//    Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n//    input and output are NxN grids of ints in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input: [[0, 1, 1, 0],\n//            [1, 0, 0, 0],\n//            [0, 0, 0, 0],\n//            [0, 1, 0, 0]\n//    output: [[0, 0, 1, 1],\n//             [1, 0, 0, 1],\n//             [0, 0, 1, 0],\n//             [1, 0, 1, 0]]\n// */\n// __global__ void cellsXOR(const int *input, int *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     int j = blockIdx.y * blockDim.y + threadIdx.y;\n// \n//     if (i < N && j < N) {\n//         int num_neighbors = 0;\n//         if (i > 0 && input[i - 1 + N * j] == 1) {\n//             num_neighbors++;\n//         }\n//         if (i < N - 1 && input[i + 1 + N * j] == 1) {\n//             num_neighbors++;\n//         }\n//         if (j > 0 && input[i + N * (j - 1)] == 1) {\n//             num_neighbors++;\n//         }\n//         if (j < N - 1 && input[i + N * (j + 1)] == 1) {\n//             num_neighbors++;\n//         }\n//         if (num_neighbors == 1) {\n//             output[i + N * j] = 1;\n//         } else {\n//             output[i + N * j] = 0;\n//         }\n//     }\n// }\n\n// Kokkos implementation of cellsXOR\n#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int num_neighbors = 0;\n        if (i > 0 && input[i - 1 + N * j] == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input[i + 1 + N * j] == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input[i + N * (j - 1)] == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input[i + N * (j + 1)] == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output[i + N * j] = 1;\n        } else {\n            output[i + N * j] = 0;\n        }\n    }\n}",
    "translation_function_name": "cellsXOR",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\n    Kokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n        for (size_t j = 0; j < N; j++) {\n            int num_neighbors = 0;\n            if (i > 0 && input(i - 1, j) == 1) {\n                num_neighbors++;\n            }\n            if (i < N - 1 && input(i + 1, j) == 1) {\n                num_neighbors++;\n            }\n            if (j > 0 && input(i, j - 1) == 1) {\n                num_neighbors++;\n            }\n            if (j < N - 1 && input(i, j + 1) == 1) {\n                num_neighbors++;\n            }\n            if (num_neighbors == 1) {\n                output(i, j) = 1;\n            } else {\n                output(i, j) = 0;\n            }\n        }\n    });\n}\n",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}",
      "\nKokkos::parallel_for(\"cellsXOR\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n        int num_neighbors = 0;\n        if (i > 0 && input(i - 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input(i + 1, j) == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input(i, j - 1) == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input(i, j + 1) == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output(i, j) = 1;\n        } else {\n            output(i, j) = 0;\n        }\n    });\n});\nKokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
    "translation_prompt": "// CUDA implementation of gameOfLife\n// /* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n//    A cell is 1 if it is alive and 0 if it is dead.\n//    If a live cell has fewer than 2 live neighbors then it dies.\n//    If a live cell has 2 or 3 live neighbors then it lives on.\n//    If a live cell has more than 3 live neighbords then it dies.\n//    If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input:  [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 0, 0, 0],\n//             [0, 1, 1, 0, 0],\n//             [0, 0, 1, 1, 0],\n//             [0, 1, 0, 0, 0]]\n//    output: [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 1, 0, 0],\n//             [0, 1, 0, 1, 0],\n//             [0, 0, 0, 1, 0],\n//             [0, 0, 1, 0, 0]]\n// */\n// __global__ void gameOfLife(const int *input, int *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//   int j = blockIdx.y * blockDim.y + threadIdx.y;\n// \n//   if (i < N && j < N) {\n//     int neighbors = 0;\n//     for (int x = -1; x <= 1; x++) {\n//       for (int y = -1; y <= 1; y++) {\n//         if (x == 0 && y == 0) {\n//           continue;\n//         }\n//         int neighbor_i = i + x;\n//         int neighbor_j = j + y;\n//         if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n//           continue;\n//         }\n//         neighbors += input[neighbor_i * N + neighbor_j];\n//       }\n//     }\n//     output[i * N + j] = input[i * N + j];\n//     if (input[i * N + j] == 1) {\n//       if (neighbors < 2 || neighbors > 3) {\n//         output[i * N + j] = 0;\n//       }\n//     } else {\n//       if (neighbors == 3) {\n//         output[i * N + j] = 1;\n//       }\n//     }\n//   }\n// }\n\n// Kokkos implementation of gameOfLife\n#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i < N && j < N) {\n    int neighbors = 0;\n    for (int x = -1; x <= 1; x++) {\n      for (int y = -1; y <= 1; y++) {\n        if (x == 0 && y == 0) {\n          continue;\n        }\n        int neighbor_i = i + x;\n        int neighbor_j = j + y;\n        if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n          continue;\n        }\n        neighbors += input[neighbor_i * N + neighbor_j];\n      }\n    }\n    output[i * N + j] = input[i * N + j];\n    if (input[i * N + j] == 1) {\n      if (neighbors < 2 || neighbors > 3) {\n        output[i * N + j] = 0;\n      }\n    } else {\n      if (neighbors == 3) {\n        output[i * N + j] = 1;\n      }\n    }\n  }\n}",
    "translation_function_name": "gameOfLife",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"game_of_life\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    });\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"gameOfLife\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int neighbors = 0;\n      for (int x = -1; x <= 1; x++) {\n        for (int y = -1; y <= 1; y++) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          int neighbor_i = i + x;\n          int neighbor_j = j + y;\n          if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n            continue;\n          }\n          neighbors += input(neighbor_i, neighbor_j);\n        }\n      }\n      output(i, j) = input(i, j);\n      if (input(i, j) == 1) {\n        if (neighbors < 2 || neighbors > 3) {\n          output(i, j) = 0;\n        }\n      } else {\n        if (neighbors == 3) {\n          output(i, j) = 1;\n        }\n      }\n    }\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {",
    "translation_prompt": "// CUDA implementation of jacobi1D\n// /* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n//    i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n//    Replace with 0 when reading past the boundaries of `input`.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [9, -6, -1, 2, 3]\n//    output: [1, 2/3, -5/3, 4/3, 5/3]\n// */\n// __global__ void jacobi1D(const double *input, double *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         if (i == 0) {\n//             output[i] = (input[i] + input[i + 1]) / 2;\n//         } else if (i == N - 1) {\n//             output[i] = (input[i - 1] + input[i]) / 2;\n//         } else {\n//             output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n//         }\n//     }\n// }\n\n// Kokkos implementation of jacobi1D\n#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
    "translation_function_name": "jacobi1D",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}\n",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA(const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n    Kokkos::fence();\n}",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n}\n",
      "\n    Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n        if (i == 0) {\n            output(i) = (input(i) + input(i + 1)) / 2;\n        } else if (i == input.extent(0) - 1) {\n            output(i) = (input(i - 1) + input(i)) / 2;\n        } else {\n            output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n        }\n    });\n}\n",
      "\n  Kokkos::parallel_for(input.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (i == 0) {\n          output(i) = (input(i) + input(i + 1)) / 2;\n      } else if (i == input.extent(0) - 1) {\n          output(i) = (input(i - 1) + input(i)) / 2;\n      } else {\n          output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n      }\n  });\n  Kokkos::fence();\n}"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {",
    "translation_prompt": "// CUDA implementation of convolveKernel\n// __constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n// \n// /* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n//    the dot product of itself and its neighbors with the edge kernel.\n//    Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n//    imageIn and imageOut are NxN grayscale images stored in row-major.\n//    Store the output of the computation in imageOut.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input: [[112, 118, 141, 152],\n//            [93, 101, 119, 203],\n//            [45, 17, 16, 232],\n//            [82, 31, 49, 101]]\n//    output: [[255, 255, 255, 255],\n//             [255, 147, 0, 255],\n//             [36, 0, 0, 255],\n//             [255, 39, 0, 255]]\n// */\n// __global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {\n// // TODO: implement the kernel\n// }\n\n// Kokkos implementation of convolveKernel\n#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {\n// TODO: implement the kernel\n}",
    "translation_function_name": "convolveKernel",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n}",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; ++j) {\n      int sum = 0;\n      for (int k = -1; k <= 1; ++k) {\n        for (int l = -1; l <= 1; ++l) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int x = i + k;\n          int y = j + l;\n          if (x >= 0 && x < N && y >= 0 && y < N) {\n            sum += imageIn(x, y) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolveKernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"convolve_kernel\", N, KOKKOS_LAMBDA(const int i) {\n    for (size_t j = 0; j < N; j++) {\n      int sum = 0;\n      for (int k = -1; k <= 1; k++) {\n        for (int l = -1; l <= 1; l++) {\n          int row = i + k;\n          int col = j + l;\n          if (row >= 0 && row < N && col >= 0 && col < N) {\n            sum += imageIn(row, col) * edgeKernel[k + 1][l + 1];\n          }\n        }\n      }\n      imageOut(i, j) = sum < 0 ? 0 : (sum > 255 ? 255 : sum);\n    }\n  });\n  Kokkos::fence();\n}\n"
    ]
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {",
    "translation_prompt": "// CUDA implementation of jacobi2D\n// /* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n//    i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n//    Replace with 0 when reading past the boundaries of `input`.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n//    output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n// */\n// __global__ void jacobi2D(const double *input, double *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     int j = blockIdx.y * blockDim.y + threadIdx.y;\n//     if (i >= N || j >= N) {\n//         return;\n//     }\n//     int idx = i + j * N;\n//     double sum = 0;\n//     if (i > 0) {\n//         sum += input[idx - 1];\n//     }\n//     if (i < N - 1) {\n//         sum += input[idx + 1];\n//     }\n//     if (j > 0) {\n//         sum += input[idx - N];\n//     }\n//     if (j < N - 1) {\n//         sum += input[idx + N];\n//     }\n//     sum += input[idx];\n//     sum /= 5;\n//     output[idx] = sum;\n// }\n\n// Kokkos implementation of jacobi2D\n#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) {\n        return;\n    }\n    int idx = i + j * N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input[idx - 1];\n    }\n    if (i < N - 1) {\n        sum += input[idx + 1];\n    }\n    if (j > 0) {\n        sum += input[idx - N];\n    }\n    if (j < N - 1) {\n        sum += input[idx + N];\n    }\n    sum += input[idx];\n    sum /= 5;\n    output[idx] = sum;\n}",
    "translation_function_name": "jacobi2D",
    "temperature": 0.2,
    "top_p": 0.95,
    "do_sample": true,
    "max_new_tokens": 1024,
    "outputs": [
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i, j);\n    }\n    if (i < N - 1) {\n        sum += input(i + 1, j);\n    }\n    if (j > 0) {\n        sum += input(i, j - 1);\n    }\n    if (j < N - 1) {\n        sum += input(i, j + 1);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i, j);\n    }\n    if (i < N - 1) {\n        sum += input(i + 1, j);\n    }\n    if (j > 0) {\n        sum += input(i, j - 1);\n    }\n    if (j < N - 1) {\n        sum += input(i, j + 1);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i - 1, j);\n    }\n    if (i < N - 1) {\n        sum += input(i + 1, j);\n    }\n    if (j > 0) {\n        sum += input(i, j - 1);\n    }\n    if (j < N - 1) {\n        sum += input(i, j + 1);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i, j);\n    }\n    if (i < N - 1) {\n        sum += input(i + 1, j);\n    }\n    if (j > 0) {\n        sum += input(i, j - 1);\n    }\n    if (j < N - 1) {\n        sum += input(i, j + 1);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 2, j);\n      }\n      if (j > 0) {\n          sum += input(i + 1, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i + 1, j + 1);\n      }\n      sum += input(i + 1, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i, j);\n    }\n    if (i < N - 1) {\n        sum += input(i + 1, j);\n    }\n    if (j > 0) {\n        sum += input(i, j - 1);\n    }\n    if (j < N - 1) {\n        sum += input(i, j + 1);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    if (i >= N || j >= N) {\n        return;\n    }\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i, j - 1);\n    }\n    if (i < N - 1) {\n        sum += input(i, j + 1);\n    }\n    if (j > 0) {\n        sum += input(i - 1, j);\n    }\n    if (j < N - 1) {\n        sum += input(i + 1, j);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 1, j);\n      }\n      if (j > 0) {\n          sum += input(i, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i, j + 1);\n      }\n      sum += input(i, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n      int i = idx % N;\n      int j = idx / N;\n      double sum = 0;\n      if (i > 0) {\n          sum += input(i - 1, j);\n      }\n      if (i < N - 1) {\n          sum += input(i + 1, j);\n      }\n      if (j > 0) {\n          sum += input(i, j - 1);\n      }\n      if (j < N - 1) {\n          sum += input(i, j + 1);\n      }\n      sum += input(i, j);\n      sum /= 5;\n      output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\n  Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int idx) {\n    int i = idx % N;\n    int j = idx / N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input(i, j);\n    }\n    if (i < N - 1) {\n        sum += input(i + 1, j);\n    }\n    if (j > 0) {\n        sum += input(i, j - 1);\n    }\n    if (j < N - 1) {\n        sum += input(i, j + 1);\n    }\n    sum += input(i, j);\n    sum /= 5;\n    output(i, j) = sum;\n  });\n  Kokkos::fence();\n}\n",
      "\nKokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < N; j++) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input(i - 1, j);\n        }\n        if (i < N - 1) {\n            sum += input(i + 1, j);\n        }\n        if (j > 0) {\n            sum += input(i, j - 1);\n        }\n        if (j < N - 1) {\n            sum += input(i, j + 1);\n        }\n        sum += input(i, j);\n        sum /= 5;\n        output(i, j) = sum;\n    }\n});\nKokkos::fence();\n}"
    ]
  }
]