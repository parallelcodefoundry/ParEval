[
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(Kokkos::View<const Point*> &points) {",
        "correct_output": "int num_points = points.extent(0);\n  double min_area = std::numeric_limits<double>::max();\n\n  Kokkos::parallel_reduce(\"smallestArea\", num_points, KOKKOS_LAMBDA(int i, double &area) {\n    for (int j = i + 1; j < num_points; ++j) {\n      for (int k = j + 1; k < num_points; ++k) {\n        double tri_area = triangleArea(points(i), points(j), points(k));\n        if (tri_area < area) {\n          area = tri_area;\n        }\n      }\n    }\n  }, Kokkos::Min<double>(min_area));\n\n  return min_area;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
        "correct_output": "double minArea = std::numeric_limits<double>::max();\n  #pragma omp parallel for reduction(min:minArea)\n  for (int i = 0; i < points.size(); i++) {\n    for (int j = i+1; j < points.size(); j++) {\n      for (int k = j+1; k < points.size(); k++) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        minArea = std::min(minArea, area);\n      }\n    }\n  }\n  return minArea;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
        "correct_output": "double minArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * fabs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Compute the area of the smallest triangle that can be formed by any 3 points.\n   Return the result in area.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\n__global__ void smallestArea(const Point *points, size_t N, double *area) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
        "correct_output": "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = points.size();\n  double local_area, global_area = 1e100; // start with a large number\n\n  if (rank == 0) {\n    for (int i = 0; i < n; ++i) {\n      for (int j = i + 1; j < n; ++j) {\n        for (int k = j + 1; k < n; ++k) {\n          local_area = triangleArea(points[i], points[j], points[k]);\n          if (local_area < global_area) {\n            global_area = local_area;\n          }\n        }\n      }\n    }\n  }\n\n  MPI_Bcast(&global_area, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  return global_area;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * fabs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Compute the area of the smallest triangle that can be formed by any 3 points.\n   Return the result in area.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\n__global__ void smallestArea(const Point *points, size_t N, double *area) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "12_geometry_smallest_triangle",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
        "correct_output": "double minArea = std::numeric_limits<double>::max();\n\n  #pragma omp parallel for reduction(min: minArea)\n  for (int i = 0; i < points.size(); i++) {\n    for (int j = i+1; j < points.size(); j++) {\n      for (int k = j+1; k < points.size(); k++) {\n        minArea = std::min(minArea, triangleArea(points[i], points[j], points[k]));\n      }\n    }\n  }\n\n  return minArea;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(Kokkos::View<const double*> &x) {",
        "correct_output": "double min_distance = std::numeric_limits<double>::max();\n\tint min_i = 0;\n\tint min_j = 0;\n\n\tint n = x.extent(0);\n\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tdouble dist = distance(x(i), x(j));\n\t\t\tif (dist < min_distance) {\n\t\t\t\tmin_distance = dist;\n\t\t\t\tmin_i = i;\n\t\t\t\tmin_j = j;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn min_distance;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
        "correct_output": "double min_dist = std::numeric_limits<double>::max();\n\tint n = x.size();\n\t#pragma omp parallel for reduction(min:min_dist)\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "serial",
        "prompt": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
        "correct_output": "double min_dist = std::numeric_limits<double>::max();\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "hip",
        "prompt": "__device__ double distanceBetweenPoints(double x1, double x2) {\n\treturn fabs(x1 - x2);\n}\n\n/* Compute the distance between the closest two elements in the vector x.\n   Store the result in distance.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\n__global__ void closestPair(const double *x, size_t N, double *distance) {",
        "correct_output": "// Initialize the minimum distance as the maximum possible double value\n    __shared__ double min_distance;\n    if (threadIdx.x == 0) {\n        min_distance = DBL_MAX;\n    }\n    __syncthreads();\n\n    // Each thread computes the distance from its element to all other elements\n    double my_distance = DBL_MAX;\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        for (size_t j = i + 1; j < N; ++j) {\n            double dist = distanceBetweenPoints(x[i], x[j]);\n            if (dist < my_distance) {\n                my_distance = dist;\n            }\n        }\n    }\n\n    // Atomically update the minimum distance if necessary\n    atomicMin(&min_distance, my_distance);\n    __syncthreads();\n\n    // Save the result\n    if (threadIdx.x == 0) {\n        *distance = min_distance;\n    }\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
        "correct_output": "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    double local_min = std::numeric_limits<double>::max();\n\n    if (rank == 0) {\n        for (int i = 0; i < n; ++i) {\n            for (int j = i + 1; j < n; ++j) {\n                local_min = std::min(local_min, distance(x[i], x[j]));\n            }\n        }\n    }\n\n    double global_min = local_min;\n    MPI_Allreduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n\n    return global_min;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "cuda",
        "prompt": "__device__ double distanceBetweenPoints(double x1, double x2) {\n\treturn fabs(x1 - x2);\n}\n\n/* Compute the distance between the closest two elements in the vector x.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\n__global__ void closestPair(const double *x, size_t N, double *distance) {",
        "correct_output": "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distanceBetweenPoints(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "14_geometry_closest_pair_1d",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
        "correct_output": "int n = x.size();\n\tif (n < 2) {\n\t\treturn 0;\n\t}\n\n\t// find the minimum and maximum values in the vector\n\tdouble min = x[0];\n\tdouble max = x[0];\n\tfor (int i = 1; i < n; i++) {\n\t\tif (x[i] < min) {\n\t\t\tmin = x[i];\n\t\t}\n\t\tif (x[i] > max) {\n\t\t\tmax = x[i];\n\t\t}\n\t}\n\n\t// compute the distance between the minimum and maximum values\n\tdouble min_max_dist = distance(min, max);\n\n\t// compute the distance between the closest two elements in the vector\n\tdouble closest_dist = min_max_dist;\n\tfor (int i = 0; i < n - 1; i++) {\n\t\tfor (int j = i + 1; j < n; j++) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < closest_dist) {\n\t\t\t\tclosest_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn closest_dist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(Kokkos::View<const Point*> &points) {",
        "correct_output": "double min_distance = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.extent(0); i++) {\n\t\tfor (int j = i+1; j < points.extent(0); j++) {\n\t\t\tdouble dist = distance(points(i), points(j));\n\t\t\tif (dist < min_distance) {\n\t\t\t\tmin_distance = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_distance;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "correct_output": "double min_dist = std::numeric_limits<double>::max();\n\tint n = points.size();\n\n\t#pragma omp parallel for reduction(min:min_dist)\n\tfor (int i = 0; i < n; i++) {\n\t\tfor (int j = i+1; j < n; j++) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\n\treturn min_dist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "correct_output": "if (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distanceBetweenPoints(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the distance between the closest two points in the vector points.\n   Store the result in distance.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\n__global__ void closestPair(const Point *points, size_t numPoints, double *distance) {",
        "correct_output": "// Initialize minDistance to a large value\n\t__shared__ double minDistance;\n\tif (threadIdx.x == 0) {\n\t\tminDistance = 1e10;\n\t}\n\t__syncthreads();\n\n\t// Each thread computes the distance between its point and all other points\n\tsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < numPoints) {\n\t\tfor (size_t j = i + 1; j < numPoints; ++j) {\n\t\t\tdouble dist = distanceBetweenPoints(points[i], points[j]);\n\t\t\tatomicMin(&minDistance, dist);\n\t\t}\n\t}\n\t__syncthreads();\n\n\t// Store the minimum distance in the output array\n\tif (threadIdx.x == 0) {\n\t\t*distance = minDistance;\n\t}\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "correct_output": "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint n = points.size();\n\tdouble local_min = std::numeric_limits<double>::max();\n\n\tfor (int i = rank; i < n; i += size) {\n\t\tfor (int j = i + 1; j < n; ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < local_min) {\n\t\t\t\tlocal_min = dist;\n\t\t\t}\n\t\t}\n\t}\n\n\tdouble global_min = 0.0;\n\tMPI_Reduce(&local_min, &global_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n\treturn global_min;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distanceBetweenPoints(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the distance between the closest two points in the vector points.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\n__global__ void closestPair(const Point *points, size_t numPoints, double *distance) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "13_geometry_closest_pair_2d",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
        "correct_output": "int n = points.size();\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tint min_i = 0, min_j = 0;\n\t#pragma omp parallel for reduction(min: min_dist)\n\tfor (int i = 0; i < n; ++i) {\n\t\tfor (int j = i+1; j < n; ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t\tmin_i = i;\n\t\t\t\tmin_j = j;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(Kokkos::View<const Point*> &points, Kokkos::View<Point*> &hull) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\n__global__ void convexHull(const Point *points, size_t numPoints, Point *hull, size_t hullSize) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\n__global__ void convexHull(const Point *points, size_t numPoints, Point *hull, size_t hullSize) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "10_geometry_convex_hull",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Find the set of points that defined the smallest convex polygon that contains all the points in the vector points. Store the result in `hull`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of points. The final hull is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: [{0, 3}, {4, 4}, {3, 1}, {0, 0}]\n*/\nvoid convexHull(std::vector<Point> const& points, std::vector<Point> &hull) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(Kokkos::View<const Point*> &points) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Store the result in perimeter.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\n__global__ void convexHullPerimeter(const Point *points, size_t numPoints, double *perimeter) {",
        "correct_output": "\n\t// Gift wrapping algorithm to find convex hull\n\tint leftmost = 0;\n\tfor (int i = 1; i < numPoints; ++i) {\n\t\tif (points[i].x < points[leftmost].x) {\n\t\t\tleftmost = i;\n\t\t}\n\t}\n\n\tint current = leftmost;\n\tint next;\n\tdouble totalPerimeter = 0;\n\n\tdo {\n\t\tnext = (current + 1) % numPoints;\n\t\tfor (int i = 0; i < numPoints; ++i) {\n\t\t\tif (i != current && i != next) {\n\t\t\t\tdouble crossProduct = (points[next].x - points[current].x) * (points[i].y - points[current].y) - (points[next].y - points[current].y) * (points[i].x - points[current].x);\n\t\t\t\tif (crossProduct > 0) {\n\t\t\t\t\tnext = i;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\ttotalPerimeter += distance(points[current], points[next]);\n\t\tcurrent = next;\n\t} while (current != leftmost);\n\n\t*perimeter = totalPerimeter;\n}\n"
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n\tdouble x, y;\n};\n\n__device__ double distance(Point const& p1, Point const& p2) {\n\treturn sqrt(pow(p2.x-p1.x, 2) + pow(p2.y-p1.y, 2));\n}\n\n/* Compute the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Store the result in perimeter.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as points.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\n__global__ void convexHullPerimeter(const Point *points, size_t numPoints, double *perimeter) {",
        "correct_output": null
    },
    {
        "problem_type": "geometry",
        "language": "cpp",
        "name": "11_geometry_convex_hull_perimeter",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the perimeter of the smallest convex polygon that contains all the points in the vector points.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example:\n\n   input: [{0, 3}, {1, 1}, {2, 2}, {4, 4}, {0, 0}, {1, 2}, {3, 1}, {3, 3}]\n   output: 13.4477\n*/\ndouble convexHullPerimeter(std::vector<Point> const& points) {",
        "correct_output": null
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(Kokkos::View<const int**> &A, size_t N) {",
        "correct_output": "\n    Kokkos::View<int*> visited(\"visited\", N);\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        visited(i) = 0;\n    });\n    Kokkos::fence();\n\n    int maxComponentSize = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (visited(i) == 0) {\n            int componentSize = 1;\n            visited(i) = 1;\n            Kokkos::View<int*> queue(\"queue\", N);\n            int front = 0, rear = 0;\n            queue(rear++) = i;\n            while (front != rear) {\n                int u = queue(front++);\n                for (size_t v = 0; v < N; ++v) {\n                    if (A(u, v) && visited(v) == 0) {\n                        visited(v) = 1;\n                        queue(rear++) = v;\n                        componentSize++;\n                    }\n                }\n            }\n            maxComponentSize = std::max(maxComponentSize, componentSize);\n        }\n    }\n    return maxComponentSize;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "correct_output": "std::vector<bool> visited(N, false);\n  int max_component = 0;\n  for (int i = 0; i < N; ++i) {\n    if (!visited[i]) {\n      int component = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        ++component;\n        for (int j = 0; j < N; ++j) {\n          if (A[v * N + j] &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      max_component = std::max(max_component, component);\n    }\n  }\n  return max_component;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "serial",
        "prompt": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "correct_output": "std::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "hip",
        "prompt": "/* Compute the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   Store the result in largestComponentSize.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void largestComponent(const int *A, size_t N, int *largestComponentSize) {",
        "correct_output": "int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row >= N || col >= N) {\n        return;\n    }\n    if (A[row * N + col] == 1) {\n        atomicMax(largestComponentSize, 1 + max(row, col));\n    }\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "correct_output": "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Each process will compute the size of the largest component for a subset of nodes\nint local_max = 0;\nstd::vector<bool> visited(N, false);\n\nauto dfs = [&](auto&& self, int node) -> int {\n    visited[node] = true;\n    int size = 1;\n    for (size_t i = 0; i < N; ++i) {\n        if (A[node * N + i] && !visited[i]) {\n            size += self(self, i);\n        }\n    }\n    return size;\n};\n\n// Start DFS from each node that hasn't been visited yet\nfor (size_t i = rank; i < N; i += size) {\n    if (!visited[i]) {\n        int component_size = dfs(dfs, i);\n        local_max = std::max(local_max, component_size);\n    }\n}\n\n// Reduce all local maxima to find the global maximum component size\nint global_max;\nMPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n// Only rank 0 will have the correct global_max\nif (rank == 0) {\n    return global_max;\n} else {\n    return 0; // Other ranks do not need to return the correct value\n}\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   Store the result in largestComponentSize.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void largestComponent(const int *A, size_t N, int *largestComponentSize) {",
        "correct_output": null
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "16_graph_largest_component",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = N / size;\n    std::vector<int> local_A(local_size * N);\n\n    // Scatter the matrix to all ranks\n    MPI_Scatter(A.data(), local_size * N, MPI_INT, local_A.data(), local_size * N, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Compute the connected components in parallel\n    std::vector<int> local_components(local_size);\n    #pragma omp parallel for\n    for (int i = 0; i < local_size; ++i) {\n        int row_begin = i * N;\n        for (int j = 0; j < N; ++j) {\n            if (local_A[row_begin + j] == 1) {\n                local_components[i] = j / local_size;\n                break;\n            }\n        }\n    }\n\n    // Gather all local_components to rank 0\n    std::vector<int> all_components(size * local_size);\n    MPI_Gather(local_components.data(), local_size, MPI_INT, all_components.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // On rank 0, find the largest connected component\n    int max_component = 0;\n    if (rank == 0) {\n        std::map<int, int> component_count;\n        for (int i = 0; i < size * local_size; ++i) {\n            component_count[all_components[i]]++;\n        }\n        for (auto const& kv : component_count) {\n            max_component = std::max(max_component, kv.second);\n        }\n    }\n\n    // Broadcast the result to all ranks\n    MPI_Bcast(&max_component, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    return max_component;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n\t Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(Kokkos::View<const int**> &A, size_t N) {",
        "correct_output": "Kokkos::View<int*, Kokkos::HostSpace> labels(\"labels\", N);\n  Kokkos::View<int*, Kokkos::HostSpace> counts(\"counts\", N);\n  Kokkos::View<int*, Kokkos::HostSpace> queue(\"queue\", N);\n\n  Kokkos::deep_copy(labels, 0);\n  Kokkos::deep_copy(counts, 0);\n  Kokkos::deep_copy(queue, 0);\n\n  int num_components = 0;\n  int num_unvisited = N;\n\n  for (int i = 0; i < N; i++) {\n    if (labels(i) == 0) {\n      num_components++;\n      Kokkos::deep_copy(queue, i);\n      while (num_unvisited > 0) {\n        int node = queue(0);\n        Kokkos::deep_copy(labels, node);\n        Kokkos::deep_copy(counts, 1);\n        num_unvisited--;\n        for (int j = 0; j < N; j++) {\n          if (A(node, j) == 1 && labels(j) == 0) {\n            Kokkos::deep_copy(queue, j);\n            num_unvisited--;\n            break;\n          }\n        }\n        for (int j = 0; j < N; j++) {\n          if (A(node, j) == 1 && labels(j) == 0) {\n            Kokkos::deep_copy(queue, j);\n            num_unvisited--;\n            break;\n          }\n        }\n      }\n    }\n  }\n\n  return num_components;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (!visited[i]) {\n\t\t\tcount++;\n\t\t\tstd::queue<int> q;\n\t\t\tq.push(i);\n\t\t\twhile (!q.empty()) {\n\t\t\t\tint v = q.front();\n\t\t\t\tq.pop();\n\t\t\t\tif (!visited[v]) {\n\t\t\t\t\tvisited[v] = 1;\n\t\t\t\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\t\t\t\tif (A[v * N + j] &&!visited[j]) {\n\t\t\t\t\t\t\tq.push(j);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "std::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] &&!visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void componentCount(const int *A, size_t N, int *numComponents) {",
        "correct_output": null
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int count = 0;\n\tstd::vector<bool> visited(N, false);\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (!visited[i]) {\n\t\t\t++count;\n\t\t\tstd::vector<int> stack = {i};\n\t\t\twhile (!stack.empty()) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tif (!visited[v]) {\n\t\t\t\t\tvisited[v] = true;\n\t\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\t\tif (A[v * N + j] == 1) {\n\t\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\n__global__ void componentCount(const int *A, size_t N, int *numComponents) {",
        "correct_output": null
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "18_graph_count_components",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int count = 0;\n\tstd::vector<int> visited(N, 0);\n\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (visited[i] == 0) {\n\t\t\t++count;\n\t\t\tstd::vector<int> stack;\n\t\t\tstack.push_back(i);\n\t\t\tvisited[i] = 1;\n\t\t\twhile (stack.size() > 0) {\n\t\t\t\tint v = stack.back();\n\t\t\t\tstack.pop_back();\n\t\t\t\tfor (size_t j = 0; j < N; ++j) {\n\t\t\t\t\tif (A[v * N + j] == 1 && visited[j] == 0) {\n\t\t\t\t\t\tstack.push_back(j);\n\t\t\t\t\t\tvisited[j] = 1;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\n\treturn count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A represents a directed graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {",
        "correct_output": "int count = 0;\n  Kokkos::parallel_reduce(\n      \"edgeCount\",\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n      KOKKOS_LAMBDA(const int i, int &update) {\n        for (int j = 0; j < N; j++) {\n          if (A(i, j) == 1) {\n            update++;\n          }\n        }\n      },\n      Kokkos::Sum<int>(count));\n  return count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int count = 0;\n  #pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i*N+j] == 1) {\n        count++;\n      }\n    }\n  }\n  return count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   Store the result in numEdges. A represents a directed graph.\n   A is an NxN adjacency matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {",
        "correct_output": "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numEdgesPerBlock = N / gridDim.x;\n\tint start = bid * numEdgesPerBlock;\n\tint end = min(start + numEdgesPerBlock, N);\n\tint count = 0;\n\tfor (int i = start + tid; i < end; i += blockDim.x) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\tatomicAdd(numEdges, count);\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int numEdges = 0;\n  for (int i = 0; i < N; i++) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        numEdges++;\n      }\n    }\n  }\n  return numEdges;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   Store the result in numEdges. A represents a directed graph.\n   A is an NxN adjacency matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {",
        "correct_output": "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numThreads = blockDim.x;\n\tint numBlocks = gridDim.x;\n\tint start = bid * numThreads + tid;\n\tint end = (bid + 1) * numThreads;\n\tint count = 0;\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\tif (tid == 0) {\n\t\tatomicAdd(numEdges, count);\n\t}\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "15_graph_edge_count",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
        "correct_output": "int numEdges = 0;\n  #pragma omp parallel for reduction(+:numEdges)\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        numEdges++;\n      }\n    }\n  }\n  return numEdges;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {",
        "correct_output": "Kokkos::View<int*, Kokkos::HostSpace> degrees(\"degrees\", N);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      degree += A(i, j);\n    }\n    degrees(i) = degree;\n  });\n  Kokkos::fence();\n  int max_degree = 0;\n  for (int i = 0; i < N; i++) {\n    max_degree = std::max(max_degree, degrees(i));\n  }\n  return max_degree;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "correct_output": "int max_degree = 0;\n  #pragma omp parallel for reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i * N + j];\n    }\n    max_degree = std::max(max_degree, degree);\n  }\n  return max_degree;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "serial",
        "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "correct_output": "int max_degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "hip",
        "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in maxDegree.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {",
        "correct_output": "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int stride = blockDim.x;\n  int max = 0;\n  for (int i = bid * stride + tid; i < N; i += stride * gridDim.x) {\n    int degree = 0;\n    for (int j = 0; j < N; j++) {\n      degree += A[i * N + j];\n    }\n    max = max > degree? max : degree;\n  }\n  __syncthreads();\n  atomicMax(maxDegree, max);\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "correct_output": "int local_max = 0;\n  for (size_t i = 0; i < N; i++) {\n    int local_degree = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (A[i * N + j] == 1) {\n        local_degree++;\n      }\n    }\n    local_max = std::max(local_max, local_degree);\n  }\n  int global_max = 0;\n  MPI_Reduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n  return global_max;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph. \n   Store the result in maxDegree.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {",
        "correct_output": "int tid = threadIdx.x;\n\tint max = 0;\n\tfor (int i = tid; i < N; i += blockDim.x) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\t__syncthreads();\n\tatomicMax(maxDegree, max);\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "17_graph_highest_degree",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
        "correct_output": "int max_degree = 0;\n  #pragma omp parallel for reduction(max:max_degree)\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      degree += A[i*N + j];\n    }\n    max_degree = std::max(max_degree, degree);\n  }\n  return max_degree;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {",
        "correct_output": "Kokkos::View<int*, Kokkos::HostSpace> dist(\"dist\", N);\n  Kokkos::View<int*, Kokkos::HostSpace> pred(\"pred\", N);\n  Kokkos::View<int*, Kokkos::HostSpace> queue(\"queue\", N);\n  Kokkos::View<int*, Kokkos::HostSpace> queue_size(\"queue_size\", 1);\n  Kokkos::View<int*, Kokkos::HostSpace> queue_head(\"queue_head\", 1);\n  Kokkos::View<int*, Kokkos::HostSpace> queue_tail(\"queue_tail\", 1);\n\n  // initialize dist, pred, queue, queue_size, queue_head, queue_tail\n  for (size_t i = 0; i < N; i++) {\n    dist(i) = INT_MAX;\n    pred(i) = -1;\n    queue(i) = i;\n    queue_size(0) = 0;\n    queue_head(0) = 0;\n    queue_tail(0) = 0;\n  }\n\n  // initialize dist, pred, queue, queue_size, queue_head, queue_tail\n  dist(source) = 0;\n  pred(source) = source;\n  queue(queue_tail(0)) = source;\n  queue_tail(0) = (queue_tail(0) + 1) % N;\n  queue_size(0) = 1;\n\n  // BFS\n  while (queue_size(0) > 0) {\n    int u = queue(queue_head(0));\n    queue_head(0) = (queue_head(0) + 1) % N;\n    queue_size(0) = queue_size(0) - 1;\n\n    for (size_t i = 0; i < N; i++) {\n      if (A(u, i) == 1 && dist(i) == INT_MAX) {\n        dist(i) = dist(u) + 1;\n        pred(i) = u;\n        queue(queue_tail(0)) = i;\n        queue_tail(0) = (queue_tail(0) + 1) % N;\n        queue_size(0) = queue_size(0) + 1;\n      }\n    }\n  }\n\n  // return dist(dest)\n  return dist(dest);\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "correct_output": "std::vector<int> dist(N, INT_MAX);\n  dist[source] = 0;\n\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    for (int j = 0; j < N; j++) {\n      if (A[i * N + j] && dist[j]!= INT_MAX) {\n        dist[i] = std::min(dist[i], dist[j] + 1);\n      }\n    }\n  }\n\n  return dist[dest];\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "serial",
        "prompt": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "correct_output": "std::vector<int> dist(N, INT_MAX);\n\tstd::vector<int> prev(N, -1);\n\tstd::queue<int> q;\n\tq.push(source);\n\tdist[source] = 0;\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tprev[v] = u;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == INT_MAX) {\n\t\treturn -1;\n\t}\n\tint path_len = 0;\n\twhile (prev[dest]!= -1) {\n\t\tpath_len++;\n\t\tdest = prev[dest];\n\t}\n\treturn path_len;\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "hip",
        "prompt": "/* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in pathLength.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint j = blockIdx.y * blockDim.y + threadIdx.y;\n\tif (i == j) {\n\t\tif (i == source) {\n\t\t\tpathLength[i] = 0;\n\t\t} else {\n\t\t\tpathLength[i] = INT_MAX;\n\t\t}\n\t} else if (A[i * N + j] == 1) {\n\t\tpathLength[i] = 1;\n\t}\n\t__syncthreads();\n\tfor (int k = 0; k < N; k++) {\n\t\tif (pathLength[i]!= INT_MAX && pathLength[k]!= INT_MAX && pathLength[k] + pathLength[i] < pathLength[j]) {\n\t\t\tpathLength[j] = pathLength[k] + pathLength[i];\n\t\t}\n\t\t__syncthreads();\n\t}\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // initialize the distance array\n  std::vector<int> dist(N, INT_MAX);\n  dist[source] = 0;\n\n  // initialize the queue\n  std::queue<int> q;\n  q.push(source);\n\n  // initialize the visited array\n  std::vector<bool> visited(N, false);\n  visited[source] = true;\n\n  // initialize the parent array\n  std::vector<int> parent(N, -1);\n  parent[source] = source;\n\n  // loop until the queue is empty\n  while (!q.empty()) {\n    // get the next node\n    int u = q.front();\n    q.pop();\n\n    // loop over the neighbors of u\n    for (int v = 0; v < N; v++) {\n      // if the neighbor is not visited and the edge is not blocked\n      if (!visited[v] && A[u * N + v] == 1) {\n        // update the distance and parent\n        dist[v] = dist[u] + 1;\n        parent[v] = u;\n\n        // add the neighbor to the queue\n        q.push(v);\n\n        // mark the neighbor as visited\n        visited[v] = true;\n      }\n    }\n  }\n\n  // return the distance to the destination\n  return dist[dest];\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in pathLength.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {",
        "correct_output": "int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        if (row == col) {\n            pathLength[row] = 0;\n        } else if (A[row * N + col] == 1) {\n            pathLength[row] = 1;\n        } else {\n            pathLength[row] = INT_MAX;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < N; ++k) {\n            if (pathLength[row] + pathLength[k] < pathLength[col]) {\n                pathLength[col] = pathLength[row] + pathLength[k];\n            }\n            __syncthreads();\n        }\n    }\n\n    if (row == dest) {\n        *pathLength = pathLength[row];\n    }\n}"
    },
    {
        "problem_type": "graph",
        "language": "cpp",
        "name": "19_graph_shortest_path",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
        "correct_output": "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// initialize the distance matrix\n\tstd::vector<int> dist(N, INT_MAX);\n\tdist[source] = 0;\n\n\t// initialize the queue\n\tstd::vector<int> queue;\n\tqueue.push_back(source);\n\n\t// initialize the visited matrix\n\tstd::vector<bool> visited(N, false);\n\tvisited[source] = true;\n\n\t// initialize the parent matrix\n\tstd::vector<int> parent(N, -1);\n\tparent[source] = source;\n\n\t// initialize the number of visited nodes\n\tint num_visited = 1;\n\n\t// loop until the queue is empty\n\twhile (!queue.empty()) {\n\t\t// get the next node\n\t\tint node = queue.front();\n\t\tqueue.erase(queue.begin());\n\n\t\t// loop over the neighbors\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\t// skip the node itself\n\t\t\tif (i == node) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t// skip the node if it has already been visited\n\t\t\tif (visited[i]) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t// skip the node if it is not connected to the current node\n\t\t\tif (A[node * N + i] == 0) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\t// update the distance of the node\n\t\t\tint new_dist = dist[node] + 1;\n\t\t\tif (new_dist < dist[i]) {\n\t\t\t\tdist[i] = new_dist;\n\t\t\t\tparent[i] = node;\n\t\t\t}\n\n\t\t\t// add the node to the queue\n\t\t\tqueue.push_back(i);\n\t\t\tvisited[i] = true;\n\t\t\tnum_visited++;\n\t\t}\n\t}\n\n\t// return the distance of the destination node\n\treturn dist[dest];\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {",
        "correct_output": "int min_odd = 0;\n  Kokkos::parallel_reduce(\n      \"smallest_odd\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n      KOKKOS_LAMBDA(int i, int& min_odd) {\n        if (x(i) % 2 == 1 && (x(i) < min_odd || min_odd == 0)) {\n          min_odd = x(i);\n        }\n      },\n      Kokkos::Min<int>(min_odd));\n  return min_odd;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "correct_output": "int smallest = x[0];\n  #pragma omp parallel for reduction(min:smallest)\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "serial",
        "prompt": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "correct_output": "int smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "hip",
        "prompt": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use AMD HIP to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {",
        "correct_output": "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    if (i < N) {\n        if (x[i] % 2 == 1) {\n            atomicMin(smallest, x[i]);\n        }\n    }\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "correct_output": "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n\n  int local_min = std::numeric_limits<int>::max();\n  int local_min_index = -1;\n\n  for (int i = rank * chunk; i < (rank + 1) * chunk; i++) {\n    if (x[i] % 2 == 1 && x[i] < local_min) {\n      local_min = x[i];\n      local_min_index = i;\n    }\n  }\n\n  int global_min = std::numeric_limits<int>::max();\n  int global_min_index = -1;\n\n  MPI_Reduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&local_min_index, &global_min_index, 1, MPI_INT, MPI_MIN, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    if (remainder > 0) {\n      for (int i = (size - remainder) * chunk; i < size * chunk; i++) {\n        if (x[i] % 2 == 1 && x[i] < global_min) {\n          global_min = x[i];\n          global_min_index = i;\n        }\n      }\n    }\n  }\n\n  return global_min;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "cuda",
        "prompt": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {",
        "correct_output": "int thread_id = threadIdx.x;\n    int block_id = blockIdx.x;\n    int stride = blockDim.x;\n\n    int local_smallest = x[block_id * stride + thread_id];\n    for (int i = 1; i < stride; i++) {\n        int value = x[block_id * stride + thread_id + i];\n        if (value % 2 == 1 && value < local_smallest) {\n            local_smallest = value;\n        }\n    }\n    if (local_smallest % 2 == 1) {\n        atomicMin(smallest, local_smallest);\n    }\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "28_reduce_smallest_odd_number",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
        "correct_output": "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_min = 1000000000;\n  int global_min = 1000000000;\n\n  #pragma omp parallel for reduction(min:local_min)\n  for (int i = 0; i < n; i++) {\n    if (x[i] % 2 == 1 && x[i] < local_min) {\n      local_min = x[i];\n    }\n  }\n\n  MPI_Reduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return global_min;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use Kokkos to compute product in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(Kokkos::View<const double*> const& x) {",
        "correct_output": "// TODO: Implement this function\n  double product = 1;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& prod) {\n    if (i % 2 == 0) {\n      prod *= x(i);\n    } else {\n      prod *= 1 / x(i);\n    }\n  }, Kokkos::Prod<double>(product));\n  return product;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "correct_output": "double result = 1;\n  #pragma omp parallel for reduction(*:result)\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 1) {\n      result *= 1 / x[i];\n    } else {\n      result *= x[i];\n    }\n  }\n  return result;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "serial",
        "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "correct_output": "double result = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "hip",
        "prompt": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Store the result in product.\n   Use AMD HIP to compute product in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\n__global__ void productWithInverses(const double *x, size_t N, double *product) {",
        "correct_output": "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double value = x[idx];\n        if (idx % 2 == 1) {\n            value = 1.0 / value;\n        }\n        atomicMul(product, value);\n    }\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int n_local = n / size;\n  int n_remainder = n % size;\n\n  int start = rank * n_local;\n  int end = start + n_local;\n\n  if (rank < n_remainder) {\n    end++;\n  }\n\n  double local_product = 1.0;\n  for (int i = start; i < end; i++) {\n    if (i % 2 == 0) {\n      local_product *= x[i];\n    } else {\n      local_product *= 1.0 / x[i];\n    }\n  }\n\n  double global_product;\n  MPI_Reduce(&local_product, &global_product, 1, MPI_DOUBLE, MPI_PROD, 0,\n             MPI_COMM_WORLD);\n\n  return global_product;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Store the result in product.\n   Use CUDA to compute product in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\n__global__ void productWithInverses(const double *x, size_t N, double *product) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "26_reduce_product_of_inverses",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI and OpenMP to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  double product = 1;\n  double local_product = 1;\n\n  // compute the local product\n  for (int i = 0; i < n; i++) {\n    local_product *= x[i];\n  }\n\n  // compute the global product\n  MPI_Allreduce(&local_product, &product, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  // compute the product with inverses\n  double inverse_product = 1;\n  for (int i = 0; i < n; i++) {\n    if (i % 2 == 1) {\n      inverse_product *= 1 / x[i];\n    }\n  }\n\n  // compute the global product with inverses\n  double global_inverse_product = 1;\n  MPI_Allreduce(&inverse_product, &global_inverse_product, 1, MPI_DOUBLE, MPI_PROD, MPI_COMM_WORLD);\n\n  // multiply the global product with inverses with the global product\n  product *= global_inverse_product;\n\n  return product;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use Kokkos to sum in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(Kokkos::View<const double*> const& x, Kokkos::View<const double*> const& y) {",
        "correct_output": "double sum = 0;\n  Kokkos::parallel_reduce(\n      \"sumOfMinimumElements\",\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i, double& lsum) {\n        lsum += std::min(x(i), y(i));\n      },\n      sum);\n  return sum;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "correct_output": "double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < x.size(); ++i) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "serial",
        "prompt": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "correct_output": "double sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "hip",
        "prompt": "/* Compute the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Store the result in sum.\n   Use AMD HIP to sum in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\n__global__ void sumOfMinimumElements(const double *x, const double *y, size_t N, double *sum) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // calculate the number of elements to be sent to each process\n  int numElements = x.size();\n  int numElementsPerRank = numElements / size;\n  int numElementsRemaining = numElements % size;\n\n  // calculate the start and end indices of the elements to be sent to each process\n  int start = rank * numElementsPerRank;\n  int end = start + numElementsPerRank;\n  if (rank < numElementsRemaining) {\n    end++;\n  }\n\n  // send the elements to each process\n  std::vector<double> x_local(numElementsPerRank);\n  std::vector<double> y_local(numElementsPerRank);\n  MPI_Scatter(&x[0], numElementsPerRank, MPI_DOUBLE, &x_local[0], numElementsPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(&y[0], numElementsPerRank, MPI_DOUBLE, &y_local[0], numElementsPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // calculate the sum of the minimum elements\n  double sum = 0;\n  for (int i = 0; i < numElementsPerRank; i++) {\n    sum += std::min(x_local[i], y_local[i]);\n  }\n\n  // sum the sums\n  double sum_local = sum;\n  MPI_Reduce(&sum_local, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return sum;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Store the result in sum.\n   Use CUDA to sum in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\n__global__ void sumOfMinimumElements(const double *x, const double *y, size_t N, double *sum) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "29_reduce_sum_of_min_of_pairs",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI and OpenMP to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
        "correct_output": "int n = x.size();\n    double sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 0; i < n; i++) {\n        sum += std::min(x[i], y[i]);\n    }\n    return sum;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {",
        "correct_output": "// TODO: implement this function\n  double sum = 0;\n  Kokkos::parallel_reduce(x.extent(0), KOKKOS_LAMBDA(const int i, double& lsum) {\n    lsum += x(i);\n  }, sum);\n  return sum / x.extent(0);\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "correct_output": "double sum = 0.0;\n\tint n = x.size();\n\t#pragma omp parallel for reduction(+:sum)\n\tfor (int i = 0; i < n; i++) {\n\t\tsum += x[i];\n\t}\n\treturn sum / n;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "serial",
        "prompt": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "correct_output": "double sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "hip",
        "prompt": "/* Compute the average of the vector x. Store the result in average.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "correct_output": "int n = x.size();\n  double sum = 0;\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n  }\n  return sum / n;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the average of the vector x. Store the result in average.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {",
        "correct_output": "\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "27_reduce_average",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
        "correct_output": "int n = x.size();\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n; i++) {\n    sum += x[i];\n  }\n  double avg = sum / n;\n  return avg;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use Kokkos to reduce in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(Kokkos::View<const bool*> const& x) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "correct_output": "bool result = false;\n#pragma omp parallel for reduction(^:result)\n  for (int i = 0; i < x.size(); i++) {\n    result ^= x[i];\n  }\n  return result;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "serial",
        "prompt": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "correct_output": "bool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "hip",
        "prompt": "/* Compute the logical XOR reduction of the vector of bools x. Store the result in output.\n   Use AMD HIP to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\n__global__ void reduceLogicalXOR(const bool *x, size_t N, bool *output) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "correct_output": "int n = x.size();\n  bool result = false;\n  for (int i = 0; i < n; ++i) {\n    result ^= x[i];\n  }\n  return result;\n}"
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the logical XOR reduction of the vector of bools x. Store the result in output.\n   Use CUDA to reduce in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\n__global__ void reduceLogicalXOR(const bool *x, size_t N, bool *output) {",
        "correct_output": null
    },
    {
        "problem_type": "reduce",
        "language": "cpp",
        "name": "25_reduce_xor",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI and OpenMP to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
        "correct_output": "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // every rank has a complete copy of x\n  std::vector<bool> x_local(x);\n\n  // compute the logical XOR of the local vector\n  bool result = false;\n  for (int i = 0; i < n; i++) {\n    result = result ^ x_local[i];\n  }\n\n  // reduce the result using MPI and OpenMP\n  bool result_global;\n  MPI_Allreduce(&result, &result_global, 1, MPI_C_BOOL, MPI_LOR, MPI_COMM_WORLD);\n\n  return result_global;\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the reverse prefix sum of the array x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(Kokkos::View<const int*> const& x, Kokkos::View<int*> &output) {",
        "correct_output": "\n// Get the size of the input array\nint N = x.extent(0);\n\n// Create a parallel for loop to compute the reverse prefix sum\nKokkos::parallel_scan(\"reverse_prefix_sum\", N, KOKKOS_LAMBDA(const int i, int& update, const bool final) {\n    // Inclusive scan, so add the current element\n    update += x(N - 1 - i);\n    if (final) {\n        // Write the result to the output array in reverse\n        output(i) = update;\n    }\n});\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "33_scan_reverse_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(Kokkos::View<const double*> const& x) {",
        "correct_output": "\n// Create a view to store the prefix sums\nKokkos::View<double*> prefix_sums(\"prefix_sums\", x.extent(0));\n\n// Compute the prefix sums in parallel\nKokkos::parallel_scan(\"compute_prefix_sums\", x.extent(0),\n    KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n        update += x(i);\n        if (final) {\n            prefix_sums(i) = update;\n        }\n    }\n);\n\n// Compute the sum of the prefix sums\ndouble total_sum = 0.0;\nKokkos::parallel_reduce(\"sum_of_prefix_sums\", x.extent(0),\n    KOKKOS_LAMBDA(const int i, double& lsum) {\n        lsum += prefix_sums(i);\n    }, total_sum\n);\n\nreturn total_sum;\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "correct_output": "int n = x.size();\n  double sum = 0;\n  double* sum_array = new double[n];\n  sum_array[0] = x[0];\n  for (int i = 1; i < n; i++) {\n    sum_array[i] = sum_array[i-1] + x[i];\n  }\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n; i++) {\n    sum += sum_array[i];\n  }\n  delete[] sum_array;\n  return sum;\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "correct_output": "double sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "hip",
        "prompt": "/* Compute the prefix sum array of the vector x and compute its sum. Store the result in sum.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ void sumOfPrefixSum(const double *x, size_t N, double *sum) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "correct_output": "int const n = x.size();\n  std::vector<double> prefixSum(n);\n  prefixSum[0] = x[0];\n  for (int i = 1; i < n; ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n  }\n  double sum = 0;\n  for (int i = 0; i < n; ++i) {\n    sum += prefixSum[i];\n  }\n  return sum;\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the prefix sum array of the vector x and compute its sum. Store the result in sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ void sumOfPrefixSum(const double *x, size_t N, double *sum) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "32_scan_sum_of_prefix_sum_array",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "correct_output": "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // compute the prefix sum\n  std::vector<double> prefixSum(n);\n  prefixSum[0] = x[0];\n  for (int i = 1; i < n; ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n  }\n\n  // compute the sum of the prefix sums\n  double sum = 0;\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = 0; i < n; ++i) {\n    sum += prefixSum[i];\n  }\n\n  return sum;\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "serial",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "correct_output": "float min = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "hip",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int local_size = x.size() / size;\n    std::vector<float> local_x(local_size);\n\n    // Scatter the vector to all processes\n    MPI_Scatter(x.data(), local_size, MPI_FLOAT, local_x.data(), local_size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    // Compute the partial minimums for each process\n    for (int i = 1; i < local_size; ++i) {\n        local_x[i] = std::min(local_x[i], local_x[i - 1]);\n    }\n\n    // Gather the results back to process 0\n    MPI_Gather(local_x.data(), local_size, MPI_FLOAT, x.data(), local_size, MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n    // Process 0 computes the final minimums\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            int start = i * local_size;\n            x[start] = std::min(x[start], x[start - 1]);\n            for (int j = 1; j < local_size; ++j) {\n                x[start + j] = std::min(x[start + j], x[start + j - 1]);\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "cuda",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "31_scan_scan_with_min_function",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(Kokkos::View<int*> const& x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "serial",
        "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Store the result in sum.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ void maximumSubarray(const int *x, size_t N, int *sum) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Store the result in sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ void maximumSubarray(const int *x, size_t N, int *sum) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "34_scan_largest_contiguous_subarray_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "correct_output": null
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {",
        "correct_output": "\n// Determine the size of the input view\nauto n = x.extent(0);\n\n// Create a parallel scan functor\nKokkos::parallel_scan(\"PrefixSum\", n, KOKKOS_LAMBDA(const int i, double& update, const bool final) {\n    update += x(i);\n    if (final) {\n        output(i) = update;\n    }\n});\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "correct_output": "int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "correct_output": "output.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double* x, double *output, size_t N) {",
        "correct_output": "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    output[tid] = x[tid];\n    for (int i = 1; i < blockDim.x; i *= 2) {\n      __syncthreads();\n      if (tid >= i) {\n        output[tid] += output[tid - i];\n      }\n    }\n  }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "correct_output": "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the size of the chunk each process will compute\n    int n = x.size();\n    int local_n = n / size;\n    int remainder = n % size;\n    if (rank < remainder) {\n        local_n++;\n    }\n\n    // Calculate the starting index for each process\n    int start_index = rank * local_n;\n    if (rank >= remainder) {\n        start_index += remainder;\n    }\n\n    // Compute the local prefix sum\n    std::vector<double> local_prefix_sum(local_n);\n    local_prefix_sum[0] = x[start_index];\n    for (int i = 1; i < local_n; ++i) {\n        local_prefix_sum[i] = local_prefix_sum[i - 1] + x[start_index + i];\n    }\n\n    // Gather all local prefix sums to the root process\n    std::vector<double> all_prefix_sums;\n    if (rank == 0) {\n        all_prefix_sums.resize(n);\n    }\n    std::vector<int> recvcounts(size, local_n);\n    std::vector<int> displs(size, 0);\n    for (int i = 0; i < size; ++i) {\n        if (i < remainder) {\n            recvcounts[i]++;\n        }\n        if (i > 0) {\n            displs[i] = displs[i - 1] + recvcounts[i - 1];\n        }\n    }\n    MPI_Gatherv(local_prefix_sum.data(), local_n, MPI_DOUBLE, all_prefix_sums.data(),\n                recvcounts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute the final prefix sum on the root process\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            int index = displs[i];\n            double add_value = all_prefix_sums[index - 1];\n            for (int j = 0; j < recvcounts[i]; ++j) {\n                all_prefix_sums[index + j] += add_value;\n            }\n        }\n        output = all_prefix_sums;\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double *x, double *output, size_t N) {",
        "correct_output": "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        double sum = 0;\n        for (size_t j = 0; j <= i; ++j) {\n            sum += x[j];\n        }\n        output[i] = sum;\n    }\n}"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "30_scan_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n  if (rank == 0) {\n    output.resize(x.size());\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n      output[i] = output[i - 1] + x[i];\n    }\n  }\n  MPI_Bcast(&output[0], output.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {",
        "correct_output": "Kokkos::parallel_for(\"negateOddsAndHalveEvens\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n    if (x(i) % 2!= 0) {\n      x(i) = -x(i);\n    } else {\n      x(i) /= 2;\n    }\n  });\n  Kokkos::fence();\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "correct_output": "int n = x.size();\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "serial",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "correct_output": "for (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "hip",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {",
        "correct_output": "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + tid;\n  if (gid < N) {\n    if (x[gid] % 2 == 1) {\n      x[gid] = -x[gid];\n    } else {\n      x[gid] /= 2;\n    }\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    std::vector<int> local_x(chunk);\n    if (rank < remainder) {\n        local_x.resize(chunk + 1);\n    }\n\n    MPI_Scatter(x.data(), local_x.size(), MPI_INT, local_x.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] % 2 == 1) {\n            local_x[i] = -local_x[i];\n        } else {\n            local_x[i] = local_x[i] / 2;\n        }\n    }\n\n    MPI_Gather(local_x.data(), local_x.size(), MPI_INT, x.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "cuda",
        "prompt": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {",
        "correct_output": "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "56_transform_negate_odds",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = (rank + 1) * chunk;\n\n  std::vector<int> local_x(x.begin() + start, x.begin() + end);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    if (local_x[i] % 2 == 0) {\n      local_x[i] /= 2;\n    } else {\n      local_x[i] = -local_x[i];\n    }\n  }\n\n  MPI_Gather(local_x.data(), local_x.size(), MPI_INT, x.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n\t\tKOKKOS_LAMBDA(int i) {\n\t\t\tmask(i) = isPowerOfTwo(x(i));\n\t\t}\n\t);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "correct_output": null
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "serial",
        "prompt": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "correct_output": "for (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "hip",
        "prompt": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "correct_output": null
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "cuda",
        "prompt": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {",
        "correct_output": "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "59_transform_map_function",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
        "correct_output": "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = x.size() / size;\n    int remainder = x.size() % size;\n\n    std::vector<int> local_x(chunk_size + (rank < remainder? 1 : 0));\n    MPI_Scatter(x.data(), chunk_size + (rank < remainder? 1 : 0), MPI_INT, local_x.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        local_x[i] = isPowerOfTwo(local_x[i]);\n    }\n\n    std::vector<int> local_mask(local_x.size());\n    std::copy(local_x.begin(), local_x.end(), local_mask.begin());\n\n    std::vector<int> global_mask(x.size());\n    MPI_Gather(local_mask.data(), local_mask.size(), MPI_INT, global_mask.data(), chunk_size + (rank < remainder? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        mask.resize(x.size());\n        for (int i = 0; i < x.size(); ++i) {\n            mask[i] = global_mask[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {",
        "correct_output": "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n    x(i) = 1.0 - 1.0 / x(i);\n  });\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "correct_output": "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        if (x[i]!= 0) {\n            x[i] = 1 - 1 / x[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "serial",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "correct_output": "for (auto &e : x) {\n    e = 1.0 - 1.0 / e;\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "hip",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = 1.0 - 1.0 / x[i];\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // get the number of elements\n    int n = x.size();\n\n    // calculate the number of elements each rank will process\n    int n_local = n / size;\n\n    // calculate the number of elements that will be left over\n    int n_left = n % size;\n\n    // calculate the starting index of the local array\n    int start = n_local * rank;\n\n    // calculate the ending index of the local array\n    int end = n_local * (rank + 1);\n\n    // if rank is less than the number of elements that will be left over,\n    // add the number of elements that will be left over to the end index\n    if (rank < n_left) {\n        end += n_left;\n    }\n\n    // create a vector to store the local array\n    std::vector<double> local_array(n_local);\n\n    // copy the local array into the local vector\n    for (int i = start; i < end; i++) {\n        local_array[i - start] = x[i];\n    }\n\n    // create a vector to store the result\n    std::vector<double> result(n_local);\n\n    // perform the calculation\n    for (int i = 0; i < n_local; i++) {\n        result[i] = 1 - 1 / local_array[i];\n    }\n\n    // gather the result from all ranks\n    MPI_Gather(result.data(), n_local, MPI_DOUBLE, x.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "cuda",
        "prompt": "/* Replace every element of the vector x with 1-1/x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {",
        "correct_output": "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = 1.0 - 1.0 / x[idx];\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "57_transform_inverse_offset",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n\n  std::vector<double> local_x(chunk);\n  std::vector<double> local_y(chunk);\n\n  for (int i = 0; i < chunk; i++) {\n    local_x[i] = x[rank * chunk + i];\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < chunk; i++) {\n    local_y[i] = 1.0 / local_x[i];\n  }\n\n  MPI_Gather(local_y.data(), chunk, MPI_DOUBLE, x.data(), chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) {\n      x[i] = 1.0 - x[i];\n    }\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) { x(i) = (x(i) < 0)? 0 : x(i); });\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "correct_output": "int n = x.size();\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "serial",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "correct_output": "for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "hip",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {",
        "correct_output": "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    x[idx] = max(x[idx], 0.0);\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // get the number of elements in x\n  int n = x.size();\n\n  // get the number of elements each rank will compute\n  int n_local = n / size;\n\n  // get the start and end indices for this rank\n  int start = n_local * rank;\n  int end = n_local * (rank + 1);\n\n  // compute the ReLU function on every element of x\n  for (int i = start; i < end; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n\n  // gather the results from all ranks\n  MPI_Gather(&x[start], n_local, MPI_DOUBLE, &x[0], n_local, MPI_DOUBLE, 0,\n             MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {",
        "correct_output": "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = max(0.0, x[idx]);\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "55_transform_relu",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = start + chunk;\n\n  // compute the relu function in parallel\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    if (x[i] < 0) {\n      x[i] = 0;\n    }\n  }\n\n  // gather the results from all ranks\n  MPI_Gather(&x[start], chunk, MPI_DOUBLE, &x[0], chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) { x(i) *= x(i); });\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "correct_output": "int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        x[i] = x[i] * x[i];\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "serial",
        "prompt": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "correct_output": "for (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "hip",
        "prompt": "/* Replace every element of x with the square of its value.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    x[i] = x[i] * x[i];\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = x.size();\n  int chunk = n / size;\n  int remainder = n % size;\n\n  std::vector<int> local_x(chunk);\n  std::vector<int> local_y(chunk);\n\n  // copy local data\n  for (int i = 0; i < chunk; i++) {\n    local_x[i] = x[i + rank * chunk];\n  }\n\n  // square local data\n  for (int i = 0; i < chunk; i++) {\n    local_y[i] = local_x[i] * local_x[i];\n  }\n\n  // gather results\n  std::vector<int> y(n);\n  MPI_Gather(&local_y[0], chunk, MPI_INT, &y[0], chunk, MPI_INT, 0,\n             MPI_COMM_WORLD);\n\n  // copy remainder\n  if (rank == 0) {\n    for (int i = 0; i < remainder; i++) {\n      y[i + chunk * size] = x[i + chunk * size] * x[i + chunk * size];\n    }\n  }\n\n  // copy results back\n  for (int i = 0; i < n; i++) {\n    x[i] = y[i];\n  }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "cuda",
        "prompt": "/* Replace every element of x with the square of its value.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}"
    },
    {
        "problem_type": "transform",
        "language": "cpp",
        "name": "58_transform_squaring",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
        "correct_output": "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size();\n  int chunk = n / size;\n  int start = rank * chunk;\n  int end = start + chunk;\n\n  std::vector<int> local_x(x.begin() + start, x.begin() + end);\n\n  #pragma omp parallel for\n  for (int i = 0; i < local_x.size(); i++) {\n    local_x[i] = local_x[i] * local_x[i];\n  }\n\n  MPI_Gather(local_x.data(), local_x.size(), MPI_INT, x.data(), local_x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {",
        "correct_output": "Kokkos::parallel_for(\"spmv\", Kokkos::RangePolicy<>(0, M), KOKKOS_LAMBDA(const int i) {\n      double sum = 0.0;\n      for (size_t j = 0; j < A.extent(0); ++j) {\n         if (A(j).row == i) {\n            sum += A(j).value * x(A(j).column);\n         }\n      }\n      y(i) = alpha * sum + beta * y(i);\n   });\n   Kokkos::fence();\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      double sum = 0.0;\n      for (size_t j = 0; j < A.size(); ++j) {\n         if (A[j].row == i) {\n            sum += A[j].value * x[A[j].column];\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "for (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use AMD HIP to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {",
        "correct_output": "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row < M) {\n      double sum = 0;\n      for (size_t i = 0; i < sizeA; i++) {\n         if (A[i].row == row) {\n            sum += A[i].value * x[A[i].column];\n         }\n      }\n      y[row] = alpha * sum + beta * y[row];\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "\n// Get the rank and size of the current MPI context\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Initialize local sum vector\nstd::vector<double> local_y(M, 0.0);\n\n// Perform the matrix-vector multiplication locally\nfor (const auto& elem : A) {\n    if (elem.row % size == rank) { // Each rank works on a subset of rows\n        local_y[elem.row] += alpha * elem.value * x[elem.column];\n    }\n}\n\n// Sum up the contributions from all ranks\nstd::vector<double> global_y(M, 0.0);\nMPI_Reduce(local_y.data(), global_y.data(), M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n// Rank 0 combines the result with the initial y vector\nif (rank == 0) {\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = global_y[i] + beta * y[i];\n    }\n}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {",
        "correct_output": "size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row < M) {\n      double sum = 0.0;\n      for (size_t i = 0; i < sizeA; i++) {\n         if (A[i].row == row) {\n            sum += A[i].value * x[A[i].column];\n         }\n      }\n      y[row] = alpha * sum + beta * y[row];\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "47_sparse_la_spmv",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI and OpenMP to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   // Each rank computes a part of y\n   #pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      double sum = 0.0;\n      for (size_t j = 0; j < A.size(); ++j) {\n         if (A[j].row == i) {\n            sum += A[j].value * x[A[j].column];\n         }\n      }\n      y[i] = alpha * sum + beta * y[i];\n   }\n\n   // Gather all y's to rank 0\n   MPI_Barrier(MPI_COMM_WORLD);\n   if (world_rank == 0) {\n      for (int i = 1; i < world_size; ++i) {\n         MPI_Recv(&y[0], M, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   } else {\n      MPI_Send(&y[0], M, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {",
        "correct_output": "\n// Initialize the result matrix Y to zero\nKokkos::deep_copy(Y, 0.0);\n\n// Create a parallel for loop to compute the matrix multiplication\nKokkos::parallel_for(\"spmm\", A.extent(0), KOKKOS_LAMBDA(const size_t &i) {\n    auto a = A(i);\n    for (size_t j = 0; j < X.extent(0); ++j) {\n        auto x = X(j);\n        if (a.column == x.row) {\n            Kokkos::atomic_add(&Y(a.row, x.column), a.value * x.value);\n        }\n    }\n});\n// Ensure all parallel operations are complete\nKokkos::fence();\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "correct_output": "#pragma omp parallel for\n   for (size_t i = 0; i < M; ++i) {\n      for (size_t j = 0; j < N; ++j) {\n         double sum = 0;\n         for (size_t k = 0; k < K; ++k) {\n            for (auto const& a : A) {\n               if (a.row == i && a.column == k) {\n                  for (auto const& x : X) {\n                     if (x.row == k && x.column == j) {\n                        sum += a.value * x.value;\n                     }\n                  }\n               }\n            }\n         }\n         Y[i * N + j] = sum;\n      }\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "correct_output": "std::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "correct_output": "\n    // Initialize Y to all zeros\n    Y.resize(M * N, 0.0);\n\n    // Perform matrix multiplication\n    for (size_t i = 0; i < A.size(); ++i) {\n        size_t row = A[i].row;\n        size_t col = A[i].column;\n        double val = A[i].value;\n\n        for (size_t j = 0; j < X.size(); ++j) {\n            if (X[j].row == col) {\n                size_t result_row = row;\n                size_t result_col = X[j].column;\n                double result_val = val * X[j].value;\n                Y[result_row * N + result_col] += result_val;\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {",
        "correct_output": "\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        for (int i = 0; i < sizeA; i++) {\n            if (A[i].row == row) {\n                int col = A[i].column;\n                double valA = A[i].value;\n                for (int j = 0; j < sizeX; j++) {\n                    if (X[j].row == col) {\n                        int colX = X[j].column;\n                        double valX = X[j].value;\n                        atomicAdd(&Y[row * N + colX], valA * valX);\n                    }\n                }\n            }\n        }\n    }\n}\n"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "46_sparse_la_spmm",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
        "correct_output": "// Initialize Y to zero\n   Y.resize(M * N, 0.0);\n\n   // Compute the multiplication in parallel\n   #pragma omp parallel for\n   for (size_t i = 0; i < A.size(); ++i) {\n      size_t row = A[i].row;\n      size_t col = A[i].column;\n      double val = A[i].value;\n\n      for (size_t j = 0; j < X.size(); ++j) {\n         if (X[j].row == col) {\n            Y[row * N + X[j].column] += val * X[j].value;\n         }\n      }\n   }\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(Kokkos::View<const COOElement*> &A, Kokkos::View<const double*> &b, Kokkos::View<double*> &x, size_t N) {",
        "correct_output": "\n// Create a mirror view to work with on the host\nauto h_A = Kokkos::create_mirror_view(A);\nauto h_b = Kokkos::create_mirror_view(b);\nauto h_x = Kokkos::create_mirror_view(x);\n\n// Copy the data from the device to the host\nKokkos::deep_copy(h_A, A);\nKokkos::deep_copy(h_b, b);\n\n// Initialize x to zero\nKokkos::deep_copy(h_x, 0.0);\n\n// Create a dense matrix representation of A on the host\nstd::vector<std::vector<double>> denseA(N, std::vector<double>(N, 0.0));\nfor (size_t i = 0; i < h_A.extent(0); ++i) {\n    denseA[h_A(i).row][h_A(i).column] = h_A(i).value;\n}\n\n// Perform Gaussian elimination\nfor (size_t i = 0; i < N; ++i) {\n    // Find pivot\n    double maxEl = std::abs(denseA[i][i]);\n    size_t maxRow = i;\n    for (size_t k = i + 1; k < N; ++k) {\n        if (std::abs(denseA[k][i]) > maxEl) {\n            maxEl = std::abs(denseA[k][i]);\n            maxRow = k;\n        }\n    }\n\n    // Swap maximum row with current row\n    for (size_t k = i; k < N; ++k) {\n        std::swap(denseA[maxRow][k], denseA[i][k]);\n    }\n    std::swap(h_b(maxRow), h_b(i));\n\n    // Make all rows below this one 0 in current column\n    for (size_t k = i + 1; k < N; ++k) {\n        double c = -denseA[k][i] / denseA[i][i];\n        for (size_t j = i; j < N; ++j) {\n            if (i == j) {\n                denseA[k][j] = 0;\n            } else {\n                denseA[k][j] += c * denseA[i][j];\n            }\n        }\n        h_b(k) += c * h_b(i);\n    }\n}\n\n// Solve equation Ax=b for an upper triangular matrix A\nfor (int i = N - 1; i >= 0; --i) {\n    h_x(i) = h_b(i) / denseA[i][i];\n    for (int k = i - 1; k >= 0; --k) {\n        h_b(k) -= denseA[k][i] * h_x(i);\n    }\n}\n\n// Copy the result back to the device\nKokkos::deep_copy(x, h_x);\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": "\n// Initialize the solution vector x with zeros\nx.assign(N, 0.0);\n\n// Create a dense matrix representation to store the values of A\nstd::vector<std::vector<double>> denseA(N, std::vector<double>(N, 0.0));\n\n// Fill the dense matrix with the values from the COO format\nfor (const auto& elem : A) {\n    denseA[elem.row][elem.column] = elem.value;\n}\n\n// Parallelize the matrix-vector multiplication using OpenMP\n#pragma omp parallel for\nfor (size_t i = 0; i < N; ++i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < N; ++j) {\n        sum += denseA[i][j] * b[j];\n    }\n    x[i] = sum;\n}\n\n// Assuming A is diagonally dominant or invertible, we can use a simple iterative method to solve for x\n// Here we use the Jacobi iterative method as an example\nstd::vector<double> prev_x(N, 0.0);\ndouble tolerance = 1e-7;\nbool converged = false;\n\nwhile (!converged) {\n    converged = true;\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double sigma = 0.0;\n        for (size_t j = 0; j < N; ++j) {\n            if (i != j) {\n                sigma += denseA[i][j] * prev_x[j];\n            }\n        }\n        double new_xi = (b[i] - sigma) / denseA[i][i];\n        if (std::abs(new_xi - prev_x[i]) > tolerance) {\n            converged = false;\n        }\n        x[i] = new_xi;\n    }\n    prev_x = x; // Prepare for the next iteration\n}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format with sizeA elements. x and b are dense vectors with N elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\n__global__ void solveLinearSystem(const COOElement *A, size_t sizeA, const double *b, double *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format with sizeA elements. x and b are dense vectors with N elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\n__global__ void solveLinearSystem(const COOElement *A, size_t sizeA, const double *b, double *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "45_sparse_la_sparse_solve",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Solve the sparse linear system Ax=b for x.\n   A is a sparse NxN matrix in COO format. x and b are dense vectors with N elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[{0,0,1}, {0,1,1}, {1,1,-2}] b=[1,4]\n   output: x=[3,-2]\n*/\nvoid solveLinearSystem(std::vector<COOElement> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": "\n// Get the rank and size of the current MPI environment\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Initialize x with zeros on all ranks\nx.assign(N, 0.0);\n\n// Create a local vector to store partial results\nstd::vector<double> local_x(N, 0.0);\n\n// Use OpenMP to parallelize the computation of Ax\n#pragma omp parallel for\nfor (size_t i = 0; i < A.size(); ++i) {\n    const COOElement& element = A[i];\n    local_x[element.row] += element.value * b[element.column];\n}\n\n// Reduce all partial results to rank 0\nMPI_Reduce(local_x.data(), x.data(), N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n// Solve for x on rank 0\nif (rank == 0) {\n    // Assuming A is diagonally dominant or properly conditioned for a simple iterative solver\n    std::vector<double> temp_x(N);\n    bool converged = false;\n    const int max_iterations = 1000;\n    const double tolerance = 1e-10;\n    int iteration = 0;\n\n    while (!converged && iteration < max_iterations) {\n        converged = true;\n        for (size_t i = 0; i < N; ++i) {\n            double sigma = 0.0;\n            for (const auto& element : A) {\n                if (element.row == i && element.column != i) {\n                    sigma += element.value * x[element.column];\n                }\n            }\n            temp_x[i] = (b[i] - sigma) / (A[i].value); // Assuming A[i] corresponds to the diagonal element\n            if (std::abs(temp_x[i] - x[i]) > tolerance) {\n                converged = false;\n            }\n        }\n        x = temp_x;\n        ++iteration;\n    }\n}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(Kokkos::View<const COOElement*> &A, Kokkos::View<double**> &L, Kokkos::View<double**> &U, size_t N) {",
        "correct_output": "\n// Assuming the matrix is square and non-singular\n// Initialize L to identity and U to zero\nKokkos::parallel_for(\"init_LU\", N, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < N; ++j) {\n        if (i == j) {\n            L(i, j) = 1.0;\n            U(i, j) = 0.0;\n        } else {\n            L(i, j) = 0.0;\n            U(i, j) = 0.0;\n        }\n    }\n});\n\n// Convert COO to a dense matrix for simplicity\nKokkos::View<double**> dense_A(\"dense_A\", N, N);\nKokkos::parallel_for(\"convert_to_dense\", A.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    const auto& elem = A(i);\n    dense_A(elem.row, elem.column) = elem.value;\n});\n\n// Perform LU factorization\nfor (size_t k = 0; k < N; ++k) {\n    // Upper Triangular\n    Kokkos::parallel_for(\"compute_U\", N - k - 1, KOKKOS_LAMBDA(const size_t i) {\n        U(k, k + i + 1) = dense_A(k, k + i + 1);\n        for (size_t p = 0; p < k; ++p) {\n            U(k, k + i + 1) -= L(k, p) * U(p, k + i + 1);\n        }\n    });\n\n    // Lower Triangular\n    Kokkos::parallel_for(\"compute_L\", N - k - 1, KOKKOS_LAMBDA(const size_t i) {\n        L(k + i + 1, k) = dense_A(k + i + 1, k) / U(k, k);\n        for (size_t p = 0; p < k; ++p) {\n            L(k + i + 1, k) -= L(k + i + 1, p) * U(p, k) / U(k, k);\n        }\n    });\n\n    // Update the remaining entries in dense_A\n    Kokkos::parallel_for(\"update_A\", (N - k - 1) * (N - k - 1), KOKKOS_LAMBDA(const size_t idx) {\n        size_t i = idx / (N - k - 1);\n        size_t j = idx % (N - k - 1);\n        dense_A(k + i + 1, k + j + 1) -= L(k + i + 1, k) * U(k, k + j + 1);\n    });\n}\n\n// Ensure all parallel operations are completed\nKokkos::fence();\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
        "correct_output": "\n// Assuming L and U are already allocated with the appropriate size (N*N)\n// Initialize L to identity and U to zero\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        if (i == j) {\n            L[i * N + j] = 1.0;\n        } else {\n            L[i * N + j] = 0.0;\n        }\n        U[i * N + j] = 0.0;\n    }\n}\n\n// Convert COO to a dense matrix representation for simplicity\nstd::vector<double> denseA(N * N, 0.0);\nfor (const auto& elem : A) {\n    denseA[elem.row * N + elem.column] = elem.value;\n}\n\n// Perform LU factorization\n#pragma omp parallel for\nfor (size_t k = 0; k < N; ++k) {\n    // Upper Triangular\n    for (size_t j = k; j < N; ++j) {\n        double sum = 0.0;\n        for (size_t p = 0; p < k; ++p) {\n            sum += L[k * N + p] * U[p * N + j];\n        }\n        U[k * N + j] = denseA[k * N + j] - sum;\n    }\n\n    // Lower Triangular\n    for (size_t i = k + 1; i < N; ++i) {\n        double sum = 0.0;\n        for (size_t p = 0; p < k; ++p) {\n            sum += L[i * N + p] * U[p * N + k];\n        }\n        L[i * N + k] = (denseA[i * N + k] - sum) / U[k * N + k];\n    }\n}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "serial",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
        "correct_output": "\n// Assuming A is sorted by row and then by column\nL.clear();\nU.clear();\n\n// Initialize L and U with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Create a map for quick access to A's elements\nstd::map<std::pair<size_t, size_t>, double> A_map;\nfor (const auto& elem : A) {\n    A_map[{elem.row, elem.column}] = elem.value;\n}\n\n// LU decomposition algorithm\nfor (size_t i = 0; i < N; ++i) {\n    // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[i * N + j] * U[j * N + k];\n        }\n        U[i * N + k] = A_map[{i, k}] - sum;\n    }\n\n    // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[k * N + j] * U[j * N + i];\n        }\n        L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n    }\n\n    // L(i, i) = 1\n    L[i * N + i] = 1.0;\n}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "hip",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format with sizeA elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\n__global__ void luFactorize(const COOElement *A, size_t sizeA, double *L, double *U, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
        "correct_output": "\n// Get the rank and size of the current MPI environment\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Initialize L and U with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Only proceed if there are elements in the matrix\nif (!A.empty()) {\n    // Create a dense matrix representation from COO format\n    std::vector<std::vector<double>> denseA(N, std::vector<double>(N, 0));\n    for (const auto& elem : A) {\n        denseA[elem.row][elem.column] = elem.value;\n    }\n\n    // Perform LU factorization using a simple serial algorithm\n    // This is a placeholder for the actual parallel algorithm\n    for (size_t k = 0; k < N; ++k) {\n        // Broadcast the pivot row to all ranks\n        if (rank == 0) {\n            for (size_t j = k; j < N; ++j) {\n                U[k * N + j] = denseA[k][j];\n            }\n        }\n        MPI_Bcast(&U[k * N + k], N - k, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n        // Compute L and U elements\n        for (size_t i = k + 1; i < N; ++i) {\n            if (rank == 0) {\n                L[i * N + k] = denseA[i][k] / U[k * N + k];\n                for (size_t j = k + 1; j < N; ++j) {\n                    denseA[i][j] -= L[i * N + k] * U[k * N + j];\n                }\n            }\n        }\n\n        // Broadcast the L column to all ranks\n        if (rank == 0) {\n            for (size_t i = k + 1; i < N; ++i) {\n                U[i * N + k] = 0; // Upper triangular matrix has zeros below the diagonal\n            }\n        }\n        MPI_Bcast(&L[(k + 1) * N + k], N - k - 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n\n    // Set the diagonal of L to 1\n    if (rank == 0) {\n        for (size_t i = 0; i < N; ++i) {\n            L[i * N + i] = 1;\n        }\n    }\n}\n\n// Only rank 0 will have the correct L and U matrices\nif (rank != 0) {\n    L.clear();\n    U.clear();\n}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "cuda",
        "prompt": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format with sizeA elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\n__global__ void luFactorize(const COOElement *A, size_t sizeA, double *L, double *U, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "49_sparse_la_sparse_lu_decomp",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
        "correct_output": "\n// Get the rank and size of the current MPI environment\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Initialize L and U as dense matrices filled with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Only proceed if we are on rank 0\nif (rank == 0) {\n    // Convert COO to dense format for easier manipulation\n    std::vector<std::vector<double>> denseA(N, std::vector<double>(N, 0));\n    for (const auto& elem : A) {\n        denseA[elem.row][elem.column] = elem.value;\n    }\n\n    // Perform LU factorization using the Doolittle algorithm\n    for (size_t k = 0; k < N; ++k) {\n        // Parallelize the computation of U's row k\n        #pragma omp parallel for\n        for (size_t j = k; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t p = 0; p < k; ++p) {\n                sum += L[k * N + p] * U[p * N + j];\n            }\n            U[k * N + j] = denseA[k][j] - sum;\n        }\n\n        // Parallelize the computation of L's column k\n        #pragma omp parallel for\n        for (size_t i = k + 1; i < N; ++i) {\n            double sum = 0.0;\n            for (size_t p = 0; p < k; ++p) {\n                sum += L[i * N + p] * U[p * N + k];\n            }\n            L[i * N + k] = (denseA[i][k] - sum) / U[k * N + k];\n        }\n    }\n\n    // Set the diagonal of L to 1\n    for (size_t i = 0; i < N; ++i) {\n        L[i * N + i] = 1.0;\n    }\n}\n\n// Synchronize all ranks before returning\nMPI_Barrier(MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<double*> &z) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "serial",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "hip",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {",
        "correct_output": "// TODO: implement sparseAxpy\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
        "correct_output": null
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "cuda",
        "prompt": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {",
        "correct_output": "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tdouble sum = 0;\n\t\tfor (size_t j = 0; j < Nx; j++) {\n\t\t\tif (x[j].index == i) {\n\t\t\t\tsum += x[j].value;\n\t\t\t}\n\t\t}\n\t\tfor (size_t j = 0; j < Ny; j++) {\n\t\t\tif (y[j].index == i) {\n\t\t\t\tsum += y[j].value;\n\t\t\t}\n\t\t}\n\t\tz[i] = alpha * sum;\n\t}\n}"
    },
    {
        "problem_type": "sparse_la",
        "language": "cpp",
        "name": "48_sparse_la_sparse_axpy",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, std::vector<Element> const& x, std::vector<Element> const& y, std::vector<double> &z) {",
        "correct_output": null
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
        "correct_output": null
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "correct_output": "// TODO: implement this function\n    // Hint: use OpenMP to parallelize the computation\n    // Hint: use std::array to store the output\n#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "serial",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "correct_output": "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "hip",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {",
        "correct_output": null
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "correct_output": "// TODO: implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = s.size();\n    int n_per_rank = n / size;\n    int remainder = n % size;\n\n    int start = rank * n_per_rank;\n    int end = start + n_per_rank;\n\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    std::array<int, 26> local_bins;\n    for (int i = 0; i < 26; i++) {\n        local_bins[i] = 0;\n    }\n\n    for (int i = start; i < end; i++) {\n        std::string word = s[i];\n        if (word.length() > 0) {\n            local_bins[word[0] - 'a']++;\n        }\n    }\n\n    std::array<int, 26> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = global_bins[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "cuda",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {",
        "correct_output": "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int stride = blockDim.x;\n    int i = bid * stride + tid;\n    if (i < N) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "23_histogram_first_letter_counts",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "correct_output": "// TODO: implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = s.size();\n    int chunk = n / size;\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank == size - 1) {\n        end = n;\n    }\n\n    std::array<int, 26> local_bins;\n    for (int i = 0; i < 26; i++) {\n        local_bins[i] = 0;\n    }\n\n    for (int i = start; i < end; i++) {\n        std::string str = s[i];\n        int first_letter = str[0] - 'a';\n        local_bins[first_letter]++;\n    }\n\n    std::array<int, 26> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = global_bins[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      const double value = x(i);\n      const size_t bin = (size_t) (value / 10);\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n    }\n  );\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "correct_output": "// Initialize bins to 0\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n\n    // Parallelize the loop\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        // Find the bin index\n        size_t binIndex = x[i] / 10;\n\n        // Increment the count in the corresponding bin\n        #pragma omp atomic\n        bins[binIndex]++;\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "serial",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "correct_output": "for (auto const& value : x) {\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "hip",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {",
        "correct_output": "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    size_t bin = (size_t)(x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // calculate the number of elements per rank\n  size_t elements_per_rank = x.size() / size;\n\n  // calculate the number of elements that are left over\n  size_t elements_left_over = x.size() % size;\n\n  // calculate the starting index of the elements that are assigned to this rank\n  size_t start_index = rank * elements_per_rank;\n\n  // calculate the end index of the elements that are assigned to this rank\n  size_t end_index = start_index + elements_per_rank;\n\n  // if this rank has elements left over, add them to the end index\n  if (rank < elements_left_over) {\n    end_index++;\n  }\n\n  // create a vector of the elements that are assigned to this rank\n  std::vector<double> local_x(x.begin() + start_index, x.begin() + end_index);\n\n  // create a vector of the counts for each bin\n  std::vector<size_t> local_bins(10, 0);\n\n  // loop over the elements in the local vector\n  for (auto const& element : local_x) {\n    // get the bin number\n    int bin = element / 10;\n\n    // increment the count for the bin\n    local_bins[bin]++;\n  }\n\n  // gather the local bins to rank 0\n  std::vector<size_t> global_bins(10, 0);\n  MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // copy the result to the output vector\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = global_bins[i];\n    }\n  }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "cuda",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {",
        "correct_output": "size_t tid = threadIdx.x;\n    size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = gid; i < N; i += stride) {\n        size_t bin = (size_t) (x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "21_histogram_bin_0-100",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "correct_output": "// TODO: implement this function\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<size_t> local_bins(10, 0);\n    int n = x.size();\n    int chunk = n / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = n;\n    }\n    for (int i = start; i < end; i++) {\n        int index = (int) (x[i] / 10);\n        local_bins[index]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA(size_t i) {\n      if (points(i).x > 0 && points(i).y > 0) {\n         Kokkos::atomic_fetch_add(&bins(0), 1);\n      } else if (points(i).x < 0 && points(i).y > 0) {\n         Kokkos::atomic_fetch_add(&bins(1), 1);\n      } else if (points(i).x < 0 && points(i).y < 0) {\n         Kokkos::atomic_fetch_add(&bins(2), 1);\n      } else {\n         Kokkos::atomic_fetch_add(&bins(3), 1);\n      }\n   });\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "correct_output": "// Initialize bins to 0\n   for (size_t i = 0; i < 4; ++i) {\n      bins[i] = 0;\n   }\n\n   // Use OpenMP to count points in parallel\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      Point const& p = points[i];\n      size_t bin;\n\n      if (p.x > 0) {\n         if (p.y > 0)\n            bin = 0; // First quadrant\n         else\n            bin = 3; // Fourth quadrant\n      } else {\n         if (p.y > 0)\n            bin = 1; // Second quadrant\n         else\n            bin = 2; // Third quadrant\n      }\n\n      // Atomic increment to avoid race conditions\n      #pragma omp atomic\n      bins[bin]++;\n   }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "correct_output": "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {",
        "correct_output": null
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "correct_output": "// TODO: implement this function\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int n = points.size();\n   int n_per_rank = n / size;\n   int remainder = n % size;\n   int start = rank * n_per_rank;\n   int end = start + n_per_rank;\n   if (rank < remainder) {\n      end++;\n   }\n\n   int n_local = end - start;\n   std::vector<Point> local_points(n_local);\n   for (int i = 0; i < n_local; i++) {\n      local_points[i] = points[start + i];\n   }\n\n   std::array<int, 4> local_bins;\n   for (int i = 0; i < 4; i++) {\n      local_bins[i] = 0;\n   }\n\n   for (int i = 0; i < n_local; i++) {\n      if (local_points[i].x > 0 && local_points[i].y > 0) {\n         local_bins[0]++;\n      } else if (local_points[i].x < 0 && local_points[i].y > 0) {\n         local_bins[1]++;\n      } else if (local_points[i].x < 0 && local_points[i].y < 0) {\n         local_bins[2]++;\n      } else {\n         local_bins[3]++;\n      }\n   }\n\n   std::array<int, 4> global_bins;\n   MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = global_bins[i];\n      }\n   }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {",
        "correct_output": "int i = threadIdx.x;\n    if (i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x >= 0 && y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (x < 0 && y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (x < 0 && y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (x >= 0 && y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "22_histogram_count_quadrants",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "correct_output": "// Initialize bins to 0\n   for (size_t i = 0; i < 4; ++i) {\n      bins[i] = 0;\n   }\n\n   // Count points in each quadrant\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else if (point.x > 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
        "correct_output": null
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "correct_output": "// Initialize bins to 0\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    // Parallelize the loop\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double frac = x[i] - std::floor(x[i]); // get the fractional part\n        size_t bin = static_cast<size_t>(frac / 0.25); // determine the bin\n        // Increment the corresponding bin\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "correct_output": "for (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {",
        "correct_output": "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = fmod(x[i], 1.0);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "correct_output": "// Initialize bins to 0\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    // Loop over all elements in x\n    for (double value : x) {\n        // Determine the bin index based on the fractional part of the value\n        size_t bin_index = static_cast<size_t>(value * 4) % 4;\n        // Increment the count for the bin\n        ++bins[bin_index];\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {",
        "correct_output": "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = fmod(x[i], 1.0);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "24_histogram_count_quartile",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "correct_output": "// Initialize bins to 0\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    // Compute the size of the chunk of x that each rank will work on\n    int world_size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t chunk_size = x.size() / world_size;\n\n    // Compute the quartiles for each rank's chunk of x\n    #pragma omp parallel for\n    for (int i = 0; i < chunk_size; ++i) {\n        double frac = x[rank * chunk_size + i] - std::floor(x[rank * chunk_size + i]);\n        if (frac >= 0 && frac < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (frac >= 0.25 && frac < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (frac >= 0.5 && frac < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (frac >= 0.75 && frac < 1) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n\n    // Gather the counts from each rank to rank 0\n    if (rank == 0) {\n        for (int i = 1; i < world_size; ++i) {\n            std::array<size_t, 4> remote_bins;\n            MPI_Recv(&remote_bins, 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 4; ++j) {\n                bins[j] += remote_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(&bins, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      const int intensity = image(i);\n      Kokkos::atomic_fetch_add(&(bins(intensity)), 1);\n    });\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "correct_output": "// Initialize bins to 0\n    for (size_t i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n\n    // Use OpenMP to count in parallel\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "correct_output": "for (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {",
        "correct_output": "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int bin = 0;\n  for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n    bin = image[i];\n    atomicAdd(&bins[bin], 1);\n  }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "correct_output": "// TODO: implement this function\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int n = image.size();\n   int chunk = n / size;\n   int remainder = n % size;\n   int start = rank * chunk + std::min(rank, remainder);\n   int end = start + chunk + (rank < remainder);\n   std::vector<int> local_image(image.begin() + start, image.begin() + end);\n   std::array<size_t, 256> local_bins;\n   for (int i = 0; i < 256; i++) {\n     local_bins[i] = 0;\n   }\n   for (int i = 0; i < local_image.size(); i++) {\n     local_bins[local_image[i]]++;\n   }\n   MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {",
        "correct_output": "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int bin = 0;\n    for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n        bin = image[i];\n        atomicAdd(&bins[bin], 1);\n    }\n}"
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_pixel_histogram",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "correct_output": "// TODO: implement this function\n    // TODO: use MPI and OpenMP to parallelize the computation\n    // TODO: assume MPI has already been initialized\n    // TODO: every rank has a complete copy of image\n    // TODO: the result is stored in bins on rank 0\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = image.size();\n    int chunk = n / size;\n    int start = rank * chunk;\n    int end = (rank + 1) * chunk;\n    if (rank == size - 1) {\n        end = n;\n    }\n    std::vector<int> local_image(image.begin() + start, image.begin() + end);\n    std::array<size_t, 256> local_bins;\n    for (int i = 0; i < 256; i++) {\n        local_bins[i] = 0;\n    }\n    for (int i = 0; i < local_image.size(); i++) {\n        local_bins[local_image[i]]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the index of the first even number in the vector x.\n   Use Kokkos to parallelize the search. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(Kokkos::View<const int*> const& x) {",
        "correct_output": "\n    size_t index = x.extent(0); // Initialize with an invalid index\n    Kokkos::parallel_reduce(\"FindFirstEven\", x.extent(0), KOKKOS_LAMBDA(const int i, size_t& update) {\n        if (x(i) % 2 == 0) {\n            Kokkos::atomic_min(&update, i);\n        }\n    }, Kokkos::Min<size_t>(index));\n    return index;\n}\n"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "correct_output": "size_t result = x.size();\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      #pragma omp critical\n      {\n        if (i < result) {\n          result = i;\n        }\n      }\n    }\n  }\n  return result;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "serial",
        "prompt": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "correct_output": "for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "hip",
        "prompt": "/* Find the index of the first even number in the vector x. Store it in firstEvenIndex.\n   Use AMD HIP to parallelize the search. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\n__global__ void findFirstEven(const int *x, size_t N, size_t *firstEvenIndex) {",
        "correct_output": "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int stride = blockDim.x;\n    int start = bid * stride;\n    int end = min(start + stride, N);\n    int i = start + tid;\n    int even = 0;\n    while (i < end) {\n        if (x[i] % 2 == 0) {\n            even = 1;\n            break;\n        }\n        i += stride;\n    }\n    if (even) {\n        atomicMin(firstEvenIndex, start + tid);\n    }\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // every rank has a complete copy of x\n    std::vector<int> local_x(x);\n\n    // find the index of the first even number in the local copy\n    size_t local_index = 0;\n    while (local_index < local_x.size() && local_x[local_index] % 2!= 0) {\n        local_index++;\n    }\n\n    // find the index of the first even number in the global copy\n    int global_index = 0;\n    MPI_Allreduce(&local_index, &global_index, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    // return the index of the first even number in the global copy\n    return global_index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "cuda",
        "prompt": "/* Find the index of the first even number in the vector x. Store it in firstEvenIndex.\n   Use CUDA to parallelize the search. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\n__global__ void findFirstEven(const int *x, size_t N, size_t *firstEvenIndex) {",
        "correct_output": null
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "38_search_find_the_first_even_number",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI and OpenMP to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // each rank has a complete copy of x\n    std::vector<int> x_local = x;\n\n    // each rank finds the index of the first even number\n    size_t index = 0;\n    for (int i = 0; i < x_local.size(); i++) {\n        if (x_local[i] % 2 == 0) {\n            index = i;\n            break;\n        }\n    }\n\n    // each rank sends its result to rank 0\n    int result = index;\n    MPI_Reduce(&result, &index, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n    return index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {",
        "correct_output": "\n    bool found = false;\n    Kokkos::parallel_reduce(x.size(), KOKKOS_LAMBDA(int i, bool& lfound) {\n        if (x(i) == target) {\n            lfound = true;\n        }\n    }, found);\n    return found;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "correct_output": "bool found = false;\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] == target) {\n            found = true;\n        }\n    }\n    return found;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "serial",
        "prompt": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "correct_output": "return std::find(x.begin(), x.end(), target)!= x.end();\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "hip",
        "prompt": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {",
        "correct_output": "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] == target) {\n      *found = true;\n    }\n  }\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int local_size = x.size();\n  int local_target = target;\n\n  int local_result = 0;\n  for (int i = 0; i < local_size; i++) {\n    if (x[i] == local_target) {\n      local_result = 1;\n      break;\n    }\n  }\n\n  int global_result = 0;\n  MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  return global_result == 1;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "cuda",
        "prompt": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {",
        "correct_output": "// TODO: implement the kernel\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "36_search_check_if_array_contains_value",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
        "correct_output": "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk_size = n / size;\n  int start = rank * chunk_size;\n  int end = start + chunk_size;\n  if (rank == size - 1) {\n    end = n;\n  }\n  int result = 0;\n  #pragma omp parallel for reduction(+:result)\n  for (int i = start; i < end; i++) {\n    if (x[i] == target) {\n      result++;\n    }\n  }\n  int total_result;\n  MPI_Reduce(&result, &total_result, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  return total_result > 0;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, Kokkos::View<const int*> const& y, int val) {",
        "correct_output": null
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "correct_output": "int n = x.size();\n    int m = y.size();\n    int count = 0;\n\n    #pragma omp parallel for reduction(+:count)\n    for (int i = 0; i < n; i++) {\n        if (x[i] == val) {\n            count++;\n        }\n    }\n\n    #pragma omp parallel for reduction(+:count)\n    for (int i = 0; i < m; i++) {\n        if (y[i] == val) {\n            count++;\n        }\n    }\n\n    return count == 1;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "serial",
        "prompt": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "correct_output": "return std::find(x.begin(), x.end(), val)!= x.end() ^\n         std::find(y.begin(), y.end(), val)!= y.end();\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "hip",
        "prompt": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {",
        "correct_output": "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    __shared__ int x_shared[1024];\n    __shared__ int y_shared[1024];\n    if (gid < N) {\n        x_shared[tid] = x[gid];\n        y_shared[tid] = y[gid];\n    }\n    __syncthreads();\n    if (gid < N) {\n        found[gid] = (x_shared[tid] ^ y_shared[tid]) == val;\n    }\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int x_size = x.size();\n    int y_size = y.size();\n\n    int x_start = x_size / size * rank;\n    int x_end = x_size / size * (rank + 1);\n    int y_start = y_size / size * rank;\n    int y_end = y_size / size * (rank + 1);\n\n    std::vector<int> x_local(x.begin() + x_start, x.begin() + x_end);\n    std::vector<int> y_local(y.begin() + y_start, y.begin() + y_end);\n\n    int x_local_size = x_local.size();\n    int y_local_size = y_local.size();\n\n    int x_local_xor_size = 0;\n    int y_local_xor_size = 0;\n\n    for (int i = 0; i < x_local_size; i++) {\n        if (x_local[i] == val) {\n            x_local_xor_size++;\n        }\n    }\n\n    for (int i = 0; i < y_local_size; i++) {\n        if (y_local[i] == val) {\n            y_local_xor_size++;\n        }\n    }\n\n    int x_xor_size = 0;\n    int y_xor_size = 0;\n\n    MPI_Reduce(&x_local_xor_size, &x_xor_size, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&y_local_xor_size, &y_xor_size, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        if (x_xor_size == 1 || y_xor_size == 1) {\n            return true;\n        } else {\n            return false;\n        }\n    } else {\n        return false;\n    }\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "cuda",
        "prompt": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {",
        "correct_output": "int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + tid;\n  int found_local = 0;\n  while (i < N) {\n    found_local ^= (x[i] == val) | (y[i] == val);\n    i += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  if (tid == 0) {\n    *found = found_local;\n  }\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "39_search_xor_contains",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
        "correct_output": "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int x_size = x.size();\n  int y_size = y.size();\n\n  int x_start = x_size / size * rank;\n  int x_end = x_size / size * (rank + 1);\n  int y_start = y_size / size * rank;\n  int y_end = y_size / size * (rank + 1);\n\n  std::vector<int> x_local(x.begin() + x_start, x.begin() + x_end);\n  std::vector<int> y_local(y.begin() + y_start, y.begin() + y_end);\n\n  int result = 0;\n  #pragma omp parallel for reduction(|:result)\n  for (int i = 0; i < x_local.size(); i++) {\n    result |= (x_local[i] ^ val);\n  }\n  for (int i = 0; i < y_local.size(); i++) {\n    result |= (y_local[i] ^ val);\n  }\n\n  int global_result = 0;\n  MPI_Reduce(&result, &global_result, 1, MPI_INT, MPI_BOR, 0, MPI_COMM_WORLD);\n\n  return global_result == 0;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {",
        "correct_output": "size_t result = 0;\n   Kokkos::parallel_reduce(books.extent(0), KOKKOS_LAMBDA(size_t i, size_t& max_i) {\n      if (books(i).pages < 100) {\n         max_i = i;\n      }\n   }, Kokkos::Max<size_t>(result));\n   return result;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "correct_output": "size_t last_short_book_index = 0;\n   #pragma omp parallel for reduction(max:last_short_book_index)\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "correct_output": "size_t last_short_book_index = 0;\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "hip",
        "prompt": "struct Book {\n   const char* title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use AMD HIP to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {",
        "correct_output": "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   int stride = blockDim.x;\n   int start = bid * stride + tid;\n   int end = min(start + stride, N);\n   int last = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         last = i;\n      }\n   }\n   __syncthreads();\n   if (tid == 0) {\n      *lastShortBookIndex = last;\n   }\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "correct_output": "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int n = books.size();\n   int chunk = n / size;\n   int remainder = n % size;\n\n   int start = rank * chunk;\n   int end = start + chunk;\n\n   if (rank < remainder) {\n      end++;\n   }\n\n   int local_result = -1;\n   for (int i = start; i < end; i++) {\n      if (books[i].pages < 100) {\n         local_result = i;\n      }\n   }\n\n   int global_result = -1;\n   MPI_Reduce(&local_result, &global_result, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   return global_result;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "cuda",
        "prompt": "struct Book {\n   const char* title;\n   int pages;\n};\n\n/* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {",
        "correct_output": "\n// Thread index\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n\n// Check if the thread index is within the bounds of the books array\nif (i < N) {\n    // Check if the book has less than 100 pages\n    if (books[i].pages < 100) {\n        // Use atomicMax to safely find the largest index among all threads\n        atomicMax((int*)lastShortBookIndex, i);\n    }\n}\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "35_search_search_for_last_struct_by_key",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI and OpenMP to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
        "correct_output": "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // divide the work among the ranks\n   size_t chunk = books.size() / size;\n   size_t remainder = books.size() % size;\n\n   // rank 0 will search from the beginning of the vector\n   size_t start = rank * chunk;\n\n   // rank 0 will search until the end of the vector\n   size_t end = start + chunk;\n\n   // rank 0 will search until the end of the vector\n   // rank 1 will search until the end of the vector + remainder\n   if (rank < remainder) {\n      end += 1;\n   }\n\n   // search in parallel\n   size_t result = 0;\n   #pragma omp parallel for reduction(max: result)\n   for (size_t i = start; i < end; ++i) {\n      if (books[i].pages < 100) {\n         result = i;\n      }\n   }\n\n   // gather the results from all ranks\n   size_t result_global;\n   MPI_Reduce(&result, &result_global, 1, MPI_UNSIGNED_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   return result_global;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(Kokkos::View<const double*> const& x) {",
        "correct_output": "size_t closest_index = 0;\n  double closest_distance = std::abs(M_PI - x(0));\n\n  for (size_t i = 1; i < x.extent(0); ++i) {\n    double distance = std::abs(M_PI - x(i));\n    if (distance < closest_distance) {\n      closest_index = i;\n      closest_distance = distance;\n    }\n  }\n\n  return closest_index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "correct_output": "size_t closest_index = 0;\n  double closest_distance = std::abs(x[0] - M_PI);\n  for (size_t i = 1; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < closest_distance) {\n      closest_distance = distance;\n      closest_index = i;\n    }\n  }\n  return closest_index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "serial",
        "prompt": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "correct_output": "double min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < min_distance) {\n      min_distance = distance;\n      min_index = i;\n    }\n  }\n  return min_index;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "hip",
        "prompt": "/* Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n   Use M_PI for the value of PI.\n   Use AMD HIP to search in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\n__global__ void findClosestToPi(const double *x, size_t N, size_t *closestToPiIndex) {",
        "correct_output": null
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "correct_output": "size_t result = 0;\n  double min_distance = std::numeric_limits<double>::max();\n\n  for (size_t i = 0; i < x.size(); i++) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < min_distance) {\n      min_distance = distance;\n      result = i;\n    }\n  }\n\n  return result;\n}"
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "cuda",
        "prompt": "/* Find the index of the value in the vector x that is closest to the math constant PI. Store the index in closestToPiIndex.\n   Use M_PI for the value of PI.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\n__global__ void findClosestToPi(const double *x, size_t N, size_t *closestToPiIndex) {",
        "correct_output": null
    },
    {
        "problem_type": "search",
        "language": "cpp",
        "name": "37_search_find_the_closest_number_to_pi",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI and OpenMP to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
        "correct_output": "size_t closest_index = 0;\n  double closest_distance = std::abs(x[0] - M_PI);\n  for (size_t i = 1; i < x.size(); i++) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < closest_distance) {\n      closest_distance = distance;\n      closest_index = i;\n    }\n  }\n  return closest_index;\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(Kokkos::View<Kokkos::complex<double>*> const& x, Kokkos::View<double*> &r, Kokkos::View<double*> &i) {",
        "correct_output": "\n// Assuming the size of the input is a power of 2\nint N = x.extent(0);\n\n// Create a deep copy of the input array to work on\nKokkos::View<Kokkos::complex<double>*> X(\"X\", N);\nKokkos::deep_copy(X, x);\n\n// Perform the Cooley-Tukey decimation-in-time FFT\nKokkos::parallel_for(\"bit_reversal\", N, KOKKOS_LAMBDA(const int n) {\n    unsigned int rev = 0;\n    for (int i = 0, j = n; i < log2(N); ++i, j >>= 1) {\n        rev = (rev << 1) | (j & 1);\n    }\n    if (rev > n) {\n        auto temp = X[n];\n        X[n] = X[rev];\n        X[rev] = temp;\n    }\n});\n\nKokkos::fence();\n\nfor (int s = 2; s <= N; s *= 2) {\n    int m = s / 2;\n    Kokkos::complex<double> wm(cos(-2 * M_PI / s), sin(-2 * M_PI / s));\n    for (int k = 0; k < N; k += s) {\n        Kokkos::complex<double> w(1, 0);\n        for (int j = 0; j < m; j++) {\n            Kokkos::parallel_for(\"fft_compute\", 1, KOKKOS_LAMBDA(const int&) {\n                Kokkos::complex<double> t = w * X[k + j + m];\n                Kokkos::complex<double> u = X[k + j];\n                X[k + j] = u + t;\n                X[k + j + m] = u - t;\n            });\n            Kokkos::fence();\n            w = w * wm;\n        }\n    }\n}\n\n// Extract the real and imaginary parts\nKokkos::parallel_for(\"extract_real_imag\", N, KOKKOS_LAMBDA(const int n) {\n    r(n) = X[n].real();\n    i(n) = X[n].imag();\n});\nKokkos::fence();\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "correct_output": "int n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "hip",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\n__global__ void fft(const hipDoubleComplex *x, double *r, double *i, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\n__global__ void fft(const cuDoubleComplex *x, double *r, double *i, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "08_fft_split_fft",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
        "correct_output": "int n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "hip",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fftConjugate(hipDoubleComplex *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fftConjugate(cuDoubleComplex *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "07_fft_fft_conjugate",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(Kokkos::View<Kokkos::complex<double>*> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "correct_output": "// Compute the inverse Fourier Transform by performing the regular Fourier Transform on the complex conjugate of the input\n    // and then taking the complex conjugate of the result\n    int n = x.size();\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Compute the Fourier Transform in-place\n    fft(x);\n\n    // Take the complex conjugate of the result\n    for (int i = 0; i < n; i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide by n, as the inverse Fourier Transform is scaled by 1/n\n    for (int i = 0; i < n; i++) {\n        x[i] /= n;\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "serial",
        "prompt": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "correct_output": "\n// Take conjugate of the complex numbers\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Apply fft\nfft(x);\n\n// Take conjugate again\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Scale the numbers\nfor (auto &elem : x) {\n    elem /= static_cast<double>(x.size());\n}\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "hip",
        "prompt": "/* Compute the inverse fourier transform of x in-place.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\n__global__ void ifft(hipDoubleComplex *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "correct_output": "\n    // Get the size of the input vector\n    int N = x.size();\n\n    // Take the conjugate of the complex numbers\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Perform the forward FFT\n    fft(x);\n\n    // Take the conjugate again\n    for (auto &elem : x) {\n        elem = std::conj(elem);\n    }\n\n    // Divide by the size to get the actual inverse FFT\n    for (auto &elem : x) {\n        elem /= N;\n    }\n\n    // Gather the results to rank 0\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    std::vector<std::complex<double>> gathered_x(N * world_size);\n    MPI_Gather(x.data(), N * sizeof(std::complex<double>), MPI_BYTE,\n               gathered_x.data(), N * sizeof(std::complex<double>), MPI_BYTE,\n               0, MPI_COMM_WORLD);\n\n    // If this is rank 0, copy the gathered results back into x\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    if (world_rank == 0) {\n        std::copy(gathered_x.begin(), gathered_x.begin() + N, x.begin());\n    }\n}\n"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the inverse fourier transform of x in-place.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\n__global__ void ifft(cuDoubleComplex *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "05_fft_inverse_fft",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
        "correct_output": "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Conjugate the complex numbers\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Forward FFT\n    fft(x);\n\n    // Conjugate the complex numbers again\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        x[i] = std::conj(x[i]);\n    }\n\n    // Divide by the number of elements\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        x[i] /= static_cast<double>(x.size());\n    }\n\n    // Gather results at rank 0\n    std::vector<std::complex<double>> all_x(x.size() * size);\n    MPI_Gather(x.data(), x.size() * sizeof(std::complex<double>), MPI_BYTE,\n               all_x.data(), x.size() * sizeof(std::complex<double>), MPI_BYTE,\n               0, MPI_COMM_WORLD);\n\n    // Only rank 0 should now have the complete result\n    if (rank == 0) {\n        x = std::vector<std::complex<double>>(all_x.begin(), all_x.begin() + x.size());\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(Kokkos::View<const double*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": "int n = x.size();\n  output.resize(n);\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    std::complex<double> sum(0, 0);\n    for (int j = 0; j < n; j++) {\n      sum += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / n));\n    }\n    output[i] = sum;\n  }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "serial",
        "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": "// TODO: implement this function\n    output.resize(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "hip",
        "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\n__global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": "\n// Get the size of the global communicator\nint world_size;\nMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n// Get the rank of the current process\nint world_rank;\nMPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n// Determine the number of elements each process will work on\nint n = x.size();\nint local_n = n / world_size;\nint start_index = world_rank * local_n;\nint end_index = (world_rank + 1) * local_n;\n\n// Handle the case where n is not divisible by world_size\nif (world_rank == world_size - 1) {\n    end_index = n;\n}\n\n// Allocate space for the local output\nstd::vector<std::complex<double>> local_output(local_n);\n\n// Compute the DFT for the local portion\nfor (int k = start_index; k < end_index; ++k) {\n    std::complex<double> sum(0.0, 0.0);\n    for (int t = 0; t < n; ++t) {\n        double angle = -2 * M_PI * t * k / n;\n        std::complex<double> w(cos(angle), sin(angle));\n        sum += x[t] * w;\n    }\n    local_output[k - start_index] = sum;\n}\n\n// Gather the local DFT results into the global output vector\nstd::vector<std::complex<double>> all_output;\nif (world_rank == 0) {\n    all_output.resize(n);\n}\n\nMPI_Gather(local_output.data(), local_n * sizeof(std::complex<double>), MPI_BYTE,\n           all_output.data(), local_n * sizeof(std::complex<double>), MPI_BYTE,\n           0, MPI_COMM_WORLD);\n\n// If this is the root process, copy the gathered results to the output vector\nif (world_rank == 0) {\n    output = all_output;\n}\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\n__global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {",
        "correct_output": "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex sum = {0, 0};\n    for (size_t k = 0; k < N; ++k) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex exp_part = {cos(angle), sin(angle)};\n        cuDoubleComplex term = cuCmul(exp_part, make_cuDoubleComplex(x[k], 0));\n        sum = cuCadd(sum, term);\n    }\n    output[tid] = sum;\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "06_fft_dft",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int N = x.size();\n    double theta;\n\n    // Allocate memory for the output array\n    output.resize(N);\n\n    // Each process computes a part of the output\n    #pragma omp parallel for private(theta)\n    for (int k = 0; k < N; ++k) {\n        std::complex<double> sum(0.0, 0.0);\n        for (int n = 0; n < N; ++n) {\n            theta = -2 * M_PI * k * n / N;\n            std::complex<double> exp_part(cos(theta), sin(theta));\n            sum += x[n] * exp_part;\n        }\n        output[k] = sum;\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<const Kokkos::complex<double>*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
        "correct_output": "\n// Assuming the size of the input is a power of 2\nint N = x.extent(0);\n\n// Create a deep copy of the input as we need to manipulate the data\nKokkos::View<Kokkos::complex<double>*> a(\"a\", N);\nKokkos::deep_copy(a, x);\n\n// Bit-reversal permutation\nauto bit_reverse = KOKKOS_LAMBDA(const int i) {\n    unsigned int n = i;\n    unsigned int rev = 0;\n    for (int j = 0; j < std::log2(N); ++j) {\n        rev = (rev << 1) | (n & 1);\n        n >>= 1;\n    }\n    if (rev > i) {\n        auto temp = a[i];\n        a[i] = a[rev];\n        a[rev] = temp;\n    }\n};\nKokkos::parallel_for(\"bit_reverse\", N, bit_reverse);\n\n// Cooley-Tukey FFT\nfor (int s = 1; s <= std::log2(N); ++s) {\n    int m = 1 << s;\n    Kokkos::complex<double> wm = exp(Kokkos::complex<double>(0, -2 * M_PI / m));\n    for (int k = 0; k < N; k += m) {\n        Kokkos::complex<double> w = 1;\n        for (int j = 0; j < m / 2; ++j) {\n            Kokkos::complex<double> t = w * a[k + j + m / 2];\n            Kokkos::complex<double> u = a[k + j];\n            a[k + j] = u + t;\n            a[k + j + m / 2] = u - t;\n            w *= wm;\n        }\n    }\n}\n\n// Copy the result to the output\nKokkos::deep_copy(output, a);\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": "int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // divide and conquer\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int i = 0; i < n/2; i++) {\n        x_even[i] = x[2*i];\n        x_odd[i] = x[2*i+1];\n    }\n\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine\n    for (int i = 0; i < n/2; i++) {\n        output[i] = y_even[i] + std::exp(std::complex<double>(0, -2*M_PI*i/n)) * y_odd[i];\n        output[i+n/2] = y_even[i] - std::exp(std::complex<double>(0, -2*M_PI*i/n)) * y_odd[i];\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "serial",
        "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": "int n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "hip",
        "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(const hipDoubleComplex *x, hipDoubleComplex *output, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *output, size_t N) {",
        "correct_output": "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        cuDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex a = x[k];\n            cuDoubleComplex b = cuCmul(make_cuDoubleComplex(cos(2 * M_PI * idx * k / N), -sin(2 * M_PI * idx * k / N)), a);\n            sum = cuCadd(sum, b);\n        }\n        output[idx] = sum;\n    }\n}"
    },
    {
        "problem_type": "fft",
        "language": "cpp",
        "name": "09_fft_fft_out_of_place",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) { z(i) = alpha * x(i) + y(i); });\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "correct_output": "int n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "serial",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "correct_output": "for (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "hip",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {",
        "correct_output": "int i = threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        for (int i = 0; i < x.size(); i++) {\n            z[i] = alpha * x[i] + y[i];\n        }\n    } else {\n        for (int i = 0; i < x.size(); i++) {\n            z[i] = alpha * x[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "cuda",
        "prompt": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "03_dense_la_axpy",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
        "correct_output": "int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    z.resize(n);\n    if (rank == 0) {\n        #pragma omp parallel for\n        for (int i = 0; i < n; ++i) {\n            z[i] = alpha * x[i] + y[i];\n        }\n    } else {\n        #pragma omp parallel for\n        for (int i = 0; i < n; ++i) {\n            z[i] = alpha * x[i];\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix. x and b have N elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(Kokkos::View<const double**> &A, Kokkos::View<const double*> &b, Kokkos::View<double*> &x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": "\n// Check if the input matrix A is square and the dimensions match\nif (A.size() != N * N || b.size() != N || x.size() != N) {\n    throw std::invalid_argument(\"Invalid dimensions for matrix A or vectors b, x.\");\n}\n\n// Initialize x with zeros\nstd::fill(x.begin(), x.end(), 0.0);\n\n// Temporary variables for the matrix inversion process\nstd::vector<double> tempA(A); // Copy of A for transformations\nstd::vector<double> tempB(b); // Copy of b for transformations\n\n// Perform Gaussian elimination with partial pivoting\nfor (size_t i = 0; i < N; ++i) {\n    // Find the pivot row\n    size_t pivot = i;\n    double maxElement = std::abs(tempA[i * N + i]);\n    for (size_t j = i + 1; j < N; ++j) {\n        double element = std::abs(tempA[j * N + i]);\n        if (element > maxElement) {\n            maxElement = element;\n            pivot = j;\n        }\n    }\n\n    // Swap the pivot row with the current row\n    if (pivot != i) {\n        for (size_t j = 0; j < N; ++j) {\n            std::swap(tempA[i * N + j], tempA[pivot * N + j]);\n        }\n        std::swap(tempB[i], tempB[pivot]);\n    }\n\n    // Perform elimination\n    #pragma omp parallel for\n    for (size_t j = i + 1; j < N; ++j) {\n        double factor = tempA[j * N + i] / tempA[i * N + i];\n        for (size_t k = i; k < N; ++k) {\n            tempA[j * N + k] -= factor * tempA[i * N + k];\n        }\n        tempB[j] -= factor * tempB[i];\n    }\n}\n\n// Back substitution\nfor (int i = N - 1; i >= 0; --i) {\n    double sum = 0.0;\n    for (size_t j = i + 1; j < N; ++j) {\n        sum += tempA[i * N + j] * x[j];\n    }\n    x[i] = (tempB[i] - sum) / tempA[i * N + i];\n}\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "serial",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": "// check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "hip",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\n__global__ void solveLinearSystem(const double *A, const double *b, double *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "cuda",
        "prompt": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\n__global__ void solveLinearSystem(const double *A, const double *b, double *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "01_dense_la_solve",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, M), [&] (const int i) {\n        for (size_t j = 0; j < N; j++) {\n            double sum = 0;\n            for (size_t k = 0; k < K; k++) {\n                sum += A(i, k) * B(k, j);\n            }\n            C(i, j) = sum;\n        }\n    });\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "correct_output": "// TODO: implement this function\n    // Hint: use OpenMP to parallelize the computation\n#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "serial",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "correct_output": "for (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "hip",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {",
        "correct_output": "int row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A[row * K + k] * B[k * N + col];\n    }\n    C[row * N + col] = sum;\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t block_size = M / size;\n\n    std::vector<double> local_A(A.begin() + rank * block_size * K, A.begin() + (rank + 1) * block_size * K);\n    std::vector<double> local_C(block_size * N);\n\n    for (size_t i = 0; i < block_size; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            for (size_t k = 0; k < K; ++k) {\n                local_C[i * N + j] += local_A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n\n    if (rank == 0) {\n        C.resize(M * N);\n    }\n\n    MPI_Gather(local_C.data(), block_size * N, MPI_DOUBLE, C.data(), block_size * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "cuda",
        "prompt": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {",
        "correct_output": "int row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A[row * K + k] * B[k * N + col];\n    }\n    C[row * N + col] = sum;\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "02_dense_la_gemm",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the number of rows each process will work on\n    size_t rows_per_proc = M / size;\n\n    // Each process calculates its part of the result\n    #pragma omp parallel for\n    for (size_t i = rank * rows_per_proc; i < (rank + 1) * rows_per_proc; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            C[i * N + j] = 0;\n            for (size_t k = 0; k < K; ++k) {\n                C[i * N + j] += A[i * K + k] * B[k * N + j];\n            }\n        }\n    }\n\n    // Gather all the results to rank 0\n    if (rank!= 0) {\n        MPI_Send(&C[rank * rows_per_proc * N], rows_per_proc * N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&C[i * rows_per_proc * N], rows_per_proc * N, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {",
        "correct_output": "Kokkos::View<double*, Kokkos::HostSpace> y_h(\"y_h\", M);\n  Kokkos::deep_copy(y_h, y);\n  for (size_t i = 0; i < M; i++) {\n    y_h(i) = 0;\n    for (size_t j = 0; j < N; j++) {\n      y_h(i) += A(i, j) * x(j);\n    }\n  }\n  Kokkos::deep_copy(y, y_h);\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "#pragma omp parallel for\n    for (size_t i = 0; i < M; ++i) {\n        y[i] = 0;\n        for (size_t j = 0; j < N; ++j) {\n            y[i] += A[i*N + j] * x[j];\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "serial",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "for (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "hip",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {",
        "correct_output": "int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0;\n    for (int col = 0; col < N; col++) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // each rank has a complete copy of A and x\n    // the result is stored in y on rank 0\n    if (rank == 0) {\n        for (size_t i = 0; i < M; ++i) {\n            y[i] = 0;\n            for (size_t j = 0; j < N; ++j) {\n                y[i] += A[i * N + j] * x[j];\n            }\n        }\n    } else {\n        for (size_t i = 0; i < M; ++i) {\n            for (size_t j = 0; j < N; ++j) {\n                y[i] += A[i * N + j] * x[j];\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "cuda",
        "prompt": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {",
        "correct_output": "int row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "04_dense_la_gemv",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
        "correct_output": "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    size_t block_size = M / world_size;\n    size_t start = block_size * world_rank;\n    size_t end = (world_rank == world_size - 1)? M : start + block_size;\n\n    #pragma omp parallel for\n    for (size_t i = start; i < end; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n\n    std::vector<double> recvbuf(M);\n    MPI_Gather(&y[start], block_size, MPI_DOUBLE, recvbuf.data(), block_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        for (int i = 1; i < world_size; ++i) {\n            size_t offset = i * block_size;\n            for (size_t j = 0; j < block_size; ++j) {\n                y[offset + j] = recvbuf[offset + j];\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(Kokkos::View<double**> &A, size_t N) {",
        "correct_output": "using namespace Kokkos;\n\n  // LU factorization\n  for (size_t i = 0; i < N; i++) {\n    for (size_t j = i; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < i; k++)\n        sum += A(i, k) * A(k, j);\n      A(i, j) -= sum;\n    }\n\n    for (size_t j = i + 1; j < N; j++) {\n      double sum = 0.0;\n      for (size_t k = 0; k < i; k++)\n        sum += A(j, k) * A(k, i);\n      A(j, i) = (A(j, i) - sum) / A(i, i);\n    }\n  }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "correct_output": "for (size_t i = 0; i < N; i++) {\n        for (size_t j = i + 1; j < N; j++) {\n            A[j * N + i] /= A[i * N + i];\n            for (size_t k = i + 1; k < N; k++) {\n                A[j * N + k] -= A[j * N + i] * A[i * N + k];\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "serial",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "correct_output": "for (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "hip",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\n__global__ void luFactorize(double *A, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "correct_output": "for (size_t k = 0; k < N; ++k) {\n        for (size_t i = k + 1; i < N; ++i) {\n            A[i * N + k] /= A[k * N + k];\n            for (size_t j = k + 1; j < N; ++j) {\n                A[i * N + j] -= A[i * N + k] * A[k * N + j];\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "cuda",
        "prompt": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\n__global__ void luFactorize(double *A, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "dense_la",
        "language": "cpp",
        "name": "00_dense_la_lu_decomp",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Perform LU factorization\n    for (size_t i = 0; i < N; ++i) {\n        // Compute U(i, i)\n        for (size_t j = i; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < i; ++k) {\n                sum += A[i * N + k] * A[k * N + j];\n            }\n            A[i * N + j] -= sum;\n        }\n\n        // Compute L(i, j)\n        for (size_t j = i + 1; j < N; ++j) {\n            double sum = 0.0;\n            for (size_t k = 0; k < i; ++k) {\n                sum += A[j * N + k] * A[k * N + i];\n            }\n            A[j * N + i] = (A[j * N + i] - sum) / A[i * N + i];\n        }\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {",
        "correct_output": "\n// Create a mirror view to work with on the host\nauto host_x = Kokkos::create_mirror_view(x);\nKokkos::deep_copy(host_x, x);\n\n// Create a vector to hold the sorted indices\nstd::vector<size_t> indices(host_x.size());\nstd::iota(indices.begin(), indices.end(), 0);\n\n// Sort the indices based on the values in x\nstd::sort(indices.begin(), indices.end(),\n    [&](size_t i1, size_t i2) { return host_x(i1) < host_x(i2); });\n\n// Create a mirror view for ranks to work with on the host\nauto host_ranks = Kokkos::create_mirror_view(ranks);\n\n// Assign ranks based on the sorted indices\nfor (size_t i = 0; i < indices.size(); ++i) {\n    host_ranks(indices[i]) = i;\n}\n\n// Copy the results back to the device\nKokkos::deep_copy(ranks, host_ranks);\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "correct_output": "// TODO: implement this function\n    size_t n = x.size();\n    ranks.resize(n);\n    std::vector<size_t> indices(n);\n    for (size_t i = 0; i < n; i++) {\n        indices[i] = i;\n    }\n    std::sort(indices.begin(), indices.end(), [&](size_t i, size_t j) {\n        return x[i] < x[j];\n    });\n    for (size_t i = 0; i < n; i++) {\n        ranks[indices[i]] = i;\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "serial",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "correct_output": "std::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "hip",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    float val = x[i];\n    size_t rank = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (x[j] < val) {\n        rank++;\n      }\n    }\n    ranks[i] = rank;\n  }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "correct_output": "int n = x.size();\n    ranks.resize(n);\n    std::vector<float> x_sorted(x);\n    std::sort(x_sorted.begin(), x_sorted.end());\n    for (int i = 0; i < n; ++i) {\n        ranks[i] = std::distance(x_sorted.begin(), std::find(x_sorted.begin(), x_sorted.end(), x[i]));\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "cuda",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {",
        "correct_output": "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        float val = x[idx];\n        size_t rank = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[i] <= val) {\n                ++rank;\n            }\n        }\n        ranks[idx] = rank - 1;\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "42_sort_sorted_ranks",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "correct_output": "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Each process sorts its own copy of x\n    std::vector<std::pair<float, size_t>> x_with_indices(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        x_with_indices[i] = {x[i], i};\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x_with_indices.size(); ++i) {\n        x_with_indices[i] = {x_with_indices[i].first, i};\n    }\n\n    std::sort(x_with_indices.begin(), x_with_indices.end());\n\n    // Process 0 collects the sorted indices from all processes\n    if (world_rank == 0) {\n        ranks.resize(x.size());\n    }\n\n    for (size_t i = 0; i < x_with_indices.size(); ++i) {\n        size_t index = x_with_indices[i].second;\n        if (world_rank == 0) {\n            ranks[index] = i;\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(Kokkos::View<int*> &x) {",
        "correct_output": "\n// First, count the non-zero elements\nint nonZeroCount = 0;\nfor (size_t i = 0; i < x.extent(0); ++i) {\n    if (x(i) != 0) {\n        ++nonZeroCount;\n    }\n}\n\n// Create a view to hold non-zero elements\nKokkos::View<int*> nonZeroElements(\"nonZeroElements\", nonZeroCount);\n\n// Copy non-zero elements into the new view\nint index = 0;\nfor (size_t i = 0; i < x.extent(0); ++i) {\n    if (x(i) != 0) {\n        nonZeroElements(index++) = x(i);\n    }\n}\n\n// Sort the non-zero elements\nKokkos::sort(nonZeroElements);\n\n// Merge sorted non-zero elements back into the original array\nindex = 0;\nfor (size_t i = 0; i < x.extent(0); ++i) {\n    if (x(i) != 0) {\n        x(i) = nonZeroElements(index++);\n    }\n}\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "correct_output": "int n = x.size();\n    std::vector<int> non_zero_elements;\n    std::vector<int> zero_locations;\n\n    // Step 1: Separate non-zero and zero elements\n    #pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        if (x[i] == 0) {\n            #pragma omp critical\n            zero_locations.push_back(i);\n        } else {\n            #pragma omp critical\n            non_zero_elements.push_back(x[i]);\n        }\n    }\n\n    // Step 2: Sort non-zero elements\n    std::sort(non_zero_elements.begin(), non_zero_elements.end());\n\n    // Step 3: Re-insert non-zero elements into original array\n    int j = 0;\n    for (int i = 0; i < n; ++i) {\n        if (std::find(zero_locations.begin(), zero_locations.end(), i)!= zero_locations.end()) {\n            x[i] = 0;\n        } else {\n            x[i] = non_zero_elements[j++];\n        }\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "correct_output": "int n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < n) {\n    if (x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < n) {\n        if (x[j] == 0) {\n          j++;\n        } else {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "hip",
        "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Remove zero elements from x\n    std::vector<int> non_zero_x;\n    for (int val : x) {\n        if (val!= 0) {\n            non_zero_x.push_back(val);\n        }\n    }\n\n    // Sort non_zero_x\n    std::sort(non_zero_x.begin(), non_zero_x.end());\n\n    // Gather all sorted non-zero elements to rank 0\n    int local_size = non_zero_x.size();\n    std::vector<int> recvcounts(size);\n    MPI_Gather(&local_size, 1, MPI_INT, recvcounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::vector<int> offsets(size);\n    offsets[0] = 0;\n    for (int i = 1; i < size; ++i) {\n        offsets[i] = offsets[i - 1] + recvcounts[i - 1];\n    }\n\n    std::vector<int> sorted_x(offsets.back() + local_size);\n    MPI_Gatherv(non_zero_x.data(), local_size, MPI_INT, sorted_x.data(), recvcounts.data(), offsets.data(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Reinsert zero elements\n        std::vector<int> final_x(x.size());\n        int sorted_index = 0;\n        for (int i = 0; i < x.size(); ++i) {\n            if (x[i] == 0) {\n                final_x[i] = 0;\n            } else {\n                final_x[i] = sorted_x[sorted_index++];\n            }\n        }\n        x = final_x;\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "44_sort_sort_non-zero_elements",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI and OpenMP to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "correct_output": "\n// Determine the rank and size of the MPI environment\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n// Extract non-zero elements and sort them\nstd::vector<int> non_zero_elements;\nfor (int i : x) {\n    if (i != 0) {\n        non_zero_elements.push_back(i);\n    }\n}\nstd::sort(non_zero_elements.begin(), non_zero_elements.end());\n\n// Gather the sorted non-zero elements at rank 0\nstd::vector<int> gathered_elements;\nint local_count = non_zero_elements.size();\nstd::vector<int> counts(size);\nMPI_Gather(&local_count, 1, MPI_INT, counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\nint total_count = 0;\nstd::vector<int> displs(size);\nif (rank == 0) {\n    for (int count : counts) {\n        displs.push_back(total_count);\n        total_count += count;\n    }\n    gathered_elements.resize(total_count);\n}\n\nMPI_Gatherv(non_zero_elements.data(), local_count, MPI_INT,\n             gathered_elements.data(), counts.data(), displs.data(), MPI_INT,\n             0, MPI_COMM_WORLD);\n\n// Merge the sorted non-zero elements back into x on rank 0\nif (rank == 0) {\n    int sorted_index = 0;\n    for (int &element : x) {\n        if (element != 0) {\n            element = gathered_elements[sorted_index++];\n        }\n    }\n}\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Find the k-th smallest element of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(Kokkos::View<const int*> const& x, int k) {",
        "correct_output": "// Create a copy of x and sort it\n  Kokkos::View<int*> sorted_x(\"sorted_x\", x.extent(0));\n  Kokkos::parallel_for(\"copy_and_sort\", Kokkos::RangePolicy<>(0, x.extent(0)), KOKKOS_LAMBDA(int i) {\n    sorted_x(i) = x(i);\n  });\n  Kokkos::sort(sorted_x);\n\n  // Return the k-th smallest element\n  return sorted_x(k - 1);\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "correct_output": "// Create a copy of the vector and sort it\n    std::vector<int> sorted_x = x;\n    std::sort(sorted_x.begin(), sorted_x.end());\n\n    // Return the k-th smallest element\n    return sorted_x[k - 1];\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "serial",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "correct_output": "int n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "hip",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int *kthSmallest) {",
        "correct_output": "\n    int tid = threadIdx.x;\n    if (tid < N) {\n        int count = 0;\n        for (int i = 0; i < N; i++) {\n            if (x[i] < x[tid] || (x[i] == x[tid] && i < tid)) {\n                count++;\n            }\n        }\n        if (count == k - 1) {\n            *kthSmallest = x[tid];\n        }\n    }\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "correct_output": "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // step 1: sort the vector x\n  std::vector<int> x_sorted = x;\n  std::sort(x_sorted.begin(), x_sorted.end());\n\n  // step 2: find the k-th smallest element\n  int k_smallest = x_sorted[k - 1];\n\n  return k_smallest;\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "cuda",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int *kthSmallest) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "41_sort_k-th_smallest_element",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "correct_output": "int n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // each rank has a complete copy of x\n  std::vector<int> local_x(x);\n\n  // sort the local copy of x\n  std::sort(local_x.begin(), local_x.end());\n\n  // find the k-th smallest element of the local copy of x\n  int kth_smallest = local_x[k - 1];\n\n  // gather the k-th smallest element from all ranks\n  int kth_smallest_global;\n  MPI_Reduce(&kth_smallest, &kth_smallest_global, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n  return kth_smallest_global;\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(Kokkos::View<Result*> &results) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "correct_output": "// Define the lambda function to be used by the sort function\n    auto compare = [](const Result &a, const Result &b) {\n        return a.startTime < b.startTime;\n    };\n\n    // Use OpenMP to sort the vector in parallel\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            std::sort(results.begin(), results.end(), compare);\n        }\n    }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "correct_output": "std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "hip",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "correct_output": "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // sort on rank 0, broadcast to all ranks\n   if (rank == 0) {\n      std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n   }\n\n   // broadcast sorted results to all ranks\n   MPI_Bcast(results.data(), results.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "cuda",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "43_sort_sort_an_array_of_structs_by_key",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the outputs in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "correct_output": "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // sort the vector on rank 0\n   if (rank == 0) {\n      std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n         return a.startTime < b.startTime;\n      });\n   }\n\n   // broadcast the sorted vector to all ranks\n   MPI_Bcast(results.data(), results.size(), MPI_FLOAT, 0, MPI_COMM_WORLD);\n\n   // sort the vector on all ranks\n   #pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n         }\n      }\n   }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x of complex numbers by their magnitude in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "correct_output": "\n// Create a mirror view to work on the host\nauto host_x = Kokkos::create_mirror_view(x);\nKokkos::deep_copy(host_x, x);\n\n// Sort on host using std::sort and a lambda function to compare magnitudes\nstd::sort(host_x.data(), host_x.data() + host_x.size(),\n    [](const Kokkos::complex<double>& a, const Kokkos::complex<double>& b) {\n        return Kokkos::abs(a) < Kokkos::abs(b);\n    });\n\n// Copy the sorted data back to the device\nKokkos::deep_copy(x, host_x);\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "correct_output": "// Step 1: Calculate the magnitudes of the complex numbers\n    std::vector<double> magnitudes(x.size());\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        magnitudes[i] = std::abs(x[i]);\n    }\n\n    // Step 2: Sort the indices of the magnitudes in ascending order\n    std::vector<int> indices(x.size());\n    for (int i = 0; i < x.size(); ++i) {\n        indices[i] = i;\n    }\n    std::sort(indices.begin(), indices.end(), [&](int a, int b) {\n        return magnitudes[a] < magnitudes[b];\n    });\n\n    // Step 3: Reorder the complex numbers according to the sorted indices\n    std::vector<std::complex<double>> sorted_x(x.size());\n    for (int i = 0; i < x.size(); ++i) {\n        sorted_x[i] = x[indices[i]];\n    }\n\n    // Step 4: Replace the original vector with the sorted one\n    x = sorted_x;\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "correct_output": "std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "hip",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(hipDoubleComplex *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "correct_output": "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // get the number of elements to be sorted\n  int n = x.size();\n\n  // get the number of elements to be sorted by each rank\n  int n_local = n / size;\n\n  // get the number of elements to be sorted by the last rank\n  int n_last = n - n_local * (size - 1);\n\n  // get the starting index of the local array\n  int start = n_local * rank;\n\n  // get the ending index of the local array\n  int end = start + n_local;\n\n  // get the starting index of the last array\n  int start_last = n_local * (size - 1);\n\n  // get the ending index of the last array\n  int end_last = start_last + n_last;\n\n  // sort the local array\n  std::sort(x.begin() + start, x.begin() + end,\n            [](const std::complex<double> &a, const std::complex<double> &b) {\n              return std::abs(a) < std::abs(b);\n            });\n\n  // sort the last array\n  std::sort(x.begin() + start_last, x.begin() + end_last,\n            [](const std::complex<double> &a, const std::complex<double> &b) {\n              return std::abs(a) < std::abs(b);\n            });\n\n  // gather the sorted arrays from all ranks\n  MPI_Gather(x.data() + start, n_local, MPI_DOUBLE_COMPLEX, x.data(), n_local,\n             MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n  // sort the array on rank 0\n  if (rank == 0) {\n    std::sort(x.begin(), x.end(),\n              [](const std::complex<double> &a, const std::complex<double> &b) {\n                return std::abs(a) < std::abs(b);\n              });\n  }\n}"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(cuDoubleComplex *x, size_t N) {",
        "correct_output": null
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "correct_output": "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Calculate the local size for each rank\n    int local_size = x.size() / world_size;\n    if (world_rank == world_size - 1) {\n        local_size += x.size() % world_size;\n    }\n\n    // Sort the local vector\n    std::vector<std::complex<double>> local_x(x.begin() + world_rank * local_size,\n                                             x.begin() + (world_rank + 1) * local_size);\n    std::sort(local_x.begin(), local_x.end(),\n              [](const std::complex<double> &a, const std::complex<double> &b) {\n                  return std::abs(a) < std::abs(b);\n              });\n\n    // Gather all sorted local vectors to rank 0\n    if (world_rank == 0) {\n        x.resize(x.size());\n    }\n    MPI_Gather(local_x.data(), local_size, MPI_DOUBLE_COMPLEX, x.data(), local_size, MPI_DOUBLE_COMPLEX, 0, MPI_COMM_WORLD);\n\n    // Sort the global vector on rank 0\n    if (world_rank == 0) {\n        std::sort(x.begin(), x.end(),\n                  [](const std::complex<double> &a, const std::complex<double> &b) {\n                      return std::abs(a) < std::abs(b);\n                  });\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
        "correct_output": "Kokkos::parallel_for(\n        Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0, 0}, {N, N}),\n        KOKKOS_LAMBDA(const int i, const int j) {\n            int count = 0;\n            if (i > 0 && input(i - 1, j) == 1) {\n                count++;\n            }\n            if (i < N - 1 && input(i + 1, j) == 1) {\n                count++;\n            }\n            if (j > 0 && input(i, j - 1) == 1) {\n                count++;\n            }\n            if (j < N - 1 && input(i, j + 1) == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output(i, j) = 1;\n            } else {\n                output(i, j) = 0;\n            }\n        });\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "// TODO: implement this function\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                count++;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "serial",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "hip",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int count = 0;\n        if (i > 0 && input[i - 1 + N * j] == 1) {\n            count++;\n        }\n        if (i < N - 1 && input[i + 1 + N * j] == 1) {\n            count++;\n        }\n        if (j > 0 && input[i + N * (j - 1)] == 1) {\n            count++;\n        }\n        if (j < N - 1 && input[i + N * (j + 1)] == 1) {\n            count++;\n        }\n        if (count == 1) {\n            output[i + N * j] = 1;\n        } else {\n            output[i + N * j] = 0;\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int blockSize = N / size;\n    int start = rank * blockSize;\n    int end = start + blockSize;\n\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    for (int i = start; i < end; i++) {\n        for (int j = 0; j < N; j++) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) {\n                count++;\n            }\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) {\n                count++;\n            }\n            if (j > 0 && input[i * N + (j - 1)] == 1) {\n                count++;\n            }\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) {\n                count++;\n            }\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; r++) {\n            MPI_Recv(&output[r * blockSize * N], blockSize * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], blockSize * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "cuda",
        "prompt": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int num_neighbors = 0;\n        if (i > 0 && input[i - 1 + N * j] == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input[i + 1 + N * j] == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input[i + N * (j - 1)] == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input[i + N * (j + 1)] == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output[i + N * j] = 1;\n        } else {\n            output[i + N * j] = 0;\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "50_stencil_xor_kernel",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (N / size);\n    int end = (rank + 1) * (N / size);\n    if (rank == size - 1) {\n        end = N;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[(i - 1) * N + j] == 1) count++;\n            if (i < N - 1 && input[(i + 1) * N + j] == 1) count++;\n            if (j > 0 && input[i * N + (j - 1)] == 1) count++;\n            if (j < N - 1 && input[i * N + (j + 1)] == 1) count++;\n            output[i * N + j] = (count == 1) ? 1 : 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int r = 1; r < size; ++r) {\n            MPI_Recv(&output[r * (N / size) * N], (N / size) * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&output[start * N], (end - start) * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n                       KOKKOS_LAMBDA(const int i) {\n                         for (int j = 0; j < N; j++) {\n                           int liveNeighbors = 0;\n                           for (int k = -1; k <= 1; k++) {\n                             for (int l = -1; l <= 1; l++) {\n                               if (k == 0 && l == 0) {\n                                 continue;\n                               }\n                               if (i + k < 0 || i + k >= N || j + l < 0 || j + l >= N) {\n                                 continue;\n                               }\n                               liveNeighbors += input(i + k, j + l);\n                             }\n                           }\n                           if (input(i, j) == 1) {\n                             if (liveNeighbors < 2 || liveNeighbors > 3) {\n                               output(i, j) = 0;\n                             } else {\n                               output(i, j) = 1;\n                             }\n                           } else {\n                             if (liveNeighbors == 3) {\n                               output(i, j) = 1;\n                             } else {\n                               output(i, j) = 0;\n                             }\n                           }\n                         }\n                       });\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "std::vector<int> temp(input.size());\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\tfor (size_t j = 0; j < N; j++) {\n\t\t\tint count = 0;\n\t\t\tfor (size_t k = i - 1; k <= i + 1; k++) {\n\t\t\t\tfor (size_t l = j - 1; l <= j + 1; l++) {\n\t\t\t\t\tif (k >= 0 && k < N && l >= 0 && l < N) {\n\t\t\t\t\t\tif (k!= i || l!= j) {\n\t\t\t\t\t\t\tcount += input[k * N + l];\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (input[i * N + j] == 1) {\n\t\t\t\tif (count < 2 || count > 3) {\n\t\t\t\t\ttemp[i * N + j] = 0;\n\t\t\t\t} else {\n\t\t\t\t\ttemp[i * N + j] = 1;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif (count == 3) {\n\t\t\t\t\ttemp[i * N + j] = 1;\n\t\t\t\t} else {\n\t\t\t\t\ttemp[i * N + j] = 0;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\toutput = temp;\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "serial",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "std::vector<int> next(input.size(), 0);\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "hip",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {",
        "correct_output": "// get the global thread id\n\tint tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\t// iterate over all elements in the input array\n\tfor (int i = tid; i < N * N; i += stride) {\n\t\t// get the row and column of the current element\n\t\tint row = i / N;\n\t\tint col = i % N;\n\n\t\t// get the number of live neighbors\n\t\tint liveNeighbors = 0;\n\t\tfor (int r = row - 1; r <= row + 1; r++) {\n\t\t\tfor (int c = col - 1; c <= col + 1; c++) {\n\t\t\t\tif (r == row && c == col) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (r < 0 || r >= N || c < 0 || c >= N) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tif (input[r * N + c] == 1) {\n\t\t\t\t\tliveNeighbors++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// update the output array\n\t\tif (input[i] == 1) {\n\t\t\tif (liveNeighbors < 2 || liveNeighbors > 3) {\n\t\t\t\toutput[i] = 0;\n\t\t\t} else {\n\t\t\t\toutput[i] = 1;\n\t\t\t}\n\t\t} else {\n\t\t\tif (liveNeighbors == 3) {\n\t\t\t\toutput[i] = 1;\n\t\t\t} else {\n\t\t\t\toutput[i] = 0;\n\t\t\t}\n\t\t}\n\t}\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "// Initialize output to 0\n    for (size_t i = 0; i < N * N; ++i) {\n        output[i] = 0;\n    }\n\n    // Compute the number of live neighbors for each cell\n    std::vector<int> live_neighbors(N * N, 0);\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            for (int di = -1; di <= 1; ++di) {\n                for (int dj = -1; dj <= 1; ++dj) {\n                    if (di == 0 && dj == 0) continue;\n                    size_t ni = (i + di + N) % N;\n                    size_t nj = (j + dj + N) % N;\n                    live_neighbors[i * N + j] += input[ni * N + nj];\n                }\n            }\n        }\n    }\n\n    // Update the grid based on the number of live neighbors\n    for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int cell = input[i * N + j];\n            int neighbors = live_neighbors[i * N + j];\n            if (cell && (neighbors < 2 || neighbors > 3)) {\n                output[i * N + j] = 0;\n            } else if (!cell && neighbors == 3) {\n                output[i * N + j] = 1;\n            } else {\n                output[i * N + j] = cell;\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "cuda",
        "prompt": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i < N && j < N) {\n    int neighbors = 0;\n    for (int x = -1; x <= 1; x++) {\n      for (int y = -1; y <= 1; y++) {\n        if (x == 0 && y == 0) {\n          continue;\n        }\n        int neighbor_i = i + x;\n        int neighbor_j = j + y;\n        if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n          continue;\n        }\n        neighbors += input[neighbor_i * N + neighbor_j];\n      }\n    }\n    output[i * N + j] = input[i * N + j];\n    if (input[i * N + j] == 1) {\n      if (neighbors < 2 || neighbors > 3) {\n        output[i * N + j] = 0;\n      }\n    } else {\n      if (neighbors == 3) {\n        output[i * N + j] = 1;\n      }\n    }\n  }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "54_stencil_game_of_life",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
        "correct_output": "// Initialize output to 0\n    std::fill(output.begin(), output.end(), 0);\n\n    // Get the rank of the process and the number of processes\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each rank computes a part of the output\n    #pragma omp parallel for\n    for (int i = rank; i < N; i += size) {\n        for (size_t j = 0; j < N; ++j) {\n            int live_neighbors = 0;\n            for (int di = -1; di <= 1; ++di) {\n                for (int dj = -1; dj <= 1; ++dj) {\n                    if (di!= 0 || dj!= 0) {\n                        int ni = (i + di + N) % N;\n                        int nj = (j + dj + N) % N;\n                        live_neighbors += input[ni * N + nj];\n                    }\n                }\n            }\n            if (input[i * N + j] == 1) {\n                if (live_neighbors == 2 || live_neighbors == 3) {\n                    output[i * N + j] = 1;\n                }\n            } else {\n                if (live_neighbors == 3) {\n                    output[i * N + j] = 1;\n                }\n            }\n        }\n    }\n\n    // Gather all the results on rank 0\n    if (rank!= 0) {\n        MPI_Send(output.data(), N * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < size; ++r) {\n            std::vector<int> temp(N * N);\n            MPI_Recv(temp.data(), N * N, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t i = 0; i < N * N; ++i) {\n                output[i] += temp[i];\n            }\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {",
        "correct_output": "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, input.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n                         if (i == 0) {\n                           output(i) = (input(i) + input(i + 1)) / 2;\n                         } else if (i == input.extent(0) - 1) {\n                           output(i) = (input(i) + input(i - 1)) / 2;\n                         } else {\n                           output(i) = (input(i - 1) + input(i) + input(i + 1)) / 3;\n                         }\n                       });\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "correct_output": "int n = input.size();\n    output.resize(n);\n\n    #pragma omp parallel for\n    for (int i = 1; i < n-1; i++) {\n        output[i] = (input[i-1] + input[i] + input[i+1]) / 3;\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "serial",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "correct_output": "// YOUR CODE HERE\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "hip",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (i == 0) {\n      output[i] = (input[i] + input[i + 1]) / 2;\n    } else if (i == N - 1) {\n      output[i] = (input[i] + input[i - 1]) / 2;\n    } else {\n      output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n  }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "correct_output": "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nint n = input.size();\nstd::vector<double> local_output(n, 0.0);\n\n// Compute the local portion of the jacobi iteration\nfor (int i = 1; i < n - 1; ++i) {\n    local_output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3.0;\n}\n\n// Handle the boundary conditions\nif (rank == 0) {\n    local_output[0] = (input[0] + input[1]) / 3.0;\n}\nif (rank == size - 1) {\n    local_output[n - 1] = (input[n - 2] + input[n - 1]) / 3.0;\n}\n\n// Gather the results to rank 0\nMPI_Reduce(local_output.data(), output.data(), n, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n// Since every rank has a complete copy of input, we need to divide the result by the number of ranks\nif (rank == 0) {\n    for (int i = 0; i < n; ++i) {\n        output[i] /= size;\n    }\n}\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "cuda",
        "prompt": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "52_stencil_1d_jacobi_3-point_stencil",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
        "correct_output": "// TODO: your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = input.size();\n  int chunk = n/size;\n  int remainder = n%size;\n  int start = rank*chunk;\n  int end = start+chunk;\n  if(rank == size-1)\n    end += remainder;\n  std::vector<double> temp(chunk);\n  for(int i=start; i<end; i++)\n  {\n    if(i==0)\n      temp[i-start] = (input[i+1]+input[i]+input[i+2])/3;\n    else if(i==n-1)\n      temp[i-start] = (input[i-1]+input[i]+input[i-2])/3;\n    else\n      temp[i-start] = (input[i-1]+input[i]+input[i+1])/3;\n  }\n  MPI_Gather(&temp[0], chunk, MPI_DOUBLE, &output[0], chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {",
        "correct_output": "Kokkos::parallel_for(\"convolveKernel\", Kokkos::MDRangePolicy<Kokkos::Rank<2>>({0, 0}, {N, N}),\n                         KOKKOS_LAMBDA(const int i, const int j) {\n                             int sum = 0;\n                             for (int k = -1; k <= 1; k++) {\n                                 for (int l = -1; l <= 1; l++) {\n                                     int x = i + k;\n                                     int y = j + l;\n                                     if (x >= 0 && x < N && y >= 0 && y < N) {\n                                         sum += edgeKernel[k + 1][l + 1] * imageIn(x, y);\n                                     }\n                                 }\n                             }\n                             imageOut(i, j) = std::max(0, std::min(255, sum));\n                         });\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "correct_output": "#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        for (size_t j = 0; j < N; j++) {\n            int sum = 0;\n            for (int k = -1; k <= 1; k++) {\n                for (int l = -1; l <= 1; l++) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, sum));\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "serial",
        "prompt": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "correct_output": "for (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "hip",
        "prompt": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {",
        "correct_output": "// TODO: implement the kernel\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "correct_output": "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Calculate the number of rows each process will work on\n    int rows_per_process = N / world_size;\n    if (world_rank == world_size - 1) {\n        rows_per_process += N % world_size;\n    }\n\n    // Calculate the starting and ending row for this process\n    int start_row = world_rank * rows_per_process;\n    int end_row = start_row + rows_per_process;\n\n    // Perform the convolution for the rows assigned to this process\n    for (int i = start_row; i < end_row; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int new_pixel = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int ii = i + k;\n                    int jj = j + l;\n                    if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n                        new_pixel += imageIn[ii * N + jj] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, new_pixel));\n        }\n    }\n\n    // Gather all the results on process 0\n    MPI_Gather(&imageOut[start_row * N], rows_per_process * N, MPI_INT,\n               &imageOut[0], rows_per_process * N, MPI_INT, 0, MPI_COMM_WORLD);\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "cuda",
        "prompt": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {",
        "correct_output": "// TODO: implement the kernel\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "51_stencil_edge_kernel",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
        "correct_output": "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int chunk_size = N / world_size;\n    int start_index = world_rank * chunk_size;\n    int end_index = (world_rank == world_size - 1)? N : (world_rank + 1) * chunk_size;\n\n    #pragma omp parallel for\n    for (int i = start_index; i < end_index; ++i) {\n        for (int j = 0; j < N; ++j) {\n            int new_pixel = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int ii = i + k;\n                    int jj = j + l;\n                    if (ii >= 0 && ii < N && jj >= 0 && jj < N) {\n                        new_pixel += imageIn[ii * N + jj] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(0, std::min(255, new_pixel));\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (world_rank!= 0) {\n        MPI_Send(imageOut.data() + start_index * N, chunk_size * N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < world_size; ++i) {\n            int start_index = i * chunk_size;\n            MPI_Recv(imageOut.data() + start_index * N, chunk_size * N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {",
        "correct_output": "Kokkos::parallel_for(\"jacobi2D\", N, KOKKOS_LAMBDA(const int i) {\n        for (int j = 1; j < N - 1; j++) {\n            output(i, j) = (input(i, j - 1) + input(i, j + 1) + input(i - 1, j) + input(i + 1, j) + input(i, j)) / 5;\n        }\n    });\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "correct_output": "// TODO: implement this function\n    #pragma omp parallel for\n    for (int i = 1; i < N-1; i++) {\n        for (int j = 1; j < N-1; j++) {\n            output[i*N+j] = (input[(i-1)*N+j] + input[(i+1)*N+j] + input[i*N+j-1] + input[i*N+j+1] + input[i*N+j])/5;\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "serial",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "correct_output": "for (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "hip",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use AMD HIP to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {",
        "correct_output": "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t j = threadIdx.y + blockIdx.y * blockDim.y;\n    if (i < N && j < N) {\n        double sum = 0;\n        if (i > 0) {\n            sum += input[i - 1 + N * j];\n        }\n        if (i < N - 1) {\n            sum += input[i + 1 + N * j];\n        }\n        if (j > 0) {\n            sum += input[i + N * (j - 1)];\n        }\n        if (j < N - 1) {\n            sum += input[i + N * (j + 1)];\n        }\n        sum += input[i + N * j];\n        sum /= 5;\n        output[i + N * j] = sum;\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "correct_output": "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Each rank will work on a different part of the grid\n    size_t chunkSize = N / size;\n    size_t start = rank * chunkSize;\n    size_t end = (rank == size - 1)? N : (rank + 1) * chunkSize;\n\n    // Compute the jacobi stencil for the part of the grid assigned to this rank\n    for (size_t i = start; i < end; ++i) {\n        for (size_t j = 1; j < N - 1; ++j) {\n            output[i * N + j] = (input[((i - 1 + N) % N) * N + j] +\n                                 input[((i + 1) % N) * N + j] +\n                                 input[i * N + j - 1] +\n                                 input[i * N + j + 1] +\n                                 input[i * N + j]) / 5;\n        }\n    }\n\n    // Gather all the results to rank 0\n    MPI_Gather(output.data() + start * N, chunkSize * N, MPI_DOUBLE,\n               output.data(), chunkSize * N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // If rank 0, fix the boundary elements\n    if (rank == 0) {\n        for (size_t i = 0; i < N; ++i) {\n            output[i * N] = output[(N - 1) * N + i] = output[i * N + N - 1] = output[i * N] = 0;\n        }\n    }\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "cuda",
        "prompt": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {",
        "correct_output": "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) {\n        return;\n    }\n    int idx = i + j * N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input[idx - 1];\n    }\n    if (i < N - 1) {\n        sum += input[idx + 1];\n    }\n    if (j > 0) {\n        sum += input[idx - N];\n    }\n    if (j < N - 1) {\n        sum += input[idx + N];\n    }\n    sum += input[idx];\n    sum /= 5;\n    output[idx] = sum;\n}"
    },
    {
        "problem_type": "stencil",
        "language": "cpp",
        "name": "53_stencil_2d_jacobi_5-point_stencil",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
        "correct_output": "\nint rank, size;\nMPI_Comm_rank(MPI_COMM_WORLD, &rank);\nMPI_Comm_size(MPI_COMM_WORLD, &size);\n\nstd::vector<double> local_output(N * N);\n\n#pragma omp parallel for collapse(2)\nfor (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n        double up = (i > 0) ? input[(i - 1) * N + j] : 0.0;\n        double down = (i < N - 1) ? input[(i + 1) * N + j] : 0.0;\n        double left = (j > 0) ? input[i * N + (j - 1)] : 0.0;\n        double right = (j < N - 1) ? input[i * N + (j + 1)] : 0.0;\n        double center = input[i * N + j];\n        local_output[i * N + j] = (up + down + left + right + center) / 5.0;\n    }\n}\n\nMPI_Reduce(local_output.data(), output.data(), N * N, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\nif (rank == 0) {\n    for (size_t i = 0; i < N * N; ++i) {\n        output[i] /= size;\n    }\n}\n}"
    }
]