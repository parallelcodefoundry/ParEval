[
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {\n    // TODO: Fill this in.\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n\n    // TODO: Fill this in.\n\n    Kokkos::finalize();\n    return 0;\n}\n\"\"\"\n\ndef test_prefix_sum():\n    # TODO: Fill this in.\n    pass\n\nif __name__ == \"__main__\":\n    test_prefix_sum()\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "output": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\nint main() {\n    std::vector<double> x = {1, 7, 4, 6, 6, 2};\n    std::vector<double> output;\n    prefixSum(x, output);\n    for (double d : output) {\n        std::cout << d << \" \";\n    }\n    std::cout << std::endl;\n}\n'''\n\n# compile and run\n\nimport os\nimport subprocess\n\n# compile\n\nos.system('g++ -std=c++11 -fopenmp -o prefixSum prefixSum.cpp')\n\n# run\n\noutput = subprocess.check_output('./prefixSum')\nprint(output.decode('utf-8'))\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "output": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n    output[0] = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double* x, double *output, size_t N) {",
        "output": "/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double* x, double *output, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = x[tid];\n        for (int i = 1; i < blockDim.x; i *= 2) {\n            __syncthreads();\n            if (tid >= i) {\n                output[tid] += output[tid - i];\n            }\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const int* x, int *output, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = x[tid];\n        for (int i = 1; i < blockDim.x; i *= 2) {\n            __syncthreads();\n            if (tid >= i) {\n                output[tid] += output[tid - i];\n            }\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const float* x, float *output, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        output[tid] = x[tid];\n        for (int i = 1; i < blockDim.x; i *= 2) {\n            __syncthreads();\n            if (tid >= i) {\n                output[tid] += output[tid - i];"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "output": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n  // TODO: Your code here.\n}\n\nint main(int argc, char** argv) {\n  int my_rank, num_procs;\n  MPI_Init(&argc, &argv);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  // TODO: Your code here.\n\n  MPI_Finalize();\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(thrust::device_vector<double> const& x, thrust::device_vector<double> &output) {",
        "output": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(thrust::device_vector<double> const& x, thrust::device_vector<double> &output) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < x.size()) {\n        if (i == 0) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i-1] + x[i];\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(thrust::device_vector<double> const& x, thrust::device_vector<double> &output, int n) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < n) {\n        if (i == 0) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i-1] + x[i];\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(thrust::device_vector<double> const& x, thrust::device_vector<double> &output, int n, int m) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < n) {\n        if (i == 0) {\n            output[i] = x[i];\n        } else {\n            output[i] = output[i-1] + x[i];\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "2_scan_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n    // TODO: implement this function\n    return;\n}\n\nint main(int argc, char** argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: read the input vector from a file\n    std::vector<double> x;\n\n    // TODO: compute the prefix sum\n    std::vector<double> output;\n\n    // TODO: write the output vector to a file\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {\n    // TODO: implement this function\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n\n    // TODO: create a Kokkos view of an array of floats\n    Kokkos::View<float*> x;\n\n    // TODO: fill the array with random values\n    // TODO: call partialMinimums()\n    // TODO: print the array\n\n    Kokkos::finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "output": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        float min = x[i];\n        for (int j = 0; j <= i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n\nint main() {\n    std::vector<float> x = {8, 6, -1, 7, 3, 4, 4};\n    partialMinimums(x);\n    for (float f : x) {\n        std::cout << f << \" \";\n    }\n    std::cout << std::endl;\n\n    x = {5, 4, 6, 4, 3, 6, 1, 1};\n    partialMinimums(x);\n    for (float f : x) {\n        std::cout << f << \" \";\n    }\n    std::cout << std::endl;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "serial",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "output": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n    // TODO: Your code here\n}\n\n/* Replace the i-th element of the vector x with the maximum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 8, 8, 8, 8, 8, 8]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [6, 6, 6, 6, 6, 6, 6, 6]\n*/\nvoid partialMaximums(std::vector<float> &x) {\n    // TODO: Your code here\n}\n\n/* Replace the i-th element of the vector x with the sum of the elements from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 14, 13, 20, 23, 27, 27]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 9, 15, 13, 12, 18, 7, 7]\n*/\nvoid partialSums(std::vector<float> &x) {\n    // TODO: Your code here\n}\n\n/* Replace the i-th element of the vector x with the product of the elements from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 48, -8, 56, 21, 16, 16]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 24, 360, 144, 24, 720, 5, 5]\n*/\nvoid partialProducts(std::vector<float> &x) {\n    // TODO: Your code"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "hip",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {",
        "output": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (size_t j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n\n/* Replace the i-th element of the vector x with the minimum value from indices i through N-1.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, 7, 3, 3, 3]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [4, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMaximums(float *x, size_t N) {\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float max = x[i];\n        for (size_t j = i + 1; j < N; j++) {\n            if (x[j] > max) {\n                max = x[j];\n            }\n        }\n        x[i] = max;\n    }\n}\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "output": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n    // TODO: Your code here.\n}\n\nint main(int argc, char **argv) {\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here.\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "cuda",
        "prompt": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(thrust::device_vector<float> &x) {",
        "output": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(thrust::device_vector<float> &x) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < x.size()) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}\n\n/* Replace the i-th element of the vector x with the maximum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 8, 8, 8, 8, 8, 8]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [6, 6, 6, 6, 6, 6, 6, 6]\n*/\n__global__ void partialMaximums(thrust::device_vector<float> &x) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < x.size()) {\n        float max = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] > max) {\n                max = x[j];\n            }\n        }\n        x[i] = max;\n    }\n}\n\n/* Replace the i-th element of the vector x with the sum of the values from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 14, 7, 14, 14, 18, 18]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 9, 15, 9, 9"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "0_scan_min",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int chunk = n / size;\n    int remainder = n % size;\n\n    int start = rank * chunk;\n    int end = start + chunk;\n    if (rank < remainder) {\n        end += 1;\n    }\n\n    float min = x[start];\n    for (int i = start + 1; i < end; i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n    }\n\n    for (int i = start; i < end; i++) {\n        x[i] = min;\n    }\n}\n\nint main(int argc, char **argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = 10000000;\n    std::vector<float> x(n);\n    for (int i = 0; i < n; i++) {\n        x[i] = rand() % 100;\n    }\n\n    partialMinimums(x);\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(Kokkos::View<const double*> const& x) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(Kokkos::View<const double*> const& x) {\n  // TODO: Implement this function.\n  return 0.0;\n}\n\nint main(int argc, char* argv[]) {\n  Kokkos::initialize(argc, argv);\n\n  // TODO: Create a Kokkos view of the vector x.\n  Kokkos::View<double*> x;\n\n  // TODO: Initialize the vector x.\n  for (int i = 0; i < x.extent(0); ++i) {\n    x(i) = 0.0;\n  }\n\n  // TODO: Compute the sum of the prefix sum array.\n  double sum = sumOfPrefixSum(x);\n\n  Kokkos::finalize();\n\n  return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "output": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    int n = x.size();\n    double *y = new double[n];\n    y[0] = x[0];\n    #pragma omp parallel for reduction(+:sum)\n    for (int i = 1; i < n; i++) {\n        y[i] = y[i-1] + x[i];\n        sum += y[i];\n    }\n    delete[] y;\n    return sum;\n}\n\nint main() {\n    std::vector<double> x = {-7, 2, 1, 9, 4, 8};\n    double sum = sumOfPrefixSum(x);\n    std::cout << sum << std::endl;\n    return 0;\n}\n\"\"\"\n\n# compile and run\n\nif __name__ == \"__main__\":\n    import os\n    import subprocess\n    import sys\n\n    # create executable\n    with open(\"prefixsum.cpp\", \"w\") as f:\n        f.write(prefixsum_cpp)\n    os.system(\"g++ -O3 -fopenmp prefixsum.cpp -o prefixsum\")\n\n    # run executable\n    output = subprocess.check_output(\"./prefixsum\")\n    print(output.decode(\"utf-8\"))\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "output": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    for (auto const& i : x) {\n        sum += i;\n    }\n    return sum;\n}\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    for (auto const& i : x) {\n        sum += i;\n    }\n    return sum;\n}\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    for (auto const& i : x) {\n        sum += i;\n    }\n    return sum;\n}\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    for (auto const& i : x) {\n        sum += i;\n    }\n    return sum;\n}\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    for (auto const& i : x) {\n        sum += i;\n    }\n    return sum;\n}\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    double sum = 0;\n    for ("
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Use HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ double sumOfPrefixSum(const double *x, size_t N) {",
        "output": "/* Compute the prefix sum array of the vector x and return its sum.\n   Use HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ double sumOfPrefixSum(const double *x, size_t N) {\n    __shared__ double sdata[BLOCK_SIZE];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0;\n    for (int j = 0; j < N; j++) {\n        sdata[tid] = x[i + j * blockDim.x];\n        __syncthreads();\n        for (int k = blockDim.x / 2; k > 0; k >>= 1) {\n            if (tid < k)\n                sdata[tid] += sdata[tid + k];\n            __syncthreads();\n        }\n        if (tid == 0)\n            sum += sdata[0];\n        __syncthreads();\n    }\n    return sum;\n}\n\nint main(int argc, char **argv) {\n    double *x, *y;\n    double *d_x, *d_y;\n    size_t N = 10000000;\n    size_t nBytes = N * sizeof(double);\n    size_t nBytes2 = 2 * N * sizeof(double);\n    x = (double *)malloc(nBytes);\n    y = (double *)malloc(nBytes);\n    for (int i = 0; i < N; i++) {\n        x[i] = 1.0;\n        y[i] = 2.0;\n    }\n    hipMalloc((void **)&d_x, nBytes);\n    hipMalloc((void **)&d_y, nBytes);\n    hipMemcpy(d_x, x, nBytes, hipMemcpyHostToDevice);\n    hipMemcpy(d_y, y, nBytes, hipMemcpyHostToDevice);\n    hipLaunchKernelGGL(sumOfPrefixSum, dim3(1), dim3(BLOCK_SIZE), 0, 0, d_x, N);\n    hipMemcpy(y, d_y, nBytes, hipMemcpyDeviceToHost);\n    double sum = 0.0;\n    for (int i = 0; i < N; i++)\n        sum += x[i];\n    printf(\"sum = %f\\n\", sum);\n    printf(\"y[0] = %f\\n\", y[0]);\n    free(x"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "output": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute the prefix sum array.\n  std::vector<double> prefixSum(x.size());\n  prefixSum[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    prefixSum[i] = prefixSum[i - 1] + x[i];\n  }\n\n  // Compute the sum of the prefix sum array.\n  double sum = 0;\n  for (int i = 0; i < prefixSum.size(); ++i) {\n    sum += prefixSum[i];\n  }\n\n  return sum;\n}\n\nint main(int argc, char** argv) {\n  MPI_Init(&argc, &argv);\n\n  // Create a vector.\n  std::vector<double> x = {-7, 2, 1, 9, 4, 8};\n\n  // Compute the sum of the prefix sum array.\n  double sum = sumOfPrefixSum(x);\n\n  // Print the sum.\n  std::cout << sum << std::endl;\n\n  MPI_Finalize();\n  return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the prefix sum array of the vector x and return its sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ double sumOfPrefixSum(thrust::device_vector<double> const& x) {",
        "output": "/* Compute the prefix sum array of the vector x and return its sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ double sumOfPrefixSum(thrust::device_vector<double> const& x) {\n    __shared__ double sdata[BLOCK_SIZE];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * BLOCK_SIZE * 2 + tid;\n    double mySum = 0;\n    for (int offset = BLOCK_SIZE; offset > 0; offset /= 2) {\n        if (i < x.size()) {\n            mySum += x[i];\n        }\n        __syncthreads();\n        if (offset > 1) {\n            sdata[tid] = mySum;\n            __syncthreads();\n            mySum += sdata[tid + offset / 2];\n            __syncthreads();\n        }\n    }\n    return mySum;\n}\n\nint main(int argc, char **argv) {\n    thrust::device_vector<double> x(1000000);\n    thrust::sequence(x.begin(), x.end());\n    double sum = sumOfPrefixSum<<<100, 1024>>>(x);\n    std::cout << \"sum = \" << sum << std::endl;\n    return 0;\n}\n\"\"\"\n\n# The following code is the same as the previous one, but it uses a\n# thrust::device_ptr instead of a thrust::device_vector.\n\nkernel_code_ptr = \"\"\"\n#include <thrust/device_ptr.h>\n#include <thrust/sequence.h>\n#include <iostream>\n\n#define BLOCK_SIZE 1024\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\n__global__ double sumOfPrefixSum(thrust::device_ptr<double> x) {\n    __shared__ double sdata[BLOCK_SIZE];\n    int tid = threadIdx.x;\n    int i = blockIdx.x * BLOCK_SIZE * 2 + tid;\n    double mySum = 0;\n    for (int offset = BLOCK_SIZE; offset > 0; offset /= 2) {\n        if (i < x.size"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "1_scan_sum_of_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI and OpenMP to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n    int n = x.size();\n    double sum = 0;\n    double local_sum = 0;\n    double local_sum_omp = 0;\n    double local_sum_omp_reduction = 0;\n    double local_sum_omp_reduction_reduction = 0;\n    double local_sum_reduction = 0;\n    double local_sum_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction = 0;\n    double local_sum_reduction_reduction_reduction_reduction_reduction_reduction_reduction_reduction"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the reverse prefix sum of the array x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(Kokkos::View<const int*> const& x, Kokkos::View<int*> &output) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Compute the reverse prefix sum of the array x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(Kokkos::View<const int*> const& x, Kokkos::View<int*> &output) {\n    // TODO: Implement this function.\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n\n    // TODO: Create a Kokkos::View to store the input array.\n    // TODO: Create a Kokkos::View to store the output array.\n    // TODO: Initialize the input array.\n    // TODO: Call reversePrefixSum.\n    // TODO: Print the output array.\n\n    Kokkos::finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "output": "#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    int n = x.size();\n    output.resize(n);\n    output[0] = x[0];\n    #pragma omp parallel for\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\nint main() {\n    std::vector<int> x = {1, 7, 4, 6, 6, 2};\n    std::vector<int> output;\n    reversePrefixSum(x, output);\n    for (int i = 0; i < output.size(); i++) {\n        std::cout << output[i] << \" \";\n    }\n    std::cout << std::endl;\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "serial",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "output": "/* Compute the reverse prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    int n = x.size();\n    output[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [3, 6, 13, 14, 12]\n*/\nvoid prefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    int n = x.size();\n    output[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [3, 6, 13, 14, 12]\n*/\nvoid prefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    int n = x.size();\n    output[0] = x[0];\n    for (int i = 1; i < n; i++) {\n        output[i] = output[i-1] + x[i];\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "hip",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {",
        "output": "/* Compute the reverse prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(const int *x, int *output, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = tid; i >= 0; i = i - blockDim.x) {\n            sum += x[i];\n            output[i] = sum;\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [3, 6, 13, 14, 12]\n*/\n__global__ void prefixSum(const int *x, int *output, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i = tid; i < N; i = i + blockDim.x) {\n            sum += x[i];\n            output[i] = sum;\n        }\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [3, 6, 13, 14, 12]\n*/\n__global__ void prefixSumInclusive(const int *x, int *output, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        int sum = 0;\n        for (int i ="
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "output": "#include <mpi.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    // TODO: Your code here.\n}\n\nint main(int argc, char** argv) {\n    MPI_Init(&argc, &argv);\n\n    // TODO: Your code here.\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "cuda",
        "prompt": "/* Compute the reverse prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(thrust::device_vector<int> const& x, thrust::device_vector<int> &output) {",
        "output": "/* Compute the reverse prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\n__global__ void reversePrefixSum(thrust::device_vector<int> const& x, thrust::device_vector<int> &output) {\n    int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int n = x.size();\n    int sum = 0;\n    if (gid < n) {\n        sum = x[gid];\n        if (gid > 0) {\n            sum += output[gid - 1];\n        }\n        output[gid] = sum;\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [3, 6, 13, 14, 12]\n*/\n__global__ void prefixSum(thrust::device_vector<int> const& x, thrust::device_vector<int> &output) {\n    int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int n = x.size();\n    int sum = 0;\n    if (gid < n) {\n        sum = x[gid];\n        if (gid > 0) {\n            sum += output[gid - 1];\n        }\n        output[gid] = sum;\n    }\n}\n\n/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [3, 6, 13, 14, 12]\n*/\n__global__ void prefixSum(thrust::device_vector<int> const& x, thrust::device_vector<int> &output"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "3_scan_reverse_prefix_sum",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Compute the reverse prefix sum of the vector x into output.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [2, 8, 14, 18, 25, 26]\n\n   input: [3, 3, 7, 1, -2]\n   output: [-2, -1, 6, 9, 12]\n*/\nvoid reversePrefixSum(std::vector<int> const& x, std::vector<int> &output) {\n    // TODO: Your code here.\n}\n\nint main(int argc, char** argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here.\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(Kokkos::View<int*> const& x) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(Kokkos::View<int*> const& x) {\n    int n = x.extent(0);\n    int max_sum = 0;\n    int sum = 0;\n    for (int i = 0; i < n; i++) {\n        sum += x(i);\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n    return max_sum;\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n    {\n        Kokkos::View<int*> x(\"x\", 9);\n        x(0) = -2;\n        x(1) = 1;\n        x(2) = -3;\n        x(3) = 4;\n        x(4) = -1;\n        x(5) = 2;\n        x(6) = 1;\n        x(7) = -5;\n        x(8) = 4;\n        int max_sum = maximumSubarray(x);\n        std::cout << \"max_sum = \" << max_sum << std::endl;\n    }\n    Kokkos::finalize();\n    return 0;\n}\n\"\"\"\n\ndef test_kokkos_example():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        with open(os.path.join(tmpdir, \"kokkos_example.cpp\"), \"w\") as f:\n            f.write(kokkos_example)\n        with open(os.path.join(tmpdir, \"CMakeLists.txt\"), \"w\") as f:\n            f.write(cmake_lists)\n        with open(os.path.join(tmpdir, \"main.cpp\"), \"w\") as f:\n            f.write(main)\n        subprocess.check_call([\"cmake\", tmpdir])\n        subprocess.check_call([\"make\"])\n        subprocess.check_call([os.path.join(tmpdir, \"a.out\")])\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "output": "#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int n = x.size();\n    int max_sum = 0;\n    int sum = 0;\n    int i;\n\n    #pragma omp parallel for reduction(+:max_sum)\n    for (i = 0; i < n; i++) {\n        sum += x[i];\n        if (sum > max_sum) {\n            max_sum = sum;\n        }\n        if (sum < 0) {\n            sum = 0;\n        }\n    }\n\n    return max_sum;\n}\n\nint main() {\n    std::vector<int> x = {\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4};\n    std::cout << maximumSubarray(x) << std::endl;\n    return 0;\n}\n\"\"\"\n\nimport numpy as np\nimport time\n\ndef maximumSubarray(x):\n    n = x.size\n    max_sum = 0\n    sum = 0\n    i = 0\n\n    for i in range(n):\n        sum += x[i]\n        if (sum > max_sum):\n            max_sum = sum\n        if (sum < 0):\n            sum = 0\n\n    return max_sum\n\ndef main():\n    x = np.array([\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4])\n    start = time.time()\n    print(maximumSubarray(x))\n    end = time.time()\n    print(end - start)\n\nif __name__ == '__main__':\n    main()\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "serial",
        "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "output": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int max_sum = 0;\n    int current_sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        current_sum += x[i];\n        if (current_sum > max_sum) {\n            max_sum = current_sum;\n        }\n        if (current_sum < 0) {\n            current_sum = 0;\n        }\n    }\n    return max_sum;\n}\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int max_sum = 0;\n    int current_sum = 0;\n    for (int i = 0; i < x.size(); i++) {\n        current_sum += x[i];\n        if (current_sum > max_sum) {\n            max_sum = current_sum;\n        }\n        if (current_sum < 0) {\n            current_sum = 0;\n        }\n    }\n    return max_sum;\n}\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int max_sum = 0;\n   "
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "hip",
        "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ int maximumSubarray(const int *x, size_t N) {",
        "output": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ int maximumSubarray(const int *x, size_t N) {\n    // TODO: Your code here\n    return 0;\n}\n\nint main(int argc, char **argv) {\n    int *x;\n    size_t N;\n    int *d_x;\n    int *d_result;\n    int result;\n    size_t threads_per_block;\n    size_t blocks_per_grid;\n\n    // TODO: Your code here\n\n    return 0;\n}\n\"\"\"\n\nimport numpy as np\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\n\ndef maximumSubarray(x):\n    \"\"\"Return the largest sum of any contiguous subarray in the vector x.\n    i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n    subarray with the largest sum of 6.\n    Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n    Example:\n\n    input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n    output: 6\n    \"\"\"\n    # TODO: Your code here\n    return 0\n\ndef main():\n    x = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    result = maximumSubarray(x)\n    print(result)\n\nif __name__ == \"__main__\":\n    main()\n"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "output": "#include <mpi.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int size = x.size();\n    int local_max = 0;\n    int global_max = 0;\n    int local_sum = 0;\n    int global_sum = 0;\n    int local_min = 0;\n    int global_min = 0;\n    int local_min_index = 0;\n    int global_min_index = 0;\n    int local_max_index = 0;\n    int global_max_index = 0;\n    int local_sum_index = 0;\n    int global_sum_index = 0;\n    int local_sum_index_end = 0;\n    int global_sum_index_end = 0;\n    int local_min_index_end = 0;\n    int global_min_index_end = 0;\n    int local_max_index_end = 0;\n    int global_max_index_end = 0;\n    int local_min_index_start = 0;\n    int global_min_index_start = 0;\n    int local_max_index_start = 0;\n    int global_max_index_start = 0;\n    int local_sum_index_start = 0;\n    int global_sum_index_start = 0;\n    int local_sum_index_end_start = 0;\n    int global_sum_index_end_start = 0;\n    int local_min_index_end_start = 0;\n    int global_min_index_end_start = 0;\n    int local_max_index_end_start = 0;\n    int global_max_index_end_start = 0;\n    int local_min_index_start_start = 0;\n    int global_min_index_start_start = 0;\n    int local_max_index_start_start = 0;\n    int global_max_index_start_start = 0;\n    int local_sum_index_start_start = 0;\n    int global_sum_index_start_start = 0;\n    int local_sum_index_end_start_start = 0;\n    int global_sum_index_end_start_start = 0;\n    int"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "cuda",
        "prompt": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ int maximumSubarray(thrust::device_vector<int> const& x) {",
        "output": "/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\n__global__ int maximumSubarray(thrust::device_vector<int> const& x) {\n    int max_so_far = x[0];\n    int max_ending_here = x[0];\n    for (int i = 1; i < x.size(); i++) {\n        max_ending_here = max(x[i], max_ending_here + x[i]);\n        max_so_far = max(max_so_far, max_ending_here);\n    }\n    return max_so_far;\n}\n\nint main() {\n    thrust::device_vector<int> x(8);\n    x[0] = -2;\n    x[1] = 1;\n    x[2] = -3;\n    x[3] = 4;\n    x[4] = -1;\n    x[5] = 2;\n    x[6] = 1;\n    x[7] = -5;\n    int max_subarray = maximumSubarray(x);\n    std::cout << \"The maximum subarray is \" << max_subarray << std::endl;\n    return 0;\n}\n\"\"\"\n\n# The following code is a Python implementation of the above CUDA code.\n# It is not meant to be efficient, but rather to demonstrate the\n# Python interface to CUDA.\n\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\nimport numpy as np\n\n# The following code is a Python implementation of the above CUDA code.\n# It is not meant to be efficient, but rather to demonstrate the\n# Python interface to CUDA.\n\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\nimport numpy as np\n\n# The following code is a Python implementation of the above CUDA code.\n# It is not meant to be efficient, but rather to demonstrate the\n# Python interface to CUDA.\n\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule\nimport numpy as np\n\n# The following code is a Python implementation of the above CUDA code.\n# It is not meant to be efficient, but rather to demonstrate the\n# Python interface to CUDA.\n\nimport pycuda.driver as cuda\nimport pycuda.autoinit\nfrom pycuda.compiler import SourceModule"
    },
    {
        "problem_type": "scan",
        "language": "cpp",
        "name": "4_scan_largest_subarray",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Return the largest sum of any contiguous subarray in the vector x.\n   i.e. if x=[\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4] then [4, \u22121, 2, 1] is the contiguous\n   subarray with the largest sum of 6.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [\u22122, 1, \u22123, 4, \u22121, 2, 1, \u22125, 4]\n   output: 6\n*/\nint maximumSubarray(std::vector<int> const& x) {\n    int n = x.size();\n    int local_max = 0;\n    int global_max = 0;\n    int local_sum = 0;\n    int global_sum = 0;\n    int local_min = 0;\n    int global_min = 0;\n    int local_min_index = 0;\n    int global_min_index = 0;\n    int local_max_index = 0;\n    int global_max_index = 0;\n    int local_max_index_temp = 0;\n    int global_max_index_temp = 0;\n    int local_min_index_temp = 0;\n    int global_min_index_temp = 0;\n    int local_sum_temp = 0;\n    int global_sum_temp = 0;\n    int local_min_temp = 0;\n    int global_min_temp = 0;\n    int local_max_temp = 0;\n    int global_max_temp = 0;\n    int local_max_index_temp_temp = 0;\n    int global_max_index_temp_temp = 0;\n    int local_min_index_temp_temp = 0;\n    int global_min_index_temp_temp = 0;\n    int local_sum_temp_temp = 0;\n    int global_sum_temp_temp = 0;\n    int local_min_temp_temp = 0;\n    int global_min_temp_temp = 0;\n    int local_max_temp_temp = 0;\n    int global_max_temp_temp = 0;\n    int local_max_index_temp_temp_temp = 0;\n    int global_max_index_temp_temp_temp = 0;\n    int local_min_index_temp_temp_temp = 0;\n    int global_min_index_temp_temp_temp = 0;\n    int local_sum_temp_temp_temp = 0;\n    int global_sum_temp_temp_temp = 0;\n    int local_min_temp_temp_temp = 0;\n    int global_min_temp_temp_temp = 0;\n    int local_max_temp_temp_"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x of complex numbers by their magnitude in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(Kokkos::View<Kokkos::complex<double>*> &x) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x of complex numbers by their magnitude in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(Kokkos::View<Kokkos::complex<double>*> &x) {\n  // TODO: Implement this function.\n}\n\nint main(int argc, char* argv[]) {\n  Kokkos::initialize(argc, argv);\n  {\n    Kokkos::View<Kokkos::complex<double>*> x(\"x\", 5);\n    x(0) = 3.0 - 1.0i;\n    x(1) = 4.5 + 2.1i;\n    x(2) = 0.0 - 1.0i;\n    x(3) = 1.0 - 0.0i;\n    x(4) = 0.5 + 0.5i;\n    sortComplexByMagnitude(x);\n    for (int i = 0; i < 5; i++) {\n      std::cout << x(i) << std::endl;\n    }\n  }\n  Kokkos::finalize();\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "output": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = i + 1; j < x.size(); j++) {\n            if (abs(x[i]) > abs(x[j])) {\n                std::complex<double> temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n        }\n    }\n}\n\nint main() {\n    std::vector<std::complex<double>> x = {3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i};\n    sortComplexByMagnitude(x);\n    for (int i = 0; i < x.size(); i++) {\n        std::cout << x[i] << std::endl;\n    }\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "output": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return abs(a) < abs(b);\n    });\n}\n\n/* Sort the vector x of complex numbers by their real part in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.0-1.0i, 0.5+0.5i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByRealPart(std::vector<std::complex<double>> &x) {\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return a.real() < b.real();\n    });\n}\n\n/* Sort the vector x of complex numbers by their imaginary part in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [3.0-1.0i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i, 4.5+2.1i]\n*/\nvoid sortComplexByImaginaryPart(std::vector<std::complex<double>> &x) {\n    std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return a.imag() < b.imag();\n    });\n}\n\n/* Sort the vector x of complex numbers by their phase in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "hip",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(hipDoubleComplex *x, size_t N) {",
        "output": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(hipDoubleComplex *x, size_t N) {\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (size_t i = tid; i < N; i += stride) {\n        hipDoubleComplex temp = x[i];\n        size_t j = i;\n        while (j > 0 && hipCabs(x[j - 1]) > hipCabs(temp)) {\n            x[j] = x[j - 1];\n            j--;\n        }\n        x[j] = temp;\n    }\n}\n\n/* Sort the vector x of complex numbers by their magnitude in descending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [4.5+2.1i, 3.0-1.0i, 1.0-0.0i, 0.5+0.5i, 0.0-1.0i]\n*/\n__global__ void sortComplexByMagnitudeDescending(hipDoubleComplex *x, size_t N) {\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    for (size_t i = tid; i < N; i += stride) {\n        hipDoubleComplex temp = x[i];\n        size_t j = i;\n        while (j > 0 && hipCabs(x[j - 1]) < hipCabs(temp)) {\n            x[j] = x[j - 1];\n            j--;\n        }\n        x[j] = temp;\n    }\n}\n\n/* Sort the vector x of complex numbers by their real part in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4."
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "output": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    // TODO: Your code here.\n}\n\nint main(int argc, char **argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here.\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(thrust::device_vector<cuDoubleComplex> &x) {",
        "output": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(thrust::device_vector<cuDoubleComplex> &x) {\n    thrust::device_vector<cuDoubleComplex> temp(x);\n    thrust::sort(temp.begin(), temp.end(), thrust::greater<cuDoubleComplex>());\n    x = temp;\n}\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(thrust::device_vector<cuFloatComplex> &x) {\n    thrust::device_vector<cuFloatComplex> temp(x);\n    thrust::sort(temp.begin(), temp.end(), thrust::greater<cuFloatComplex>());\n    x = temp;\n}\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\n__global__ void sortComplexByMagnitude(thrust::device_vector<float2> &x) {\n    thrust::device_vector<float2> temp(x);\n    thrust::sort(temp.begin(), temp.end(), thrust::greater<float2>());\n    x = temp;\n}\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use CUDA to sort"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "3_sort_complex",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n    // TODO: implement this function\n    return;\n}\n\nint main(int argc, char **argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: read the input vector from a file\n    std::vector<std::complex<double>> x;\n\n    // TODO: sort the vector in parallel\n    sortComplexByMagnitude(x);\n\n    // TODO: write the sorted vector to a file\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(Kokkos::View<int*> &x) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(Kokkos::View<int*> &x) {\n  // TODO: implement this function\n}\n\nint main(int argc, char* argv[]) {\n  Kokkos::initialize(argc, argv);\n\n  // TODO: create a Kokkos view to an array of ints\n  Kokkos::View<int*> x;\n\n  // TODO: fill the array with some data\n  //...\n\n  // TODO: call the function to sort the array\n  sortIgnoreZero(x);\n\n  // TODO: print the sorted array\n  //...\n\n  Kokkos::finalize();\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "output": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n    int n = x.size();\n    int *x_ptr = &x[0];\n    int *x_end = x_ptr + n;\n    int *x_zero = std::partition(x_ptr, x_end, [](int x) { return x!= 0; });\n    int *x_nonzero = std::partition(x_zero, x_end, [](int x) { return x == 0; });\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            #pragma omp task\n            {\n                std::sort(x_ptr, x_zero);\n            }\n            #pragma omp task\n            {\n                std::sort(x_nonzero, x_end);\n            }\n        }\n    }\n}\n\nint main() {\n    std::vector<int> x = {8, 4, 0, 9, 8, 0, 1, -1, 7};\n    sortIgnoreZero(x);\n    for (int i = 0; i < x.size(); i++) {\n        std::cout << x[i] << \" \";\n    }\n    std::cout << std::endl;\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "serial",
        "prompt": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "output": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n    int n = x.size();\n    int i = 0;\n    int j = 0;\n    int k = 0;\n    int temp = 0;\n    while (i < n) {\n        if (x[i]!= 0) {\n            temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n            j++;\n        }\n        i++;\n    }\n    while (j < n) {\n        if (x[j]!= 0) {\n            temp = x[j];\n            x[j] = x[k];\n            x[k] = temp;\n            k++;\n        }\n        j++;\n    }\n}\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Move zero valued elements to the end of the vector.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 4, 7, 8, 8, 9, 0, 0]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n    int n = x.size();\n    int i = 0;\n    int j = 0;\n    int k = 0;\n    int temp = 0;\n    while (i < n) {\n        if (x[i]!= 0) {\n            temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n            j++;\n        }\n        i++;\n    }\n    while (j < n) {\n        if (x[j]!= 0) {\n            temp = x[j];\n            x[j] = x[k];\n            x[k] = temp;\n            k++;\n        }\n        j++;\n    }\n    while (k < n) {\n        if (x[k] == 0) {\n            temp = x[k];\n            x[k] = x[n - 1];\n            x[n - 1] = temp;\n            n--;\n        }\n        k++;\n    }\n}\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Move zero valued elements to the front of the vector.\n   Example"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "hip",
        "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {",
        "output": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == 0) {\n            x[tid] = 0;\n        } else {\n            int key = x[tid];\n            int i = tid - 1;\n            while (i >= 0 && x[i] > key) {\n                x[i + 1] = x[i];\n                i = i - 1;\n            }\n            x[i + 1] = key;\n        }\n    }\n}\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(int *x, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] == 0) {\n            x[tid] = 0;\n        } else {\n            int key = x[tid];\n            int i = tid - 1;\n            while (i >= 0 && x[i] > key) {\n                x[i + 1] = x[i];\n                i = i - 1;\n            }\n            x[i + 1] = key;\n        }\n    }\n}\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use AMD HIP to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "output": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n    // TODO: Your code here\n}\n\nint main(int argc, char **argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(thrust::device_vector<int> &x) {",
        "output": "/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(thrust::device_vector<int> &x) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < x.size()) {\n        if (x[tid] == 0) {\n            x[tid] = 0;\n        } else {\n            int temp = x[tid];\n            int i = tid - 1;\n            while (i >= 0 && temp < x[i]) {\n                x[i + 1] = x[i];\n                i = i - 1;\n            }\n            x[i + 1] = temp;\n        }\n    }\n}\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\n__global__ void sortIgnoreZero(thrust::device_vector<int> &x, thrust::device_vector<int> &y) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < x.size()) {\n        if (x[tid] == 0) {\n            x[tid] = 0;\n            y[tid] = 0;\n        } else {\n            int temp = x[tid];\n            int temp2 = y[tid];\n            int i = tid - 1;\n            while (i >= 0 && temp < x[i]) {\n                x[i + 1] = x[i];\n                y[i + 1] = y[i];\n                i = i - 1;\n            }\n            x[i + 1] = temp;\n            y[i + 1] = temp2;\n        }\n    }\n}\n\n/* Sort the array x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use CUDA to compute in parallel. The kernel will be launched with 1 thread per element.\n   Example:\n\n   input: [8, 4, "
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "0_sort_non-zero",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use MPI and OpenMP to sort x in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n   Use MPI and OpenMP to sort x in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n    // TODO: implement me!\n}\n\nint main(int argc, char **argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: read input from file\n    std::vector<int> x;\n\n    // TODO: sort x in parallel\n    sortIgnoreZero(x);\n\n    // TODO: print x to stdout\n    for (int i = 0; i < x.size(); i++) {\n        std::cout << x[i] << \" \";\n    }\n    std::cout << std::endl;\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Sort the first half of the array x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use Kokkos to sort in parallel. Assume Kokkos is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(Kokkos::View<int*> &x) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Sort the first half of the array x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use Kokkos to sort in parallel. Assume Kokkos is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(Kokkos::View<int*> &x) {\n    // TODO: Implement this function.\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n\n    // TODO: Create a Kokkos::View to hold the input array.\n    // TODO: Create a Kokkos::View to hold the output array.\n    // TODO: Initialize the input array.\n    // TODO: Sort the input array.\n    // TODO: Print the output array.\n\n    Kokkos::finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use OpenMP to sort in parallel.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "output": "#include <omp.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use OpenMP to sort in parallel.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {\n    // TODO: Your code here\n    int n = x.size();\n    int mid = n/2;\n    int i = 0;\n    int j = n-1;\n    int temp;\n    int mid_val = x[mid];\n    int temp_val;\n    int k = 0;\n    int k_val;\n    int k_index;\n    int k_index_val;\n    int k_index_val_temp;\n    int k_index_val_temp_temp;\n    int k_index_val_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp_temp;\n    int k_index_val_temp_temp_temp_temp"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "serial",
        "prompt": "/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "output": "/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {\n    // TODO: Your code here\n    int n = x.size();\n    int mid = n/2;\n    int i = 0;\n    int j = n-1;\n    int temp;\n    while(i<mid && j>=mid){\n        if(x[i] > x[j]){\n            temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        i++;\n        j--;\n    }\n}\n\n/*\n   Sort the first half of the vector x in ascending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [-4, 2, 3, 5, 6, 7, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [-8, 1, 3, 4, 6, 1]\n*/\nvoid sortFirstHalfAscending(std::vector<int> &x) {\n    // TODO: Your code here\n    int n = x.size();\n    int mid = n/2;\n    int i = 0;\n    int j = n-1;\n    int temp;\n    while(i<mid && j>=mid){\n        if(x[i] < x[j]){\n            temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        i++;\n        j--;\n    }\n}\n\n/*\n   Sort the second half of the vector x in descending order. Leave the first half in-place. \n   If x.size() is odd, then include the middle element in the second half. \n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [2, 5, -4, 3, 6, -1, 7]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [-8, 4, 1, 3, 1, 6]\n*/\nvoid sort"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "hip",
        "prompt": "/* Sort the first half of the array x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use AMD HIP to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\n__global__ void sortFirstHalfDescending(int *x, size_t N) {",
        "output": "/* Sort the first half of the array x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use AMD HIP to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\n__global__ void sortFirstHalfDescending(int *x, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N/2) {\n        int i = tid;\n        int j = N - 1 - tid;\n        int temp;\n        while (i < j) {\n            if (x[i] > x[j]) {\n                temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n            i++;\n            j--;\n        }\n    }\n}\n\n/* Sort the second half of the array x in ascending order. Leave the first half in-place. \n   If x.size() is odd, then include the middle element in the second half. \n   Use AMD HIP to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [2, 5, -4, 3, 6, -1, 7]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [-8, 4, 1, 3, 1, 6]\n*/\n__global__ void sortSecondHalfAscending(int *x, size_t N) {\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N/2) {\n        int i = tid;\n        int j = N - 1 - tid;\n        int temp;\n        while (i < j) {\n            if (x[i] < x[j]) {\n                temp = x[i];\n                x[i] = x[j];\n                x[j] = temp;\n            }\n            i++;\n            j--;\n        }\n    }\n}\n\n/* Sort the second half of the array x in descending order. Leave the first half in-place. \n   If x.size() is odd, then include the middle element in the second half. \n   Use AMD HIP to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [2, 5, -4,"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use MPI to sort in parallel. Assume MPI is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "output": "#include <mpi.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use MPI to sort in parallel. Assume MPI is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {\n    // TODO: Your code here\n}\n\nint main(int argc, char** argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "cuda",
        "prompt": "/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use CUDA to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\n__global__ void sortFirstHalfDescending(thrust::device_vector<int> &x) {",
        "output": "/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use CUDA to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\n__global__ void sortFirstHalfDescending(thrust::device_vector<int> &x) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < x.size() / 2) {\n        int j = x.size() - i - 1;\n        int temp = x[i];\n        x[i] = x[j];\n        x[j] = temp;\n    }\n}\n\n/* Sort the second half of the vector x in ascending order. Leave the first half in-place. \n   If x.size() is odd, then include the middle element in the second half. \n   Use CUDA to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [2, 5, -4, 3, 6, -1, 7]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [-8, 4, 1, 3, 1, 6]\n*/\n__global__ void sortSecondHalfAscending(thrust::device_vector<int> &x) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < x.size() / 2) {\n        int j = x.size() - i - 1;\n        int temp = x[i];\n        x[i] = x[j];\n        x[j] = temp;\n    }\n}\n\n/* Sort the second half of the vector x in descending order. Leave the first half in-place. \n   If x.size() is odd, then include the middle element in the second half. \n   Use CUDA to sort in parallel. Kernel will be launched with 1 thread per element.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [2, 5, -4, 7, 6, -1, 3]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [-8, 4, 6, 3, 1, 1]\n*/\n__global__ void sortSecondHalfDescending(th"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "1_sort_first_half",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use MPI and OpenMP to sort in parallel. Assume MPI is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Sort the first half of the vector x in descending order. Leave the second half in-place. \n   If x.size() is odd, then include the middle element in the first half. \n   Use MPI and OpenMP to sort in parallel. Assume MPI is already initialized.\n   Examples:\n   \n   input: [2, 5, -4, 7, 3, 6, -1]\n   output: [7, 5, 2, -4, 3, 6, -1]\n\n   input: [-8, 4, 6, 1, 3, 1]\n   output: [6, 4, -8, 1, 3, 1]\n*/\nvoid sortFirstHalfDescending(std::vector<int> &x) {\n    // TODO: Your code here\n}\n\nint main(int argc, char** argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {\n    // TODO: Implement this function.\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n\n    // TODO: Create a Kokkos::View to store the input array.\n    // TODO: Create a Kokkos::View to store the output array.\n    // TODO: Fill the input array with random values.\n    // TODO: Call the ranks function.\n    // TODO: Print the output array.\n\n    Kokkos::finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "output": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n    // TODO: implement this function\n    // You may need to add more variables to store intermediate results.\n    // You may also need to use the omp_get_thread_num() function.\n    // You may also need to use the omp_get_num_threads() function.\n    // You may also need to use the omp_get_num_procs() function.\n    // You may also need to use the omp_get_max_threads() function.\n    // You may also need to use the omp_get_thread_limit() function.\n    // You may also need to use the omp_get_num_places() function.\n    // You may also need to use the omp_get_place_num_procs() function.\n    // You may also need to use the omp_get_place_proc_ids() function.\n    // You may also need to use the omp_get_place_num() function.\n    // You may also need to use the omp_get_partition_num_places() function.\n    // You may also need to use the omp_get_partition_place_nums() function.\n    // You may also need to use the omp_get_partition_place_proc_ids() function.\n    // You may also need to use the omp_get_cancellation() function.\n    // You may also need to use the omp_get_thread_limit() function.\n    // You may also need to use the omp_get_active_level() function.\n    // You may also need to use the omp_in_parallel() function.\n    // You may also need to use the omp_set_get_num_threads() function.\n    // You may also need to use the omp_set_get_max_threads() function.\n    // You may also need to use the omp_set_get_schedule() function.\n    // You may also need to use the omp_set_get_default_schedule() function.\n    // You may also need to use the omp_set_get_schedule_static() function.\n    // You may also need to use the omp_set_get_default_schedule_static() function.\n    // You may also need to use the omp_set_get_nested() function.\n    // You may also need to use the omp_set_get_dynamic() function.\n    //"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "serial",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "output": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n    // TODO: implement this function\n    // You may need to add additional variables\n    // You may not use any loops\n    // You may not use any built-in sorting functions\n    // You may not use any built-in functions that return random values\n    // You may not use any built-in functions that return the rank of a value\n    // You may not use any built-in functions that return the index of a value\n    // You may not use any built-in functions that return the value of a rank\n    // You may not use any built-in functions that return the value of an index\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a vector\n    // You may not use any built-in functions that return the size of a"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "hip",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(float const* x, size_t *ranks, size_t N) {",
        "output": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(float const* x, size_t *ranks, size_t N) {\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        ranks[tid] = amd::hsa_amd_memory_async_copy(ranks + tid, x + tid, 1, amd::MEMORY_DEVICE, amd::MEMORY_DEVICE);\n    }\n}\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(double const* x, size_t *ranks, size_t N) {\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        ranks[tid] = amd::hsa_amd_memory_async_copy(ranks + tid, x + tid, 1, amd::MEMORY_DEVICE, amd::MEMORY_DEVICE);\n    }\n}\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use AMD HIP to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(int const* x, size_t *ranks, size_t N) {\n    size"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "output": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n    // TODO: implement this function\n    return;\n}\n\nint main(int argc, char** argv) {\n    MPI_Init(&argc, &argv);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (argc!= 2) {\n        if (rank == 0) {\n            std::cerr << \"Usage: \" << argv[0] << \" <vector_size>\" << std::endl;\n        }\n        MPI_Finalize();\n        return 1;\n    }\n\n    size_t n = std::stoul(argv[1]);\n    if (n == 0) {\n        if (rank == 0) {\n            std::cerr << \"Vector size must be positive\" << std::endl;\n        }\n        MPI_Finalize();\n        return 1;\n    }\n\n    std::vector<float> x(n);\n    std::iota(x.begin(), x.end(), 0);\n    std::vector<size_t> ranks(n);\n\n    ranks(x, ranks);\n\n    if (rank == 0) {\n        std::cout << \"x = [\";\n        for (size_t i = 0; i < n; ++i) {\n            std::cout << x[i] << \", \";\n        }\n        std::cout << \"]\" << std::endl;\n        std::cout << \"ranks = [\";\n        for (size_t i = 0; i < n; ++i) {\n            std::cout << ranks[i] << \", \";\n        }\n        std::cout << \"]\" << std::endl;\n    }\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "cuda",
        "prompt": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(thrust::device_vector<float> const& x, thrust::device_vector<size_t> &ranks) {",
        "output": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(thrust::device_vector<float> const& x, thrust::device_vector<size_t> &ranks) {\n    // get the index of the thread\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < x.size()) {\n        // get the value of the current element\n        float value = x[idx];\n        // get the index of the first element that is greater than value\n        size_t rank = thrust::distance(x.begin(), thrust::upper_bound(x.begin(), x.end(), value));\n        // store the result\n        ranks[idx] = rank;\n    }\n}\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(thrust::device_vector<double> const& x, thrust::device_vector<size_t> &ranks) {\n    // get the index of the thread\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < x.size()) {\n        // get the value of the current element\n        double value = x[idx];\n        // get the index of the first element that is greater than value\n        size_t rank = thrust::distance(x.begin(), thrust::upper_bound(x.begin(), x.end(), value));\n        // store the result\n        ranks[idx] = rank;\n    }\n}\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "4_sort_ranks",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n    // TODO: implement this function\n    return;\n}\n\nint main(int argc, char** argv) {\n    int rank, size;\n    MPI_Init(&argc, &argv);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (argc!= 2) {\n        if (rank == 0) {\n            std::cerr << \"Usage: \" << argv[0] << \" <vector size>\" << std::endl;\n        }\n        MPI_Finalize();\n        return 1;\n    }\n\n    size_t n = std::stoul(argv[1]);\n    if (n == 0) {\n        std::cerr << \"Vector size must be positive\" << std::endl;\n        MPI_Finalize();\n        return 1;\n    }\n\n    std::vector<float> x(n);\n    std::iota(x.begin(), x.end(), 0);\n    std::vector<size_t> ranks(n);\n\n    // TODO: generate random numbers for x\n\n    ranks(x, ranks);\n\n    if (rank == 0) {\n        std::cout << \"x = \" << x << std::endl;\n        std::cout << \"ranks = \" << ranks << std::endl;\n    }\n\n    MPI_Finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(Kokkos::View<Result*> &results) {",
        "output": "#include <Kokkos_Core.hpp>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(Kokkos::View<Result*> &results) {\n   Kokkos::parallel_for(results.extent(0), KOKKOS_LAMBDA(const int i) {\n      int j = i;\n      while (j > 0 && results(j).startTime < results(j-1).startTime) {\n         Result temp = results(j);\n         results(j) = results(j-1);\n         results(j-1) = temp;\n         j--;\n      }\n   });\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(Kokkos::View<Result*> &results, Kokkos::View<int*> &indices) {\n   Kokkos::parallel_for(results.extent(0), KOKKOS_LAMBDA(const int i) {\n      int j = i;\n      while (j > 0 && results(j).startTime < results(j-1).startTime) {\n         Result temp = results(j);\n         results(j) = results(j-1);\n         results(j-1) = temp;\n         int tempIndex = indices(j);\n         indices(j) = indices(j-1);\n         indices(j-1) = tempIndex;\n         j--;\n      }\n   });\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use Kokkos to sort in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0},"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "output": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   #pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n         }\n      }\n   }\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   #pragma omp parallel for\n   for (int i = 0; i < results.size(); i++) {\n      for (int j = i + 1; j < results.size(); j++) {\n         if (results[i].startTime > results[j].startTime) {\n            Result temp = results[i];\n            results[i] = results[j];\n            results[j] = temp;\n         }\n      }\n   }\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   #pragma omp parallel for\n   for (int i = 0; i < results.size"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "serial",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "output": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "hip",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {",
        "output": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use AMD HIP to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(Result *results, size_t N) {\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n   for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n      for (int j = i; j > 0; j--) {\n         if (results[j].startTime < results[j-1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j-1];\n            results[j-1] = temp;\n         } else {\n            break;\n         }\n      }\n   }\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTimeCuda(Result *results, size_t N) {\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n   for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n      for (int j = i; j > 0; j--) {\n         if (results[j].startTime < results[j-1].startTime) {\n            Result temp = results[j];\n            results[j] = results[j-1];\n            results[j-1] = temp;\n         } else {\n            break;\n         }\n      }\n   }\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "output": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   int n = results.size();\n   int *sendbuf = new int[n];\n   int *recvbuf = new int[n];\n   for (int i = 0; i < n; i++) {\n      sendbuf[i] = results[i].startTime;\n   }\n   MPI_Alltoall(sendbuf, 1, MPI_INT, recvbuf, 1, MPI_INT, MPI_COMM_WORLD);\n   std::vector<Result> sortedResults;\n   for (int i = 0; i < n; i++) {\n      sortedResults.push_back(results[recvbuf[i]]);\n   }\n   results = sortedResults;\n   delete[] sendbuf;\n   delete[] recvbuf;\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   int n = results.size();\n   int *sendbuf = new int[n];\n   int *recvbuf = new int[n];\n   for (int i = 0; i < n; i++) {\n      sendbuf[i] = results[i].startTime;\n   }\n   MPI_Alltoall(sendbuf, 1, MPI_INT, recvbuf, 1, MPI_INT, MPI_COMM_WORLD);\n   std::vector<Result> sortedResults;\n   for (int i = 0; i < n; i++) {\n      sortedResults.push_back(results[recvbuf[i]]);\n   }\n   results = sortedResults;\n   delete[] sendbuf;\n   delete[] recvbuf;\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n  "
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "cuda",
        "prompt": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(thrust::device_vector<Result> &results) {",
        "output": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(thrust::device_vector<Result> &results) {\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= results.size()) return;\n   int i = tid;\n   int j = tid;\n   Result tmp;\n   while (j < results.size()) {\n      if (results[i].startTime > results[j].startTime) {\n         tmp = results[i];\n         results[i] = results[j];\n         results[j] = tmp;\n      }\n      i = j;\n      j = 2*i + 1;\n   }\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\n__global__ void sortByStartTime(thrust::device_vector<Result> &results, int start, int end) {\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= end - start) return;\n   int i = tid;\n   int j = tid;\n   Result tmp;\n   while (j < end - start) {\n      if (results[start + i].startTime > results[start + j].startTime) {\n         tmp = results[start + i];\n         results[start + i] = results[start + j];\n         results[start + j] = tmp;\n      }\n      i = j;\n      j = 2*i + 1;\n   }\n}\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use CUDA to sort in parallel. The kernel is launched with at least as many threads as there are elements.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "2_sort_structs_by_key",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI and OpenMP to sort in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n   int numThreads = omp_get_max_threads();\n   int numResults = results.size();\n   int numTasks = numResults / numThreads;\n   int remainder = numResults % numThreads;\n   int *taskSize = new int[numThreads];\n   int *taskOffset = new int[numThreads];\n   int *taskStart = new int[numThreads];\n   int *taskEnd = new int[numThreads];\n   int *taskResults = new int[numThreads];\n   int *taskResultsSize = new int[numThreads];\n   int *taskResultsOffset = new int[numThreads];\n   int *taskResultsStart = new int[numThreads];\n   int *taskResultsEnd = new int[numThreads];\n   int *taskResultsSorted = new int[numThreads];\n   int *taskResultsSortedSize = new int[numThreads];\n   int *taskResultsSortedOffset = new int[numThreads];\n   int *taskResultsSortedStart = new int[numThreads];\n   int *taskResultsSortedEnd = new int[numThreads];\n   int *taskResultsSortedIndex = new int[numThreads];\n   int *taskResultsSortedIndexSize = new int[numThreads];\n   int *taskResultsSortedIndexOffset = new int[numThreads];\n   int *taskResultsSortedIndexStart = new int[numThreads];\n   int *taskResultsSortedIndexEnd = new int[numThreads];\n   int *taskResultsSortedIndexSorted = new int[numThreads];\n   int *taskResultsSortedIndexSortedSize = new int[numThreads];\n   int *taskResultsSortedIndexSortedOffset = new int[numThreads];\n   int *taskResultsSortedIndexSortedStart = new int[numThreads];\n   int *taskResultsSortedIndexSortedEnd = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndex = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndexSize = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndexOffset = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndexStart = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndexEnd = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndexSorted = new int[numThreads];\n   int *taskResultsSortedIndexSortedIndexSortedSize = new int[numThreads];\n   int *task"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Find the k-th smallest element of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(Kokkos::View<const int*> const& x, int k) {",
        "output": "#include <Kokkos_Core.hpp>\n\n/* Find the k-th smallest element of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(Kokkos::View<const int*> const& x, int k) {\n    // TODO: Implement this function.\n    return 0;\n}\n\nint main(int argc, char* argv[]) {\n    Kokkos::initialize(argc, argv);\n    {\n        // TODO: Create a Kokkos::View to hold the input vector.\n        Kokkos::View<const int*> x;\n        // TODO: Read the input vector from the command line.\n        // TODO: Call findKthSmallest() to find the k-th smallest element.\n        // TODO: Print the result.\n    }\n    Kokkos::finalize();\n    return 0;\n}\n"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "output": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    int n = x.size();\n    int *y = new int[n];\n    for (int i = 0; i < n; i++) {\n        y[i] = x[i];\n    }\n    int *z = new int[n];\n    for (int i = 0; i < n; i++) {\n        z[i] = i;\n    }\n    int *p = new int[n];\n    for (int i = 0; i < n; i++) {\n        p[i] = 0;\n    }\n    int *q = new int[n];\n    for (int i = 0; i < n; i++) {\n        q[i] = 0;\n    }\n    int *r = new int[n];\n    for (int i = 0; i < n; i++) {\n        r[i] = 0;\n    }\n    int *s = new int[n];\n    for (int i = 0; i < n; i++) {\n        s[i] = 0;\n    }\n    int *t = new int[n];\n    for (int i = 0; i < n; i++) {\n        t[i] = 0;\n    }\n    int *u = new int[n];\n    for (int i = 0; i < n; i++) {\n        u[i] = 0;\n    }\n    int *v = new int[n];\n    for (int i = 0; i < n; i++) {\n        v[i] = 0;\n    }\n    int *w = new int[n];\n    for (int i = 0; i < n; i++) {\n        w[i] = 0;\n    }\n    int *x1 = new int[n];\n    for (int i = 0; i < n; i++) {\n        x1[i] = 0;\n    }\n    int *y1 = new int[n];\n    for (int i = 0; i < n; i++) {\n        y1[i] = 0;\n    }\n    int *z1 = new int[n];\n    for (int i = 0; i < n; i++) {\n        z1[i] = 0;\n    }\n    int *a ="
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "serial",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "output": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    // write your code here\n    return 0;\n}\n\n/* Find the k-th largest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 10\n*/\nint findKthLargest(std::vector<int> const& x, int k) {\n    // write your code here\n    return 0;\n}\n\n/* Find the k-th largest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 10\n*/\nint findKthLargest(std::vector<int> const& x, int k) {\n    // write your code here\n    return 0;\n}\n\n/* Find the k-th largest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 10\n*/\nint findKthLargest(std::vector<int> const& x, int k) {\n    // write your code here\n    return 0;\n}\n\n/* Find the k-th largest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 10\n*/\nint findKthLargest(std::vector<int> const& x, int k) {\n    // write your code here\n    return 0;\n}\n\n/* Find the k-th largest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 10\n*/\nint findKthLargest(std::vector<int> const& x, int k) {\n    // write your code here\n    return 0;\n}\n\n/* Find the k-th largest element of the vector x.\n   Example:\n   \n   input: x"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "hip",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int &kthSmallest) {",
        "output": "/* Find the k-th smallest element of the vector x.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(const int *x, size_t N, int k, int &kthSmallest) {\n    // TODO: implement this function\n    // You may need to add additional variables to store intermediate results\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads() to ensure all threads are done before returning\n    // You may need to use __shared__ variables to store intermediate results\n    // You may need to use atomicCAS() to update kthSmallest\n    // You may need to use __syncthreads"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "output": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    int n = x.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Step 1: partition x into n/size sub-vectors\n    int local_n = n / size;\n    int local_start = local_n * rank;\n    int local_end = local_start + local_n;\n    std::vector<int> local_x(x.begin() + local_start, x.begin() + local_end);\n\n    // Step 2: sort local_x\n    std::sort(local_x.begin(), local_x.end());\n\n    // Step 3: find k-th smallest element\n    int k_smallest = local_x[k - 1];\n\n    // Step 4: gather k-th smallest element from all processes\n    int k_smallest_all;\n    MPI_Allreduce(&k_smallest, &k_smallest_all, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n    return k_smallest_all;\n}\n\nint main() {\n    int rank, size;\n    MPI_Init(NULL, NULL);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // test\n    std::vector<int> x = {1, 7, 6, 0, 2, 2, 10, 6};\n    int k = 4;\n    int k_smallest = findKthSmallest(x, k);\n    if (rank == 0) {\n        std::cout << \"The \" << k << \"-th smallest element is \" << k_smallest << std::endl;\n    }\n\n    MPI_Finalize();\n    return 0;\n}\n'''\n\n# %%\n# \u7f16\u8bd1\u5e76\u8fd0\u884c\n\n#!mpic++ -o findKthSmallest findKthSmallest.cpp\n#!mpirun -np 4./findKthSmallest\n\n# %%\n# \u8f93\u51fa\n# ```\n# The 4-th smallest element is 6\n# ```\n\n# %%\n# \u6211\u4eec\u6765"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "cuda",
        "prompt": "/* Find the k-th smallest element of the vector x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(thrust::device_vector<int> const& x, int k, int &kthSmallest) {",
        "output": "/* Find the k-th smallest element of the vector x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\n__global__ void findKthSmallest(thrust::device_vector<int> const& x, int k, int &kthSmallest) {\n    // TODO: implement this function\n    // You may need to use thrust::device_vector<int> to store the input vector x\n    // You may need to use thrust::device_vector<int>::iterator to iterate through the vector x\n    // You may need to use thrust::device_vector<int>::iterator::operator[] to access the value of an element in the vector x\n    // You may need to use thrust::device_vector<int>::iterator::operator- to compute the distance between two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator< to compare two values in the vector x\n    // You may need to use thrust::device_vector<int>::iterator::operator+ to compute the offset of an iterator\n    // You may need to use thrust::device_vector<int>::iterator::operator++ to increment an iterator\n    // You may need to use thrust::device_vector<int>::iterator::operator< to compare two values in the vector x\n    // You may need to use thrust::device_vector<int>::iterator::operator* to dereference an iterator\n    // You may need to use thrust::device_vector<int>::iterator::operator== to compare two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator!= to compare two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator- to compute the distance between two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator+ to compute the offset of an iterator\n    // You may need to use thrust::device_vector<int>::iterator::operator++ to increment an iterator\n    // You may need to use thrust::device_vector<int>::iterator::operator< to compare two values in the vector x\n    // You may need to use thrust::device_vector<int>::iterator::operator* to dereference an iterator\n    // You may need to use thrust::device_vector<int>::iterator::operator== to compare two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator!= to compare two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator- to compute the distance between two iterators\n    // You may need to use thrust::device_vector<int>::iterator::operator+ to compute the offset"
    },
    {
        "problem_type": "sort",
        "language": "cpp",
        "name": "5_sort_kth_smallest",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
        "output": "#include <mpi.h>\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n    int n = x.size();\n    int local_min = x[0];\n    int local_min_index = 0;\n    int local_min_count = 0;\n    int local_min_count_threshold = k;\n    int local_min_count_threshold_max = n - k;\n    int local_min_count_threshold_min = k;\n    int local_min_count_threshold_step = 1;\n    int local_min_count_threshold_step_max = 1;\n    int local_min_count_threshold_step_min = 1;\n    int local_min_count_threshold_step_step = 1;\n    int local_min_count_threshold_step_step_max = 1;\n    int local_min_count_threshold_step_step_min = 1;\n    int local_min_count_threshold_step_step_step = 1;\n    int local_min_count_threshold_step_step_step_max = 1;\n    int local_min_count_threshold_step_step_step_min = 1;\n    int local_min_count_threshold_step_step_step_step = 1;\n    int local_min_count_threshold_step_step_step_step_max = 1;\n    int local_min_count_threshold_step_step_step_step_min = 1;\n    int local_min_count_threshold_step_step_step_step_step = 1;\n    int local_min_count_threshold_step_step_step_step_step_max = 1;\n    int local_min_count_threshold_step_step_step_step_step_min = 1;\n    int local_min_count_threshold_step_step_step_step_step_step = 1;\n    int local_min_count_threshold_step_step_step_step_step_step_max = 1;\n    int local_min_count_threshold_step_step_step_step_step_step_min = 1;\n    int local_min_count_threshold_step_step_step_step_step_step_step = 1;\n    int local_min_count"
    }
]