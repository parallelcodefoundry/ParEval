/* Compute the ReLU function on every element of x. Elements less than zero become zero,
   while elements greater than zero stay the same.
   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.
   Example:

   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]
   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]
*/
__global__ void relu(double *x, size_t N) {