{"section_title": "1 Introduction", "chunk": ""}
{"section_title": "1 Introduction", "chunk": "1 1 Introduction \n2 This collection of programming examples supplements the OpenMP API for Shared Memory \n3 Parallelization specifications, and is not part of the formal specifications. It assumes familiarity \n4 with the OpenMP specifications, and shares the typographical conventions used in that document. \n5 The OpenMP API specification provides a model for parallel programming that is portable across \n6 shared memory architectures from different vendors. Compilers from numerous vendors support \n7 the OpenMP API. \n8 The directives, library routines, and environment variables demonstrated in this document allow \n9 users to create and manage parallel programs while permitting portability. The directives extend the \n10 C, C++ and Fortran base languages with single program multiple data (SPMD) constructs, tasking \n11 constructs, device constructs, worksharing constructs, and synchronization constructs, and they \n12 provide support for sharing and privatizing data. The functionality to control the runtime \n13 environment is provided by library routines and environment variables. Compilers that support the \n14 OpenMP API often include a command line option to the compiler that activates and allows \n15 interpretation of all OpenMP directives. \n16 The documents and source codes for OpenMP Examples can be downloaded from \n17 https://github.com/OpenMP/Examples. Each directory holds the contents of a chapter and has a \n18 sources subdirectory of its codes. This OpenMP Examples 5.2.1 document and its codes are tagged \n19 as v5.2.1. \n20 Complete information about the OpenMP API and a list of the compilers that support the OpenMP \n21 API can be found at the OpenMP.org web site \n22 https://www.openmp.org \n1 \n"}
{"section_title": "1.1 Examples Organization", "chunk": ""}
{"section_title": "1.1 Examples Organization", "chunk": "2 This document includes examples of the OpenMP API directives, constructs, and routines. \n3 Each example is labeled with ename.seq-id.ext, where ename is the example name, seq-id is the \n4 sequence identifier in a section, and ext is the source file extension to indicate the code type and \n5 source form. ext is one of the following: \n6 c \u2013 C code, \n7 cpp \u2013 C++ code, \n8 f \u2013 Fortran code in fixed form, and \n9 f90 \u2013 Fortran code in free form. \n10 Example labels include version information of the form (omp_verno) to indicate features that are \n11 illustrated by an example for a specific OpenMP version, such as \u201cscan.1.c (omp_5.0).\u201d Some of \n12 the example labels include version information of the form (pre_omp_3.0) to indicate features that \n13 are specified prior to OpenMP version 3.0, such as \u201cploop.1.c (pre_omp_3.0).\u201d \n14 Language markers may be used to indicate text or codes that are specific to a particular base \n15 language. \nC / C++ \n16 This is C/C++ specific: A statement following a directive is compound only when necessary, and a \n17 non-compound statement is indented with respect to a directive preceding it. \nC / C++ \nFortran \n18 This is Fortran specific... \nFortran \n2 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "2 OpenMP Directive Syntax", "chunk": ""}
{"section_title": "2 OpenMP Directive Syntax", "chunk": "2 OpenMP directives use base-language mechanisms to specify OpenMP program behavior. In C \n3 code, the directives are formed exclusively with pragmas, whereas in C++ code, directives are \n4 formed from either pragmas or attributes. Fortran directives are formed with comments in free form \n5 and fixed form sources (codes). All of these mechanisms allow the compilation to ignore the \n6 OpenMP directives if OpenMP is not supported or enabled. \n7 The OpenMP directive is a combination of the base-language mechanism and a \n8 directive-specification, as shown below. The directive-specification consists of the directive-name \n9 which may seldomly have arguments, followed by optional clauses. Full details of the syntax can be \n10 found in the OpenMP Specification. Illustrations of the syntax is given in the examples. \n11 The formats for combining a base-language mechanism and a directive-specification are: \n12 C/C++ pragmas \n13 #pragma omp directive-specification \n14 C++ attributes \n15 [[omp :: directive( directive-specification )]] \n16 [[using omp : directive( directive-specification )]] \n17 Fortran comments \n18 !$omp directive-specification \n19 where c$omp and *$omp may be used in Fortran fixed form sources. \n20 Most OpenMP directives accept clauses that alter the semantics of the directive in some way, and \n21 some directives also accept parenthesized arguments that follow the directive name. A clause may \n22 just be a keyword (e.g., untied) or it may also accept argument lists (e.g., shared(x,y,z)) \n23 and/or optional modifiers (e.g., tofrom in map(tofrom: x,y,z)). Clause modifiers may be \n24 \u201csimple\u201d or \u201ccomplex\u201d \u2013 a complex modifier consists of a keyword followed by one or more \n25 parameters, bracketed by parentheses, while a simple modifier does not. An example of a complex \n26 modifier is the iterator modifier, as in map(iterator(i=0:n), tofrom: p[i]), or the \n27 step modifier, as in linear(x: ref, step(4)). In the preceding examples, tofrom and \n28 ref are simple modifiers. \n29 For Fortran, a declarative directive (such as declare reduction) must appear after any USE, \n30 IMPORT, and IMPLICIT statements in the specification part. \n3 \n"}
{"section_title": "2.1 C/C++ Pragmas", "chunk": ""}
{"section_title": "2.1 C/C++ Pragmas", "chunk": "2 OpenMP C and C++ directives can be specified with the C/C++ #pragma directive. An OpenMP \n3 directive begins with #pragma omp and is followed by the OpenMP directive name, and required \n4 and optional clauses. Lines are continued in the usual manner, and comments may be included at \n5 the end. Directives are case sensitive. \n6 The example below illustrates the use of the OpenMP pragma form. The first pragma (PRAG 1) \n7 specifies a combined parallel for directive, with a num_threads clause, and a comment. \n8 The second pragma (PRAG 2) shows the same directive split across two lines. The next nested \n9 pragmas (PRAG 3 and 4) show the previous combined directive as two separate directives. The \n10 executable directives above all apply to the next statement. The parallel directive can be \n11 applied to a structured block as shown in PRAG 5. \nC / C++ \n12 Example directive_syntax_pragma.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 #define NT 4 \nS-4 #define thrd_no omp_get_thread_num \nS-5 \nS-6 int main(){ \nS-7 #pragma omp parallel for num_threads(NT) // PRAG 1 \nS-8 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-9 \nS-10 #pragma omp parallel for \\ \nS-11 num_threads(NT) // PRAG 2 \nS-12 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-13 \nS-14 #pragma omp parallel num_threads(NT) // PRAG 3-4 \nS-15 #pragma omp for \nS-16 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-17 \nS-18 #pragma omp parallel num_threads(NT) // PRAG 5 \nS-19 { \nS-20 int no = thrd_no(); \nS-21 if (no%2) { printf(\"thrd no %d is Odd"}
{"section_title": "2.1 C/C++ Pragmas", "chunk": "2 OpenMP C and C++ directives can be specified with the C/C++ #pragma directive. An OpenMP \n3 directive begins with #pragma omp and is followed by the OpenMP directive name, and required \n4 and optional clauses. Lines are continued in the usual manner, and comments may be included at \n5 the end. Directives are case sensitive. \n6 The example below illustrates the use of the OpenMP pragma form. The first pragma (PRAG 1) \n7 specifies a combined parallel for directive, with a num_threads clause, and a comment. \n8 The second pragma (PRAG 2) shows the same directive split across two lines. The next nested \n9 pragmas (PRAG 3 and 4) show the previous combined directive as two separate directives. The \n10 executable directives above all apply to the next statement. The parallel directive can be \n11 applied to a structured block as shown in PRAG 5. \nC / C++ \n12 Example directive_syntax_pragma.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 #define NT 4 \nS-4 #define thrd_no omp_get_thread_num \nS-5 \nS-6 int main(){ \nS-7 #pragma omp parallel for num_threads(NT) // PRAG 1 \nS-8 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-9 \nS-10 #pragma omp parallel for \\ \nS-11 num_threads(NT) // PRAG 2 \nS-12 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-13 \nS-14 #pragma omp parallel num_threads(NT) // PRAG 3-4 \nS-15 #pragma omp for \nS-16 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-17 \nS-18 #pragma omp parallel num_threads(NT) // PRAG 5 \nS-19 { \nS-20 int no = thrd_no(); \nS-21 if (no%2) { printf(\"thrd no %d is Odd\",no);} \nS-22 else { printf(\"thrd no %d is Even\\n\",no);} \nS-23 \nS-24 #pragma omp for \nS-25 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-26 } \nS-27 } \nS-28 /* \nS-29 repeated 4 times, any order \nS-30 OUTPUT: thrd no 0 \n4 OpenMP Examples Version 5.2.1 - November 2022 \nS-31 OUTPUT: thrd no 1 \nS-32 OUTPUT: thrd no 2 \nS-33 OUTPUT: thrd no 3 \nS-34 \nS-35 any order \nS-36 OUTPUT: thrd no 0 is Even \nS-37 OUTPUT: thrd no 2 is Even \nS-38 OUTPUT: thrd no 1 is Odd \nS-39 OUTPUT: thrd no 3 is Odd \nS-40 */ \nC / C++ \n"}
{"section_title": "2.2 C++ Attributes", "chunk": ""}
{"section_title": "2.2 C++ Attributes", "chunk": "2 OpenMP directives for C++ can also be specified with the directive extension for the C++11 \n3 standard attributes. \n4 The C++ example below shows two ways to parallelize a for loop using the #pragma syntax. \n5 The first pragma uses the combined parallel for directive, and the second applies the \n6 uncombined closely nested directives, parallel and for, directly to the same statement. These \n7 are labeled PRAG 1-3. \n8 Using the attribute syntax, the same construct in PRAG 1 is applied two different ways in attribute \n9 form, as shown in the ATTR 1 and ATTR 2 sections. In ATTR 1 the attribute syntax is used with \n10 the omp :: namespace form. In ATTR 2 the attribute syntax is used with the using omp : \n11 namespace form. \n12 Next, parallelization is attempted by applying directives using two different syntaxes. For ATTR 3 \n13 and PRAG 4, the loop parallelization will fail to compile because multiple directives that apply to \n14 the same statement must all use either the attribute syntax or the pragma syntax. The lines have \n15 been commented out and labeled INVALID. \n16 While multiple attributes may be applied to the same statement, compilation may fail if the \n17 ordering of the directive matters. For the ATTR 4-5 loop parallelization, the parallel directive \n18 precedes the for directive, but the compiler may reorder consecutive attributes. If the directives \n19 are reversed, compilation will fail. \n20 The attribute directive of the ATTR 6 section resolves the previous problem (in ATTR 4-5). Here, \n21 the sequence attribute is used to apply ordering to the directives of ATTR 4-5, using the omp :: \n22 namespace qualifier. (The using omp : namespace form is not available for the sequence \n23 attribute.) Note, for the sequence attribute a comma must separate the directive extensions. \n24 The last 3 pairs of sections (PRAG DECL 1-2, 3-4, and 5-6) show cases where directive ordering \n25 does not matter for declare simd directives. \nCHAPTER 2. OPENMP DIRECTIVE SYNTAX 5 \n1 In section PRAG DECL 1-2, the two loops use different SIMD forms of the P function (one with \n2 simdlen(4) and the other with simdlen(8)), as prescribed by the two different \n3 declare simd directives applied to the P function definitions (at the beginning of the code). The \n4 directives use the pragma syntax, and order is not important. For the next set of loops (PRAG \n5 DECL 3-4) that use the Q function, the attribute syntax is used for the declare simd directives. \n6 The result is compliant code since directive order is irrelevant. Sections ATTR DECL 5-6 are \n7 included for completeness. Here, the attribute form of the simd directive is used for loops calling \n8 the Q function, in combination with the attribute form of the declare simd directives declaring \n9 the variants for Q. \nC++ \n10 Example directive_syntax_attribute.1.cpp (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 #define NT 4 \nS-4 #define thrd_no omp_get_thread_num \nS-5 \nS-6 #pragma omp declare simd linear(i) simdlen(4) \nS-7 #pragma omp declare simd linear(i) simdlen(8) \nS-8 double P(int i){ return (double)i * (double)i; } \nS-9 \nS-10 [[omp::directive(declare simd linear(i) simdlen(4))]] \nS-11 [[omp::directive(declare simd linear(i) simdlen(8))]] \nS-12 double Q(int i){ return (double)i * (double)i; } \nS-13 \nS-14 int main() { \nS-15 \nS-16 #pragma omp parallel for num_threads(NT) // PRAG 1 \nS-17 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-18 \nS-19 #pragma omp parallel num_threads(NT) // PRAG 2 \nS-20 #pragma omp for // PRAG 3 \nS-21 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-22 \nS-23 // ATTR 1 \nS-24 [[omp::directive( parallel for num_threads(NT))]] \nS-25 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-26 \nS-27 // ATTR 2 \nS-28 [[using omp : directive( parallel for num_threads(NT))]] \nS-29 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-30 \nS-31 // INVALID-- attribute and non-attribute on same statement \nS-32 // [[ omp :: directive( parallel num_threads(NT) ) ]] ATTR 3 \nS-33 // #pragma omp for PRAG 4 \nS-34 // for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \n6 OpenMP Examples Version 5.2.1 - November 2022 \nS-35 \nS-36 \nS-37 // INVALID-- directive order not guaranteed \nS-38 // [[ omp :: directive( parallel num_threads(NT) ) ]] ATTR 4 \nS-39 // [[ omp :: directive( for ) ]] ATTR 5 \nS-40 // for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-41 \nS-42 // ATTR 6 \nS-43 [[omp::sequence(directive(parallel num_threads(NT)),directive(for))]] \nS-44 for(int i=0; i<NT; i++) printf(\"thrd no %d\\n\",thrd_no()); \nS-45 \nS-46 double tmp=0.0f; \nS-47 #pragma omp simd reduction(+:tmp) simdlen(4) \nS-48 for(int i=0;i<100;i++) tmp += P(i); // PRAG DECL 1 \nS-49 #pragma omp simd reduction(+:tmp) simdlen(8) \nS-50 for(int i=0;i<100;i++) tmp += P(i); // PRAG DECL 2 \nS-51 printf(\"%f\\n\",tmp); \nS-52 \nS-53 tmp=0.0f; \nS-54 #pragma omp simd reduction(+:tmp) simdlen(4) \nS-55 for(int i=0;i<100;i++) tmp += Q(i); // ATTR DECL 3 \nS-56 #pragma omp simd reduction(+:tmp) simdlen(8) \nS-57 for(int i=0;i<100;i++) tmp += Q(i); // ATTR DECL 4 \nS-58 printf(\"%f\\n\",tmp); \nS-59 \nS-60 tmp=0.0f; \nS-61 [[ omp :: directive(simd reduction(+:tmp) simdlen(4))]] \nS-62 for(int i=0;i<100;i++) tmp += Q(i); // ATTR DECL 5 \nS-63 [[ omp :: directive(simd reduction(+:tmp) simdlen(8))]] \nS-64 for(int i=0;i<100;i++) tmp += Q(i); // ATTR DECL 6 \nS-65 printf(\"%f\\n\",tmp); \nS-66 } \nS-67 // repeated 5 times, any order: \nS-68 // OUTPUT: thrd no 0 \nS-69 // OUTPUT: thrd no 1 \nS-70 // OUTPUT: thrd no 2 \nS-71 // OUTPUT: thrd no 3 \nS-72 \nS-73 // repeated 3 time: \nS-74 // OUTPUT: 656700.000000 \nC++ \nCHAPTER 2. OPENMP DIRECTIVE SYNTAX 7 \n"}
{"section_title": "2.3 Fortran Comments (Fixed Source Form)", "chunk": ""}
{"section_title": "2.3 Fortran Comments (Fixed Source Form)", "chunk": "2 OpenMP directives in Fortran codes with fixed source form are specified as comments with one of \n3 the !$omp, c$omp, and *$omp sentinels, followed by a directive name, and required and optional \n4 clauses. The sentinel must begin in column 1. \n5 In the example below the first directive (DIR 1) specifies the parallel do combined directive, \n6 with a num_threads clause, and a comment. The second directive (DIR 2) shows the same \n7 directive split across two lines. The next nested directives (DIR 3 and 4) show the previous \n8 combined directive as two separate directives. Here, an end directive (end parallel) must be \n9 specified to demarcate the range (region) of the parallel directive. \nFortran \n10 Example directive_syntax_F_fixed_comment.1.f (pre_omp_3.0) \nS-1 program main \nS-2 include \u2019omp_lib.h\u2019 \nS-3 integer NT \nS-4 \nS-5 NT =4 \nS-6 \nS-7 c sentinel c$omp or *$omp can also be used \nS-8 \nS-9 c$omp parallel do num_threads(NT) !comments allowed here DIR 1 \nS-10 do i = 1,NT \nS-11 write(*,\u2019(\"thrd no\", i2)\u2019) omp_get_thread_num() \nS-12 end do \nS-13 \nS-14 !$omp parallel do \nS-15 !$omp+ num_threads(NT) !cont. w. char in col. 6 DIR 2 \nS-16 do i = 1,NT \nS-17 write(*,\u2019(\"thrd no\", i2)\u2019) omp_get_thread_num() \nS-18 end do \nS-19 \nS-20 *$omp parallel num_threads(NT) !multi-directive form DIR 3 \nS-21 *$omp do ! DIR 4 \nS-22 do i = 1,NT \nS-23 write(*,\u2019(\"thrd no\", i2)\u2019) omp_get_thread_num() \nS-24 end do \nS-25 *$omp end parallel \nS-26 end \nS-27 ! repeated 3 times, any order \nS-28 ! OUTPUT: thrd no 0 \nS-29 ! OUTPUT: thrd no 1 \nS-30 ! OUTPUT: thrd no 2 \nS-31 ! OUTPUT: thrd no 3 \nFortran \n8 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "2.4 Fortran Comments (Free Source Form)", "chunk": ""}
{"section_title": "2.4 Fortran Comments (Free Source Form)", "chunk": "2 OpenMP directives in Fortran codes with free source form are specified as comments that use the \n3 !$omp sentinel, followed by the directive name, and required and optional clauses. Lines are \n4 continued with an ending ampersand (&), and the continued line begins with !$omp or !$omp&. \n5 Comments may appear on the same line as the directive. Directives are case insensitive. \n6 In the example below the first directive (DIR 1) specifies the parallel do combined directive, \n7 with a num_threads clause, and a comment. The second directive (DIR 2) shows the same \n8 directive split across two lines. The next nested directives (DIR 3 and 4) show the previous \n9 combined directive as two separate directives. Here, an end directive (end parallel) must be \n10 specified to demarcate the range (region) of the parallel directive. \nFortran \n11 Example directive_syntax_F_free_comment.1.f90 (pre_omp_3.0) \nS-1 program main \nS-2 use omp_lib \nS-3 integer,parameter :: NT = 4 \nS-4 \nS-5 !$omp parallel do num_threads(NT) !DIR 1 \nS-6 do i = 1,NT \nS-7 write(*,\u2019(\"thrd no\", i2)\u2019) omp_get_thread_num() \nS-8 end do \nS-9 \nS-10 !$omp parallel do & !continue line !DIR 2 \nS-11 !$omp num_threads(NT) !or !$omp& \nS-12 do i = 1,NT \nS-13 write(*,\u2019(\"thrd no\", i2)\u2019) omp_get_thread_num() \nS-14 end do \nS-15 \nS-16 !$omp parallel num_threads(NT) !DIR 3 \nS-17 !$omp do !DIR 4 \nS-18 do i = 1,NT \nS-19 write(*,\u2019(\"thrd no\", i2)\u2019) omp_get_thread_num() \nS-20 end do \nS-21 !$omp end parallel \nS-22 \nS-23 end program \nS-24 \nS-25 ! repeated 3 times, any order \nS-26 ! OUTPUT: thrd no 0 \nS-27 ! OUTPUT: thrd no 1 \nS-28 ! OUTPUT: thrd no 2 \nS-29 ! OUTPUT: thrd no 3 \nFortran \nCHAPTER 2. OPENMP DIRECTIVE SYNTAX 9 \n1 As of OpenMP 5.1, block and end block statements can be used to designate a structured block \n2 for an OpenMP region, and any paired OpenMP end directive becomes optional, as shown in the \n3 next example. Note, the variables i and thrd_no are declared within the block structure and are \n4 hence private. It was necessary to explicitly declare the i variable, due to the implicit none \n5 statement; it could have also been declared outside the structured block. \nFortran \n6 Example directive_syntax_F_block.1.f90 (omp_5.1) \nS-1 program main \nS-2 \nS-3 use omp_lib \nS-4 implicit none \nS-5 integer,parameter :: NT = 2, chunks=3 \nS-6 \nS-7 !$omp parallel num_threads(NT) \nS-8 block ! Fortran 2008 OMP 5.1 \nS-9 integer :: thrd_no,i \nS-10 thrd_no= omp_get_thread_num() \nS-11 !$omp do schedule(static,chunks) \nS-12 do i = 1,NT*chunks \nS-13 write(*,\u2019(\"ndx=\",i0.2,\" thrd_no=\", i0.2)\u2019) i,thrd_no \nS-14 end do \nS-15 end block \nS-16 end program \nS-17 \nS-18 ! any order \nS-19 ! OUTPUT: ndx=01 thrd_no=00 \nS-20 ! OUTPUT: ndx=02 thrd_no=00 \nS-21 ! OUTPUT: ndx=03 thrd_no=00 \nS-22 ! OUTPUT: ndx=04 thrd_no=01 \nS-23 ! OUTPUT: ndx=05 thrd_no=01 \nS-24 ! OUTPUT: ndx=06 thrd_no=01 \nFortran \n7 A Fortran BLOCK construct may eliminate the need for a paired end directive for an OpenMP \n8 construct, as illustrated in the following example. \n9 The first parallel construct is specified with an OpenMP loosely structured block (where the \n10 first executable construct is not a Fortran 2008 BLOCK construct). A paired end directive must \n11 end the OpenMP construct. The second parallel construct is specified with an OpenMP strictly \n12 structured block (consists only of a single Fortran BLOCK construct). The paired end directive is \n13 optional in this case, and is not used here. \n14 The next two parallel directives form an enclosing outer parallel construct and a nested \n15 inner parallel construct. The first end parallel directive that subsequently appears \n16 terminates the inner parallel construct, because a paired end directive immediately following a \n10 OpenMP Examples Version 5.2.1 - November 2022 \n1 BLOCK construct that is a strictly structured block of an OpenMP construct is treated as the \n2 terminating end directive of that construct. The next end parallel directive is required to \n3 terminate the outer parallel construct. \nFortran \n4 Example directive_syntax_F_block.2.f90 (omp_5.1) \nS-1 program main \nS-2 \nS-3 use omp_lib \nS-4 implicit none \nS-5 \nS-6 !$omp parallel num_threads(2) \nS-7 if( omp_get_thread_num() == 0 ) & \nS-8 print*, \"Loosely structured block -- end required.\" \nS-9 block ! BLOCK Fortran 2008 \nS-10 if( omp_get_thread_num() == 0 ) & \nS-11 print*, \" --\" \nS-12 end block \nS-13 !$omp end parallel \nS-14 \nS-15 !$omp parallel num_threads(2) \nS-16 block \nS-17 if( omp_get_thread_num() == 0 ) & \nS-18 print*, \"Strictly structured block -- end not required.\" \nS-19 end block \nS-20 !!$omp end parallel !is optional for strictly structured block \nS-21 \nS-22 print*, \"Sequential part\" \nS-23 \nS-24 !$omp parallel num_threads(2) !outer parallel \nS-25 if( omp_get_thread_num() == 0 ) & \nS-26 print*, \"Outer, loosely structured block.\" \nS-27 !$omp parallel num_threads(2) !inner parallel \nS-28 block \nS-29 if( omp_get_thread_num() == 0 ) & \nS-30 print*, \"Inner, strictly structured block.\" \nS-31 end block \nS-32 !$omp end parallel \nS-33 !$omp end parallel \nS-34 ! Two end directives are required here. \nS-35 ! A single \"!$omp end parallel\" terminator will fail. \nS-36 ! 1st end directive is assumed to be for inner parallel construct. \nS-37 ! 2nd end directive applies to outer parallel construct. \nS-38 \nS-39 end program \nS-40 \nCHAPTER 2. OPENMP DIRECTIVE SYNTAX 11 \nS-41 !OUTPUT, in order: \nS-42 ! Loosely structured block -- end required. \nS-43 ! -- \nS-44 ! Strictly structured block -- end not required. \nS-45 ! Sequential part \nS-46 ! Outer, loosely structured block. \nS-47 ! Inner, strictly structured block. \nS-48 ! Inner, strictly structured block. \nFortran \n12 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3 Parallel Execution", "chunk": ""}
{"section_title": "3 Parallel Execution", "chunk": "2 A single thread, the initial thread, begins sequential execution of an OpenMP enabled program, as \n3 if the whole program is in an implicit parallel region consisting of an implicit task executed by the \n4 initial thread. \n5 A parallel construct encloses code, forming a parallel region. An initial thread encountering a \n6 parallel region forks (creates) a team of threads at the beginning of the parallel region, and \n7 joins them (removes from execution) at the end of the region. The initial thread becomes the \n8 primary thread of the team in a parallel region with a thread number equal to zero, the other \n9 threads are numbered from 1 to number of threads minus 1. A team may be comprised of just a \n10 single thread. \n11 Each thread of a team is assigned an implicit task consisting of code within the parallel region. The \n12 task that creates a parallel region is suspended while the tasks of the team are executed. A thread is \n13 tied to its task; that is, only the thread assigned to the task can execute that task. After completion \n14 of the parallel region, the primary thread resumes execution of the generating task. \n15 Any task within a parallel region is allowed to encounter another parallel region to form a \n16 nested parallel region. The parallelism of a nested parallel region (whether it forks \n17 additional threads, or is executed serially by the encountering task) can be controlled by the \n18 OMP_NESTED environment variable or the omp_set_nested() API routine with arguments \n19 indicating true or false. \n20 The number of threads of a parallel region can be set by the OMP_NUM_THREADS \n21 environment variable, the omp_set_num_threads() routine, or on the parallel directive \n22 with the num_threads clause. The routine overrides the environment variable, and the clause \n23 overrides all. Use the OMP_DYNAMIC or the omp_set_dynamic() function to specify that the \n24 OpenMP implementation dynamically adjust the number of threads for parallel regions. The \n25 default setting for dynamic adjustment is implementation defined. When dynamic adjustment is on \n26 and the number of threads is specified, the number of threads becomes an upper limit for the \n27 number of threads to be provided by the OpenMP runtime. \n28 WORKSHARING CONSTRUCTS \n29 A worksharing construct distributes the execution of the associated region among the members of \n30 the team that encounter it. There is an implied barrier at the end of the worksharing region (there is \n31 no barrier at the beginning). \n13 \n1 The worksharing constructs are: \n2 \u2022 loop constructs: for and do \n3 \u2022 sections \n4 \u2022 single \n5 \u2022 workshare \n6 The for and do constructs (loop constructs) create a region consisting of a loop. A loop controlled \n7 by a loop construct is called an associated loop. Nested loops can form a single region when the \n8 collapse clause (with an integer argument) designates the number of associated loops to be \n9 executed in parallel, by forming a \u201csingle iteration space\u201d for the specified number of nested loops. \n10 The ordered clause can also control multiple associated loops. \n11 An associated loop must adhere to a \u201ccanonical form\u201d (specified in the Canonical Loop Form of the \n12 OpenMP Specifications document) which allows the iteration count (of all associated loops) to be \n13 computed before the (outermost) loop is executed. Most common loops comply with the canonical \n14 form, including C++ iterators. \n15 A single construct forms a region in which only one thread (any one of the team) executes the \n16 region. The other threads wait at the implied barrier at the end, unless the nowait clause is \n17 specified. \n18 The sections construct forms a region that contains one or more structured blocks. Each block \n19 of a sections directive is constructed with a section construct, and executed once by one of \n20 the threads (any one) in the team. (If only one block is formed in the region, the section \n21 construct, which is used to separate blocks, is not required.) The other threads wait at the implied \n22 barrier at the end, unless the nowait clause is specified. \n23 The workshare construct is a Fortran feature that consists of a region with a single structure \n24 block (section of code). Statements in the workshare region are divided into units of work, and \n25 executed (once) by threads of the team. \n26 MASKED CONSTRUCT \n27 The masked construct is not a worksharing construct. The masked region is executed only by the \n28 primary thread. There is no implicit barrier (and flush) at the end of the masked region; hence the \n29 other threads of the team continue execution beyond code statements beyond the masked region. \n30 The master construct, which has been deprecated in OpenMP 5.1, has identical semantics to the \n31 masked construct with no filter clause. \n14 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.1 A Simple Parallel Loop", "chunk": "2 The following example demonstrates how to parallelize a simple loop using the parallel \n3 worksharing-loop construct. The loop iteration variable is private by default, so it is not necessary \n4 to specify it explicitly in a private clause. \nC / C++ \n5 Example ploop.1.c (pre_omp_3.0) \nS-1 void simple(int n, float *a, float *b) \nS-2 { \nS-3 int i; \nS-4 \nS-5 #pragma omp parallel for \nS-6 for (i=1; i<n; i++) /* i is private by default */ \nS-7 b[i] = (a[i] + a[i-1]) / 2.0; \nS-8 } \nC / C++ \nFortran \n6 Example ploop.1.f (pre_omp_3.0) \nS-1 SUBROUTINE SIMPLE(N, A, B) \nS-2 \nS-3 INTEGER I, N \nS-4 REAL B(N), A(N) \nS-5 \nS-6 !$OMP PARALLEL DO !I is private by default \nS-7 DO I=2,N \nS-8 B(I) = (A(I) + A(I-1)) / 2.0 \nS-9 ENDDO \nS-10 !$OMP END PARALLEL DO \nS-11 \nS-12 END SUBROUTINE SIMPLE \nFortran \nCHAPTER 3. PARALLEL EXECUTION 15 \n"}
{"section_title": "3.2 parallel Construct", "chunk": ""}
{"section_title": "3.2 parallel Construct", "chunk": "2 The parallel construct can be used in coarse-grain parallel programs. In the following example, \n3 each thread in the parallel region decides what part of the global array x to work on, based on \n4 the thread number: \nC / C++ \n5 Example parallel.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 \nS-3 void subdomain(float *x, int istart, int ipoints) \nS-4 { \nS-5 int i; \nS-6 \nS-7 for (i = 0; i < ipoints; i++) \nS-8 x[istart+i] = 123.456; \nS-9 } \nS-10 \nS-11 void sub(float *x, int npoints) \nS-12 { \nS-13 int iam, nt, ipoints, istart; \nS-14 \nS-15 #pragma omp parallel default(shared) private(iam,nt,ipoints,istart) \nS-16 { \nS-17 iam = omp_get_thread_num(); \nS-18 nt = omp_get_num_threads(); \nS-19 ipoints = npoints / nt; /* size of partition */ \nS-20 istart = iam * ipoints; /* starting array index */ \nS-21 if (iam == nt-1) /* last thread may do more */ \nS-22 ipoints = npoints - istart; \nS-23 subdomain(x, istart, ipoints); \nS-24 } \nS-25 } \nS-26 \nS-27 int main() \nS-28 { \nS-29 float array[10000]; \nS-30 \nS-31 sub(array, 10000); \nS-32 \nS-33 return 0; \nS-34 } \nC / C++ \n16 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example parallel.1.f (pre_omp_3.0) \nS-1 SUBROUTINE SUBDOMAIN(X, ISTART, IPOINTS) \nS-2 INTEGER ISTART, IPOINTS \nS-3 REAL X(*) \nS-4 \nS-5 INTEGER I \nS-6 \nS-7 DO 100 I=1,IPOINTS \nS-8 X(ISTART+I) = 123.456 \nS-9 100 CONTINUE \nS-10 \nS-11 END SUBROUTINE SUBDOMAIN \nS-12 \nS-13 SUBROUTINE SUB(X, NPOINTS) \nS-14 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-15 \nS-16 REAL X(*) \nS-17 INTEGER NPOINTS \nS-18 INTEGER IAM, NT, IPOINTS, ISTART \nS-19 \nS-20 !$OMP PARALLEL DEFAULT(PRIVATE) SHARED(X,NPOINTS) \nS-21 \nS-22 IAM = OMP_GET_THREAD_NUM() \nS-23 NT = OMP_GET_NUM_THREADS() \nS-24 IPOINTS = NPOINTS/NT \nS-25 ISTART = IAM * IPOINTS \nS-26 IF (IAM .EQ. NT-1) THEN \nS-27 IPOINTS = NPOINTS - ISTART \nS-28 ENDIF \nS-29 CALL SUBDOMAIN(X,ISTART,IPOINTS) \nS-30 \nS-31 !$OMP END PARALLEL \nS-32 END SUBROUTINE SUB \nS-33 \nS-34 PROGRAM PAREXAMPLE \nS-35 REAL ARRAY(10000) \nS-36 CALL SUB(ARRAY, 10000) \nS-37 END PROGRAM PAREXAMPLE \nFortran \nCHAPTER 3. PARALLEL EXECUTION 17 \n"}
{"section_title": "3.3 teams Construct on Host", "chunk": ""}
{"section_title": "3.3 teams Construct on Host", "chunk": "2 Originally the teams construct was created for devices (such as GPUs) for independent executions \n3 of a structured block by teams within a league (on SMs). It was only available through offloading \n4 with the target construct, and the execution of a teams region could only be directed to host \n5 execution by various means such as if and device clauses, and the OMP_TARGET_OFFLOAD \n6 environment variable. \n7 In OpenMP 5.0 the teams construct was extended to enable the host to execute a teams region \n8 (without an associated target construct), with anticipation of further affinity and threading \n9 controls in future OpenMP releases. \n10 In the example below the teams construct is used to create two teams, one to execute single \n11 precision code, and the other to execute double precision code. Two teams are required, and the \n12 thread limit for each team is set to 1/2 of the number of available processors. \nC / C++ \n13 Example host_teams.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <math.h> \nS-4 #include <omp.h> \nS-5 #define N 1000 \nS-6 \nS-7 int main(){ \nS-8 int nteams_required=2, max_thrds, tm_id; \nS-9 float sp_x[N], sp_y[N], sp_a=0.0001e0; \nS-10 double dp_x[N], dp_y[N], dp_a=0.0001e0; \nS-11 \nS-12 max_thrds = omp_get_num_procs()/nteams_required; \nS-13 \nS-14 // Create 2 teams, each team works in a different precision \nS-15 #pragma omp teams num_teams(nteams_required) \\ \nS-16 thread_limit(max_thrds) private(tm_id) \nS-17 { \nS-18 tm_id = omp_get_team_num(); \nS-19 \nS-20 if( omp_get_num_teams() != 2 ) //if only getting 1, quit \nS-21 { printf(\"error: Insufficient teams on host, 2 required\\n\"); \nS-22 exit(0); \nS-23 } \nS-24 \nS-25 if(tm_id == 0) // Do Single Precision Work (SAXPY) with this team \nS-26 { \nS-27 #pragma omp parallel \nS-28 { \n18 OpenMP Examples Version 5.2.1 - November 2022 \nS-29 #pragma omp for //init \nS-30 for(int i=0; i<N; i++){sp_x[i] = i*0.0001; sp_y[i]=i; } \nS-31 \nS-32 #pragma omp for simd simdlen(8) \nS-33 for(int i=0; i<N; i++){sp_x[i] = sp_a*sp_x[i] + sp_y[i];} \nS-34 } \nS-35 } \nS-36 \nS-37 if(tm_id == 1) // Do Double Precision Work (DAXPY) with this team \nS-38 { \nS-39 #pragma omp parallel \nS-40 { \nS-41 #pragma omp for //init \nS-42 for(int i=0; i<N; i++){dp_x[i] = i*0.0001; dp_y[i]=i; } \nS-43 \nS-44 #pragma omp for simd simdlen(4) \nS-45 for(int i=0; i<N; i++){dp_x[i] = dp_a*dp_x[i] + dp_y[i];} \nS-46 } \nS-47 } \nS-48 } \nS-49 \nS-50 printf(\"i=%d sp|dp %f %f"}
{"section_title": "3.3 teams Construct on Host", "chunk": "2 Originally the teams construct was created for devices (such as GPUs) for independent executions \n3 of a structured block by teams within a league (on SMs). It was only available through offloading \n4 with the target construct, and the execution of a teams region could only be directed to host \n5 execution by various means such as if and device clauses, and the OMP_TARGET_OFFLOAD \n6 environment variable. \n7 In OpenMP 5.0 the teams construct was extended to enable the host to execute a teams region \n8 (without an associated target construct), with anticipation of further affinity and threading \n9 controls in future OpenMP releases. \n10 In the example below the teams construct is used to create two teams, one to execute single \n11 precision code, and the other to execute double precision code. Two teams are required, and the \n12 thread limit for each team is set to 1/2 of the number of available processors. \nC / C++ \n13 Example host_teams.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <math.h> \nS-4 #include <omp.h> \nS-5 #define N 1000 \nS-6 \nS-7 int main(){ \nS-8 int nteams_required=2, max_thrds, tm_id; \nS-9 float sp_x[N], sp_y[N], sp_a=0.0001e0; \nS-10 double dp_x[N], dp_y[N], dp_a=0.0001e0; \nS-11 \nS-12 max_thrds = omp_get_num_procs()/nteams_required; \nS-13 \nS-14 // Create 2 teams, each team works in a different precision \nS-15 #pragma omp teams num_teams(nteams_required) \\ \nS-16 thread_limit(max_thrds) private(tm_id) \nS-17 { \nS-18 tm_id = omp_get_team_num(); \nS-19 \nS-20 if( omp_get_num_teams() != 2 ) //if only getting 1, quit \nS-21 { printf(\"error: Insufficient teams on host, 2 required\\n\"); \nS-22 exit(0); \nS-23 } \nS-24 \nS-25 if(tm_id == 0) // Do Single Precision Work (SAXPY) with this team \nS-26 { \nS-27 #pragma omp parallel \nS-28 { \n18 OpenMP Examples Version 5.2.1 - November 2022 \nS-29 #pragma omp for //init \nS-30 for(int i=0; i<N; i++){sp_x[i] = i*0.0001; sp_y[i]=i; } \nS-31 \nS-32 #pragma omp for simd simdlen(8) \nS-33 for(int i=0; i<N; i++){sp_x[i] = sp_a*sp_x[i] + sp_y[i];} \nS-34 } \nS-35 } \nS-36 \nS-37 if(tm_id == 1) // Do Double Precision Work (DAXPY) with this team \nS-38 { \nS-39 #pragma omp parallel \nS-40 { \nS-41 #pragma omp for //init \nS-42 for(int i=0; i<N; i++){dp_x[i] = i*0.0001; dp_y[i]=i; } \nS-43 \nS-44 #pragma omp for simd simdlen(4) \nS-45 for(int i=0; i<N; i++){dp_x[i] = dp_a*dp_x[i] + dp_y[i];} \nS-46 } \nS-47 } \nS-48 } \nS-49 \nS-50 printf(\"i=%d sp|dp %f %f\",N-1, sp_x[N-1], dp_x[N-1]); \nS-51 printf(\"i=%d sp|dp %f %f"}
{"section_title": "3.3 teams Construct on Host", "chunk": "\",N-1, sp_x[N-1], dp_x[N-1]); \nS-51 printf(\"i=%d sp|dp %f %f\",N/2, sp_x[N/2], dp_x[N/2]); \nS-52 //OUTPUT1:i=999 sp|dp 999.000000 999.000010 \nS-53 //OUTPUT2:i=500 sp|dp 500.000000 500.000005 \nS-54 \nS-55 return 0; \nS-56 } \nC / C++ \nFortran \n1 Example host_teams.1.f90 (omp_5.0) \nS-1 program main \nS-2 use omp_lib \nS-3 integer :: nteams_required=2, max_thrds, tm_id \nS-4 integer,parameter :: N=1000 \nS-5 real :: sp_x(N), sp_y(N), sp_a=0.0001e0 \nS-6 double precision :: dp_x(N), dp_y(N), dp_a=0.0001d0 \nS-7 \nS-8 max_thrds = omp_get_num_procs()/nteams_required \nS-9 \nS-10 !! Create 2 teams, each team works in a different precision \nS-11 !$omp teams num_teams(nteams_required) thread_limit(max_thrds) & \nS-12 !$omp& private(tm_id) \nS-13 \nS-14 tm_id = omp_get_team_num() \nCHAPTER 3. PARALLEL EXECUTION 19 \nS-15 \nS-16 if( omp_get_num_teams() /= 2 ) then !! if only getting 1, quit \nS-17 stop \"error: Insufficient teams on host, 2 required.\" \nS-18 endif \nS-19 \nS-20 !! Do Single Precision Work (SAXPY) with this team \nS-21 if(tm_id == 0) then \nS-22 \nS-23 !$omp parallel \nS-24 !$omp do !! init \nS-25 do i = 1,N \nS-26 sp_x(i) = i*0.0001e0 \nS-27 sp_y(i) = i \nS-28 end do \nS-29 \nS-30 !$omp do simd simdlen(8) \nS-31 do i = 1,N \nS-32 sp_x(i) = sp_a*sp_x(i) + sp_y(i) \nS-33 end do \nS-34 !$omp end parallel \nS-35 \nS-36 endif \nS-37 \nS-38 !! Do Double Precision Work (DAXPY) with this team \nS-39 if(tm_id == 1) then \nS-40 \nS-41 !$omp parallel \nS-42 !$omp do !! init \nS-43 do i = 1,N \nS-44 dp_x(i) = i*0.0001d0 \nS-45 dp_y(i) = i \nS-46 end do \nS-47 \nS-48 !$omp do simd simdlen(4) \nS-49 do i = 1,N \nS-50 dp_x(i) = dp_a*dp_x(i) + dp_y(i) \nS-51 end do \nS-52 !$omp end parallel \nS-53 \nS-54 endif \nS-55 !$omp end teams \nS-56 \nS-57 write(*,\u2019( \"i=\",i4,\" sp|dp= \", e15.7, d25.16 )\u2019) & \nS-58 N, sp_x(N), dp_x(N) \nS-59 write(*,\u2019( \"i=\",i4,\" sp|dp= \", e15.7, d25.16 )\u2019) & \nS-60 N/2, sp_x(N/2), dp_x(N/2) \nS-61 !! i=1000 sp|dp= 0.1000000E+04 0.1000000010000000D+04 \n20 OpenMP Examples Version 5.2.1 - November 2022 \nS-62 !! i= 500 sp|dp= 0.5000000E+03 0.5000000050000000D+03 \nS-63 end program \nFortran \nCHAPTER 3. PARALLEL EXECUTION 21 \n"}
{"section_title": "3.4 Controlling the Number of Threads on Multiple Nesting Levels", "chunk": ""}
{"section_title": "3.4 Controlling the Number of Threads on Multiple Nesting Levels", "chunk": "2 on Multiple Nesting Levels \n3 The following examples demonstrate how to use the OMP_NUM_THREADS environment variable to \n4 control the number of threads on multiple nesting levels: \nC / C++ \n5 Example nthrs_nesting.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 int main (void) \nS-4 { \nS-5 omp_set_nested(1); \nS-6 omp_set_dynamic(0); \nS-7 #pragma omp parallel \nS-8 { \nS-9 #pragma omp parallel \nS-10 { \nS-11 #pragma omp single \nS-12 { \nS-13 /* \nS-14 * If OMP_NUM_THREADS=2,3 was set, the following should print: \nS-15 * Inner: num_thds=3 \nS-16 * Inner: num_thds=3 \nS-17 * \nS-18 * If nesting is not supported, the following should print: \nS-19 * Inner: num_thds=1 \nS-20 * Inner: num_thds=1 \nS-21 */ \nS-22 printf (\"Inner: num_thds=%d\\n\", omp_get_num_threads()); \nS-23 } \nS-24 } \nS-25 #pragma omp barrier \nS-26 omp_set_nested(0); \nS-27 #pragma omp parallel \nS-28 { \nS-29 #pragma omp single \nS-30 { \nS-31 /* \nS-32 * Even if OMP_NUM_THREADS=2,3 was set, the following should \nS-33 * print, because nesting is disabled: \nS-34 * Inner: num_thds=1 \nS-35 * Inner: num_thds=1 \nS-36 */ \nS-37 printf (\"Inner: num_thds=%d\\n\", omp_get_num_threads()); \nS-38 } \n22 OpenMP Examples Version 5.2.1 - November 2022 \nS-39 } \nS-40 #pragma omp barrier \nS-41 #pragma omp single \nS-42 { \nS-43 /* \nS-44 * If OMP_NUM_THREADS=2,3 was set, the following should print: \nS-45 * Outer: num_thds=2 \nS-46 */ \nS-47 printf (\"Outer: num_thds=%d\\n\", omp_get_num_threads()); \nS-48 } \nS-49 } \nS-50 return 0; \nS-51 } \nC / C++ \nFortran \n1 Example nthrs_nesting.1.f (pre_omp_3.0) \nS-1 program icv \nS-2 use omp_lib \nS-3 call omp_set_nested(.true.) \nS-4 call omp_set_dynamic(.false.) \nS-5 !$omp parallel \nS-6 !$omp parallel \nS-7 !$omp single \nS-8 ! If OMP_NUM_THREADS=2,3 was set, the following should print: \nS-9 ! Inner: num_thds= 3 \nS-10 ! Inner: num_thds= 3 \nS-11 ! If nesting is not supported, the following should print: \nS-12 ! Inner: num_thds= 1 \nS-13 ! Inner: num_thds= 1 \nS-14 print *, \"Inner: num_thds=\", omp_get_num_threads() \nS-15 !$omp end single \nS-16 !$omp end parallel \nS-17 !$omp barrier \nS-18 call omp_set_nested(.false.) \nS-19 !$omp parallel \nS-20 !$omp single \nS-21 ! Even if OMP_NUM_THREADS=2,3 was set, the following should print, \nS-22 ! because nesting is disabled: \nS-23 ! Inner: num_thds= 1 \nS-24 ! Inner: num_thds= 1 \nS-25 print *, \"Inner: num_thds=\", omp_get_num_threads() \nS-26 !$omp end single \nS-27 !$omp end parallel \nS-28 !$omp barrier \nS-29 !$omp single \nCHAPTER 3. PARALLEL EXECUTION 23 \nS-30 ! If OMP_NUM_THREADS=2,3 was set, the following should print: \nS-31 ! Outer: num_thds= 2 \nS-32 print *, \"Outer: num_thds=\", omp_get_num_threads() \nS-33 !$omp end single \nS-34 !$omp end parallel \nS-35 end \nFortran \n24 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.5 Interaction Between the num_threads Clause and omp_set_dynamic", "chunk": ""}
{"section_title": "3.5 Interaction Between the num_threads Clause and omp_set_dynamic", "chunk": "2 Clause and omp_set_dynamic \n3 The following example demonstrates the num_threads clause and the effect of the \n4 omp_set_dynamic routine on it. \n5 The call to the omp_set_dynamic routine with argument 0 in C/C++, or .FALSE. in Fortran, \n6 disables the dynamic adjustment of the number of threads in OpenMP implementations that support \n7 it. In this case, 10 threads are provided. Note that in case of an error the OpenMP implementation \n8 is free to abort the program or to supply any number of threads available. \nC / C++ \n9 Example nthrs_dynamic.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 int main() \nS-3 { \nS-4 omp_set_dynamic(0); \nS-5 #pragma omp parallel num_threads(10) \nS-6 { \nS-7 /* do work here */ \nS-8 } \nS-9 return 0; \nS-10 } \nC / C++ \nFortran \n10 Example nthrs_dynamic.1.f (pre_omp_3.0) \nS-1 PROGRAM EXAMPLE \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 CALL OMP_SET_DYNAMIC(.FALSE.) \nS-4 !$OMP PARALLEL NUM_THREADS(10) \nS-5 ! do work here \nS-6 !$OMP END PARALLEL \nS-7 END PROGRAM EXAMPLE \nFortran \nCHAPTER 3. PARALLEL EXECUTION 25 \n1 The call to the omp_set_dynamic routine with a non-zero argument in C/C++, or .TRUE. in \n2 Fortran, allows the OpenMP implementation to choose any number of threads between 1 and 10. \nC / C++ \n3 Example nthrs_dynamic.2.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 int main() \nS-3 { \nS-4 omp_set_dynamic(1); \nS-5 #pragma omp parallel num_threads(10) \nS-6 { \nS-7 /* do work here */ \nS-8 } \nS-9 return 0; \nS-10 } \nC / C++ \nFortran \n4 Example nthrs_dynamic.2.f (pre_omp_3.0) \nS-1 PROGRAM EXAMPLE \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 CALL OMP_SET_DYNAMIC(.TRUE.) \nS-4 !$OMP PARALLEL NUM_THREADS(10) \nS-5 ! do work here \nS-6 !$OMP END PARALLEL \nS-7 END PROGRAM EXAMPLE \nFortran \n5 It is good practice to set the dyn-var ICV explicitly by calling the omp_set_dynamic routine, as \n6 its default setting is implementation defined. \n26 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.6 Fortran Restrictions on the do Construct", "chunk": ""}
{"section_title": "3.6 Fortran Restrictions on the do Construct", "chunk": "Fortran \n2 If an end do directive follows a do-construct in which several DO statements share a DO \n3 termination statement, then a do directive can only be specified for the outermost of these DO \n4 statements. The following example contains correct usages of loop constructs: \n5 Example fort_do.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I, J) \nS-2 INTEGER I,J \nS-3 END SUBROUTINE WORK \nS-4 \nS-5 SUBROUTINE DO_GOOD() \nS-6 INTEGER I, J \nS-7 REAL A(1000) \nS-8 \nS-9 DO 100 I = 1,10 \nS-10 !$OMP DO \nS-11 DO 100 J = 1,10 \nS-12 CALL WORK(I,J) \nS-13 100 CONTINUE ! !$OMP ENDDO implied here \nS-14 \nS-15 !$OMP DO \nS-16 DO 200 J = 1,10 \nS-17 200 A(I) = I + 1 \nS-18 !$OMP ENDDO \nS-19 \nS-20 !$OMP DO \nS-21 DO 300 I = 1,10 \nS-22 DO 300 J = 1,10 \nS-23 CALL WORK(I,J) \nS-24 300 CONTINUE \nS-25 !$OMP ENDDO \nS-26 END SUBROUTINE DO_GOOD \n6 The following example is non-conforming because the matching do directive for the end do does \n7 not precede the outermost loop: \n8 Example fort_do.2.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I, J) \nS-2 INTEGER I,J \nS-3 END SUBROUTINE WORK \nS-4 \nS-5 SUBROUTINE DO_WRONG \nS-6 INTEGER I, J \nS-7 \nCHAPTER 3. PARALLEL EXECUTION 27 \nS-8 DO 100 I = 1,10 \nS-9 !$OMP DO \nS-10 DO 100 J = 1,10 \nS-11 CALL WORK(I,J) \nS-12 100 CONTINUE \nS-13 !$OMP ENDDO \nS-14 END SUBROUTINE DO_WRONG \nFortran \n28 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.7 nowait Clause", "chunk": ""}
{"section_title": "3.7 nowait Clause", "chunk": "2 If there are multiple independent loops within a parallel region, you can use the nowait \n3 clause to avoid the implied barrier at the end of the loop construct, as follows: \nC / C++ \n4 Example nowait.1.c (pre_omp_3.0) \nS-1 #include <math.h> \nS-2 \nS-3 void nowait_example(int n, int m, float *a, float *b, float *y, float *z) \nS-4 { \nS-5 int i; \nS-6 #pragma omp parallel \nS-7 { \nS-8 #pragma omp for nowait \nS-9 for (i=1; i<n; i++) \nS-10 b[i] = (a[i] + a[i-1]) / 2.0; \nS-11 \nS-12 #pragma omp for nowait \nS-13 for (i=0; i<m; i++) \nS-14 y[i] = sqrt(z[i]); \nS-15 } \nS-16 } \nC / C++ \nFortran \n5 Example nowait.1.f (pre_omp_3.0) \nS-1 SUBROUTINE NOWAIT_EXAMPLE(N, M, A, B, Y, Z) \nS-2 \nS-3 INTEGER N, M \nS-4 REAL A(*), B(*), Y(*), Z(*) \nS-5 \nS-6 INTEGER I \nS-7 \nS-8 !$OMP PARALLEL \nS-9 \nS-10 !$OMP DO \nS-11 DO I=2,N \nS-12 B(I) = (A(I) + A(I-1)) / 2.0 \nS-13 ENDDO \nS-14 !$OMP END DO NOWAIT \nS-15 \nS-16 !$OMP DO \nS-17 DO I=1,M \nS-18 Y(I) = SQRT(Z(I)) \nCHAPTER 3. PARALLEL EXECUTION 29 \nS-19 ENDDO \nS-20 !$OMP END DO NOWAIT \nS-21 \nS-22 !$OMP END PARALLEL \nS-23 \nS-24 END SUBROUTINE NOWAIT_EXAMPLE \nFortran \n1 In the following example, static scheduling distributes the same logical iteration numbers to the \n2 threads that execute the three loop regions. This allows the nowait clause to be used, even though \n3 there is a data dependence between the loops. The dependence is satisfied as long the same thread \n4 executes the same logical iteration numbers in each loop. \n5 Note that the iteration count of the loops must be the same. The example satisfies this requirement, \n6 since the iteration space of the first two loops is from 0 to n-1 (from 1 to N in the Fortran version), \n7 while the iteration space of the last loop is from 1 to n (2 to N+1 in the Fortran version). \nC / C++ \n8 Example nowait.2.c (pre_omp_3.0) \nS-1 #include <math.h> \nS-2 void nowait_example2(int n, float *a, float *b, float *c, float *y, float \nS-3 *z) \nS-4 { \nS-5 int i; \nS-6 #pragma omp parallel \nS-7 { \nS-8 #pragma omp for schedule(static) nowait \nS-9 for (i=0; i<n; i++) \nS-10 c[i] = (a[i] + b[i]) / 2.0f; \nS-11 #pragma omp for schedule(static) nowait \nS-12 for (i=0; i<n; i++) \nS-13 z[i] = sqrtf(c[i]); \nS-14 #pragma omp for schedule(static) nowait \nS-15 for (i=1; i<=n; i++) \nS-16 y[i] = z[i-1] + a[i]; \nS-17 } \nS-18 } \nC / C++ \n30 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example nowait.2.f90 (pre_omp_3.0) \nS-1 SUBROUTINE NOWAIT_EXAMPLE2(N, A, B, C, Y, Z) \nS-2 INTEGER N \nS-3 REAL A(*), B(*), C(*), Y(*), Z(*) \nS-4 INTEGER I \nS-5 !$OMP PARALLEL \nS-6 !$OMP DO SCHEDULE(STATIC) \nS-7 DO I=1,N \nS-8 C(I) = (A(I) + B(I)) / 2.0 \nS-9 ENDDO \nS-10 !$OMP END DO NOWAIT \nS-11 !$OMP DO SCHEDULE(STATIC) \nS-12 DO I=1,N \nS-13 Z(I) = SQRT(C(I)) \nS-14 ENDDO \nS-15 !$OMP END DO NOWAIT \nS-16 !$OMP DO SCHEDULE(STATIC) \nS-17 DO I=2,N+1 \nS-18 Y(I) = Z(I-1) + A(I) \nS-19 ENDDO \nS-20 !$OMP END DO NOWAIT \nS-21 !$OMP END PARALLEL \nS-22 END SUBROUTINE NOWAIT_EXAMPLE2 \nFortran \nCHAPTER 3. PARALLEL EXECUTION 31 \n"}
{"section_title": "3.8 collapse Clause", "chunk": ""}
{"section_title": "3.8 collapse Clause", "chunk": "2 In the following example, the k and j loops are associated with the loop construct. So the iterations \n3 of the k and j loops are collapsed into one loop with a larger iteration space, and that loop is then \n4 divided among the threads in the current team. Since the i loop is not associated with the loop \n5 construct, it is not collapsed, and the i loop is executed sequentially in its entirety in every iteration \n6 of the collapsed k and j loop. \n7 The variable j can be omitted from the private clause when the collapse clause is used since \n8 it is implicitly private. However, if the collapse clause is omitted then j will be shared if it is \n9 omitted from the private clause. In either case, k is implicitly private and could be omitted from \n10 the private clause. \nC / C++ \n11 Example collapse.1.c (omp_3.0) \nS-1 void bar(float *a, int i, int j, int k); \nS-2 \nS-3 int kl, ku, ks, jl, ju, js, il, iu,is; \nS-4 \nS-5 void sub(float *a) \nS-6 { \nS-7 int i, j, k; \nS-8 \nS-9 #pragma omp for collapse(2) private(i, k, j) \nS-10 for (k=kl; k<=ku; k+=ks) \nS-11 for (j=jl; j<=ju; j+=js) \nS-12 for (i=il; i<=iu; i+=is) \nS-13 bar(a,i,j,k); \nS-14 } \nC / C++ \nFortran \n12 Example collapse.1.f (omp_3.0) \nS-1 subroutine sub(a) \nS-2 \nS-3 real a(*) \nS-4 integer kl, ku, ks, jl, ju, js, il, iu, is \nS-5 common /csub/ kl, ku, ks, jl, ju, js, il, iu, is \nS-6 integer i, j, k \nS-7 \nS-8 !$omp do collapse(2) private(i,j,k) \nS-9 do k = kl, ku, ks \nS-10 do j = jl, ju, js \nS-11 do i = il, iu, is \nS-12 call bar(a,i,j,k) \n32 OpenMP Examples Version 5.2.1 - November 2022 \nS-13 enddo \nS-14 enddo \nS-15 enddo \nS-16 !$omp end do \nS-17 \nS-18 end subroutine \nFortran \n1 In the next example, the k and j loops are associated with the loop construct. So the iterations of \n2 the k and j loops are collapsed into one loop with a larger iteration space, and that loop is then \n3 divided among the threads in the current team. \n4 The sequential execution of the iterations in the k and j loops determines the order of the iterations \n5 in the collapsed iteration space. This implies that in the sequentially last iteration of the collapsed \n6 iteration space, k will have the value 2 and j will have the value 3. Since klast and jlast are \n7 lastprivate, their values are assigned by the sequentially last iteration of the collapsed k and j \n8 loop. This example prints: 2 3. \nC / C++ \n9 Example collapse.2.c (omp_3.0) \nS-1 #include <stdio.h> \nS-2 int main() \nS-3 { \nS-4 int j, k, jlast, klast; \nS-5 #pragma omp parallel \nS-6 { \nS-7 #pragma omp for collapse(2) lastprivate(jlast, klast) \nS-8 for (k=1; k<=2; k++) \nS-9 for (j=1; j<=3; j++) \nS-10 { \nS-11 jlast=j; \nS-12 klast=k; \nS-13 } \nS-14 #pragma omp single \nS-15 printf(\"%d %d\\n\", klast, jlast); //2 3 \nS-16 } \nS-17 } \nC / C++ \nCHAPTER 3. PARALLEL EXECUTION 33 \nFortran \n1 Example collapse.2.f (omp_3.0) \nS-1 program test \nS-2 !$omp parallel \nS-3 !$omp do private(j,k) collapse(2) lastprivate(jlast, klast) \nS-4 do k = 1,2 \nS-5 do j = 1,3 \nS-6 jlast=j \nS-7 klast=k \nS-8 enddo \nS-9 enddo \nS-10 !$omp end do \nS-11 !$omp single \nS-12 print *, klast, jlast !2, 3 \nS-13 !$omp end single \nS-14 !$omp end parallel \nS-15 end program test \nFortran \n2 The next example illustrates the interaction of the collapse and ordered clauses. \n3 In the example, the loop construct has both a collapse clause and an ordered clause. The \n4 collapse clause causes the iterations of the k and j loops to be collapsed into one loop with a \n5 larger iteration space, and that loop is divided among the threads in the current team. An ordered \n6 clause is added to the loop construct because an ordered region binds to the loop region arising \n7 from the loop construct. \n8 According to Section 2.12.8 of the OpenMP 4.0 specification, a thread must not execute more than \n9 one ordered region that binds to the same loop region. So the collapse clause is required for the \n10 example to be conforming. With the collapse clause, the iterations of the k and j loops are \n11 collapsed into one loop, and therefore only one ordered region will bind to the collapsed k and j \n12 loop. Without the collapse clause, there would be two ordered regions that bind to each \n13 iteration of the k loop (one arising from the first iteration of the j loop, and the other arising from \n14 the second iteration of the j loop). \n15 The code prints \n16 0 1 1 \n17 0 1 2 \n18 0 2 1 \n19 1 2 2 \n20 1 3 1 \n21 1 3 2 \n34 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example collapse.3.c (omp_3.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 void work(int a, int j, int k); \nS-4 void sub() \nS-5 { \nS-6 int j, k, a = 5; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 #pragma omp for collapse(2) ordered private(j,k) schedule(static,3) \nS-10 for (k=1; k<=3; k++) \nS-11 for (j=1; j<=2; j++) \nS-12 { \nS-13 #pragma omp ordered \nS-14 printf(\"%d %d %d\\n\", omp_get_thread_num(), k, j); \nS-15 /* end ordered */ \nS-16 work(a,j,k); \nS-17 } \nS-18 } \nS-19 } \nC / C++ \nFortran \n2 Example collapse.3.f (omp_3.0) \nS-1 program test \nS-2 include \u2019omp_lib.h\u2019 \nS-3 !$omp parallel num_threads(2) \nS-4 !$omp do collapse(2) ordered private(j,k) schedule(static,3) \nS-5 do k = 1,3 \nS-6 do j = 1,2 \nS-7 !$omp ordered \nS-8 print *, omp_get_thread_num(), k, j \nS-9 !$omp end ordered \nS-10 call work(a,j,k) \nS-11 enddo \nS-12 enddo \nS-13 !$omp end do \nS-14 !$omp end parallel \nS-15 end program test \nFortran \nCHAPTER 3. PARALLEL EXECUTION 35 \n1 The following example illustrates the collapse of a non-rectangular loop nest, a new feature in \n2 OpenMP 5.0. In a loop nest, a non-rectangular loop has a loop bound that references the iteration \n3 variable of an enclosing loop. \n4 The motivation for this feature is illustrated in the example below that creates a symmetric \n5 correlation matrix for a set of variables. Note that the initial value of the second loop depends on \n6 the index variable of the first loop for the loops to be collapsed. Here the data are represented by a \n7 2D array, each row corresponds to a variable and each column corresponds to a sample of the \n8 variable \u2013 the last two columns are the sample mean and standard deviation (for Fortran, rows and \n9 columns are swapped). \nC / C++ \n10 Example collapse.4.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #define N 20 \nS-3 #define M 10 \nS-4 \nS-5 // routine to calculate a \nS-6 // For variable a[i]: \nS-7 // a[i][0],...,a[i][n-1] contains the n samples \nS-8 // a[i][n] contains the sample mean \nS-9 // a[i][n+1] contains the standard deviation \nS-10 extern void calc_a(int n,int m, float a[][N+2]); \nS-11 \nS-12 int main(){ \nS-13 float a[M][N+2], b[M][M]; \nS-14 \nS-15 calc_a(N,M,a); \nS-16 \nS-17 #pragma omp parallel for collapse(2) \nS-18 for (int i = 0; i < M; i++) \nS-19 for (int j = i; j < M; j++) \nS-20 { \nS-21 float temp = 0.0f; \nS-22 for (int k = 0; k < N; k++) \nS-23 temp += (a[i][k]-a[i][N])*(a[j][k]-a[j][N]); \nS-24 \nS-25 b[i][j] = temp / (a[i][N+1] * a[j][N+1] * (N - 1)); \nS-26 b[j][i] = b[i][j]; \nS-27 } \nS-28 \nS-29 printf(\"b[0][0] = %f, b[M-1][M-1] = %f\\n\", b[0][0], b[M-1][M-1]); \nS-30 \nS-31 return 0; \nS-32 } \nC / C++ \n36 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example collapse.4.f90 (omp_5.0) \nS-1 module calc_m \nS-2 interface \nS-3 subroutine calc_a(n, m, a) \nS-4 integer n, m \nS-5 real a(n+2,m) \nS-6 ! routine to calculate a \nS-7 ! For variable a(*,j): \nS-8 ! a(1,j),...,a(n,j) contains the n samples \nS-9 ! a(n+1,j) contains the sample mean \nS-10 ! a(n+2,j) contains the standard deviation \nS-11 end subroutine \nS-12 end interface \nS-13 end module \nS-14 \nS-15 program main \nS-16 use calc_m \nS-17 integer, parameter :: N=20, M=10 \nS-18 real a(N+2,M), b(M,M) \nS-19 real temp \nS-20 integer i, j, k \nS-21 \nS-22 call calc_a(N,M,a) \nS-23 \nS-24 !$omp parallel do collapse(2) private(k,temp) \nS-25 do i = 1, M \nS-26 do j = i, M \nS-27 temp = 0.0 \nS-28 do k = 1, N \nS-29 temp = temp + (a(k,i)-a(N+1,i))*(a(k,j)-a(N+1,j)) \nS-30 end do \nS-31 \nS-32 b(i,j) = temp / (a(N+2,i) * a(N+2,j) * (N - 1)) \nS-33 b(j,i) = b(i,j) \nS-34 end do \nS-35 end do \nS-36 \nS-37 print *,\"b(1,1) = \",b(1,1),\", b(M,M) = \",b(M,M) \nS-38 \nS-39 end program \nFortran \nCHAPTER 3. PARALLEL EXECUTION 37 \n"}
{"section_title": "3.9 linear Clause in Loop Constructs", "chunk": ""}
{"section_title": "3.9 linear Clause in Loop Constructs", "chunk": "2 The following example shows the use of the linear clause in a loop construct to allow the proper \n3 parallelization of a loop that contains an induction variable (j). At the end of the execution of the \n4 loop construct, the original variable j is updated with the value N/2 from the last iteration of the \n5 loop. \nC / C++ \n6 Example linear_in_loop.1.c (omp_4.5) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define N 100 \nS-4 int main(void) \nS-5 { \nS-6 float a[N], b[N/2]; \nS-7 int i, j; \nS-8 \nS-9 for ( i = 0; i < N; i++ ) \nS-10 a[i] = i + 1; \nS-11 \nS-12 j = 0; \nS-13 #pragma omp parallel \nS-14 #pragma omp for linear(j:1) \nS-15 for ( i = 0; i < N; i += 2 ) { \nS-16 b[j] = a[i] * 2.0f; \nS-17 j++; \nS-18 } \nS-19 \nS-20 printf( \"%d %f %f\\n\", j, b[0], b[j-1] ); \nS-21 /* print out: 50 2.0 198.0 */ \nS-22 \nS-23 return 0; \nS-24 } \nC / C++ \n38 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example linear_in_loop.1.f90 (omp_4.5) \nS-1 program linear_loop \nS-2 implicit none \nS-3 integer, parameter :: N = 100 \nS-4 real :: a(N), b(N/2) \nS-5 integer :: i, j \nS-6 \nS-7 do i = 1, N \nS-8 a(i) = i \nS-9 end do \nS-10 \nS-11 j = 0 \nS-12 !$omp parallel \nS-13 !$omp do linear(j:1) \nS-14 do i = 1, N, 2 \nS-15 j = j + 1 \nS-16 b(j) = a(i) * 2.0 \nS-17 end do \nS-18 !$omp end parallel \nS-19 \nS-20 print *, j, b(1), b(j) \nS-21 ! print out: 50 2.0 198.0 \nS-22 \nS-23 end program \nFortran \nCHAPTER 3. PARALLEL EXECUTION 39 \n"}
{"section_title": "3.10 parallel sections Construct", "chunk": "2 In the following example routines XAXIS, YAXIS, and ZAXIS can be executed concurrently. The \n3 first section directive is optional. Note that all section directives need to appear in the \n4 parallel sections construct. \nC / C++ \n5 Example psections.1.c (pre_omp_3.0) \nS-1 void XAXIS(); \nS-2 void YAXIS(); \nS-3 void ZAXIS(); \nS-4 \nS-5 void sect_example() \nS-6 { \nS-7 #pragma omp parallel sections \nS-8 { \nS-9 #pragma omp section \nS-10 XAXIS(); \nS-11 \nS-12 #pragma omp section \nS-13 YAXIS(); \nS-14 \nS-15 #pragma omp section \nS-16 ZAXIS(); \nS-17 } \nS-18 } \nC / C++ \nFortran \n6 Example psections.1.f (pre_omp_3.0) \nS-1 SUBROUTINE SECT_EXAMPLE() \nS-2 !$OMP PARALLEL SECTIONS \nS-3 !$OMP SECTION \nS-4 CALL XAXIS() \nS-5 !$OMP SECTION \nS-6 CALL YAXIS() \nS-7 \nS-8 !$OMP SECTION \nS-9 CALL ZAXIS() \nS-10 \nS-11 !$OMP END PARALLEL SECTIONS \nS-12 END SUBROUTINE SECT_EXAMPLE \nFortran \n40 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.11 firstprivate Clause and sections Construct", "chunk": ""}
{"section_title": "3.11 firstprivate Clause and sections Construct", "chunk": "2 Construct \n3 In the following example of the sections construct the firstprivate clause is used to \n4 initialize the private copy of section_count of each thread. The problem is that the section \n5 constructs modify section_count, which breaks the independence of the section constructs. \n6 When different threads execute each section, both sections will print the value 1. When the same \n7 thread executes the two sections, one section will print the value 1 and the other will print the value \n8 2. Since the order of execution of the two sections in this case is unspecified, it is unspecified which \n9 section prints which value. \nC / C++ \n10 Example fpriv_sections.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 #define NT 4 \nS-4 int main( ) { \nS-5 int section_count = 0; \nS-6 omp_set_dynamic(0); \nS-7 omp_set_num_threads(NT); \nS-8 #pragma omp parallel \nS-9 #pragma omp sections firstprivate( section_count ) \nS-10 { \nS-11 #pragma omp section \nS-12 { \nS-13 section_count++; \nS-14 /* may print the number one or two */ \nS-15 printf( \"section_count %d\\n\", section_count ); \nS-16 } \nS-17 #pragma omp section \nS-18 { \nS-19 section_count++; \nS-20 /* may print the number one or two */ \nS-21 printf( \"section_count %d\\n\", section_count ); \nS-22 } \nS-23 } \nS-24 return 0; \nS-25 } \nC / C++ \nCHAPTER 3. PARALLEL EXECUTION 41 \nFortran \n1 Example fpriv_sections.1.f90 (pre_omp_3.0) \nS-1 program section \nS-2 use omp_lib \nS-3 integer :: section_count = 0 \nS-4 integer, parameter :: NT = 4 \nS-5 call omp_set_dynamic(.false.) \nS-6 call omp_set_num_threads(NT) \nS-7 !$omp parallel \nS-8 !$omp sections firstprivate ( section_count ) \nS-9 !$omp section \nS-10 section_count = section_count + 1 \nS-11 ! may print the number one or two \nS-12 print *, \u2019section_count\u2019, section_count \nS-13 !$omp section \nS-14 section_count = section_count + 1 \nS-15 ! may print the number one or two \nS-16 print *, \u2019section_count\u2019, section_count \nS-17 !$omp end sections \nS-18 !$omp end parallel \nS-19 end program section \nFortran \n42 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.12 single Construct", "chunk": ""}
{"section_title": "3.12 single Construct", "chunk": "2 The following example demonstrates the single construct. In the example, only one thread prints \n3 each of the progress messages. All other threads will skip the single region and stop at the \n4 barrier at the end of the single construct until all threads in the team have reached the barrier. If \n5 other threads can proceed without waiting for the thread executing the single region, a nowait \n6 clause can be specified, as is done in the third single construct in this example. The user must \n7 not make any assumptions as to which thread will execute a single region. \nC / C++ \n8 Example single.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 void work1() {} \nS-4 void work2() {} \nS-5 \nS-6 int main() \nS-7 { \nS-8 #pragma omp parallel \nS-9 { \nS-10 #pragma omp single \nS-11 printf(\"Beginning work1.\\n\"); \nS-12 \nS-13 work1(); \nS-14 \nS-15 #pragma omp single \nS-16 printf(\"Finishing work1.\\n\"); \nS-17 \nS-18 #pragma omp single nowait \nS-19 printf(\"Finished work1 and beginning work2.\\n\"); \nS-20 \nS-21 work2(); \nS-22 } \nS-23 } \nC / C++ \nCHAPTER 3. PARALLEL EXECUTION 43 \nFortran \n1 Example single.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK1() \nS-2 END SUBROUTINE WORK1 \nS-3 \nS-4 SUBROUTINE WORK2() \nS-5 END SUBROUTINE WORK2 \nS-6 \nS-7 PROGRAM SINGLE_EXAMPLE \nS-8 !$OMP PARALLEL \nS-9 \nS-10 !$OMP SINGLE \nS-11 print *, \"Beginning work1.\" \nS-12 !$OMP END SINGLE \nS-13 \nS-14 CALL WORK1() \nS-15 \nS-16 !$OMP SINGLE \nS-17 print *, \"Finishing work1.\" \nS-18 !$OMP END SINGLE \nS-19 \nS-20 !$OMP SINGLE \nS-21 print *, \"Finished work1 and beginning work2.\" \nS-22 !$OMP END SINGLE NOWAIT \nS-23 \nS-24 CALL WORK2() \nS-25 \nS-26 !$OMP END PARALLEL \nS-27 \nS-28 END PROGRAM SINGLE_EXAMPLE \nFortran \n44 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.13 workshare Construct", "chunk": ""}
{"section_title": "3.13 workshare Construct", "chunk": "Fortran \n2 The following are examples of the workshare construct. \n3 In the following example, workshare spreads work across the threads executing the parallel \n4 region, and there is a barrier after the last statement. Implementations must enforce Fortran \n5 execution rules inside of the workshare block. \n6 Example workshare.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE1(AA, BB, CC, DD, EE, FF, N) \nS-2 INTEGER N \nS-3 REAL AA(N,N), BB(N,N), CC(N,N), DD(N,N), EE(N,N), FF(N,N) \nS-4 \nS-5 !$OMP PARALLEL \nS-6 !$OMP WORKSHARE \nS-7 AA = BB \nS-8 CC = DD \nS-9 EE = FF \nS-10 !$OMP END WORKSHARE \nS-11 !$OMP END PARALLEL \nS-12 \nS-13 END SUBROUTINE WSHARE1 \n7 In the following example, the barrier at the end of the first workshare region is eliminated with a \n8 nowait clause. Threads doing CC = DD immediately begin work on EE = FF when they are \n9 done with CC = DD. \n10 Example workshare.2.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE2(AA, BB, CC, DD, EE, FF, N) \nS-2 INTEGER N \nS-3 REAL AA(N,N), BB(N,N), CC(N,N) \nS-4 REAL DD(N,N), EE(N,N), FF(N,N) \nS-5 \nS-6 !$OMP PARALLEL \nS-7 !$OMP WORKSHARE \nS-8 AA = BB \nS-9 CC = DD \nS-10 !$OMP END WORKSHARE NOWAIT \nS-11 !$OMP WORKSHARE \nS-12 EE = FF \nS-13 !$OMP END WORKSHARE \nS-14 !$OMP END PARALLEL \nS-15 END SUBROUTINE WSHARE2 \nCHAPTER 3. PARALLEL EXECUTION 45 \nFortran (cont.) \n1 The following example shows the use of an atomic directive inside a workshare construct. The \n2 computation of SUM(AA) is workshared, but the update to R is atomic. \n3 Example workshare.3.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE3(AA, BB, CC, DD, N) \nS-2 INTEGER N \nS-3 REAL AA(N,N), BB(N,N), CC(N,N), DD(N,N) \nS-4 REAL R \nS-5 R=0 \nS-6 !$OMP PARALLEL \nS-7 !$OMP WORKSHARE \nS-8 AA = BB \nS-9 !$OMP ATOMIC UPDATE \nS-10 R = R + SUM(AA) \nS-11 CC = DD \nS-12 !$OMP END WORKSHARE \nS-13 !$OMP END PARALLEL \nS-14 END SUBROUTINE WSHARE3 \n4 Fortran WHERE and FORALL statements are compound statements, made up of a control part and a \n5 statement part. When workshare is applied to one of these compound statements, both the \n6 control and the statement parts are workshared. The following example shows the use of a WHERE \n7 statement in a workshare construct. \n8 Each task gets worked on in order by the threads: \n9 AA = BB then \n10 CC = DD then \n11 EE .ne. 0 then \n12 FF = 1 / EE then \n13 GG = HH \n14 Example workshare.4.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE4(AA, BB, CC, DD, EE, FF, GG, HH, N) \nS-2 INTEGER N \nS-3 REAL AA(N,N), BB(N,N), CC(N,N) \nS-4 REAL DD(N,N), EE(N,N), FF(N,N) \nS-5 REAL GG(N,N), HH(N,N) \nS-6 \nS-7 !$OMP PARALLEL \nS-8 !$OMP WORKSHARE \nS-9 AA = BB \nS-10 CC = DD \nS-11 WHERE (EE .ne. 0) FF = 1 / EE \n46 OpenMP Examples Version 5.2.1 - November 2022 \nFortran (cont.) \nS-12 GG = HH \nS-13 !$OMP END WORKSHARE \nS-14 !$OMP END PARALLEL \nS-15 \nS-16 END SUBROUTINE WSHARE4 \n1 In the following example, an assignment to a shared scalar variable is performed by one thread in a \n2 workshare while all other threads in the team wait. \n3 Example workshare.5.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE5(AA, BB, CC, DD, N) \nS-2 INTEGER N \nS-3 REAL AA(N,N), BB(N,N), CC(N,N), DD(N,N) \nS-4 \nS-5 INTEGER SHR \nS-6 \nS-7 !$OMP PARALLEL SHARED(SHR) \nS-8 !$OMP WORKSHARE \nS-9 AA = BB \nS-10 SHR = 1 \nS-11 CC = DD * SHR \nS-12 !$OMP END WORKSHARE \nS-13 !$OMP END PARALLEL \nS-14 \nS-15 END SUBROUTINE WSHARE5 \n4 The following example contains an assignment to a private scalar variable, which is performed by \n5 one thread in a workshare while all other threads wait. It is non-conforming because the private \n6 scalar variable is undefined after the assignment statement. \n7 Example workshare.6.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE6_WRONG(AA, BB, CC, DD, N) \nS-2 INTEGER N \nS-3 REAL AA(N,N), BB(N,N), CC(N,N), DD(N,N) \nS-4 \nS-5 INTEGER PRI \nS-6 \nS-7 !$OMP PARALLEL PRIVATE(PRI) \nS-8 !$OMP WORKSHARE \nS-9 AA = BB \nS-10 PRI = 1 \nS-11 CC = DD * PRI \nS-12 !$OMP END WORKSHARE \nS-13 !$OMP END PARALLEL \nCHAPTER 3. PARALLEL EXECUTION 47 \nS-14 \nS-15 END SUBROUTINE WSHARE6_WRONG \n1 Fortran execution rules must be enforced inside a workshare construct. In the following \n2 example, the same result is produced in the following program fragment regardless of whether the \n3 code is executed sequentially or inside an OpenMP program with multiple threads: \n4 Example workshare.7.f (pre_omp_3.0) \nS-1 SUBROUTINE WSHARE7(AA, BB, CC, N) \nS-2 INTEGER N \nS-3 REAL AA(N), BB(N), CC(N) \nS-4 \nS-5 !$OMP PARALLEL \nS-6 !$OMP WORKSHARE \nS-7 AA(1:50) = BB(11:60) \nS-8 CC(11:20) = AA(1:10) \nS-9 !$OMP END WORKSHARE \nS-10 !$OMP END PARALLEL \nS-11 \nS-12 END SUBROUTINE WSHARE7 \nFortran \n48 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.14 masked Construct", "chunk": ""}
{"section_title": "3.14 masked Construct", "chunk": "2 The following example demonstrates the masked construct. In the example, the primary thread \n3 (thread number 0) keeps track of how many iterations have been executed and prints out a progress \n4 report in the iteration loop. The other threads skip the masked region without waiting. The \n5 filter clause can be used to specify a thread number other than the primary thread to execute a \n6 structured block, as illustrated by the second masked construct after the iteration loop. If the \n7 thread specified in a filter clause does not exist in the team then the structured block is not \n8 executed by any thread. \nC / C++ \n9 Example masked.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 \nS-3 extern float average(float,float,float); \nS-4 \nS-5 void masked_example( float* x, float* xold, int n, float tol ) \nS-6 { \nS-7 int c, i, toobig; \nS-8 float error, y; \nS-9 c = 0; \nS-10 #pragma omp parallel \nS-11 { \nS-12 do { \nS-13 #pragma omp for private(i) \nS-14 for( i = 1; i < n-1; ++i ){ \nS-15 xold[i] = x[i]; \nS-16 } \nS-17 #pragma omp single \nS-18 { \nS-19 toobig = 0; \nS-20 } \nS-21 #pragma omp for private(i,y,error) reduction(+:toobig) \nS-22 for( i = 1; i < n-1; ++i ){ \nS-23 y = x[i]; \nS-24 x[i] = average( xold[i-1], x[i], xold[i+1] ); \nS-25 error = y - x[i]; \nS-26 if( error > tol || error < -tol ) ++toobig; \nS-27 } \nS-28 #pragma omp masked // primary thread (thread 0) \nS-29 { \nS-30 ++c; \nS-31 printf( \"iteration %d, toobig=%d\\n\", c, toobig ); \nS-32 } \nS-33 } while( toobig > 0 ); \nS-34 #pragma omp barrier \nCHAPTER 3. PARALLEL EXECUTION 49 \nS-35 #pragma omp masked filter(1) // thread 1 \nS-36 { \nS-37 // The printf statement will not be executed \nS-38 // if the number of threads is less than 2. \nS-39 printf( \"total number of iterations = %d\\n\", c ); \nS-40 } \nS-41 } \nS-42 } \nC / C++ \nFortran \n1 Example masked.1.f (omp_5.1) \nS-1 SUBROUTINE MASKED_EXAMPLE( X, XOLD, N, TOL ) \nS-2 REAL X(*), XOLD(*), TOL \nS-3 INTEGER N \nS-4 INTEGER C, I, TOOBIG \nS-5 REAL ERROR, Y, AVERAGE \nS-6 EXTERNAL AVERAGE \nS-7 C = 0 \nS-8 TOOBIG = 1 \nS-9 !$OMP PARALLEL \nS-10 DO WHILE( TOOBIG > 0 ) \nS-11 !$OMP DO PRIVATE(I) \nS-12 DO I = 2, N-1 \nS-13 XOLD(I) = X(I) \nS-14 ENDDO \nS-15 !$OMP SINGLE \nS-16 TOOBIG = 0 \nS-17 !$OMP END SINGLE \nS-18 !$OMP DO PRIVATE(I,Y,ERROR), REDUCTION(+:TOOBIG) \nS-19 DO I = 2, N-1 \nS-20 Y = X(I) \nS-21 X(I) = AVERAGE( XOLD(I-1), X(I), XOLD(I+1) ) \nS-22 ERROR = Y-X(I) \nS-23 IF( ERROR > TOL .OR. ERROR < -TOL ) TOOBIG = TOOBIG+1 \nS-24 ENDDO \nS-25 !$OMP MASKED ! primary thread (thread 0) \nS-26 C = C + 1 \nS-27 PRINT *, \u2019Iteration \u2019, C, \u2019TOOBIG=\u2019, TOOBIG \nS-28 !$OMP END MASKED \nS-29 ENDDO \nS-30 !$OMP BARRIER \nS-31 !$OMP MASKED FILTER(1) ! thread 1 \nS-32 ! The print statement will not be executed \nS-33 ! if the number of threads is less than 2. \nS-34 PRINT *, \u2019Total number of iterations =\u2019, C \n50 OpenMP Examples Version 5.2.1 - November 2022 \nS-35 !$OMP END MASKED \nS-36 !$OMP END PARALLEL \nS-37 END SUBROUTINE MASKED_EXAMPLE \nFortran \nCHAPTER 3. PARALLEL EXECUTION 51 \n"}
{"section_title": "3.15 loop Construct", "chunk": ""}
{"section_title": "3.15 loop Construct", "chunk": "2 The following example illustrates the use of the OpenMP 5.0 loop construct for the execution of a \n3 loop. The loop construct asserts to the compiler that the iterations of the loop are free of data \n4 dependencies and may be executed concurrently. It allows the compiler to use heuristics to select \n5 the parallelization scheme and compiler-level optimizations for the concurrency. \nC / C++ \n6 Example loop.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 int main() \nS-4 { \nS-5 float x[N], y[N]; \nS-6 float a = 2.0; \nS-7 for(int i=0;i<N;i++){ x[i]=i; y[i]=0;} // initialize \nS-8 \nS-9 #pragma omp parallel \nS-10 { \nS-11 #pragma omp loop \nS-12 for(int i = 0; i < N; ++i) y[i] = a*x[i] + y[i]; \nS-13 } \nS-14 if(y[N-1] != (N-1)*2.0) printf(\"Error: 2*(N-1) != y[N-1]=%f\",y[N-1]); \nS-15 } \nC / C++ \nFortran \n7 Example loop.1.f90 (omp_5.0) \nS-1 program main \nS-2 integer, parameter :: N=100 \nS-3 real :: x(N), y(N) \nS-4 real :: a = 2.0e0 \nS-5 \nS-6 x=(/ (i,i=1,N) /); y=0.0e0 !! initialize \nS-7 \nS-8 !$omp parallel \nS-9 !$omp loop \nS-10 do i=1,N; y(i) = a*x(i) + y(i); enddo \nS-11 !$omp end parallel \nS-12 \nS-13 if(y(N) /= N*2.0e0) print*,\"Error: 2*N /= y(N); y(N)=\",y(N) \nS-14 end program \nFortran \n52 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "3.16 Parallel Random Access Iterator Loop", "chunk": "C++ \n2 The following example shows a parallel random access iterator loop. \n3 Example pra_iterator.1.cpp (omp_3.0) \nS-1 #include <vector> \nS-2 void iterator_example() \nS-3 { \nS-4 std::vector<int> vec(23); \nS-5 std::vector<int>::iterator it; \nS-6 #pragma omp parallel for default(none) shared(vec) \nS-7 for (it = vec.begin(); it < vec.end(); it++) \nS-8 { \nS-9 // do work with *it // \nS-10 } \nS-11 } \nC++ \nCHAPTER 3. PARALLEL EXECUTION 53 \n"}
{"section_title": "3.17 omp_set_dynamic and  omp_set_num_threads Routines", "chunk": ""}
{"section_title": "3.17 omp_set_dynamic and  omp_set_num_threads Routines", "chunk": "2 omp_set_num_threads Routines \n3 Some programs rely on a fixed, prespecified number of threads to execute correctly. Because the \n4 default setting for the dynamic adjustment of the number of threads is implementation defined, such \n5 programs can choose to turn off the dynamic threads capability and set the number of threads \n6 explicitly to ensure portability. The following example shows how to do this using \n7 omp_set_dynamic, and omp_set_num_threads. \n8 In this example, the program executes correctly only if it is executed by 16 threads. If the \n9 implementation is not capable of supporting 16 threads, the behavior of this example is \n10 implementation defined. Note that the number of threads executing a parallel region remains \n11 constant during the region, regardless of the dynamic threads setting. The dynamic threads \n12 mechanism determines the number of threads to use at the start of the parallel region and keeps \n13 it constant for the duration of the region. \nC / C++ \n14 Example set_dynamic_nthrs.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 #include <stdlib.h> \nS-3 \nS-4 void do_by_16(float *x, int iam, int ipoints) {} \nS-5 \nS-6 void dynthreads(float *x, int npoints) \nS-7 { \nS-8 int iam, ipoints; \nS-9 \nS-10 omp_set_dynamic(0); \nS-11 omp_set_num_threads(16); \nS-12 \nS-13 #pragma omp parallel shared(x, npoints) private(iam, ipoints) \nS-14 { \nS-15 if (omp_get_num_threads() != 16) \nS-16 abort(); \nS-17 \nS-18 iam = omp_get_thread_num(); \nS-19 ipoints = npoints/16; \nS-20 do_by_16(x, iam, ipoints); \nS-21 } \nS-22 } \nC / C++ \n54 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example set_dynamic_nthrs.1.f (pre_omp_3.0) \nS-1 SUBROUTINE DO_BY_16(X, IAM, IPOINTS) \nS-2 REAL X(*) \nS-3 INTEGER IAM, IPOINTS \nS-4 END SUBROUTINE DO_BY_16 \nS-5 \nS-6 SUBROUTINE DYNTHREADS(X, NPOINTS) \nS-7 \nS-8 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-9 \nS-10 INTEGER NPOINTS \nS-11 REAL X(NPOINTS) \nS-12 \nS-13 INTEGER IAM, IPOINTS \nS-14 \nS-15 CALL OMP_SET_DYNAMIC(.FALSE.) \nS-16 CALL OMP_SET_NUM_THREADS(16) \nS-17 \nS-18 !$OMP PARALLEL SHARED(X,NPOINTS) PRIVATE(IAM, IPOINTS) \nS-19 \nS-20 IF (OMP_GET_NUM_THREADS() .NE. 16) THEN \nS-21 STOP \nS-22 ENDIF \nS-23 \nS-24 IAM = OMP_GET_THREAD_NUM() \nS-25 IPOINTS = NPOINTS/16 \nS-26 CALL DO_BY_16(X,IAM,IPOINTS) \nS-27 \nS-28 !$OMP END PARALLEL \nS-29 \nS-30 END SUBROUTINE DYNTHREADS \nFortran \nCHAPTER 3. PARALLEL EXECUTION 55 \n"}
{"section_title": "3.18 omp_get_num_threads Routine", "chunk": ""}
{"section_title": "3.18 omp_get_num_threads Routine", "chunk": "2 In the following example, the omp_get_num_threads call returns 1 in the sequential part of \n3 the code, so np will always be equal to 1. To determine the number of threads that will be deployed \n4 for the parallel region, the call should be inside the parallel region. \nC / C++ \n5 Example get_nthrs.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 void work(int i); \nS-3 \nS-4 void incorrect() { \nS-5 int np, i; \nS-6 \nS-7 np = omp_get_num_threads(); /* misplaced */ \nS-8 \nS-9 #pragma omp parallel for schedule(static) \nS-10 for (i=0; i < np; i++) \nS-11 work(i); \nS-12 } \nC / C++ \nFortran \n6 Example get_nthrs.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I) \nS-2 INTEGER I \nS-3 I = I + 1 \nS-4 END SUBROUTINE WORK \nS-5 \nS-6 SUBROUTINE INCORRECT() \nS-7 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-8 INTEGER I, NP \nS-9 \nS-10 NP = OMP_GET_NUM_THREADS() !misplaced: will return 1 \nS-11 !$OMP PARALLEL DO SCHEDULE(STATIC) \nS-12 DO I = 0, NP-1 \nS-13 CALL WORK(I) \nS-14 ENDDO \nS-15 !$OMP END PARALLEL DO \nS-16 END SUBROUTINE INCORRECT \nFortran \n56 OpenMP Examples Version 5.2.1 - November 2022 \n1 The following example shows how to rewrite this program without including a query for the \n2 number of threads: \nC / C++ \n3 Example get_nthrs.2.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 void work(int i); \nS-3 \nS-4 void correct() \nS-5 { \nS-6 int i; \nS-7 \nS-8 #pragma omp parallel private(i) \nS-9 { \nS-10 i = omp_get_thread_num(); \nS-11 work(i); \nS-12 } \nS-13 } \nC / C++ \nFortran \n4 Example get_nthrs.2.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I) \nS-2 INTEGER I \nS-3 \nS-4 I = I + 1 \nS-5 \nS-6 END SUBROUTINE WORK \nS-7 \nS-8 SUBROUTINE CORRECT() \nS-9 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-10 INTEGER I \nS-11 \nS-12 !$OMP PARALLEL PRIVATE(I) \nS-13 I = OMP_GET_THREAD_NUM() \nS-14 CALL WORK(I) \nS-15 !$OMP END PARALLEL \nS-16 \nS-17 END SUBROUTINE CORRECT \nFortran \nCHAPTER 3. PARALLEL EXECUTION 57 \nThis page intentionally left blank \n"}
{"section_title": "4 OpenMP Affinity", "chunk": ""}
{"section_title": "4 OpenMP Affinity", "chunk": "2 OpenMP Affinity consists of a proc_bind policy (thread affinity policy) and a specification of \n3 places (\u201clocation units\u201d or processors that may be cores, hardware threads, sockets, etc.). OpenMP \n4 Affinity enables users to bind computations on specific places. The placement will hold for the \n5 duration of the parallel region. However, the runtime is free to migrate the OpenMP threads to \n6 different cores (hardware threads, sockets, etc.) prescribed within a given place, if two or more \n7 cores (hardware threads, sockets, etc.) have been assigned to a given place. \n8 Often the binding can be managed without resorting to explicitly setting places. Without the \n9 specification of places in the OMP_PLACES variable, the OpenMP runtime will distribute and bind \n10 threads using the entire range of processors for the OpenMP program, according to the \n11 OMP_PROC_BIND environment variable or the proc_bind clause. When places are specified, \n12 the OMP runtime binds threads to the places according to a default distribution policy, or those \n13 specified in the OMP_PROC_BIND environment variable or the proc_bind clause. \n14 In the OpenMP Specifications document a processor refers to an execution unit that is enabled for \n15 an OpenMP thread to use. A processor is a core when there is no SMT (Simultaneous \n16 Multi-Threading) support or SMT is disabled. When SMT is enabled, a processor is a hardware \n17 thread (HW-thread). (This is the usual case; but actually, the execution unit is implementation \n18 defined.) Processor numbers are numbered sequentially from 0 to the number of cores less one \n19 (without SMT), or 0 to the number HW-threads less one (with SMT). OpenMP places use the \n20 processor number to designate binding locations (unless an \u201cabstract name\u201d is used.) \n21 The processors available to a process may be a subset of the system\u2019s processors. This restriction \n22 may be the result of a wrapper process controlling the execution (such as numactl on Linux \n23 systems), compiler options, library-specific environment variables, or default kernel settings. For \n24 instance, the execution of multiple MPI processes, launched on a single compute node, will each \n25 have a subset of processors as determined by the MPI launcher or set by MPI affinity environment \n26 variables for the MPI library. \n27 Threads of a team are positioned onto places in a compact manner, a scattered distribution, or onto \n28 the primary thread\u2019s place, by setting the OMP_PROC_BIND environment variable or the \n29 proc_bind clause to close, spread, or primary (master has been deprecated), \n30 respectively. When OMP_PROC_BIND is set to FALSE no binding is enforced; and when the value \n31 is TRUE, the binding is implementation defined to a set of places in the OMP_PLACES variable or \n32 to places defined by the implementation if the OMP_PLACES variable is not set. \n33 The OMP_PLACES variable can also be set to an abstract name (threads, cores, sockets) to \n34 specify that a place is either a single hardware thread, a core, or a socket, respectively. This \n35 description of the OMP_PLACES is most useful when the number of threads is equal to the number \n36 of hardware thread, cores or sockets. It can also be used with a close or spread distribution \n37 policy when the equality doesn\u2019t hold. \n59 \n"}
{"section_title": "4.1 proc_bind Clause", "chunk": "2 The following examples demonstrate how to use the proc_bind clause to control the thread \n3 binding for a team of threads in a parallel region. The machine architecture is depicted in \n4 Figure 4.1. It consists of two sockets, each equipped with a quad-core processor and configured to \n5 execute two hardware threads simultaneously on each core. These examples assume a contiguous \n6 core numbering starting from 0, such that the hardware threads 0,1 form the first physical core. \np0 p1 p2 p3  \nphysical core w/ 2  \nhardware threads \nsocket w/  \n4 physical cores \np4 p5 p6 p7  \nFIGURE 4.1: A machine architecture with two quad-core processors \n7 The following equivalent place list declarations consist of eight places (which we designate as p0 to \n8 p7): \n9 OMP_PLACES=\"{0,1},{2,3},{4,5},{6,7},{8,9},{10,11},{12,13},{14,15}\" \n10 or \n11 OMP_PLACES=\"{0:2}:8:2\" \n"}
{"section_title": "4.1.1 Spread Affinity Policy", "chunk": ""}
{"section_title": "4.1.1 Spread Affinity Policy", "chunk": "13 The following example shows the result of the spread affinity policy on the partition list when the \n14 number of threads is less than or equal to the number of places in the parent\u2019s place partition, for \n15 the machine architecture depicted above. Note that the threads are bound to the first place of each \n16 subpartition. \nC / C++ \n17 Example affinity.1.c (omp_4.0) \nS-1 void work(); \nS-2 \nS-3 int main() \nS-4 { \nS-5 \nS-6 #pragma omp parallel proc_bind(spread) num_threads(4) \nS-7 { \nS-8 work(); \n60 OpenMP Examples Version 5.2.1 - November 2022 \nS-9 } \nS-10 \nS-11 return 0; \nS-12 \nS-13 } \nC / C++ \nFortran \n1 Example affinity.1.f (omp_4.0) \nS-1 PROGRAM EXAMPLE \nS-2 !$OMP PARALLEL PROC_BIND(SPREAD) NUM_THREADS(4) \nS-3 CALL WORK() \nS-4 !$OMP END PARALLEL \nS-5 END PROGRAM EXAMPLE \nFortran \n2 It is unspecified on which place the primary thread is initially started. If the primary thread is \n3 initially started on p0, the following placement of threads will be applied in the parallel region: \n4 \u2022 thread 0 executes on p0 with the place partition p0,p1 \n5 \u2022 thread 1 executes on p2 with the place partition p2,p3 \n6 \u2022 thread 2 executes on p4 with the place partition p4,p5 \n7 \u2022 thread 3 executes on p6 with the place partition p6,p7 \n8 If the primary thread would initially be started on p2, the placement of threads and distribution of \n9 the place partition would be as follows: \n10 \u2022 thread 0 executes on p2 with the place partition p2,p3 \n11 \u2022 thread 1 executes on p4 with the place partition p4,p5 \n12 \u2022 thread 2 executes on p6 with the place partition p6,p7 \n13 \u2022 thread 3 executes on p0 with the place partition p0,p1 \n14 The following example illustrates the spread thread affinity policy when the number of threads is \n15 greater than the number of places in the parent\u2019s place partition. \n16 Let T be the number of threads in the team, and P be the number of places in the parent\u2019s place \n17 partition. The first T/P threads of the team (including the primary thread) execute on the parent\u2019s \n18 place. The next T/P threads execute on the next place in the place partition, and so on, with wrap \n19 around. \nCHAPTER 4. OPENMP AFFINITY 61 \nC / C++ \n1 Example affinity.2.c (omp_4.0) \nS-1 void work(); \nS-2 void foo() \nS-3 { \nS-4 #pragma omp parallel num_threads(16) proc_bind(spread) \nS-5 { \nS-6 work(); \nS-7 } \nS-8 } \nC / C++ \nFortran \n2 Example affinity.2.f90 (omp_4.0) \nS-1 subroutine foo \nS-2 !$omp parallel num_threads(16) proc_bind(spread) \nS-3 call work() \nS-4 !$omp end parallel \nS-5 end subroutine \nFortran \n3 It is unspecified on which place the primary thread is initially started. If the primary thread is \n4 initially started on p0, the following placement of threads will be applied in the parallel region: \n5 \u2022 threads 0,1 execute on p0 with the place partition p0 \n6 \u2022 threads 2,3 execute on p1 with the place partition p1 \n7 \u2022 threads 4,5 execute on p2 with the place partition p2 \n8 \u2022 threads 6,7 execute on p3 with the place partition p3 \n9 \u2022 threads 8,9 execute on p4 with the place partition p4 \n10 \u2022 threads 10,11 execute on p5 with the place partition p5 \n11 \u2022 threads 12,13 execute on p6 with the place partition p6 \n12 \u2022 threads 14,15 execute on p7 with the place partition p7 \n13 If the primary thread would initially be started on p2, the placement of threads and distribution of \n14 the place partition would be as follows: \n15 \u2022 threads 0,1 execute on p2 with the place partition p2 \n16 \u2022 threads 2,3 execute on p3 with the place partition p3 \n17 \u2022 threads 4,5 execute on p4 with the place partition p4 \n18 \u2022 threads 6,7 execute on p5 with the place partition p5 \n19 \u2022 threads 8,9 execute on p6 with the place partition p6 \n20 \u2022 threads 10,11 execute on p7 with the place partition p7 \n21 \u2022 threads 12,13 execute on p0 with the place partition p0 \n22 \u2022 threads 14,15 execute on p1 with the place partition p1 \n62 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "4.1.2 Close Affinity Policy", "chunk": ""}
{"section_title": "4.1.2 Close Affinity Policy", "chunk": "2 The following example shows the result of the close affinity policy on the partition list when the \n3 number of threads is less than or equal to the number of places in parent\u2019s place partition, for the \n4 machine architecture depicted above. The place partition is not changed by the close policy. \nC / C++ \n5 Example affinity.3.c (omp_4.0) \nS-1 void work(); \nS-2 int main() \nS-3 { \nS-4 #pragma omp parallel proc_bind(close) num_threads(4) \nS-5 { \nS-6 work(); \nS-7 } \nS-8 return 0; \nS-9 } \nC / C++ \nFortran \n6 Example affinity.3.f (omp_4.0) \nS-1 PROGRAM EXAMPLE \nS-2 !$OMP PARALLEL PROC_BIND(CLOSE) NUM_THREADS(4) \nS-3 CALL WORK() \nS-4 !$OMP END PARALLEL \nS-5 END PROGRAM EXAMPLE \nFortran \n7 It is unspecified on which place the primary thread is initially started. If the primary thread is \n8 initially started on p0, the following placement of threads will be applied in the parallel region: \n9 \u2022 thread 0 executes on p0 with the place partition p0-p7 \n10 \u2022 thread 1 executes on p1 with the place partition p0-p7 \n11 \u2022 thread 2 executes on p2 with the place partition p0-p7 \n12 \u2022 thread 3 executes on p3 with the place partition p0-p7 \n13 If the primary thread would initially be started on p2, the placement of threads and distribution of \n14 the place partition would be as follows: \n15 \u2022 thread 0 executes on p2 with the place partition p0-p7 \n16 \u2022 thread 1 executes on p3 with the place partition p0-p7 \n17 \u2022 thread 2 executes on p4 with the place partition p0-p7 \n18 \u2022 thread 3 executes on p5 with the place partition p0-p7 \nCHAPTER 4. OPENMP AFFINITY 63 \n1 The following example illustrates the close thread affinity policy when the number of threads is \n2 greater than the number of places in the parent\u2019s place partition. \n3 Let T be the number of threads in the team, and P be the number of places in the parent\u2019s place \n4 partition. The first T/P threads of the team (including the primary thread) execute on the parent\u2019s \n5 place. The next T/P threads execute on the next place in the place partition, and so on, with wrap \n6 around. The place partition is not changed by the close policy. \nC / C++ \n7 Example affinity.4.c (omp_4.0) \nS-1 void work(); \nS-2 void foo() \nS-3 { \nS-4 #pragma omp parallel num_threads(16) proc_bind(close) \nS-5 { \nS-6 work(); \nS-7 } \nS-8 } \nC / C++ \nFortran \n8 Example affinity.4.f90 (omp_4.0) \nS-1 subroutine foo \nS-2 !$omp parallel num_threads(16) proc_bind(close) \nS-3 call work() \nS-4 !$omp end parallel \nS-5 end subroutine \nFortran \n9 It is unspecified on which place the primary thread is initially started. If the primary thread is \n10 initially running on p0, the following placement of threads will be applied in the parallel region: \n11 \u2022 threads 0,1 execute on p0 with the place partition p0-p7 \n12 \u2022 threads 2,3 execute on p1 with the place partition p0-p7 \n13 \u2022 threads 4,5 execute on p2 with the place partition p0-p7 \n14 \u2022 threads 6,7 execute on p3 with the place partition p0-p7 \n15 \u2022 threads 8,9 execute on p4 with the place partition p0-p7 \n16 \u2022 threads 10,11 execute on p5 with the place partition p0-p7 \n17 \u2022 threads 12,13 execute on p6 with the place partition p0-p7 \n18 \u2022 threads 14,15 execute on p7 with the place partition p0-p7 \n19 If the primary thread would initially be started on p2, the placement of threads and distribution of \n20 the place partition would be as follows: \n21 \u2022 threads 0,1 execute on p2 with the place partition p0-p7 \n64 OpenMP Examples Version 5.2.1 - November 2022 \n1 \u2022 threads 2,3 execute on p3 with the place partition p0-p7 \n2 \u2022 threads 4,5 execute on p4 with the place partition p0-p7 \n3 \u2022 threads 6,7 execute on p5 with the place partition p0-p7 \n4 \u2022 threads 8,9 execute on p6 with the place partition p0-p7 \n5 \u2022 threads 10,11 execute on p7 with the place partition p0-p7 \n6 \u2022 threads 12,13 execute on p0 with the place partition p0-p7 \n7 \u2022 threads 14,15 execute on p1 with the place partition p0-p7 \n"}
{"section_title": "4.1.3 Primary Affinity Policy", "chunk": ""}
{"section_title": "4.1.3 Primary Affinity Policy", "chunk": "9 The following example shows the result of the primary affinity policy on the partition list for the \n10 machine architecture depicted above. The place partition is not changed by the primary policy. \nC / C++ \n11 Example affinity.5.c (omp_5.1) \nS-1 void work(); \nS-2 int main() \nS-3 { \nS-4 #pragma omp parallel proc_bind(primary) num_threads(4) \nS-5 { \nS-6 work(); \nS-7 } \nS-8 return 0; \nS-9 } \nC / C++ \nFortran \n12 Example affinity.5.f (omp_5.1) \nS-1 PROGRAM EXAMPLE \nS-2 !$OMP PARALLEL PROC_BIND(primary) NUM_THREADS(4) \nS-3 CALL WORK() \nS-4 !$OMP END PARALLEL \nS-5 END PROGRAM EXAMPLE \nFortran \nCHAPTER 4. OPENMP AFFINITY 65 \n1 It is unspecified on which place the primary thread is initially started. If the primary thread is \n2 initially running on p0, the following placement of threads will be applied in the parallel region: \n3 \u2022 threads 0-3 execute on p0 with the place partition p0-p7 \n4 If the primary thread would initially be started on p2, the placement of threads and distribution of \n5 the place partition would be as follows: \n6 \u2022 threads 0-3 execute on p2 with the place partition p0-p7 \n"}
{"section_title": "4.2 Task Affinity", "chunk": ""}
{"section_title": "4.2 Task Affinity", "chunk": "8 The next example illustrates the use of the affinity clause with a task construct. The variables \n9 in the affinity clause provide a hint to the runtime that the task should execute \u201cclose\u201d to the \n10 physical storage location of the variables. For example, on a two-socket platform with a local \n11 memory component close to each processor socket, the runtime will attempt to schedule the task \n12 execution on the socket where the storage is located. \n13 Because the C/C++ code employs a pointer, an array section is used in the affinity clause. \n14 Fortran code can use an array reference to specify the storage, as shown here. \n15 Note, in the second task of the C/C++ code the B pointer is declared shared. Otherwise, by default, \n16 it would be firstprivate since it is a local variable, and would probably be saved for the second task \n17 before being assigned a storage address by the first task. Also, one might think it reasonable to use \n18 the affinity clause affinity(B[:N]) on the second task construct. However, the storage behind \n19 B is created in the first task, and the array section reference may not be valid when the second task \n20 is generated. The use of the A array is sufficient for this case, because one would expect the storage \n21 for A and B would be physically \u201cclose\u201d (as provided by the hint in the first task). \nC / C++ \n22 Example affinity.6.c (omp_5.0) \nS-1 double * alloc_init_B(double *A, int N); \nS-2 void compute_on_B(double *B, int N); \nS-3 \nS-4 void task_affinity(double *A, int N) \nS-5 { \nS-6 double * B; \nS-7 #pragma omp task depend(out:B) shared(B) affinity(A[0:N]) \nS-8 { \nS-9 B = alloc_init_B(A,N); \nS-10 } \nS-11 \nS-12 #pragma omp task depend( in:B) shared(B) affinity(A[0:N]) \nS-13 { \nS-14 compute_on_B(B,N); \n66 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 } \nS-16 \nS-17 #pragma omp taskwait \nS-18 } \nC / C++ \nFortran \n1 Example affinity.6.f90 (omp_5.0) \nS-1 subroutine task_affinity(A, N) \nS-2 \nS-3 external alloc_init_B \nS-4 external compute_on_B \nS-5 double precision, allocatable :: B(:) \nS-6 \nS-7 !$omp task depend(out:B) shared(B) affinity(A) \nS-8 call alloc_init_B(B,A) \nS-9 !$omp end task \nS-10 \nS-11 !$omp task depend(in:B) shared(B) affinity(A) \nS-12 call compute_on_B(B) \nS-13 !$omp end task \nS-14 \nS-15 !$omp taskwait \nS-16 \nS-17 end subroutine \nFortran \n"}
{"section_title": "4.3 Affinity Display", "chunk": ""}
{"section_title": "4.3 Affinity Display", "chunk": "3 The following examples illustrate ways to display thread affinity. Automatic display of affinity can \n4 be invoked by setting the OMP_DISPLAY_AFFINITY environment variable to TRUE. The format \n5 of the output can be customized by setting the OMP_AFFINITY_FORMAT environment variable to \n6 an appropriate string. Also, there are API calls for the user to display thread affinity at selected \n7 locations within code. \n8 For the first example the environment variable OMP_DISPLAY_AFFINITY has been set to TRUE, \n9 and execution occurs on an 8-core system with OMP_NUM_THREADS set to 8. \n10 The affinity for the primary thread is reported through a call to the API \n11 omp_display_affinity() routine. For default affinity settings the report shows that the \n12 primary thread can execute on any of the cores. In the following parallel region the affinity for each \n13 of the team threads is reported automatically since the OMP_DISPLAY_AFFINITY environment \n14 variable has been set to TRUE. \nCHAPTER 4. OPENMP AFFINITY 67 \n1 These two reports are often useful (as in hybrid codes using both MPI and OpenMP) to observe the \n2 affinity (for an MPI task) before the parallel region, and during an OpenMP parallel region. Note: \n3 the next parallel region uses the same number of threads as in the previous parallel region and \n4 affinities are not changed, so affinity is NOT reported. \n5 In the last parallel region, the thread affinities are reported because the thread affinity has changed. \nC / C++ \n6 Example affinity_display.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main(void){ //MAX threads = 8, single socket system \nS-5 \nS-6 //API call-- Displays Affinity of Primary Thread \nS-7 omp_display_affinity(NULL); \nS-8 \nS-9 // API CALL OUTPUT (default format): \nS-10 // team_num= 0, nesting_level= 0, thread_num= 0, \nS-11 // thread_affinity= 0,1,2,3,4,5,6,7 \nS-12 \nS-13 // OMP_DISPLAY_AFFINITY=TRUE, OMP_NUM_THREADS=8 \nS-14 #pragma omp parallel num_threads(omp_get_num_procs()) \nS-15 { \nS-16 if(omp_get_thread_num()==0) \nS-17 printf(\"1st Parallel Region -- Affinity Reported"}
{"section_title": "4.3 Affinity Display", "chunk": "3 The following examples illustrate ways to display thread affinity. Automatic display of affinity can \n4 be invoked by setting the OMP_DISPLAY_AFFINITY environment variable to TRUE. The format \n5 of the output can be customized by setting the OMP_AFFINITY_FORMAT environment variable to \n6 an appropriate string. Also, there are API calls for the user to display thread affinity at selected \n7 locations within code. \n8 For the first example the environment variable OMP_DISPLAY_AFFINITY has been set to TRUE, \n9 and execution occurs on an 8-core system with OMP_NUM_THREADS set to 8. \n10 The affinity for the primary thread is reported through a call to the API \n11 omp_display_affinity() routine. For default affinity settings the report shows that the \n12 primary thread can execute on any of the cores. In the following parallel region the affinity for each \n13 of the team threads is reported automatically since the OMP_DISPLAY_AFFINITY environment \n14 variable has been set to TRUE. \nCHAPTER 4. OPENMP AFFINITY 67 \n1 These two reports are often useful (as in hybrid codes using both MPI and OpenMP) to observe the \n2 affinity (for an MPI task) before the parallel region, and during an OpenMP parallel region. Note: \n3 the next parallel region uses the same number of threads as in the previous parallel region and \n4 affinities are not changed, so affinity is NOT reported. \n5 In the last parallel region, the thread affinities are reported because the thread affinity has changed. \nC / C++ \n6 Example affinity_display.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main(void){ //MAX threads = 8, single socket system \nS-5 \nS-6 //API call-- Displays Affinity of Primary Thread \nS-7 omp_display_affinity(NULL); \nS-8 \nS-9 // API CALL OUTPUT (default format): \nS-10 // team_num= 0, nesting_level= 0, thread_num= 0, \nS-11 // thread_affinity= 0,1,2,3,4,5,6,7 \nS-12 \nS-13 // OMP_DISPLAY_AFFINITY=TRUE, OMP_NUM_THREADS=8 \nS-14 #pragma omp parallel num_threads(omp_get_num_procs()) \nS-15 { \nS-16 if(omp_get_thread_num()==0) \nS-17 printf(\"1st Parallel Region -- Affinity Reported\"); \nS-18 \nS-19 // DISPLAY OUTPUT (default format) has been sorted: \nS-20 // team_num= 0, nesting_level= 1, thread_num= 0, thread_affinity= 0 \nS-21 // team_num= 0, nesting_level= 1, thread_num= 1, thread_affinity= 1 \nS-22 // ... \nS-23 // team_num= 0, nesting_level= 1, thread_num= 7, thread_affinity= 7 \nS-24 \nS-25 // doing work here \nS-26 } \nS-27 \nS-28 #pragma omp parallel num_threads( omp_get_num_procs() ) \nS-29 { \nS-30 if(omp_get_thread_num()==0) \nS-31 printf(\"%s%s\\n\",\"Same Affinity as in Previous Parallel Region\", \nS-32 \" -- no Affinity Reported\\n\"); \nS-33 \nS-34 // NO AFFINITY OUTPUT: \nS-35 //(output in 1st parallel region only for OMP_DISPLAY_AFFINITY=TRUE) \nS-36 \nS-37 // doing more work here \n68 OpenMP Examples Version 5.2.1 - November 2022 \nS-38 } \nS-39 \nS-40 // Report Affinity for 1/2 number of threads \nS-41 #pragma omp parallel num_threads( omp_get_num_procs()/2 ) \nS-42 { \nS-43 if(omp_get_thread_num()==0) \nS-44 printf(\"Report Affinity for using 1/2 of max threads.\\n\"); \nS-45 \nS-46 // DISPLAY OUTPUT (default format) has been sorted: \nS-47 // team_num= 0, nesting_level= 1, thread_num= 0, thread_affinity= 0,1 \nS-48 // team_num= 0, nesting_level= 1, thread_num= 1, thread_affinity= 2,3 \nS-49 // team_num= 0, nesting_level= 1, thread_num= 2, thread_affinity= 4,5 \nS-50 // team_num= 0, nesting_level= 1, thread_num= 3, thread_affinity= 6,7 \nS-51 \nS-52 // do work \nS-53 } \nS-54 \nS-55 return 0; \nS-56 } \nC / C++ \nFortran \n1 Example affinity_display.1.f90 (omp_5.0) \nS-1 program affinity_display ! MAX threads = 8, single socket system \nS-2 \nS-3 use omp_lib \nS-4 implicit none \nS-5 character(len=0) :: null \nS-6 \nS-7 ! API call - Displays Affinity of Primary Thread \nS-8 call omp_display_affinity(null) \nS-9 \nS-10 ! API CALL OUTPUT (default format): \nS-11 ! team_num= 0, nesting_level= 0, thread_num= 0, & \nS-12 ! thread_affinity= 0,1,2,3,4,5,6,7 \nS-13 \nS-14 \nS-15 ! OMP_DISPLAY_AFFINITY=TRUE, OMP_NUM_THREADS=8 \nS-16 \nS-17 !$omp parallel num_threads(omp_get_num_procs()) \nS-18 \nS-19 if(omp_get_thread_num()==0) then \nS-20 print*, \"1st Parallel Region -- Affinity Reported\" \nS-21 endif \nS-22 \nS-23 ! DISPLAY OUTPUT (default format) has been sorted: \nCHAPTER 4. OPENMP AFFINITY 69 \nS-24 ! team_num= 0, nesting_level= 1, thread_num= 0, thread_affinity= 0 \nS-25 ! team_num= 0, nesting_level= 1, thread_num= 1, thread_affinity= 1 \nS-26 ! ... \nS-27 ! team_num= 0, nesting_level= 1, thread_num= 7, thread_affinity= 7 \nS-28 \nS-29 ! doing work here \nS-30 \nS-31 !$omp end parallel \nS-32 \nS-33 !$omp parallel num_threads( omp_get_num_procs() ) \nS-34 \nS-35 if(omp_get_thread_num()==0) then \nS-36 print*, \"Same Affinity in Parallel Region -- no Affinity Reported\" \nS-37 endif \nS-38 \nS-39 ! NO AFFINITY OUTPUT: \nS-40 ! (output in 1st parallel region only for \nS-41 ! OMP_DISPLAY_AFFINITY=TRUE) \nS-42 \nS-43 ! doing more work here \nS-44 \nS-45 !$omp end parallel \nS-46 \nS-47 ! Report Affinity for 1/2 number of threads \nS-48 !$omp parallel num_threads( omp_get_num_procs()/2 ) \nS-49 \nS-50 if(omp_get_thread_num()==0) then \nS-51 print*, \"Altered Affinity in Parallel Region -- Affinity Reported\" \nS-52 endif \nS-53 \nS-54 ! DISPLAY OUTPUT (default format) has been sorted: \nS-55 ! team_num= 0, nesting_level= 1, thread_num= 0, & \nS-56 ! thread_affinity= 0,1 \nS-57 ! team_num= 0, nesting_level= 1, thread_num= 1, & \nS-58 ! thread_affinity= 2,3 \nS-59 ! team_num= 0, nesting_level= 1, thread_num= 2, & \nS-60 ! thread_affinity= 4,5 \nS-61 ! team_num= 0, nesting_level= 1, thread_num= 3, & \nS-62 ! thread_affinity= 6,7 \nS-63 \nS-64 ! do work \nS-65 \nS-66 !$omp end parallel \nS-67 \nS-68 end program \nFortran \n70 OpenMP Examples Version 5.2.1 - November 2022 \n1 In the following example 2 threads are forked, and each executes on a socket. Next, a nested parallel \n2 region runs half of the available threads on each socket. \n3 These OpenMP environment variables have been set: \n4 \u2022 OMP_PROC_BIND=\"TRUE\" \n5 \u2022 OMP_NUM_THREADS=\"2,4\" \n6 \u2022 OMP_PLACES=\"{0,2,4,6},{1,3,5,7}\" \n7 \u2022 OMP_AFFINITY_FORMAT=\"nest_level= %L, parent_thrd_num= %a, thrd_num= %n, \n8 thrd_affinity= %A\" \n9 where the numbers correspond to core ids for the system. Note, OMP_DISPLAY_AFFINITY is \n10 not set and is FALSE by default. This example shows how to use API routines to perform affinity \n11 display operations. \n12 For each of the two first-level threads the OMP_PLACES variable specifies a place with all the \n13 core-ids of the socket ({0,2,4,6} for one thread and {1,3,5,7} for the other). (As is sometimes the \n14 case in 2-socket systems, one socket may consist of the even id numbers, while the other may have \n15 the odd id numbers.) The affinities are printed according to the OMP_AFFINITY_FORMAT \n16 format: providing the parallel nesting level (%L), the ancestor thread number (%a), the thread \n17 number (%n) and the thread affinity (%A). In the nested parallel region within the socket_work \n18 routine the affinities for the threads on each socket are printed according to this format. \nC / C++ \n19 Example affinity_display.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <omp.h> \nS-4 \nS-5 void socket_work(int socket_num, int n_thrds); \nS-6 \nS-7 int main(void) \nS-8 { \nS-9 int n_sockets, socket_num, n_thrds_on_socket; \nS-10 \nS-11 omp_set_nested(1); // or env var= OMP_NESTED=true \nS-12 omp_set_max_active_levels(2); // or env var= OMP_MAX_ACTIVE_LEVELS=2 \nS-13 \nS-14 n_sockets = omp_get_num_places(); \nS-15 n_thrds_on_socket = omp_get_place_num_procs(0); \nS-16 \nS-17 // OMP_NUM_THREADS=2,4 \nS-18 // OMP_PLACES=\"{0,2,4,6},{1,3,5,7}\" #2 sockets; even/odd proc-ids \nS-19 // OMP_AFFINITY_FORMAT=\\ \nS-20 // \"nest_level= %L, parent_thrd_num= %a, thrd_num= %n, thrd_affinity= %A\" \nS-21 \nCHAPTER 4. OPENMP AFFINITY 71 \nS-22 #pragma omp parallel num_threads(n_sockets) private(socket_num) \nS-23 { \nS-24 socket_num = omp_get_place_num(); \nS-25 \nS-26 if(socket_num==0) \nS-27 printf(\" LEVEL 1 AFFINITIES 1 thread/socket, %d sockets:\\n\\n\", \nS-28 n_sockets); \nS-29 \nS-30 // not needed if OMP_DISPLAY_AFFINITY=TRUE \nS-31 omp_display_affinity(NULL); \nS-32 \nS-33 // OUTPUT: \nS-34 // LEVEL 1 AFFINITIES 1 thread/socket, 2 sockets: \nS-35 // nest_level= 1, parent_thrd_num= 0, thrd_num= 0, thrd_affinity= 0,2,4,6 \nS-36 // nest_level= 1, parent_thrd_num= 0, thrd_num= 1, thrd_affinity= 1,3,5,7 \nS-37 \nS-38 socket_work(socket_num, n_thrds_on_socket); \nS-39 } \nS-40 \nS-41 return 0; \nS-42 } \nS-43 \nS-44 void socket_work(int socket_num, int n_thrds) \nS-45 { \nS-46 #pragma omp parallel num_threads(n_thrds) \nS-47 { \nS-48 if(omp_get_thread_num()==0) \nS-49 printf(\" LEVEL 2 AFFINITIES, %d threads on socket %d\\n\", \nS-50 n_thrds, socket_num); \nS-51 \nS-52 // not needed if OMP_DISPLAY_AFFINITY=TRUE \nS-53 omp_display_affinity(NULL); \nS-54 \nS-55 // OUTPUT: \nS-56 // LEVEL 2 AFFINITIES, 4 threads on socket 0 \nS-57 // nest_level= 2, parent_thrd_num= 0, thrd_num= 0, thrd_affinity= 0 \nS-58 // nest_level= 2, parent_thrd_num= 0, thrd_num= 1, thrd_affinity= 2 \nS-59 // nest_level= 2, parent_thrd_num= 0, thrd_num= 2, thrd_affinity= 4 \nS-60 // nest_level= 2, parent_thrd_num= 0, thrd_num= 3, thrd_affinity= 6 \nS-61 \nS-62 // LEVEL 2 AFFINITIES, 4 threads on socket 1 \nS-63 // nest_level= 2, parent_thrd_num= 1, thrd_num= 0, thrd_affinity= 1 \nS-64 // nest_level= 2, parent_thrd_num= 1, thrd_num= 1, thrd_affinity= 3 \nS-65 // nest_level= 2, parent_thrd_num= 1, thrd_num= 2, thrd_affinity= 5 \nS-66 // nest_level= 2, parent_thrd_num= 1, thrd_num= 3, thrd_affinity= 7 \nS-67 \nS-68 // ... Do Some work on Socket \n72 OpenMP Examples Version 5.2.1 - November 2022 \nS-69 } \nS-70 } \nC / C++ \nFortran \n1 Example affinity_display.2.f90 (omp_5.0) \nS-1 program affinity_display \nS-2 \nS-3 use omp_lib \nS-4 implicit none \nS-5 character(len=0) :: null \nS-6 integer :: n_sockets, socket_num, n_thrds_on_socket; \nS-7 \nS-8 call omp_set_nested(.true.) ! or env var= OMP_NESTED=true \nS-9 call omp_set_max_active_levels(2) ! or env var= OMP_MAX_ACTIVE_LEVELS=2 \nS-10 \nS-11 n_sockets = omp_get_num_places() \nS-12 n_thrds_on_socket = omp_get_place_num_procs(0) \nS-13 \nS-14 ! OMP_NUM_THREADS=2,4 \nS-15 ! OMP_PLACES=\"{0,2,4,6},{1,3,5,7}\" #2 sockets; even/odd proc-ids \nS-16 ! OMP_AFFINITY_FORMAT=\\ \nS-17 !\"nest_level= %L, parent_thrd_num= %a, thrd_num= %n, thrd_affinity= %A\" \nS-18 \nS-19 !$omp parallel num_threads(n_sockets) private(socket_num) \nS-20 \nS-21 socket_num = omp_get_place_num() \nS-22 \nS-23 if(socket_num==0) then \nS-24 write(*,\u2019(\"LEVEL 1 AFFINITIES 1 thread/socket \",i0,\" sockets\")\u2019) & \nS-25 n_sockets \nS-26 endif \nS-27 \nS-28 call omp_display_affinity(null) ! not needed \nS-29 ! if OMP_DISPLAY_AFFINITY=TRUE \nS-30 \nS-31 ! OUTPUT: \nS-32 ! LEVEL 1 AFFINITIES 1 thread/socket, 2 sockets: \nS-33 ! nest_level= 1, parent_thrd_num= 0, thrd_num= 0, & \nS-34 ! thrd_affinity= 0,2,4,6 \nS-35 ! nest_level= 1, parent_thrd_num= 0, thrd_num= 1, & \nS-36 ! thrd_affinity= 1,3,5,7 \nS-37 \nS-38 call socket_work(socket_num, n_thrds_on_socket) \nS-39 \nS-40 !$omp end parallel \nCHAPTER 4. OPENMP AFFINITY 73 \nS-41 \nS-42 end program \nS-43 \nS-44 subroutine socket_work(socket_num, n_thrds) \nS-45 use omp_lib \nS-46 implicit none \nS-47 integer :: socket_num, n_thrds \nS-48 character(len=0) :: null \nS-49 \nS-50 !$omp parallel num_threads(n_thrds) \nS-51 \nS-52 if(omp_get_thread_num()==0) then \nS-53 write(*,\u2019(\"LEVEL 2 AFFINITIES, \",i0,\" threads on socket \",i0)\u2019) & \nS-54 n_thrds,socket_num \nS-55 endif \nS-56 \nS-57 call omp_display_affinity(null) ! not needed \nS-58 ! if OMP_DISPLAY_AFFINITY=TRUE \nS-59 \nS-60 ! OUTPUT: \nS-61 ! LEVEL 2 AFFINITIES, 4 threads on socket 0 \nS-62 ! nest_level= 2, parent_thrd_num= 0, thrd_num= 0, thrd_affinity= 0 \nS-63 ! nest_level= 2, parent_thrd_num= 0, thrd_num= 1, thrd_affinity= 2 \nS-64 ! nest_level= 2, parent_thrd_num= 0, thrd_num= 2, thrd_affinity= 4 \nS-65 ! nest_level= 2, parent_thrd_num= 0, thrd_num= 3, thrd_affinity= 6 \nS-66 \nS-67 ! LEVEL 2 AFFINITIES, 4 thrds on socket 1 \nS-68 ! nest_level= 2, parent_thrd_num= 1, thrd_num= 0, thrd_affinity= 1 \nS-69 ! nest_level= 2, parent_thrd_num= 1, thrd_num= 1, thrd_affinity= 3 \nS-70 ! nest_level= 2, parent_thrd_num= 1, thrd_num= 2, thrd_affinity= 5 \nS-71 ! nest_level= 2, parent_thrd_num= 1, thrd_num= 3, thrd_affinity= 7 \nS-72 \nS-73 ! ... Do Some work on Socket \nS-74 \nS-75 !$omp end parallel \nS-76 \nS-77 end subroutine \nFortran \n1 The next example illustrates more details about affinity formatting. First, the \n2 omp_get_affinity_format() API routine is used to obtain the default format. The code \n3 checks to make sure the storage provides enough space to hold the format. Next, the \n4 omp_set_affinity_format() API routine sets a user-defined format: host=%20H \n5 thrd_num=%0.4n binds_to=%A. \n6 The host, thread number and affinity fields are specified by %20H, %0.4n and %A: H, n and A are \n7 single character \u201cshort names\u201d for the host, thread_num and thread_affinity data to be printed, with \n74 OpenMP Examples Version 5.2.1 - November 2022 \n1 format sizes of 20, 4, and \u201csize as needed\u201d. The period (.) indicates that the field is displayed \n2 right-justified (default is left-justified) and the \u201c0\u201d indicates that any unused space is to be prefixed \n3 with zeros (e.g. instead of \u201c1\u201d, \u201c0001\u201d is displayed for the field size of 4). \n4 Within the parallel region the affinity for each thread is captured by \n5 omp_capture_affinity() into a buffer array with elements indexed by the thread number \n6 (thrd_num). After the parallel region, the thread affinities are printed in thread-number order. \n7 If the storage area in buffer is inadequate for holding the affinity data, the stored affinity data is \n8 truncated. The maximum value for the number of characters (nchars) returned by \n9 omp_capture_affinity is captured by the reduction(max:max_req_store) clause \n10 and the if(nchars >= max_req_store) max_req_store=nchars statement. It is used to report possible \n11 truncation (if max_req_store > buffer_store). \nC / C++ \n12 Example affinity_display.3.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> // also null is in <stddef.h> \nS-3 #include <stddef.h> \nS-4 #include <string.h> \nS-5 #include <omp.h> \nS-6 \nS-7 #define FORMAT_STORE 80 \nS-8 #define BUFFER_STORE 80 \nS-9 \nS-10 int main(void){ \nS-11 \nS-12 int i, n, thrd_num, max_req_store; \nS-13 size_t nchars; \nS-14 \nS-15 char default_format[FORMAT_STORE]; \nS-16 char my_format[] = \"host=%20H thrd_num=%0.4n binds_to=%A\"; \nS-17 char **buffer; \nS-18 \nS-19 \nS-20 // CODE SEGMENT 1 AFFINITY FORMAT \nS-21 \nS-22 // Get and Display Default Affinity Format \nS-23 \nS-24 nchars = omp_get_affinity_format(default_format,(size_t)FORMAT_STORE); \nS-25 printf(\"Default Affinity Format is: %s\\n\",default_format); \nS-26 \nS-27 if(nchars >= FORMAT_STORE){ \nS-28 printf(\"Caution: Reported Format is truncated. Increase\\n\"); \nS-29 printf(\" FORMAT_STORE to %d.\\n\", nchars+1); \nS-30 } \nCHAPTER 4. OPENMP AFFINITY 75 \nS-31 \nS-32 // Set Affinity Format \nS-33 \nS-34 omp_set_affinity_format(my_format); \nS-35 printf(\"Affinity Format set to: %s\\n\",my_format); \nS-36 \nS-37 \nS-38 // CODE SEGMENT 2 CAPTURE AFFINITY \nS-39 \nS-40 // Set up buffer for affinity of n threads \nS-41 \nS-42 n = omp_get_num_procs(); \nS-43 buffer = (char **)malloc( sizeof(char *) * n ); \nS-44 for(i=0;i<n;i++){ \nS-45 buffer[i]=(char *)malloc( sizeof(char) * BUFFER_STORE); \nS-46 } \nS-47 \nS-48 // Capture Affinity using Affinity Format set above. \nS-49 // Use max reduction to check size of buffer areas \nS-50 max_req_store = 0; \nS-51 #pragma omp parallel private(thrd_num,nchars) \\ \nS-52 reduction(max:max_req_store) \nS-53 { \nS-54 //safety: don\u2019t exceed # of buffers \nS-55 if(omp_get_num_threads()>n) exit(1); \nS-56 \nS-57 thrd_num=omp_get_thread_num(); \nS-58 nchars=omp_capture_affinity(buffer[thrd_num], \nS-59 (size_t)BUFFER_STORE,NULL); \nS-60 if(nchars > max_req_store) max_req_store=nchars; \nS-61 \nS-62 // ... \nS-63 } \nS-64 \nS-65 for(i=0;i<n;i++){ \nS-66 printf(\"thrd_num= %d, affinity: %s\\n\", i,buffer[i]); \nS-67 } \nS-68 // For 4 threads with OMP_PLACES=\u2019{0,1},{2,3},{4,5},{6,7}\u2019 \nS-69 // Format host=%20H thrd_num=%0.4n binds_to=%A \nS-70 \nS-71 // affinity: host=hpc.cn567 thrd_num=0000 binds_to=0,1 \nS-72 // affinity: host=hpc.cn567 thrd_num=0001 binds_to=2,3 \nS-73 // affinity: host=hpc.cn567 thrd_num=0002 binds_to=4,5 \nS-74 // affinity: host=hpc.cn567 thrd_num=0003 binds_to=6,7 \nS-75 \nS-76 \nS-77 if(max_req_store>=BUFFER_STORE){ \n76 OpenMP Examples Version 5.2.1 - November 2022 \nS-78 printf(\"Caution: Affinity string truncated. Increase\\n\"); \nS-79 printf(\" BUFFER_STORE to %d\\n\",max_req_store+1); \nS-80 } \nS-81 \nS-82 for(i=0;i<n;i++) free(buffer[i]); \nS-83 free (buffer); \nS-84 \nS-85 return 0; \nS-86 } \nC / C++ \nFortran \n1 Example affinity_display.3.f90 (omp_5.0) \nS-1 program affinity_display \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer, parameter :: FORMAT_STORE=80 \nS-5 integer, parameter :: BUFFER_STORE=80 \nS-6 \nS-7 integer :: i, n, thrd_num, nchars, max_req_store \nS-8 \nS-9 character(FORMAT_STORE) :: default_format \nS-10 character(*), parameter :: my_format = & \nS-11 \"host=%20H thrd_num=%0.4n binds_to=%A\" \nS-12 character(:), allocatable :: buffer(:) \nS-13 character(len=0) :: null \nS-14 \nS-15 \nS-16 ! CODE SEGMENT 1 AFFINITY FORMAT \nS-17 \nS-18 ! Get and Display Default Affinity Format \nS-19 \nS-20 nchars = omp_get_affinity_format(default_format) \nS-21 print*,\"Default Affinity Format: \", trim(default_format) \nS-22 \nS-23 if( nchars > FORMAT_STORE) then \nS-24 print*,\"Caution: Reported Format is truncated. Increase\" \nS-25 print*,\" FORMAT_STORE to \", nchars \nS-26 endif \nS-27 \nS-28 ! Set Affinity Format \nS-29 \nS-30 call omp_set_affinity_format(my_format) \nS-31 print*,\"Affinity Format set to: \", my_format \nS-32 \nS-33 \nCHAPTER 4. OPENMP AFFINITY 77 \nS-34 ! CODE SEGMENT 2 CAPTURE AFFINITY \nS-35 \nS-36 ! Set up buffer for affinity of n threads \nS-37 \nS-38 n = omp_get_num_procs() \nS-39 allocate( character(len=BUFFER_STORE)::buffer(0:n-1) ) \nS-40 \nS-41 ! Capture Affinity using Affinity Format set above. \nS-42 ! Use max reduction to check size of buffer areas \nS-43 max_req_store = 0 \nS-44 !$omp parallel private(thrd_num,nchars) reduction(max:max_req_store) \nS-45 \nS-46 if(omp_get_num_threads()>n) stop \"ERROR: increase buffer lines\" \nS-47 \nS-48 thrd_num=omp_get_thread_num() \nS-49 nchars=omp_capture_affinity(buffer(thrd_num),null) \nS-50 if(nchars>max_req_store) max_req_store=nchars \nS-51 ! ... \nS-52 \nS-53 !$omp end parallel \nS-54 \nS-55 do i = 0, n-1 \nS-56 print*, \"thrd_num= \",i,\" affinity:\", trim(buffer(i)) \nS-57 end do \nS-58 ! For 4 threads with OMP_PLACES=\u2019{0,1},{2,3},{4,5},{6,7}\u2019 \nS-59 ! Format: host=%20H thrd_num=%0.4n binds_to=%A \nS-60 \nS-61 ! affinity: host=hpc.cn567 thrd_num=0000 binds_to=0,1 \nS-62 ! affinity: host=hpc.cn567 thrd_num=0001 binds_to=2,3 \nS-63 ! affinity: host=hpc.cn567 thrd_num=0002 binds_to=4,5 \nS-64 ! affinity: host=hpc.cn567 thrd_num=0003 binds_to=6,7 \nS-65 \nS-66 if(max_req_store > BUFFER_STORE) then \nS-67 print*, \"Caution: Affinity string truncated. Increase\" \nS-68 print*, \" BUFFER_STORE to \",max_req_store \nS-69 endif \nS-70 \nS-71 deallocate(buffer) \nS-72 end program \nFortran \n78 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "4.4 Affinity Query Functions", "chunk": ""}
{"section_title": "4.4 Affinity Query Functions", "chunk": "2 In the example below a team of threads is generated on each socket of the system, using nested \n3 parallelism. Several query functions are used to gather information to support the creation of the \n4 teams and to obtain socket and thread numbers. \n5 For proper execution of the code, the user must create a place partition, such that each place is a \n6 listing of the core numbers for a socket. For example, in a 2 socket system with 8 cores in each \n7 socket, and sequential numbering in the socket for the core numbers, the OMP_PLACES variable \n8 would be set to \"{0:8},{8:8}\", using the place syntax {lower_bound:length:stride}, and the default \n9 stride of 1. \n10 The code determines the number of sockets (n_sockets) using the omp_get_num_places() \n11 query function. In this example each place is constructed with a list of each socket\u2019s core numbers, \n12 hence the number of places is equal to the number of sockets. \n13 The outer parallel region forms a team of threads, and each thread executes on a socket (place) \n14 because the proc_bind clause uses spread in the outer parallel construct. Next, in the \n15 socket_init function, an inner parallel region creates a team of threads equal to the number of \n16 elements (core numbers) from the place of the parent thread. Because the outer parallel \n17 construct uses a spread affinity policy, each of its threads inherits a subpartition of the original \n18 partition. Hence, the omp_get_place_num_procs query function returns the number of \n19 elements (here procs = cores) in the subpartition of the thread. After each parent thread creates its \n20 nested parallel region on the section, the socket number and thread number are reported. \n21 Note: Portable tools like hwloc (Portable HardWare LOCality package), which support many \n22 common operating systems, can be used to determine the configuration of a system. On some \n23 systems there are utilities, files or user guides that provide configuration information. For instance, \n24 the socket number and proc_id\u2019s for a socket can be found in the /proc/cpuinfo text file on Linux \n25 systems. \nC / C++ \n26 Example affinity_query.1.c (omp_4.5) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 void socket_init(int socket_num) \nS-5 { \nS-6 int n_procs; \nS-7 \nS-8 n_procs = omp_get_place_num_procs(socket_num); \nS-9 #pragma omp parallel num_threads(n_procs) proc_bind(close) \nS-10 { \nS-11 printf(\"Reporting in from socket num, thread num: %d %d\\n\", \nS-12 socket_num,omp_get_thread_num() ); \nS-13 } \nCHAPTER 4. OPENMP AFFINITY 79 \nS-14 } \nS-15 \nS-16 int main() \nS-17 { \nS-18 int n_sockets, socket_num; \nS-19 \nS-20 omp_set_nested(1); // or export OMP_NESTED=true \nS-21 omp_set_max_active_levels(2); // or export OMP_MAX_ACTIVE_LEVELS=2 \nS-22 \nS-23 n_sockets = omp_get_num_places(); \nS-24 #pragma omp parallel num_threads(n_sockets) private(socket_num) \\ \nS-25 proc_bind(spread) \nS-26 { \nS-27 socket_num = omp_get_place_num(); \nS-28 socket_init(socket_num); \nS-29 } \nS-30 \nS-31 return 0; \nS-32 } \nC / C++ \nFortran \n1 Example affinity_query.1.f90 (omp_4.5) \nS-1 subroutine socket_init(socket_num) \nS-2 use omp_lib \nS-3 integer :: socket_num, n_procs \nS-4 \nS-5 n_procs = omp_get_place_num_procs(socket_num) \nS-6 !$omp parallel num_threads(n_procs) proc_bind(close) \nS-7 \nS-8 print*,\"Reporting in from socket num, thread num: \", & \nS-9 socket_num,omp_get_thread_num() \nS-10 !$omp end parallel \nS-11 end subroutine \nS-12 \nS-13 program numa_teams \nS-14 use omp_lib \nS-15 integer :: n_sockets, socket_num \nS-16 \nS-17 call omp_set_nested(.true.) ! or export OMP_NESTED=true \nS-18 call omp_set_max_active_levels(2) ! or export OMP_MAX_ACTIVE_LEVELS=2 \nS-19 \nS-20 n_sockets = omp_get_num_places() \nS-21 !$omp parallel num_threads(n_sockets) private(socket_num) & \nS-22 !$omp& proc_bind(spread) \nS-23 \n80 OpenMP Examples Version 5.2.1 - November 2022 \nS-24 socket_num = omp_get_place_num() \nS-25 call socket_init(socket_num) \nS-26 \nS-27 !$omp end parallel \nS-28 end program \nFortran \nCHAPTER 4. OPENMP AFFINITY 81 \nThis page intentionally left blank \n"}
{"section_title": "5 Tasking", "chunk": ""}
{"section_title": "5 Tasking", "chunk": "2 Tasking constructs provide units of work to a thread for execution. Worksharing constructs do this, \n3 too (e.g. for, do, sections, and singles constructs); but the work units are tightly controlled \n4 by an iteration limit and limited scheduling, or a limited number of sections or single \n5 regions. Worksharing was designed with \u201cdata parallel\u201d computing in mind. Tasking was designed \n6 for \u201ctask parallel\u201d computing and often involves non-locality or irregularity in memory access. \n7 The task construct can be used to execute work chunks: in a while loop; while traversing nodes in \n8 a list; at nodes in a tree graph; or in a normal loop (with a taskloop construct). Unlike the \n9 statically scheduled loop iterations of worksharing, a task is often enqueued, and then dequeued for \n10 execution by any of the threads of the team within a parallel region. The generation of tasks can be \n11 from a single generating thread (creating sibling tasks), or from multiple generators in a recursive \n12 graph tree traversals. A taskloop construct bundles iterations of an associated loop into tasks, \n13 and provides similar controls found in the task construct. \n14 Sibling tasks are synchronized by the taskwait construct, and tasks and their descendent tasks \n15 can be synchronized by containing them in a taskgroup region. Ordered execution is \n16 accomplished by specifying dependences with a depend clause. Also, priorities can be specified \n17 as hints to the scheduler through a priority clause. \n18 Various clauses can be used to manage and optimize task generation, as well as reduce the overhead \n19 of execution and to relinquish control of threads for work balance and forward progress. \n20 Once a thread starts executing a task, it is the designated thread for executing the task to \n21 completion, even though it may leave the execution at a scheduling point and return later. The \n22 thread is tied to the task. Scheduling points can be introduced with the taskyield construct. \n23 With an untied clause any other thread is allowed to continue the task. An if clause with an \n24 expression that evaluates to false results in an undeferred task, which instructs the runtime to \n25 suspend the generating task until the undeferred task completes its execution. By including the data \n26 environment of the generating task into the generated task with the mergeable and final \n27 clauses, task generation overhead can be reduced. \n28 A complete list of the tasking constructs and details of their clauses can be found in the Tasking \n29 Constructs chapter of the OpenMP Specifications, in the OpenMP Application Programming \n30 Interface section. \n83 \n"}
{"section_title": "5.1 task and taskwait Constructs", "chunk": ""}
{"section_title": "5.1 task and taskwait Constructs", "chunk": "2 The following example shows how to traverse a tree-like structure using explicit tasks. Note that the \n3 traverse function should be called from within a parallel region for the different specified tasks \n4 to be executed in parallel. Also note that the tasks will be executed in no specified order because \n5 there are no synchronization directives. Thus, assuming that the traversal will be done in post order, \n6 as in the sequential code, is wrong. \nC / C++ \n7 Example tasking.1.c (omp_3.0) \nS-1 struct node { \nS-2 struct node *left; \nS-3 struct node *right; \nS-4 }; \nS-5 \nS-6 extern void process(struct node *); \nS-7 \nS-8 void traverse( struct node *p ) \nS-9 { \nS-10 if (p->left) \nS-11 #pragma omp task // p is firstprivate by default \nS-12 traverse(p->left); \nS-13 if (p->right) \nS-14 #pragma omp task // p is firstprivate by default \nS-15 traverse(p->right); \nS-16 process(p); \nS-17 } \nC / C++ \nFortran \n8 Example tasking.1.f90 (omp_3.0) \nS-1 RECURSIVE SUBROUTINE traverse ( P ) \nS-2 TYPE Node \nS-3 TYPE(Node), POINTER :: left, right \nS-4 END TYPE Node \nS-5 TYPE(Node) :: P \nS-6 \nS-7 IF (associated(P%left)) THEN \nS-8 !$OMP TASK ! P is firstprivate by default \nS-9 CALL traverse(P%left) \nS-10 !$OMP END TASK \nS-11 ENDIF \nS-12 IF (associated(P%right)) THEN \nS-13 !$OMP TASK ! P is firstprivate by default \nS-14 CALL traverse(P%right) \n84 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 !$OMP END TASK \nS-16 ENDIF \nS-17 CALL process ( P ) \nS-18 \nS-19 END SUBROUTINE \nFortran \n1 In the next example, we force a postorder traversal of the tree by adding a taskwait directive. \n2 Now, we can safely assume that the left and right sons have been executed before we process the \n3 current node. \nC / C++ \n4 Example tasking.2.c (omp_3.0) \nS-1 struct node { \nS-2 struct node *left; \nS-3 struct node *right; \nS-4 }; \nS-5 extern void process(struct node *); \nS-6 void postorder_traverse( struct node *p ) { \nS-7 if (p->left) \nS-8 #pragma omp task // p is firstprivate by default \nS-9 postorder_traverse(p->left); \nS-10 if (p->right) \nS-11 #pragma omp task // p is firstprivate by default \nS-12 postorder_traverse(p->right); \nS-13 #pragma omp taskwait \nS-14 process(p); \nS-15 } \nC / C++ \nCHAPTER 5. TASKING 85 \nFortran \n1 Example tasking.2.f90 (omp_3.0) \nS-1 RECURSIVE SUBROUTINE traverse ( P ) \nS-2 TYPE Node \nS-3 TYPE(Node), POINTER :: left, right \nS-4 END TYPE Node \nS-5 TYPE(Node) :: P \nS-6 IF (associated(P%left)) THEN \nS-7 !$OMP TASK ! P is firstprivate by default \nS-8 CALL traverse(P%left) \nS-9 !$OMP END TASK \nS-10 ENDIF \nS-11 IF (associated(P%right)) THEN \nS-12 !$OMP TASK ! P is firstprivate by default \nS-13 CALL traverse(P%right) \nS-14 !$OMP END TASK \nS-15 ENDIF \nS-16 !$OMP TASKWAIT \nS-17 CALL process ( P ) \nS-18 END SUBROUTINE \nFortran \n2 The following example demonstrates how to use the task construct to process elements of a linked \n3 list in parallel. The thread executing the single region generates all of the explicit tasks, which \n4 are then executed by the threads in the current team. The pointer p is firstprivate by default \n5 on the task construct so it is not necessary to specify it in a firstprivate clause. \nC / C++ \n6 Example tasking.3.c (omp_3.0) \nS-1 typedef struct node node; \nS-2 struct node { \nS-3 int data; \nS-4 node * next; \nS-5 }; \nS-6 \nS-7 void process(node * p) \nS-8 { \nS-9 /* do work here */ \nS-10 } \nS-11 \nS-12 void increment_list_items(node * head) \nS-13 { \nS-14 #pragma omp parallel \nS-15 { \nS-16 #pragma omp single \n86 OpenMP Examples Version 5.2.1 - November 2022 \nS-17 { \nS-18 node * p = head; \nS-19 while (p) { \nS-20 #pragma omp task \nS-21 // p is firstprivate by default \nS-22 process(p); \nS-23 p = p->next; \nS-24 } \nS-25 } \nS-26 } \nS-27 } \nC / C++ \nFortran \n1 Example tasking.3.f90 (omp_3.0) \nS-1 MODULE LIST \nS-2 TYPE NODE \nS-3 INTEGER :: PAYLOAD \nS-4 TYPE (NODE), POINTER :: NEXT \nS-5 END TYPE NODE \nS-6 CONTAINS \nS-7 \nS-8 SUBROUTINE PROCESS(p) \nS-9 TYPE (NODE), POINTER :: P \nS-10 ! do work here \nS-11 END SUBROUTINE \nS-12 \nS-13 SUBROUTINE INCREMENT_LIST_ITEMS (HEAD) \nS-14 \nS-15 TYPE (NODE), POINTER :: HEAD \nS-16 TYPE (NODE), POINTER :: P \nS-17 !$OMP PARALLEL PRIVATE(P) \nS-18 !$OMP SINGLE \nS-19 P => HEAD \nS-20 DO \nS-21 !$OMP TASK \nS-22 ! P is firstprivate by default \nS-23 CALL PROCESS(P) \nS-24 !$OMP END TASK \nS-25 P => P%NEXT \nS-26 IF ( .NOT. ASSOCIATED (P) ) EXIT \nS-27 END DO \nS-28 !$OMP END SINGLE \nS-29 !$OMP END PARALLEL \nS-30 \nS-31 END SUBROUTINE \nCHAPTER 5. TASKING 87 \nS-32 \nS-33 END MODULE \nFortran \n1 The fib() function should be called from within a parallel region for the different specified \n2 tasks to be executed in parallel. Also, only one thread of the parallel region should call fib() \n3 unless multiple concurrent Fibonacci computations are desired. \nC / C++ \n4 Example tasking.4.c (omp_3.0) \nS-1 int fib(int n) { \nS-2 int i, j; \nS-3 if (n<2) \nS-4 return n; \nS-5 else { \nS-6 #pragma omp task shared(i) \nS-7 i=fib(n-1); \nS-8 #pragma omp task shared(j) \nS-9 j=fib(n-2); \nS-10 #pragma omp taskwait \nS-11 return i+j; \nS-12 } \nS-13 } \nC / C++ \nFortran \n5 Example tasking.4.f (omp_3.0) \nS-1 RECURSIVE INTEGER FUNCTION fib(n) RESULT(res) \nS-2 INTEGER n, i, j \nS-3 IF ( n .LT. 2) THEN \nS-4 res = n \nS-5 ELSE \nS-6 !$OMP TASK SHARED(i) \nS-7 i = fib( n-1 ) \nS-8 !$OMP END TASK \nS-9 !$OMP TASK SHARED(j) \nS-10 j = fib( n-2 ) \nS-11 !$OMP END TASK \nS-12 !$OMP TASKWAIT \nS-13 res = i+j \nS-14 END IF \nS-15 END FUNCTION \nFortran \n88 OpenMP Examples Version 5.2.1 - November 2022 \n1 Note: There are more efficient algorithms for computing Fibonacci numbers. This classic recursion \n2 algorithm is for illustrative purposes. \n3 The following example demonstrates a way to generate a large number of tasks with one thread and \n4 execute them with the threads in the team. While generating these tasks, the implementation may \n5 reach its limit on unassigned tasks. If it does, the implementation is allowed to cause the thread \n6 executing the task generating loop to suspend its task at the task scheduling point in the task \n7 directive, and start executing unassigned tasks. Once the number of unassigned tasks is sufficiently \n8 low, the thread may resume execution of the task generating loop. \nC / C++ \n9 Example tasking.5.c (omp_3.0) \nS-1 #define LARGE_NUMBER 10000000 \nS-2 double item[LARGE_NUMBER]; \nS-3 extern void process(double); \nS-4 \nS-5 int main() \nS-6 { \nS-7 #pragma omp parallel \nS-8 { \nS-9 #pragma omp single \nS-10 { \nS-11 int i; \nS-12 for (i=0; i<LARGE_NUMBER; i++) \nS-13 #pragma omp task // i is firstprivate, item is shared \nS-14 process(item[i]); \nS-15 } \nS-16 } \nS-17 } \nC / C++ \nFortran \n10 Example tasking.5.f (omp_3.0) \nS-1 real*8 item(10000000) \nS-2 integer i \nS-3 \nS-4 !$omp parallel \nS-5 !$omp single ! loop iteration variable i is private \nS-6 do i=1,10000000 \nS-7 !$omp task \nS-8 ! i is firstprivate, item is shared \nS-9 call process(item(i)) \nS-10 !$omp end task \nS-11 end do \nS-12 !$omp end single \nCHAPTER 5. TASKING 89 \nS-13 !$omp end parallel \nS-14 \nS-15 end \nFortran \n1 The following example is the same as the previous one, except that the tasks are generated in an \n2 untied task. While generating the tasks, the implementation may reach its limit on unassigned tasks. \n3 If it does, the implementation is allowed to cause the thread executing the task generating loop to \n4 suspend its task at the task scheduling point in the task directive, and start executing unassigned \n5 tasks. If that thread begins execution of a task that takes a long time to complete, the other threads \n6 may complete all the other tasks before it is finished. \n7 In this case, since the loop is in an untied task, any other thread is eligible to resume the task \n8 generating loop. In the previous examples, the other threads would be forced to idle until the \n9 generating thread finishes its long task, since the task generating loop was in a tied task. \nC / C++ \n10 Example tasking.6.c (omp_3.0) \nS-1 #define LARGE_NUMBER 10000000 \nS-2 double item[LARGE_NUMBER]; \nS-3 extern void process(double); \nS-4 int main() { \nS-5 #pragma omp parallel \nS-6 { \nS-7 #pragma omp single \nS-8 { \nS-9 int i; \nS-10 #pragma omp task untied \nS-11 // i is firstprivate, item is shared \nS-12 { \nS-13 for (i=0; i<LARGE_NUMBER; i++) \nS-14 #pragma omp task \nS-15 process(item[i]); \nS-16 } \nS-17 } \nS-18 } \nS-19 return 0; \nS-20 } \nC / C++ \n90 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example tasking.6.f (omp_3.0) \nS-1 real*8 item(10000000) \nS-2 !$omp parallel \nS-3 !$omp single \nS-4 !$omp task untied \nS-5 ! loop iteration variable i is private \nS-6 do i=1,10000000 \nS-7 !$omp task ! i is firstprivate, item is shared \nS-8 call process(item(i)) \nS-9 !$omp end task \nS-10 end do \nS-11 !$omp end task \nS-12 !$omp end single \nS-13 !$omp end parallel \nS-14 end \nFortran \n2 The following two examples demonstrate how the scheduling rules illustrated in Section 2.11.3 of \n3 the OpenMP 4.0 specification affect the usage of threadprivate variables in tasks. A \n4 threadprivate variable can be modified by another task that is executed by the same thread. \n5 Thus, the value of a threadprivate variable cannot be assumed to be unchanged across a task \n6 scheduling point. In untied tasks, task scheduling points may be added in any place by the \n7 implementation. \n8 A task switch may occur at a task scheduling point. A single thread may execute both of the task \n9 regions that modify tp. The parts of these task regions in which tp is modified may be executed in \n10 any order so the resulting value of var can be either 1 or 2. \nC / C++ \n11 Example tasking.7.c (omp_3.0) \nS-1 int tp; \nS-2 #pragma omp threadprivate(tp) \nS-3 int var; \nS-4 void work() \nS-5 { \nS-6 #pragma omp task \nS-7 { \nS-8 /* do work here */ \nS-9 #pragma omp task \nS-10 { \nS-11 tp = 1; \nS-12 /* do work here */ \nS-13 #pragma omp task \nS-14 { \nCHAPTER 5. TASKING 91 \nS-15 /* no modification of tp */ \nS-16 } \nS-17 var = tp; //value of tp can be 1 or 2 \nS-18 } \nS-19 tp = 2; \nS-20 } \nS-21 } \nC / C++ \nFortran \n1 Example tasking.7.f (omp_3.0) \nS-1 module example \nS-2 integer tp \nS-3 !$omp threadprivate(tp) \nS-4 integer var \nS-5 contains \nS-6 subroutine work \nS-7 !$omp task \nS-8 ! do work here \nS-9 !$omp task \nS-10 tp = 1 \nS-11 ! do work here \nS-12 !$omp task \nS-13 ! no modification of tp \nS-14 !$omp end task \nS-15 var = tp ! value of var can be 1 or 2 \nS-16 !$omp end task \nS-17 tp = 2 \nS-18 !$omp end task \nS-19 end subroutine \nS-20 end module \nFortran \n2 In this example, scheduling constraints prohibit a thread in the team from executing a new task that \n3 modifies tp while another such task region tied to the same thread is suspended. Therefore, the \n4 value written will persist across the task scheduling point. \n92 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example tasking.8.c (omp_3.0) \nS-1 int tp; \nS-2 #pragma omp threadprivate(tp) \nS-3 int var; \nS-4 void work() \nS-5 { \nS-6 #pragma omp parallel \nS-7 { \nS-8 /* do work here */ \nS-9 #pragma omp task \nS-10 { \nS-11 tp++; \nS-12 /* do work here */ \nS-13 #pragma omp task \nS-14 { \nS-15 /* do work here but don\u2019t modify tp */ \nS-16 } \nS-17 var = tp; //Value does not change after write above \nS-18 } \nS-19 } \nS-20 } \nC / C++ \nFortran \n2 Example tasking.8.f (omp_3.0) \nS-1 module example \nS-2 integer tp \nS-3 !$omp threadprivate(tp) \nS-4 integer var \nS-5 end module \nS-6 \nS-7 subroutine work \nS-8 use example \nS-9 !$omp parallel \nS-10 ! do work here \nS-11 !$omp task \nS-12 tp = tp + 1 \nS-13 ! do work here \nS-14 !$omp task \nS-15 ! do work here but don\u2019t modify tp \nS-16 !$omp end task \nS-17 var = tp ! value does not change after write above \nS-18 !$omp end task \nCHAPTER 5. TASKING 93 \nS-19 !$omp end parallel \nS-20 end subroutine \nFortran \n1 The following two examples demonstrate how the scheduling rules illustrated in Section 2.11.3 of \n2 the OpenMP 4.0 specification affect the usage of locks and critical sections in tasks. If a lock is \n3 held across a task scheduling point, no attempt should be made to acquire the same lock in any code \n4 that may be interleaved. Otherwise, a deadlock is possible. \n5 In the example below, suppose the thread executing task 1 defers task 2. When it encounters the \n6 task scheduling point at task 3, it could suspend task 1 and begin task 2 which will result in a \n7 deadlock when it tries to enter critical region 1. \nC / C++ \n8 Example tasking.9.c (omp_3.0) \nS-1 void work() \nS-2 { \nS-3 #pragma omp task \nS-4 { //Task 1 \nS-5 #pragma omp task \nS-6 { //Task 2 \nS-7 #pragma omp critical //Critical region 1 \nS-8 {/*do work here */ } \nS-9 } \nS-10 #pragma omp critical //Critical Region 2 \nS-11 { \nS-12 //Capture data for the following task \nS-13 #pragma omp task \nS-14 { /* do work here */ } //Task 3 \nS-15 } \nS-16 } \nS-17 } \nC / C++ \n94 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example tasking.9.f (omp_3.0) \nS-1 module example \nS-2 contains \nS-3 subroutine work \nS-4 !$omp task \nS-5 ! Task 1 \nS-6 !$omp task \nS-7 ! Task 2 \nS-8 !$omp critical \nS-9 ! Critical region 1 \nS-10 ! do work here \nS-11 !$omp end critical \nS-12 !$omp end task \nS-13 !$omp critical \nS-14 ! Critical region 2 \nS-15 ! Capture data for the following task \nS-16 !$omp task \nS-17 !Task 3 \nS-18 ! do work here \nS-19 !$omp end task \nS-20 !$omp end critical \nS-21 !$omp end task \nS-22 end subroutine \nS-23 end module \nFortran \n2 In the following example, lock is held across a task scheduling point. However, according to the \n3 scheduling restrictions, the executing thread can\u2019t begin executing one of the non-descendant tasks \n4 that also acquires lock before the task region is complete. Therefore, no deadlock is possible. \nC / C++ \n5 Example tasking.10.c (omp_3.0) \nS-1 #include <omp.h> \nS-2 void work() { \nS-3 omp_lock_t lock; \nS-4 omp_init_lock(&lock); \nS-5 #pragma omp parallel \nS-6 { \nS-7 int i; \nS-8 #pragma omp for \nS-9 for (i = 0; i < 100; i++) { \nS-10 #pragma omp task \nS-11 { \nS-12 // lock is shared by default in the task \nCHAPTER 5. TASKING 95 \nS-13 omp_set_lock(&lock); \nS-14 // Capture data for the following task \nS-15 #pragma omp task \nS-16 // Task Scheduling Point 1 \nS-17 { /* do work here */ } \nS-18 omp_unset_lock(&lock); \nS-19 } \nS-20 } \nS-21 } \nS-22 omp_destroy_lock(&lock); \nS-23 } \nC / C++ \nFortran \n1 Example tasking.10.f90 (omp_3.0) \nS-1 module example \nS-2 include \u2019omp_lib.h\u2019 \nS-3 integer (kind=omp_lock_kind) lock \nS-4 integer i \nS-5 \nS-6 contains \nS-7 \nS-8 subroutine work \nS-9 call omp_init_lock(lock) \nS-10 !$omp parallel \nS-11 !$omp do \nS-12 do i=1,100 \nS-13 !$omp task \nS-14 ! Outer task \nS-15 call omp_set_lock(lock) ! lock is shared by \nS-16 ! default in the task \nS-17 ! Capture data for the following task \nS-18 !$omp task ! Task Scheduling Point 1 \nS-19 ! do work here \nS-20 !$omp end task \nS-21 call omp_unset_lock(lock) \nS-22 !$omp end task \nS-23 end do \nS-24 !$omp end parallel \nS-25 call omp_destroy_lock(lock) \nS-26 end subroutine \nS-27 \nS-28 end module \nFortran \n96 OpenMP Examples Version 5.2.1 - November 2022 \n1 The following examples illustrate the use of the mergeable clause in the task construct. In this \n2 first example, the task construct has been annotated with the mergeable clause. The addition \n3 of this clause allows the implementation to reuse the data environment (including the ICVs) of the \n4 parent task for the task inside foo if the task is included or undeferred. Thus, the result of the \n5 execution may differ depending on whether the task is merged or not. Therefore the mergeable \n6 clause needs to be used with caution. In this example, the use of the mergeable clause is safe. As x \n7 is a shared variable the outcome does not depend on whether or not the task is merged (that is, the \n8 task will always increment the same variable and will always compute the same value for x). \nC / C++ \n9 Example tasking.11.c (omp_3.1) \nS-1 #include <stdio.h> \nS-2 void foo ( ) \nS-3 { \nS-4 int x = 2; \nS-5 #pragma omp task shared(x) mergeable \nS-6 { \nS-7 x++; \nS-8 } \nS-9 #pragma omp taskwait \nS-10 printf(\"%d\\n\",x); // prints 3 \nS-11 } \nC / C++ \nFortran \n10 Example tasking.11.f90 (omp_3.1) \nS-1 subroutine foo() \nS-2 integer :: x \nS-3 x = 2 \nS-4 !$omp task shared(x) mergeable \nS-5 x = x + 1 \nS-6 !$omp end task \nS-7 !$omp taskwait \nS-8 print *, x ! prints 3 \nS-9 end subroutine \nFortran \n11 This second example shows an incorrect use of the mergeable clause. In this example, the \n12 created task will access different instances of the variable x if the task is not merged, as x is \n13 firstprivate, but it will access the same variable x if the task is merged. As a result, the \n14 behavior of the program is unspecified, and it can print two different values for x depending on the \n15 decisions taken by the implementation. \nCHAPTER 5. TASKING 97 \nC / C++ \n1 Example tasking.12.c (omp_3.1) \nS-1 #include <stdio.h> \nS-2 void foo ( ) \nS-3 { \nS-4 int x = 2; \nS-5 #pragma omp task mergeable \nS-6 { \nS-7 x++; \nS-8 } \nS-9 #pragma omp taskwait \nS-10 printf(\"%d\\n\",x); // prints 2 or 3 \nS-11 } \nC / C++ \nFortran \n2 Example tasking.12.f90 (omp_3.1) \nS-1 subroutine foo() \nS-2 integer :: x \nS-3 x = 2 \nS-4 !$omp task mergeable \nS-5 x = x + 1 \nS-6 !$omp end task \nS-7 !$omp taskwait \nS-8 print *, x ! prints 2 or 3 \nS-9 end subroutine \nFortran \n3 The following example shows the use of the final clause and the omp_in_final API call in a \n4 recursive binary search program. To reduce overhead, once a certain depth of recursion is reached \n5 the program uses the final clause to create only included tasks, which allow additional \n6 optimizations. \n7 The use of the omp_in_final API call allows programmers to optimize their code by specifying \n8 which parts of the program are not necessary when a task can create only included tasks (that is, the \n9 code is inside a final task). In this example, the use of a different state variable is not necessary \n10 so once the program reaches the part of the computation that is finalized and copying from the \n11 parent state to the new state is eliminated. The allocation of new_state in the stack could also be \n12 avoided but it would make this example less clear. The final clause is most effective when used \n13 in conjunction with the mergeable clause since all tasks created in a final task region are \n14 included tasks that can be merged if the mergeable clause is present. \n98 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example tasking.13.c (omp_3.1) \nS-1 #include <string.h> \nS-2 #include <omp.h> \nS-3 #define LIMIT 3 /* arbitrary limit on recursion depth */ \nS-4 void check_solution(char *); \nS-5 void bin_search (int pos, int n, char *state) \nS-6 { \nS-7 if ( pos == n ) { \nS-8 check_solution(state); \nS-9 return; \nS-10 } \nS-11 #pragma omp task final( pos > LIMIT ) mergeable \nS-12 { \nS-13 char new_state[n]; \nS-14 if (!omp_in_final() ) { \nS-15 memcpy(new_state, state, pos ); \nS-16 state = new_state; \nS-17 } \nS-18 state[pos] = 0; \nS-19 bin_search(pos+1, n, state ); \nS-20 } \nS-21 #pragma omp task final( pos > LIMIT ) mergeable \nS-22 { \nS-23 char new_state[n]; \nS-24 if (! omp_in_final() ) { \nS-25 memcpy(new_state, state, pos ); \nS-26 state = new_state; \nS-27 } \nS-28 state[pos] = 1; \nS-29 bin_search(pos+1, n, state ); \nS-30 } \nS-31 #pragma omp taskwait \nS-32 } \nC / C++ \nCHAPTER 5. TASKING 99 \nFortran \n1 Example tasking.13.f90 (omp_3.1) \nS-1 recursive subroutine bin_search(pos, n, state) \nS-2 use omp_lib \nS-3 integer :: pos, n \nS-4 character, pointer :: state(:) \nS-5 character, target, dimension(n) :: new_state1, new_state2 \nS-6 integer, parameter :: LIMIT = 3 \nS-7 if (pos .eq. n) then \nS-8 call check_solution(state) \nS-9 return \nS-10 endif \nS-11 !$omp task final(pos > LIMIT) mergeable \nS-12 if (.not. omp_in_final()) then \nS-13 new_state1(1:pos) = state(1:pos) \nS-14 state => new_state1 \nS-15 endif \nS-16 state(pos+1) = \u2019z\u2019 \nS-17 call bin_search(pos+1, n, state) \nS-18 !$omp end task \nS-19 !$omp task final(pos > LIMIT) mergeable \nS-20 if (.not. omp_in_final()) then \nS-21 new_state2(1:pos) = state(1:pos) \nS-22 state => new_state2 \nS-23 endif \nS-24 state(pos+1) = \u2019y\u2019 \nS-25 call bin_search(pos+1, n, state) \nS-26 !$omp end task \nS-27 !$omp taskwait \nS-28 end subroutine \nFortran \n2 The following example illustrates the difference between the if and the final clauses. The if \n3 clause has a local effect. In the first nest of tasks, the one that has the if clause will be undeferred \n4 but the task nested inside that task will not be affected by the if clause and will be created as usual. \n5 Alternatively, the final clause affects all task constructs in the final task region but not the \n6 final task itself. In the second nest of tasks, the nested tasks will be created as included tasks. \n7 Note also that the conditions for the if and final clauses are usually the opposite. \n100 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example tasking.14.c (omp_3.1) \nS-1 void bar(void); \nS-2 \nS-3 void foo ( ) \nS-4 { \nS-5 int i; \nS-6 #pragma omp task if(0) // This task is undeferred \nS-7 { \nS-8 #pragma omp task // This task is a regular task \nS-9 for (i = 0; i < 3; i++) { \nS-10 #pragma omp task // This task is a regular task \nS-11 bar(); \nS-12 } \nS-13 } \nS-14 #pragma omp task final(1) // This task is a regular task \nS-15 { \nS-16 #pragma omp task // This task is included \nS-17 for (i = 0; i < 3; i++) { \nS-18 #pragma omp task // This task is also included \nS-19 bar(); \nS-20 } \nS-21 } \nS-22 } \nC / C++ \nFortran \n2 Example tasking.14.f90 (omp_3.1) \nS-1 subroutine foo() \nS-2 integer i \nS-3 !$omp task if(.FALSE.) ! This task is undeferred \nS-4 !$omp task ! This task is a regular task \nS-5 do i = 1, 3 \nS-6 !$omp task ! This task is a regular task \nS-7 call bar() \nS-8 !$omp end task \nS-9 enddo \nS-10 !$omp end task \nS-11 !$omp end task \nS-12 !$omp task final(.TRUE.) ! This task is a regular task \nS-13 !$omp task ! This task is included \nS-14 do i = 1, 3 \nS-15 !$omp task ! This task is also included \nS-16 call bar() \nS-17 !$omp end task \nCHAPTER 5. TASKING 101 \nS-18 enddo \nS-19 !$omp end task \nS-20 !$omp end task \nS-21 end subroutine \nFortran \n102 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "5.2 Task Priority", "chunk": ""}
{"section_title": "5.2 Task Priority", "chunk": "2 In this example we compute arrays in a matrix through a compute_array routine. Each task has a \n3 priority value equal to the value of the loop variable i at the moment of its creation. A higher \n4 priority on a task means that a task is a candidate to run sooner. \n5 The creation of tasks occurs in ascending order (according to the iteration space of the loop) but a \n6 hint, by means of the priority clause, is provided to reverse the execution order. \nC / C++ \n7 Example task_priority.1.c (omp_4.5) \nS-1 void compute_array (float *node, int M); \nS-2 \nS-3 void compute_matrix (float *array, int N, int M) \nS-4 { \nS-5 int i; \nS-6 #pragma omp parallel private(i) \nS-7 #pragma omp single \nS-8 { \nS-9 for (i=0;i<N; i++) { \nS-10 #pragma omp task priority(i) \nS-11 compute_array(&array[i*M], M); \nS-12 } \nS-13 } \nS-14 } \nC / C++ \nFortran \n8 Example task_priority.1.f90 (omp_4.5) \nS-1 subroutine compute_matrix(matrix, M, N) \nS-2 implicit none \nS-3 integer :: M, N \nS-4 real :: matrix(M, N) \nS-5 integer :: i \nS-6 interface \nS-7 subroutine compute_array(node, M) \nS-8 implicit none \nS-9 integer :: M \nS-10 real :: node(M) \nS-11 end subroutine \nS-12 end interface \nS-13 !$omp parallel private(i) \nS-14 !$omp single \nS-15 do i=1,N \nS-16 !$omp task priority(i) \nCHAPTER 5. TASKING 103 \nS-17 call compute_array(matrix(:, i), M) \nS-18 !$omp end task \nS-19 enddo \nS-20 !$omp end single \nS-21 !$omp end parallel \nS-22 end subroutine compute_matrix \nFortran \n104 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "5.3 Task Dependences", "chunk": ""}
{"section_title": "5.3.1 Flow Dependence", "chunk": ""}
{"section_title": "5.3.1 Flow Dependence", "chunk": "3 This example shows a simple flow dependence using a depend clause on the task construct. \nC / C++ \n4 Example task_dep.1.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 int main() { \nS-3 int x = 1; \nS-4 #pragma omp parallel \nS-5 #pragma omp single \nS-6 { \nS-7 #pragma omp task shared(x) depend(out: x) \nS-8 x = 2; \nS-9 #pragma omp task shared(x) depend(in: x) \nS-10 printf(\"x = %d\\n\", x); \nS-11 } \nS-12 return 0; \nS-13 } \nC / C++ \nFortran \n5 Example task_dep.1.f90 (omp_4.0) \nS-1 program example \nS-2 integer :: x \nS-3 x = 1 \nS-4 !$omp parallel \nS-5 !$omp single \nS-6 !$omp task shared(x) depend(out: x) \nS-7 x = 2 \nS-8 !$omp end task \nS-9 !$omp task shared(x) depend(in: x) \nS-10 print*, \"x = \", x \nS-11 !$omp end task \nS-12 !$omp end single \nS-13 !$omp end parallel \nS-14 end program \nFortran \n6 The program will always print \u201cx = 2\u201d, because the depend clauses enforce the ordering of the \n7 tasks. If the depend clauses had been omitted, then the tasks could execute in any order and the \n8 program and the program would have a race condition. \nCHAPTER 5. TASKING 105 \n"}
{"section_title": "5.3.2 Anti-dependence", "chunk": ""}
{"section_title": "5.3.2 Anti-dependence", "chunk": "2 This example shows an anti-dependence using the depend clause on the task construct. \nC / C++ \n3 Example task_dep.2.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 int main() \nS-3 { \nS-4 int x = 1; \nS-5 #pragma omp parallel \nS-6 #pragma omp single \nS-7 { \nS-8 #pragma omp task shared(x) depend(in: x) \nS-9 printf(\"x = %d\\n\", x); \nS-10 #pragma omp task shared(x) depend(out: x) \nS-11 x = 2; \nS-12 } \nS-13 return 0; \nS-14 } \nC / C++ \nFortran \n4 Example task_dep.2.f90 (omp_4.0) \nS-1 program example \nS-2 integer :: x \nS-3 x = 1 \nS-4 !$omp parallel \nS-5 !$omp single \nS-6 !$omp task shared(x) depend(in: x) \nS-7 print*, \"x = \", x \nS-8 !$omp end task \nS-9 !$omp task shared(x) depend(out: x) \nS-10 x = 2 \nS-11 !$omp end task \nS-12 !$omp end single \nS-13 !$omp end parallel \nS-14 end program \nFortran \n5 The program will always print \u201cx = 1\u201d, because the depend clauses enforce the ordering of the \n6 tasks. If the depend clauses had been omitted, then the tasks could execute in any order and the \n7 program would have a race condition. \n106 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "5.3.3 Output Dependence", "chunk": ""}
{"section_title": "5.3.3 Output Dependence", "chunk": "2 This example shows an output dependence using the depend clause on the task construct. \nC / C++ \n3 Example task_dep.3.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 int main() { \nS-3 int x; \nS-4 #pragma omp parallel \nS-5 #pragma omp single \nS-6 { \nS-7 #pragma omp task shared(x) depend(out: x) \nS-8 x = 1; \nS-9 #pragma omp task shared(x) depend(out: x) \nS-10 x = 2; \nS-11 #pragma omp taskwait \nS-12 printf(\"x = %d\\n\", x); \nS-13 } \nS-14 return 0; \nS-15 } \nC / C++ \nFortran \n4 Example task_dep.3.f90 (omp_4.0) \nS-1 program example \nS-2 integer :: x \nS-3 !$omp parallel \nS-4 !$omp single \nS-5 !$omp task shared(x) depend(out: x) \nS-6 x = 1 \nS-7 !$omp end task \nS-8 !$omp task shared(x) depend(out: x) \nS-9 x = 2 \nS-10 !$omp end task \nS-11 !$omp taskwait \nS-12 print*, \"x = \", x \nS-13 !$omp end single \nS-14 !$omp end parallel \nS-15 end program \nFortran \n5 The program will always print \u201cx = 2\u201d, because the depend clauses enforce the ordering of the \n6 tasks. If the depend clauses had been omitted, then the tasks could execute in any order and the \n7 program would have a race condition. \nCHAPTER 5. TASKING 107 \n"}
{"section_title": "5.3.4 Concurrent Execution with Dependences", "chunk": ""}
{"section_title": "5.3.4 Concurrent Execution with Dependences", "chunk": "2 In this example we show potentially concurrent execution of tasks using multiple flow dependences \n3 expressed using the depend clause on the task construct. \n4 The last two tasks are dependent on the first task. However, there is no dependence between the last \n5 two tasks, which may execute in any order (or concurrently if more than one thread is available). \n6 Thus, the possible outputs are \u201cx + 1 = 3. x + 2 = 4.\u201d and \u201cx + 2 = 4. x + 1 = 3.\u201d. If the depend \n7 clauses had been omitted, then all of the tasks could execute in any order and the program would \n8 have a race condition. \nC / C++ \n9 Example task_dep.4.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 int main() { \nS-3 int x = 1; \nS-4 #pragma omp parallel \nS-5 #pragma omp single \nS-6 { \nS-7 #pragma omp task shared(x) depend(out: x) \nS-8 x = 2; \nS-9 #pragma omp task shared(x) depend(in: x) \nS-10 printf(\"x + 1 = %d. \", x+1); \nS-11 #pragma omp task shared(x) depend(in: x) \nS-12 printf(\"x + 2 = %d\\n\", x+2); \nS-13 } \nS-14 return 0; \nS-15 } \nC / C++ \nFortran \n10 Example task_dep.4.f90 (omp_4.0) \nS-1 program example \nS-2 integer :: x \nS-3 \nS-4 x = 1 \nS-5 \nS-6 !$omp parallel \nS-7 !$omp single \nS-8 \nS-9 !$omp task shared(x) depend(out: x) \nS-10 x = 2 \nS-11 !$omp end task \nS-12 \nS-13 !$omp task shared(x) depend(in: x) \nS-14 print*, \"x + 1 = \", x+1, \".\" \n108 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 !$omp end task \nS-16 \nS-17 !$omp task shared(x) depend(in: x) \nS-18 print*, \"x + 2 = \", x+2, \".\" \nS-19 !$omp end task \nS-20 \nS-21 !$omp end single \nS-22 !$omp end parallel \nS-23 end program \nFortran \n1 The following example illustrates the semantic difference between inout and inoutset \n2 dependence types. In CASE 1, tasks generated at T1 inside the loop have dependences among \n3 themselves due to the inout dependence type and with task T2. As a result, these tasks are \n4 executed sequentially before the print statement from task T2. In CASE 2, tasks generated at T3 \n5 inside the loop have no dependences among themselves from the inoutset dependence type, but \n6 have dependences with task T4. As a result, these tasks are executed concurrently before the print \n7 statement from task T4. \nC / C++ \n8 Example task_dep.4b.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 \nS-3 extern int f(int i); \nS-4 \nS-5 void task_dep(int N) \nS-6 { \nS-7 int i, v, R; \nS-8 \nS-9 #pragma omp parallel private(i,v) shared(R) \nS-10 #pragma omp single \nS-11 { \nS-12 // CASE 1: tasks with inout dependence type. \nS-13 // tasks are serialized here. \nS-14 R = 0; \nS-15 for ( i = 0; i < N; i++ ) { \nS-16 #pragma omp task depend(inout: R) // T1 \nS-17 { \nS-18 v = f(i); \nS-19 R += v; \nS-20 } \nS-21 } \nS-22 \nS-23 #pragma omp task depend(in: R) // T2 \nS-24 printf(\"result is %d\\n\", R); \nS-25 #pragma omp taskwait // to avoid race with CASE 2 \nCHAPTER 5. TASKING 109 \nS-26 \nS-27 // CASE 2: tasks with inoutset dependence type. \nS-28 // tasks are executed concurrently. \nS-29 R = 0; \nS-30 for ( i = 0; i < N; i++ ) { \nS-31 #pragma omp task depend(inoutset: R) // T3 \nS-32 { \nS-33 v = f(i); \nS-34 #pragma omp atomic \nS-35 R += v; \nS-36 } \nS-37 } \nS-38 \nS-39 #pragma omp task depend(in: R) // T4 \nS-40 printf(\"result is %d\\n\", R); \nS-41 } \nS-42 } \nC / C++ \nFortran \n1 Example task_dep.4b.f90 (omp_5.1) \nS-1 subroutine task_dep(N) \nS-2 implicit none \nS-3 integer :: N \nS-4 \nS-5 integer :: i, v, R \nS-6 integer, external :: f \nS-7 \nS-8 !$omp parallel private(i,v) shared(R) \nS-9 !$omp single \nS-10 !! CASE 1: tasks with inout dependence type. \nS-11 !! tasks are serialized here. \nS-12 R = 0 \nS-13 do i = 1, N \nS-14 !$omp task depend(inout: R) !! T1 \nS-15 v = f(i) \nS-16 R = R + v \nS-17 !$omp end task \nS-18 end do \nS-19 \nS-20 !$omp task depend(in: R) !! T2 \nS-21 print *, \"result is \", R \nS-22 !$omp end task \nS-23 !$omp taskwait !! to avoid race with CASE 2 \nS-24 \nS-25 !! CASE 2: tasks with inoutset dependence type. \n110 OpenMP Examples Version 5.2.1 - November 2022 \nS-26 !! tasks are executed concurrently. \nS-27 R = 0 \nS-28 do i = 1, N \nS-29 !$omp task depend(inoutset: R) !! T3 \nS-30 v = f(i) \nS-31 !$omp atomic \nS-32 R = R + v \nS-33 !$omp end task \nS-34 end do \nS-35 \nS-36 !$omp task depend(in: R) !! T4 \nS-37 print *, \"result is \", R \nS-38 !$omp end task \nS-39 \nS-40 !$omp end single \nS-41 !$omp end parallel \nS-42 end subroutine \nFortran \n"}
{"section_title": "5.3.5 Matrix multiplication", "chunk": ""}
{"section_title": "5.3.5 Matrix multiplication", "chunk": "2 This example shows a task-based blocked matrix multiplication. Matrices are of NxN elements, and \n3 the multiplication is implemented using blocks of BSxBS elements. \nC / C++ \n4 Example task_dep.5.c (omp_4.0) \nS-1 #define N 100 \nS-2 // Assume BS divides N perfectly \nS-3 void matmul_depend(int BS, float A[N][N], float B[N][N], \nS-4 float C[N][N]) \nS-5 { \nS-6 int i, j, k, ii, jj, kk; \nS-7 for (i = 0; i < N; i+=BS) { \nS-8 for (j = 0; j < N; j+=BS) { \nS-9 for (k = 0; k < N; k+=BS) { \nS-10 // Note 1: i, j, k, A, B, C are firstprivate by default \nS-11 // Note 2: A, B and C are just pointers \nS-12 #pragma omp task private(ii, jj, kk) \\ \nS-13 depend ( in: A[i:BS][k:BS], B[k:BS][j:BS] ) \\ \nS-14 depend ( inout: C[i:BS][j:BS] ) \nS-15 for (ii = i; ii < i+BS; ii++ ) \nS-16 for (jj = j; jj < j+BS; jj++ ) \nS-17 for (kk = k; kk < k+BS; kk++ ) \nS-18 C[ii][jj] = C[ii][jj] + A[ii][kk] * B[kk][jj]; \nCHAPTER 5. TASKING 111 \nS-19 } \nS-20 } \nS-21 } \nS-22 } \nC / C++ \nFortran \n1 Example task_dep.5.f90 (omp_4.0) \nS-1 ! Assume BS divides N perfectly \nS-2 subroutine matmul_depend (N, BS, A, B, C) \nS-3 implicit none \nS-4 integer :: N, BS, BM \nS-5 real, dimension(N, N) :: A, B, C \nS-6 integer :: i, j, k, ii, jj, kk \nS-7 BM = BS - 1 \nS-8 do i = 1, N, BS \nS-9 do j = 1, N, BS \nS-10 do k = 1, N, BS \nS-11 !$omp task shared(A,B,C) private(ii,jj,kk) & \nS-12 !$omp depend ( in: A(i:i+BM, k:k+BM), B(k:k+BM, j:j+BM) ) & \nS-13 !$omp depend ( inout: C(i:i+BM, j:j+BM) ) \nS-14 ! I,J,K are firstprivate by default \nS-15 do ii = i, i+BM \nS-16 do jj = j, j+BM \nS-17 do kk = k, k+BM \nS-18 C(jj,ii) = C(jj,ii) + A(kk,ii) * B(jj,kk) \nS-19 end do \nS-20 end do \nS-21 end do \nS-22 !$omp end task \nS-23 end do \nS-24 end do \nS-25 end do \nS-26 end subroutine \nFortran \n112 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "5.3.6 taskwait with Dependences", "chunk": ""}
{"section_title": "5.3.6 taskwait with Dependences", "chunk": "2 In this subsection three examples illustrate how the depend clause can be applied to a taskwait \n3 construct to make the generating task wait for specific child tasks to complete. This is an OpenMP \n4 5.0 feature. In the same manner that dependences can order executions among child tasks with \n5 depend clauses on task constructs, the generating task can be scheduled to wait on child tasks at \n6 a taskwait before it can proceed. \n7 Note: Since the depend clause on a taskwait construct relaxes the default synchronization \n8 behavior (waiting for all children to finish), it is important to realize that child tasks that are not \n9 predecessor tasks, as determined by the depend clause of the taskwait construct, may be \n10 running concurrently while the generating task is executing after the taskwait. \n11 In the first example the generating task waits at the taskwait construct for the completion of the \n12 first child task because a dependence on the first task is produced by x with an in dependence type \n13 within the depend clause of the taskwait construct. Immediately after the first taskwait \n14 construct it is safe to access the x variable by the generating task, as shown in the print statement. \n15 There is no completion restraint on the second child task. Hence, immediately after the first \n16 taskwait it is unsafe to access the y variable since the second child task may still be executing. \n17 The second taskwait ensures that the second child task has completed; hence it is safe to access \n18 the y variable in the following print statement. \nC / C++ \n19 Example task_dep.6.c (omp_5.0) \nS-1 #include<stdio.h> \nS-2 \nS-3 void foo() \nS-4 { \nS-5 int x = 0, y = 2; \nS-6 \nS-7 #pragma omp task depend(inout: x) shared(x) \nS-8 x++; // 1st child task \nS-9 \nS-10 #pragma omp task shared(y) \nS-11 y--; // 2nd child task \nS-12 \nS-13 #pragma omp taskwait depend(in: x) // 1st taskwait \nS-14 \nS-15 printf(\"x=%d\\n\",x); \nS-16 \nS-17 // Second task may not be finished. \nS-18 // Accessing y here will create a race condition. \nS-19 \nS-20 #pragma omp taskwait // 2nd taskwait \nS-21 \nS-22 printf(\"y=%d\\n\",y); \nCHAPTER 5. TASKING 113 \nS-23 } \nS-24 \nS-25 int main() \nS-26 { \nS-27 #pragma omp parallel \nS-28 #pragma omp single \nS-29 foo(); \nS-30 \nS-31 return 0; \nS-32 } \nC / C++ \nFortran \n1 Example task_dep.6.f90 (omp_5.0) \nS-1 subroutine foo() \nS-2 implicit none \nS-3 integer :: x, y \nS-4 \nS-5 x = 0 \nS-6 y = 2 \nS-7 \nS-8 !$omp task depend(inout: x) shared(x) \nS-9 x = x + 1 !! 1st child task \nS-10 !$omp end task \nS-11 \nS-12 !$omp task shared(y) \nS-13 y = y - 1 !! 2nd child task \nS-14 !$omp end task \nS-15 \nS-16 !$omp taskwait depend(in: x) !! 1st taskwait \nS-17 \nS-18 print*, \"x=\", x \nS-19 \nS-20 !! Second task may not be finished. \nS-21 !! Accessing y here will create a race condition. \nS-22 \nS-23 !$omp taskwait !! 2nd taskwait \nS-24 \nS-25 print*, \"y=\", y \nS-26 \nS-27 end subroutine foo \nS-28 \nS-29 program p \nS-30 implicit none \nS-31 !$omp parallel \nS-32 !$omp single \n114 OpenMP Examples Version 5.2.1 - November 2022 \nS-33 call foo() \nS-34 !$omp end single \nS-35 !$omp end parallel \nS-36 end program p \nFortran \n1 In this example the first two tasks are serialized, because a dependence on the first child is produced \n2 by x with the in dependence type in the depend clause of the second task. However, the \n3 generating task at the first taskwait waits only on the first child task to complete, because a \n4 dependence on only the first child task is produced by x with an in dependence type within the \n5 depend clause of the taskwait construct. The second taskwait (without a depend clause) \n6 is included to guarantee completion of the second task before y is accessed. (While unnecessary, \n7 the depend(inout: y) clause on the 2nd child task is included to illustrate how the child task \n8 dependences can be completely annotated in a data-flow model.) \nC / C++ \n9 Example task_dep.7.c (omp_5.0) \nS-1 #include<stdio.h> \nS-2 \nS-3 void foo() \nS-4 { \nS-5 int x = 0, y = 2; \nS-6 \nS-7 #pragma omp task depend(inout: x) shared(x) \nS-8 x++; // 1st child task \nS-9 \nS-10 #pragma omp task depend(in: x) depend(inout: y) shared(x, y) \nS-11 y -= x; // 2nd child task \nS-12 \nS-13 #pragma omp taskwait depend(in: x) // 1st taskwait \nS-14 \nS-15 printf(\"x=%d\\n\",x); \nS-16 \nS-17 // Second task may not be finished. \nS-18 // Accessing y here would create a race condition. \nS-19 \nS-20 #pragma omp taskwait // 2nd taskwait \nS-21 \nS-22 printf(\"y=%d\\n\",y); \nS-23 \nS-24 } \nS-25 \nS-26 int main() \nS-27 { \nS-28 #pragma omp parallel \nS-29 #pragma omp single \nCHAPTER 5. TASKING 115 \nS-30 foo(); \nS-31 \nS-32 return 0; \nS-33 } \nC / C++ \nFortran \n1 Example task_dep.7.f90 (omp_5.0) \nS-1 subroutine foo() \nS-2 implicit none \nS-3 integer :: x, y \nS-4 \nS-5 x = 0 \nS-6 y = 2 \nS-7 \nS-8 !$omp task depend(inout: x) shared(x) \nS-9 x = x + 1 !! 1st child task \nS-10 !$omp end task \nS-11 \nS-12 !$omp task depend(in: x) depend(inout: y) shared(x, y) \nS-13 y = y - x !! 2nd child task \nS-14 !$omp end task \nS-15 \nS-16 !$omp taskwait depend(in: x) !! 1st taskwait \nS-17 \nS-18 print*, \"x=\", x \nS-19 \nS-20 !! Second task may not be finished. \nS-21 !! Accessing y here would create a race condition. \nS-22 \nS-23 !$omp taskwait !! 2nd taskwait \nS-24 \nS-25 print*, \"y=\", y \nS-26 \nS-27 end subroutine foo \nS-28 \nS-29 program p \nS-30 implicit none \nS-31 !$omp parallel \nS-32 !$omp single \nS-33 call foo() \nS-34 !$omp end single \nS-35 !$omp end parallel \nS-36 end program p \nFortran \n116 OpenMP Examples Version 5.2.1 - November 2022 \n1 This example is similar to the previous one, except the generating task is directed to also wait for \n2 completion of the second task. \n3 The depend clause of the taskwait construct now includes an in dependence type for y. \n4 Hence the generating task must now wait on completion of any child task having y with an out \n5 (here inout) dependence type in its depend clause. So, the depend clause of the taskwait \n6 construct now constrains the second task to complete at the taskwait, too. (This change makes \n7 the second taskwait of the previous example unnecessary\u2013 it has been removed in this example.) \n8 Note: While a taskwait construct ensures that all child tasks have completed; a depend clause on a \n9 taskwait construct only waits for specific child tasks (prescribed by the dependence type and list \n10 items in the taskwait\u2019s depend clause). This and the previous example illustrate the need to \n11 carefully determine the dependence type of variables in the taskwait depend clause when \n12 selecting child tasks that the generating task must wait on, so that its execution after the taskwait \n13 does not produce race conditions on variables accessed by non-completed child tasks. \nC / C++ \n14 Example task_dep.8.c (omp_5.0) \nS-1 #include<stdio.h> \nS-2 \nS-3 void foo() \nS-4 { \nS-5 int x = 0, y = 2; \nS-6 \nS-7 #pragma omp task depend(inout: x) shared(x) \nS-8 x++; // 1st child task \nS-9 \nS-10 #pragma omp task depend(in: x) depend(inout: y) shared(x, y) \nS-11 y -= x; // 2st child task \nS-12 \nS-13 #pragma omp taskwait depend(in: x,y) \nS-14 \nS-15 printf(\"x=%d\\n\",x); \nS-16 printf(\"y=%d\\n\",y); \nS-17 \nS-18 } \nS-19 \nS-20 int main() \nS-21 { \nS-22 #pragma omp parallel \nS-23 #pragma omp single \nS-24 foo(); \nS-25 \nS-26 return 0; \nS-27 } \nC / C++ \nCHAPTER 5. TASKING 117 \nFortran \n1 Example task_dep.8.f90 (omp_5.0) \nS-1 subroutine foo() \nS-2 implicit nonE \nS-3 integer :: x, y \nS-4 \nS-5 x = 0 \nS-6 y = 2 \nS-7 \nS-8 !$omp task depend(inout: x) shared(x) \nS-9 x = x + 1 !! 1st child task \nS-10 !$omp end task \nS-11 \nS-12 !$omp task depend(in: x) depend(inout: y) shared(x, y) \nS-13 y = y - x !! 2nd child task \nS-14 !$omp end task \nS-15 \nS-16 !$omp taskwait depend(in: x,y) \nS-17 \nS-18 print*, \"x=\", x \nS-19 print*, \"y=\", y \nS-20 \nS-21 end subroutine foo \nS-22 \nS-23 program p \nS-24 implicit none \nS-25 !$omp parallel \nS-26 !$omp single \nS-27 call foo() \nS-28 !$omp end single \nS-29 !$omp end parallel \nS-30 end program p \nFortran \n118 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "5.3.7 Mutually Exclusive Execution with Dependences", "chunk": ""}
{"section_title": "5.3.7 Mutually Exclusive Execution with Dependences", "chunk": "2 In this example we show a series of tasks, including mutually exclusive tasks, expressing \n3 dependences using the depend clause on the task construct. \n4 The program will always print 6. Tasks T1, T2 and T3 will be scheduled first, in any order. Task T4 \n5 will be scheduled after tasks T1 and T2 are completed. T5 will be scheduled after tasks T1 and T3 \n6 are completed. Due to the mutexinoutset dependence type on c, T4 and T5 may be scheduled \n7 in any order with respect to each other, but not at the same time. Tasks T6 will be scheduled after \n8 both T4 and T5 are completed. \nC / C++ \n9 Example task_dep.9.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 int main() \nS-3 { \nS-4 int a, b, c, d; \nS-5 #pragma omp parallel \nS-6 #pragma omp single \nS-7 { \nS-8 #pragma omp task depend(out: c) \nS-9 c = 1; /* Task T1 */ \nS-10 #pragma omp task depend(out: a) \nS-11 a = 2; /* Task T2 */ \nS-12 #pragma omp task depend(out: b) \nS-13 b = 3; /* Task T3 */ \nS-14 #pragma omp task depend(in: a) depend(mutexinoutset: c) \nS-15 c += a; /* Task T4 */ \nS-16 #pragma omp task depend(in: b) depend(mutexinoutset: c) \nS-17 c += b; /* Task T5 */ \nS-18 #pragma omp task depend(in: c) \nS-19 d = c; /* Task T6 */ \nS-20 } \nS-21 printf(\"%d\\n\", d); \nS-22 return 0; \nS-23 } \nC / C++ \nCHAPTER 5. TASKING 119 \nFortran \n1 Example task_dep.9.f90 (omp_5.0) \nS-1 program example \nS-2 integer :: a, b, c, d \nS-3 !$omp parallel \nS-4 !$omp single \nS-5 !$omp task depend(out: c) \nS-6 c = 1 ! Task T1 \nS-7 !$omp end task \nS-8 !$omp task depend(out: a) \nS-9 a = 2 ! Task T2 \nS-10 !$omp end task \nS-11 !$omp task depend(out: b) \nS-12 b = 3 ! Task T3 \nS-13 !$omp end task \nS-14 !$omp task depend(in: a) depend(mutexinoutset: c) \nS-15 c = c + a ! Task T4 \nS-16 !$omp end task \nS-17 !$omp task depend(in: b) depend(mutexinoutset: c) \nS-18 c = c + b ! Task T5 \nS-19 !$omp end task \nS-20 !$omp task depend(in: c) \nS-21 d = c ! Task T6 \nS-22 !$omp end task \nS-23 !$omp end single \nS-24 !$omp end parallel \nS-25 print *, d \nS-26 end program \nFortran \n2 The following example demonstrates a situation where the mutexinoutset dependence type is \n3 advantageous. If shortTaskB completes before longTaskA, the runtime can take advantage of \n4 this by scheduling longTaskBC before shortTaskAC. \nC / C++ \n5 Example task_dep.10.c (omp_5.0) \nS-1 extern int longTaskA(), shortTaskB(); \nS-2 extern int shortTaskAC(int,int), longTaskBC(int,int); \nS-3 void foo (void) \nS-4 { \nS-5 int a, b, c; \nS-6 c = 0; \nS-7 #pragma omp parallel \nS-8 #pragma omp single \nS-9 { \n120 OpenMP Examples Version 5.2.1 - November 2022 \nS-10 #pragma omp task depend(out: a) \nS-11 a = longTaskA(); \nS-12 #pragma omp task depend(out: b) \nS-13 b = shortTaskB(); \nS-14 #pragma omp task depend(in: a) depend(mutexinoutset: c) \nS-15 c = shortTaskAC(a,c); \nS-16 #pragma omp task depend(in: b) depend(mutexinoutset: c) \nS-17 c = longTaskBC(b,c); \nS-18 } \nS-19 } \nC / C++ \nFortran \n1 Example task_dep.10.f90 (omp_5.0) \nS-1 subroutine foo \nS-2 integer :: a,b,c \nS-3 c = 0 \nS-4 !$omp parallel \nS-5 !$omp single \nS-6 !$omp task depend(out: a) \nS-7 a = longTaskA() \nS-8 !$omp end task \nS-9 !$omp task depend(out: b) \nS-10 b = shortTaskB() \nS-11 !$omp end task \nS-12 !$omp task depend(in: a) depend(mutexinoutset: c) \nS-13 c = shortTaskAC(a,c) \nS-14 !$omp end task \nS-15 !$omp task depend(in: b) depend(mutexinoutset: c) \nS-16 c = longTaskBC(b,c) \nS-17 !$omp end task \nS-18 !$omp end single \nS-19 !$omp end parallel \nS-20 end subroutine foo \nFortran \nCHAPTER 5. TASKING 121 \n"}
{"section_title": "5.3.8 Multidependences Using Iterators", "chunk": ""}
{"section_title": "5.3.8 Multidependences Using Iterators", "chunk": "2 The following example uses an iterator to define a dynamic number of dependences. \n3 In the single construct of a parallel region a loop generates n tasks and each task has an out \n4 dependence specified through an element of the v array. This is followed by a single task that defines \n5 an in dependence on each element of the array. This is accomplished by using the iterator \n6 modifier in the depend clause, supporting a dynamic number of dependences (n here). \n7 The task for the print_all_elements function is not executed until all dependences prescribed (or \n8 registered) by the iterator are fulfilled; that is, after all the tasks generated by the loop have \n9 completed. \n10 Note, one cannot simply use an array section in the depend clause of the second task construct \n11 because this would violate the depend clause restriction: \n12 \u201cList items used in depend clauses of the same task or sibling tasks must indicate identical storage \n13 locations or disjoint storage locations\u201d. \n14 In this case each of the loop tasks use a single disjoint (different storage) element in their depend \n15 clause; however, the array-section storage area prescribed in the commented directive is neither \n16 identical nor disjoint to the storage prescribed by the elements of the loop tasks. The iterator \n17 overcomes this restriction by effectively creating n disjoint storage areas. \nC / C++ \n18 Example task_dep.11.c (omp_5.0) \nS-1 #include<stdio.h> \nS-2 \nS-3 void set_an_element(int *p, int val) { \nS-4 *p = val; \nS-5 } \nS-6 \nS-7 void print_all_elements(int *v, int n) { \nS-8 int i; \nS-9 for (i = 0; i < n; ++i) { \nS-10 printf(\"%d, \", v[i]); \nS-11 } \nS-12 printf(\"\\n\"); \nS-13 } \nS-14 \nS-15 void parallel_computation(int n) { \nS-16 int v[n]; \nS-17 #pragma omp parallel \nS-18 #pragma omp single \nS-19 { \nS-20 int i; \nS-21 for (i = 0; i < n; ++i) \nS-22 #pragma omp task depend(out: v[i]) \n122 OpenMP Examples Version 5.2.1 - November 2022 \nS-23 set_an_element(&v[i], i); \nS-24 \nS-25 #pragma omp task depend(iterator(it = 0:n), in: v[it]) \nS-26 // The following violates array-section restriction: \nS-27 // #pragma omp task depend(in: v[0:n]) \nS-28 print_all_elements(v, n); \nS-29 } \nS-30 } \nC / C++ \nFortran \n1 Example task_dep.11.f90 (omp_5.0) \nS-1 subroutine set_an_element(e, val) \nS-2 implicit none \nS-3 integer :: e, val \nS-4 \nS-5 e = val \nS-6 \nS-7 end subroutine \nS-8 \nS-9 subroutine print_all_elements(v, n) \nS-10 implicit none \nS-11 integer :: n, v(n) \nS-12 \nS-13 print *, v \nS-14 \nS-15 end subroutine \nS-16 \nS-17 subroutine parallel_computation(n) \nS-18 implicit none \nS-19 integer :: n \nS-20 integer :: i, v(n) \nS-21 \nS-22 !$omp parallel \nS-23 !$omp single \nS-24 do i=1, n \nS-25 !$omp task depend(out: v(i)) \nS-26 call set_an_element(v(i), i) \nS-27 !$omp end task \nS-28 enddo \nS-29 \nS-30 !$omp task depend(iterator(it = 1:n), in: v(it)) \nS-31 !!$omp task depend(in: v(1:n)) Violates Array section restriction. \nS-32 call print_all_elements(v, n) \nS-33 !$omp end task \nS-34 \nCHAPTER 5. TASKING 123 \nS-35 !$omp end single \nS-36 !$omp end parallel \nS-37 end subroutine \nFortran \n"}
{"section_title": "5.3.9 Dependence for Undeferred Tasks", "chunk": ""}
{"section_title": "5.3.9 Dependence for Undeferred Tasks", "chunk": "2 In the following example, we show that even if a task is undeferred as specified by an if clause that \n3 evaluates to false, task dependences are still honored. \n4 The depend clauses of the first and second explicit tasks specify that the first task is completed \n5 before the second task. \n6 The second explicit task has an if clause that evaluates to false. This means that the execution of \n7 the generating task (the implicit task of the single region) must be suspended until the second \n8 explicit task is completed. But, because of the dependence, the first explicit task must complete \n9 first, then the second explicit task can execute and complete, and only then the generating task can \n10 resume to the print statement. Thus, the program will always print \u201cx = 2\u201d. \nC / C++ \n11 Example task_dep.12.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 int main () \nS-3 { \nS-4 int x = 0; \nS-5 #pragma omp parallel \nS-6 #pragma omp single \nS-7 { \nS-8 /* first explicit task */ \nS-9 #pragma omp task shared(x) depend(out: x) \nS-10 x = 1; \nS-11 \nS-12 /* second explicit task */ \nS-13 #pragma omp task shared(x) depend(inout: x) if(0) \nS-14 x = 2; \nS-15 \nS-16 /* statement executed by parent implicit task \nS-17 prints: x = 2 */ \nS-18 printf(\"x = %d\\n\", x); \nS-19 } \nS-20 return 0; \nS-21 } \nC / C++ \n124 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example task_dep.12.f90 (omp_4.0) \nS-1 program example \nS-2 integer :: x \nS-3 x = 0 \nS-4 !$omp parallel \nS-5 !$omp single \nS-6 !... first explicit task \nS-7 !$omp task shared(x) depend(out: x) \nS-8 x = 1 \nS-9 !$omp end task \nS-10 \nS-11 !... second explicit task \nS-12 !$omp task shared(x) depend(inout: x) if(.false.) \nS-13 x = 2 \nS-14 !$omp end task \nS-15 \nS-16 !... statement executed by parent implicit task \nS-17 ! prints: x = 2 \nS-18 print*, \"x = \", x \nS-19 !$omp end single \nS-20 !$omp end parallel \nS-21 end program \nFortran \n2 In OpenMP 5.1 the omp_all_memory reserved locator was introduced to specify storage of all \n3 objects in memory. In the following example, it is used in Task 4 as a convenient way to specify that \n4 the locator (list item) denotes the storage of all objects (locations) in memory, and will therefore \n5 match the a and d locators of Task 2, Task 3 and Task 6. The dependences guarantee the ordered \n6 execution of Tasks 2 and 3 before 4, and Task 4 before Task 6. Since there are no dependences \n7 imposed on Task 1 and Task 5, they can be scheduled to execute at any time, with no ordering. \nC / C++ \n8 Example task_dep.13.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 \nS-3 int main(){ \nS-4 int a=1, d=1; \nS-5 \nS-6 #pragma omp parallel masked num_threads(5) \nS-7 { \nS-8 #pragma omp task // Task 1 \nS-9 { printf(\"T1\\n\"); } \nS-10 \nCHAPTER 5. TASKING 125 \nS-11 #pragma omp task depend(out: a) // Task 2 \nS-12 { a++; \nS-13 printf(\"T2 a=%i\\n\", a); } \nS-14 \nS-15 #pragma omp task depend(out: d) // Task 3 \nS-16 { d++; \nS-17 printf(\"T3 d=%i\\n\", d); } \nS-18 \nS-19 #pragma omp task depend(inout: omp_all_memory) // Task 4 \nS-20 { a++; d++; \nS-21 printf(\"T4 a=%i d=%i\\n\", a,d);} \nS-22 \nS-23 #pragma omp task // Task 5 \nS-24 { printf(\"T5\\n\"); } \nS-25 \nS-26 #pragma omp task depend(in: a,d) // Task 6 \nS-27 { a++; d++; \nS-28 printf(\"T6 a=%i d=%i\\n\", a,d); } \nS-29 } \nS-30 } \nS-31 \nS-32 /* OUTPUT: ordered {T2,T3 any order}, {T4}, {T6} \nS-33 T2 a=2 \nS-34 T3 d=2 \nS-35 T4 a=3 d=3 \nS-36 T6 a=4 d=4 \nS-37 \nS-38 OUTPUT: unordered (can appear interspersed in ordered output) \nS-39 T1 \nS-40 T5 \nS-41 */ \nC / C++ \nFortran \n1 Example task_dep.13.f90 (omp_5.1) \nS-1 program main \nS-2 integer :: a=1, d=1 \nS-3 \nS-4 !$omp parallel masked num_threads(5) \nS-5 \nS-6 !$omp task !! Task 1 \nS-7 write(*,\u2019(\"T1\")\u2019) \nS-8 !$omp end task \nS-9 \nS-10 !$omp task depend(out: a) !! Task 2 \nS-11 a=a+1 \n126 OpenMP Examples Version 5.2.1 - November 2022 \nS-12 write(*,\u2019(\"T2 a=\",i1)\u2019) a \nS-13 !$omp end task \nS-14 \nS-15 !$omp task depend(out: d) !! Task 3 \nS-16 d=d+1 \nS-17 write(*,\u2019(\"T3 d=\",i1)\u2019) d \nS-18 !$omp end task \nS-19 \nS-20 \nS-21 !$omp task depend(inout: omp_all_memory) !! Task 4 \nS-22 a=a+1; d=d+1 \nS-23 write(*,\u2019(\"T4 a=\",i1,\" d=\",i1)\u2019) a, d \nS-24 !$omp end task \nS-25 \nS-26 !$omp task !! Task 5 \nS-27 write(*,\u2019(\"T5\")\u2019) \nS-28 !$omp end task \nS-29 \nS-30 !$omp task depend(in: a,d) !! Task 6 \nS-31 a=a+1; d=d+1 \nS-32 write(*,\u2019(\"T6 a=\",i1,\" d=\",i1)\u2019) a, d \nS-33 !$omp end task \nS-34 \nS-35 !$omp end parallel masked \nS-36 \nS-37 end program \nS-38 \nS-39 ! OUTPUT: ordered {T2,T3 any order}, {T4}, {T6} \nS-40 ! T2 a=2 \nS-41 ! T3 d=2 \nS-42 ! T4 a=3 d=3 \nS-43 ! T6 a=4 d=4 \nS-44 ! OUTPUT: unordered (can appear interspersed in ordered output) \nS-45 ! T1 \nS-46 ! T5 \nFortran \nCHAPTER 5. TASKING 127 \n"}
{"section_title": "5.4 Task Detachment", "chunk": ""}
{"section_title": "5.4 Task Detachment", "chunk": "2 The detach clause on a task construct provides a mechanism for an asynchronous routine to be \n3 called within a task block, and for the routine\u2019s callback to signal completion to the OpenMP \n4 runtime, through an event fulfillment, triggered by a call to the omp_fulfill_event routine. \n5 When a detach clause is used on a task construct, completion of the detachable task occurs when \n6 the task\u2019s structured block is completed AND an allow-completion event is fulfilled by a call to the \n7 omp_fulfill_event routine with the event-handle argument. \n8 The first example illustrates the basic components used in a detachable task. The second example is \n9 a program that executes asynchronous IO, and illustrates methods that are also inherent in \n10 asynchronous messaging within MPI and asynchronous commands in streams within GPU codes. \n11 Interfaces to asynchronous operations found in IO, MPI and GPU parallel computing platforms and \n12 their programming models are not standardized. \n13 \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014- \n14 The first example creates a detachable task that executes the asynchronous async_work routine, \n15 passing the omp_fulfill_event function and the (firstprivate) event handle to the function. Here, the \n16 omp_fulfill_event function is the \u201ccallback\u201d function to be executed at the end of the \n17 async_work function\u2019s asynchronous operations, with the associated data, event. \nC / C++ \n18 Example task_detach.1.c (omp_5.0) \nS-1 #include <omp.h> \nS-2 \nS-3 void async_work(void (*)(void*), void*); \nS-4 void work(); \nS-5 \nS-6 int main() { \nS-7 int async=1; \nS-8 #pragma omp parallel \nS-9 #pragma omp masked \nS-10 { \nS-11 \nS-12 omp_event_handle_t event; \nS-13 #pragma omp task detach(event) \nS-14 { \nS-15 if(async) { \nS-16 async_work( (void (*)(void*)) omp_fulfill_event, (void*) event ); \nS-17 } else { \nS-18 work(); \nS-19 omp_fulfill_event(event); \nS-20 } \nS-21 } \nS-22 // Other work \n128 OpenMP Examples Version 5.2.1 - November 2022 \nS-23 #pragma omp taskwait \nS-24 } \nS-25 return 0; \nS-26 } \nC / C++ \nFortran \n1 Example task_detach.1.f90 (omp_5.0) \nS-1 program main \nS-2 use omp_lib \nS-3 implicit none \nS-4 \nS-5 external :: async_work, work \nS-6 \nS-7 logical :: async=.true. \nS-8 integer(omp_event_handle_kind) :: event \nS-9 \nS-10 !$omp parallel \nS-11 !$omp masked \nS-12 \nS-13 !$omp task detach(event) \nS-14 \nS-15 if(async) then \nS-16 call async_work(omp_fulfill_event, event) \nS-17 else \nS-18 call work() \nS-19 call omp_fulfill_event(event) \nS-20 endif \nS-21 \nS-22 !$omp end task \nS-23 !! Other work \nS-24 \nS-25 !$omp taskwait \nS-26 \nS-27 !$omp end masked \nS-28 !$omp end parallel \nS-29 \nS-30 end program \nFortran \nCHAPTER 5. TASKING 129 \n1 In the following example, text data is written asynchronously to the file async_data, using POSIX \n2 asynchronous IO (aio). An aio \u201ccontrol block\u201d, cb, is set up to send a signal when IO is complete, \n3 and the sigaction function registers the signal action, a callback to callback_aioSigHandler. \n4 The first task (TASK1) starts the asynchronous IO and runs as a detachable task. The second and \n5 third tasks (TASK2 and TASK3) perform synchronous IO to stdout with print statements. The \n6 difference between the two types of tasks is that the thread for TASK1 is freed for other execution \n7 within the parallel region, while the threads for TASK2 and TASK3 wait on the (synchronous) IO \n8 to complete, and cannot perform other work while the operating system is performing the \n9 synchronous IO. The if clause ensures that the detachable task is launched and the call to the \n10 aio_write function returns before TASK2 and TASK3 are generated (while the async IO occurs in \n11 the \u201cbackground\u201d and eventually executes the callback function). The barrier at the end of the \n12 parallel region ensures that the detachable task has completed. \nC / C++ \n13 Example task_detach.2.c (omp_5.0) \nS-1 // use -lrt on loader line \nS-2 #include <stdio.h> \nS-3 #include <unistd.h> \nS-4 #include <fcntl.h> \nS-5 #include <aio.h> \nS-6 #include <errno.h> \nS-7 #include <signal.h> \nS-8 #include <stdint.h> \nS-9 \nS-10 #include <omp.h> \nS-11 \nS-12 #define IO_SIGNAL SIGUSR1 // Signal used to notify I/O completion \nS-13 \nS-14 // Handler for I/O completion signal \nS-15 static void callback_aioSigHandler(int sig, siginfo_t *si, \nS-16 void *ucontext) { \nS-17 if (si->si_code == SI_ASYNCIO){ \nS-18 printf( \"OUT: I/O completion signal received.\\n\"); \nS-19 omp_fulfill_event( (omp_event_handle_t)(uintptr_t) \nS-20 (si->si_value.sival_ptr) ); \nS-21 } \nS-22 } \nS-23 \nS-24 void work(int i){ printf(\"OUT: Executing work(%d)\\n\", i);} \nS-25 \nS-26 int main() { \nS-27 // Write \"Written Asynchronously.\" to file data, using POSIX \nS-28 // asynchronous IO. Error checking not included for clarity \nS-29 // and simplicity. \nS-30 \n130 OpenMP Examples Version 5.2.1 - November 2022 \nS-31 char data[] = \"Written Asynchronously.\"; \nS-32 \nS-33 struct aiocb cb; \nS-34 struct sigaction sa; \nS-35 \nS-36 omp_event_handle_t event; \nS-37 \nS-38 int fd = open( \"async_data\", O_CREAT|O_RDWR|O_TRUNC,0664); \nS-39 \nS-40 // Setup async io (aio) control block (cb) \nS-41 cb.aio_nbytes = sizeof(data)-1; \nS-42 cb.aio_fildes = fd; \nS-43 cb.aio_buf = data; \nS-44 cb.aio_reqprio = 0; \nS-45 cb.aio_offset = 0; \nS-46 cb.aio_sigevent.sigev_notify = SIGEV_SIGNAL; \nS-47 cb.aio_sigevent.sigev_signo = IO_SIGNAL; \nS-48 \nS-49 // Setup Signal Handler Callback \nS-50 sigemptyset(&sa.sa_mask); \nS-51 sa.sa_flags = SA_RESTART | SA_SIGINFO; \nS-52 sa.sa_sigaction = callback_aioSigHandler; //callback \nS-53 sigaction(IO_SIGNAL, &sa, NULL); \nS-54 \nS-55 #pragma omp parallel num_threads(2) \nS-56 #pragma omp masked \nS-57 { \nS-58 \nS-59 #pragma omp task detach(event) if(0) // TASK1 \nS-60 { \nS-61 cb.aio_sigevent.sigev_value.sival_ptr = (void *) event; \nS-62 aio_write(&cb); \nS-63 } \nS-64 \nS-65 #pragma omp task // TASK2 \nS-66 work(1); \nS-67 #pragma omp task // TASK3 \nS-68 work(2); \nS-69 \nS-70 } // Parallel region barrier ensures completion of detachable task. \nS-71 \nS-72 // Making sure the aio operation completed. \nS-73 // With OpenMP detachable task the condition will always be false: \nS-74 while(aio_error(&cb) == EINPROGRESS) { \nS-75 printf(\" INPROGRESS\\n\");} //Safeguard \nS-76 \nS-77 close(fd); \nCHAPTER 5. TASKING 131 \nS-78 return 0; \nS-79 } \nS-80 /* Any Order: \nS-81 OUT: I/O completion signal received. \nS-82 OUT: Executing work(1) \nS-83 OUT: Executing work(2) \nS-84 */ \nC / C++ \n132 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "5.5 taskgroup Construct", "chunk": ""}
{"section_title": "5.5 taskgroup Construct", "chunk": "2 In this example, tasks are grouped and synchronized using the taskgroup construct. \n3 Initially, one task (the task executing the start_background_work() call) is created in the \n4 parallel region, and later a parallel tree traversal is started (the task executing the root of the \n5 recursive compute_tree() calls). While synchronizing tasks at the end of each tree traversal, \n6 using the taskgroup construct ensures that the formerly started background task does not \n7 participate in the synchronization and is left free to execute in parallel. This is opposed to the \n8 behavior of the taskwait construct, which would include the background tasks in the \n9 synchronization. \nC / C++ \n10 Example taskgroup.1.c (omp_4.0) \nS-1 extern void start_background_work(void); \nS-2 extern void check_step(void); \nS-3 extern void print_results(void); \nS-4 struct tree_node \nS-5 { \nS-6 struct tree_node *left; \nS-7 struct tree_node *right; \nS-8 }; \nS-9 typedef struct tree_node* tree_type; \nS-10 extern void init_tree(tree_type); \nS-11 #define max_steps 100 \nS-12 void compute_something(tree_type tree) \nS-13 { \nS-14 // some computation \nS-15 } \nS-16 void compute_tree(tree_type tree) \nS-17 { \nS-18 if (tree->left) \nS-19 { \nS-20 #pragma omp task \nS-21 compute_tree(tree->left); \nS-22 } \nS-23 if (tree->right) \nS-24 { \nS-25 #pragma omp task \nS-26 compute_tree(tree->right); \nS-27 } \nS-28 #pragma omp task \nS-29 compute_something(tree); \nS-30 } \nS-31 int main() \nS-32 { \nCHAPTER 5. TASKING 133 \nS-33 int i; \nS-34 tree_type tree; \nS-35 init_tree(tree); \nS-36 #pragma omp parallel \nS-37 #pragma omp single \nS-38 { \nS-39 #pragma omp task \nS-40 start_background_work(); \nS-41 for (i = 0; i < max_steps; i++) \nS-42 { \nS-43 #pragma omp taskgroup \nS-44 { \nS-45 #pragma omp task \nS-46 compute_tree(tree); \nS-47 } // wait on tree traversal in this step \nS-48 check_step(); \nS-49 } \nS-50 } // only now is background work required to be complete \nS-51 print_results(); \nS-52 return 0; \nS-53 } \nC / C++ \nFortran \n1 Example taskgroup.1.f90 (omp_4.0) \nS-1 module tree_type_mod \nS-2 integer, parameter :: max_steps=100 \nS-3 type tree_type \nS-4 type(tree_type), pointer :: left, right \nS-5 end type \nS-6 contains \nS-7 subroutine compute_something(tree) \nS-8 type(tree_type), pointer :: tree \nS-9 ! some computation \nS-10 end subroutine \nS-11 recursive subroutine compute_tree(tree) \nS-12 type(tree_type), pointer :: tree \nS-13 if (associated(tree%left)) then \nS-14 !$omp task \nS-15 call compute_tree(tree%left) \nS-16 !$omp end task \nS-17 endif \nS-18 if (associated(tree%right)) then \nS-19 !$omp task \nS-20 call compute_tree(tree%right) \nS-21 !$omp end task \n134 OpenMP Examples Version 5.2.1 - November 2022 \nS-22 endif \nS-23 !$omp task \nS-24 call compute_something(tree) \nS-25 !$omp end task \nS-26 end subroutine \nS-27 end module \nS-28 program main \nS-29 use tree_type_mod \nS-30 type(tree_type), pointer :: tree \nS-31 call init_tree(tree); \nS-32 !$omp parallel \nS-33 !$omp single \nS-34 !$omp task \nS-35 call start_background_work() \nS-36 !$omp end task \nS-37 do i=1, max_steps \nS-38 !$omp taskgroup \nS-39 !$omp task \nS-40 call compute_tree(tree) \nS-41 !$omp end task \nS-42 !$omp end taskgroup ! wait on tree traversal in this step \nS-43 call check_step() \nS-44 enddo \nS-45 !$omp end single \nS-46 !$omp end parallel ! only now is background work required to be complete \nS-47 call print_results() \nS-48 end program \nFortran \nCHAPTER 5. TASKING 135 \n"}
{"section_title": "5.6 taskyield Construct", "chunk": ""}
{"section_title": "5.6 taskyield Construct", "chunk": "2 The following example illustrates the use of the taskyield directive. The tasks in the example \n3 compute something useful and then do some computation that must be done in a critical region. By \n4 using taskyield when a task cannot get access to the critical region the implementation \n5 can suspend the current task and schedule some other task that can do something useful. \nC / C++ \n6 Example taskyield.1.c (omp_3.1) \nS-1 #include <omp.h> \nS-2 \nS-3 void something_useful ( void ); \nS-4 void something_critical ( void ); \nS-5 void foo ( omp_lock_t * lock, int n ) \nS-6 { \nS-7 int i; \nS-8 \nS-9 for ( i = 0; i < n; i++ ) \nS-10 #pragma omp task \nS-11 { \nS-12 something_useful(); \nS-13 while ( !omp_test_lock(lock) ) { \nS-14 #pragma omp taskyield \nS-15 } \nS-16 something_critical(); \nS-17 omp_unset_lock(lock); \nS-18 } \nS-19 } \nC / C++ \nFortran \n7 Example taskyield.1.f90 (omp_3.1) \nS-1 subroutine foo ( lock, n ) \nS-2 use omp_lib \nS-3 integer (kind=omp_lock_kind) :: lock \nS-4 integer n \nS-5 integer i \nS-6 \nS-7 do i = 1, n \nS-8 !$omp task \nS-9 call something_useful() \nS-10 do while ( .not. omp_test_lock(lock) ) \nS-11 !$omp taskyield \nS-12 end do \nS-13 call something_critical() \n136 OpenMP Examples Version 5.2.1 - November 2022 \nS-14 call omp_unset_lock(lock) \nS-15 !$omp end task \nS-16 end do \nS-17 \nS-18 end subroutine \nFortran \nCHAPTER 5. TASKING 137 \n"}
{"section_title": "5.7 taskloop Construct", "chunk": ""}
{"section_title": "5.7 taskloop Construct", "chunk": "2 The following example illustrates how to execute a long running task concurrently with tasks \n3 created with a taskloop directive for a loop having unbalanced amounts of work for its iterations. \n4 The grainsize clause specifies that each task is to execute at least 500 iterations of the loop. \n5 The nogroup clause removes the implicit taskgroup of the taskloop construct; the explicit \n6 taskgroup construct in the example ensures that the function is not exited before the \n7 long-running task and the loops have finished execution. \nC / C++ \n8 Example taskloop.1.c (omp_4.5) \nS-1 void long_running_task(void); \nS-2 void loop_body(int i, int j); \nS-3 \nS-4 void parallel_work(void) { \nS-5 int i, j; \nS-6 #pragma omp taskgroup \nS-7 { \nS-8 #pragma omp task \nS-9 long_running_task(); // can execute concurrently \nS-10 \nS-11 #pragma omp taskloop private(j) grainsize(500) nogroup \nS-12 for (i = 0; i < 10000; i++) { // can execute concurrently \nS-13 for (j = 0; j < i; j++) { \nS-14 loop_body(i, j); \nS-15 } \nS-16 } \nS-17 } \nS-18 } \nC / C++ \n138 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example taskloop.1.f90 (omp_4.5) \nS-1 subroutine parallel_work \nS-2 integer i \nS-3 integer j \nS-4 !$omp taskgroup \nS-5 \nS-6 !$omp task \nS-7 call long_running_task() \nS-8 !$omp end task \nS-9 \nS-10 !$omp taskloop private(j) grainsize(500) nogroup \nS-11 do i=1,10000 \nS-12 do j=1,i \nS-13 call loop_body(i, j) \nS-14 end do \nS-15 end do \nS-16 !$omp end taskloop \nS-17 \nS-18 !$omp end taskgroup \nS-19 end subroutine \nFortran \n2 Because a taskloop construct encloses a loop, it is often incorrectly perceived as a worksharing \n3 construct (when it is directly nested in a parallel region). \n4 While a worksharing construct distributes the loop iterations across all threads in a team, the entire \n5 loop of a taskloop construct is executed by every thread of the team. \n6 In the example below the first taskloop occurs closely nested within a parallel region and the \n7 entire loop is executed by each of the T threads; hence the reduction sum is executed T*N times. \n8 The loop of the second taskloop is within a single region and is executed by a single thread so \n9 that only N reduction sums occur. (The other N-1 threads of the parallel region will participate \n10 in executing the tasks. This is the common use case for the taskloop construct.) \n11 In the example, the code thus prints x1 = 16384 (T*N) and x2 = 1024 (N). \nCHAPTER 5. TASKING 139 \nC / C++ \n1 Example taskloop.2.c (omp_4.5) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define T 16 \nS-4 #define N 1024 \nS-5 \nS-6 void parallel_work() { \nS-7 int x1 = 0, x2 = 0; \nS-8 \nS-9 #pragma omp parallel shared(x1,x2) num_threads(T) \nS-10 { \nS-11 #pragma omp taskloop \nS-12 for (int i = 0; i < N; ++i) { \nS-13 #pragma omp atomic \nS-14 x1++; // executed T*N times \nS-15 } \nS-16 \nS-17 #pragma omp single \nS-18 #pragma omp taskloop \nS-19 for (int i = 0; i < N; ++i) { \nS-20 #pragma omp atomic \nS-21 x2++; // executed N times \nS-22 } \nS-23 } \nS-24 \nS-25 printf(\"x1 = %d, x2 = %d\\n\", x1, x2); \nS-26 } \nC / C++ \nFortran \n2 Example taskloop.2.f90 (omp_4.5) \nS-1 subroutine parallel_work \nS-2 implicit none \nS-3 integer :: x1, x2 \nS-4 integer :: i \nS-5 integer, parameter :: T = 16 \nS-6 integer, parameter :: N = 1024 \nS-7 \nS-8 x1 = 0 \nS-9 x2 = 0 \nS-10 !$omp parallel shared(x1,x2) num_threads(T) \nS-11 !$omp taskloop \nS-12 do i = 1,N \nS-13 !$omp atomic \n140 OpenMP Examples Version 5.2.1 - November 2022 \nS-14 x1 = x1 + 1 ! executed T*N times \nS-15 !$omp end atomic \nS-16 end do \nS-17 !$omp end taskloop \nS-18 \nS-19 !$omp single \nS-20 !$omp taskloop \nS-21 do i = 1,N \nS-22 !$omp atomic \nS-23 x2 = x2 + 1 ! executed N times \nS-24 !$omp end atomic \nS-25 end do \nS-26 !$omp end taskloop \nS-27 !$omp end single \nS-28 !$omp end parallel \nS-29 \nS-30 write (*,\u2019(A,I0,A,I0)\u2019) \u2019x1 = \u2019, x1, \u2019, x2 = \u2019,x2 \nS-31 end subroutine \nFortran \nCHAPTER 5. TASKING 141 \n"}
{"section_title": "5.8 Combined parallel masked and taskloop Constructs", "chunk": ""}
{"section_title": "5.8 Combined parallel masked and taskloop Constructs", "chunk": "2 Constructs \n3 Just as the for and do constructs were combined with the parallel construct for convenience, \n4 so too, the combined parallel masked taskloop and \n5 parallel masked taskloop simd constructs have been created for convenience when using \n6 the taskloop construct. \n7 In the following example the first taskloop construct is enclosed by the usual parallel and \n8 masked constructs to form a team of threads, and a single task generator (primary thread) for the \n9 taskloop construct. \n10 The same OpenMP operations for the first taskloop are accomplished by the second taskloop with \n11 the parallel masked taskloop combined construct. The third taskloop uses the combined \n12 parallel masked taskloop simd construct to accomplish the same behavior as closely \n13 nested parallel masked, and taskloop simd constructs. \n14 As with any combined construct the clauses of the components may be used with appropriate \n15 restrictions. The combination of the parallel masked construct with the taskloop or \n16 taskloop simd construct produces no additional restrictions. \nC / C++ \n17 Example parallel_masked_taskloop.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 \nS-4 int main() \nS-5 { \nS-6 int i, a[N],b[N],c[N]; \nS-7 \nS-8 for(int i=0; i<N; i++){ b[i]=i; c[i]=i; } //init \nS-9 \nS-10 #pragma omp parallel \nS-11 #pragma omp masked \nS-12 #pragma omp taskloop // taskloop 1 \nS-13 for(i=0;i<N;i++){ a[i] = b[i] + c[i]; } \nS-14 \nS-15 #pragma omp parallel masked taskloop // taskloop 2 \nS-16 for(i=0;i<N;i++){ b[i] = a[i] + c[i]; } \nS-17 \nS-18 #pragma omp parallel masked taskloop simd // taskloop 3 \nS-19 for(i=0;i<N;i++){ c[i] = a[i] + b[i]; } \nS-20 \nS-21 printf(\" %d %d\\n\",c[0],c[N-1]); // 0 and 495 \nS-22 } \nC / C++ \n142 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example parallel_masked_taskloop.1.f90 (omp_5.1) \nS-1 program main \nS-2 \nS-3 integer, parameter :: N=100 \nS-4 integer :: i, a(N),b(N),c(N) \nS-5 \nS-6 do i=1,N !! initialize \nS-7 b(i) = i \nS-8 c(i) = i \nS-9 enddo \nS-10 \nS-11 !$omp parallel \nS-12 !$omp masked \nS-13 !$omp taskloop !! taskloop 1 \nS-14 do i=1,N \nS-15 a(i) = b(i) + c(i) \nS-16 enddo \nS-17 !$omp end taskloop \nS-18 !$omp end masked \nS-19 !$omp end parallel \nS-20 \nS-21 !$omp parallel masked taskloop !! taskloop 2 \nS-22 do i=1,N \nS-23 b(i) = a(i) + c(i) \nS-24 enddo \nS-25 !$omp end parallel masked taskloop \nS-26 \nS-27 !$omp parallel masked taskloop simd !! taskloop 3 \nS-28 do i=1,N \nS-29 c(i) = a(i) + b(i) \nS-30 enddo \nS-31 !$omp end parallel masked taskloop simd \nS-32 \nS-33 print*,c(1),c(N) !! 5 and 500 \nS-34 \nS-35 end program \nFortran \nCHAPTER 5. TASKING 143 \nThis page intentionally left blank \n"}
{"section_title": "6 Devices", "chunk": ""}
{"section_title": "6 Devices", "chunk": "2 The target construct consists of a target directive and an execution region. The target \n3 region is executed on the default device or the device specified in the device clause. \n4 In OpenMP version 4.0, by default, all variables within the lexical scope of the construct are copied \n5 to and from the device, unless the device is the host, or the data exists on the device from a \n6 previously executed data-type construct that has created space on the device and possibly copied \n7 host data to the device storage. \n8 The constructs that explicitly create storage, transfer data, and free storage on the device are \n9 categorized as structured and unstructured. The target data construct is structured. It creates a \n10 data region around target constructs, and is convenient for providing persistent data throughout \n11 multiple target regions. The target enter data and target exit data constructs are \n12 unstructured, because they can occur anywhere and do not support a \u201cstructure\u201d (a region) for \n13 enclosing target constructs, as does the target data construct. \n14 The map clause is used on target constructs and the data-type constructs to map host data. It \n15 specifies the device storage and data movement to and from the device, and controls on the \n16 storage duration. \n17 There is an important change in the OpenMP 4.5 specification that alters the data model for scalar \n18 variables and C/C++ pointer variables. The default behavior for scalar variables and C/C++ pointer \n19 variables in a 4.5 compliant code is firstprivate. Example codes that have been updated to \n20 reflect this new behavior are annotated with a description that describes changes required for \n21 correct execution. Often it is a simple matter of mapping the variable as tofrom to obtain the \n22 intended 4.0 behavior. \n23 In OpenMP version 4.5 the mechanism for target execution is specified as occurring through a \n24 target task. When the target construct is encountered a new target task is generated. The target \n25 task completes after the target region has executed and all data transfers have finished. \n26 This new specification does not affect the execution of pre-4.5 code; it is a necessary element for \n27 asynchronous execution of the target region when using the new nowait clause introduced in \n28 OpenMP 4.5. \n145 \n"}
{"section_title": "6.1 target Construct", "chunk": ""}
{"section_title": "6.1.1 target Construct on parallel Construct", "chunk": "3 This following example shows how the target construct offloads a code region to a target device. \n4 The variables p, v1, v2, and N are implicitly mapped to the target device. \nC / C++ \n5 Example target.1.c (omp_4.0) \nS-1 extern void init(float*, float*, int); \nS-2 extern void output(float*, int); \nS-3 void vec_mult(int N) \nS-4 { \nS-5 int i; \nS-6 float p[N], v1[N], v2[N]; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target \nS-9 #pragma omp parallel for private(i) \nS-10 for (i=0; i<N; i++) \nS-11 p[i] = v1[i] * v2[i]; \nS-12 output(p, N); \nS-13 } \nC / C++ \nFortran \n6 Example target.1.f90 (omp_4.0) \nS-1 subroutine vec_mult(N) \nS-2 integer :: i,N \nS-3 real :: p(N), v1(N), v2(N) \nS-4 call init(v1, v2, N) \nS-5 !$omp target \nS-6 !$omp parallel do \nS-7 do i=1,N \nS-8 p(i) = v1(i) * v2(i) \nS-9 end do \nS-10 !$omp end target \nS-11 call output(p, N) \nS-12 end subroutine \nFortran \n146 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.1.2 target Construct with map Clause", "chunk": "2 This following example shows how the target construct offloads a code region to a target device. \n3 The variables p, v1 and v2 are explicitly mapped to the target device using the map clause. The \n4 variable N is implicitly mapped to the target device. \nC / C++ \n5 Example target.2.c (omp_4.0) \nS-1 extern void init(float*, float*, int); \nS-2 extern void output(float*, int); \nS-3 void vec_mult(int N) \nS-4 { \nS-5 int i; \nS-6 float p[N], v1[N], v2[N]; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target map(v1, v2, p) \nS-9 #pragma omp parallel for \nS-10 for (i=0; i<N; i++) \nS-11 p[i] = v1[i] * v2[i]; \nS-12 output(p, N); \nS-13 } \nC / C++ \nFortran \n6 Example target.2.f90 (omp_4.0) \nS-1 subroutine vec_mult(N) \nS-2 integer :: i,N \nS-3 real :: p(N), v1(N), v2(N) \nS-4 call init(v1, v2, N) \nS-5 !$omp target map(v1,v2,p) \nS-6 !$omp parallel do \nS-7 do i=1,N \nS-8 p(i) = v1(i) * v2(i) \nS-9 end do \nS-10 !$omp end target \nS-11 call output(p, N) \nS-12 end subroutine \nFortran \nCHAPTER 6. DEVICES 147 \n"}
{"section_title": "6.1.3 map Clause with to/from map-types", "chunk": ""}
{"section_title": "6.1.3 map Clause with to/from map-types", "chunk": "2 The following example shows how the target construct offloads a code region to a target device. \n3 In the map clause, the to and from map-types define the mapping between the original (host) data \n4 and the target (device) data. The to map-type specifies that the data will only be read on the \n5 device, and the from map-type specifies that the data will only be written to on the device. By \n6 specifying a guaranteed access on the device, data transfers can be reduced for the target region. \n7 The to map-type indicates that at the start of the target region the variables v1 and v2 are \n8 initialized with the values of the corresponding variables on the host device, and at the end of the \n9 target region the variables v1 and v2 are not assigned to their corresponding variables on the \n10 host device. \n11 The from map-type indicates that at the start of the target region the variable p is not initialized \n12 with the value of the corresponding variable on the host device, and at the end of the target \n13 region the variable p is assigned to the corresponding variable on the host device. \nC / C++ \n14 Example target.3.c (omp_4.0) \nS-1 extern void init(float*, float*, int); \nS-2 extern void output(float*, int); \nS-3 void vec_mult(int N) \nS-4 { \nS-5 int i; \nS-6 float p[N], v1[N], v2[N]; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target map(to: v1, v2) map(from: p) \nS-9 #pragma omp parallel for \nS-10 for (i=0; i<N; i++) \nS-11 p[i] = v1[i] * v2[i]; \nS-12 output(p, N); \nS-13 } \nC / C++ \n15 The to and from map-types allow programmers to optimize data motion. Since data for the v \n16 arrays are not returned, and data for the p array are not transferred to the device, only one-half of \n17 the data is moved, compared to the default behavior of an implicit mapping. \n148 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example target.3.f90 (omp_4.0) \nS-1 subroutine vec_mult(N) \nS-2 integer :: i,N \nS-3 real :: p(N), v1(N), v2(N) \nS-4 call init(v1, v2, N) \nS-5 !$omp target map(to: v1,v2) map(from: p) \nS-6 !$omp parallel do \nS-7 do i=1,N \nS-8 p(i) = v1(i) * v2(i) \nS-9 end do \nS-10 !$omp end target \nS-11 call output(p, N) \nS-12 end subroutine \nFortran \n"}
{"section_title": "6.1.4 map Clause with Array Sections", "chunk": ""}
{"section_title": "6.1.4 map Clause with Array Sections", "chunk": "3 The following example shows how the target construct offloads a code region to a target device. \n4 In the map clause, map-types are used to optimize the mapping of variables to the target device. \n5 Because variables p, v1 and v2 are pointers, array section notation must be used to map the arrays. \n6 The notation :N is equivalent to 0:N. \nC / C++ \n7 Example target.4.c (omp_4.0) \nS-1 extern void init(float*, float*, int); \nS-2 extern void output(float*, int); \nS-3 void vec_mult(float *p, float *v1, float *v2, int N) \nS-4 { \nS-5 int i; \nS-6 init(v1, v2, N); \nS-7 #pragma omp target map(to: v1[0:N], v2[:N]) map(from: p[0:N]) \nS-8 #pragma omp parallel for \nS-9 for (i=0; i<N; i++) \nS-10 p[i] = v1[i] * v2[i]; \nS-11 output(p, N); \nS-12 } \nC / C++ \nCHAPTER 6. DEVICES 149 \n1 In C, the length of the pointed-to array must be specified. In Fortran the extent of the array is \n2 known and the length need not be specified. A section of the array can be specified with the usual \n3 Fortran syntax, as shown in the following example. The value 1 is assumed for the lower bound for \n4 array section v2(:N). \nFortran \n5 Example target.4.f90 (omp_4.0) \nS-1 module mults \nS-2 contains \nS-3 subroutine vec_mult(p,v1,v2,N) \nS-4 real,pointer,dimension(:) :: p, v1, v2 \nS-5 integer :: N,i \nS-6 call init(v1, v2, N) \nS-7 !$omp target map(to: v1(1:N), v2(:N)) map(from: p(1:N)) \nS-8 !$omp parallel do \nS-9 do i=1,N \nS-10 p(i) = v1(i) * v2(i) \nS-11 end do \nS-12 !$omp end target \nS-13 call output(p, N) \nS-14 end subroutine \nS-15 end module \nFortran \n6 A more realistic situation in which an assumed-size array is passed to vec_mult requires that the \n7 length of the arrays be specified, because the compiler does not know the size of the storage. A \n8 section of the array must be specified with the usual Fortran syntax, as shown in the following \n9 example. The value 1 is assumed for the lower bound for array section v2(:N). \nFortran \n10 Example target.4b.f90 (omp_4.0) \nS-1 module mults \nS-2 contains \nS-3 subroutine vec_mult(p,v1,v2,N) \nS-4 real,dimension(*) :: p, v1, v2 \nS-5 integer :: N,i \nS-6 call init(v1, v2, N) \nS-7 !$omp target map(to: v1(1:N), v2(:N)) map(from: p(1:N)) \nS-8 !$omp parallel do \nS-9 do i=1,N \nS-10 p(i) = v1(i) * v2(i) \nS-11 end do \nS-12 !$omp end target \nS-13 call output(p, N) \n150 OpenMP Examples Version 5.2.1 - November 2022 \nS-14 end subroutine \nS-15 end module \nFortran \n"}
{"section_title": "6.1.5 target Construct with if Clause", "chunk": ""}
{"section_title": "6.1.5 target Construct with if Clause", "chunk": "2 The following example shows how the target construct offloads a code region to a target device. \n3 The if clause on the target construct indicates that if the variable N is smaller than a given \n4 threshold, then the target region will be executed by the host device. \n5 The if clause on the parallel construct indicates that if the variable N is smaller than a second \n6 threshold then the parallel region is inactive. \nC / C++ \n7 Example target.5.c (omp_4.0) \nS-1 #define THRESHOLD1 1000000 \nS-2 #define THRESHOLD2 1000 \nS-3 \nS-4 extern void init(float*, float*, int); \nS-5 extern void output(float*, int); \nS-6 \nS-7 void vec_mult(float *p, float *v1, float *v2, int N) \nS-8 { \nS-9 int i; \nS-10 \nS-11 init(v1, v2, N); \nS-12 \nS-13 #pragma omp target if(N>THRESHOLD1) map(to: v1[0:N], v2[:N])\\ \nS-14 map(from: p[0:N]) \nS-15 #pragma omp parallel for if(N>THRESHOLD2) \nS-16 for (i=0; i<N; i++) \nS-17 p[i] = v1[i] * v2[i]; \nS-18 \nS-19 output(p, N); \nS-20 } \nC / C++ \nCHAPTER 6. DEVICES 151 \nFortran \n1 Example target.5.f90 (omp_4.0) \nS-1 module params \nS-2 integer,parameter :: THRESHOLD1=1000000, THRESHHOLD2=1000 \nS-3 end module \nS-4 \nS-5 subroutine vec_mult(p, v1, v2, N) \nS-6 use params \nS-7 real :: p(N), v1(N), v2(N) \nS-8 integer :: i \nS-9 \nS-10 call init(v1, v2, N) \nS-11 \nS-12 !$omp target if(N>THRESHHOLD1) map(to: v1, v2 ) map(from: p) \nS-13 !$omp parallel do if(N>THRESHOLD2) \nS-14 do i=1,N \nS-15 p(i) = v1(i) * v2(i) \nS-16 end do \nS-17 !$omp end target \nS-18 \nS-19 call output(p, N) \nS-20 end subroutine \nFortran \n2 The following example is a modification of the above target.5 code to show the combined target \n3 and parallel loop directives. It uses the directive-name modifier in multiple if clauses to specify \n4 the component directive to which it applies. \n5 The if clause with the target modifier applies to the target component of the combined \n6 directive, and the if clause with the parallel modifier applies to the parallel component of \n7 the combined directive. \nC / C++ \n8 Example target.6.c (omp_4.5) \nS-1 #define THRESHOLD1 1000000 \nS-2 #define THRESHOLD2 1000 \nS-3 \nS-4 extern void init(float*, float*, int); \nS-5 extern void output(float*, int); \nS-6 \nS-7 void vec_mult(float *p, float *v1, float *v2, int N) \nS-8 { \nS-9 int i; \nS-10 \nS-11 init(v1, v2, N); \n152 OpenMP Examples Version 5.2.1 - November 2022 \nS-12 \nS-13 #pragma omp target parallel for \\ \nS-14 if(target: N>THRESHOLD1) if(parallel: N>THRESHOLD2) \\ \nS-15 map(to: v1[0:N], v2[:N]) map(from: p[0:N]) \nS-16 for (i=0; i<N; i++) \nS-17 p[i] = v1[i] * v2[i]; \nS-18 \nS-19 output(p, N); \nS-20 } \nC / C++ \nFortran \n1 Example target.6.f90 (omp_4.5) \nS-1 module params \nS-2 integer,parameter :: THRESHOLD1=1000000, THRESHHOLD2=1000 \nS-3 end module \nS-4 \nS-5 subroutine vec_mult(p, v1, v2, N) \nS-6 use params \nS-7 real :: p(N), v1(N), v2(N) \nS-8 integer :: i \nS-9 \nS-10 call init(v1, v2, N) \nS-11 \nS-12 !$omp target parallel do & \nS-13 !$omp& if(target: N>THRESHHOLD1) if(parallel: N>THRESHOLD2) & \nS-14 !$omp& map(to: v1, v2 ) map(from: p) \nS-15 do i=1,N \nS-16 p(i) = v1(i) * v2(i) \nS-17 end do \nS-18 !$omp end target parallel do \nS-19 \nS-20 call output(p, N) \nS-21 end subroutine \nFortran \nCHAPTER 6. DEVICES 153 \n"}
{"section_title": "6.1.6 Target Reverse Offload", "chunk": ""}
{"section_title": "6.1.6 Target Reverse Offload", "chunk": "2 Beginning with OpenMP 5.0, implementations are allowed to offload back to the host (reverse \n3 offload). \n4 In the example below the error_handler function is executed back on the host, if an erroneous value \n5 is detected in the A array on the device. \n6 This is accomplished by specifying the device-modifier ancestor modifier, along with a device \n7 number of 1, to indicate that the execution is to be performed on the immediate parent (1st \n8 ancestor)\u2013 the host. \n9 The requires directive (another 5.0 feature) uses the reverse_offload clause to guarantee \n10 that the reverse offload is implemented. \n11 Note that the declare target directive uses the device_type clause (another 5.0 feature) to \n12 specify that the error_handler function is compiled to execute on the host only. This ensures that \n13 no attempt will be made to create a device version of the function. This feature may be necessary if \n14 the function exists in another compile unit. \nC / C++ \n15 Example target_reverse_offload.7.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 \nS-4 #define N 100 \nS-5 \nS-6 #pragma omp requires reverse_offload \nS-7 \nS-8 void error_handler(int wrong_value, int index) \nS-9 { \nS-10 printf(\" Error in offload: A[%d]=%d\\n\", index,wrong_value); \nS-11 printf(\" Expecting: A[i ]=i\\n\"); \nS-12 exit(1); \nS-13 // output: Error in offload: A[99]=-1 \nS-14 // Expecting: A[i ]=i \nS-15 \nS-16 } \nS-17 #pragma omp declare target device_type(host) enter(error_handler) \nS-18 \nS-19 int main() \nS-20 { \nS-21 int A[N]; \nS-22 \nS-23 for (int i=0; i<N; i++) A[i] = i; \nS-24 \nS-25 A[N-1]=-1; \n154 OpenMP Examples Version 5.2.1 - November 2022 \nS-26 \nS-27 #pragma omp target map(A) \nS-28 { \nS-29 for (int i=0; i<N; i++) \nS-30 { \nS-31 if (A[i] != i) \nS-32 { \nS-33 #pragma omp target device(ancestor: 1) map(always,to: A[i:1]) \nS-34 error_handler(A[i], i); \nS-35 } \nS-36 } \nS-37 } \nS-38 return 0; \nS-39 } \nC / C++ \nFortran \n1 Example target_reverse_offload.7.f90 (omp_5.0) \nS-1 subroutine error_handler(wrong_value, index) \nS-2 implicit none \nS-3 integer :: wrong_value,index \nS-4 !$omp requires reverse_offload \nS-5 !$omp declare target device_type(host) \nS-6 \nS-7 write( *,\u2019(\"Error in offload: A(\",i3,\")=\",i3)\u2019 ) index,wrong_value \nS-8 write( *,\u2019(\" Expecting: A( i)= i\")\u2019 ) \nS-9 stop \nS-10 !!output: Error in offload: A( 99)= -1 \nS-11 !! Expecting: A( i)= i \nS-12 end subroutine \nS-13 \nS-14 program rev_off \nS-15 implicit none \nS-16 !$omp requires reverse_offload \nS-17 integer, parameter :: N=100 \nS-18 integer :: i \nS-19 integer :: A(N) = (/ (i, i=1,100) /) \nS-20 \nS-21 A(N-1)=-1 \nS-22 \nS-23 !$omp target map(A) \nS-24 do i=1,N \nS-25 if (A(i) /= i) then \nS-26 !$omp target device(ancestor: 1) map(always,to: A(i)) \nS-27 call error_handler(A(i), i) \nS-28 !$omp end target \nCHAPTER 6. DEVICES 155 \nS-29 endif \nS-30 end do \nS-31 !$omp end target \nS-32 \nS-33 end program \nFortran \n156 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.2 defaultmap Clause", "chunk": ""}
{"section_title": "6.2 defaultmap Clause", "chunk": "2 The implicitly determined data-mapping and data-sharing attribute rules of variables referenced in \n3 a target construct can be changed by the defaultmap clause. As of OpenMP 5.0, the implicit \n4 behavior is specified as alloc, to, from, tofrom, firstprivate, none, default or \n5 present, and is optionally applied to a variable category specified as scalar, aggregate, \n6 allocatable, or pointer. \n7 A referenced variable that is in a specified \u201ccategory\u201c is treated as having the specified implicit \n8 behavior. In C/C++, scalar refers to base-language scalar variables, except pointers. In Fortran it \n9 refers to a scalar variable, as defined by the base language, of intrinsic type but excluding the \n10 character type. The aggregate category refers to arrays and structures (which includes variables \n11 of any derived type and of character type for Fortran). Fortran has the additional category of \n12 allocatable for variables that have the allocatable attribute. The pointer category refers to \n13 pointers, which for Fortran are variables that have the pointer attribute. \n14 In the example below, the first target construct uses defaultmap clauses to set data-mapping \n15 and possibly data-sharing attributes that reproduce the default rules for implicitly determined \n16 data-mapping and data-sharing attributes for variables in the construct. That is, if the \n17 defaultmap clauses were removed, the results would be identical. \n18 In the second target construct all implicit behavior is removed by specifying the none implicit \n19 behavior in the defaultmap clause. Hence, all variables that don\u2019t have predetermined attributes \n20 must be given an explicit data-mapping or data-sharing attribute. A scalar (s), an array (A) and a \n21 structure (S for the C/C++ example and D for the Fortran example) are explicitly mapped with the \n22 tofrom map type. \n23 The third target construct shows another usual case for using the defaultmap clause. The \n24 default mapping for (non-pointer) scalar variables is specified. Here, the default implicit mapping \n25 for s3 is tofrom as specified in the defaultmap clause, while s1 and s2 are instead explicitly \n26 treated as firstprivate. \n27 In the fourth target construct all arrays and structures are given firstprivate implicit \n28 behavior by default with the use of the aggregate variable category. For the Fortran example, \n29 the allocatable category is used in a separate defaultmap clause to specify default \n30 firstprivate implicit behavior for referenced allocatable variables (in this case, H). \nC / C++ \n31 Example target_defaultmap.1.c (omp_5.0) \nS-1 #include <stdlib.h> \nS-2 #include <stdio.h> \nS-3 #define N 2 \nS-4 \nS-5 int main(){ \nS-6 typedef struct S_struct { int s; int A[N]; } S_struct_t; \nS-7 \nCHAPTER 6. DEVICES 157 \nS-8 \nS-9 int s; //scalar int variable (scalar) \nS-10 int A[N]; //aggregate variable (array) \nS-11 S_struct_t S; //aggregate variable (structure) \nS-12 int *ptr; //scalar, pointer variable (pointer) \nS-13 \nS-14 int s1, s2, s3; \nS-15 \nS-16 // Initialize everything to zero; \nS-17 s=2; s1=s2=s3=0; \nS-18 A[0]=0; A[1]=0; \nS-19 S.s=0; S.A[0]=0; S.A[1]=0; \nS-20 \nS-21 // Target Region 1 \nS-22 // Uses defaultmap to set scalars, aggregates & \nS-23 // pointers to normal defaults. \nS-24 #pragma omp target \\ \nS-25 defaultmap(firstprivate: scalar) /* could also be default */ \\ \nS-26 defaultmap(tofrom: aggregate)/* could also be default */ \\ \nS-27 defaultmap(default: pointer) /* must be default */ \nS-28 { \nS-29 s = 3; //SCALAR firstprivate, value not returned \nS-30 \nS-31 A[0] = 3; A[1] = 3; //AGGREGATE array, default map tofrom \nS-32 \nS-33 //AGGREGATE structure, default tofrom \nS-34 S.s = 2; \nS-35 S.A[0] = 2; S.A[1] = 2; \nS-36 \nS-37 ptr = &A[0]; //POINTER is private \nS-38 ptr[0] = 2; ptr[1] = 2; \nS-39 \nS-40 } \nS-41 if(s==2 && A[0]==2 && S.s==2 && S.A[0]==2) \nS-42 printf(\" PASSED 1 of 4\\n\"); \nS-43 \nS-44 \nS-45 // Target Region 2 \nS-46 // no implicit mapping allowed. \nS-47 #pragma omp target defaultmap(none) map(tofrom: s, A, S) \nS-48 { \nS-49 s +=5; // All variables must be explicitly mapped \nS-50 A[0] +=5; A[1]+=5; \nS-51 S.s +=5; \nS-52 S.A[0]+=5; S.A[1]+=5; \nS-53 } \nS-54 if(s==7 && A[0]==7 && S.s==7 && S.A[0]==7) \n158 OpenMP Examples Version 5.2.1 - November 2022 \nS-55 printf(\" PASSED 2 of 4\\n\"); \nS-56 \nS-57 \nS-58 // Target Region 3 \nS-59 // defaultmap & explicit data-sharing clause \nS-60 // with variables in same category \nS-61 s1=s2=s3=1; \nS-62 #pragma omp target defaultmap(tofrom: scalar) firstprivate(s1,s2) \nS-63 { \nS-64 s1 += 5; // firstprivate (s1 value not returned to host) \nS-65 s2 += 5; // firstprivate (s2 value not returned to host) \nS-66 s3 += s1 + s2; // mapped as tofrom \nS-67 } \nS-68 if(s1==1 && s2==1 && s3==13 ) printf(\" PASSED 3 of 4\\n\"); \nS-69 \nS-70 \nS-71 // Target Region 4 \nS-72 A[0]=0; A[1]=0; \nS-73 S.A[0]=0; S.A[1]=0; \nS-74 \nS-75 // arrays and structure are firstprivate, and scalars are from \nS-76 #pragma omp target defaultmap(firstprivate: aggregate) \\ \nS-77 map(from: s1, s2) \nS-78 { \nS-79 \nS-80 A[0]+=1; S.A[0]+=1; //Aggregate changes not returned to host \nS-81 A[1]+=1; S.A[1]+=1; //Aggregate changes not returned to host \nS-82 s1 = A[0]+S.A[0]; //s1 value returned to host \nS-83 s2 = A[1]+S.A[1]; //s1 value returned to host \nS-84 } \nS-85 if( A[0]==0 && S.A[0]==0 && s1==2 ) printf(\" PASSED 4 of 4\\n\"); \nS-86 \nS-87 } \nC / C++ \nFortran \n1 Example target_defaultmap.1.f90 (omp_5.0) \nS-1 program defaultmap \nS-2 integer, parameter :: N=2 \nS-3 \nS-4 type DDT_sA \nS-5 integer :: s \nS-6 integer :: A(N) \nS-7 end type \nS-8 \nS-9 integer :: s,s1,s2,s3 !! SCALAR: variable (integer) \nCHAPTER 6. DEVICES 159 \nS-10 integer,target :: A(N) !! AGGREGATE: Array \nS-11 type(DDT_sA) :: D !! AGGREGATE: Derived Data Type (D) \nS-12 integer,allocatable :: H(:) !! ALLOCATABLE: Heap allocated array \nS-13 integer,pointer :: ptrA(:) !! POINTER: points to array \nS-14 \nS-15 ! Assign values to scalar, array, allocatable, and pointers \nS-16 \nS-17 s=2; \nS-18 s1=0; s2=0; s3=0 \nS-19 D%s=0; D%A(1)=0; D%A(2)=0 \nS-20 A(1)=0; A(2)=0 \nS-21 \nS-22 allocate( H(2) ) \nS-23 H(1)=0; H(2)=0 \nS-24 \nS-25 !! Target Region 1 \nS-26 !! Using defaultmap to set scalars, aggregates & \nS-27 !! pointers and allocatables to normal defaults. \nS-28 !$omp target & \nS-29 !$omp& defaultmap( firstprivate: scalar) & \nS-30 !$omp& defaultmap( tofrom: aggregate) & \nS-31 !$omp& defaultmap( tofrom: allocatable) & \nS-32 !$omp& defaultmap( default: pointer) \nS-33 \nS-34 s = 3 !! SCALAR firstprivate, val not returned \nS-35 \nS-36 A(1) = 3 !! AGGREGATE array, default map tofrom \nS-37 A(2) = 3 \nS-38 \nS-39 D%s = 2 !! AGGR. Derived Type, default map tofrom \nS-40 D%A(1) = 2; D%A(2) = 2 \nS-41 \nS-42 H(1) = 2; H(2) = 2 !! ALLOCATABLE, default map tofrom \nS-43 \nS-44 ptrA=>A !! POINTER is private \nS-45 ptrA(1) = 2; ptrA(2) = 2 \nS-46 \nS-47 !$omp end target \nS-48 \nS-49 if(s==2 .and. A(1)==2 .and. D%s==2 .and. D%A(1)==2 .and. H(1) == 2) & \nS-50 print*,\" PASSED 1 of 4\" \nS-51 \nS-52 !! Target Region 2 \nS-53 !! no implicit mapping allowed \nS-54 !$omp target defaultmap(none) map(tofrom: s, A, D) \nS-55 \nS-56 s=s+5 !! All variables must be explicitly mapped \n160 OpenMP Examples Version 5.2.1 - November 2022 \nS-57 A(1)=A(1)+5; A(2)=A(2)+5 \nS-58 D%s=D%s+5 \nS-59 D%A(1)=D%A(1)+5; D%A(2)=D%A(2)+5 \nS-60 \nS-61 !$omp end target \nS-62 if(s==7 .and. A(1)==7 .and. D%s==7 .and. D%A(1)==7) & \nS-63 print*,\" PASSED 2 of 4\" \nS-64 \nS-65 !! Target Region 3 \nS-66 !! defaultmap & explicit data-sharing clause \nS-67 !! with variables in same category \nS-68 s1=1; s2=1; s3=1 \nS-69 !$omp target defaultmap(tofrom: scalar) firstprivate(s1,s2) \nS-70 \nS-71 s1 = s1+5; !! firstprivate (s1 value not returned to host) \nS-72 s2 = s2+5; !! firstprivate (s2 value not returned to host) \nS-73 s3 = s3 +s1 + s2; !! mapped as tofrom \nS-74 \nS-75 !$omp end target \nS-76 if(s1==1 .and. s2==1 .and. s3==13) print*,\" PASSED 3 of 4\" \nS-77 \nS-78 !! Target Region 4 \nS-79 A(1)=0; A(2)=0 \nS-80 D%A(1)=0; D%A(2)=0 \nS-81 H(1)=0; H(2)=0 \nS-82 !! non-allocated arrays & derived types are in AGGREGATE cat \nS-83 !! Allocatable arrays are in ALLOCATABLE category \nS-84 !! Scalars are explicitly mapped from \nS-85 !$omp target defaultmap(firstprivate: aggregate ) & \nS-86 !$omp& defaultmap(firstprivate: allocatable) & \nS-87 !$omp& map(from: s1, s2) \nS-88 \nS-89 A(1)=A(1)+1; D%A(1)=D%A(1)+1; H(1)=H(1)+1 !! changes to A, D%A, H \nS-90 A(2)=A(2)+1; D%A(2)=D%A(2)+1; H(2)=H(2)+1 !! not returned to host \nS-91 s1 = A(1)+D%A(1)+H(1) !! s1 returned to host \nS-92 s2 = A(2)+D%A(2)+H(1) !! s2 returned to host \nS-93 \nS-94 !$omp end target \nS-95 if(A(1)==0 .and. D%A(1)==0 .and. H(1)==0 .and. s1==3) & \nS-96 print*,\" PASSED 4 of 4\" \nS-97 \nS-98 deallocate(H) \nS-99 \nS-100 end program \nFortran \nCHAPTER 6. DEVICES 161 \n"}
{"section_title": "6.3 Pointer Mapping", "chunk": ""}
{"section_title": "6.3 Pointer Mapping", "chunk": "2 Pointers that contain host addresses require that those addresses are translated to device addresses \n3 for them to be useful in the context of a device data environment. Broadly speaking, there are two \n4 scenarios where this is important. \n5 The first scenario is where the pointer is mapped to the device data environment, such that \n6 references to the pointer inside a target region are to the corresponding pointer. Pointer \n7 attachment ensures that the corresponding pointer will contain a device address when all of the \n8 following conditions are true: \n9 \u2022 the pointer is mapped by directive A to a device; \n10 \u2022 a list item that uses the pointer as its base pointer (call it the pointee) is mapped, to the same \n11 device, by directive B, which may be the same as A; \n12 \u2022 the effect of directive B is to create either the corresponding pointer or pointee in the device data \n13 environment of the device. \n14 Given the above conditions, pointer attachment is initiated as a result of directive B and subsequent \n15 references to the pointee list item in a target region that use the pointer will access the \n16 corresponding pointee. The corresponding pointer remains in this attached state until it is removed \n17 from the device data environment. \n18 The second scenario, which is only applicable for C/C++, is where the pointer is implicitly \n19 privatized inside a target construct when it appears as the base pointer to a list item on the \n20 construct and does not appear explicitly as a list item in a map clause, is_device_ptr clause, \n21 or data-sharing attribute clause. This scenario can be further split into two cases: the list item is a \n22 zero-length array section (e.g., p[:0]) or it is not. \n23 If it is a zero-length array section, this will trigger a runtime check on entry to the target region \n24 for a previously mapped list item where the value of the pointer falls within the range of its base \n25 address and ending address. If such a match is found the private pointer is initialized to the device \n26 address corresponding to the value of the original pointer, and otherwise it is initialized to NULL \n27 (or retains its original value if the unified_address requirement is specified for that \n28 compilation unit). \n29 If the list item (again, call it the pointee) is not a zero-length array section, the private pointer will \n30 be initialized such that references in the target region to the pointee list item that use the pointer \n31 will access the corresponding pointee. \n32 The following example shows the basics of mapping pointers with and without associated storage \n33 on the host. \n34 Storage for pointers ptr1 and ptr2 is created on the host. To map storage that is associated with a \n35 pointer on the host, the data can be explicitly mapped as an array section so that the compiler knows \n36 the amount of data to be assigned in the device (to the \u201ccorresponding\u201d data storage area). On the \n37 target construct array sections are mapped; however, the pointer ptr1 is mapped, while ptr2 is \n162 OpenMP Examples Version 5.2.1 - November 2022 \n1 not. Since ptr2 is not explicitly mapped, it is firstprivate. This creates a subtle difference in the way \n2 these pointers can be used. \n3 As a firstprivate pointer, ptr2 can be manipulated on the device; however, as an explicitly mapped \n4 pointer, ptr1 becomes an attached pointer and cannot be manipulated. In both cases the host pointer \n5 is not updated with the device pointer address\u2014as one would expect for distributed memory. The \n6 storage data on the host is updated from the corresponding device data at the end of the target \n7 region. \n8 As a comparison, note that the aray array is automatically mapped, since the compiler knows the \n9 extent of the array. \n10 The pointer ptr3 is used inside the target construct, but it does not appear in a data-mapping or \n11 data-sharing clause. Nor is there a defaultmap clause on the construct to indicate what its \n12 implicit data-mapping or data-sharing attribute should be. For such a case, ptr3 will be implicitly \n13 privatized within the construct and there will be a runtime check to see if the host memory to which \n14 it is pointing has corresponding memory in the device data environment. If this runtime check \n15 passes, the private ptr3 would be initialized to point to the corresponding memory. But in this case \n16 the check does not pass and so it is initialized to null. Since ptr3 is private, the value to which it is \n17 assigned in the target region is not returned into the original ptr3 on the host. \nC / C++ \n18 Example target_ptr_map.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #define N 100 \nS-4 \nS-5 int main() \nS-6 { \nS-7 int *ptr1; \nS-8 int *ptr2; \nS-9 int *ptr3; \nS-10 int aray[N]; \nS-11 \nS-12 ptr1 = (int *)malloc(sizeof(int)*N); \nS-13 ptr2 = (int *)malloc(sizeof(int)*N); \nS-14 \nS-15 #pragma omp target map(ptr1, ptr1[:N]) map(ptr2[:N] ) \nS-16 { \nS-17 for (int i=0; i<N; i++) \nS-18 { \nS-19 ptr1[i] = i; \nS-20 ptr2[i] = i; \nS-21 aray[i] = i; \nS-22 } \nS-23 \nCHAPTER 6. DEVICES 163 \nS-24 //*(++ptr1) = 9; //NOT ALLOWED since ptr1 is an attached pointer \nS-25 *(++ptr2) = 9; // allowed since ptr2 is firstprivate \nS-26 \nS-27 ptr3=(int *)malloc(sizeof(int)*N); // ptr3 is firstprivate \nS-28 // ptr3 value not returned \nS-29 for (int i=0; i<N; i++) ptr3[i] = 5; \nS-30 \nS-31 for (int i=0; i<N; i++) ptr1[i] += ptr3[i]; \nS-32 \nS-33 free(ptr3); // explicitly free allocated storage on device \nS-34 } \nS-35 \nS-36 printf(\" %d %d\\n\",ptr1[1],ptr2[1]); \nS-37 // 6 9 \nS-38 \nS-39 free(ptr1); \nS-40 free(ptr2); \nS-41 return 0; \nS-42 } \nC / C++ \n1 In the following example the global pointer p appears in a declare target directive. Hence, the \n2 pointer p will persist on the device throughout executions in all target regions. \n3 The pointer is also used in an array section of a map clause on a target construct. When the \n4 pointer of storage associated with a declare target directive is mapped, as for the array section p[:N] \n5 in the target construct, the array section on the device is attached to the device pointer p on \n6 entry to the construct, and the value of the device pointer p becomes undefined on exit. (Of course, \n7 storage allocation for the array section on the device will occur before the pointer on the device is \n8 attached.) \nC / C++ \n9 Example target_ptr_map.2.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #define N 100 \nS-4 \nS-5 #pragma omp begin declare target \nS-6 int *p; \nS-7 extern void use_arg_p(int *p, int n); \nS-8 extern void use_global_p( int n); \nS-9 #pragma omp end declare target \nS-10 \nS-11 int main() \nS-12 { \nS-13 int i; \n164 OpenMP Examples Version 5.2.1 - November 2022 \nS-14 p = (int *)malloc(sizeof(int)*N); \nS-15 \nS-16 #pragma omp target map(p[:N]) // device p attached to array section \nS-17 { \nS-18 for (i=0; i<N; i++) p[i] = i; \nS-19 use_arg_p(p, N); \nS-20 use_global_p(N); \nS-21 } // value of host p is preserved \nS-22 \nS-23 printf(\" %3.3d %3.3d\\n\", p[1], p[N-1]); \nS-24 // 003 297 <- output \nS-25 \nS-26 free(p); \nS-27 return 0; \nS-28 } \nS-29 \nS-30 // A #pragma omp begin declare target is optional here \nS-31 // because of prototype spec \nS-32 void use_arg_p(int *q, int n) \nS-33 { \nS-34 int i; \nS-35 for (i=0; i<n; i++) \nS-36 q[i] *= 2; \nS-37 } \nS-38 \nS-39 void use_global_p(int n) \nS-40 { \nS-41 int i; \nS-42 for (i=0; i<n; i++) \nS-43 p[i] += i; // valid since p is in declare target and called from \nS-44 // inside target region where p was attached to \nS-45 // valid memory \nS-46 } \nS-47 // A #pragma omp end declare target is optional here \nS-48 // because of prototype spec \nC / C++ \n1 The following two examples illustrate subtle differences in pointer attachment to device address \n2 because of the order of data mapping. \n3 In example target_ptr_map.3a the global pointer p1 points to array x and p2 points to array y on the \n4 host. The array section x[:N] is mapped by the target enter data directive while array y is \n5 mapped on the target construct. Since the begin declare target directive is applied to the \n6 declaration of p1, p1 is a treated like a mapped variable on the target construct and references to \n7 p1 inside the construct will be to the corresponding p1 that exists on the device. However, the \n8 corresponding p1 will be undefined since there is no pointer attachment for it. Pointer attachment \n9 for p1 would require that (1) p1 (or an lvalue expression that refers to the same storage as p1) \nCHAPTER 6. DEVICES 165 \n1 appears as a base pointer to a list item in a map clause, and (2) the construct that has the map \n2 clause causes the list item to transition from not mapped to mapped. The conditions are clearly not \n3 satisfied for this example. \n4 The problem for p2 in this example is also subtle. It will be privatized inside the target \n5 construct, with a runtime check for whether the memory to which it is pointing has corresponding \n6 memory that is accessible on the device. If this check is successful, then the p2 inside the construct \n7 would be appropriately initialized to point to that corresponding memory. Unfortunately, despite \n8 there being an implicit map of the array y (to which p2 is pointing) on the construct, the order of \n9 this map relative to the initialization of p2 is unspecified. Therefore, the initial value of p2 will also \n10 be undefined. \n11 Thus, referencing values via either p1 or p2 inside the target region would be invalid. \nC / C++ \n12 Example target_ptr_map.3a.c (omp_5.1) \nS-1 #define N 100 \nS-2 \nS-3 int x[N], y[N]; \nS-4 #pragma omp begin declare target \nS-5 int *p1; \nS-6 #pragma omp end declare target \nS-7 int *p2; \nS-8 \nS-9 int foo() \nS-10 { \nS-11 p1 = &x[0]; \nS-12 p2 = &y[0]; \nS-13 \nS-14 // Explicitly map array section x[:N] \nS-15 #pragma omp target enter data map(x[:N]) \nS-16 \nS-17 #pragma omp target // as if .. map(p1) map(p1[:0]) map(p2[:0]) map(y) \nS-18 { \nS-19 // Accessing the mapped arrays x,y is OK here. \nS-20 x[0] = 1; \nS-21 y[1] = 2; \nS-22 \nS-23 // Pointer attachment for p1 does not occur here \nS-24 // because p1[:0] does not allocate a new array section and \nS-25 // array x is present on the target construct as it was mapped \nS-26 // before by the target enter data directive. \nS-27 p1[0] = 3; // accessing p1 is undefined \nS-28 \nS-29 // The initial value of p2 in the target region is undefined \nS-30 // because map(y) may occur after map(p2[:0]). \n166 OpenMP Examples Version 5.2.1 - November 2022 \nS-31 p2[1] = 4; // accessing p2 is undefined \nS-32 \nS-33 } \nS-34 return 0; \nS-35 } \nC / C++ \n1 In example target_ptr_map.3b the mapping orders for arrays x and y were rearranged to allow \n2 proper pointer attachments. On the target construct, the map(x) clause triggers pointer \n3 attachment for p1 to the device address of x. Pointer p2 is assigned the device address of the \n4 previously mapped array y. Referencing values via either p1 or p2 inside the target region is \n5 now valid. \nC / C++ \n6 Example target_ptr_map.3b.c (omp_5.1) \nS-1 #define N 100 \nS-2 \nS-3 int x[N], y[N]; \nS-4 #pragma omp begin declare target \nS-5 int *p1; \nS-6 #pragma omp end declare target \nS-7 int *p2; \nS-8 \nS-9 int foo() \nS-10 { \nS-11 p1 = &x[0]; \nS-12 p2 = &y[0]; \nS-13 \nS-14 // Explicitly map array section y[:N] \nS-15 #pragma omp target enter data map(y[:N]) \nS-16 \nS-17 #pragma omp target map(x[:N]) map(p1[:N]) map(p2[:0]) \nS-18 { \nS-19 // Accessing the mapped arrays x,y is OK here. \nS-20 x[0] = 1; \nS-21 y[1] = 2; \nS-22 \nS-23 // Pointer attachment for p1 occurs here when array x is mapped \nS-24 // on the target construct (as p1 = &x[0] on the device) \nS-25 p1[0] = 3; // accessing p1 is OK \nS-26 \nS-27 // p2 in the target region is initialized to &y[0] \nS-28 p2[1] = 4; // accessing p2 is OK \nS-29 } \nS-30 \nCHAPTER 6. DEVICES 167 \nS-31 return 0; \nS-32 } \nC / C++ \n1 In the following example, storage allocated on the host is not mapped in a target region if it is \n2 determined that the host memory is accessible from the device. On platforms that support host \n3 memory access from a target device, it may be more efficient to omit map clauses and avoid the \n4 potential memory allocation and data transfers that may result from the map. The \n5 omp_target_is_accessible API routine is used to determine if the host storage of size \n6 buf_size is accessible on the device, and a metadirective is used to select the directive variant (a \n7 target with/without a map clause). \n8 The omp_target_is_accessible routine will return true if the storage indicated by the first \n9 and second arguments is accessible on the target device. In this case, the host pointer ptr may be \n10 directly dereferenced in the subsequent target region to access this storage, rather than mapping \n11 an array section based off the pointer. By explicitly specifying the host pointer in a \n12 firstprivate clause on the construct, its original value will be used directly in the target \n13 region. In OpenMP 5.1, removing the firstprivate clause will result in an implicit presence \n14 check of the storage to which ptr points, and since this storage is not mapped by the program, ptr \n15 will be NULL-initialized in the target region. In the next version of the OpenMP Specification, \n16 a false presence check without the firstprivate clause will cause the pointer to retain its \n17 original value. \nC / C++ \n18 Example target_ptr_map.4.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <omp.h> \nS-4 \nS-5 void do_work(int *ptr, const int size); \nS-6 \nS-7 int main() \nS-8 { \nS-9 const int n = 1000; \nS-10 const int buf_size = sizeof(int) * n; \nS-11 const int dev = omp_get_default_device(); \nS-12 \nS-13 int *ptr = (int *) malloc(buf_size); // possibly compiled on \nS-14 // Unified Shared Memory system \nS-15 const int accessible = omp_target_is_accessible(ptr, buf_size, dev); \nS-16 \nS-17 #pragma omp metadirective \\ \nS-18 when(user={condition(accessible)}: target firstprivate(ptr) ) \\ \nS-19 otherwise( target map(ptr[:n]) ) \nS-20 { \n168 OpenMP Examples Version 5.2.1 - November 2022 \nS-21 do_work(ptr, n); \nS-22 } \nS-23 \nS-24 free(ptr); \nS-25 return 0; \nS-26 } \nC / C++ \n1 Similar to the previous example, the omp_target_is_accessible routine is used to discover \n2 if a deep copy is required for the platform. Here, the deep_copy map, defined in the \n3 declare mapper directive, is used if the host storage referenced by s.ptr (or s%ptr in Fortran) is \n4 not accessible from the device. \nC / C++ \n5 Example target_ptr_map.5.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <omp.h> \nS-4 \nS-5 typedef struct { \nS-6 int *ptr; \nS-7 int buf_size; \nS-8 } T; \nS-9 \nS-10 #pragma omp declare mapper(deep_copy: T s) map(s, s.ptr[:s.buf_size]) \nS-11 \nS-12 void do_work(int *ptr, const int size); \nS-13 \nS-14 int main() \nS-15 { \nS-16 const int n = 1000; \nS-17 const int buf_size = sizeof(int) * n; \nS-18 T s = { 0, buf_size }; \nS-19 const int dev = omp_get_default_device(); \nS-20 s.ptr = (int *)malloc(buf_size); \nS-21 const int accessible = \nS-22 omp_target_is_accessible(s.ptr, s.buf_size, dev); \nS-23 \nS-24 #pragma omp metadirective \\ \nS-25 when(user={condition(accessible)}: target) \\ \nS-26 otherwise(target map(mapper(deep_copy),tofrom:s) ) \nS-27 { \nS-28 do_work(s.ptr, n); \nS-29 } \nS-30 \nS-31 free(s.ptr); \nCHAPTER 6. DEVICES 169 \nS-32 return 0; \nS-33 } \nC / C++ \nFortran \n1 Example target_ptr_map.5.f90 (omp_5.2) \nS-1 program main \nS-2 use omp_lib \nS-3 \nS-4 use, intrinsic :: iso_c_binding, only : c_loc, c_size_t, c_sizeof, c_int \nS-5 implicit none \nS-6 external :: do_work \nS-7 \nS-8 type T \nS-9 integer,pointer :: ptr(:) \nS-10 integer :: buf_size \nS-11 end type \nS-12 \nS-13 !$omp declare mapper(deep_copy: T :: s) map(s, s%ptr(:s%buf_size)) \nS-14 \nS-15 integer,parameter :: n = 1000 \nS-16 integer(c_int) :: dev, accessible \nS-17 integer(c_size_t) :: buf_size \nS-18 \nS-19 type(T) s \nS-20 \nS-21 allocate(s%ptr(n)) \nS-22 \nS-23 buf_size = c_sizeof(s%ptr(1))*n \nS-24 dev = omp_get_default_device() \nS-25 \nS-26 accessible = omp_target_is_accessible(c_loc(s%ptr(1)), buf_size, dev) \nS-27 \nS-28 !$omp begin metadirective & \nS-29 !$omp& when(user={condition(accessible)}: target) & \nS-30 !$omp& otherwise( target map(mapper(deep_copy),tofrom:s) ) \nS-31 \nS-32 call do_work(s, n) \nS-33 \nS-34 !$omp end metadirective \nS-35 \nS-36 deallocate(s%ptr) \nS-37 \nS-38 end program \nFortran \n170 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.4 Structure Mapping", "chunk": ""}
{"section_title": "6.4 Structure Mapping", "chunk": "2 In the example below, only structure elements S.a, S.b and S.p of the S structure appear in map \n3 clauses of a target construct. Only these components have corresponding variables and storage \n4 on the device. Hence, the large arrays, S.buffera and S.bufferb, and the S.x component have no \n5 storage on the device and cannot be accessed. \n6 Also, since the pointer member S.p is used in an array section of a map clause, the array storage of \n7 the array section on the device, S.p[:N], is attached to the pointer member S.p on the device. \n8 Explicitly mapping the pointer member S.p is optional in this case. \n9 Note: The buffer arrays and the x variable have been grouped together, so that the components that \n10 will reside on the device are all together (without gaps). This allows the runtime to optimize the \n11 transfer and the storage footprint on the device. \nC / C++ \n12 Example target_struct_map.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #define N 100 \nS-4 #define BAZILLION 2000000 \nS-5 \nS-6 struct foo { \nS-7 char buffera[BAZILLION]; \nS-8 char bufferb[BAZILLION]; \nS-9 float x; \nS-10 float a, b; \nS-11 float *p; \nS-12 }; \nS-13 \nS-14 #pragma omp begin declare target \nS-15 void saxpyfun(struct foo *S) \nS-16 { \nS-17 int i; \nS-18 for(i=0; i<N; i++) \nS-19 S->p[i] = S->p[i]*S->a + S->b; \nS-20 } \nS-21 #pragma omp end declare target \nS-22 \nS-23 int main() \nS-24 { \nS-25 struct foo S; \nS-26 int i; \nS-27 \nS-28 S.a = 2.0; \nS-29 S.b = 4.0; \nCHAPTER 6. DEVICES 171 \nS-30 S.p = (float *)malloc(sizeof(float)*N); \nS-31 for(i=0; i<N; i++) S.p[i] = i; \nS-32 \nS-33 #pragma omp target map(alloc:S.p) map(S.p[:N]) map(to:S.a, S.b) \nS-34 saxpyfun(&S); \nS-35 \nS-36 printf(\" %4.0f %4.0f\\n\", S.p[0], S.p[N-1]); \nS-37 // 4 202 <- output \nS-38 \nS-39 free(S.p); \nS-40 return 0; \nS-41 } \nC / C++ \n1 The following example is a slight modification of the above example for a C++ class. In the member \n2 function SAXPY::driver the array section p[:N] is attached to the pointer member p on the device. \nC++ \n3 Example target_struct_map.2.cpp (omp_5.1) \nS-1 #include <cstdio> \nS-2 #include <cstdlib> \nS-3 #define N 100 \nS-4 \nS-5 class SAXPY { \nS-6 private: \nS-7 float a, b, *p; \nS-8 public: \nS-9 float buffer[N]; \nS-10 \nS-11 SAXPY(float arg_a, float arg_b){ a=arg_a; b=arg_b; } \nS-12 void driver(); \nS-13 void saxpyfun(float *p); \nS-14 }; \nS-15 \nS-16 #pragma omp begin declare target \nS-17 void SAXPY::saxpyfun(float *q) \nS-18 { \nS-19 for(int i=0; i<N; i++) \nS-20 buffer[i] = q[i]*a + b; \nS-21 } \nS-22 #pragma omp end declare target \nS-23 \nS-24 void SAXPY::driver() \nS-25 { \nS-26 p = (float *) malloc(N*sizeof(float)); \nS-27 for(int i=0; i<N; i++) p[i]=i; \n172 OpenMP Examples Version 5.2.1 - November 2022 \nS-28 \nS-29 #pragma omp target map(alloc:p) map(to:p[:N]) map(to:a,b) \\ \nS-30 map(from:buffer[:N]) // attach(p) to device_malloc() \nS-31 { \nS-32 saxpyfun(p); \nS-33 } \nS-34 \nS-35 free(p); \nS-36 } \nS-37 \nS-38 int main() \nS-39 { \nS-40 SAXPY my_saxpy(2.0,4.0); \nS-41 \nS-42 my_saxpy.driver(); \nS-43 \nS-44 printf(\" %4.0f %4.0f\\n\", my_saxpy.buffer[0], my_saxpy.buffer[N-1]); \nS-45 // 4 202 <- output \nS-46 \nS-47 return 0; \nS-48 } \nC++ \n1 The next example shows two ways in which the structure may be incorrectly mapped. \n2 In Case 1, the array section S1.p[:N] is first mapped in an enclosing target data construct, and \n3 the target construct then implicitly maps the structure S1. The initial map of the array section \n4 does not map the base pointer S1.p \u2013 it only maps the elements of the array section. Furthermore, \n5 the implicit map is not sufficient to ensure pointer attachment for the structure member S1.p (refer \n6 to the conditions for pointer attachment described in Section 6.3). Consequentially, the dereference \n7 operation S1.p[i] in the call to saxpyfun will probably fail because S1.p contains a host address. \n8 In Case 2, again an array section is mapped on an enclosing target data construct. This time, \n9 the nested target construct explicitly maps S2.p, S2.a, and S2.b. But as in Case 1, this does not \n10 satisfy the conditions for pointer attachment since the construct must map a list item for which S2.p \n11 is a base pointer, and it must do so when the S2.p is already present on the device or will be created \n12 on the device as a result of the same construct. \nC / C++ \n13 Example target_struct_map.3.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #define N 100 \nS-4 #define BAZILLION 2000000 \nS-5 \nS-6 struct foo { \nCHAPTER 6. DEVICES 173 \nS-7 char buffera[BAZILLION]; \nS-8 char bufferb[BAZILLION]; \nS-9 float x; \nS-10 float a, b; \nS-11 float *p; \nS-12 }; \nS-13 \nS-14 #pragma omp begin declare target \nS-15 void saxpyfun(struct foo *S) \nS-16 { \nS-17 int i; \nS-18 for(i=0; i<N; i++) \nS-19 S->p[i] = S->p[i] * S->a + S->b; // S->p[i] invalid \nS-20 } \nS-21 #pragma omp end declare target \nS-22 \nS-23 int main() \nS-24 { \nS-25 struct foo S1, S2; \nS-26 int i; \nS-27 \nS-28 // Case 1 \nS-29 \nS-30 S1.a = 2.0; \nS-31 S1.b = 4.0; \nS-32 S1.p = (float *)malloc(sizeof(float)*N); \nS-33 for(i=0; i<N; i++) S1.p[i] = i; \nS-34 \nS-35 // No pointer attachment for S1.p here \nS-36 #pragma omp target data map(S1.p[:N]) \nS-37 #pragma omp target // implicit map of S1 \nS-38 saxpyfun(&S1); \nS-39 \nS-40 // Case 2 \nS-41 \nS-42 S2.a = 2.0; \nS-43 S2.b = 4.0; \nS-44 S2.p = (float *)malloc(sizeof(float)*N); \nS-45 for(i=0; i<N; i++) S2.p[i] = i; \nS-46 \nS-47 // No pointer attachment for S2.p here either \nS-48 #pragma omp target data map(S2.p[:N]) \nS-49 #pragma omp target map(S2.p, S2.a, S2.b) // implicit map of S2 \nS-50 saxpyfun(&S2); \nS-51 \nS-52 // These print statement may not execute because the \nS-53 // above code is invalid \n174 OpenMP Examples Version 5.2.1 - November 2022 \nS-54 printf(\" %4.0f %4.0f\\n\", S1.p[0], S1.p[N-1]); \nS-55 printf(\" %4.0f %4.0f\\n\", S2.p[0], S2.p[N-1]); \nS-56 \nS-57 free(S1.p); \nS-58 free(S2.p); \nS-59 return 0; \nS-60 } \nC / C++ \n1 The following example correctly implements pointer attachment cases that involve implicit \n2 structure maps. \n3 In Case 1, members p, a, and b of the structure S1 are explicitly mapped by the target data \n4 construct, to avoid mapping parts of S1 that aren\u2019t required on the device. The mapped S1.p is \n5 attached to the array section S1.p[:N], and remains attached while it exists on the device (for the \n6 duration of target data region). Due to the S1 reference inside the nested target construct, \n7 the construct implicitly maps S1 so that the reference refers to the corresponding storage created by \n8 the enclosing target data region. Note that only the members a, b, and p may be accessed from \n9 this storage. \n10 In Case 2, only the storage for the array section S2.p[:N] is mapped by the target data \n11 construct. The nested target construct explicitly maps S2.a and S2.b and explicitly maps an array \n12 section for which S2.p is a base pointer. This satisfies the conditions for S2.p becoming an attached \n13 pointer. The array section in this case is zero-length, but the effect would be the same if the length \n14 was a positive integer less than or equal to N. There is also an implicit map of the containing \n15 structure S2, again due to the reference to S2 inside the construct. The effect of this implicit map \n16 permits access only to members a, b, and p, as for Case 1. \n17 In Case 3, there is no target data construct. The target construct explicitly maps S3.a and \n18 S3.b and explicitly maps an array section for which S3.p is a base pointer. Again, there is an \n19 implicit map of the structure referenced in the construct, S3. This implicit map also causes S3.p to \n20 be implicitly mapped, because no other part of S3 is present prior to the construct being \n21 encountered. The result is an attached pointer S3.p on the device. As for Cases 1 and 2, this implicit \n22 map only ensures that storage for the members a, b, and p are accessible within the corresponding \n23 S3 that is created on the device. \nC / C++ \n24 Example target_struct_map.4.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #define N 100 \nS-4 #define BAZILLION 2000000 \nS-5 \nS-6 struct foo { \nS-7 char buffera[BAZILLION]; \nS-8 char bufferb[BAZILLION]; \nCHAPTER 6. DEVICES 175 \nS-9 float x; \nS-10 float a, b; \nS-11 float *p; \nS-12 }; \nS-13 \nS-14 #pragma omp begin declare target \nS-15 void saxpyfun(struct foo *S) \nS-16 { \nS-17 int i; \nS-18 for(i=0; i<N; i++) \nS-19 S->p[i] = S->p[i]*S->a + S->b; \nS-20 } \nS-21 #pragma omp end declare target \nS-22 \nS-23 int main() \nS-24 { \nS-25 struct foo S1, S2, S3; \nS-26 int i; \nS-27 \nS-28 // Case 1 \nS-29 \nS-30 S1.a = 2.0; \nS-31 S1.b = 4.0; \nS-32 S1.p = (float *)malloc(sizeof(float)*N); \nS-33 for(i=0; i<N; i++) S1.p[i] = i; \nS-34 \nS-35 // The target data construct results in pointer attachment for S1.p. \nS-36 // Explicitly mapping S1.p, S1.a, and S1.b rather than S1 avoids \nS-37 // mapping the entire structure (including members buffera, bufferb, \nS-38 // and x). \nS-39 #pragma omp target data map(S1.p[:N],S1.p,S1.a,S1.b) \nS-40 #pragma omp target //implicit map of S1 \nS-41 saxpyfun(&S1); \nS-42 \nS-43 \nS-44 // Case 2 \nS-45 \nS-46 S2.a = 2.0; \nS-47 S2.b = 4.0; \nS-48 S2.p = (float *)malloc(sizeof(float)*N); \nS-49 for(i=0; i<N; i++) S2.p[i] = i; \nS-50 \nS-51 // The target construct results in pointer attachment for S2.p. \nS-52 #pragma omp target data map(S2.p[:N]) \nS-53 #pragma omp target map(S2.p[:0], S2.a, S2.b) // implicit map of S2 \nS-54 saxpyfun(&S2); \nS-55 \n176 OpenMP Examples Version 5.2.1 - November 2022 \nS-56 // Case 3 \nS-57 \nS-58 S3.a = 2.0; \nS-59 S3.b = 4.0; \nS-60 S3.p = (float *)malloc(sizeof(float)*N); \nS-61 for(i=0; i<N; i++) S3.p[i] = i; \nS-62 \nS-63 // The target construct results in pointer attachment for S3.p. \nS-64 // Note that S3.p is implicitly mapped due to the implicit map of S3 \nS-65 // (but corresponding storage is NOT created for members buffera, \nS-66 // bufferb, and x). \nS-67 #pragma omp target map(S3.p[:N], S3.a, S3.b) // implicit map of S3 \nS-68 saxpyfun(&S3); \nS-69 \nS-70 printf(\" %4.0f %4.0f\\n\", S1.p[0], S1.p[N-1]); //OUT1 4 202 \nS-71 printf(\" %4.0f %4.0f\\n\", S2.p[0], S2.p[N-1]); //OUT2 4 202 \nS-72 printf(\" %4.0f %4.0f\\n\", S3.p[0], S3.p[N-1]); //OUT3 4 202 \nS-73 \nS-74 free(S1.p); \nS-75 free(S2.p); \nS-76 free(S3.p); \nS-77 return 0; \nS-78 } \nC / C++ \nCHAPTER 6. DEVICES 177 \n"}
{"section_title": "6.5 Fortran Allocatable Array Mapping", "chunk": ""}
{"section_title": "6.5 Fortran Allocatable Array Mapping", "chunk": "2 The following examples illustrate the use of Fortran allocatable arrays in target regions. \n3 In the first example, allocatable variables (a and b) are first allocated on the host, and then mapped \n4 onto a device in the Target 1 and 2 sections, respectively. For a the map is implicit and for b an \n5 explicit map is used. Both are mapped with the default tofrom map type. The user-level behavior \n6 is similar to non-allocatable arrays. However, the mapping operations include creation of the \n7 allocatable variable, creation of the allocated storage, setting the allocation status to allocated, and \n8 making sure the allocatable variable references the storage. \n9 In Target 3 and 4 sections, allocatable variables are mapped in two different ways before they are \n10 allocated on the host and subsequently used on the device. In one case, a target data construct \n11 creates an enclosing region for the allocatable variable to persist, and in the other case a \n12 declare target directive maps the allocation variable for all device executions. In both cases \n13 the new array storage is mapped tofrom with the always modifier. An explicit map is used here \n14 with an always modifier to ensure that the allocatable variable status is updated on the device. \n15 Note: OpenMP 5.1 specifies that an always map modifier guarantees the allocation status update \n16 for an existing allocatable variable on the device. In OpenMP 6.0, this restriction may be relaxed to \n17 also guarantee updates without the always modifier. \n18 In Target 3 and 4 sections, the behavior of an allocatable variable is very much like a Fortran \n19 pointer, in which a pointer can be mapped to a device with an associated or disassociated status, and \n20 associated storage can be mapped and attached as needed. For allocatable variables, the update of \n21 the allocation status to allocated (allowing reference to allocated storage) on the device, is similar to \n22 pointer attachment. \nFortran \n23 Example target_fort_allocatable_map.1.f90 (omp_5.1) \nS-1 program main \nS-2 implicit none \nS-3 integer :: i \nS-4 \nS-5 integer, save, allocatable :: d(:) \nS-6 !$omp declare target(d) \nS-7 \nS-8 integer, allocatable :: a(:) \nS-9 integer, allocatable :: b(:) \nS-10 integer, allocatable :: c(:) \nS-11 \nS-12 allocate(a(4)) \nS-13 !$omp target ! Target 1 \nS-14 a(:) = 4 \nS-15 !$omp end target \nS-16 print *, a ! prints 4*4 \n178 OpenMP Examples Version 5.2.1 - November 2022 \nS-17 \nS-18 allocate(b(4)) \nS-19 !$omp target map(b) ! Target 2 \nS-20 b(:) = 4 \nS-21 !$omp end target \nS-22 print *, b ! prints 4*4 \nS-23 \nS-24 !$omp target data map(c) \nS-25 \nS-26 allocate(c(4), source=[1,2,3,4]) \nS-27 !$omp target map(always,tofrom:c) ! Target 3 \nS-28 c(:) = 4 \nS-29 !$omp end target \nS-30 print *, c ! prints 4*4 \nS-31 \nS-32 deallocate(c) \nS-33 \nS-34 !$omp end target data \nS-35 \nS-36 allocate(d(4), source=[1,2,3,4]) \nS-37 !$omp target map(always,tofrom:d) ! Target 4 \nS-38 d(:) = d(:) + [ ( i,i=size(d),1,-1) ] \nS-39 !$omp end target \nS-40 print *, d ! prints 4*5 \nS-41 \nS-42 deallocate(a, b, d) \nS-43 \nS-44 end program \nFortran \n1 Once an allocatable variable has been allocated on the host, its allocation status may not be changed \n2 in a target region, either explicitly or implicitly. The following example illustrates typical \n3 operations on allocatable variables that violate this restriction. Note, an assignment that reshapes or \n4 reassigns (causing a deallocation and allocation) in a target region is not conforming. Also, an \n5 initial intrinsic assignment of an allocatable variable requires deallocation before the target \n6 region ends. \nFortran \n7 Example target_fort_allocatable_map.2.f90 (omp_5.1) \nS-1 program main \nS-2 implicit none \nS-3 \nS-4 integer, allocatable :: a(:,:), b(:), c(:) \nS-5 integer :: x(10,2) \nS-6 \nS-7 allocate(a(2,10)) \nCHAPTER 6. DEVICES 179 \nS-8 \nS-9 !$omp target \nS-10 a = x ! Reshape (or resize) NOT ALLOWED (implicit change) \nS-11 \nS-12 deallocate(a) ! Allocation status change of \"a\" NOT ALLOWED. \nS-13 \nS-14 allocate(b(20)) ! Allocation of b * \nS-15 \nS-16 c = 10 ! Intrinsic assignment allocates c * \nS-17 \nS-18 ! * Since an explicit deallocation for b and c does not occur before \nS-19 ! the end of the target region, the PROGRAM BEHAVIOR IS UNSPECIFIED. \nS-20 !$omp end target \nS-21 \nS-22 end program \nFortran \n1 The next example illustrates a corner case of this restriction (allocatable status change in a target \n2 region). Two allocatable arrays are passed to a subroutine within a target region. The \n3 dummy-variable arrays are declared allocatable. Also, the ain variable has the intent(in) attribute, \n4 and bout has the intent(out) attribute. For the dummy argument with the attributes allocatable and \n5 intent(out), the compiler will deallocate the associated actual argument when the subroutine is \n6 invoked. (However, the allocation on procedure entry can be avoided by specifying the intent as \n7 intent(inout), making the intended use conforming.) \nFortran \n8 Example target_fort_allocatable_map.3.f90 (omp_5.1) \nS-1 module corfu \nS-2 contains \nS-3 subroutine foo(ain,bout) \nS-4 implicit none \nS-5 integer, allocatable, intent( in) :: ain(:) \nS-6 integer, allocatable, intent(out) :: bout(:) !\"out\" causes de/realloc \nS-7 !$omp declare target \nS-8 bout = ain \nS-9 end subroutine \nS-10 end module \nS-11 \nS-12 program main \nS-13 use corfu \nS-14 implicit none \nS-15 \nS-16 integer, allocatable :: a(:) \nS-17 integer, allocatable :: b(:) \nS-18 allocate(a(10),b(10)) \nS-19 a(:)=10 \n180 OpenMP Examples Version 5.2.1 - November 2022 \nS-20 b(:)=10 \nS-21 \nS-22 !$omp target \nS-23 \nS-24 call foo(a,b) !ERROR: b deallocation/reallocation not allowed \nS-25 ! in target region \nS-26 \nS-27 !$omp end target \nS-28 \nS-29 end program \nFortran \nCHAPTER 6. DEVICES 181 \n"}
{"section_title": "6.6 Array Sections in Device Constructs", "chunk": ""}
{"section_title": "6.6 Array Sections in Device Constructs", "chunk": "2 The following examples show the usage of array sections in map clauses on target and target \n3 data constructs. \n4 This example shows the invalid usage of two separate sections of the same array inside of a \n5 target construct. \nC / C++ \n6 Example array_sections.1.c (omp_4.0) \nS-1 void foo () \nS-2 { \nS-3 int A[30]; \nS-4 #pragma omp target data map( A[0:4] ) \nS-5 { \nS-6 /* Cannot map distinct parts of the same array */ \nS-7 #pragma omp target map( A[7:20] ) \nS-8 { \nS-9 A[2] = 0; \nS-10 } \nS-11 } \nS-12 } \nC / C++ \nFortran \n7 Example array_sections.1.f90 (omp_4.0) \nS-1 subroutine foo() \nS-2 integer :: A(30) \nS-3 A = 1 \nS-4 !$omp target data map( A(1:4) ) \nS-5 ! Cannot map distinct parts of the same array \nS-6 !$omp target map( A(8:27) ) \nS-7 A(3) = 0 \nS-8 !$omp end target \nS-9 !$omp end target data \nS-10 end subroutine \nFortran \n182 OpenMP Examples Version 5.2.1 - November 2022 \n1 This example shows the invalid usage of two separate sections of the same array inside of a \n2 target construct. \nC / C++ \n3 Example array_sections.2.c (omp_4.0) \nS-1 void foo () \nS-2 { \nS-3 int A[30], *p; \nS-4 #pragma omp target data map( A[0:4] ) \nS-5 { \nS-6 p = &A[0]; \nS-7 /* invalid because p[3] and A[3] are the same \nS-8 * location on the host but the array section \nS-9 * specified via p[...] is not a subset of A[0:4] */ \nS-10 #pragma omp target map( p[3:20] ) \nS-11 { \nS-12 A[2] = 0; \nS-13 p[8] = 0; \nS-14 } \nS-15 } \nS-16 } \nC / C++ \nFortran \n4 Example array_sections.2.f90 (omp_4.0) \nS-1 subroutine foo() \nS-2 integer,target :: A(30) \nS-3 integer,pointer :: p(:) \nS-4 A=1 \nS-5 !$omp target data map( A(1:4) ) \nS-6 p=>A \nS-7 ! invalid because p(4) and A(4) are the same \nS-8 ! location on the host but the array section \nS-9 ! specified via p(...) is not a subset of A(1:4) \nS-10 !$omp target map( p(4:23) ) \nS-11 A(3) = 0 \nS-12 p(9) = 0 \nS-13 !$omp end target \nS-14 !$omp end target data \nS-15 end subroutine \nFortran \nCHAPTER 6. DEVICES 183 \n1 This example shows the valid usage of two separate sections of the same array inside of a target \n2 construct. \nC / C++ \n3 Example array_sections.3.c (omp_4.0) \nS-1 void foo () \nS-2 { \nS-3 int A[30], *p; \nS-4 #pragma omp target data map( A[0:4] ) \nS-5 { \nS-6 p = &A[0]; \nS-7 #pragma omp target map( p[7:20] ) \nS-8 { \nS-9 A[2] = 0; \nS-10 p[8] = 0; \nS-11 } \nS-12 } \nS-13 } \nC / C++ \nFortran \n4 Example array_sections.3.f90 (omp_4.0) \nS-1 subroutine foo() \nS-2 integer,target :: A(30) \nS-3 integer,pointer :: p(:) \nS-4 !$omp target data map( A(1:4) ) \nS-5 p=>A \nS-6 !$omp target map( p(8:27) ) \nS-7 A(3) = 0 \nS-8 p(9) = 0 \nS-9 !$omp end target \nS-10 !$omp end target data \nS-11 end subroutine \nFortran \n184 OpenMP Examples Version 5.2.1 - November 2022 \n1 This example shows the valid usage of a wholly contained array section of an already mapped array \n2 section inside of a target construct. \nC / C++ \n3 Example array_sections.4.c (omp_4.0) \nS-1 void foo () \nS-2 { \nS-3 int A[30], *p; \nS-4 #pragma omp target data map( A[0:10] ) \nS-5 { \nS-6 p = &A[0]; \nS-7 #pragma omp target map( p[3:7] ) \nS-8 { \nS-9 A[2] = 0; \nS-10 p[8] = 0; \nS-11 A[8] = 1; \nS-12 } \nS-13 } \nS-14 } \nC / C++ \nFortran \n4 Example array_sections.4.f90 (omp_4.0) \nS-1 subroutine foo() \nS-2 integer,target :: A(30) \nS-3 integer,pointer :: p(:) \nS-4 !$omp target data map( A(1:10) ) \nS-5 p=>A \nS-6 !$omp target map( p(4:10) ) \nS-7 A(3) = 0 \nS-8 p(9) = 0 \nS-9 A(9) = 1 \nS-10 !$omp end target \nS-11 !$omp end target data \nS-12 end subroutine \nFortran \nCHAPTER 6. DEVICES 185 \n"}
{"section_title": "6.7 C++ Virtual Functions", "chunk": ""}
{"section_title": "6.7 C++ Virtual Functions", "chunk": "2 The 5.2 OpenMP Specification clarified restrictions on the use of polymorphic classes and virtual \n3 functions when used within target regions. The following example identifies problem cases in \n4 which the restrictions are not followed (for Unified Shared Memory, as prescribed by the \n5 requires directive). \n6 The first section illustrates the restriction that when mapping an object for the first time, the static \n7 and dynamic types must match. \n8 For the first target region the behavior of the implicit map of ar is not specified\u2013 its static type (A) \n9 doesn\u2019t match its dynamic type (D). Hence access to the virtual functions is undefined. However, \n10 the second target region can access D::vf() since the object to which ap points is not mapped and \n11 therefore the restriction does not apply. \n12 The second section illustrates the restriction: \n13 \u201cInvoking a virtual member function of an object on a device other than the device on which the \n14 object was constructed results in unspecified behavior, unless the object is accessible and was \n15 constructed on the host device.\u201d \n16 An instantiation of a polymorphic class (A) occurs in the target region, and access of its virtual \n17 function is incorrectly attempted on the host (another device). However, once the object is deleted \n18 on the target device and instantiated on the host, access within the next target region is permitted. \nC++ \n19 Example virtual_functions.1.cpp (omp_5.2) \nS-1 #include <iostream> \nS-2 #pragma omp requires unified_shared_memory \nS-3 \nS-4 #pragma omp begin declare target \nS-5 class A { \nS-6 public: \nS-7 virtual void vf() { std::cout << \"In A\\n\"; } \nS-8 }; \nS-9 \nS-10 class D: public A { \nS-11 public: \nS-12 void vf() override { std::cout << \"In D\\n\"; } \nS-13 }; \nS-14 #pragma omp end declare target \nS-15 \nS-16 int main(){ \nS-17 \nS-18 // Section 1 -------------------------------------------------------- \nS-19 D d; // D derives from A, and A::vf() is virtual \nS-20 A &ar = d; // reference to Derived object d \n186 OpenMP Examples Version 5.2.1 - November 2022 \nS-21 \nS-22 #pragma omp target // implicit map of ar is illegal here \nS-23 { \nS-24 ar.vf(); // unspecified whether A::vf() or D::vf() is called \nS-25 } \nS-26 \nS-27 A *ap = &d; // pointer to derived object d \nS-28 #pragma omp target // No need for mapping with Unified Share Memory \nS-29 { // implicit ap[:0] map is fine \nS-30 ap->vf(); // calls D::vf() \nS-31 } \nS-32 \nS-33 // Section 2 -------------------------------------------------------- \nS-34 ap = nullptr; \nS-35 #pragma omp target map(ap) \nS-36 { \nS-37 ap = new A(); \nS-38 } \nS-39 \nS-40 ap->vf(); // illegal \nS-41 \nS-42 #pragma omp target \nS-43 { \nS-44 delete ap; \nS-45 } \nS-46 ap = new A(); \nS-47 #pragma omp target // No need for mapping with Unified Share Memory \nS-48 { \nS-49 ap->vf(); // ok \nS-50 } \nS-51 \nS-52 return 0; \nS-53 } \nC++ \n"}
{"section_title": "6.8 Array Shaping", "chunk": ""}
{"section_title": "6.8 Array Shaping", "chunk": "C / C++ \n2 A pointer variable can be shaped to a multi-dimensional array to facilitate data access. This is \n3 achieved by a shape-operator casted in front of a pointer (lvalue expression): \n4 ([s1][s2]...[sn])pointer \nCHAPTER 6. DEVICES 187 \nwhere each si 1 is an integral-type expression of positive value. The shape-operator can appear in \n2 either the motion-clause of the target update directive or the depend clause. \n3 The following example shows the use of the shape-operator in the target update directive. The \n4 shape-operator ([nx][ny+2]) casts pointer variable a to a 2-dimensional array of size \n5 nx\u00d7(ny+2). The resulting array is then accessed as array sections (such as [0:nx][1] and \n6 [0:nx][ny]) in the from or to clause for transferring two columns of noncontiguous boundary \n7 data from or to the device. Note the use of additional parentheses around the shape-operator and a \n8 to ensure the correct precedence over array-section operations. \n9 Example array_shaping.1.c (omp_5.1) \nS-1 #pragma omp begin declare target \nS-2 int do_work(double *a, int nx, int ny); \nS-3 int other_work(double *a, int nx, int ny); \nS-4 #pragma omp end declare target \nS-5 \nS-6 void exch_data(double *a, int nx, int ny); \nS-7 \nS-8 void array_shaping(double *a, int nx, int ny) \nS-9 { \nS-10 // map data to device and do work \nS-11 #pragma omp target data map(a[0:nx*(ny+2)]) \nS-12 { \nS-13 // do work on the device \nS-14 #pragma omp target // map(a[0:nx*(ny+2)]) is optional here \nS-15 do_work(a, nx, ny); \nS-16 \nS-17 // update boundary points (two columns of 2D array) on the host \nS-18 // pointer is shaped to 2D array using the shape-operator \nS-19 #pragma omp target update from( (([nx][ny+2])a)[0:nx][1], \\ \nS-20 (([nx][ny+2])a)[0:nx][ny] ) \nS-21 \nS-22 // exchange ghost points with neighbors \nS-23 exch_data(a, nx, ny); \nS-24 \nS-25 // update ghost points (two columns of 2D array) on the device \nS-26 // pointer is shaped to 2D array using the shape-operator \nS-27 #pragma omp target update to( (([nx][ny+2])a)[0:nx][0], \\ \nS-28 (([nx][ny+2])a)[0:nx][ny+1] ) \nS-29 \nS-30 // perform other work on the device \nS-31 #pragma omp target // map(a[0:nx*(ny+2)]) is optional here \nS-32 other_work(a, nx, ny); \nS-33 } \nS-34 } \nC / C++ \n188 OpenMP Examples Version 5.2.1 - November 2022 \n1 The shape operator is not defined for Fortran. Explicit array shaping of procedure arguments can be \n2 used instead to achieve a similar goal. Below is the Fortran-equivalent of the above example that \n3 illustrates the support of transferring two rows of noncontiguous boundary data in the \n4 target update directive. \nFortran \n5 Example array_shaping.1.f90 (omp_5.2) \nS-1 module m \nS-2 interface \nS-3 subroutine do_work(a, nx, ny) \nS-4 !$omp declare target enter(do_work) \nS-5 integer, intent(in) :: nx, ny \nS-6 double precision a(0:nx+1,ny) \nS-7 end subroutine do_work \nS-8 \nS-9 subroutine other_work(a, nx, ny) \nS-10 !$omp declare target enter(other_work) \nS-11 integer, intent(in) :: nx, ny \nS-12 double precision a(0:nx+1,ny) \nS-13 end subroutine other_work \nS-14 \nS-15 subroutine exch_data(a, nx, ny) \nS-16 integer, intent(in) :: nx, ny \nS-17 double precision a(0:nx+1,ny) \nS-18 end subroutine exch_data \nS-19 end interface \nS-20 end module m \nS-21 \nS-22 subroutine array_shaping(a, nx, ny) \nS-23 use m \nS-24 implicit none \nS-25 integer, intent(in) :: nx, ny \nS-26 double precision a(0:nx+1,ny) \nS-27 \nS-28 ! map data to device and do work \nS-29 !$omp target data map(a) \nS-30 \nS-31 ! do work on the device \nS-32 !$omp target ! map(a) is optional here \nS-33 call do_work(a, nx, ny) \nS-34 !$omp end target \nS-35 \nS-36 ! update boundary points (two rows of 2D array) on the host. \nS-37 ! data transferred are noncontiguous \nS-38 !$omp target update from( a(1,1:ny), a(nx,1:ny) ) \nS-39 \nCHAPTER 6. DEVICES 189 \nS-40 ! exchange ghost points with neighbors \nS-41 call exch_data(a, nx, ny) \nS-42 \nS-43 ! update ghost points (two rows of 2D array) on the device. \nS-44 ! data transferred are noncontiguous \nS-45 !$omp target update to( a(0,1:ny), a(nx+1,1:ny) ) \nS-46 \nS-47 ! perform other work on the device \nS-48 !$omp target ! map(a) is optional here \nS-49 call other_work(a, nx, ny) \nS-50 !$omp end target \nS-51 \nS-52 !$omp end target data \nS-53 \nS-54 end subroutine \nFortran \n190 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.9 declare mapper Directive", "chunk": ""}
{"section_title": "6.9 declare mapper Directive", "chunk": "2 The following examples show how to use the declare mapper directive to prescribe a map for \n3 later use. It is also quite useful for pre-defining partitioned and nested structure elements. \n4 In the first example the declare mapper directive specifies that any structure of type myvec_t \n5 for which implicit data-mapping rules apply will be mapped according to its map clause. The \n6 variable v is used for referencing the structure and its elements within the map clause. Within the \n7 map clause the v variable specifies that all elements of the structure are to be mapped. Additionally, \n8 the array section v.data[0:v.len] specifies that the dynamic storage for data is to be mapped. \n9 Within the main program the s variable is typed as myvec_t. Since the variable is found within the \n10 target region and the type has a mapping prescribed by a declare mapper directive, it will be \n11 automatically mapped according to its prescription: full structure, plus the dynamic storage of the \n12 data element. \nC / C++ \n13 Example target_mapper.1.c (omp_5.0) \nS-1 #include <stdlib.h> \nS-2 #include <stdio.h> \nS-3 #define N 100 \nS-4 \nS-5 typedef struct myvec{ \nS-6 size_t len; \nS-7 double *data; \nS-8 } myvec_t; \nS-9 \nS-10 #pragma omp declare mapper(myvec_t v) \\ \nS-11 map(v, v.data[0:v.len]) \nS-12 void init(myvec_t *s); \nS-13 \nS-14 int main(){ \nS-15 myvec_t s; \nS-16 \nS-17 s.data = (double *)calloc(N,sizeof(double)); \nS-18 s.len = N; \nS-19 \nS-20 #pragma omp target \nS-21 init(&s); \nS-22 \nS-23 printf(\"s.data[%d]=%lf\\n\",N-1,s.data[N-1]); //s.data[99]=99.000000 \nS-24 } \nS-25 \nS-26 void init(myvec_t *s) \nS-27 { for(size_t i=0; i<s->len; i++) s->data[i]=i; } \nC / C++ \nCHAPTER 6. DEVICES 191 \nFortran \n1 Example target_mapper.1.f90 (omp_5.0) \nS-1 module my_structures \nS-2 type myvec_t \nS-3 integer :: len \nS-4 double precision, pointer :: data(:) \nS-5 end type \nS-6 end module \nS-7 \nS-8 program main \nS-9 use my_structures \nS-10 integer, parameter :: N=100 \nS-11 \nS-12 !$omp declare mapper(myvec_t :: v) & \nS-13 !$omp& map(v, v%data(1:v%len)) \nS-14 \nS-15 type(myvec_t) :: s \nS-16 \nS-17 allocate(s%data(N)) \nS-18 s%data(1:N) = 0.0d0 \nS-19 s%len = N \nS-20 \nS-21 !$omp target \nS-22 call init(s) \nS-23 !$omp end target \nS-24 \nS-25 print*,\"s%data(\",N,\")=\",s%data(N) !! s%data( 100 )=100.000000000000 \nS-26 end program \nS-27 \nS-28 subroutine init(s) \nS-29 use my_structures \nS-30 type(myvec_t) :: s \nS-31 \nS-32 s%data = [ (i, i=1,s%len) ] \nS-33 end subroutine \nFortran \n2 The next example illustrates the use of the mapper-identifier and deep copy within a structure. The \n3 structure, dzmat_t, represents a complex matrix, with separate real (r_m) and imaginary (i_m) \n4 elements. Two map identifiers are created for partitioning the dzmat_t structure. \n5 For the C/C++ code the first identifier is named top_id and maps the top half of two matrices of \n6 type dzmat_t; while the second identifier, bottom_id, maps the lower half of two matrices. Each \n7 identifier is applied to a different target construct, as map(mapper(top_id), tofrom: \n8 a,b) and map(mapper(bottom_id), tofrom: a,b). Each target offload is allowed to \n9 execute concurrently on two different devices (0 and 1) through the nowait clause. \n192 OpenMP Examples Version 5.2.1 - November 2022 \n1 The Fortran code uses the left_id and right_id map identifiers in the \n2 map(mapper(left_id),tofrom: a,b) and map(mapper(right_id),tofrom: \n3 a,b) map clauses. The array sections for these left and right contiguous portions of the matrices \n4 were defined previously in the declare mapper directive. \n5 Note, the is and ie scalars are firstprivate by default for a target region, but are declared firstprivate \n6 anyway to remind the user of important firstprivate data-sharing properties required here. \nC / C++ \n7 Example target_mapper.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 // N MUST BE EVEN \nS-3 #define N 100 \nS-4 \nS-5 typedef struct dzmat \nS-6 { \nS-7 double r_m[N][N]; \nS-8 double i_m[N][N]; \nS-9 } dzmat_t; \nS-10 \nS-11 #pragma omp declare mapper( top_id: dzmat_t v) \\ \nS-12 map(v.r_m[0:N/2][0:N], \\ \nS-13 v.i_m[0:N/2][0:N] ) \nS-14 \nS-15 #pragma omp declare mapper(bottom_id: dzmat_t v) \\ \nS-16 map(v.r_m[N/2:N/2][0:N], \\ \nS-17 v.i_m[N/2:N/2][0:N] ) \nS-18 //initialization \nS-19 void dzmat_init(dzmat_t *z, int is, int ie, int n); \nS-20 //matrix add: c=a+b \nS-21 void host_add( dzmat_t *a, dzmat_t *b, dzmat_t *c, int n); \nS-22 \nS-23 \nS-24 int main() \nS-25 { \nS-26 dzmat_t a,b,c; \nS-27 int is,ie; \nS-28 \nS-29 is=0; ie=N/2-1; //top N/2 rows on device 0 \nS-30 #pragma omp target map(mapper(top_id), tofrom: a,b) device(0) \\ \nS-31 firstprivate(is,ie) nowait \nS-32 { \nS-33 dzmat_init(&a,is,ie,N); \nS-34 dzmat_init(&b,is,ie,N); \nS-35 } \nS-36 \nCHAPTER 6. DEVICES 193 \nS-37 is=N/2; ie=N-1; //bottom N/2 rows on device 1 \nS-38 #pragma omp target map(mapper(bottom_id), tofrom: a,b) device(1) \\ \nS-39 firstprivate(is,ie) nowait \nS-40 { \nS-41 dzmat_init(&a,is,ie,N); \nS-42 dzmat_init(&b,is,ie,N); \nS-43 } \nS-44 \nS-45 #pragma omp taskwait \nS-46 \nS-47 host_add(&a,&b,&c,N); \nS-48 } \nC / C++ \nFortran \n1 Example target_mapper.2.f90 (omp_5.0) \nS-1 module complex_mats \nS-2 \nS-3 integer, parameter :: N=100 !N must be even \nS-4 type dzmat_t \nS-5 double precision :: r_m(N,N), i_m(N,N) \nS-6 end type \nS-7 \nS-8 !$omp declare mapper( left_id: dzmat_t :: v) map( v%r_m(N, 1:N/2), & \nS-9 !$omp& v%i_m(N, 1:N/2)) \nS-10 \nS-11 !$omp declare mapper(right_id: dzmat_t :: v) map( v%r_m(N,N/2+1:N), & \nS-12 !$omp& v%i_m(N,N/2+1:N)) \nS-13 \nS-14 end module \nS-15 \nS-16 \nS-17 program main \nS-18 use complex_mats \nS-19 type(dzmat_t) :: a,b,c \nS-20 external dzmat_init, host_add !initialization and matrix add: a=b+c \nS-21 \nS-22 integer :: is,ie \nS-23 \nS-24 \nS-25 is=1; ie=N/2 !left N/2 columns on device 0 \nS-26 !$omp target map(mapper( left_id), tofrom: a,b) device(0) & \nS-27 !$omp& firstprivate(is,ie) nowait \nS-28 call dzmat_init(a,is,ie) \nS-29 call dzmat_init(b,is,ie) \nS-30 !$omp end target \n194 OpenMP Examples Version 5.2.1 - November 2022 \nS-31 \nS-32 is=N/2+1; ie=N !right N/2 columns on device 1 \nS-33 !$omp target map(mapper(right_id), tofrom: a,b) device(1) & \nS-34 !$omp& firstprivate(is,ie) nowait \nS-35 call dzmat_init(a,is,ie) \nS-36 call dzmat_init(b,is,ie) \nS-37 !$omp end target \nS-38 \nS-39 !$omp taskwait \nS-40 \nS-41 call host_add(a,b,c) \nS-42 \nS-43 end program main \nFortran \n1 In the third example myvec structures are nested within a mypoints structure. The myvec_t type is \n2 mapped as in the first example. Following the mypoints structure declaration, the mypoints_t type is \n3 mapped by a declare mapper directive. For this structure the hostonly_data element will not \n4 be mapped; also the array section of x (v.x[:1]) and x will be mapped; and scratch will be allocated \n5 and used as scratch storage on the device. The default map-type mapping, tofrom, applies to the x \n6 array section, but not to scratch which is explicitly mapped with the alloc map-type. Note: the \n7 variable v is not included in the map list (otherwise the hostonly_data would be mapped)\u2013 just the \n8 elements to be mapped are listed. \n9 The two mappers are combined when a mypoints_t structure type is mapped, because the mapper \n10 myvec_t structure type is used within a mypoints_t type structure. \nC / C++ \n11 Example target_mapper.3.c (omp_5.0) \nS-1 #include <stdlib.h> \nS-2 #include <stdio.h> \nS-3 \nS-4 #define N 100 \nS-5 \nS-6 typedef struct myvec { \nS-7 size_t len; \nS-8 double *data; \nS-9 } myvec_t; \nS-10 \nS-11 #pragma omp declare mapper(myvec_t v) \\ \nS-12 map(v, v.data[0:v.len]) \nS-13 \nS-14 typedef struct mypoints { \nS-15 struct myvec scratch; \nS-16 struct myvec *x; \nS-17 double hostonly_data[500000]; \nCHAPTER 6. DEVICES 195 \nS-18 } mypoints_t; \nS-19 \nS-20 #pragma omp declare mapper(mypoints_t v) \\ \nS-21 map(v.x, v.x[0] ) map(alloc:v.scratch) \nS-22 \nS-23 void init_mypts_array(mypoints_t *P, int n); \nS-24 void eval_mypts_array(mypoints_t *P, int n); \nS-25 \nS-26 int main(){ \nS-27 \nS-28 mypoints_t P; \nS-29 \nS-30 init_mypts_array(&P, N); \nS-31 \nS-32 #pragma omp target map(P) \nS-33 eval_mypts_array(&P, N); \nS-34 \nS-35 } \nC / C++ \nFortran \n1 Example target_mapper.3.f90 (omp_5.0) \nS-1 module my_structures \nS-2 type myvec_t \nS-3 integer :: len \nS-4 double precision, pointer :: data(:) \nS-5 end type \nS-6 !$omp declare mapper(myvec_t :: v) & \nS-7 !$omp& map(v, v%data(:)) \nS-8 \nS-9 type mypoints_t \nS-10 type(myvec_t) :: scratch \nS-11 type(myvec_t), pointer :: x(:) \nS-12 double precision :: hostonly_data(500000) \nS-13 end type \nS-14 !$omp declare mapper(mypoints_t :: v) & \nS-15 !$omp& map(v%x, v%x(1)) map(alloc:v%scratch) \nS-16 \nS-17 end module \nS-18 \nS-19 \nS-20 program main \nS-21 use my_structures \nS-22 external init_mypts_array, eval_mypts_array \nS-23 \nS-24 type(mypoints_t) :: P \n196 OpenMP Examples Version 5.2.1 - November 2022 \nS-25 \nS-26 call init_mypts_array(P) \nS-27 \nS-28 !$omp target map(P) \nS-29 call eval_mypts_array(P) \nS-30 \nS-31 end program \nFortran \nCHAPTER 6. DEVICES 197 \n"}
{"section_title": "6.10 target data Construct", "chunk": ""}
{"section_title": "6.10.1 Simple target data Construct", "chunk": ""}
{"section_title": "6.10.1 Simple target data Construct", "chunk": "3 This example shows how the target data construct maps variables to a device data \n4 environment. The target data construct creates a new device data environment and maps the \n5 variables v1, v2, and p to the new device data environment. The target construct enclosed in the \n6 target data region creates a new device data environment, which inherits the variables v1, v2, \n7 and p from the enclosing device data environment. The variable N is mapped into the new device \n8 data environment from the encountering task\u2019s data environment. \nC / C++ \n9 Example target_data.1.c (omp_4.0) \nS-1 extern void init(float*, float*, int); \nS-2 extern void output(float*, int); \nS-3 void vec_mult(float *p, float *v1, float *v2, int N) \nS-4 { \nS-5 int i; \nS-6 init(v1, v2, N); \nS-7 #pragma omp target data map(to: v1[0:N], v2[:N]) map(from: p[0:N]) \nS-8 { \nS-9 #pragma omp target \nS-10 #pragma omp parallel for \nS-11 for (i=0; i<N; i++) \nS-12 p[i] = v1[i] * v2[i]; \nS-13 } \nS-14 output(p, N); \nS-15 } \nC / C++ \n198 OpenMP Examples Version 5.2.1 - November 2022 \n1 The Fortran code passes a reference and specifies the extent of the arrays in the declaration. No \n2 length information is necessary in the map clause, as is required with C/C++ pointers. \nFortran \n3 Example target_data.1.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 real :: p(N), v1(N), v2(N) \nS-3 integer :: i \nS-4 call init(v1, v2, N) \nS-5 !$omp target data map(to: v1, v2) map(from: p) \nS-6 !$omp target \nS-7 !$omp parallel do \nS-8 do i=1,N \nS-9 p(i) = v1(i) * v2(i) \nS-10 end do \nS-11 !$omp end target \nS-12 !$omp end target data \nS-13 call output(p, N) \nS-14 end subroutine \nFortran \n"}
{"section_title": "6.10.2 target data Region Enclosing Multiple target Regions", "chunk": ""}
{"section_title": "6.10.2 target data Region Enclosing Multiple target Regions", "chunk": "5 target Regions \n6 The following examples show how the target data construct maps variables to a device data \n7 environment of a target region. The target data construct creates a device data environment \n8 and encloses target regions, which have their own device data environments. The device data \n9 environment of the target data region is inherited by the device data environment of an \n10 enclosed target region. The target data construct is used to create variables that will persist \n11 throughout the target data region. \n12 In the following example the variables v1 and v2 are mapped at each target construct. Instead of \n13 mapping the variable p twice, once at each target construct, p is mapped once by the target \n14 data construct. \nCHAPTER 6. DEVICES 199 \nC / C++ \n1 Example target_data.2.c (omp_4.0) \nS-1 extern void init(float*, float*, int); \nS-2 extern void init_again(float*, float*, int); \nS-3 extern void output(float*, int); \nS-4 void vec_mult(float *p, float *v1, float *v2, int N) \nS-5 { \nS-6 int i; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target data map(from: p[0:N]) \nS-9 { \nS-10 #pragma omp target map(to: v1[:N], v2[:N]) \nS-11 #pragma omp parallel for \nS-12 for (i=0; i<N; i++) \nS-13 p[i] = v1[i] * v2[i]; \nS-14 init_again(v1, v2, N); \nS-15 #pragma omp target map(to: v1[:N], v2[:N]) \nS-16 #pragma omp parallel for \nS-17 for (i=0; i<N; i++) \nS-18 p[i] = p[i] + (v1[i] * v2[i]); \nS-19 } \nS-20 output(p, N); \nS-21 } \nC / C++ \n2 The Fortran code uses reference and specifies the extent of the p, v1 and v2 arrays. No length \n3 information is necessary in the map clause, as is required with C/C++ pointers. The arrays v1 and \n4 v2 are mapped at each target construct. Instead of mapping the array p twice, once at each target \n5 construct, p is mapped once by the target data construct. \nFortran \n6 Example target_data.2.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 real :: p(N), v1(N), v2(N) \nS-3 integer :: i \nS-4 call init(v1, v2, N) \nS-5 !$omp target data map(from: p) \nS-6 !$omp target map(to: v1, v2 ) \nS-7 !$omp parallel do \nS-8 do i=1,N \nS-9 p(i) = v1(i) * v2(i) \nS-10 end do \nS-11 !$omp end target \nS-12 call init_again(v1, v2, N) \n200 OpenMP Examples Version 5.2.1 - November 2022 \nS-13 !$omp target map(to: v1, v2 ) \nS-14 !$omp parallel do \nS-15 do i=1,N \nS-16 p(i) = p(i) + v1(i) * v2(i) \nS-17 end do \nS-18 !$omp end target \nS-19 !$omp end target data \nS-20 call output(p, N) \nS-21 end subroutine \nFortran \n1 In the following example, the array Q is mapped once at the enclosing target data region \n2 instead of at each target construct. In OpenMP 4.0, a scalar variable is implicitly mapped with \n3 the tofrom map-type. But since OpenMP 4.5, a scalar variable, such as the tmp variable, has to be \n4 explicitly mapped with the tofrom map-type at the first target construct in order to return its \n5 reduced value from the parallel loop construct to the host. The variable defaults to firstprivate at the \n6 second target construct. \nC / C++ \n7 Example target_data.3.c (omp_4.0) \nS-1 #include <math.h> \nS-2 #define COLS 100 \nS-3 \nS-4 void gramSchmidt(float Q[][COLS], const int rows) \nS-5 { \nS-6 int cols = COLS; \nS-7 #pragma omp target data map(Q[0:rows][0:cols]) \nS-8 for(int k=0; k < cols; k++) \nS-9 { \nS-10 double tmp = 0.0; \nS-11 \nS-12 #pragma omp target map(tofrom: tmp) \nS-13 #pragma omp parallel for reduction(+:tmp) \nS-14 for(int i=0; i < rows; i++) \nS-15 tmp += (Q[i][k] * Q[i][k]); \nS-16 \nS-17 tmp = 1/sqrt(tmp); \nS-18 \nS-19 #pragma omp target \nS-20 #pragma omp parallel for \nS-21 for(int i=0; i < rows; i++) \nS-22 Q[i][k] *= tmp; \nS-23 } \nS-24 } \nS-25 \nS-26 /* Note: The variable tmp is now mapped with tofrom, for correct \nCHAPTER 6. DEVICES 201 \nS-27 execution with 4.5 (and pre-4.5) compliant compilers. \nS-28 See Devices Intro. \nS-29 */ \nC / C++ \nFortran \n1 Example target_data.3.f90 (omp_4.0) \nS-1 subroutine gramSchmidt(Q,rows,cols) \nS-2 integer :: rows,cols, i,k \nS-3 double precision :: Q(rows,cols), tmp \nS-4 !$omp target data map(Q) \nS-5 do k=1,cols \nS-6 tmp = 0.0d0 \nS-7 !$omp target map(tofrom: tmp) \nS-8 !$omp parallel do reduction(+:tmp) \nS-9 do i=1,rows \nS-10 tmp = tmp + (Q(i,k) * Q(i,k)) \nS-11 end do \nS-12 !$omp end target \nS-13 \nS-14 tmp = 1.0d0/sqrt(tmp) \nS-15 \nS-16 !$omp target \nS-17 !$omp parallel do \nS-18 do i=1,rows \nS-19 Q(i,k) = Q(i,k)*tmp \nS-20 enddo \nS-21 !$omp end target \nS-22 end do \nS-23 !$omp end target data \nS-24 end subroutine \nS-25 \nS-26 ! Note: The variable tmp is now mapped with tofrom, for correct \nS-27 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \n202 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.10.3 target data Construct with Orphaned Call", "chunk": ""}
{"section_title": "6.10.3 target data Construct with Orphaned Call", "chunk": "2 Call \n3 The following two examples show how the target data construct maps variables to a device \n4 data environment. The target data construct\u2019s device data environment encloses the target \n5 construct\u2019s device data environment in the function vec_mult(). \n6 When the type of the variable appearing in an array section is pointer, the pointer variable and the \n7 storage location of the corresponding array section are mapped to the device data environment. The \n8 pointer variable is treated as if it had appeared in a map clause with a map-type of alloc. The \n9 array section\u2019s storage location is mapped according to the map-type in the map clause (the default \n10 map-type is tofrom). \n11 The target construct\u2019s device data environment inherits the storage locations of the array \n12 sections v1[0:N], v2[:n], and p0[0:N] from the enclosing target data construct\u2019s device data \n13 environment. Neither initialization nor assignment is performed for the array sections in the new \n14 device data environment. \n15 The pointer variables p1, v3, and v4 are mapped into the target construct\u2019s device data \n16 environment with an implicit map-type of alloc and they are assigned the address of the storage \n17 location associated with their corresponding array sections. Note that the following pairs of array \n18 section storage locations are equivalent (p0[:N], p1[:N]), (v1[:N],v3[:N]), and (v2[:N],v4[:N]). \nC / C++ \n19 Example target_data.4.c (omp_4.0) \nS-1 void vec_mult(float*, float*, float*, int); \nS-2 \nS-3 extern void init(float*, float*, int); \nS-4 extern void output(float*, int); \nS-5 \nS-6 \nS-7 void foo(float *p0, float *v1, float *v2, int N) \nS-8 { \nS-9 init(v1, v2, N); \nS-10 \nS-11 #pragma omp target data map(to: v1[0:N], v2[:N]) map(from: p0[0:N]) \nS-12 { \nS-13 vec_mult(p0, v1, v2, N); \nS-14 } \nS-15 \nS-16 output(p0, N); \nS-17 } \nS-18 \nS-19 \nS-20 void vec_mult(float *p1, float *v3, float *v4, int N) \nS-21 { \nCHAPTER 6. DEVICES 203 \nS-22 int i; \nS-23 #pragma omp target map(to: v3[0:N], v4[:N]) map(from: p1[0:N]) \nS-24 #pragma omp parallel for \nS-25 for (i=0; i<N; i++) \nS-26 { \nS-27 p1[i] = v3[i] * v4[i]; \nS-28 } \nS-29 } \nC / C++ \n1 The Fortran code maps the pointers and storage in an identical manner (same extent, but uses \n2 indices from 1 to N). \n3 The target construct\u2019s device data environment inherits the storage locations of the arrays v1, v2 \n4 and p0 from the enclosing target data constructs\u2019s device data environment. However, in \n5 Fortran the associated data of the pointer is known, and the shape is not required. \n6 The pointer variables p1, v3, and v4 are mapped into the target construct\u2019s device data \n7 environment with an implicit map-type of alloc and they are assigned the address of the storage \n8 location associated with their corresponding array sections. Note that the following pair of array \n9 storage locations are equivalent (p0,p1), (v1,v3), and (v2,v4). \nFortran \n10 Example target_data.4.f90 (omp_4.0) \nS-1 module mults \nS-2 contains \nS-3 subroutine foo(p0,v1,v2,N) \nS-4 real,pointer,dimension(:) :: p0, v1, v2 \nS-5 integer :: N,i \nS-6 \nS-7 call init(v1, v2, N) \nS-8 \nS-9 !$omp target data map(to: v1, v2) map(from: p0) \nS-10 call vec_mult(p0,v1,v2,N) \nS-11 !$omp end target data \nS-12 \nS-13 call output(p0, N) \nS-14 \nS-15 end subroutine \nS-16 \nS-17 subroutine vec_mult(p1,v3,v4,N) \nS-18 real,pointer,dimension(:) :: p1, v3, v4 \nS-19 integer :: N,i \nS-20 \nS-21 !$omp target map(to: v3, v4) map(from: p1) \nS-22 !$omp parallel do \n204 OpenMP Examples Version 5.2.1 - November 2022 \nS-23 do i=1,N \nS-24 p1(i) = v3(i) * v4(i) \nS-25 end do \nS-26 !$omp end target \nS-27 \nS-28 end subroutine \nS-29 end module \nFortran \n1 In the following example, the variables p1, v3, and v4 are references to the pointer variables p0, v1 \n2 and v2 respectively. The target construct\u2019s device data environment inherits the pointer variables \n3 p0, v1, and v2 from the enclosing target data construct\u2019s device data environment. Thus, p1, \n4 v3, and v4 are already present in the device data environment. \nC++ \n5 Example target_data.5.cpp (omp_4.0) \nS-1 void vec_mult(float* &, float* &, float* &, int &); \nS-2 extern void init(float*, float*, int); \nS-3 extern void output(float*, int); \nS-4 void foo(float *p0, float *v1, float *v2, int N) \nS-5 { \nS-6 init(v1, v2, N); \nS-7 #pragma omp target data map(to: v1[0:N], v2[:N]) map(from: p0[0:N]) \nS-8 { \nS-9 vec_mult(p0, v1, v2, N); \nS-10 } \nS-11 output(p0, N); \nS-12 } \nS-13 void vec_mult(float* &p1, float* &v3, float* &v4, int &N) \nS-14 { \nS-15 int i; \nS-16 #pragma omp target map(to: v3[0:N], v4[:N]) map(from: p1[0:N]) \nS-17 #pragma omp parallel for \nS-18 for (i=0; i<N; i++) \nS-19 p1[i] = v3[i] * v4[i]; \nS-20 } \nC++ \n6 In the following example, the usual Fortran approach is used for dynamic memory. The p0, v1, and \n7 v2 arrays are allocated in the main program and passed as references from one routine to another. In \n8 vec_mult, p1, v3 and v4 are references to the p0, v1, and v2 arrays, respectively. The target \n9 construct\u2019s device data environment inherits the arrays p0, v1, and v2 from the enclosing target data \n10 construct\u2019s device data environment. Thus, p1, v3, and v4 are already present in the device data \n11 environment. \nCHAPTER 6. DEVICES 205 \nFortran \n1 Example target_data.5.f90 (omp_4.0) \nS-1 module my_mult \nS-2 contains \nS-3 subroutine foo(p0,v1,v2,N) \nS-4 real,dimension(:) :: p0, v1, v2 \nS-5 integer :: N,i \nS-6 call init(v1, v2, N) \nS-7 !$omp target data map(to: v1, v2) map(from: p0) \nS-8 call vec_mult(p0,v1,v2,N) \nS-9 !$omp end target data \nS-10 call output(p0, N) \nS-11 end subroutine \nS-12 subroutine vec_mult(p1,v3,v4,N) \nS-13 real,dimension(:) :: p1, v3, v4 \nS-14 integer :: N,i \nS-15 !$omp target map(to: v3, v4) map(from: p1) \nS-16 !$omp parallel do \nS-17 do i=1,N \nS-18 p1(i) = v3(i) * v4(i) \nS-19 end do \nS-20 !$omp end target \nS-21 end subroutine \nS-22 end module \nS-23 program main \nS-24 use my_mult \nS-25 integer, parameter :: N=1024 \nS-26 real,allocatable, dimension(:) :: p, v1, v2 \nS-27 allocate( p(N), v1(N), v2(N) ) \nS-28 call foo(p,v1,v2,N) \nS-29 deallocate( p, v1, v2 ) \nS-30 end program \nFortran \n"}
{"section_title": "6.10.4 target data Construct with if Clause", "chunk": ""}
{"section_title": "6.10.4 target data Construct with if Clause", "chunk": "3 The following two examples show how the target data construct maps variables to a device \n4 data environment. \n5 In the following example, the if clause on the target data construct indicates that if the variable \n6 N is smaller than a given threshold, then the target data construct will not create a device data \n7 environment. \n206 OpenMP Examples Version 5.2.1 - November 2022 \n1 The target constructs enclosed in the target data region must also use an if clause on the \n2 same condition, otherwise the pointer variable p is implicitly mapped with a map-type of tofrom, \n3 but the storage location for the array section p[0:N] will not be mapped in the device data \n4 environments of the target constructs. \nC / C++ \n5 Example target_data.6.c (omp_4.0) \nS-1 #define THRESHOLD 1000000 \nS-2 extern void init(float*, float*, int); \nS-3 extern void init_again(float*, float*, int); \nS-4 extern void output(float*, int); \nS-5 void vec_mult(float *p, float *v1, float *v2, int N) \nS-6 { \nS-7 int i; \nS-8 init(v1, v2, N); \nS-9 #pragma omp target data if(N>THRESHOLD) map(from: p[0:N]) \nS-10 { \nS-11 #pragma omp target if (N>THRESHOLD) map(to: v1[:N], v2[:N]) \nS-12 #pragma omp parallel for \nS-13 for (i=0; i<N; i++) \nS-14 p[i] = v1[i] * v2[i]; \nS-15 init_again(v1, v2, N); \nS-16 #pragma omp target if (N>THRESHOLD) map(to: v1[:N], v2[:N]) \nS-17 #pragma omp parallel for \nS-18 for (i=0; i<N; i++) \nS-19 p[i] = p[i] + (v1[i] * v2[i]); \nS-20 } \nS-21 output(p, N); \nS-22 } \nC / C++ \nCHAPTER 6. DEVICES 207 \n1 The if clauses work the same way for the following Fortran code. The target constructs \n2 enclosed in the target data region should also use an if clause with the same condition, so \n3 that the target data region and the target region are either both created for the device, or are \n4 both ignored. \nFortran \n5 Example target_data.6.f90 (omp_4.0) \nS-1 module params \nS-2 integer,parameter :: THRESHOLD=1000000 \nS-3 end module \nS-4 subroutine vec_mult(p, v1, v2, N) \nS-5 use params \nS-6 real :: p(N), v1(N), v2(N) \nS-7 integer :: i \nS-8 call init(v1, v2, N) \nS-9 !$omp target data if(N>THRESHOLD) map(from: p) \nS-10 !$omp target if(N>THRESHOLD) map(to: v1, v2) \nS-11 !$omp parallel do \nS-12 do i=1,N \nS-13 p(i) = v1(i) * v2(i) \nS-14 end do \nS-15 !$omp end target \nS-16 call init_again(v1, v2, N) \nS-17 !$omp target if(N>THRESHOLD) map(to: v1, v2) \nS-18 !$omp parallel do \nS-19 do i=1,N \nS-20 p(i) = p(i) + v1(i) * v2(i) \nS-21 end do \nS-22 !$omp end target \nS-23 !$omp end target data \nS-24 call output(p, N) \nS-25 end subroutine \nFortran \n208 OpenMP Examples Version 5.2.1 - November 2022 \n1 In the following example, when the if clause conditional expression on the target construct \n2 evaluates to false, the target region will execute on the host device. However, the target data \n3 construct created an enclosing device data environment that mapped p[0:N] to a device data \n4 environment on the default device. At the end of the target data region the array section \n5 p[0:N] will be assigned from the device data environment to the corresponding variable in the data \n6 environment of the task that encountered the target data construct, resulting in undefined \n7 values in p[0:N]. \nC / C++ \n8 Example target_data.7.c (omp_4.0) \nS-1 #define THRESHOLD 1000000 \nS-2 extern void init(float*, float*, int); \nS-3 extern void output(float*, int); \nS-4 void vec_mult(float *p, float *v1, float *v2, int N) \nS-5 { \nS-6 int i; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target data map(from: p[0:N]) \nS-9 { \nS-10 #pragma omp target if (N>THRESHOLD) map(to: v1[:N], v2[:N]) \nS-11 #pragma omp parallel for \nS-12 for (i=0; i<N; i++) \nS-13 p[i] = v1[i] * v2[i]; \nS-14 } /* UNDEFINED behavior if N<=THRESHOLD */ \nS-15 output(p, N); \nS-16 } \nC / C++ \nCHAPTER 6. DEVICES 209 \n1 The if clauses work the same way for the following Fortran code. When the if clause conditional \n2 expression on the target construct evaluates to false, the target region will execute on the host \n3 device. However, the target data construct created an enclosing device data environment that \n4 mapped the p array (and v1 and v2) to a device data environment on the default target device. At the \n5 end of the target data region the p array will be assigned from the device data environment to \n6 the corresponding variable in the data environment of the task that encountered the target data \n7 construct, resulting in undefined values in p. \nFortran \n8 Example target_data.7.f90 (omp_4.0) \nS-1 module params \nS-2 integer, parameter :: THRESHOLD=1000000 \nS-3 end module \nS-4 subroutine vec_mult(p, v1, v2, N) \nS-5 use params \nS-6 real :: p(N), v1(N), v2(N) \nS-7 integer :: i \nS-8 call init(v1, v2, N) \nS-9 !$omp target data map(from: p) \nS-10 !$omp target if(N>THRESHOLD) map(to: v1, v2) \nS-11 !$omp parallel do \nS-12 do i=1,N \nS-13 p(i) = v1(i) * v2(i) \nS-14 end do \nS-15 !$omp end target \nS-16 !$omp end target data \nS-17 call output(p, N) !*** UNDEFINED behavior if N<=THRESHOLD \nS-18 end subroutine \nFortran \n210 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.11 target enter data and target exit data Constructs", "chunk": ""}
{"section_title": "6.11 target enter data and target exit data Constructs", "chunk": "2 exit data Constructs \n3 The structured data construct (target data) provides persistent data on a device for subsequent \n4 target constructs as shown in the target data examples above. This is accomplished by \n5 creating a single target data region containing target constructs. \n6 The unstructured data constructs allow the creation and deletion of data on the device at any \n7 appropriate point within the host code, as shown below with the target enter data and \n8 target exit data constructs. \n9 The following C++ code creates/deletes a vector in a constructor/destructor of a class. The \n10 constructor creates a vector with target enter data and uses an alloc modifier in the map \n11 clause to avoid copying values to the device. The destructor deletes the data \n12 (target exit data) and uses the delete modifier in the map clause to avoid copying data \n13 back to the host. Note, the stand-alone target enter data occurs after the host vector is \n14 created, and the target exit data construct occurs before the host data is deleted. \nC++ \n15 Example target_unstructured_data.1.cpp (omp_4.5) \nS-1 class Matrix \nS-2 { \nS-3 \nS-4 Matrix(int n) { \nS-5 len = n; \nS-6 v = new double[len]; \nS-7 #pragma omp target enter data map(alloc:v[0:len]) \nS-8 } \nS-9 \nS-10 ~Matrix() { \nS-11 // NOTE: delete map type should be used, since the corresponding \nS-12 // host data will cease to exist after the destructor is called. \nS-13 \nS-14 #pragma omp target exit data map(delete:v[0:len]) \nS-15 delete[] v; \nS-16 } \nS-17 \nS-18 private: \nS-19 double* v; \nS-20 int len; \nS-21 \nS-22 }; \nC++ \nCHAPTER 6. DEVICES 211 \n1 The following C code allocates and frees the data member of a Matrix structure. The \n2 init_matrix function allocates the memory used in the structure and uses the \n3 target enter data directive to map it to the target device. The free_matrix function \n4 removes the mapped array from the target device and then frees the memory on the host. Note, the \n5 stand-alone target enter data occurs after the host memory is allocated, and the \n6 target exit data construct occurs before the host data is freed. \nC / C++ \n7 Example target_unstructured_data.1.c (omp_4.5) \nS-1 #include <stdlib.h> \nS-2 typedef struct { \nS-3 double *A; \nS-4 int N; \nS-5 } Matrix; \nS-6 \nS-7 void init_matrix(Matrix *mat, int n) \nS-8 { \nS-9 mat->A = (double *)malloc(n*sizeof(double)); \nS-10 mat->N = n; \nS-11 #pragma omp target enter data map(alloc:mat->A[:n]) \nS-12 } \nS-13 \nS-14 void free_matrix(Matrix *mat) \nS-15 { \nS-16 #pragma omp target exit data map(delete:mat->A[:mat->N]) \nS-17 mat->N = 0; \nS-18 free(mat->A); \nS-19 mat->A = NULL; \nS-20 } \nC / C++ \n212 OpenMP Examples Version 5.2.1 - November 2022 \n1 The following Fortran code allocates and deallocates a module array. The initialize \n2 subroutine allocates the module array and uses the target enter data directive to map it to the \n3 target device. The finalize subroutine removes the mapped array from the target device and \n4 then deallocates the array on the host. Note, the stand-alone target enter data occurs after \n5 the host memory is allocated, and the target exit data construct occurs before the host data is \n6 deallocated. \nFortran \n7 Example target_unstructured_data.1.f90 (omp_4.5) \nS-1 module example \nS-2 real(8), allocatable :: A(:) \nS-3 \nS-4 contains \nS-5 subroutine initialize(N) \nS-6 integer :: N \nS-7 \nS-8 allocate(A(N)) \nS-9 !$omp target enter data map(alloc:A) \nS-10 \nS-11 end subroutine initialize \nS-12 \nS-13 subroutine finalize() \nS-14 \nS-15 !$omp target exit data map(delete:A) \nS-16 deallocate(A) \nS-17 \nS-18 end subroutine finalize \nS-19 end module example \nFortran \nCHAPTER 6. DEVICES 213 \n"}
{"section_title": "6.12 target update Construct", "chunk": ""}
{"section_title": "6.12.1 Simple target data and target update Constructs", "chunk": ""}
{"section_title": "6.12.1 Simple target data and target update Constructs", "chunk": "3 Constructs \n4 The following example shows how the target update construct updates variables in a device \n5 data environment. \n6 The target data construct maps array sections v1[:N] and v2[:N] (arrays v1 and v2 in the \n7 Fortran code) into a device data environment. \n8 The task executing on the host device encounters the first target region and waits for the \n9 completion of the region. \n10 After the execution of the first target region, the task executing on the host device then assigns \n11 new values to v1[:N] and v2[:N] (v1 and v2 arrays in Fortran code) in the task\u2019s data environment \n12 by calling the function init_again(). \n13 The target update construct assigns the new values of v1 and v2 from the task\u2019s data \n14 environment to the corresponding mapped array sections in the device data environment of the \n15 target data construct. \n16 The task executing on the host device then encounters the second target region and waits for the \n17 completion of the region. \n18 The second target region uses the updated values of v1[:N] and v2[:N]. \nC / C++ \n19 Example target_update.1.c (omp_4.0) \nS-1 extern void init(float *, float *, int); \nS-2 extern void init_again(float *, float *, int); \nS-3 extern void output(float *, int); \nS-4 void vec_mult(float *p, float *v1, float *v2, int N) \nS-5 { \nS-6 int i; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target data map(to: v1[:N], v2[:N]) map(from: p[0:N]) \nS-9 { \nS-10 #pragma omp target \nS-11 #pragma omp parallel for \nS-12 for (i=0; i<N; i++) \nS-13 p[i] = v1[i] * v2[i]; \nS-14 init_again(v1, v2, N); \nS-15 #pragma omp target update to(v1[:N], v2[:N]) \nS-16 #pragma omp target \nS-17 #pragma omp parallel for \nS-18 for (i=0; i<N; i++) \n214 OpenMP Examples Version 5.2.1 - November 2022 \nS-19 p[i] = p[i] + (v1[i] * v2[i]); \nS-20 } \nS-21 output(p, N); \nS-22 } \nC / C++ \nFortran \n1 Example target_update.1.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 real :: p(N), v1(N), v2(N) \nS-3 integer :: i \nS-4 call init(v1, v2, N) \nS-5 !$omp target data map(to: v1, v2) map(from: p) \nS-6 !$omp target \nS-7 !$omp parallel do \nS-8 do i=1,N \nS-9 p(i) = v1(i) * v2(i) \nS-10 end do \nS-11 !$omp end target \nS-12 call init_again(v1, v2, N) \nS-13 !$omp target update to(v1, v2) \nS-14 !$omp target \nS-15 !$omp parallel do \nS-16 do i=1,N \nS-17 p(i) = p(i) + v1(i) * v2(i) \nS-18 end do \nS-19 !$omp end target \nS-20 !$omp end target data \nS-21 call output(p, N) \nS-22 end subroutine \nFortran \nCHAPTER 6. DEVICES 215 \n"}
{"section_title": "6.12.2 target update Construct with if Clause", "chunk": ""}
{"section_title": "6.12.2 target update Construct with if Clause", "chunk": "2 The following example shows how the target update construct updates variables in a device \n3 data environment. \n4 The target data construct maps array sections v1[:N] and v2[:N] (arrays v1 and v2 in the \n5 Fortran code) into a device data environment. In between the two target regions, the task \n6 executing on the host device conditionally assigns new values to v1 and v2 in the task\u2019s data \n7 environment. The function maybe_init_again() returns true if new data is written. \n8 When the conditional expression (the return value of maybe_init_again()) in the if clause \n9 is true, the target update construct assigns the new values of v1 and v2 from the task\u2019s data \n10 environment to the corresponding mapped array sections in the target data construct\u2019s device \n11 data environment. \nC / C++ \n12 Example target_update.2.c (omp_4.0) \nS-1 extern void init(float *, float *, int); \nS-2 extern int maybe_init_again(float *, int); \nS-3 extern void output(float *, int); \nS-4 void vec_mult(float *p, float *v1, float *v2, int N) \nS-5 { \nS-6 int i; \nS-7 init(v1, v2, N); \nS-8 #pragma omp target data map(to: v1[:N], v2[:N]) map(from: p[0:N]) \nS-9 { \nS-10 int changed; \nS-11 #pragma omp target \nS-12 #pragma omp parallel for \nS-13 for (i=0; i<N; i++) \nS-14 p[i] = v1[i] * v2[i]; \nS-15 changed = maybe_init_again(v1, N); \nS-16 #pragma omp target update if (changed) to(v1[:N]) \nS-17 changed = maybe_init_again(v2, N); \nS-18 #pragma omp target update if (changed) to(v2[:N]) \nS-19 #pragma omp target \nS-20 #pragma omp parallel for \nS-21 for (i=0; i<N; i++) \nS-22 p[i] = p[i] + (v1[i] * v2[i]); \nS-23 } \nS-24 output(p, N); \nS-25 } \nC / C++ \n216 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example target_update.2.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 interface \nS-3 logical function maybe_init_again (v1, N) \nS-4 real :: v1(N) \nS-5 integer :: N \nS-6 end function \nS-7 end interface \nS-8 real :: p(N), v1(N), v2(N) \nS-9 integer :: i \nS-10 logical :: changed \nS-11 call init(v1, v2, N) \nS-12 !$omp target data map(to: v1, v2) map(from: p) \nS-13 !$omp target \nS-14 !$omp parallel do \nS-15 do i=1, N \nS-16 p(i) = v1(i) * v2(i) \nS-17 end do \nS-18 !$omp end target \nS-19 changed = maybe_init_again(v1, N) \nS-20 !$omp target update if(changed) to(v1(:N)) \nS-21 changed = maybe_init_again(v2, N) \nS-22 !$omp target update if(changed) to(v2(:N)) \nS-23 !$omp target \nS-24 !$omp parallel do \nS-25 do i=1, N \nS-26 p(i) = p(i) + v1(i) * v2(i) \nS-27 end do \nS-28 !$omp end target \nS-29 !$omp end target data \nS-30 call output(p, N) \nS-31 end subroutine \nFortran \nCHAPTER 6. DEVICES 217 \n"}
{"section_title": "6.13 Declare Target Directive", "chunk": ""}
{"section_title": "6.13.1 Declare Target Directive for a Procedure", "chunk": ""}
{"section_title": "6.13.1 Declare Target Directive for a Procedure", "chunk": "3 The following example shows how the declare target directive is used to indicate that the \n4 corresponding call inside a target region is to a fib function that can execute on the default \n5 target device. \n6 A version of the function is also available on the host device. When the if clause conditional \n7 expression on the target construct evaluates to false, the target region (thus fib) will execute \n8 on the host device. \n9 For the following C/C++ code the declaration of the function fib appears between the \n10 begin declare target and end declare target directives. In the corresponding Fortran \n11 code, the declare target directive appears at the end of the specification part of the subroutine. \nC / C++ \n12 Example declare_target.1.c (omp_5.1) \nS-1 #pragma omp begin declare target \nS-2 extern void fib(int N); \nS-3 #pragma omp end declare target \nS-4 \nS-5 #define THRESHOLD 1000000 \nS-6 void fib_wrapper(int n) \nS-7 { \nS-8 #pragma omp target if(n > THRESHOLD) \nS-9 { \nS-10 fib(n); \nS-11 } \nS-12 } \nC / C++ \n13 The Fortran fib subroutine contains a declare target declaration to indicate to the compiler \n14 to create an device executable version of the procedure. The subroutine name has not been included \n15 on the declare target directive and is, therefore, implicitly assumed. \n16 The program uses the module_fib module, which presents an explicit interface to the compiler \n17 with the declare target declarations for processing the fib call. \n218 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example declare_target.1.f90 (omp_4.0) \nS-1 module module_fib \nS-2 integer :: THRESHOLD=1000000 \nS-3 contains \nS-4 subroutine fib(N) \nS-5 integer :: N \nS-6 !$omp declare target \nS-7 !... \nS-8 end subroutine \nS-9 end module \nS-10 subroutine my_fib(N) \nS-11 use module_fib \nS-12 integer :: N \nS-13 !$omp target if( N > THRESHOLD ) \nS-14 call fib(N) \nS-15 !$omp end target \nS-16 end subroutine \nFortran \n2 The next Fortran example shows the use of an external subroutine. As the subroutine is neither use \n3 associated nor an internal procedure, the declare target declarations within a external \n4 subroutine are unknown to the main program unit; therefore, a declare target must be \n5 provided within the program scope for the compiler to determine that a target binary should be \n6 available. \nFortran \n7 Example declare_target.2.f90 (omp_4.0) \nS-1 program my_fib \nS-2 integer :: N = 8 \nS-3 interface \nS-4 subroutine fib(N) \nS-5 !$omp declare target \nS-6 integer :: N \nS-7 end subroutine fib \nS-8 end interface \nS-9 !$omp target \nS-10 call fib(N) \nS-11 !$omp end target \nS-12 end program \nS-13 subroutine fib(N) \nS-14 integer :: N \nS-15 !$omp declare target \nS-16 print*,\"hello from fib\" \nCHAPTER 6. DEVICES 219 \nS-17 !... \nS-18 end subroutine \nFortran \n"}
{"section_title": "6.13.2 Declare Target Directive for Class Type", "chunk": ""}
{"section_title": "6.13.2 Declare Target Directive for Class Type", "chunk": "2 The following example shows the use of the begin declare target and \n3 end declare target pair to designate the beginning and end of the affected declarations, as \n4 introduced in OpenMP 5.1. The begin declare target directive was defined to \n5 symmetrically complement the terminating (\u201cend\u201d) directive. \nC++ \n6 The example also shows 3 different ways to use a declare target directive for a class and an external \n7 member-function definition (for the XOR1, XOR2, and XOR3 classes and definitions for their \n8 corresponding foo member functions). \n9 For XOR1, a begin declare target and end declare target directive enclose both the \n10 class and its member function definition. The compiler immediately knows to create a device \n11 version of the function for execution in a target region. \n12 For XOR2, the class member function definition is not specified with a declare target directive. An \n13 implicit declare target is created for the member function definition. The same applies if this \n14 declaration arrangement for the class and function are included through a header file. \n15 For XOR3, the class and its member function are not enclosed by begin declare target and \n16 end declare target directives, but there is an implicit declare target since the class, its \n17 function and the target construct are in the same file scope. That is, the class and its function are \n18 treated as if delimited by a declare target directive. The same applies if the class and function are \n19 included through a header file. \n20 Example declare_target.2a.cpp (omp_5.1) \nS-1 #include <iostream> \nS-2 using namespace std; \nS-3 \nS-4 #pragma omp begin declare target // declare target--class and function \nS-5 class XOR1 \nS-6 { \nS-7 int a; \nS-8 public: \nS-9 XOR1(int arg): a(arg) {}; \nS-10 int foo(); \nS-11 }; \nS-12 int XOR1::foo() { return a^0x01;} \nS-13 #pragma omp end declare target \nS-14 \n220 OpenMP Examples Version 5.2.1 - November 2022 \nC++ (cont.) \nS-15 #pragma omp begin declare target // declare target--class, not function \nS-16 class XOR2 \nS-17 { \nS-18 int a; \nS-19 public: \nS-20 XOR2(int arg): a(arg) {}; \nS-21 int foo(); \nS-22 }; \nS-23 #pragma omp end declare target \nS-24 \nS-25 int XOR2::foo() { return a^0x01;} \nS-26 \nS-27 class XOR3 // declare target--neither class nor function \nS-28 { \nS-29 int a; \nS-30 public: \nS-31 XOR3(int arg): a(arg) {}; \nS-32 int foo(); \nS-33 }; \nS-34 int XOR3::foo() { return a^0x01;} \nS-35 \nS-36 int main (){ \nS-37 \nS-38 XOR1 my_XOR1(3); \nS-39 XOR2 my_XOR2(3); \nS-40 XOR3 my_XOR3(3); \nS-41 int res1, res2, res3; \nS-42 \nS-43 #pragma omp target map(tofrom:res1) \nS-44 res1=my_XOR1.foo(); \nS-45 \nS-46 #pragma omp target map(tofrom:res2) \nS-47 res2=my_XOR2.foo(); \nS-48 \nS-49 #pragma omp target map(tofrom:res3) \nS-50 res3=my_XOR3.foo(); \nS-51 \nS-52 cout << res1 << endl; // OUT1: 2 \nS-53 cout << res2 << endl; // OUT2: 2 \nS-54 cout << res3 << endl; // OUT3: 2 \nS-55 } \n1 Often class definitions and their function definitions are included in separate files, as shown in \n2 declare_target.2b_classes.hpp and declare_target.2b_functions.cpp below. In this case, it is \nCHAPTER 6. DEVICES 221 \nC++ (cont.) \n1 necessary to specify in a declare target directive for the classes. However, as long as the \n2 2b_functions.cpp file includes the corresponding declare target classes, there is no need to specify \n3 the functions with a declare target directive. The functions are treated as if they are specified with a \n4 declare target directive. Compiling the declare_target.2b_functions.cpp and \n5 declare_target.2b_main.cpp files separately and linking them, will create appropriate executable \n6 device functions for the target device. \n7 Example declare_target.2b_classes.hpp (omp_5.1) \nS-1 #pragma omp begin declare target \nS-2 class XOR1 \nS-3 { \nS-4 int a; \nS-5 public: \nS-6 XOR1(int arg): a(arg) {}; \nS-7 int foo(); \nS-8 }; \nS-9 #pragma omp end declare target \n8 Example declare_target.2b_functions.cpp (omp_5.1) \nS-1 #include \"declare_target.2b_classes.hpp\" \nS-2 int XOR1::foo() { return a^0x01;} \n9 Example declare_target.2b_main.cpp (omp_5.1) \nS-1 #include <iostream> \nS-2 using namespace std; \nS-3 \nS-4 #include \"declare_target.2b_classes.hpp\" \nS-5 \nS-6 int main (){ \nS-7 \nS-8 XOR1 my_XOR1(3); \nS-9 int res1; \nS-10 \nS-11 #pragma omp target map(from: res1) \nS-12 res1=my_XOR1.foo(); \nS-13 \nS-14 cout << res1 << endl; // OUT1: 2 \nS-15 } \n10 The following example shows how the begin declare target and end declare target \n11 directives are used to enclose the declaration of a variable varY with a class type typeY. \n12 This example shows pre-OpenMP 5.0 behavior for the varY.foo() function call (an error). The \n222 OpenMP Examples Version 5.2.1 - November 2022 \n1 member function typeY::foo() cannot be accessed on a target device because its declaration \n2 does not appear between begin declare target and end declare target directives. As \n3 of OpenMP 5.0, the function is implicitly declared with a declare target directive and will \n4 successfully execute the function on the device. See previous examples. \n5 Example declare_target.2c.cpp (omp_5.1) \nS-1 struct typeX \nS-2 { \nS-3 int a; \nS-4 }; \nS-5 class typeY \nS-6 { \nS-7 int a; \nS-8 public: \nS-9 int foo() { return a^0x01;} \nS-10 }; \nS-11 \nS-12 #pragma omp begin declare target \nS-13 struct typeX varX; // ok \nS-14 class typeY varY; // ok if varY.foo() not called on target device \nS-15 #pragma omp end declare target \nS-16 \nS-17 void foo() \nS-18 { \nS-19 #pragma omp target \nS-20 { \nS-21 varX.a = 100; // ok \nS-22 varY.foo(); // error foo() is not available on a target device \nS-23 } \nS-24 } \nC++ \n"}
{"section_title": "6.13.3 Declare Target Directive for Variables", "chunk": ""}
{"section_title": "6.13.3 Declare Target Directive for Variables", "chunk": "7 The following examples show how the declare target directive is used to indicate that global \n8 variables are mapped to the implicit device data environment of each target device. \n9 In the following example, the declarations of the variables p, v1, and v2 appear between \n10 begin declare target and end declare target directives indicating that the variables \n11 are mapped to the implicit device data environment of each target device. The target update \n12 directive is then used to manage the consistency of the variables p, v1, and v2 between the data \n13 environment of the encountering host device task and the implicit device data environment of the \n14 default target device. \nCHAPTER 6. DEVICES 223 \nC / C++ \n1 Example declare_target.3.c (omp_5.1) \nS-1 #define N 1000 \nS-2 \nS-3 #pragma omp begin declare target \nS-4 float p[N], v1[N], v2[N]; \nS-5 #pragma omp end declare target \nS-6 \nS-7 extern void init(float *, float *, int); \nS-8 extern void output(float *, int); \nS-9 \nS-10 void vec_mult() \nS-11 { \nS-12 int i; \nS-13 init(v1, v2, N); \nS-14 #pragma omp target update to(v1, v2) \nS-15 #pragma omp target \nS-16 #pragma omp parallel for \nS-17 for (i=0; i<N; i++) \nS-18 p[i] = v1[i] * v2[i]; \nS-19 #pragma omp target update from(p) \nS-20 output(p, N); \nS-21 } \nC / C++ \n2 The Fortran version of the above C code uses a different syntax. Fortran modules use a list syntax \n3 on the declare target directive to declare mapped variables. \nFortran \n4 Example declare_target.3.f90 (omp_4.0) \nS-1 module my_arrays \nS-2 !$omp declare target (N, p, v1, v2) \nS-3 integer, parameter :: N=1000 \nS-4 real :: p(N), v1(N), v2(N) \nS-5 end module \nS-6 subroutine vec_mult() \nS-7 use my_arrays \nS-8 integer :: i \nS-9 call init(v1, v2, N); \nS-10 !$omp target update to(v1, v2) \nS-11 !$omp target \nS-12 !$omp parallel do \nS-13 do i = 1,N \nS-14 p(i) = v1(i) * v2(i) \nS-15 end do \n224 OpenMP Examples Version 5.2.1 - November 2022 \nS-16 !$omp end target \nS-17 !$omp target update from (p) \nS-18 call output(p, N) \nS-19 end subroutine \nFortran \n1 The following example also indicates that the function Pfun() is available on the target device, as \n2 well as the variable Q, which is mapped to the implicit device data environment of each target \n3 device. The target update directive is then used to manage the consistency of the variable Q \n4 between the data environment of the encountering host device task and the implicit device data \n5 environment of the default target device. \n6 In the following example, the function and variable declarations appear between the \n7 begin declare target and end declare target directives. \nC / C++ \n8 Example declare_target.4.c (omp_5.1) \nS-1 #define N 10000 \nS-2 \nS-3 #pragma omp begin declare target \nS-4 float Q[N][N]; \nS-5 float Pfun(const int i, const int k) { return Q[i][k] * Q[k][i]; } \nS-6 #pragma omp end declare target \nS-7 \nS-8 float accum(int k) \nS-9 { \nS-10 float tmp = 0.0; \nS-11 #pragma omp target update to(Q) \nS-12 #pragma omp target map(tofrom: tmp) \nS-13 #pragma omp parallel for reduction(+:tmp) \nS-14 for(int i=0; i < N; i++) \nS-15 tmp += Pfun(i,k); \nS-16 return tmp; \nS-17 } \nS-18 \nS-19 /* Note: The variable tmp is now mapped with tofrom, for correct \nS-20 execution with 4.5 (and pre-4.5) compliant compilers. \nS-21 See Devices Intro. \nS-22 */ \nC / C++ \n9 The Fortran version of the above C code uses a different syntax. In Fortran modules a list syntax on \n10 the declare target directive is used to declare mapped variables and procedures. The N and Q \n11 variables are declared as a comma separated list. When the declare target directive is used to \n12 declare just the procedure, the procedure name need not be listed \u2013 it is implicitly assumed, as \n13 illustrated in the Pfun() function. \nCHAPTER 6. DEVICES 225 \nFortran \n1 Example declare_target.4.f90 (omp_4.0) \nS-1 module my_global_array \nS-2 !$omp declare target (N,Q) \nS-3 integer, parameter :: N=10 \nS-4 real :: Q(N,N) \nS-5 contains \nS-6 function Pfun(i,k) \nS-7 !$omp declare target \nS-8 real :: Pfun \nS-9 integer,intent(in) :: i,k \nS-10 Pfun=(Q(i,k) * Q(k,i)) \nS-11 end function \nS-12 end module \nS-13 \nS-14 function accum(k) result(tmp) \nS-15 use my_global_array \nS-16 real :: tmp \nS-17 integer :: i, k \nS-18 tmp = 0.0e0 \nS-19 !$omp target map(tofrom: tmp) \nS-20 !$omp parallel do reduction(+:tmp) \nS-21 do i=1,N \nS-22 tmp = tmp + Pfun(k,i) \nS-23 end do \nS-24 !$omp end target \nS-25 end function \nS-26 \nS-27 ! Note: The variable tmp is now mapped with tofrom, for correct \nS-28 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \n"}
{"section_title": "6.13.4 Declare Target Directive with declare simd", "chunk": ""}
{"section_title": "6.13.4 Declare Target Directive with declare simd", "chunk": "3 The following example shows how the begin declare target and end declare target \n4 directives are used to indicate that a function is available on a target device. The declare simd \n5 directive indicates that there is a SIMD version of the function P() that is available on the target \n6 device as well as one that is available on the host device. \n226 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example declare_target.5.c (omp_5.1) \nS-1 #define N 10000 \nS-2 #define M 1024 \nS-3 \nS-4 #pragma omp begin declare target \nS-5 float Q[N][N]; \nS-6 \nS-7 #pragma omp declare simd uniform(i) linear(k) notinbranch \nS-8 float P(const int i, const int k) \nS-9 { \nS-10 return Q[i][k] * Q[k][i]; \nS-11 } \nS-12 #pragma omp end declare target \nS-13 \nS-14 float accum(void) \nS-15 { \nS-16 float tmp = 0.0; \nS-17 int i, k; \nS-18 #pragma omp target map(tofrom: tmp) \nS-19 #pragma omp parallel for reduction(+:tmp) \nS-20 for (i=0; i < N; i++) { \nS-21 float tmp1 = 0.0; \nS-22 #pragma omp simd reduction(+:tmp1) \nS-23 for (k=0; k < M; k++) { \nS-24 tmp1 += P(i,k); \nS-25 } \nS-26 tmp += tmp1; \nS-27 } \nS-28 return tmp; \nS-29 } \nS-30 \nS-31 /* Note: The variable tmp is now mapped with tofrom, for correct \nS-32 execution with 4.5 (and pre-4.5) compliant compilers. \nS-33 See Devices Intro. \nS-34 */ \nC / C++ \n2 The Fortran version of the above C code uses a different syntax. Fortran modules use a list syntax \n3 of the declare target declaration for the mapping. Here the N and Q variables are declared in \n4 the list form as a comma separated list. The function declaration does not use a list and implicitly \n5 assumes the function name. In this Fortran example row and column indices are reversed relative to \n6 the C/C++ example, as is usual for codes optimized for memory access. \nCHAPTER 6. DEVICES 227 \nFortran \n1 Example declare_target.5.f90 (omp_4.0) \nS-1 module my_global_array \nS-2 !$omp declare target (N,Q) \nS-3 integer, parameter :: N=10000, M=1024 \nS-4 real :: Q(N,N) \nS-5 contains \nS-6 function P(k,i) \nS-7 !$omp declare simd uniform(i) linear(k) notinbranch \nS-8 !$omp declare target \nS-9 real :: P \nS-10 integer,intent(in) :: k,i \nS-11 P=(Q(k,i) * Q(i,k)) \nS-12 end function \nS-13 end module \nS-14 \nS-15 function accum() result(tmp) \nS-16 use my_global_array \nS-17 real :: tmp, tmp1 \nS-18 integer :: i \nS-19 tmp = 0.0e0 \nS-20 !$omp target map(tofrom: tmp) \nS-21 !$omp parallel do private(tmp1) reduction(+:tmp) \nS-22 do i=1,N \nS-23 tmp1 = 0.0e0 \nS-24 !$omp simd reduction(+:tmp1) \nS-25 do k = 1,M \nS-26 tmp1 = tmp1 + P(k,i) \nS-27 end do \nS-28 tmp = tmp + tmp1 \nS-29 end do \nS-30 !$omp end target \nS-31 end function \nS-32 \nS-33 ! Note: The variable tmp is now mapped with tofrom, for correct \nS-34 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \n228 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.13.5 Declare Target Directive with link Clause", "chunk": ""}
{"section_title": "6.13.5 Declare Target Directive with link Clause", "chunk": "2 In the OpenMP 4.5 standard the declare target directive was extended to allow static data to be \n3 mapped, when needed, through a link clause. \n4 Data storage for items listed in the link clause becomes available on the device when it is mapped \n5 implicitly or explicitly in a map clause, and it persists for the scope of the mapping (as specified by \n6 a target construct, a target data construct, or target enter/exit data constructs). \n7 Tip: When all the global data items will not fit on a device and are not needed simultaneously, use \n8 the link clause and map the data only when it is needed. \n9 The following C and Fortran examples show two sets of data (single precision and double \n10 precision) that are global on the host for the entire execution on the host; but are only used globally \n11 on the device for part of the program execution. The single precision data are allocated and persist \n12 only for the first target region. Similarly, the double precision data are in scope on the device \n13 only for the second target region. \nC / C++ \n14 Example declare_target.6.c (omp_5.1) \nS-1 #define N 100000000 \nS-2 \nS-3 float sp[N], sv1[N], sv2[N]; \nS-4 double dp[N], dv1[N], dv2[N]; \nS-5 #pragma omp declare target link(sp,sv1,sv2) \\ \nS-6 link(dp,dv1,dv2) \nS-7 \nS-8 void s_init(float *, float *, int); \nS-9 void d_init(double *, double *, int); \nS-10 void s_output(float *, int); \nS-11 void d_output(double *, int); \nS-12 \nS-13 #pragma omp begin declare target \nS-14 \nS-15 void s_vec_mult_accum() \nS-16 { \nS-17 int i; \nS-18 \nS-19 #pragma omp parallel for \nS-20 for (i=0; i<N; i++) \nS-21 sp[i] = sv1[i] * sv2[i]; \nS-22 } \nS-23 \nS-24 void d_vec_mult_accum() \nS-25 { \nS-26 int i; \nS-27 \nCHAPTER 6. DEVICES 229 \nS-28 #pragma omp parallel for \nS-29 for (i=0; i<N; i++) \nS-30 dp[i] = dv1[i] * dv2[i]; \nS-31 } \nS-32 #pragma omp end declare target \nS-33 \nS-34 int main() \nS-35 { \nS-36 s_init(sv1, sv2, N); \nS-37 #pragma omp target map(to:sv1,sv2) map(from:sp) \nS-38 s_vec_mult_accum(); \nS-39 s_output(sp, N); \nS-40 \nS-41 d_init(dv1, dv2, N); \nS-42 #pragma omp target map(to:dv1,dv2) map(from:dp) \nS-43 d_vec_mult_accum(); \nS-44 d_output(dp, N); \nS-45 \nS-46 return 0; \nS-47 } \nC / C++ \nFortran \n1 Example declare_target.6.f90 (omp_4.5) \nS-1 module m_dat \nS-2 integer, parameter :: N=100000000 \nS-3 !$omp declare target link(sp,sv1,sv2) \nS-4 real :: sp(N), sv1(N), sv2(N) \nS-5 \nS-6 !$omp declare target link(dp,dv1,dv2) \nS-7 double precision :: dp(N), dv1(N), dv2(N) \nS-8 \nS-9 contains \nS-10 subroutine s_vec_mult_accum() \nS-11 !$omp declare target \nS-12 integer :: i \nS-13 \nS-14 !$omp parallel do \nS-15 do i = 1,N \nS-16 sp(i) = sv1(i) * sv2(i) \nS-17 end do \nS-18 \nS-19 end subroutine s_vec_mult_accum \nS-20 \nS-21 subroutine d_vec_mult_accum() \nS-22 !$omp declare target \n230 OpenMP Examples Version 5.2.1 - November 2022 \nS-23 integer :: i \nS-24 \nS-25 !$omp parallel do \nS-26 do i = 1,N \nS-27 dp(i) = dv1(i) * dv2(i) \nS-28 end do \nS-29 \nS-30 end subroutine \nS-31 end module m_dat \nS-32 \nS-33 program prec_vec_mult \nS-34 use m_dat \nS-35 \nS-36 call s_init(sv1, sv2, N) \nS-37 !$omp target map(to:sv1,sv2) map(from:sp) \nS-38 call s_vec_mult_accum() \nS-39 !$omp end target \nS-40 call s_output(sp, N) \nS-41 \nS-42 call d_init(dv1, dv2, N) \nS-43 !$omp target map(to:dv1,dv2) map(from:dp) \nS-44 call d_vec_mult_accum() \nS-45 !$omp end target \nS-46 call d_output(dp, N) \nS-47 \nS-48 end program \nFortran \n"}
{"section_title": "6.13.6 Declare Target Directive with device_type Clause", "chunk": ""}
{"section_title": "6.13.6 Declare Target Directive with device_type Clause", "chunk": "2 Clause \n3 The declare target directives apply to procedures to ensure that they can be executed or \n4 accessed on a device. The device_type clause specifies whether a version of the procedure or \n5 variable should be made available on the host, device or both. This example uses nohost for a \n6 procedure foo. Only a device version of the procedure foo is made available. If the variant function \n7 foo_onhost is not specified for the host fallback execution, the call to foo from the target region \n8 will result in a link time error due to the code generated for host execution of the target region. This \n9 is because host symbol for the device routine foo marked as nohost is not required to be present \n10 in the host environment. \nCHAPTER 6. DEVICES 231 \nC / C++ \n1 Example declare_target.7.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 \nS-3 void foo(); \nS-4 void foo_onhost(); \nS-5 \nS-6 #pragma omp declare target enter(foo) device_type(nohost) \nS-7 \nS-8 #pragma omp declare variant(foo_onhost) match(device={kind(host)}) \nS-9 void foo(){ \nS-10 //device specific computation \nS-11 } \nS-12 \nS-13 void foo_onhost(){ \nS-14 printf(\"On host\\n\"); \nS-15 } \nS-16 \nS-17 int main(){ \nS-18 #pragma omp target teams \nS-19 { \nS-20 foo(); //calls foo() on target device or \nS-21 //foo_onhost() in case of host fallback \nS-22 } \nS-23 return 0; \nS-24 \nS-25 } \nC / C++ \nFortran \n2 Example declare_target.7.f90 (omp_5.2) \nS-1 module subs \nS-2 \nS-3 contains \nS-4 subroutine foo() \nS-5 !$omp declare target enter(foo) device_type(nohost) \nS-6 !$omp declare variant(foo_onhost) match(device={kind(host)}) \nS-7 ! device specific computation \nS-8 end subroutine \nS-9 \nS-10 subroutine foo_onhost() \nS-11 print *,\u2019 On host.\u2019 \nS-12 end subroutine \nS-13 \nS-14 end module \n232 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 \nS-16 program main \nS-17 \nS-18 use subs \nS-19 !$omp target \nS-20 call foo !calls foo() on device or \nS-21 !foo_onhost() in case of host fallback \nS-22 !$omp end target \nS-23 \nS-24 end program \nFortran \nCHAPTER 6. DEVICES 233 \n"}
{"section_title": "6.14 Lambda Expressions", "chunk": ""}
{"section_title": "6.14 Lambda Expressions", "chunk": "C++ \n2 The following example illustrates the usage of lambda expressions and their corresponding closure \n3 objects within a target region. \n4 In CASE 1, a lambda expression is defined inside a target construct that implicitly maps the \n5 structure s. Inside the construct, the lambda captures (by reference) the corresponding s, and the \n6 resulting closure object is assigned to lambda1. When the call operator is invoked on lambda1, the \n7 captured reference to s is used in the call. The modified s is then copied back to the host device on \n8 exit from the target construct. \n9 In CASE 2, a lambda expression is instead defined before the target construct and captures (by \n10 copy) the pointer sp. A target data construct is used to first map the structure, and then the \n11 target construct implicitly maps the closure object referenced by lambda2, a zero-length array \n12 section based on the structure pointer sp, and a zero-length array section based on the captured \n13 pointer in the closure object. The implicit maps result in attached pointers to the corresponding \n14 structure. The call for lambda2 inside the target construct will access sp->a and sp->b from the \n15 corresponding structure. \n16 CASE 3 is similar to CASE 2, except s is instead captured by reference by the lambda expression. \n17 As for CASE 2, the structure is first mapped by an enclosing target data construct, and then the \n18 target construct implicitly maps s and the closure object referenced by lambda3. The effect of \n19 the map is to make the the call for lambda3 refer to the corresponding s inside the target \n20 construct rather than the original s. \n21 In CASE 4, the program defines a static variable ss of the same structure type as s. While the body \n22 of the lambda expression refers to ss, it is not captured. In order for lambda4 to be callable in the \n23 target region, the reference to ss should be to a device copy of ss that also has static storage. \n24 This is achieved with the use of the declare target directive. Inside the target construct, all \n25 references to ss, including in the lambda4() call, will refer to the corresponding ss that results from \n26 the declare target directive. The always modifier is used on the map clause to transfer the \n27 updated values for the structure back to the host device. \n28 Example lambda_expressions.1.cpp (omp_5.0) \nS-1 #include <iostream> \nS-2 using namespace std; \nS-3 \nS-4 struct S { int a; int b; }; \nS-5 \nS-6 int main() \nS-7 { \nS-8 \nS-9 // CASE 1 Lambda defined in target region \nS-10 \nS-11 S s = S {0,1}; \n234 OpenMP Examples Version 5.2.1 - November 2022 \nS-12 \nS-13 #pragma omp target \nS-14 { \nS-15 auto lambda1 = [&s]() { s.a = s.b * 2; }; \nS-16 s.b += 2; \nS-17 lambda1(); // s.a = 3 * 2 \nS-18 } \nS-19 cout << s.a << \" \" << s.b << endl; //OUT 6 3 \nS-20 \nS-21 // CASE 2 Host defined lambda, Capture pointer to s \nS-22 \nS-23 s = {0,1}; \nS-24 S *sp = &s; \nS-25 auto lambda2 = [sp]() {sp->a = sp->b * 2; }; \nS-26 \nS-27 // closure object\u2019s sp attaches to corresponding s on target \nS-28 // construct \nS-29 #pragma omp target data map(sp[0]) \nS-30 #pragma omp target \nS-31 { \nS-32 sp->b += 2; \nS-33 lambda2(); \nS-34 } \nS-35 cout << s.a << \" \" << s.b << endl; //OUT 6 3 \nS-36 \nS-37 // CASE 3 Host defined lambda, Capture s by reference \nS-38 \nS-39 s = {0,1}; \nS-40 auto lambda3 = [&s]() {s.a = s.b * 2; }; \nS-41 \nS-42 // closure object\u2019s s refers to corresponding s in target \nS-43 // construct \nS-44 #pragma omp target data map(s) \nS-45 #pragma omp target \nS-46 { \nS-47 s.b += 2; \nS-48 lambda3(); \nS-49 } \nS-50 cout << s.a << \" \" << s.b << endl; //OUT 6 3 \nS-51 \nS-52 // CASE 4 Host defined lambda, references static variable \nS-53 \nS-54 static S ss = {0,1}; \nS-55 #pragma omp declare target enter(ss) \nS-56 auto lambda4 = [&]() {ss.a = ss.b * 2; }; \nS-57 \nS-58 #pragma omp target map(always,from:ss) \nCHAPTER 6. DEVICES 235 \nS-59 { \nS-60 ss.b += 2; \nS-61 lambda4(); \nS-62 } \nS-63 cout << ss.a << \" \" << ss.b << endl; //OUT 6 3 \nS-64 \nS-65 return 0; \nS-66 } \nC++ \n236 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.15 teams Construct and Related Combined Constructs", "chunk": "2 Constructs \n"}
{"section_title": "6.15.1 target and teams Constructs with omp_get_num_teams and omp_get_team_num Routines", "chunk": ""}
{"section_title": "6.15.1 target and teams Constructs with omp_get_num_teams and omp_get_team_num Routines", "chunk": "4 and omp_get_team_num Routines \n5 The following example shows how the target and teams constructs are used to create a league \n6 of thread teams that execute a region. The teams construct creates a league of at most two teams \n7 where the primary thread of each team executes the teams region. \n8 The omp_get_num_teams routine returns the number of teams executing in a teams region. \n9 The omp_get_team_num routine returns the team number, which is an integer between 0 and \n10 one less than the value returned by omp_get_num_teams. The following example manually \n11 distributes a loop across two teams. \nC / C++ \n12 Example teams.1.c (omp_4.0) \nS-1 #include <stdlib.h> \nS-2 #include <omp.h> \nS-3 float dotprod(float B[], float C[], int N) \nS-4 { \nS-5 float sum0 = 0.0; \nS-6 float sum1 = 0.0; \nS-7 #pragma omp target map(to: B[:N], C[:N]) map(tofrom: sum0, sum1) \nS-8 #pragma omp teams num_teams(2) \nS-9 { \nS-10 int i; \nS-11 if (omp_get_num_teams() != 2) \nS-12 abort(); \nS-13 if (omp_get_team_num() == 0) \nS-14 { \nS-15 #pragma omp parallel for reduction(+:sum0) \nS-16 for (i=0; i<N/2; i++) \nS-17 sum0 += B[i] * C[i]; \nS-18 } \nS-19 else if (omp_get_team_num() == 1) \nS-20 { \nS-21 #pragma omp parallel for reduction(+:sum1) \nS-22 for (i=N/2; i<N; i++) \nS-23 sum1 += B[i] * C[i]; \nS-24 } \nS-25 } \nS-26 return sum0 + sum1; \nS-27 } \nS-28 \nCHAPTER 6. DEVICES 237 \nS-29 /* Note: The variables sum0,sum1 are now mapped with tofrom, for \nS-30 correct execution with 4.5 (and pre-4.5) compliant compilers. \nS-31 See Devices Intro. \nS-32 */ \nC / C++ \nFortran \n1 Example teams.1.f90 (omp_4.0) \nS-1 function dotprod(B,C,N) result(sum) \nS-2 use omp_lib, ONLY : omp_get_num_teams, omp_get_team_num \nS-3 real :: B(N), C(N), sum,sum0, sum1 \nS-4 integer :: N, i \nS-5 sum0 = 0.0e0 \nS-6 sum1 = 0.0e0 \nS-7 !$omp target map(to: B, C) map(tofrom: sum0, sum1) \nS-8 !$omp teams num_teams(2) \nS-9 if (omp_get_num_teams() /= 2) stop \"2 teams required\" \nS-10 if (omp_get_team_num() == 0) then \nS-11 !$omp parallel do reduction(+:sum0) \nS-12 do i=1,N/2 \nS-13 sum0 = sum0 + B(i) * C(i) \nS-14 end do \nS-15 else if (omp_get_team_num() == 1) then \nS-16 !$omp parallel do reduction(+:sum1) \nS-17 do i=N/2+1,N \nS-18 sum1 = sum1 + B(i) * C(i) \nS-19 end do \nS-20 end if \nS-21 !$omp end teams \nS-22 !$omp end target \nS-23 sum = sum0 + sum1 \nS-24 end function \nS-25 \nS-26 ! Note: The variables sum0,sum1 are now mapped with tofrom, for correct \nS-27 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \n238 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.15.2 target, teams, and distribute Constructs", "chunk": ""}
{"section_title": "6.15.2 target, teams, and distribute Constructs", "chunk": "2 The following example shows how the target, teams, and distribute constructs are used to \n3 execute a loop nest in a target region. The teams construct creates a league and the primary \n4 thread of each team executes the teams region. The distribute construct schedules the \n5 subsequent loop iterations across the primary threads of each team. \n6 The number of teams in the league is less than or equal to the variable num_blocks. Each team in \n7 the league has a number of threads less than or equal to the variable block_threads. The iterations \n8 in the outer loop are distributed among the primary threads of each team. \n9 When a team\u2019s primary thread encounters the parallel loop construct before the inner loop, the \n10 other threads in its team are activated. The team executes the parallel region and then \n11 workshares the execution of the loop. \n12 Each primary thread executing the teams region has a private copy of the variable sum that is \n13 created by the reduction clause on the teams construct. The primary thread and all threads in \n14 its team have a private copy of the variable sum that is created by the reduction clause on the \n15 parallel loop construct. The second private sum is reduced into the primary thread\u2019s private copy of \n16 sum created by the teams construct. At the end of the teams region, each primary thread\u2019s \n17 private copy of sum is reduced into the final sum that is implicitly mapped into the target region. \nC / C++ \n18 Example teams.2.c (omp_4.0) \nS-1 #define min(x, y) (((x) < (y)) ? (x) : (y)) \nS-2 \nS-3 float dotprod(float B[], float C[], int N, int block_size, \nS-4 int num_teams, int block_threads) \nS-5 { \nS-6 float sum = 0.0; \nS-7 int i, i0; \nS-8 #pragma omp target map(to: B[0:N], C[0:N]) map(tofrom: sum) \nS-9 #pragma omp teams num_teams(num_teams) thread_limit(block_threads) \\ \nS-10 reduction(+:sum) \nS-11 #pragma omp distribute \nS-12 for (i0=0; i0<N; i0 += block_size) \nS-13 #pragma omp parallel for reduction(+:sum) \nS-14 for (i=i0; i< min(i0+block_size,N); i++) \nS-15 sum += B[i] * C[i]; \nS-16 return sum; \nS-17 } \nS-18 /* Note: The variable sum is now mapped with tofrom, for correct \nS-19 execution with 4.5 (and pre-4.5) compliant compilers. See \nS-20 Devices Intro. \nS-21 */ \nC / C++ \nCHAPTER 6. DEVICES 239 \nFortran \n1 Example teams.2.f90 (omp_4.0) \nS-1 function dotprod(B,C,N, block_size, num_teams, block_threads) result(sum) \nS-2 implicit none \nS-3 real :: B(N), C(N), sum \nS-4 integer :: N, block_size, num_teams, block_threads, i, i0 \nS-5 sum = 0.0e0 \nS-6 !$omp target map(to: B, C) map(tofrom: sum) \nS-7 !$omp teams num_teams(num_teams) thread_limit(block_threads) & \nS-8 !$omp& reduction(+:sum) \nS-9 !$omp distribute \nS-10 do i0=1,N, block_size \nS-11 !$omp parallel do reduction(+:sum) \nS-12 do i = i0, min(i0+block_size,N) \nS-13 sum = sum + B(i) * C(i) \nS-14 end do \nS-15 end do \nS-16 !$omp end teams \nS-17 !$omp end target \nS-18 end function \nS-19 \nS-20 ! Note: The variable sum is now mapped with tofrom, for correct \nS-21 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \n"}
{"section_title": "6.15.3 target teams, and Distribute Parallel Loop Constructs", "chunk": ""}
{"section_title": "6.15.3 target teams, and Distribute Parallel Loop Constructs", "chunk": "3 Loop Constructs \n4 The following example shows how the target teams and distribute parallel loop constructs are \n5 used to execute a target region. The target teams construct creates a league of teams where \n6 the primary thread of each team executes the teams region. \n7 The distribute parallel loop construct schedules the loop iterations across the primary threads of \n8 each team and then across the threads of each team. \n240 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example teams.3.c (omp_4.5) \nS-1 float dotprod(float B[], float C[], int N) \nS-2 { \nS-3 float sum = 0; \nS-4 int i; \nS-5 #pragma omp target teams map(to: B[0:N], C[0:N]) \\ \nS-6 defaultmap(tofrom:scalar) reduction(+:sum) \nS-7 #pragma omp distribute parallel for reduction(+:sum) \nS-8 for (i=0; i<N; i++) \nS-9 sum += B[i] * C[i]; \nS-10 return sum; \nS-11 } \nS-12 \nS-13 /* Note: The variable sum is now mapped with tofrom from the defaultmap \nS-14 clause on the combined target teams construct, for correct \nS-15 execution with 4.5 (and pre-4.5) compliant compilers. \nS-16 See Devices Intro. \nS-17 */ \nC / C++ \nFortran \n2 Example teams.3.f90 (omp_4.5) \nS-1 function dotprod(B,C,N) result(sum) \nS-2 real :: B(N), C(N), sum \nS-3 integer :: N, i \nS-4 sum = 0.0e0 \nS-5 !$omp target teams map(to: B, C) & \nS-6 !$omp& defaultmap(tofrom:scalar) reduction(+:sum) \nS-7 !$omp distribute parallel do reduction(+:sum) \nS-8 do i = 1,N \nS-9 sum = sum + B(i) * C(i) \nS-10 end do \nS-11 !$omp end target teams \nS-12 end function \nS-13 \nS-14 ! Note: The variable sum is now mapped with tofrom from the defaultmap \nS-15 ! clause on the combined target teams construct, for correct \nS-16 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \nCHAPTER 6. DEVICES 241 \n"}
{"section_title": "6.15.4 target teams and Distribute Parallel Loop Constructs with Scheduling Clauses", "chunk": ""}
{"section_title": "6.15.4 target teams and Distribute Parallel Loop Constructs with Scheduling Clauses", "chunk": "2 Loop Constructs with Scheduling Clauses \n3 The following example shows how the target teams and distribute parallel loop constructs are \n4 used to execute a target region. The teams construct creates a league of at most eight teams \n5 where the primary thread of each team executes the teams region. The number of threads in each \n6 team is less than or equal to 16. \n7 The distribute parallel loop construct schedules the subsequent loop iterations across the \n8 primary threads of each team and then across the threads of each team. \n9 The dist_schedule clause on the distribute parallel loop construct indicates that loop iterations \n10 are distributed to the primary thread of each team in chunks of 1024 iterations. \n11 The schedule clause indicates that the 1024 iterations distributed to a primary thread are then \n12 assigned to the threads in its associated team in chunks of 64 iterations. \nC / C++ \n13 Example teams.4.c (omp_4.0) \nS-1 #define N 1024*1024 \nS-2 float dotprod(float B[], float C[]) \nS-3 { \nS-4 float sum = 0.0; \nS-5 int i; \nS-6 #pragma omp target map(to: B[0:N], C[0:N]) map(tofrom: sum) \nS-7 #pragma omp teams num_teams(8) thread_limit(16) reduction(+:sum) \nS-8 #pragma omp distribute parallel for reduction(+:sum) \\ \nS-9 dist_schedule(static, 1024) schedule(static, 64) \nS-10 for (i=0; i<N; i++) \nS-11 sum += B[i] * C[i]; \nS-12 return sum; \nS-13 } \nS-14 \nS-15 /* Note: The variable sum is now mapped with tofrom, for correct \nS-16 execution with 4.5 (and pre-4.5) compliant compilers. \nS-17 See Devices Intro. \nS-18 */ \nC / C++ \n242 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example teams.4.f90 (omp_4.0) \nS-1 module arrays \nS-2 integer,parameter :: N=1024*1024 \nS-3 real :: B(N), C(N) \nS-4 end module \nS-5 function dotprod() result(sum) \nS-6 use arrays \nS-7 real :: sum \nS-8 integer :: i \nS-9 sum = 0.0e0 \nS-10 !$omp target map(to: B, C) map(tofrom: sum) \nS-11 !$omp teams num_teams(8) thread_limit(16) reduction(+:sum) \nS-12 !$omp distribute parallel do reduction(+:sum) & \nS-13 !$omp& dist_schedule(static, 1024) schedule(static, 64) \nS-14 do i = 1,N \nS-15 sum = sum + B(i) * C(i) \nS-16 end do \nS-17 !$omp end teams \nS-18 !$omp end target \nS-19 end function \nS-20 \nS-21 ! Note: The variable sum is now mapped with tofrom, for correct \nS-22 ! execution with 4.5 (and pre-4.5) compliant compilers. See Devices Intro. \nFortran \n"}
{"section_title": "6.15.5 target teams and distribute simd Constructs", "chunk": ""}
{"section_title": "6.15.5 target teams and distribute simd Constructs", "chunk": "3 Constructs \n4 The following example shows how the target teams and distribute simd constructs are \n5 used to execute a loop in a target region. The target teams construct creates a league of \n6 teams where the primary thread of each team executes the teams region. \n7 The distribute simd construct schedules the loop iterations across the primary thread of each \n8 team and then uses SIMD parallelism to execute the iterations. \nCHAPTER 6. DEVICES 243 \nC / C++ \n1 Example teams.5.c (omp_4.0) \nS-1 extern void init(float *, float *, int); \nS-2 extern void output(float *, int); \nS-3 void vec_mult(float *p, float *v1, float *v2, int N) \nS-4 { \nS-5 int i; \nS-6 init(v1, v2, N); \nS-7 #pragma omp target teams map(to: v1[0:N], v2[:N]) map(from: p[0:N]) \nS-8 #pragma omp distribute simd \nS-9 for (i=0; i<N; i++) \nS-10 p[i] = v1[i] * v2[i]; \nS-11 output(p, N); \nS-12 } \nC / C++ \nFortran \n2 Example teams.5.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 real :: p(N), v1(N), v2(N) \nS-3 integer :: i \nS-4 call init(v1, v2, N) \nS-5 !$omp target teams map(to: v1, v2) map(from: p) \nS-6 !$omp distribute simd \nS-7 do i=1,N \nS-8 p(i) = v1(i) * v2(i) \nS-9 end do \nS-10 !$omp end target teams \nS-11 call output(p, N) \nS-12 end subroutine \nFortran \n244 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.15.6 target teams and Distribute Parallel Loop SIMD Constructs", "chunk": ""}
{"section_title": "6.15.6 target teams and Distribute Parallel Loop SIMD Constructs", "chunk": "2 Loop SIMD Constructs \n3 The following example shows how the target teams and the distribute parallel loop SIMD \n4 constructs are used to execute a loop in a target teams region. The target teams construct \n5 creates a league of teams where the primary thread of each team executes the teams region. \n6 The distribute parallel loop SIMD construct schedules the loop iterations across the primary thread \n7 of each team and then across the threads of each team where each thread uses SIMD parallelism. \nC / C++ \n8 Example teams.6.c (omp_4.0) \nS-1 extern void init(float *, float *, int); \nS-2 extern void output(float *, int); \nS-3 void vec_mult(float *p, float *v1, float *v2, int N) \nS-4 { \nS-5 int i; \nS-6 init(v1, v2, N); \nS-7 #pragma omp target teams map(to: v1[0:N], v2[:N]) map(from: p[0:N]) \nS-8 #pragma omp distribute parallel for simd \nS-9 for (i=0; i<N; i++) \nS-10 p[i] = v1[i] * v2[i]; \nS-11 output(p, N); \nS-12 } \nC / C++ \nFortran \n9 Example teams.6.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 real :: p(N), v1(N), v2(N) \nS-3 integer :: i \nS-4 call init(v1, v2, N) \nS-5 !$omp target teams map(to: v1, v2) map(from: p) \nS-6 !$omp distribute parallel do simd \nS-7 do i=1,N \nS-8 p(i) = v1(i) * v2(i) \nS-9 end do \nS-10 !$omp end target teams \nS-11 call output(p, N) \nS-12 end subroutine \nFortran \nCHAPTER 6. DEVICES 245 \n"}
{"section_title": "6.16 Asynchronous target Execution and Dependences", "chunk": "2 and Dependences \n3 Asynchronous execution of a target region can be accomplished by creating an explicit task \n4 around the target region. Examples with explicit tasks are shown at the beginning of this section. \n5 As of OpenMP 4.5 and beyond the nowait clause can be used on the target directive for \n6 asynchronous execution. Examples with nowait clauses follow the explicit task examples. \n7 This section also shows the use of depend clauses to order executions through dependences. \n"}
{"section_title": "6.16.1 Asynchronous target with Tasks", "chunk": ""}
{"section_title": "6.16.1 Asynchronous target with Tasks", "chunk": "9 The following example shows how the task and target constructs are used to execute multiple \n10 target regions asynchronously. The task that encounters the task construct generates an \n11 explicit task that contains a target region. The thread executing the explicit task encounters a \n12 task scheduling point while waiting for the execution of the target region to complete, allowing \n13 the thread to switch back to the execution of the encountering task or one of the previously \n14 generated explicit tasks. \nC / C++ \n15 Example async_target.1.c (omp_5.1) \nS-1 #pragma omp begin declare target \nS-2 float F(float); \nS-3 #pragma omp end declare target \nS-4 \nS-5 #define N 1000000000 \nS-6 #define CHUNKSZ 1000000 \nS-7 void init(float *, int); \nS-8 float Z[N]; \nS-9 void pipedF(){ \nS-10 int C, i; \nS-11 init(Z, N); \nS-12 for (C=0; C<N; C+=CHUNKSZ){ \nS-13 #pragma omp task shared(Z) \nS-14 #pragma omp target map(Z[C:CHUNKSZ]) \nS-15 #pragma omp parallel for \nS-16 for (i=0; i<CHUNKSZ; i++) Z[i] = F(Z[i]); \nS-17 } \nS-18 #pragma omp taskwait \nS-19 } \nC / C++ \n246 OpenMP Examples Version 5.2.1 - November 2022 \n1 The Fortran version has an interface block that contains the declare target. An identical \n2 statement exists in the function declaration (not shown here). \nFortran \n3 Example async_target.1.f90 (omp_4.0) \nS-1 module parameters \nS-2 integer, parameter :: N=1000000000, CHUNKSZ=1000000 \nS-3 end module \nS-4 subroutine pipedF() \nS-5 use parameters, ONLY: N, CHUNKSZ \nS-6 integer :: C, i \nS-7 real :: z(N) \nS-8 \nS-9 interface \nS-10 function F(z) \nS-11 !$omp declare target \nS-12 real, intent(IN) ::z \nS-13 real ::F \nS-14 end function F \nS-15 end interface \nS-16 \nS-17 call init(z,N) \nS-18 \nS-19 do C=1,N,CHUNKSZ \nS-20 \nS-21 !$omp task shared(z) \nS-22 !$omp target map(z(C:C+CHUNKSZ-1)) \nS-23 !$omp parallel do \nS-24 do i=C,C+CHUNKSZ-1 \nS-25 z(i) = F(z(i)) \nS-26 end do \nS-27 !$omp end target \nS-28 !$omp end task \nS-29 \nS-30 end do \nS-31 !$omp taskwait \nS-32 print*, z \nS-33 \nS-34 end subroutine pipedF \nFortran \n4 The following example shows how the task and target constructs are used to execute multiple \n5 target regions asynchronously. The task dependence ensures that the storage is allocated and \n6 initialized on the device before it is accessed. \nCHAPTER 6. DEVICES 247 \nC / C++ \n1 Example async_target.2.c (omp_5.1) \nS-1 #include <stdlib.h> \nS-2 #include <omp.h> \nS-3 \nS-4 #pragma omp begin declare target \nS-5 extern void init(float *, float *, int); \nS-6 #pragma omp end declare target \nS-7 \nS-8 extern void foo(); \nS-9 extern void output(float *, int); \nS-10 void vec_mult(float *p, int N, int dev) \nS-11 { \nS-12 float *v1, *v2; \nS-13 int i; \nS-14 #pragma omp task shared(v1, v2) depend(out: v1, v2) \nS-15 #pragma omp target device(dev) map(v1, v2) \nS-16 { \nS-17 // check whether on device dev \nS-18 if (omp_is_initial_device()) \nS-19 abort(); \nS-20 v1 = (float *)malloc(N*sizeof(float)); \nS-21 v2 = (float *)malloc(N*sizeof(float)); \nS-22 init(v1, v2, N); \nS-23 } \nS-24 foo(); // execute other work asychronously \nS-25 #pragma omp task shared(v1, v2, p) depend(in: v1, v2) \nS-26 #pragma omp target device(dev) map(to: v1, v2) map(from: p[0:N]) \nS-27 { \nS-28 // check whether on device dev \nS-29 if (omp_is_initial_device()) \nS-30 abort(); \nS-31 #pragma omp parallel for \nS-32 for (i=0; i<N; i++) \nS-33 p[i] = v1[i] * v2[i]; \nS-34 free(v1); \nS-35 free(v2); \nS-36 } \nS-37 #pragma omp taskwait \nS-38 output(p, N); \nS-39 } \nC / C++ \n2 The Fortran example below is similar to the C version above. Instead of pointers, though, it uses the \n3 convenience of Fortran allocatable arrays on the device. In order to preserve the arrays allocated on \n4 the device across multiple target regions, a target data region is used in this case. \n248 OpenMP Examples Version 5.2.1 - November 2022 \n1 If there is no shape specified for an allocatable array in a map clause, only the array descriptor (also \n2 called a dope vector) is mapped. That is, device space is created for the descriptor, and it is initially \n3 populated with host values. In this case, the v1 and v2 arrays will be in a non-associated state on the \n4 device. When space for v1 and v2 is allocated on the device in the first target region the \n5 addresses to the space will be included in their descriptors. \n6 At the end of the first target region, the arrays v1 and v2 are preserved on the device for access in \n7 the second target region. At the end of the second target region, the data in array p is copied \n8 back, the arrays v1 and v2 are not. \n9 A depend clause is used in the task directive to provide a wait at the beginning of the second \n10 target region, to insure that there is no race condition with v1 and v2 in the two tasks. It would \n11 be noncompliant to use v1 and/or v2 in lieu of N in the depend clauses, because the use of \n12 non-allocated allocatable arrays as list items in a depend clause would lead to unspecified \n13 behavior. \n14 Note \u2013 This example is not strictly compliant with the OpenMP 4.5 specification since the \n15 allocation status of allocatable arrays v1 and v2 is changed inside the target region, which is not \n16 allowed. (See the restrictions for the map clause in the Data-mapping Attribute Rules and Clauses \n17 section of the specification.) However, the intention is to relax the restrictions on mapping of \n18 allocatable variables in the next release of the specification so that the example will be compliant. \nFortran \n19 Example async_target.2.f90 (omp_4.0) \nS-1 subroutine mult(p, N, idev) \nS-2 use omp_lib, ONLY: omp_is_initial_device \nS-3 real :: p(N) \nS-4 real,allocatable :: v1(:), v2(:) \nS-5 integer :: i, idev \nS-6 !$omp declare target (init) \nS-7 \nS-8 !$omp target data map(v1,v2) \nS-9 \nS-10 !$omp task shared(v1,v2) depend(out: N) \nS-11 !$omp target device(idev) \nS-12 if( omp_is_initial_device() ) & \nS-13 stop \"not executing on target device\" \nS-14 allocate(v1(N), v2(N)) \nS-15 call init(v1,v2,N) \nS-16 !$omp end target \nS-17 !$omp end task \nS-18 \nS-19 call foo() ! execute other work asychronously \nS-20 \nS-21 !$omp task shared(v1,v2,p) depend(in: N) \nS-22 !$omp target device(idev) map(from: p) \nCHAPTER 6. DEVICES 249 \nS-23 if( omp_is_initial_device() ) & \nS-24 stop \"not executing on target device\" \nS-25 !$omp parallel do \nS-26 do i = 1,N \nS-27 p(i) = v1(i) * v2(i) \nS-28 end do \nS-29 deallocate(v1,v2) \nS-30 \nS-31 !$omp end target \nS-32 !$omp end task \nS-33 \nS-34 !$omp taskwait \nS-35 \nS-36 !$omp end target data \nS-37 \nS-38 call output(p, N) \nS-39 \nS-40 end subroutine \nFortran \n"}
{"section_title": "6.16.2 nowait Clause on target Construct", "chunk": ""}
{"section_title": "6.16.2 nowait Clause on target Construct", "chunk": "2 The following example shows how to execute code asynchronously on a device without an explicit \n3 task. The nowait clause on a target construct allows the thread of the target task to perform \n4 other work while waiting for the target region execution to complete. Hence, the target \n5 region can execute asynchronously on the device (without requiring a host thread to idle while \n6 waiting for the target task execution to complete). \n7 In this example the product of two vectors (arrays), v1 and v2, is formed. One half of the operations \n8 is performed on the device, and the last half on the host, concurrently. \n9 After a team of threads is formed the primary thread generates the target task while the other \n10 threads can continue on, without a barrier, to the execution of the host portion of the vector product. \n11 The completion of the target task (asynchronous target execution) is guaranteed by the \n12 synchronization in the implicit barrier at the end of the host vector-product worksharing loop \n13 region. See the barrier glossary entry in the OpenMP specification for details. \n14 The host loop scheduling is dynamic, to balance the host thread executions, since one thread is \n15 being used for offload generation. In the situation where little time is spent by the target task in \n16 setting up and tearing down the target execution, static scheduling may be desired. \n250 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example async_target.3.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define N 1000000 //N must be even \nS-4 void init(int n, float *v1, float *v2); \nS-5 \nS-6 int main(){ \nS-7 int i, n=N; \nS-8 int chunk=1000; \nS-9 float v1[N],v2[N],vxv[N]; \nS-10 \nS-11 init(n, v1,v2); \nS-12 \nS-13 #pragma omp parallel \nS-14 { \nS-15 \nS-16 #pragma omp masked \nS-17 #pragma omp target teams distribute parallel for nowait \\ \nS-18 map(to: v1[0:n/2]) \\ \nS-19 map(to: v2[0:n/2]) \\ \nS-20 map(from: vxv[0:n/2]) \nS-21 for(i=0; i<n/2; i++){ vxv[i] = v1[i]*v2[i]; } \nS-22 \nS-23 #pragma omp for schedule(dynamic,chunk) \nS-24 for(i=n/2; i<n; i++){ vxv[i] = v1[i]*v2[i]; } \nS-25 \nS-26 } \nS-27 printf(\" vxv[0] vxv[n-1] %f %f\\n\", vxv[0], vxv[n-1]); \nS-28 return 0; \nS-29 } \nC / C++ \nFortran \n2 Example async_target.3.f90 (omp_5.1) \nS-1 program concurrent_async \nS-2 use omp_lib \nS-3 integer,parameter :: n=1000000 !!n must be even \nS-4 integer :: i, chunk=1000 \nS-5 real :: v1(n),v2(n),vxv(n) \nS-6 \nS-7 call init(n, v1,v2) \nS-8 \nS-9 !$omp parallel \nS-10 \nCHAPTER 6. DEVICES 251 \nS-11 !$omp masked \nS-12 !$omp target teams distribute parallel do nowait & \nS-13 !$omp& map(to: v1(1:n/2)) & \nS-14 !$omp& map(to: v2(1:n/2)) & \nS-15 !$omp& map(from: vxv(1:n/2)) \nS-16 do i = 1,n/2; vxv(i) = v1(i)*v2(i); end do \nS-17 !$omp end masked \nS-18 \nS-19 !$omp do schedule(dynamic,chunk) \nS-20 do i = n/2+1,n; vxv(i) = v1(i)*v2(i); end do \nS-21 \nS-22 !$omp end parallel \nS-23 \nS-24 print*, \" vxv(1) vxv(n) :\", vxv(1), vxv(n) \nS-25 \nS-26 end program \nFortran \n"}
{"section_title": "6.16.3 Asynchronous target with nowait and depend Clauses", "chunk": ""}
{"section_title": "6.16.3 Asynchronous target with nowait and depend Clauses", "chunk": "2 depend Clauses \n3 More details on dependences can be found in Section 5.3 on page 105, Task Dependences. In this \n4 example, there are three flow dependences. In the first two dependences the target task does not \n5 execute until the preceding explicit tasks have finished. These dependences are produced by arrays \n6 v1 and v2 with the out dependence type in the first two tasks, and the in dependence type in the \n7 target task. \n8 The last dependence is produced by array p with the out dependence type in the target task, and \n9 the in dependence type in the last task. The last task does not execute until the target task finishes. \n10 The nowait clause on the target construct creates a deferrable target task, allowing the \n11 encountering task to continue execution without waiting for the completion of the target task. \nC / C++ \n12 Example async_target.4.c (omp_4.5) \nS-1 extern void init( float*, int); \nS-2 extern void output(float*, int); \nS-3 \nS-4 void vec_mult(int N) \nS-5 { \nS-6 int i; \nS-7 float p[N], v1[N], v2[N]; \nS-8 \nS-9 #pragma omp parallel num_threads(2) \n252 OpenMP Examples Version 5.2.1 - November 2022 \nS-10 { \nS-11 #pragma omp single \nS-12 { \nS-13 #pragma omp task depend(out:v1) \nS-14 init(v1, N); \nS-15 \nS-16 #pragma omp task depend(out:v2) \nS-17 init(v2, N); \nS-18 \nS-19 #pragma omp target nowait depend(in:v1,v2) depend(out:p) \\ \nS-20 map(to:v1,v2) map( from: p) \nS-21 #pragma omp parallel for private(i) \nS-22 for (i=0; i<N; i++) \nS-23 p[i] = v1[i] * v2[i]; \nS-24 \nS-25 #pragma omp task depend(in:p) \nS-26 output(p, N); \nS-27 } \nS-28 } \nS-29 } \nC / C++ \nFortran \n1 Example async_target.4.f90 (omp_4.5) \nS-1 subroutine vec_mult(N) \nS-2 implicit none \nS-3 integer :: i, N \nS-4 real, allocatable :: p(:), v1(:), v2(:) \nS-5 allocate( p(N), v1(N), v2(N) ) \nS-6 \nS-7 !$omp parallel num_threads(2) \nS-8 \nS-9 !$omp single \nS-10 \nS-11 !$omp task depend(out:v1) \nS-12 call init(v1, N) \nS-13 !$omp end task \nS-14 \nS-15 !$omp task depend(out:v2) \nS-16 call init(v2, N) \nS-17 !$omp end task \nS-18 \nS-19 !$omp target nowait depend(in:v1,v2) depend(out:p) & \nS-20 !$omp& map(to:v1,v2) map(from: p) \nS-21 !$omp parallel do \nS-22 do i=1,N \nCHAPTER 6. DEVICES 253 \nS-23 p(i) = v1(i) * v2(i) \nS-24 end do \nS-25 !$omp end target \nS-26 \nS-27 \nS-28 !$omp task depend(in:p) \nS-29 call output(p, N) \nS-30 !$omp end task \nS-31 \nS-32 !$omp end single \nS-33 !$omp end parallel \nS-34 \nS-35 deallocate( p, v1, v2 ) \nS-36 \nS-37 end subroutine \nFortran \n254 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.17 Device Routines", "chunk": ""}
{"section_title": "6.17.1 omp_is_initial_device Routine", "chunk": ""}
{"section_title": "6.17.1 omp_is_initial_device Routine", "chunk": "3 The following example shows how the omp_is_initial_device runtime library routine can \n4 be used to query if a code is executing on the initial host device or on a target device. The example \n5 then sets the number of threads in the parallel region based on where the code is executing. \nC / C++ \n6 Example device.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 #pragma omp begin declare target \nS-5 void vec_mult(float *p, float *v1, float *v2, int N); \nS-6 extern float *p, *v1, *v2; \nS-7 extern int N; \nS-8 #pragma omp end declare target \nS-9 \nS-10 extern void init_vars(float *, float *, int); \nS-11 extern void output(float *, int); \nS-12 \nS-13 void foo() \nS-14 { \nS-15 init_vars(v1, v2, N); \nS-16 #pragma omp target device(42) map(p[:N], v1[:N], v2[:N]) \nS-17 { \nS-18 vec_mult(p, v1, v2, N); \nS-19 } \nS-20 output(p, N); \nS-21 } \nS-22 \nS-23 void vec_mult(float *p, float *v1, float *v2, int N) \nS-24 { \nS-25 int i; \nS-26 int nthreads; \nS-27 if (!omp_is_initial_device()) \nS-28 { \nS-29 printf(\"1024 threads on target device\\n\"); \nS-30 nthreads = 1024; \nS-31 } \nS-32 else \nS-33 { \nS-34 printf(\"8 threads on initial device\\n\"); \nS-35 nthreads = 8; \nS-36 } \nCHAPTER 6. DEVICES 255 \nS-37 #pragma omp parallel for private(i) num_threads(nthreads) \nS-38 for (i=0; i<N; i++) \nS-39 p[i] = v1[i] * v2[i]; \nS-40 } \nC / C++ \nFortran \n1 Example device.1.f90 (omp_4.0) \nS-1 module params \nS-2 integer,parameter :: N=1024 \nS-3 end module params \nS-4 module vmult \nS-5 contains \nS-6 subroutine vec_mult(p, v1, v2, N) \nS-7 use omp_lib, ONLY : omp_is_initial_device \nS-8 !$omp declare target \nS-9 real :: p(N), v1(N), v2(N) \nS-10 integer :: i, nthreads, N \nS-11 if (.not. omp_is_initial_device()) then \nS-12 print*, \"1024 threads on target device\" \nS-13 nthreads = 1024 \nS-14 else \nS-15 print*, \"8 threads on initial device\" \nS-16 nthreads = 8 \nS-17 endif \nS-18 !$omp parallel do private(i) num_threads(nthreads) \nS-19 do i = 1,N \nS-20 p(i) = v1(i) * v2(i) \nS-21 end do \nS-22 end subroutine vec_mult \nS-23 end module vmult \nS-24 program prog_vec_mult \nS-25 use params \nS-26 use vmult \nS-27 real :: p(N), v1(N), v2(N) \nS-28 call init(v1,v2,N) \nS-29 !$omp target device(42) map(p, v1, v2) \nS-30 call vec_mult(p, v1, v2, N) \nS-31 !$omp end target \nS-32 call output(p, N) \nS-33 end program \nFortran \n256 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.17.2 omp_get_num_devices Routine", "chunk": ""}
{"section_title": "6.17.2 omp_get_num_devices Routine", "chunk": "2 The following example shows how the omp_get_num_devices runtime library routine can be \n3 used to determine the number of devices. \nC / C++ \n4 Example device.2.c (omp_4.0) \nS-1 #include <omp.h> \nS-2 extern void init(float *, float *, int); \nS-3 extern void output(float *, int); \nS-4 void vec_mult(float *p, float *v1, float *v2, int N) \nS-5 { \nS-6 int i; \nS-7 init(v1, v2, N); \nS-8 int ndev = omp_get_num_devices(); \nS-9 int do_offload = (ndev>0 && N>1000000); \nS-10 #pragma omp target if(do_offload) \\ \nS-11 map(to: v1[0:N], v2[:N]) \\ \nS-12 map(from: p[0:N]) \nS-13 #pragma omp parallel for if(N>1000) private(i) \nS-14 for (i=0; i<N; i++) \nS-15 p[i] = v1[i] * v2[i]; \nS-16 output(p, N); \nS-17 } \nC / C++ \nFortran \n5 Example device.2.f90 (omp_4.0) \nS-1 subroutine vec_mult(p, v1, v2, N) \nS-2 use omp_lib, ONLY : omp_get_num_devices \nS-3 real :: p(N), v1(N), v2(N) \nS-4 integer :: N, i, ndev \nS-5 logical :: do_offload \nS-6 call init(v1, v2, N) \nS-7 ndev = omp_get_num_devices() \nS-8 do_offload = (ndev>0) .and. (N>1000000) \nS-9 !$omp target if(do_offload) map(to: v1, v2) map(from: p) \nS-10 !$omp parallel do if(N>1000) \nS-11 do i=1,N \nS-12 p(i) = v1(i) * v2(i) \nS-13 end do \nS-14 !$omp end target \nS-15 call output(p, N) \nS-16 end subroutine \nFortran \nCHAPTER 6. DEVICES 257 \n"}
{"section_title": "6.17.3 omp_set_default_device and  omp_get_default_device Routines", "chunk": ""}
{"section_title": "6.17.3 omp_set_default_device and  omp_get_default_device Routines", "chunk": "2 omp_get_default_device Routines \n3 The following example shows how the omp_set_default_device and \n4 omp_get_default_device runtime library routines can be used to set the default device and \n5 determine the default device respectively. \nC / C++ \n6 Example device.3.c (omp_4.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 void foo(void) \nS-4 { \nS-5 int default_device = omp_get_default_device(); \nS-6 printf(\"Default device = %d\\n\", default_device); \nS-7 omp_set_default_device(default_device+1); \nS-8 if (omp_get_default_device() != default_device+1) \nS-9 printf(\"Default device is still = %d\\n\", default_device); \nS-10 } \nC / C++ \nFortran \n7 Example device.3.f90 (omp_4.0) \nS-1 program foo \nS-2 use omp_lib, ONLY : omp_get_default_device, omp_set_default_device \nS-3 integer :: old_default_device, new_default_device \nS-4 old_default_device = omp_get_default_device() \nS-5 print*, \"Default device = \", old_default_device \nS-6 new_default_device = old_default_device + 1 \nS-7 call omp_set_default_device(new_default_device) \nS-8 if (omp_get_default_device() == old_default_device) & \nS-9 print*,\"Default device is STILL = \", old_default_device \nS-10 end program \nFortran \n258 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "6.17.4 Device and Host Memory Association", "chunk": ""}
{"section_title": "6.17.4 Device and Host Memory Association", "chunk": "2 The association of device memory with host memory can be established by calling the \n3 omp_target_associate_ptr API routine as part of the mapping. The following example \n4 shows the use of this routine to associate device memory of size CS, allocated by the \n5 omp_target_alloc routine and pointed to by the device pointer dev_ptr, with a chunk of the \n6 host array arr starting at index ioff. In Fortran, the intrinsic function c_loc is called to obtain the \n7 corresponding C pointer (h_ptr) of arr(ioff) for use in the call to the API routine. \n8 Since the reference count of the resulting mapping is infinite, it is necessary to use the \n9 target update directive (or the always modifier in a map clause) to accomplish a data \n10 transfer between host and device. The explicit mapping of the array section arr[ioff:CS] (or \n11 arr(ioff:ioff+CS-1) in Fortran) on the target construct ensures that the allocated and associated \n12 device memory is used when referencing the array arr in the target region. The device pointer \n13 dev_ptr cannot be accessed directly after a call to the omp_target_associate_ptr routine. \n14 After the target region, the device pointer is disassociated from the current chunk of the host \n15 memory by calling the omp_target_disassociate_ptr routine before working on the next \n16 chunk. The device memory is freed by calling the omp_target_free routine at the end. \nC / C++ \n17 Example target_associate_ptr.1.c (omp_4.5) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 #define CS 50 \nS-5 #define N (CS*2) \nS-6 \nS-7 int main() { \nS-8 int arr[N]; \nS-9 int *dev_ptr; \nS-10 int dev; \nS-11 \nS-12 for (int i = 0; i < N; i++) \nS-13 arr[i] = i; \nS-14 \nS-15 dev = omp_get_default_device(); \nS-16 \nS-17 // Allocate device memory \nS-18 dev_ptr = (int *)omp_target_alloc(sizeof(int) * CS, dev); \nS-19 \nS-20 // Loop over chunks \nS-21 for (int ioff = 0; ioff < N; ioff += CS) { \nS-22 \nS-23 // Associate device memory with one chunk of host memory \nS-24 omp_target_associate_ptr(&arr[ioff], dev_ptr, \nCHAPTER 6. DEVICES 259 \nS-25 sizeof(int) * CS, 0, dev); \nS-26 \nS-27 printf(\"before: arr[%d]=%d\\n\", ioff, arr[ioff]); \nS-28 \nS-29 // Update the device data \nS-30 #pragma omp target update to(arr[ioff:CS]) device(dev) \nS-31 \nS-32 // Explicit mapping of arr to make sure that we use the allocated \nS-33 // and associated memory. No host-device data update here. \nS-34 #pragma omp target map(tofrom : arr[ioff:CS]) device(dev) \nS-35 for (int i = 0; i < CS; i++) { \nS-36 arr[i+ioff]++; \nS-37 } \nS-38 \nS-39 // Update the host data \nS-40 #pragma omp target update from(arr[ioff:CS]) device(dev) \nS-41 \nS-42 printf(\"after: arr[%d]=%d\\n\", ioff, arr[ioff]); \nS-43 \nS-44 // Disassociate device pointer from the current chunk of host memory \nS-45 // before next use \nS-46 omp_target_disassociate_ptr(&arr[ioff], dev); \nS-47 } \nS-48 \nS-49 // Free device memory \nS-50 omp_target_free(dev_ptr, dev); \nS-51 \nS-52 return 0; \nS-53 } \nS-54 /* Outputs: \nS-55 before: arr[0]=0 \nS-56 after: arr[0]=1 \nS-57 before: arr[50]=50 \nS-58 after: arr[50]=51 \nS-59 */ \nC / C++ \nFortran \n1 Example target_associate_ptr.1.f90 (omp_5.1) \nS-1 program target_associate \nS-2 use omp_lib \nS-3 use, intrinsic :: iso_c_binding \nS-4 implicit none \nS-5 \nS-6 integer, parameter :: CS = 50 \nS-7 integer, parameter :: N = CS*2 \n260 OpenMP Examples Version 5.2.1 - November 2022 \nS-8 integer, target :: arr(N) \nS-9 type(c_ptr) :: h_ptr, dev_ptr \nS-10 integer(c_size_t) :: csize, dev_off \nS-11 integer(c_int) :: dev \nS-12 integer :: i, ioff, s \nS-13 \nS-14 do i = 1, N \nS-15 arr(i) = i \nS-16 end do \nS-17 \nS-18 dev = omp_get_default_device() \nS-19 csize = c_sizeof(arr(1)) * CS \nS-20 \nS-21 ! Allocate device memory \nS-22 dev_ptr = omp_target_alloc(csize, dev) \nS-23 dev_off = 0 \nS-24 \nS-25 ! Loop over chunks \nS-26 do ioff = 1, N, CS \nS-27 \nS-28 ! Associate device memory with one chunk of host memory \nS-29 h_ptr = c_loc(arr(ioff)) \nS-30 s = omp_target_associate_ptr(h_ptr, dev_ptr, csize, dev_off, dev) \nS-31 \nS-32 print *, \"before: arr(\", ioff, \")=\", arr(ioff) \nS-33 \nS-34 ! Update the device data \nS-35 !$omp target update to(arr(ioff:ioff+CS-1)) device(dev) \nS-36 \nS-37 ! Explicit mapping of arr to make sure that we use the allocated \nS-38 ! and associated memory. No host-device data update here. \nS-39 !$omp target map(tofrom: arr(ioff:ioff+CS-1)) device(dev) \nS-40 do i = 0, CS-1 \nS-41 arr(i+ioff) = arr(i+ioff) + 1 \nS-42 end do \nS-43 !$omp end target \nS-44 \nS-45 ! Update the host data \nS-46 !$omp target update from(arr(ioff:ioff+CS-1)) device(dev) \nS-47 \nS-48 print *, \"after: arr(\", ioff, \")=\", arr(ioff) \nS-49 \nS-50 ! Disassociate device pointer from the current chunk of host memory \nS-51 ! before next use \nS-52 s = omp_target_disassociate_ptr(h_ptr, dev) \nS-53 end do \nS-54 \nCHAPTER 6. DEVICES 261 \nS-55 ! Free device memory \nS-56 call omp_target_free(dev_ptr, dev) \nS-57 \nS-58 end \nS-59 ! Outputs: \nS-60 ! before: arr( 1 )= 1 \nS-61 ! after: arr( 1 )= 2 \nS-62 ! before: arr( 51 )= 51 \nS-63 ! after: arr( 51 )= 52 \nFortran \n"}
{"section_title": "6.17.5 Target Memory and Device Pointers Routines", "chunk": ""}
{"section_title": "6.17.5 Target Memory and Device Pointers Routines", "chunk": "2 The following example shows how to create space on a device, transfer data to and from that space, \n3 and free the space, using API calls. The API calls directly execute allocation, copy and free \n4 operations on the device, without invoking any mapping through a target directive. The \n5 omp_target_alloc routine allocates space and returns a device pointer for referencing the \n6 space in the omp_target_memcpy API routine on the host. The omp_target_free routine \n7 frees the space on the device. \n8 The example also illustrates how to access that space in a target region by exposing the device \n9 pointer in an is_device_ptr clause. \n10 The example creates an array of cosine values on the default device, to be used on the host device. \n11 The function fails if a default device is not available. \nC / C++ \n12 Example device.4.c (omp_4.5) \nS-1 #include <stdio.h> \nS-2 #include <math.h> \nS-3 #include <stdlib.h> \nS-4 #include <omp.h> \nS-5 \nS-6 void get_dev_cos(double *mem, int s) \nS-7 { \nS-8 int h, t, i; \nS-9 double * mem_dev_cpy; \nS-10 h = omp_get_initial_device(); \nS-11 t = omp_get_default_device(); \nS-12 \nS-13 if (omp_get_num_devices() < 1 || t < 0){ \nS-14 printf(\" ERROR: No device found.\\n\"); \nS-15 exit(1); \nS-16 } \nS-17 \n262 OpenMP Examples Version 5.2.1 - November 2022 \nS-18 mem_dev_cpy = (double *)omp_target_alloc( sizeof(double) * s, t); \nS-19 if(mem_dev_cpy == NULL){ \nS-20 printf(\" ERROR: No space left on device.\\n\"); \nS-21 exit(1); \nS-22 } \nS-23 \nS-24 /* dst src */ \nS-25 omp_target_memcpy(mem_dev_cpy, mem, sizeof(double)*s, \nS-26 0, 0, \nS-27 t, h); \nS-28 \nS-29 #pragma omp target is_device_ptr(mem_dev_cpy) device(t) \nS-30 #pragma omp teams distribute parallel for \nS-31 for(i=0;i<s;i++){ mem_dev_cpy[i] = cos((double)i); } /* init data */ \nS-32 \nS-33 /* dst src */ \nS-34 omp_target_memcpy(mem, mem_dev_cpy, sizeof(double)*s, \nS-35 0, 0, \nS-36 h, t); \nS-37 \nS-38 omp_target_free(mem_dev_cpy, t); \nS-39 } \nC / C++ \n1 The following Fortran example illustrates how to use the omp_target_alloc and \n2 omp_target_memcpy functions to directly allocate device storage and transfer data to and from \n3 a device. It also shows how to check for the presence of device data with the \n4 omp_target_is_present function and to associate host and device storage with the \n5 omp_target_associate_ptr function. \n6 In Section 1 of the code, 40 bytes of storage are allocated on the default device with the \n7 omp_target_alloc function, which returns a value (of type C_PTR) that contains the device \n8 address of the storage. In the subsequent target construct, cp is specified on the \n9 is_device_ptr clause to instruct the compiler that cp is a device pointer. The device pointer \n10 (cp) is then associated with the Fortran pointer (fp) via the c_f_pointer routine inside the \n11 target construct. As a result, fp points to the storage on the device that is allocated by the \n12 omp_target_alloc routine. In the target region, the value 4 is assigned to the storage on \n13 the device, using the Fortran pointer. A trivial test checks that all values were correctly assigned. \n14 The Fortran pointer (fp) is nullified before the end of the target region. After the target \n15 construct, the space on the device is freed with the omp_target_free function, using the \n16 device cp pointer which is set to null after the call. \n17 In Section 2, the content of the storage allocated on the host is directly copied to the OpenMP \n18 allocated storage on the device. First, storage is allocated for the device and host using \n19 omp_target_alloc. Next, on the host the device pointer, returned from the allocation \n20 omp_target_alloc function, is associated with a Fortran pointer, and values are assigned to \nCHAPTER 6. DEVICES 263 \n1 the storage. Similarly, values are assigned on the device to the device storage, after associating a \n2 Fortran pointer (fp_dst) with the device\u2019s storage pointer (cp_dst). \n3 Next the omp_target_memcpy function directly copies the host data to the device storage, \n4 specified by the respective host and device pointers. This copy will overwrite -1 values in the \n5 device storage, and is checked in the next target construct. Keyword arguments are used here for \n6 clarity. (A positional argument list is used in the next Section.) \n7 In Section 3, space is allocated (with a Fortran ALLOCATE statement) and initialized using a host \n8 Fortran pointer (h_fp), and the address of the storage is directly assigned to a host C pointer \n9 (h_cp). The following omp_target_is_present function returns 0 (false, of integer(C_INT) \n10 type) to indicate that h_cp does not have any corresponding storage on the default device. \n11 Next, the same amount of space is allocated on the default device with the omp_target_alloc \n12 function, which returns a device pointer (d_cp). The device pointer d_cp and host pointer h_cp \n13 are then associated using the omp_target_associate_ptr function. The device storage to \n14 which d_cp points becomes the corresponding storage of the host storage to which h_cp points. \n15 The following omp_target_is_present call confirms this, by returning a non-zero value of \n16 integer(C_INT) type for true. \n17 After the association, the content of the host storage is copied to the device using the \n18 omp_target_memcpy function. In the final target construct an array section of h_fp is \n19 mapped to the device, and evaluated for correctness. The mapping establishes a connection of \n20 h_fp with the corresponding device data in the target construct, but does not produce an update \n21 on the device because the previous omp_target_associate_ptr routine sets the reference \n22 count of the mapped object to infinity, meaning a mapping without the always modifier will not \n23 update the device object. \nFortran \n24 Example device.4.f90 (omp_5.0) \nS-1 program device_mem \nS-2 use omp_lib \nS-3 use, intrinsic :: iso_c_binding \nS-4 \nS-5 integer(kind=4),parameter :: N = 10 \nS-6 type(c_ptr) :: cp \nS-7 integer(c_int), pointer :: fp(:) \nS-8 integer(c_int) :: rc, host_dev, targ_dev \nS-9 integer(c_size_t) :: int_bytes \nS-10 \nS-11 integer, pointer :: fp_src(:), fp_dst(:) ! Section 2 vars \nS-12 type(c_ptr) :: cp_src, cp_dst ! Section 2 vars \nS-13 \nS-14 integer, pointer :: h_fp(:) ! Section 3 vars \nS-15 type(c_ptr) :: h_cp, d_cp ! Section 3 vars \nS-16 \n264 OpenMP Examples Version 5.2.1 - November 2022 \nS-17 integer :: i \nS-18 \nS-19 host_dev = omp_get_initial_device() \nS-20 targ_dev = omp_get_default_device() \nS-21 int_bytes = C_SIZEOF(rc) \nS-22 \nS-23 !------------------------------------------------Section 1 vv----------- \nS-24 cp = omp_target_alloc(N*int_bytes, targ_dev) \nS-25 \nS-26 !$omp target is_device_ptr(cp) device(targ_dev) !fp implicit map \nS-27 call c_f_pointer(cp, fp, [ N ]) !fp becomes associated \nS-28 fp(:) = 4 \nS-29 if( all(fp == 4) ) print*,\"PASSED 1 of 5\" \nS-30 nullify(fp) !fp must be returned as disassociated \nS-31 !$omp end target \nS-32 \nS-33 call omp_target_free(cp, targ_dev) \nS-34 cp = c_null_ptr \nS-35 \nS-36 !------------------------------------------------Section 2 vv----------- \nS-37 \nS-38 cp_src = omp_target_alloc((N+1)*int_bytes, host_dev) \nS-39 cp_dst = omp_target_alloc( N *int_bytes, targ_dev) \nS-40 \nS-41 ! Initialize host array (src) \nS-42 call c_f_pointer(cp_src, fp_src, [N+1]) \nS-43 fp_src = [(i,i=1,N+1)] \nS-44 \nS-45 !$omp target device(targ_dev) is_device_ptr(cp_dst) \nS-46 call c_f_pointer(cp_dst, fp_dst, [N]) ! fp_dst becomes associated \nS-47 fp_dst(:) = -1 ! Initial device storage \nS-48 nullify(fp_dst) ! return as disassociated \nS-49 !$omp end target \nS-50 \nS-51 ! Copy subset of host (src) array to device (dst) array \nS-52 rc = omp_target_memcpy( & \nS-53 dst=cp_dst, src=cp_src, length=N*int_bytes, & \nS-54 dst_offset=0_c_size_t, src_offset=int_bytes, & \nS-55 dst_device_num=targ_dev,src_device_num=host_dev) \nS-56 \nS-57 ! Check dst array on device \nS-58 \nS-59 !$omp target device(targ_dev) is_device_ptr(cp_dst) \nS-60 call c_f_pointer(cp_dst, fp_dst, [N]) \nS-61 if ( all(fp_dst == [(i,i=1,N)]) ) print*,\"PASSED 2 of 5\" \nS-62 nullify(fp_dst) \nS-63 !$omp end target \nCHAPTER 6. DEVICES 265 \nS-64 \nS-65 !------------------------------------------------Section 3 vv----------- \nS-66 \nS-67 !allocate host memory and initialize. \nS-68 allocate(h_fp(N), source=[(i,i=1,N)]) \nS-69 \nS-70 h_cp = c_loc(h_fp) \nS-71 ! Device is not aware of allocation on host \nS-72 if(omp_target_is_present(h_cp, targ_dev) == 0) & \nS-73 print*, \"PASSED 3 of 5\" \nS-74 \nS-75 ! Allocate device memory \nS-76 d_cp = omp_target_alloc(c_sizeof(h_fp(1))*size(h_fp), targ_dev) \nS-77 \nS-78 ! now associate host and device storage \nS-79 rc=omp_target_associate_ptr(h_cp,d_cp,c_sizeof(h_fp(1))*size(h_fp), & \nS-80 0_c_size_t,targ_dev) \nS-81 \nS-82 ! check presence of device data, associated w. host pointer \nS-83 if(omp_target_is_present(h_cp, targ_dev) /= 0) & \nS-84 print*,\"PASSED 4 of 5\" \nS-85 \nS-86 ! copy from host to device via C pointers \nS-87 rc=omp_target_memcpy(d_cp, h_cp,c_sizeof(h_fp(1))*size(h_fp), & \nS-88 0_c_size_t, 0_c_size_t, & \nS-89 targ_dev, host_dev) \nS-90 \nS-91 ! validate the device data in the target region \nS-92 ! no data copy here since the reference count is infinity \nS-93 !$omp target device(targ_dev) map(h_fp) \nS-94 if ( all(h_fp == [(i,i=1,N)]) ) print*, \"PASSED 5 of 5\" \nS-95 !$omp end target \nS-96 \nS-97 call omp_target_free(d_cp,targ_dev) \nS-98 deallocate(h_fp) \nS-99 end program \nFortran \n266 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "7 SIMD", "chunk": ""}
{"section_title": "7 SIMD", "chunk": "2 Single instruction, multiple data (SIMD) is a form of parallel execution in which the same operation \n3 is performed on multiple data elements independently in hardware vector processing units (VPU), \n4 also called SIMD units. The addition of two vectors to form a third vector is a SIMD operation. \n5 Many processors have SIMD (vector) units that can perform simultaneously 2, 4, 8 or more \n6 executions of the same operation (by a single SIMD unit). \n7 Loops without loop-carried backward dependency (or with dependency preserved using ordered \n8 simd) are candidates for vectorization by the compiler for execution with SIMD units. In addition, \n9 with state-of-the-art vectorization technology and declare simd directive extensions for \n10 function vectorization in the OpenMP 4.5 specification, loops with function calls can be vectorized \n11 as well. The basic idea is that a scalar function call in a loop can be replaced by a vector version of \n12 the function, and the loop can be vectorized simultaneously by combining a loop vectorization \n13 (simd directive on the loop) and a function vectorization (declare simd directive on the \n14 function). \n15 A simd construct states that SIMD operations be performed on the data within the loop. A number \n16 of clauses are available to provide data-sharing attributes (private, linear, reduction and \n17 lastprivate). Other clauses provide vector length preference/restrictions (simdlen / \n18 safelen), loop fusion (collapse), and data alignment (aligned). \n19 The declare simd directive designates that a vector version of the function should also be \n20 constructed for execution within loops that contain the function and have a simd directive. Clauses \n21 provide argument specifications (linear, uniform, and aligned), a requested vector length \n22 (simdlen), and designate whether the function is always/never called conditionally in a loop \n23 (notinbranch/inbranch). The latter is for optimizing performance. \n24 Also, the simd construct has been combined with the worksharing loop constructs (for simd \n25 and do simd) to enable simultaneous thread execution in different SIMD units. \n"}
{"section_title": "7.1 simd and declare simd Directives", "chunk": ""}
{"section_title": "7.1 simd and declare simd Directives", "chunk": "27 The following example illustrates the basic use of the simd construct to assure the compiler that \n28 the loop can be vectorized. \n267 \nC / C++ \n1 Example SIMD.1.c (omp_4.0) \nS-1 void star( double *a, double *b, double *c, int n, int *ioff ) \nS-2 { \nS-3 int i; \nS-4 #pragma omp simd \nS-5 for ( i = 0; i < n; i++ ) \nS-6 a[i] *= b[i] * c[i+ *ioff]; \nS-7 } \nC / C++ \nFortran \n2 Example SIMD.1.f90 (omp_4.0) \nS-1 subroutine star(a,b,c,n,ioff_ptr) \nS-2 implicit none \nS-3 double precision :: a(*),b(*),c(*) \nS-4 integer :: n, i \nS-5 integer, pointer :: ioff_ptr \nS-6 \nS-7 !$omp simd \nS-8 do i = 1,n \nS-9 a(i) = a(i) * b(i) * c(i+ioff_ptr) \nS-10 end do \nS-11 \nS-12 end subroutine \nFortran \n3 When a function can be inlined within a loop the compiler has an opportunity to vectorize the loop. \n4 By guaranteeing SIMD behavior of a function\u2019s operations, characterizing the arguments of the \n5 function and privatizing temporary variables of the loop, the compiler can often create faster, vector \n6 code for the loop. In the examples below the declare simd directive is used on the add1 and \n7 add2 functions to enable creation of their corresponding SIMD function versions for execution \n8 within the associated SIMD loop. The functions characterize two different approaches of accessing \n9 data within the function: by a single variable and as an element in a data array, respectively. The \n10 add3 C function uses dereferencing. \n11 The declare simd directives also illustrate the use of uniform and linear clauses. The \n12 uniform(fact) clause indicates that the variable fact is invariant across the SIMD lanes. In the \n13 add2 function a and b are included in the uniform list because the C pointer and the Fortran array \n14 references are constant. The i index used in the add2 function is included in a linear clause with \n15 a constant-linear-step of 1, to guarantee a unity increment of the associated loop. In the declare \n16 simd directive for the add3 C function the linear(a,b:1) clause instructs the compiler to \n17 generate unit-stride loads across the SIMD lanes; otherwise, costly gather instructions would be \n18 generated for the unknown sequence of access of the pointer dereferences. \n268 OpenMP Examples Version 5.2.1 - November 2022 \n1 In the simd constructs for the loops the private(tmp) clause is necessary to assure that the \n2 each vector operation has its own tmp variable. \nC / C++ \n3 Example SIMD.2.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 #pragma omp declare simd uniform(fact) \nS-4 double add1(double a, double b, double fact) \nS-5 { \nS-6 double c; \nS-7 c = a + b + fact; \nS-8 return c; \nS-9 } \nS-10 \nS-11 #pragma omp declare simd uniform(a,b,fact) linear(i:1) \nS-12 double add2(double *a, double *b, int i, double fact) \nS-13 { \nS-14 double c; \nS-15 c = a[i] + b[i] + fact; \nS-16 return c; \nS-17 } \nS-18 \nS-19 #pragma omp declare simd uniform(fact) linear(a,b:1) \nS-20 double add3(double *a, double *b, double fact) \nS-21 { \nS-22 double c; \nS-23 c = *a + *b + fact; \nS-24 return c; \nS-25 } \nS-26 \nS-27 void work( double *a, double *b, int n ) \nS-28 { \nS-29 int i; \nS-30 double tmp; \nS-31 #pragma omp simd private(tmp) \nS-32 for ( i = 0; i < n; i++ ) { \nS-33 tmp = add1( a[i], b[i], 1.0); \nS-34 a[i] = add2( a, b, i, 1.0) + tmp; \nS-35 a[i] = add3(&a[i], &b[i], 1.0); \nS-36 } \nS-37 } \nS-38 \nS-39 int main(){ \nS-40 int i; \nS-41 const int N=32; \nCHAPTER 7. SIMD 269 \nS-42 double a[N], b[N]; \nS-43 \nS-44 for ( i=0; i<N; i++ ) { \nS-45 a[i] = i; b[i] = N-i; \nS-46 } \nS-47 \nS-48 work(a, b, N ); \nS-49 \nS-50 for ( i=0; i<N; i++ ) { \nS-51 printf(\"%d %f\\n\", i, a[i]); \nS-52 } \nS-53 \nS-54 return 0; \nS-55 } \nC / C++ \nFortran \n1 Example SIMD.2.f90 (omp_4.0) \nS-1 program main \nS-2 implicit none \nS-3 integer, parameter :: N=32 \nS-4 integer :: i \nS-5 double precision :: a(N), b(N) \nS-6 do i = 1,N \nS-7 a(i) = i-1 \nS-8 b(i) = N-(i-1) \nS-9 end do \nS-10 call work(a, b, N ) \nS-11 do i = 1,N \nS-12 print*, i,a(i) \nS-13 end do \nS-14 end program \nS-15 \nS-16 function add1(a,b,fact) result(c) \nS-17 implicit none \nS-18 !$omp declare simd(add1) uniform(fact) \nS-19 double precision :: a,b,fact, c \nS-20 c = a + b + fact \nS-21 end function \nS-22 \nS-23 function add2(a,b,i, fact) result(c) \nS-24 implicit none \nS-25 !$omp declare simd(add2) uniform(a,b,fact) linear(i:1) \nS-26 integer :: i \nS-27 double precision :: a(*),b(*),fact, c \nS-28 c = a(i) + b(i) + fact \n270 OpenMP Examples Version 5.2.1 - November 2022 \nS-29 end function \nS-30 \nS-31 subroutine work(a, b, n ) \nS-32 implicit none \nS-33 double precision :: a(n),b(n), tmp \nS-34 integer :: n, i \nS-35 double precision, external :: add1, add2 \nS-36 \nS-37 !$omp simd private(tmp) \nS-38 do i = 1,n \nS-39 tmp = add1(a(i), b(i), 1.0d0) \nS-40 a(i) = add2(a, b, i, 1.0d0) + tmp \nS-41 a(i) = a(i) + b(i) + 1.0d0 \nS-42 end do \nS-43 end subroutine \nFortran \n1 A thread that encounters a SIMD construct executes a vectorized code of the iterations. Similar to \n2 the concerns of a worksharing loop a loop vectorized with a SIMD construct must assure that \n3 temporary and reduction variables are privatized and declared as reductions with clauses. The \n4 example below illustrates the use of private and reduction clauses in a SIMD construct. \nC / C++ \n5 Example SIMD.3.c (omp_4.0) \nS-1 double work( double *a, double *b, int n ) \nS-2 { \nS-3 int i; \nS-4 double tmp, sum; \nS-5 sum = 0.0; \nS-6 #pragma omp simd private(tmp) reduction(+:sum) \nS-7 for (i = 0; i < n; i++) { \nS-8 tmp = a[i] + b[i]; \nS-9 sum += tmp; \nS-10 } \nS-11 return sum; \nS-12 } \nC / C++ \nCHAPTER 7. SIMD 271 \nFortran \n1 Example SIMD.3.f90 (omp_4.0) \nS-1 subroutine work( a, b, n, sum ) \nS-2 implicit none \nS-3 integer :: i, n \nS-4 double precision :: a(n), b(n), sum, tmp \nS-5 \nS-6 sum = 0.0d0 \nS-7 !$omp simd private(tmp) reduction(+:sum) \nS-8 do i = 1,n \nS-9 tmp = a(i) + b(i) \nS-10 sum = sum + tmp \nS-11 end do \nS-12 \nS-13 end subroutine work \nFortran \n2 A safelen(N) clause in a simd construct assures the compiler that there are no loop-carried \n3 dependencies for vectors of size N or below. If the safelen clause is not specified, then the \n4 default safelen value is the number of loop iterations. \n5 The safelen(16) clause in the example below guarantees that the vector code is safe for vectors \n6 up to and including size 16. In the loop, m can be 16 or greater, for correct code execution. If the \n7 value of m is less than 16, the behavior is undefined. \nC / C++ \n8 Example SIMD.4.c (omp_4.0) \nS-1 void work( float *b, int n, int m ) \nS-2 { \nS-3 int i; \nS-4 #pragma omp simd safelen(16) \nS-5 for (i = m; i < n; i++) \nS-6 b[i] = b[i-m] - 1.0f; \nS-7 } \nC / C++ \n272 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example SIMD.4.f90 (omp_4.0) \nS-1 subroutine work( b, n, m ) \nS-2 implicit none \nS-3 real :: b(n) \nS-4 integer :: i,n,m \nS-5 \nS-6 !$omp simd safelen(16) \nS-7 do i = m+1, n \nS-8 b(i) = b(i-m) - 1.0 \nS-9 end do \nS-10 end subroutine work \nFortran \n2 The following SIMD construct instructs the compiler to collapse the i and j loops into a single \n3 SIMD loop in which SIMD chunks are executed by threads of the team. Within the workshared \n4 loop chunks of a thread, the SIMD chunks are executed in the lanes of the vector units. \nC / C++ \n5 Example SIMD.5.c (omp_4.0) \nS-1 void work( double **a, double **b, double **c, int n ) \nS-2 { \nS-3 int i, j; \nS-4 double tmp; \nS-5 #pragma omp for simd collapse(2) private(tmp) \nS-6 for (i = 0; i < n; i++) { \nS-7 for (j = 0; j < n; j++) { \nS-8 tmp = a[i][j] + b[i][j]; \nS-9 c[i][j] = tmp; \nS-10 } \nS-11 } \nS-12 } \nC / C++ \nCHAPTER 7. SIMD 273 \nFortran \n1 Example SIMD.5.f90 (omp_4.0) \nS-1 subroutine work( a, b, c, n ) \nS-2 implicit none \nS-3 integer :: i,j,n \nS-4 double precision :: a(n,n), b(n,n), c(n,n), tmp \nS-5 \nS-6 !$omp do simd collapse(2) private(tmp) \nS-7 do j = 1,n \nS-8 do i = 1,n \nS-9 tmp = a(i,j) + b(i,j) \nS-10 c(i,j) = tmp \nS-11 end do \nS-12 end do \nS-13 \nS-14 end subroutine work \nFortran \n"}
{"section_title": "7.2 inbranch and notinbranch Clauses", "chunk": ""}
{"section_title": "7.2 inbranch and notinbranch Clauses", "chunk": "3 The following examples illustrate the use of the declare simd directive with the inbranch \n4 and notinbranch clauses. The notinbranch clause informs the compiler that the function \n5 foo is never called conditionally in the SIMD loop of the function myaddint. On the other hand, the \n6 inbranch clause for the function goo indicates that the function is always called conditionally in \n7 the SIMD loop inside the function myaddfloat. \nC / C++ \n8 Example SIMD.6.c (omp_4.0) \nS-1 #pragma omp declare simd linear(p:1) notinbranch \nS-2 int foo(int *p){ \nS-3 *p = *p + 10; \nS-4 return *p; \nS-5 } \nS-6 \nS-7 int myaddint(int *a, int *b, int n) \nS-8 { \nS-9 #pragma omp simd \nS-10 for (int i=0; i<n; i++){ \nS-11 a[i] = foo(&b[i]); /* foo is not called under a condition */ \nS-12 } \nS-13 return a[n-1]; \nS-14 } \n274 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 \nS-16 #pragma omp declare simd linear(p:1) inbranch \nS-17 float goo(float *p){ \nS-18 *p = *p + 18.5f; \nS-19 return *p; \nS-20 } \nS-21 \nS-22 int myaddfloat(float *x, float *y, int n) \nS-23 { \nS-24 #pragma omp simd \nS-25 for (int i=0; i<n; i++){ \nS-26 x[i] = (x[i] > y[i]) ? goo(&y[i]) : y[i]; \nS-27 /* goo is called under the condition (or within a branch) */ \nS-28 } \nS-29 return x[n-1]; \nS-30 } \nC / C++ \nFortran \n1 Example SIMD.6.f90 (omp_4.0) \nS-1 function foo(p) result(r) \nS-2 implicit none \nS-3 !$omp declare simd(foo) notinbranch \nS-4 integer :: p, r \nS-5 p = p + 10 \nS-6 r = p \nS-7 end function foo \nS-8 \nS-9 function myaddint(a, b, n) result(r) \nS-10 implicit none \nS-11 integer :: a(*), b(*), n, r \nS-12 integer :: i \nS-13 integer, external :: foo \nS-14 \nS-15 !$omp simd \nS-16 do i=1, n \nS-17 a(i) = foo(b(i)) ! foo is not called under a condition \nS-18 end do \nS-19 r = a(n) \nS-20 \nS-21 end function myaddint \nS-22 \nS-23 function goo(p) result(r) \nS-24 implicit none \nS-25 !$omp declare simd(goo) inbranch \nS-26 real :: p, r \nCHAPTER 7. SIMD 275 \nS-27 p = p + 18.5 \nS-28 r = p \nS-29 end function goo \nS-30 \nS-31 function myaddfloat(x, y, n) result(r) \nS-32 implicit none \nS-33 real :: x(*), y(*), r \nS-34 integer :: n \nS-35 integer :: i \nS-36 real, external :: goo \nS-37 \nS-38 !$omp simd \nS-39 do i=1, n \nS-40 if (x(i) > y(i)) then \nS-41 x(i) = goo(y(i)) \nS-42 ! goo is called under the condition (or within a branch) \nS-43 else \nS-44 x(i) = y(i) \nS-45 endif \nS-46 end do \nS-47 \nS-48 r = x(n) \nS-49 end function myaddfloat \nFortran \n1 In the code below, the function fib() is called in the main program and also recursively called in the \n2 function fib() within an if condition. The compiler creates a masked vector version and a \n3 non-masked vector version for the function fib() while retaining the original scalar version of the \n4 fib() function. \nC / C++ \n5 Example SIMD.7.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 \nS-4 #define N 45 \nS-5 int a[N], b[N], c[N]; \nS-6 \nS-7 #pragma omp declare simd inbranch \nS-8 int fib( int n ) \nS-9 { \nS-10 if (n <= 1) \nS-11 return n; \nS-12 else { \nS-13 return fib(n-1) + fib(n-2); \nS-14 } \n276 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 } \nS-16 \nS-17 int main(void) \nS-18 { \nS-19 int i; \nS-20 \nS-21 #pragma omp simd \nS-22 for (i=0; i < N; i++) b[i] = i; \nS-23 \nS-24 #pragma omp simd \nS-25 for (i=0; i < N; i++) { \nS-26 a[i] = fib(b[i]); \nS-27 } \nS-28 printf(\"Done a[%d] = %d\\n\", N-1, a[N-1]); //Done a[44] = 701408733 \nS-29 return 0; \nS-30 } \nC / C++ \nFortran \n1 Example SIMD.7.f90 (omp_4.0) \nS-1 program fibonacci \nS-2 implicit none \nS-3 integer,parameter :: N=45 \nS-4 integer :: a(0:N-1), b(0:N-1) \nS-5 integer :: i \nS-6 integer, external :: fib \nS-7 \nS-8 !$omp simd \nS-9 do i = 0,N-1 \nS-10 b(i) = i \nS-11 end do \nS-12 \nS-13 !$omp simd \nS-14 do i=0,N-1 \nS-15 a(i) = fib(b(i)) \nS-16 end do \nS-17 \nS-18 write(*,*) \"Done a(\", N-1, \") = \", a(N-1) \nS-19 ! 44 701408733 \nS-20 end program \nS-21 \nS-22 recursive function fib(n) result(r) \nS-23 implicit none \nS-24 !$omp declare simd(fib) inbranch \nS-25 integer :: n, r \nS-26 \nCHAPTER 7. SIMD 277 \nS-27 if (n <= 1) then \nS-28 r = n \nS-29 else \nS-30 r = fib(n-1) + fib(n-2) \nS-31 endif \nS-32 \nS-33 end function fib \nFortran \n278 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "7.3 Loop-Carried Lexical Forward Dependence", "chunk": ""}
{"section_title": "7.3 Loop-Carried Lexical Forward Dependence", "chunk": "2 The following example tests the restriction on an SIMD loop with the loop-carried lexical \n3 forward-dependence. This dependence must be preserved for the correct execution of SIMD loops. \n4 A loop can be vectorized even though the iterations are not completely independent when it has \n5 loop-carried dependences that are forward lexical dependences, indicated in the code below by the \n6 read of A[j+1] and the write to A[j] in C/C++ code (or A(j+1) and A(j) in Fortran). That is, the \n7 read of A[j+1] (or A(j+1) in Fortran) before the write to A[j] (or A(j) in Fortran) ordering must be \n8 preserved for each iteration in j for valid SIMD code generation. \n9 This test assures that the compiler preserves the loop carried lexical forward-dependence for \n10 generating a correct SIMD code. \nC / C++ \n11 Example SIMD.8.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 #include <math.h> \nS-3 \nS-4 int P[1000]; \nS-5 float A[1000]; \nS-6 \nS-7 float do_work(float *arr) \nS-8 { \nS-9 float pri; \nS-10 int i; \nS-11 #pragma omp simd lastprivate(pri) \nS-12 for (i = 0; i < 999; ++i) { \nS-13 int j = P[i]; \nS-14 \nS-15 pri = 0.5f; \nS-16 if (j % 2 == 0) { \nS-17 pri = A[j+1] + arr[i]; \nS-18 } \nS-19 A[j] = pri * 1.5f; \nS-20 pri = pri + A[j]; \nS-21 } \nS-22 return pri; \nS-23 } \nS-24 \nS-25 int main(void) \nS-26 { \nS-27 float pri, arr[1000]; \nS-28 int i; \nS-29 \nS-30 for (i = 0; i < 1000; ++i) { \nCHAPTER 7. SIMD 279 \nS-31 P[i] = i; \nS-32 A[i] = i * 1.5f; \nS-33 arr[i] = i * 1.8f; \nS-34 } \nS-35 pri = do_work(&arr[0]); \nS-36 if (pri == 8237.25) { \nS-37 printf(\"passed: result pri = %7.2f (8237.25)"}
{"section_title": "7.3 Loop-Carried Lexical Forward Dependence", "chunk": "2 The following example tests the restriction on an SIMD loop with the loop-carried lexical \n3 forward-dependence. This dependence must be preserved for the correct execution of SIMD loops. \n4 A loop can be vectorized even though the iterations are not completely independent when it has \n5 loop-carried dependences that are forward lexical dependences, indicated in the code below by the \n6 read of A[j+1] and the write to A[j] in C/C++ code (or A(j+1) and A(j) in Fortran). That is, the \n7 read of A[j+1] (or A(j+1) in Fortran) before the write to A[j] (or A(j) in Fortran) ordering must be \n8 preserved for each iteration in j for valid SIMD code generation. \n9 This test assures that the compiler preserves the loop carried lexical forward-dependence for \n10 generating a correct SIMD code. \nC / C++ \n11 Example SIMD.8.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 #include <math.h> \nS-3 \nS-4 int P[1000]; \nS-5 float A[1000]; \nS-6 \nS-7 float do_work(float *arr) \nS-8 { \nS-9 float pri; \nS-10 int i; \nS-11 #pragma omp simd lastprivate(pri) \nS-12 for (i = 0; i < 999; ++i) { \nS-13 int j = P[i]; \nS-14 \nS-15 pri = 0.5f; \nS-16 if (j % 2 == 0) { \nS-17 pri = A[j+1] + arr[i]; \nS-18 } \nS-19 A[j] = pri * 1.5f; \nS-20 pri = pri + A[j]; \nS-21 } \nS-22 return pri; \nS-23 } \nS-24 \nS-25 int main(void) \nS-26 { \nS-27 float pri, arr[1000]; \nS-28 int i; \nS-29 \nS-30 for (i = 0; i < 1000; ++i) { \nCHAPTER 7. SIMD 279 \nS-31 P[i] = i; \nS-32 A[i] = i * 1.5f; \nS-33 arr[i] = i * 1.8f; \nS-34 } \nS-35 pri = do_work(&arr[0]); \nS-36 if (pri == 8237.25) { \nS-37 printf(\"passed: result pri = %7.2f (8237.25)\", pri); \nS-38 } \nS-39 else { \nS-40 printf(\"failed: result pri = %7.2f (8237.25)"}
{"section_title": "7.3 Loop-Carried Lexical Forward Dependence", "chunk": "\", pri); \nS-38 } \nS-39 else { \nS-40 printf(\"failed: result pri = %7.2f (8237.25)\", pri); \nS-41 } \nS-42 return 0; \nS-43 } \nC / C++ \nFortran \n1 Example SIMD.8.f90 (omp_4.0) \nS-1 module work \nS-2 \nS-3 integer :: P(1000) \nS-4 real :: A(1000) \nS-5 \nS-6 contains \nS-7 function do_work(arr) result(pri) \nS-8 implicit none \nS-9 real, dimension(*) :: arr \nS-10 \nS-11 real :: pri \nS-12 integer :: i, j \nS-13 \nS-14 !$omp simd private(j) lastprivate(pri) \nS-15 do i = 1, 999 \nS-16 j = P(i) \nS-17 \nS-18 pri = 0.5 \nS-19 if (mod(j-1, 2) == 0) then \nS-20 pri = A(j+1) + arr(i) \nS-21 endif \nS-22 A(j) = pri * 1.5 \nS-23 pri = pri + A(j) \nS-24 end do \nS-25 \nS-26 end function do_work \nS-27 \nS-28 end module work \nS-29 \n280 OpenMP Examples Version 5.2.1 - November 2022 \nS-30 program simd_8f \nS-31 use work \nS-32 implicit none \nS-33 real :: pri, arr(1000) \nS-34 integer :: i \nS-35 \nS-36 do i = 1, 1000 \nS-37 P(i) = i \nS-38 A(i) = (i-1) * 1.5 \nS-39 arr(i) = (i-1) * 1.8 \nS-40 end do \nS-41 pri = do_work(arr) \nS-42 if (pri == 8237.25) then \nS-43 print 2, \"passed\", pri \nS-44 else \nS-45 print 2, \"failed\", pri \nS-46 endif \nS-47 2 format(a, \": result pri = \", f7.2, \" (8237.25)\") \nS-48 \nS-49 end program \nFortran \n"}
{"section_title": "7.4 ref, val, uval Modifiers for linear Clause", "chunk": ""}
{"section_title": "7.4 ref, val, uval Modifiers for linear Clause", "chunk": "2 Clause \n3 When generating vector functions from declare simd directives, it is important for a compiler \n4 to know the proper types of function arguments in order to generate efficient codes. This is \n5 especially true for C++ reference types and Fortran arguments. \n6 In the following example, the function add_one2 has a C++ reference parameter (or Fortran \n7 argument) p. Variable p gets incremented by 1 in the function. The caller loop i in the main \n8 program passes a variable k as a reference to the function add_one2 call. The ref modifier for the \n9 linear clause on the declare simd directive specifies that the reference-type parameter p is to \n10 match the property of the variable k in the loop. This use of reference type is equivalent to the \n11 second call to add_one2 with a direct passing of the array element a[i]. In the example, the \n12 preferred vector length 8 is specified for both the caller loop and the callee function. \n13 When linear(p: ref) is applied to an argument passed by reference, it tells the compiler that \n14 the addresses in its vector argument are consecutive, and so the compiler can generate a single \n15 vector load or store instead of a gather or scatter. This allows more efficient SIMD code to be \n16 generated with less source changes. \nCHAPTER 7. SIMD 281 \nC++ \n1 Example linear_modifier.1.cpp (omp_5.2) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define NN 1023 \nS-4 int a[NN]; \nS-5 \nS-6 #pragma omp declare simd linear(p: ref) simdlen(8) \nS-7 void add_one2(int& p) \nS-8 { \nS-9 p += 1; \nS-10 } \nS-11 \nS-12 int main(void) \nS-13 { \nS-14 int i; \nS-15 int* p = a; \nS-16 \nS-17 for (i = 0; i < NN; i++) { \nS-18 a[i] = i; \nS-19 } \nS-20 \nS-21 #pragma omp simd linear(p) simdlen(8) \nS-22 for (i = 0; i < NN; i++) { \nS-23 int& k = *p; \nS-24 add_one2(k); \nS-25 add_one2(a[i]); \nS-26 p++; \nS-27 } \nS-28 \nS-29 for (i = 0; i < NN; i++) { \nS-30 if (a[i] != i+2) { \nS-31 printf(\"failed\\n\"); \nS-32 return 1; \nS-33 } \nS-34 } \nS-35 printf(\"passed\\n\"); \nS-36 return 0; \nS-37 } \nC++ \n282 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example linear_modifier.1.f90 (omp_5.2) \nS-1 module m \nS-2 integer, parameter :: NN = 1023 \nS-3 integer :: a(NN) \nS-4 \nS-5 contains \nS-6 subroutine add_one2(p) \nS-7 implicit none \nS-8 !$omp declare simd(add_one2) linear(p: ref) simdlen(8) \nS-9 \nS-10 integer :: p \nS-11 \nS-12 p = p + 1 \nS-13 end subroutine \nS-14 end module \nS-15 \nS-16 program main \nS-17 use m \nS-18 implicit none \nS-19 integer :: i, p \nS-20 \nS-21 do i = 1, NN \nS-22 a(i) = i \nS-23 end do \nS-24 \nS-25 p = 1 \nS-26 !$omp simd linear(p) simdlen(8) \nS-27 do i = 1, NN \nS-28 associate(k => a(p)) \nS-29 call add_one2(k) \nS-30 end associate \nS-31 call add_one2(a(i)) \nS-32 p = p + 1 \nS-33 end do \nS-34 \nS-35 do i = 1, NN \nS-36 if (a(i) /= i+2) then \nS-37 print *, \"failed\" \nS-38 stop \nS-39 endif \nS-40 end do \nS-41 print *, \"passed\" \nS-42 end program \nFortran \nCHAPTER 7. SIMD 283 \n1 The following example is a variant of the above example. The function add_one2 in the C++ code \n2 includes an additional C++ reference parameter i. The loop index i of the caller loop i in the main \n3 program is passed as a reference to the function add_one2 call. The loop index i has a uniform \n4 address with linear value of step 1 across SIMD lanes. Thus, the uval modifier is used for the \n5 linear clause to specify that the C++ reference-type parameter i is to match the property of loop \n6 index i. \n7 In the corresponding Fortran code the arguments p and i in the routine add_on2 are passed by \n8 references. Similar modifiers are used for these variables in the linear clauses to match with the \n9 property at the caller loop in the main program. \n10 When linear(i: uval) is applied to an argument passed by reference, it tells the compiler \n11 that its addresses in the vector argument are uniform so that the compiler can generate a scalar load \n12 or scalar store and create linear values. This allows more efficient SIMD code to be generated with \n13 less source changes. \nC++ \n14 Example linear_modifier.2.cpp (omp_5.2) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define NN 1023 \nS-4 int a[NN]; \nS-5 \nS-6 #pragma omp declare simd linear(p: ref) linear(i: uval) \nS-7 void add_one2(int& p, const int& i) \nS-8 { \nS-9 p += i; \nS-10 } \nS-11 \nS-12 int main(void) \nS-13 { \nS-14 int i; \nS-15 int* p = a; \nS-16 \nS-17 for (i = 0; i < NN; i++) { \nS-18 a[i] = i; \nS-19 } \nS-20 \nS-21 #pragma omp simd linear(p) \nS-22 for (i = 0; i < NN; i++) { \nS-23 int& k = *p; \nS-24 add_one2(k, i); \nS-25 p++; \nS-26 } \nS-27 \nS-28 for (i = 0; i < NN; i++) { \n284 OpenMP Examples Version 5.2.1 - November 2022 \nS-29 if (a[i] != i*2) { \nS-30 printf(\"failed\\n\"); \nS-31 return 1; \nS-32 } \nS-33 } \nS-34 printf(\"passed\\n\"); \nS-35 return 0; \nS-36 } \nC++ \nFortran \n1 Example linear_modifier.2.f90 (omp_5.2) \nS-1 module m \nS-2 integer, parameter :: NN = 1023 \nS-3 integer :: a(NN) \nS-4 \nS-5 contains \nS-6 subroutine add_one2(p, i) \nS-7 implicit none \nS-8 !$omp declare simd(add_one2) linear(p: ref) linear(i: uval) \nS-9 \nS-10 integer :: p \nS-11 integer, intent(in) :: i \nS-12 \nS-13 p = p + i \nS-14 end subroutine \nS-15 end module \nS-16 \nS-17 program main \nS-18 use m \nS-19 implicit none \nS-20 integer :: i, p \nS-21 \nS-22 do i = 1, NN \nS-23 a(i) = i \nS-24 end do \nS-25 \nS-26 p = 1 \nS-27 !$omp simd linear(p) \nS-28 do i = 1, NN \nS-29 call add_one2(a(p), i) \nS-30 p = p + 1 \nS-31 end do \nS-32 \nS-33 do i = 1, NN \nCHAPTER 7. SIMD 285 \nS-34 if (a(i) /= i*2) then \nS-35 print *, \"failed\" \nS-36 stop \nS-37 endif \nS-38 end do \nS-39 print *, \"passed\" \nS-40 end program \nFortran \n1 In the following example, the function func takes arrays x and y as arguments, and accesses the \n2 array elements referenced by the index i. The caller loop i in the main program passes a linear copy \n3 of the variable k to the function func. The val modifier is used for the linear clause in the \n4 declare simd directive for the function func to specify that the argument i is to match the \n5 property of the actual argument k passed in the SIMD loop. Arrays x and y have uniform addresses \n6 across SIMD lanes. \n7 When linear(i: val,step(1)) is applied to an argument, it tells the compiler that its \n8 addresses in the vector argument may not be consecutive, however, their values are linear (with \n9 stride 1 here). When the value of i is used in subscript of array references (e.g., x[i]), the compiler \n10 can generate a vector load or store instead of a gather or scatter. This allows more efficient SIMD \n11 code to be generated with less source changes. \nC / C++ \n12 Example linear_modifier.3.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define N 128 \nS-4 \nS-5 #pragma omp declare simd simdlen(4) uniform(x, y) linear(i:val,step(1)) \nS-6 double func(double x[], double y[], int i) \nS-7 { \nS-8 return (x[i] + y[i]); \nS-9 } \nS-10 \nS-11 int main(void) \nS-12 { \nS-13 double x[N], y[N], z1[N], z2; \nS-14 int i, k; \nS-15 \nS-16 for (i = 0; i < N; i++) { \nS-17 x[i] = (double)i; \nS-18 y[i] = (double)i*2; \nS-19 } \nS-20 \nS-21 k = 0; \nS-22 #pragma omp simd linear(k) \n286 OpenMP Examples Version 5.2.1 - November 2022 \nS-23 for (i = 0; i < N; i++) { \nS-24 z1[i] = func(x, y, k); \nS-25 k++; \nS-26 } \nS-27 \nS-28 for (i = 0; i < N; i++) { \nS-29 z2 = (double)(i + i*2); \nS-30 if (z1[i] != z2) { \nS-31 printf(\"failed\\n\"); \nS-32 return 1; \nS-33 } \nS-34 } \nS-35 printf(\"passed\\n\"); \nS-36 return 0; \nS-37 } \nC / C++ \nFortran \n1 Example linear_modifier.3.f90 (omp_5.2) \nS-1 module func_mod \nS-2 contains \nS-3 real(8) function func(x, y, i) \nS-4 implicit none \nS-5 !$omp declare simd(func) simdlen(4) uniform(x, y) linear(i:val,step(1)) \nS-6 \nS-7 real(8), intent(in) :: x(*), y(*) \nS-8 integer, intent(in) :: i \nS-9 \nS-10 func = x(i) + y(i) \nS-11 \nS-12 end function func \nS-13 end module func_mod \nS-14 \nS-15 program main \nS-16 use func_mod \nS-17 implicit none \nS-18 integer, parameter :: n = 128 \nS-19 real(8) :: x(n), y(n), z1(n), z2 \nS-20 integer :: i, k \nS-21 \nS-22 do i=1, n \nS-23 x(i) = real(i, kind=8) \nS-24 y(i) = real(i*2, kind=8) \nS-25 enddo \nS-26 \nS-27 k = 1 \nCHAPTER 7. SIMD 287 \nS-28 !$omp simd linear(k) \nS-29 do i=1, n \nS-30 z1(i) = func(x, y, k) \nS-31 k = k + 1 \nS-32 enddo \nS-33 \nS-34 do i=1, n \nS-35 z2 = real(i+i*2, kind=8) \nS-36 if (z1(i) /= z2) then \nS-37 print *, \u2019failed\u2019 \nS-38 stop \nS-39 endif \nS-40 enddo \nS-41 print *, \u2019passed\u2019 \nS-42 end program main \nFortran \n288 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "8 Loop Transformations", "chunk": ""}
{"section_title": "8 Loop Transformations", "chunk": "2 To obtain better performance on a platform, code may need to be restructured relative to the way it \n3 is written (which is often for best readability). User-directed loop transformations accomplish this \n4 goal by providing a means to separate code semantics and its optimization. \n5 A loop transformation construct states that a transformation operation is to be performed on set of \n6 nested loops. This directive approach can target specific loops for transformation, rather than \n7 applying more time-consuming general compiler heuristics methods with compiler options that \n8 may not be able to discover optimal transformations. \n9 Loop transformations can be augmented by preprocessor support or OpenMP metadirective \n10 directives, to select optimal dimension and size parameters for specific platforms, facilitating a \n11 single code base for multiple platforms. Moreover, directive-based transformations make \n12 experimenting easier: whereby specific hot spots can be affected by transformation directives. \n"}
{"section_title": "8.1 tile Construct", "chunk": ""}
{"section_title": "8.1 tile Construct", "chunk": "14 In the following example a tile construct transforms two nested loops within the func1 function \n15 into four nested loops. The tile sizes in the sizes clause are applied from outermost to innermost \n16 loops (left-to-right). The effective tiling operation is illustrated in the func2 function. (For easier \n17 illustration, tile sizes for all examples in this section evenly divide the iteration counts so that there \n18 are no remainders.) \n19 In the following C/C++ code the inner loop traverses columns and the outer loop traverses the rows \n20 of a 100x128 (row x column) matrix. The sizes(5,16) clause of the tile construct specifies a \n21 5x16 blocking, applied to the outer (row) and inner (column) loops. The worksharing-loop \n22 construct before the tile construct is applied after the transform. \nC / C++ \n23 Example tile.1.c (omp_5.1) \nS-1 void func1(int A[100][128]) \nS-2 { \nS-3 #pragma omp parallel for \nS-4 #pragma omp tile sizes(5,16) \nS-5 for (int i = 0; i < 100; ++i) \nS-6 for (int j = 0; j < 128; ++j) \nS-7 A[i][j] = i*1000 + j; \nS-8 } \nS-9 \n289 \nS-10 void func2(int A[100][128]) \nS-11 { \nS-12 #pragma omp parallel for \nS-13 for (int i1 = 0; i1 < 100; i1+=5) \nS-14 for (int j1 = 0; j1 < 128; j1+=16) \nS-15 for (int i2 = i1; i2 < i1+5; ++i2) \nS-16 for (int j2 = j1; j2 < j1+16; ++j2) \nS-17 A[i2][j2] = i2*1000 + j2; \nS-18 } \nC / C++ \n1 In the following Fortran code the inner loop traverses rows and the outer loop traverses the columns \n2 of a 128x100 (row x column) matrix. The sizes(5,16) clause of the tile construct specifies a \n3 5x16 blocking, applied to the outer (column) and inner (row) loops. The worksharing-loop \n4 construct before the tile construct is applied after the transform. \nFortran \n5 Example tile.1.f90 (omp_5.1) \nS-1 subroutine func1(A) \nS-2 integer :: A(128,100) \nS-3 integer :: i, j \nS-4 !$omp parallel do \nS-5 !$omp tile sizes(5,16) \nS-6 do i = 1, 100 \nS-7 do j = 1, 128 \nS-8 A(j,i) = j*1000 + i \nS-9 end do; end do \nS-10 end subroutine \nS-11 \nS-12 subroutine func2(A) \nS-13 integer :: A(128,100) \nS-14 integer :: i1, j1, i2, j2 \nS-15 !$omp parallel do \nS-16 do i1 = 1, 100,5 \nS-17 do j1 = 1, 128,16 \nS-18 do i2 = i1, i1+( 5-1) \nS-19 do j2 = j1, j1+(16-1) \nS-20 A(j2,i2) = j2*1000 + i2 \nS-21 end do; end do \nS-22 end do; end do \nS-23 end subroutine \nFortran \n290 OpenMP Examples Version 5.2.1 - November 2022 \n1 This example illustrates transformation nesting. Here, a 4x4 \u201couter\u201d tile construct is applied to \n2 the \u201cinner\u201d tile transform shown in the example above. The effect of the inner loop is shown in \n3 func2 (cf. func2 in tile.1.c). The outer tile construct\u2019s sizes(4,4) clause applies a 4x4 tile \n4 upon the resulting blocks of the inner transform. The effective looping is shown in func3. \nC / C++ \n5 Example tile.2.c (omp_5.1) \nS-1 void func1(int A[100][128]) \nS-2 { \nS-3 #pragma omp tile sizes(4, 4) \nS-4 #pragma omp tile sizes(5,16) \nS-5 for (int i = 0; i < 100; ++i) \nS-6 for (int j = 0; j < 128; ++j) \nS-7 A[i][j] = i*1000 + j; \nS-8 } \nS-9 \nS-10 void func2(int A[100][128]) \nS-11 { \nS-12 #pragma omp tile sizes(4,4) \nS-13 for (int i1 = 0; i1 < 100; i1+=5) \nS-14 for (int j1 = 0; j1 < 128; j1+=16) \nS-15 for (int i2 = i1; i2 < i1+5; ++i2) \nS-16 for (int j2 = j1; j2 < j1+16; ++j2) \nS-17 A[i2][j2] = i2*1000 + j2; \nS-18 } \nS-19 \nS-20 void func3(int A[100][128]) \nS-21 { \nS-22 for (int i11 = 0; i11 < 100; i11+= 5*4) \nS-23 for (int j11 = 0; j11 < 128; j11+=16*4) \nS-24 \nS-25 for (int i12 = i11; i12 < i11+( 5*4); i12+= 5) \nS-26 for (int j12 = j11; j12 < j11+(16*4); j12+=16) \nS-27 \nS-28 for (int i2 = i12; i2 < i12+ 5; ++i2) \nS-29 for (int j2 = j12; j2 < j12+16; ++j2) \nS-30 A[i2][j2] = i2*1000 + j2; \nS-31 } \nC / C++ \nCHAPTER 8. LOOP TRANSFORMATIONS 291 \nFortran \n1 Example tile.2.f90 (omp_5.1) \nS-1 subroutine func1(A) \nS-2 integer :: A(128,100) \nS-3 integer :: i, j \nS-4 !$omp tile sizes(4, 4) \nS-5 !$omp tile sizes(5,16) \nS-6 do i = 1, 100 \nS-7 do j = 1, 128 \nS-8 A(j,i) = j*1000 + i \nS-9 end do; end do \nS-10 end subroutine \nS-11 \nS-12 subroutine func2(A) \nS-13 integer :: A(128,100) \nS-14 integer :: i1, j1, i2, j2 \nS-15 !$omp tile sizes(4,4) \nS-16 do i1 = 1, 100,5 \nS-17 do j1 = 1, 128,16 \nS-18 do i2 = i1, i1+( 5-1) \nS-19 do j2 = j1, j1+(16-1) \nS-20 A(j2,i2) = j2*1000 + i2 \nS-21 end do; end do \nS-22 end do; end do \nS-23 \nS-24 end subroutine \nS-25 \nS-26 subroutine func3(A) \nS-27 integer :: A(128,100) \nS-28 integer :: i11, j11, i12, j12, i2, j2 \nS-29 do i11 = 1, 100, 5*4 \nS-30 do j11 = 1, 128, 16*4 \nS-31 do i12 = i11, i11+( 5*4-1), 5 \nS-32 do j12 = j11, j11+(16*4-1), 16 \nS-33 do i2 = i12, i12+ 5-1 \nS-34 do j2 = j12, j12+16-1 \nS-35 A(j2,i2) = j2*1000 + i2 \nS-36 enddo; enddo; \nS-37 enddo; enddo; \nS-38 enddo; enddo \nS-39 \nS-40 end subroutine \nFortran \n292 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "8.2 unroll Construct", "chunk": ""}
{"section_title": "8.2 unroll Construct", "chunk": "2 The unroll construct is a loop transformation that increases the number of loop blocks in a loop, \n3 while reducing the number of iterations. The full clause specifies that the loop is to be \n4 completely unrolled. That is, a loop block for each iteration is created, and the loop is removed. A \n5 partial clause with a unroll-factor specifies that the number of iterations will be reduced \n6 multiplicatively by the factor while the number of blocks will be increased by the same factor. \n7 Operationally, the loop is tiled by the factor, and the tiled loop is fully expanded, resulting in a \n8 single loop with multiple blocks. \n9 Unrolling can reduce control-flow overhead and provide additional optimization opportunities for \n10 the compiler and the processor pipeline. Nevertheless, unrolling can increase the code size, and \n11 saturate the instruction cache. Hence, the trade-off may need to be assessed. Unrolling a loop does \n12 not change the code\u2019s semantics. Also, compilers may unroll loops without explicit directives, at \n13 various optimization levels. \n14 In the example below, the unroll construct is used without any clause, and then with a full \n15 clause, in the first two functions, respectively. When no clause is used, it is up to the \n16 implementation (compiler) to decide if and how the loop is to be unrolled. The iteration count can \n17 have a run time value. In the second function, the unroll construct uses a full clause to \n18 completely unroll the loop. A compile-time constant is required for the iteration count. The \n19 statements in the third function (unroll_full_equivalent) illustrates equivalent code for the full \n20 unrolling in the second function. \nC / C++ \n21 Example unroll.1.c (omp_5.1) \nS-1 void unroll(double A[], int n) \nS-2 { \nS-3 #pragma omp unroll \nS-4 for (int i = 0; i < n; ++i) \nS-5 A[i] = 0; \nS-6 } \nS-7 \nS-8 void unroll_full(double A[]) \nS-9 { \nS-10 #pragma omp unroll full \nS-11 for (int i = 0; i < 4; ++i) \nS-12 A[i] = 0; \nS-13 } \nS-14 \nS-15 void unroll_full_equivalent(double A[]) \nS-16 { \nS-17 A[0] = 0; \nS-18 A[1] = 0; \nS-19 A[2] = 0; \nCHAPTER 8. LOOP TRANSFORMATIONS 293 \nS-20 A[3] = 0; \nS-21 } \nC / C++ \nFortran \n1 Example unroll.1.f90 (omp_5.1) \nS-1 subroutine unroll(A, n) \nS-2 implicit none \nS-3 integer :: i,n \nS-4 double precision :: A(n) \nS-5 \nS-6 !$omp unroll \nS-7 do i = 1,n \nS-8 A(i) = 0.0d0 \nS-9 end do \nS-10 end subroutine \nS-11 \nS-12 subroutine unroll_full(A) \nS-13 implicit none \nS-14 integer :: i \nS-15 double precision :: A(*) \nS-16 \nS-17 !$omp unroll full \nS-18 do i = 1,4 \nS-19 A(i) = 0.0d0 \nS-20 end do \nS-21 end subroutine \nS-22 \nS-23 subroutine unroll_full_equivalent(A) \nS-24 implicit none \nS-25 double precision :: A(*) \nS-26 \nS-27 A(1) = 0.0d0 \nS-28 A(2) = 0.0d0 \nS-29 A(3) = 0.0d0 \nS-30 A(4) = 0.0d0 \nS-31 end subroutine \nFortran \n2 The next example shows cases when it is incorrect to use full unrolling. \n294 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example unroll.2.c (omp_5.1) \nS-1 void illegal_2a(double A[]) \nS-2 { \nS-3 #pragma omp for \nS-4 #pragma omp unroll full // ERROR: No loop left after full unrolling. \nS-5 for (int i = 0; i < 12; ++i) \nS-6 A[i] = 0; \nS-7 } \nS-8 \nS-9 void illegal_2b(double A[]) \nS-10 { \nS-11 // Loop might be fully unrolled (or a partially unrolled loop \nS-12 // replacement). Hence, no canonical for-loop, resulting in \nS-13 // non-compliant code. Implementations may suggest adding a \nS-14 // \"partial\" clause. \nS-15 \nS-16 #pragma omp for // Requires a canonical loop \nS-17 #pragma omp unroll // ERROR: may result in non-compliant code \nS-18 for (int i = 0; i < 12; ++i) \nS-19 A[i] = 0; \nS-20 } \nS-21 \nS-22 void illegal_2c(int n, double A[]) \nS-23 { \nS-24 #pragma omp unroll full // ERROR: Constant iteration count required. \nS-25 for (int i = 0; i < n; ++i) \nS-26 A[i] = 0; \nS-27 } \nC / C++ \nFortran \n2 Example unroll.2.f90 (omp_5.1) \nS-1 subroutine illegal_2a(A) \nS-2 implicit none \nS-3 double precision :: A(*) \nS-4 integer :: i \nS-5 \nS-6 !$omp do \nS-7 !$omp unroll full !! ERROR: No loop left after full unrolling \nS-8 do i = 1,12 \nS-9 A(i) = 0.0d0 \nS-10 end do \nS-11 end subroutine \nS-12 \nCHAPTER 8. LOOP TRANSFORMATIONS 295 \nS-13 subroutine illegal_2b(A) \nS-14 implicit none \nS-15 double precision :: A(*) \nS-16 integer :: i \nS-17 \nS-18 !! Loop might be fully unrolled (or a partially unrolled loop \nS-19 !! replacement). Hence, no canonical do-loop will exist, \nS-20 !! resulting in non-compliant code. \nS-21 !! Implementations may suggest to adding a \"partial\" clause. \nS-22 \nS-23 !$omp do !! Requires a canonical loop \nS-24 !$omp unroll !! ERROR: may result in non-compliant code \nS-25 do i = 1,12 \nS-26 A(i) = 0.0d0 \nS-27 end do \nS-28 end subroutine \nS-29 \nS-30 subroutine illegal_2c(n, A) \nS-31 implicit none \nS-32 integer :: i,n \nS-33 double precision :: A(*) \nS-34 \nS-35 !$omp unroll full !! Full unroll requires constant iteration count \nS-36 do i = 1,n \nS-37 A(i) = 0.0d0 \nS-38 end do \nS-39 end subroutine \nFortran \n1 In many cases, when the iteration count is large and/or dynamic, it is reasonable to partially unroll a \n2 loop by including a partial clause. In the unroll3_partial function below, the unroll-factor \n3 value of 4 is used to create a tile size of 4 that is unrolled to create 4 unrolled statements. The \n4 equivalent \u201chand unrolled\u201d loop code is presented in the unroll3_partial_equivalent function. If the \n5 unroll-factor is omitted, as in the unroll3_partial_nofactor function, the implementation may \n6 optimally select a factor from 1 (no unrolling) to the iteration count (full unrolling). In the latter \n7 case the construct generates a loop with a single iteration. \nC / C++ \n8 Example unroll.3.c (omp_5.1) \nS-1 void unroll3_partial(double A[]) \nS-2 { \nS-3 #pragma omp unroll partial(4) \nS-4 for (int i = 0; i < 128; ++i) \nS-5 A[i] = 0; \nS-6 } \nS-7 \n296 OpenMP Examples Version 5.2.1 - November 2022 \nS-8 void unroll3_partial_equivalent(double A[]) \nS-9 { \nS-10 for (int i_iv = 0; i_iv < 32; ++i_iv) { \nS-11 A[i_iv * 4 + 0] = 0; \nS-12 A[i_iv * 4 + 1] = 0; \nS-13 A[i_iv * 4 + 2] = 0; \nS-14 A[i_iv * 4 + 3] = 0; \nS-15 } \nS-16 } \nS-17 \nS-18 void unroll3_partial_nofactor(double A[]) \nS-19 { \nS-20 #pragma omp unroll partial \nS-21 for (int i = 0; i < 128; ++i) \nS-22 A[i] = 0; \nS-23 } \nC / C++ \nFortran \n1 Example unroll.3.f90 (omp_5.1) \nS-1 subroutine unroll3_partial(A) \nS-2 implicit none \nS-3 double precision :: A(*) \nS-4 integer :: i \nS-5 \nS-6 !$omp unroll partial(4) \nS-7 do i = 1,128 \nS-8 A(i) = 0 \nS-9 end do \nS-10 end subroutine \nS-11 \nS-12 subroutine unroll3_partial_equivalent(A) \nS-13 implicit none \nS-14 double precision :: A(*) \nS-15 integer :: i_iv \nS-16 \nS-17 do i_iv = 0, 31 \nS-18 A(i_iv * 4 + 1) = 0 \nS-19 A(i_iv * 4 + 2) = 0 \nS-20 A(i_iv * 4 + 3) = 0 \nS-21 A(i_iv * 4 + 4) = 0 \nS-22 end do \nS-23 end subroutine \nS-24 \nS-25 subroutine unroll3_partial_nofactor(A) \nS-26 implicit none \nCHAPTER 8. LOOP TRANSFORMATIONS 297 \nS-27 double precision :: A(*) \nS-28 integer :: i \nS-29 \nS-30 !$omp unroll partial \nS-31 do i = 1, 128 \nS-32 A(i) = 0 \nS-33 end do \nS-34 end subroutine \nFortran \n1 When the iteration count is not a multiple of the unroll-factor, iterations that should not produce \n2 executions must be conditionally protected from execution. In this example, the first function \n3 unrolls a loop that has a variable iteration count. Since the unroll construct uses a \n4 partial( 4 ) clause, the compiler will need to create code that can account for cases when the \n5 iteration count is not a multiple of 4. A brute-force, simple-to-understand approach for \n6 implementing the conditionals is shown in the unroll_partial_remainder_option1 function. \n7 The remaining two functions show more optimal algorithms the compiler may select to implement \n8 the transformation. Optimal approaches may reduce the number of conditionals as shown in \n9 unroll_partial_remainder_option2, and may eliminate conditionals completely by peeling off a \n10 \u201cremainder\u201d into a separate loop as in unroll_partial_remainder_option3. \n11 Regardless of the optimization, implementations must ensure that the semantics remain the same, \n12 especially when additional directives are applied to the unrolled loop. For the case in the \n13 unroll_partial_remainder_option3 function, the fission of the worksharing-loop construct may \n14 result in a different distribution of threads to the iterations. Since no reproducible scheduling is \n15 specified on the work-sharing construct, the worksharing-loop and unrolling are compliant. \nC / C++ \n16 Example unroll.4.c (omp_5.1) \nS-1 void unroll_partial_remainder(int n, int A[]) \nS-2 { \nS-3 #pragma omp parallel for \nS-4 #pragma omp unroll partial(4) \nS-5 for (int i = 0; i < n; ++i) \nS-6 A[i] = i; \nS-7 } \nS-8 \nS-9 void unroll_partial_remainder_option1(int n, int A[]) \nS-10 { \nS-11 #pragma omp parallel for \nS-12 for (int i_iv = 0; i_iv < (n+3)/4; ++i_iv) { \nS-13 A[i_iv * 4 + 0] = i_iv * 4 + 0; \nS-14 if (i_iv * 4 + 1 < n) A[i_iv * 4 + 1] = i_iv * 4 + 1; \nS-15 if (i_iv * 4 + 2 < n) A[i_iv * 4 + 2] = i_iv * 4 + 2; \nS-16 if (i_iv * 4 + 3 < n) A[i_iv * 4 + 3] = i_iv * 4 + 3; \n298 OpenMP Examples Version 5.2.1 - November 2022 \nS-17 } \nS-18 } \nS-19 \nS-20 void unroll_partial_remainder_option2(int n, int A[]) \nS-21 { \nS-22 #pragma omp parallel for \nS-23 for (int i_iv = 0; i_iv < (n+3)/4; ++i_iv) { \nS-24 if (i_iv < n/4) { \nS-25 A[i_iv * 4 + 0] = i_iv * 4 + 0; \nS-26 A[i_iv * 4 + 1] = i_iv * 4 + 1; \nS-27 A[i_iv * 4 + 2] = i_iv * 4 + 2; \nS-28 A[i_iv * 4 + 3] = i_iv * 4 + 3; \nS-29 } else { \nS-30 // remainder loop \nS-31 for (int i_rem = i_iv*4; i_rem < n; ++i_rem) \nS-32 A[i_rem] = i_rem; \nS-33 } \nS-34 } \nS-35 } \nS-36 \nS-37 void unroll_partial_remainder_option3(int n, int A[]) \nS-38 { \nS-39 // main loop \nS-40 #pragma omp parallel for \nS-41 for (int i_iv = 0; i_iv < n/4; ++i_iv) { \nS-42 A[i_iv * 4 + 0] = i_iv * 4 + 0; \nS-43 A[i_iv * 4 + 1] = i_iv * 4 + 1; \nS-44 A[i_iv * 4 + 2] = i_iv * 4 + 2; \nS-45 A[i_iv * 4 + 3] = i_iv * 4 + 3; \nS-46 } \nS-47 \nS-48 // remainder loop \nS-49 #pragma omp parallel for \nS-50 for (int i_rem = (n/4)*4; i_rem < n; ++i_rem) \nS-51 A[i_rem] = i_rem; \nS-52 } \nS-53 \nS-54 #include <stdio.h> \nS-55 #define NT 12 \nS-56 \nS-57 int main(){ \nS-58 int error=0, A[NT],C[NT]; \nS-59 for(int i = 0; i<NT; i++){ A[i]=0; C[i]=i; } \nS-60 \nS-61 for(int i = 0; i<NT; i++) A[i]=0.0; \nS-62 unroll_partial_remainder(NT,A); \nS-63 for(int i = 0; i<NT; i++) if(A[i] != C[i]) error=1; \nCHAPTER 8. LOOP TRANSFORMATIONS 299 \nS-64 \nS-65 for(int i = 0; i<NT; i++) A[i]=0.0; \nS-66 unroll_partial_remainder_option1(NT,A); \nS-67 for(int i = 0; i<NT; i++) if(A[i] != C[i]) error=1; \nS-68 \nS-69 for(int i = 0; i<NT; i++) A[i]=0.0; \nS-70 unroll_partial_remainder_option2(NT,A); \nS-71 for(int i = 0; i<NT; i++) if(A[i] != C[i]) error=1; \nS-72 \nS-73 for(int i = 0; i<NT; i++) A[i]=0.0; \nS-74 unroll_partial_remainder_option3(NT,A); \nS-75 for(int i = 0; i<NT; i++) if(A[i] != C[i]) error=1; \nS-76 \nS-77 if(!error) printf(\"OUT: Passed\\n\"); \nS-78 if( error) printf(\"OUT: Failed\\n\"); \nS-79 } \nC / C++ \nFortran \n1 Example unroll.4.f90 (omp_5.1) \nS-1 subroutine unroll_partial_remainder(n, A) \nS-2 implicit none \nS-3 integer :: n, i \nS-4 integer :: A(*) \nS-5 \nS-6 !$omp parallel do \nS-7 !$omp unroll partial(4) \nS-8 do i = 1, n \nS-9 A(i) = i \nS-10 end do \nS-11 \nS-12 end subroutine \nS-13 \nS-14 subroutine unroll_partial_remainder_option1(n, A) \nS-15 implicit none \nS-16 integer :: n, i_iv \nS-17 integer :: A(*) \nS-18 \nS-19 !$omp parallel do \nS-20 do i_iv = 0,(n+3)/4 -1 \nS-21 A(i_iv * 4 + 1) = i_iv * 4 + 1 \nS-22 if (i_iv * 4 + 2 <= n) A(i_iv * 4 + 2) = i_iv * 4 + 2 \nS-23 if (i_iv * 4 + 3 <= n) A(i_iv * 4 + 3) = i_iv * 4 + 3 \nS-24 if (i_iv * 4 + 4 <= n) A(i_iv * 4 + 4) = i_iv * 4 + 4 \nS-25 end do \nS-26 \n300 OpenMP Examples Version 5.2.1 - November 2022 \nS-27 end subroutine \nS-28 \nS-29 subroutine unroll_partial_remainder_option2(n, A) \nS-30 implicit none \nS-31 integer :: n, i_iv, i_rem \nS-32 integer :: A(*) \nS-33 \nS-34 !$omp parallel do \nS-35 do i_iv = 0, (n+3)/4 -1 \nS-36 if (i_iv < n/4) then \nS-37 A(i_iv * 4 + 1) = i_iv * 4 + 1 \nS-38 A(i_iv * 4 + 2) = i_iv * 4 + 2 \nS-39 A(i_iv * 4 + 3) = i_iv * 4 + 3 \nS-40 A(i_iv * 4 + 4) = i_iv * 4 + 4 \nS-41 else \nS-42 !! remainder loop \nS-43 do i_rem = i_iv*4 +1, n \nS-44 A(i_rem) = i_rem \nS-45 end do \nS-46 end if \nS-47 end do \nS-48 \nS-49 end subroutine \nS-50 \nS-51 subroutine unroll_partial_remainder_option3(n, A) \nS-52 implicit none \nS-53 integer :: n, i_iv, i_rem \nS-54 integer :: A(*) \nS-55 \nS-56 !$omp parallel do \nS-57 do i_iv = 0, (n/4) -1 \nS-58 \nS-59 A(i_iv * 4 + 1) = i_iv * 4 + 1 \nS-60 A(i_iv * 4 + 2) = i_iv * 4 + 2 \nS-61 A(i_iv * 4 + 3) = i_iv * 4 + 3 \nS-62 A(i_iv * 4 + 4) = i_iv * 4 + 4 \nS-63 end do \nS-64 \nS-65 !! remainder loop \nS-66 !$omp parallel do \nS-67 do i_rem = (n/4)*4 +1, n \nS-68 A(i_rem) = i_rem \nS-69 end do \nS-70 \nS-71 end subroutine \nS-72 \nS-73 program main \nCHAPTER 8. LOOP TRANSFORMATIONS 301 \nS-74 implicit none \nS-75 integer, parameter :: NT=12 \nS-76 \nS-77 integer :: i \nS-78 logical :: error=.false. \nS-79 integer :: A(NT), C(NT)=[ (i, i=1,NT) ] \nS-80 \nS-81 A(1:NT)=0 \nS-82 call unroll_partial_remainder(NT, A) \nS-83 if( .not. all(A(1:NT) == C(1:NT)) ) error = .true. \nS-84 \nS-85 A(1:NT)=0 \nS-86 call unroll_partial_remainder_option1(NT, A) \nS-87 if( .not. all(A(1:NT) == C(1:NT)) ) error = .true. \nS-88 \nS-89 A(1:NT)=0 \nS-90 call unroll_partial_remainder_option2(NT, A) \nS-91 if( .not. all(A(1:NT) == C(1:NT)) ) error = .true. \nS-92 \nS-93 A(1:NT)=0 \nS-94 call unroll_partial_remainder_option3(NT, A) \nS-95 if( .not. all(A(1:NT) == C(1:NT)) ) error = .true. \nS-96 \nS-97 if(.not. error) print*, \"OUT: Passed.\" \nS-98 if( error) print*, \"OUT: Failed\" \nS-99 end program \nFortran \n302 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "8.3 Incomplete Tiles", "chunk": ""}
{"section_title": "8.3 Incomplete Tiles", "chunk": "2 Optimal performance for tiled loops is achieved when the loop iteration count is a multiple of the \n3 tile size. When this condition does not exist, the implementation is free to execute the partial loops \n4 in a manner that optimizes performance, while preserving the specified order of iterations in the \n5 complete-tile loops. \n6 Figure 8.1a shows an example of a 2-by-2 tiling for a 5-by-5 iteration space. There are nine \n7 resulting tiles. Four are complete 2-by-2 tiles, and the remaining five tiles are partial tiles. \n0 1 2 3 4 5 j \ni \n0 \n1 \n2 \n3 \n4 \n5 \n(A) 2-dimensional tiling with partial tiles \n0 16 32 48 64 80 96 100 j \ni \n0 \n5 \n10 \n15 \n20 \n25 \n30 \n35 \n40 \n45 \n50 \n55 \n60 \n65 \n70 \n75 \n80 \n85 \n90 \n95 \n100 (B) Partial tiles of Example partial_tile.1 \nFIGURE 8.1: Tiling illustrations \n8 In the following example, function func1 uses the tile construct with a sizes(4,16) tiling \n9 clause. Because the second tile dimension of 16 does not evenly divide into the iteration count of \n10 the j-loop, the iterations corresponding to the remainder for the j-loop correspond to partial tiles as \n11 shown in Figure 8.1b. Each remaining function illustrates a code implementation that a compiler \n12 may generate to implement the tile construct in func1. \n13 The order of tile execution relative to other tiles can be changed, but execution order of iterations \n14 within the same tile must be preserved. Implementations must ensure that dependencies that are \n15 valid with any tile size need to be preserved (including tile size of 1 and tiles as large as the \n16 iteration space). \n17 Functions func2 through func6 are valid implementations of func1. In func2 the unrolling is \n18 illustrated as a pair of nested loops with a simple adjustment in the size of the final iteration block \n19 in the j2 iteration space for the partial tile. \n20 Performance of the implementation depends on the hardware architecture, the instruction set and \n21 compiler optimization goals. Functions func3, func4, and func5 have the advantage that the \nCHAPTER 8. LOOP TRANSFORMATIONS 303 \n1 innermost loop for the complete tile is a constant size and can be replaced with SIMD instructions. \n2 If the target platform has masked SIMD instructions with no overhead, then avoiding the \n3 construction of a remainder loop, as in func5, might be the best option. Another option is to use a \n4 remainder loop without tiling, as shown in func6, to reduce control-flow overhead. \nC / C++ \n5 Example partial_tile.1.c (omp_5.1) \nS-1 int min(int a, int b){ return (a < b)? a : b; } \nS-2 \nS-3 void func1(double A[100][100]) \nS-4 { \nS-5 #pragma omp tile sizes(4,16) \nS-6 for (int i = 0; i < 100; ++i) \nS-7 for (int j = 0; j < 100; ++j) \nS-8 A[i][j] = A[i][j] + 1; \nS-9 } \nS-10 \nS-11 void func2(double A[100][100]) \nS-12 { \nS-13 for (int i1 = 0; i1 < 100; i1+=4) \nS-14 for (int j1 = 0; j1 < 100; j1+=16) \nS-15 for (int i2 = i1; i2 < i1+4; ++i2) \nS-16 for (int j2 = j1; j2 < min(j1+16,100); ++j2) \nS-17 A[i2][j2] = A[i2][j2] + 1; \nS-18 } \nS-19 \nS-20 void func3(double A[100][100]) \nS-21 { \nS-22 // complete tiles \nS-23 for (int i1 = 0; i1 < 100; i1+=4) \nS-24 for (int j1 = 0; j1 < 96; j1+=16) \nS-25 for (int i2 = i1; i2 < i1+4; ++i2) \nS-26 for (int j2 = j1; j2 < j1+16; ++j2) \nS-27 A[i2][j2] = A[i2][j2] + 1; \nS-28 // partial tiles / remainder \nS-29 for (int i1 = 0; i1 < 100; i1+=4) \nS-30 for (int i2 = i1; i2 < i1+4; ++i2) \nS-31 for (int j = 96; j < 100; j+=1) \nS-32 A[i2][j] = A[i2][j] + 1; \nS-33 } \nS-34 \nS-35 void func4(double A[100][100]) \nS-36 { \nS-37 for (int i1 = 0; i1 < 100; i1+=4) { \nS-38 // complete tiles \nS-39 for (int j1 = 0; j1 < 96; j1+=16) \n304 OpenMP Examples Version 5.2.1 - November 2022 \nS-40 for (int i2 = i1; i2 < i1+4; ++i2) \nS-41 for (int j2 = j1; j2 < j1+16; ++j2) \nS-42 A[i2][j2] = A[i2][j2] + 1; \nS-43 // partial tiles \nS-44 for (int i2 = i1; i2 < i1+4; ++i2) \nS-45 for (int j = 96; j < 100; j+=1) \nS-46 A[i2][j] = A[i2][j] + 1; \nS-47 } \nS-48 } \nS-49 \nS-50 void func5(double A[100][100]) \nS-51 { \nS-52 for (int i1 = 0; i1 < 100; i1+=4) \nS-53 for (int j1 = 0; j1 < 100; j1+=16) \nS-54 for (int i2 = i1; i2 < i1+4; ++i2) \nS-55 for (int j2 = j1; j2 <j1+16; ++j2) \nS-56 if (j2 < 100) \nS-57 A[i2][j2] = A[i2][j2] + 1; \nS-58 } \nS-59 \nS-60 void func6(double A[100][100]) \nS-61 { \nS-62 // complete tiles \nS-63 for (int i1 = 0; i1 < 100; i1+=4) \nS-64 for (int j1 = 0; j1 < 96; j1+=16) \nS-65 for (int i2 = i1; i2 < i1+4; ++i2) \nS-66 for (int j2 = j1; j2 < j1+16; ++j2) \nS-67 A[i2][j2] = A[i2][j2] + 1; \nS-68 // partial tiles / remainder (not tiled) \nS-69 for (int i = 0; i < 100; ++i) \nS-70 for (int j = 96; j < 100; ++j) \nS-71 A[i][j] = A[i][j] + 1; \nS-72 } \nC / C++ \nFortran \n1 Example partial_tile.1.f90 (omp_5.1) \nS-1 subroutine func1(A) \nS-2 implicit none \nS-3 double precision :: A(100,100) \nS-4 integer :: i,j \nS-5 \nS-6 !$omp tile sizes(4,16) \nS-7 do i = 1, 100 \nS-8 do j = 1, 100 \nS-9 A(j,i) = A(j,i) + 1 \nCHAPTER 8. LOOP TRANSFORMATIONS 305 \nS-10 end do; end do \nS-11 \nS-12 end subroutine \nS-13 \nS-14 \nS-15 subroutine func2(A) \nS-16 implicit none \nS-17 double precision :: A(100,100) \nS-18 integer :: i1,i2,j1,j2 \nS-19 \nS-20 do i1 = 1, 100, 4 \nS-21 do j1 = 1, 100, 16 \nS-22 do i2 = i1, i1 + 3 \nS-23 do j2 = j1, min(j1+15,100) \nS-24 A(j2,i2) = A(j2,i2) + 1 \nS-25 end do; end do; end do; end do \nS-26 \nS-27 end subroutine \nS-28 \nS-29 \nS-30 subroutine func3(A) \nS-31 implicit none \nS-32 double precision :: A(100,100) \nS-33 integer :: i1,i2,j1,j2, j \nS-34 \nS-35 !! complete tiles \nS-36 do i1 = 1, 100, 4 \nS-37 do j1 = 1, 96, 16 \nS-38 do i2 = i1, i1 + 3 \nS-39 do j2 = j1, j1 +15 \nS-40 A(j2,i2) = A(j2,i2) + 1 \nS-41 end do; end do; end do; end do \nS-42 \nS-43 !! partial tiles / remainder \nS-44 do i1 = 1, 100, 4 \nS-45 do i2 = i1, i1 +3 \nS-46 do j = 97, 100 \nS-47 A(j,i2) = A(j,i2) + 1 \nS-48 end do; end do; end do \nS-49 \nS-50 end subroutine \nS-51 \nS-52 \nS-53 subroutine func4(A) \nS-54 implicit none \nS-55 double precision :: A(100,100) \nS-56 integer :: i1,i2,j1,j2, j \n306 OpenMP Examples Version 5.2.1 - November 2022 \nS-57 \nS-58 do i1 = 1, 100, 4 \nS-59 \nS-60 !! complete tiles \nS-61 do j1 = 1, 96, 16 \nS-62 do i2 = i1, i1 + 3 \nS-63 do j2 = j1, j1 +15 \nS-64 A(j2,i2) = A(j2,i2) + 1 \nS-65 end do; end do; end do \nS-66 \nS-67 !! partial tiles \nS-68 do i2 = i1, i1 +3 \nS-69 do j = 97, 100 \nS-70 A(j,i2) = A(j,i2) + 1 \nS-71 end do; end do \nS-72 \nS-73 end do \nS-74 \nS-75 end subroutine \nS-76 \nS-77 \nS-78 subroutine func5(A) \nS-79 implicit none \nS-80 double precision :: A(100,100) \nS-81 integer :: i1,i2,j1,j2 \nS-82 \nS-83 do i1 = 1, 100, 4 \nS-84 do j1 = 1, 100, 16 \nS-85 do i2 = i1, i1 + 3 \nS-86 do j2 = j1, j1 +15 \nS-87 if (j2 < 101) A(j2,i2) = A(j2,i2) + 1 \nS-88 end do; end do; end do; end do \nS-89 \nS-90 end subroutine \nS-91 \nS-92 \nS-93 subroutine func6(A) \nS-94 implicit none \nS-95 double precision :: A(100,100) \nS-96 integer :: i1,i2,j1,j2, i,j \nS-97 \nS-98 !! complete tiles \nS-99 do i1 = 1, 100, 4 \nS-100 do j1 = 1, 96, 16 \nS-101 do i2 = i1, i1 + 3 \nS-102 do j2 = j1, j1 +15 \nS-103 A(j2,i2) = A(j2,i2) + 1 \nCHAPTER 8. LOOP TRANSFORMATIONS 307 \nS-104 end do; end do; end do; end do \nS-105 \nS-106 !! partial tiles / remainder (not tiled) \nS-107 do i = 1, 100 \nS-108 do j = 97, 100 \nS-109 A(j,i) = A(j,i) + 1 \nS-110 end do; end do \nS-111 \nS-112 end subroutine \nFortran \n1 In the following example, function func7 tiles nested loops with a size of (4,16), resulting in partial \n2 tiles that cover the last 4 iterations of the j-loop, as in the previous example. However, the outer \n3 loop is parallelized with a parallel worksharing-loop construct. \n4 Functions func8 and func9 illustrate two implementations of the tiling with parallel and \n5 worksharing-loop directives. Function func8 uses a single outer loop, with a min function to \n6 accommodate the partial tiles. Function func9 uses two sets of nested loops, the first iterates over \n7 the complete tiles and the second covers iterations from the partial tiles. When fissioning loops that \n8 are in a parallel worksharing-loop region, each iteration of each workshared loop must be \n9 executed on the same thread as in an un-fissioned loop. The schedule(static) clause in \n10 func7 forces the implementation to use static scheduling and allows the fission in function func8. \n11 When dynamic scheduling is prescribed, fissioning is not allowed. When no scheduling is \n12 specified, the compiler implementation will select a scheduling kind and adhere to its restrictions. \nC / C++ \n13 Example partial_tile.2.c (omp_5.1) \nS-1 int min(int a, int b){ return (a < b)? a : b; } \nS-2 \nS-3 void func7(double A[100][100]) \nS-4 { \nS-5 #pragma omp parallel for schedule(static) \nS-6 #pragma omp tile sizes(4,16) \nS-7 for (int i = 0; i < 100; ++i) \nS-8 for (int j = 0; j < 100; ++j) \nS-9 A[i][j] = A[i][j] + 1; \nS-10 } \nS-11 \nS-12 void func8(double A[100][100]) \nS-13 { \nS-14 #pragma omp parallel for schedule(static) \nS-15 for (int i1 = 0; i1 < 100; i1+=4) \nS-16 for (int j1 = 0; j1 < 100; j1+=16) \nS-17 for (int i2 = i1; i2 < i1+4; ++i2) \nS-18 for (int j2 = j1; j2 < min(j1+16,100); ++j2) \nS-19 A[i2][j2] = A[i2][j2] + 1; \n308 OpenMP Examples Version 5.2.1 - November 2022 \nS-20 } \nS-21 \nS-22 void func9(double A[100][100]) \nS-23 { \nS-24 #pragma omp parallel \nS-25 { \nS-26 #pragma omp for schedule(static) nowait \nS-27 for (int i1 = 0; i1 < 100; i1+=4) \nS-28 for (int j1 = 0; j1 < 96; j1+=16) \nS-29 for (int i2 = i1; i2 < i1+4; ++i2) \nS-30 for (int j2 = j1; j2 < j1+16; ++j2) \nS-31 A[i2][j2] = A[i2][j2] + 1; \nS-32 #pragma omp for schedule(static) \nS-33 for (int i1 = 0; i1 < 100; i1+=4) \nS-34 for (int i2 = i1; i2 < i1+4; ++i2) \nS-35 for (int j = 96; j < 100; j+=1) \nS-36 A[i2][j] = A[i2][j] + 1; \nS-37 } \nS-38 } \nC / C++ \nFortran \n1 Example partial_tile.2.f90 (omp_5.1) \nS-1 subroutine func7(A) \nS-2 implicit none \nS-3 double precision :: A(100,100) \nS-4 integer :: i,j \nS-5 \nS-6 !$omp parallel do schedule(static) \nS-7 !$omp tile sizes(4,16) \nS-8 do i=1,100 \nS-9 do j = 1, 100 \nS-10 A(j,i) = A(j,i) + 1 \nS-11 end do; end do \nS-12 \nS-13 end subroutine \nS-14 \nS-15 subroutine func8(A) \nS-16 implicit none \nS-17 double precision :: A(100,100) \nS-18 integer :: i1,i2,j1,j2 \nS-19 \nS-20 do i1 = 1, 100, 4 \nS-21 do j1 = 1, 100, 16 \nS-22 do i2 = i1, i1 + 3 \nS-23 do j2 = j1, min(j1+15,100) \nCHAPTER 8. LOOP TRANSFORMATIONS 309 \nS-24 A(j2,i2) = A(j2,i2) + 1 \nS-25 end do; end do; end do; end do \nS-26 \nS-27 end subroutine \nS-28 \nS-29 subroutine func9(A) \nS-30 implicit none \nS-31 double precision :: A(100,100) \nS-32 integer :: i1,i2,j1,j2,j \nS-33 \nS-34 !$omp parallel \nS-35 \nS-36 !$omp do schedule(static) \nS-37 do i1 = 1, 100, 4 \nS-38 do j1 = 1, 96, 16 \nS-39 do i2 = i1, i1 + 3 \nS-40 do j2 = j1, j1 +15 \nS-41 A(j2,i2) = A(j2,i2) + 1 \nS-42 end do; end do; end do; end do \nS-43 !$omp end do nowait \nS-44 \nS-45 !$omp do schedule(static) \nS-46 do i1 = 1, 100, 4 \nS-47 do i2 = i1, i1 +3 \nS-48 do j = 97, 100 \nS-49 A(j,i2) = A(j,i2) + 1 \nS-50 end do; end do; end do; \nS-51 \nS-52 !$omp end parallel \nS-53 \nS-54 end subroutine \nFortran \n310 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9 Synchronization", "chunk": ""}
{"section_title": "9 Synchronization", "chunk": "2 The barrier construct is a stand-alone directive that requires all threads of a team (within a \n3 contention group) to execute the barrier and complete execution of all tasks within the region, \n4 before continuing past the barrier. \n5 The critical construct is a directive that contains a structured block. The construct allows only \n6 a single thread at a time to execute the structured block (region). Multiple critical regions may exist \n7 in a parallel region, and may act cooperatively (only one thread at a time in all critical regions), \n8 or separately (only one thread at a time in each critical regions when a unique name is supplied \n9 on each critical construct). An optional (lock) hint clause may be specified on a named \n10 critical construct to provide the OpenMP runtime guidance in selection a locking mechanism. \n11 On a finer scale the atomic construct allows only a single thread at a time to have atomic access to \n12 a storage location involving a single read, write, update or capture statement, and a limited number \n13 of combinations when specifying the capture atomic-clause clause. The atomic-clause clause is \n14 required for some expression statements, but is not required for update statements. The \n15 memory-order clause can be used to specify the degree of memory ordering enforced by an \n16 atomic construct. From weakest to strongest, they are relaxed (the default), acquire and/or \n17 release clauses (specified with acquire, release, or acq_rel), and seq_cst. Please see \n18 the details in the atomic Construct subsection of the Directives chapter in the OpenMP \n19 Specifications document. \n20 The ordered construct either specifies a structured block in a loop, simd, or loop SIMD region \n21 that will be executed in the order of the loop iterations. The ordered construct sequentializes and \n22 orders the execution of ordered regions while allowing code outside the region to run in parallel. \n23 Since OpenMP 4.5 the ordered construct can also be a stand-alone directive that specifies \n24 cross-iteration dependences in a doacross loop nest. The depend clause uses a sink \n25 dependence-type, along with an iteration vector argument (vec) to indicate the iteration that satisfies \n26 the dependence. The depend clause with a source dependence-type specifies dependence \n27 satisfaction. \n28 The flush directive is a stand-alone construct for enforcing consistency between a thread\u2019s view \n29 of memory and the view of memory for other threads (see the Memory Model chapter of this \n30 document for more details). When the construct is used with an explicit variable list, a strong flush \n31 that forces a thread\u2019s temporary view of memory to be consistent with the actual memory is applied \n32 to all listed variables. When the construct is used without an explicit variable list and without a \n33 memory-order clause, a strong flush is applied to all locally thread-visible data as defined by the \n34 base language, and additionally the construct provides both acquire and release memory ordering \n35 semantics. When an explicit variable list is not present and a memory-order clause is present, the \n36 construct provides acquire and/or release memory ordering semantics according to the \n37 memory-order clause, but no strong flush is performed. A resulting strong flush that applies to a set \n311 \n1 of variables effectively ensures that no memory (load or store) operation for the affected variables \n2 may be reordered across the flush directive. \n3 General-purpose routines provide mutual exclusion semantics through locks, represented by lock \n4 variables. The semantics allows a task to set, and hence own a lock, until it is unset by the task that \n5 set it. A nestable lock can be set multiple times by a task, and is used when in code requires nested \n6 control of locks. A simple lock can only be set once by the owning task. There are specific calls for \n7 the two types of locks, and the variable of a specific lock type cannot be used by the other lock type. \n8 Any explicit task will observe the synchronization prescribed in a barrier construct and an \n9 implied barrier. Also, additional synchronizations are available for tasks. All children of a task will \n10 wait at a taskwait (for their siblings to complete). A taskgroup construct creates a region in \n11 which the current task is suspended at the end of the region until all sibling tasks, and their \n12 descendants, have completed. Scheduling constraints on task execution can be prescribed by the \n13 depend clause to enforce dependence on previously generated tasks. More details on controlling \n14 task executions can be found in the Tasking Chapter in the OpenMP Specifications document. \n312 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.1 critical Construct", "chunk": ""}
{"section_title": "9.1 critical Construct", "chunk": "2 The following example includes several critical constructs. The example illustrates a queuing \n3 model in which a task is dequeued and worked on. To guard against multiple threads dequeuing the \n4 same task, the dequeuing operation must be in a critical region. Because the two queues in this \n5 example are independent, they are protected by critical constructs with different names, xaxis \n6 and yaxis. \nC / C++ \n7 Example critical.1.c (pre_omp_3.0) \nS-1 int dequeue(float *a); \nS-2 void work(int i, float *a); \nS-3 \nS-4 void critical_example(float *x, float *y) \nS-5 { \nS-6 int ix_next, iy_next; \nS-7 \nS-8 #pragma omp parallel shared(x, y) private(ix_next, iy_next) \nS-9 { \nS-10 #pragma omp critical (xaxis) \nS-11 ix_next = dequeue(x); \nS-12 work(ix_next, x); \nS-13 \nS-14 #pragma omp critical (yaxis) \nS-15 iy_next = dequeue(y); \nS-16 work(iy_next, y); \nS-17 } \nS-18 \nS-19 } \nC / C++ \nFortran \n8 Example critical.1.f (pre_omp_3.0) \nS-1 SUBROUTINE CRITICAL_EXAMPLE(X, Y) \nS-2 \nS-3 REAL X(*), Y(*) \nS-4 INTEGER IX_NEXT, IY_NEXT \nS-5 \nS-6 !$OMP PARALLEL SHARED(X, Y) PRIVATE(IX_NEXT, IY_NEXT) \nS-7 \nS-8 !$OMP CRITICAL(XAXIS) \nS-9 CALL DEQUEUE(IX_NEXT, X) \nS-10 !$OMP END CRITICAL(XAXIS) \nS-11 CALL WORK(IX_NEXT, X) \nS-12 \nCHAPTER 9. SYNCHRONIZATION 313 \nS-13 !$OMP CRITICAL(YAXIS) \nS-14 CALL DEQUEUE(IY_NEXT,Y) \nS-15 !$OMP END CRITICAL(YAXIS) \nS-16 CALL WORK(IY_NEXT, Y) \nS-17 \nS-18 !$OMP END PARALLEL \nS-19 \nS-20 END SUBROUTINE CRITICAL_EXAMPLE \nFortran \n1 The following example extends the previous example by adding the hint clause to the critical \n2 constructs. \nC / C++ \n3 Example critical.2.c (omp_5.0) \nS-1 #include <omp.h> \nS-2 \nS-3 int dequeue(float *a); \nS-4 void work(int i, float *a); \nS-5 \nS-6 void critical_example(float *x, float *y) \nS-7 { \nS-8 int ix_next, iy_next; \nS-9 \nS-10 #pragma omp parallel shared(x, y) private(ix_next, iy_next) \nS-11 { \nS-12 #pragma omp critical (xaxis) hint(omp_sync_hint_contended) \nS-13 ix_next = dequeue(x); \nS-14 work(ix_next, x); \nS-15 \nS-16 #pragma omp critical (yaxis) hint(omp_sync_hint_contended) \nS-17 iy_next = dequeue(y); \nS-18 work(iy_next, y); \nS-19 } \nS-20 \nS-21 } \nC / C++ \n314 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example critical.2.f (omp_5.0) \nS-1 SUBROUTINE CRITICAL_EXAMPLE(X, Y) \nS-2 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-3 \nS-4 REAL X(*), Y(*) \nS-5 INTEGER IX_NEXT, IY_NEXT \nS-6 \nS-7 !$OMP PARALLEL SHARED(X, Y) PRIVATE(IX_NEXT, IY_NEXT) \nS-8 \nS-9 !$OMP CRITICAL(XAXIS) HINT(OMP_SYNC_HINT_CONTENDED) \nS-10 CALL DEQUEUE(IX_NEXT, X) \nS-11 !$OMP END CRITICAL(XAXIS) \nS-12 CALL WORK(IX_NEXT, X) \nS-13 \nS-14 !$OMP CRITICAL(YAXIS) HINT(OMP_SYNC_HINT_CONTENDED) \nS-15 CALL DEQUEUE(IY_NEXT,Y) \nS-16 !$OMP END CRITICAL(YAXIS) \nS-17 CALL WORK(IY_NEXT, Y) \nS-18 \nS-19 !$OMP END PARALLEL \nS-20 \nS-21 END SUBROUTINE CRITICAL_EXAMPLE \nFortran \nCHAPTER 9. SYNCHRONIZATION 315 \n"}
{"section_title": "9.2 Worksharing Constructs Inside a critical Construct", "chunk": ""}
{"section_title": "9.2 Worksharing Constructs Inside a critical Construct", "chunk": "2 Construct \n3 The following example demonstrates using a worksharing construct inside a critical construct. \n4 This example is conforming because the worksharing single region is not closely nested inside \n5 the critical region. A single thread executes the one and only section in the sections \n6 region, and executes the critical region. The same thread encounters the nested parallel \n7 region, creates a new team of threads, and becomes the primary thread of the new team. One of the \n8 threads in the new team enters the single region and increments i by 1. At the end of this \n9 example i is equal to 2. \nC / C++ \n10 Example worksharing_critical.1.c (pre_omp_3.0) \nS-1 void critical_work() \nS-2 { \nS-3 int i = 1; \nS-4 #pragma omp parallel sections \nS-5 { \nS-6 #pragma omp section \nS-7 { \nS-8 #pragma omp critical (name) \nS-9 { \nS-10 #pragma omp parallel \nS-11 { \nS-12 #pragma omp single \nS-13 { \nS-14 i++; \nS-15 } \nS-16 } \nS-17 } \nS-18 } \nS-19 } \nS-20 } \nC / C++ \n316 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example worksharing_critical.1.f (pre_omp_3.0) \nS-1 SUBROUTINE CRITICAL_WORK() \nS-2 \nS-3 INTEGER I \nS-4 I = 1 \nS-5 \nS-6 !$OMP PARALLEL SECTIONS \nS-7 !$OMP SECTION \nS-8 !$OMP CRITICAL (NAME) \nS-9 !$OMP PARALLEL \nS-10 !$OMP SINGLE \nS-11 I = I + 1 \nS-12 !$OMP END SINGLE \nS-13 !$OMP END PARALLEL \nS-14 !$OMP END CRITICAL (NAME) \nS-15 !$OMP END PARALLEL SECTIONS \nS-16 END SUBROUTINE CRITICAL_WORK \nFortran \nCHAPTER 9. SYNCHRONIZATION 317 \n"}
{"section_title": "9.3 Binding of barrier Regions", "chunk": ""}
{"section_title": "9.3 Binding of barrier Regions", "chunk": "2 The binding rules call for a barrier region to bind to the closest enclosing parallel region. \n3 In the following example, the call from the main program to sub2 is conforming because the \n4 barrier region (in sub3) binds to the parallel region in sub2. The call from the main \n5 program to sub1 is conforming because the barrier region binds to the parallel region in \n6 subroutine sub2. \n7 The call from the main program to sub3 is conforming because the barrier region binds to the \n8 implicit inactive parallel region enclosing the sequential part. Also note that the barrier \n9 region in sub3 when called from sub2 only synchronizes the team of threads in the enclosing \n10 parallel region and not all the threads created in sub1. \nC / C++ \n11 Example barrier_regions.1.c (pre_omp_3.0) \nS-1 void work(int n) {} \nS-2 \nS-3 void sub3(int n) \nS-4 { \nS-5 work(n); \nS-6 #pragma omp barrier \nS-7 work(n); \nS-8 } \nS-9 \nS-10 void sub2(int k) \nS-11 { \nS-12 #pragma omp parallel shared(k) \nS-13 sub3(k); \nS-14 } \nS-15 \nS-16 void sub1(int n) \nS-17 { \nS-18 int i; \nS-19 #pragma omp parallel private(i) shared(n) \nS-20 { \nS-21 #pragma omp for \nS-22 for (i=0; i<n; i++) \nS-23 sub2(i); \nS-24 } \nS-25 } \nS-26 \nS-27 int main() \nS-28 { \nS-29 sub1(2); \nS-30 sub2(2); \n318 OpenMP Examples Version 5.2.1 - November 2022 \nS-31 sub3(2); \nS-32 return 0; \nS-33 } \nC / C++ \nFortran \n1 Example barrier_regions.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(N) \nS-2 INTEGER N \nS-3 END SUBROUTINE WORK \nS-4 \nS-5 SUBROUTINE SUB3(N) \nS-6 INTEGER N \nS-7 CALL WORK(N) \nS-8 !$OMP BARRIER \nS-9 CALL WORK(N) \nS-10 END SUBROUTINE SUB3 \nS-11 \nS-12 SUBROUTINE SUB2(K) \nS-13 INTEGER K \nS-14 !$OMP PARALLEL SHARED(K) \nS-15 CALL SUB3(K) \nS-16 !$OMP END PARALLEL \nS-17 END SUBROUTINE SUB2 \nS-18 \nS-19 \nS-20 SUBROUTINE SUB1(N) \nS-21 INTEGER N \nS-22 INTEGER I \nS-23 !$OMP PARALLEL PRIVATE(I) SHARED(N) \nS-24 !$OMP DO \nS-25 DO I = 1, N \nS-26 CALL SUB2(I) \nS-27 END DO \nS-28 !$OMP END PARALLEL \nS-29 END SUBROUTINE SUB1 \nS-30 \nS-31 PROGRAM EXAMPLE \nS-32 CALL SUB1(2) \nS-33 CALL SUB2(2) \nS-34 CALL SUB3(2) \nS-35 END PROGRAM EXAMPLE \nFortran \nCHAPTER 9. SYNCHRONIZATION 319 \n"}
{"section_title": "9.4 atomic Construct", "chunk": ""}
{"section_title": "9.4 atomic Construct", "chunk": "2 The following example avoids race conditions (simultaneous updates of an element of x by multiple \n3 threads) by using the atomic construct . \n4 The advantage of using the atomic construct in this example is that it allows updates of two \n5 different elements of x to occur in parallel. If a critical construct were used instead, then all \n6 updates to elements of x would be executed serially (though not in any guaranteed order). \n7 Note that the atomic directive applies only to the statement immediately following it. As a result, \n8 elements of y are not updated atomically in this example. \nC / C++ \n9 Example atomic.1.c (omp_3.1) \nS-1 float work1(int i) \nS-2 { \nS-3 return 1.0 * i; \nS-4 } \nS-5 \nS-6 float work2(int i) \nS-7 { \nS-8 return 2.0 * i; \nS-9 } \nS-10 \nS-11 void atomic_example(float *x, float *y, int *index, int n) \nS-12 { \nS-13 int i; \nS-14 \nS-15 #pragma omp parallel for shared(x, y, index, n) \nS-16 for (i=0; i<n; i++) { \nS-17 #pragma omp atomic update \nS-18 x[index[i]] += work1(i); \nS-19 y[i] += work2(i); \nS-20 } \nS-21 } \nS-22 \nS-23 int main() \nS-24 { \nS-25 float x[1000]; \nS-26 float y[10000]; \nS-27 int index[10000]; \nS-28 int i; \nS-29 \nS-30 for (i = 0; i < 10000; i++) { \nS-31 index[i] = i % 1000; \nS-32 y[i]=0.0; \nS-33 } \n320 OpenMP Examples Version 5.2.1 - November 2022 \nS-34 for (i = 0; i < 1000; i++) \nS-35 x[i] = 0.0; \nS-36 atomic_example(x, y, index, 10000); \nS-37 return 0; \nS-38 } \nC / C++ \nFortran \n1 Example atomic.1.f (omp_3.1) \nS-1 REAL FUNCTION WORK1(I) \nS-2 INTEGER I \nS-3 WORK1 = 1.0 * I \nS-4 RETURN \nS-5 END FUNCTION WORK1 \nS-6 \nS-7 REAL FUNCTION WORK2(I) \nS-8 INTEGER I \nS-9 WORK2 = 2.0 * I \nS-10 RETURN \nS-11 END FUNCTION WORK2 \nS-12 \nS-13 SUBROUTINE SUB(X, Y, INDEX, N) \nS-14 REAL X(*), Y(*) \nS-15 INTEGER INDEX(*), N \nS-16 \nS-17 INTEGER I \nS-18 \nS-19 !$OMP PARALLEL DO SHARED(X, Y, INDEX, N) \nS-20 DO I=1,N \nS-21 !$OMP ATOMIC UPDATE \nS-22 X(INDEX(I)) = X(INDEX(I)) + WORK1(I) \nS-23 Y(I) = Y(I) + WORK2(I) \nS-24 ENDDO \nS-25 \nS-26 END SUBROUTINE SUB \nS-27 \nS-28 PROGRAM ATOMIC_EXAMPLE \nS-29 REAL X(1000), Y(10000) \nS-30 INTEGER INDEX(10000) \nS-31 INTEGER I \nS-32 \nS-33 DO I=1,10000 \nS-34 INDEX(I) = MOD(I, 1000) + 1 \nS-35 Y(I) = 0.0 \nS-36 ENDDO \nS-37 \nCHAPTER 9. SYNCHRONIZATION 321 \nS-38 DO I = 1,1000 \nS-39 X(I) = 0.0 \nS-40 ENDDO \nS-41 \nS-42 CALL SUB(X, Y, INDEX, 10000) \nS-43 \nS-44 END PROGRAM ATOMIC_EXAMPLE \nFortran \n1 The following example illustrates the read and write clauses for the atomic directive. These \n2 clauses ensure that the given variable is read or written, respectively, as a whole. Otherwise, some \n3 other thread might read or write part of the variable while the current thread was reading or writing \n4 another part of the variable. Note that most hardware provides atomic reads and writes for some set \n5 of properly aligned variables of specific sizes, but not necessarily for all the variable types \n6 supported by the OpenMP API. \nC / C++ \n7 Example atomic.2.c (omp_3.1) \nS-1 int atomic_read(const int *p) \nS-2 { \nS-3 int value; \nS-4 /* Guarantee that the entire value of *p is read atomically. No part of \nS-5 * *p can change during the read operation. \nS-6 */ \nS-7 #pragma omp atomic read \nS-8 value = *p; \nS-9 return value; \nS-10 } \nS-11 \nS-12 void atomic_write(int *p, int value) \nS-13 { \nS-14 /* Guarantee that value is stored atomically into *p. No part of *p can \nS-15 change \nS-16 * until after the entire write operation is completed. \nS-17 */ \nS-18 #pragma omp atomic write \nS-19 *p = value; \nS-20 } \nC / C++ \n322 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example atomic.2.f (omp_3.1) \nS-1 function atomic_read(p) \nS-2 integer :: atomic_read \nS-3 integer, intent(in) :: p \nS-4 ! Guarantee that the entire value of p is read atomically. No part of \nS-5 ! p can change during the read operation. \nS-6 \nS-7 !$omp atomic read \nS-8 atomic_read = p \nS-9 return \nS-10 end function atomic_read \nS-11 \nS-12 subroutine atomic_write(p, value) \nS-13 integer, intent(out) :: p \nS-14 integer, intent(in) :: value \nS-15 ! Guarantee that value is stored atomically into p. No part of p can change \nS-16 ! until after the entire write operation is completed. \nS-17 !$omp atomic write \nS-18 p = value \nS-19 end subroutine atomic_write \nFortran \n2 The following example illustrates the capture clause for the atomic directive. In this case the \n3 value of a variable is captured, and then the variable is incremented. These operations occur \n4 atomically. This example could be implemented using the fetch-and-add instruction available on \n5 many kinds of hardware. The example also shows a way to implement a spin lock using the \n6 capture and read clauses. \nC / C++ \n7 Example atomic.3.c (omp_3.1) \nS-1 int fetch_and_add(int *p) \nS-2 { \nS-3 /* Atomically read the value of *p and then increment it. The \nS-4 previous value is returned. This can be used to implement a \nS-5 simple lock as shown below. \nS-6 */ \nS-7 int old; \nS-8 #pragma omp atomic capture \nS-9 { old = *p; (*p)++; } \nS-10 return old; \nS-11 } \nS-12 \nS-13 /* \nCHAPTER 9. SYNCHRONIZATION 323 \nS-14 * Use fetch_and_add to implement a lock \nS-15 */ \nS-16 struct locktype { \nS-17 int ticketnumber; \nS-18 int turn; \nS-19 }; \nS-20 void do_locked_work(struct locktype *lock) \nS-21 { \nS-22 int atomic_read(const int *p); \nS-23 void work(); \nS-24 \nS-25 // Obtain the lock \nS-26 int myturn = fetch_and_add(&lock->ticketnumber); \nS-27 while (atomic_read(&lock->turn) != myturn) \nS-28 ; \nS-29 // Do some work. The flush is needed to ensure visibility of \nS-30 // variables not involved in atomic directives \nS-31 \nS-32 #pragma omp flush \nS-33 work(); \nS-34 #pragma omp flush \nS-35 // Release the lock \nS-36 fetch_and_add(&lock->turn); \nS-37 } \nC / C++ \nFortran \n1 Example atomic.3.f (omp_3.1) \nS-1 function fetch_and_add(p) \nS-2 integer:: fetch_and_add \nS-3 integer, intent(inout) :: p \nS-4 \nS-5 ! Atomically read the value of p and then increment it. The previous value \nS-6 ! is returned. This can be used to implement a simple lock as shown below. \nS-7 !$omp atomic capture \nS-8 fetch_and_add = p \nS-9 p = p + 1 \nS-10 !$omp end atomic \nS-11 end function fetch_and_add \nS-12 module m \nS-13 interface \nS-14 function fetch_and_add(p) \nS-15 integer :: fetch_and_add \nS-16 integer, intent(inout) :: p \nS-17 end function \nS-18 function atomic_read(p) \n324 OpenMP Examples Version 5.2.1 - November 2022 \nS-19 integer :: atomic_read \nS-20 integer, intent(in) :: p \nS-21 end function \nS-22 end interface \nS-23 type locktype \nS-24 integer ticketnumber \nS-25 integer turn \nS-26 end type \nS-27 contains \nS-28 subroutine do_locked_work(lock) \nS-29 type(locktype), intent(inout) :: lock \nS-30 integer myturn \nS-31 integer junk \nS-32 ! obtain the lock \nS-33 myturn = fetch_and_add(lock%ticketnumber) \nS-34 do while (atomic_read(lock%turn) .ne. myturn) \nS-35 continue \nS-36 enddo \nS-37 ! Do some work. The flush is needed to ensure visibility of variables \nS-38 ! not involved in atomic directives \nS-39 !$omp flush \nS-40 call work \nS-41 !$omp flush \nS-42 ! Release the lock \nS-43 junk = fetch_and_add(lock%turn) \nS-44 end subroutine \nS-45 end module \nFortran \nCHAPTER 9. SYNCHRONIZATION 325 \n"}
{"section_title": "9.5 Atomic Compare", "chunk": ""}
{"section_title": "9.5 Atomic Compare", "chunk": "2 In OpenMP 5.1 the compare clause was added to the extended-atomic clauses. The compare \n3 clause extends the semantics to perform the atomic update conditionally. \n4 In the following C/C++ example, two formats of structured blocks are shown for associated \n5 atomic constructs with the compare clause. In the first atomic construct, the format forms a \n6 conditional update statement. In the second atomic construct the format forms a conditional \n7 expression statement. The \u201cgreater than\u201d and \u201cless than\u201d forms are not available with the Fortran \n8 compare clause. One can use the max and min functions with the atomic update construct to \n9 perform the C/C++ example operations. \nC / C++ \n10 Example cas.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #define N 10 \nS-3 \nS-4 void init(int *); \nS-5 \nS-6 int main(){ \nS-7 int val_min=2*N, val_max=-2*N; \nS-8 int val[N]; \nS-9 \nS-10 init(val); \nS-11 \nS-12 #pragma omp parallel for num_threads(2) \nS-13 for (int i=1; i<N-1; i++) { \nS-14 \nS-15 // compare and update val_min using one atomic form \nS-16 #pragma omp atomic compare \nS-17 if (val[i] < val_min) { val_min = val[i]; } \nS-18 \nS-19 // compare and update val_max using another atomic form \nS-20 #pragma omp atomic compare \nS-21 val_max = val[i] > val_max ? val[i] : val_max; \nS-22 } \nS-23 \nS-24 if(val_max != 2*N || val_min != -2*N){ printf(\"FAILED\\n\");} \nS-25 else { printf(\"PASSED\\n\");} \nS-26 // OUT: PASSED \nS-27 } \nS-28 \nS-29 void init(int *val){ \nS-30 for (int i=0; i<N; i++) val[i]=i; \nS-31 val[N/2 ] = 2*N; \n326 OpenMP Examples Version 5.2.1 - November 2022 \nS-32 val[N/2+1] = -2*N; \nS-33 } \nC / C++ \n1 In OpenMP 5.1 the compare clause was also added to support Compare And Swap (CAS) \n2 semantics. In the following example the enqueue routine (a naive implementation of a Michael and \n3 Scott enqueue function), uses the compare clause, with the capture clause, to perform and \n4 compare (q->head == node->next) and swap (if-else assignments) of the form: \n5 { r = x == e; if(r) { x = d; } else { v = x; } }. \n6 The example program concurrently enqueues nodes from an array of nodes (nodes[N]). Since the \n7 equivalence of Fortran pointers can be determined only with a function (such as associated), no \n8 Fortran version is provided here. The use of the associated function in an atomic compare syntax is \n9 being considered in a future release. \nC / C++ \n10 Example cas.2.c (omp_5.1) \nS-1 #include <stdlib.h> \nS-2 #include <stdio.h> \nS-3 #include <stdbool.h> \nS-4 #include <stddef.h> \nS-5 \nS-6 #define N 10 \nS-7 \nS-8 typedef struct Node{ struct Node *next; int id; } Node; \nS-9 typedef struct Queue{ Node *head; Node *tail; } Queue; \nS-10 \nS-11 void enqueue( Queue *, Node * ); \nS-12 \nS-13 int main(){ \nS-14 Queue q; \nS-15 Node nodes[N]; \nS-16 int id_check[N]; \nS-17 \nS-18 // Initializing \nS-19 for(int i=0; i<N; i++){ \nS-20 nodes[i].next=NULL; nodes[i].id=i; id_check[i]=-1; \nS-21 } \nS-22 \nS-23 q.tail=&nodes[0]; // Fill initial tail \nS-24 \nS-25 // Enqueue \nS-26 #pragma omp parallel for num_threads(2) \nS-27 for(int i=1; i<N; i++){ \nS-28 enqueue(&q,&nodes[i]); \nCHAPTER 9. SYNCHRONIZATION 327 \nS-29 } \nS-30 \nS-31 // Checking Results Below \nS-32 Node *node=q.tail; \nS-33 do{ \nS-34 id_check[node->id]=node->id; //Store found id at position id \nS-35 node =node->next; \nS-36 }while(node->next != NULL); \nS-37 id_check[node->id]=node->id; //checking also the 1st node here \nS-38 \nS-39 for(int id=0; id<N; id++){ // all ids should be found \nS-40 if(id != id_check[id]) {printf(\"FAILED\\n\"); exit(1);} \nS-41 } \nS-42 printf(\"PASSED\\n\"); \nS-43 \nS-44 return 0; \nS-45 } \nS-46 \nS-47 void enqueue(Queue *queue, Node *node) { \nS-48 bool result = false; \nS-49 \nS-50 #pragma omp atomic read \nS-51 node->next = queue->tail; \nS-52 do{ \nS-53 #pragma omp atomic compare capture \nS-54 { \nS-55 result = queue->tail == node->next; \nS-56 if(result) { \nS-57 queue->tail = node; \nS-58 }else{ \nS-59 node->next = queue->tail; \nS-60 } \nS-61 } \nS-62 }while(!result); \nS-63 } \nC / C++ \n328 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.6 Restrictions on the atomic Construct", "chunk": ""}
{"section_title": "9.6 Restrictions on the atomic Construct", "chunk": "2 The following non-conforming examples illustrate the restrictions on the atomic construct. \nC / C++ \n3 Example atomic_restrict.1.c (omp_3.1) \nS-1 void atomic_wrong () \nS-2 { \nS-3 union {int n; float x;} u; \nS-4 \nS-5 #pragma omp parallel \nS-6 { \nS-7 #pragma omp atomic update \nS-8 u.n++; \nS-9 \nS-10 #pragma omp atomic update \nS-11 u.x += 1.0; \nS-12 \nS-13 /* Incorrect because the atomic constructs reference the same location \nS-14 through incompatible types */ \nS-15 } \nS-16 } \nC / C++ \nFortran \n4 Example atomic_restrict.1.f (omp_3.1) \nS-1 SUBROUTINE ATOMIC_WRONG() \nS-2 INTEGER:: I \nS-3 REAL:: R \nS-4 EQUIVALENCE(I,R) \nS-5 \nS-6 !$OMP PARALLEL \nS-7 !$OMP ATOMIC UPDATE \nS-8 I = I + 1 \nS-9 !$OMP ATOMIC UPDATE \nS-10 R = R + 1.0 \nS-11 ! incorrect because I and R reference the same location \nS-12 ! but have different types \nS-13 !$OMP END PARALLEL \nS-14 END SUBROUTINE ATOMIC_WRONG \nFortran \nCHAPTER 9. SYNCHRONIZATION 329 \nC / C++ \n1 Example atomic_restrict.2.c (omp_3.1) \nS-1 void atomic_wrong2 () \nS-2 { \nS-3 int x; \nS-4 int *i; \nS-5 float *r; \nS-6 \nS-7 i = &x; \nS-8 r = (float *)&x; \nS-9 \nS-10 #pragma omp parallel \nS-11 { \nS-12 #pragma omp atomic update \nS-13 *i += 1; \nS-14 \nS-15 #pragma omp atomic update \nS-16 *r += 1.0; \nS-17 \nS-18 /* Incorrect because the atomic constructs reference the same location \nS-19 through incompatible types */ \nS-20 \nS-21 } \nS-22 } \nC / C++ \nFortran \n2 The following example is non-conforming because I and R reference the same location but have \n3 different types. \n4 Example atomic_restrict.2.f (omp_3.1) \nS-1 SUBROUTINE SUB() \nS-2 COMMON /BLK/ R \nS-3 REAL R \nS-4 \nS-5 !$OMP ATOMIC UPDATE \nS-6 R = R + 1.0 \nS-7 END SUBROUTINE SUB \nS-8 \nS-9 SUBROUTINE ATOMIC_WRONG2() \nS-10 COMMON /BLK/ I \nS-11 INTEGER I \nS-12 \nS-13 !$OMP PARALLEL \nS-14 \n330 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 !$OMP ATOMIC UPDATE \nS-16 I = I + 1 \nS-17 CALL SUB() \nS-18 !$OMP END PARALLEL \nS-19 END SUBROUTINE ATOMIC_WRONG2 \n1 Although the following example might work on some implementations, this is also non-conforming: \n2 Example atomic_restrict.3.f (omp_3.1) \nS-1 SUBROUTINE ATOMIC_WRONG3 \nS-2 INTEGER:: I \nS-3 REAL:: R \nS-4 EQUIVALENCE(I,R) \nS-5 \nS-6 !$OMP PARALLEL \nS-7 !$OMP ATOMIC UPDATE \nS-8 I = I + 1 \nS-9 ! incorrect because I and R reference the same location \nS-10 ! but have different types \nS-11 !$OMP END PARALLEL \nS-12 \nS-13 !$OMP PARALLEL \nS-14 !$OMP ATOMIC UPDATE \nS-15 R = R + 1.0 \nS-16 ! incorrect because I and R reference the same location \nS-17 ! but have different types \nS-18 !$OMP END PARALLEL \nS-19 \nS-20 END SUBROUTINE ATOMIC_WRONG3 \nFortran \nCHAPTER 9. SYNCHRONIZATION 331 \n"}
{"section_title": "9.7 flush Construct without a List", "chunk": ""}
{"section_title": "9.7 flush Construct without a List", "chunk": "2 The following example distinguishes the shared variables affected by a flush construct with no \n3 list from the shared objects that are not affected: \nC / C++ \n4 Example flush_nolist.1.c (pre_omp_3.0) \nS-1 int x, *p = &x; \nS-2 \nS-3 void f1(int *q) \nS-4 { \nS-5 *q = 1; \nS-6 #pragma omp flush \nS-7 /* x, p, and *q are flushed */ \nS-8 /* because they are shared and accessible */ \nS-9 /* q is not flushed because it is not shared. */ \nS-10 } \nS-11 \nS-12 void f2(int *q) \nS-13 { \nS-14 #pragma omp barrier \nS-15 *q = 2; \nS-16 #pragma omp barrier \nS-17 \nS-18 /* a barrier implies a flush */ \nS-19 /* x, p, and *q are flushed */ \nS-20 /* because they are shared and accessible */ \nS-21 /* q is not flushed because it is not shared. */ \nS-22 } \nS-23 \nS-24 int g(int n) \nS-25 { \nS-26 int i = 1, j, sum = 0; \nS-27 *p = 1; \nS-28 #pragma omp parallel reduction(+: sum) num_threads(10) \nS-29 { \nS-30 f1(&j); \nS-31 \nS-32 /* i, n and sum were not flushed */ \nS-33 /* because they were not accessible in f1 */ \nS-34 /* j was flushed because it was accessible */ \nS-35 sum += j; \nS-36 \nS-37 f2(&j); \nS-38 \nS-39 /* i, n, and sum were not flushed */ \n332 OpenMP Examples Version 5.2.1 - November 2022 \nS-40 /* because they were not accessible in f2 */ \nS-41 /* j was flushed because it was accessible */ \nS-42 sum += i + j + *p + n; \nS-43 } \nS-44 return sum; \nS-45 } \nS-46 \nS-47 int main() \nS-48 { \nS-49 int result = g(7); \nS-50 return result; \nS-51 } \nC / C++ \nFortran \n1 Example flush_nolist.1.f (pre_omp_3.0) \nS-1 SUBROUTINE F1(Q) \nS-2 COMMON /DATA/ X, P \nS-3 INTEGER, TARGET :: X \nS-4 INTEGER, POINTER :: P \nS-5 INTEGER Q \nS-6 \nS-7 Q = 1 \nS-8 !$OMP FLUSH \nS-9 ! X, P and Q are flushed \nS-10 ! because they are shared and accessible \nS-11 END SUBROUTINE F1 \nS-12 \nS-13 SUBROUTINE F2(Q) \nS-14 COMMON /DATA/ X, P \nS-15 INTEGER, TARGET :: X \nS-16 INTEGER, POINTER :: P \nS-17 INTEGER Q \nS-18 \nS-19 !$OMP BARRIER \nS-20 Q = 2 \nS-21 !$OMP BARRIER \nS-22 ! a barrier implies a flush \nS-23 ! X, P and Q are flushed \nS-24 ! because they are shared and accessible \nS-25 END SUBROUTINE F2 \nS-26 \nS-27 INTEGER FUNCTION G(N) \nS-28 COMMON /DATA/ X, P \nS-29 INTEGER, TARGET :: X \nS-30 INTEGER, POINTER :: P \nCHAPTER 9. SYNCHRONIZATION 333 \nS-31 INTEGER N \nS-32 INTEGER I, J, SUM \nS-33 \nS-34 I = 1 \nS-35 SUM = 0 \nS-36 P = 1 \nS-37 !$OMP PARALLEL REDUCTION(+: SUM) NUM_THREADS(10) \nS-38 CALL F1(J) \nS-39 ! I, N and SUM were not flushed \nS-40 ! because they were not accessible in F1 \nS-41 ! J was flushed because it was accessible \nS-42 SUM = SUM + J \nS-43 \nS-44 CALL F2(J) \nS-45 ! I, N, and SUM were not flushed \nS-46 ! because they were not accessible in f2 \nS-47 ! J was flushed because it was accessible \nS-48 SUM = SUM + I + J + P + N \nS-49 !$OMP END PARALLEL \nS-50 \nS-51 G = SUM \nS-52 END FUNCTION G \nS-53 \nS-54 PROGRAM FLUSH_NOLIST \nS-55 COMMON /DATA/ X, P \nS-56 INTEGER, TARGET :: X \nS-57 INTEGER, POINTER :: P \nS-58 INTEGER RESULT, G \nS-59 \nS-60 P => X \nS-61 RESULT = G(7) \nS-62 PRINT *, RESULT \nS-63 END PROGRAM FLUSH_NOLIST \nFortran \n334 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.8 Synchronization Based on Acquire/Release Semantics", "chunk": ""}
{"section_title": "9.8 Synchronization Based on Acquire/Release Semantics", "chunk": "2 Semantics \n3 As explained in the Memory Model chapter of this document, a flush operation may be an acquire \n4 flush and/or a release flush, and OpenMP 5.0 defines acquire/release semantics in terms of these \n5 fundamental flush operations. For any synchronization between two threads that is specified by \n6 OpenMP, a release flush logically occurs at the source of the synchronization and an acquire flush \n7 logically occurs at the sink of the synchronization. OpenMP 5.0 added memory ordering clauses \u2013 \n8 acquire, release, and acq_rel \u2013 to the flush and atomic constructs for explicitly \n9 requesting acquire/release semantics. Furthermore, implicit flushes for all OpenMP constructs and \n10 runtime routines that synchronize OpenMP threads in some manner were redefined in terms of \n11 synchronizing release and acquire flushes to avoid the requirement of strong memory fences (see \n12 the Flush Synchronization and Happens Before and Implicit Flushes sections of the OpenMP \n13 Specifications document). \n14 The examples that follow in this section illustrate how acquire and release flushes may be \n15 employed, implicitly or explicitly, for synchronizing threads. A flush directive without a list and \n16 without any memory ordering clause can also function as both an acquire and release flush for \n17 facilitating thread synchronization. Flushes implied on entry to, or exit from, an atomic operation \n18 (specified by an atomic construct) may function as an acquire flush or a release flush if a memory \n19 ordering clause appears on the construct. On entry to and exit from a critical construct there is \n20 now an implicit acquire flush and release flush, respectively. \n21 The first example illustrates how the release and acquire flushes implied by a critical region \n22 guarantee a value written by the first thread is visible to a read of the value on the second thread. \n23 Thread 0 writes to x and then executes a critical region in which it writes to y; the write to x \n24 happens before the execution of the critical region, consistent with the program order of the \n25 thread. Meanwhile, thread 1 executes a critical region in a loop until it reads a non-zero value \n26 from y in the critical region, after which it prints the value of x; again, the execution of the \n27 critical regions happen before the read from x based on the program order of the thread. The \n28 critical regions executed by the two threads execute in a serial manner, with a pairwise \n29 synchronization from the exit of one critical region to the entry to the next critical region. \n30 These pairwise synchronizations result from the implicit release flushes that occur on exit from \n31 critical regions and the implicit acquire flushes that occur on entry to critical regions; \n32 hence, the execution of each critical region in the sequence happens before the execution of the \n33 next critical region. A \u201chappens before\u201d order is therefore established between the assignment \n34 to x by thread 0 and the read from x by thread 1, and so thread 1 must see that x equals 10. \nCHAPTER 9. SYNCHRONIZATION 335 \nC / C++ \n1 Example acquire_release.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 int x = 0, y = 0; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 int thrd = omp_get_thread_num(); \nS-10 if (thrd == 0) { \nS-11 x = 10; \nS-12 #pragma omp critical \nS-13 { y = 1; } \nS-14 } else { \nS-15 int tmp = 0; \nS-16 while (tmp == 0) { \nS-17 #pragma omp critical \nS-18 { tmp = y; } \nS-19 } \nS-20 printf(\"x = %d\\n\", x); // always \"x = 10\" \nS-21 } \nS-22 } \nS-23 return 0; \nS-24 } \nC / C++ \nFortran \n2 Example acquire_release.1.f90 (omp_5.0) \nS-1 program rel_acq_ex1 \nS-2 use omp_lib \nS-3 integer :: x, y, thrd, tmp \nS-4 x = 0 \nS-5 y = 0 \nS-6 !$omp parallel num_threads(2) private(thrd, tmp) \nS-7 thrd = omp_get_thread_num() \nS-8 if (thrd == 0) then \nS-9 x = 10 \nS-10 !$omp critical \nS-11 y = 1 \nS-12 !$omp end critical \nS-13 else \nS-14 tmp = 0 \nS-15 do while (tmp == 0) \n336 OpenMP Examples Version 5.2.1 - November 2022 \nS-16 !$omp critical \nS-17 tmp = y \nS-18 !$omp end critical \nS-19 end do \nS-20 print *, \"x = \", x !! always \"x = 10\" \nS-21 end if \nS-22 !$omp end parallel \nS-23 end program \nFortran \n1 In the second example, the critical constructs are exchanged with atomic constructs that have \n2 explicit memory ordering specified. When the atomic read operation on thread 1 reads a non-zero \n3 value from y, this results in a release/acquire synchronization that in turn implies that the \n4 assignment to x on thread 0 happens before the read of x on thread 1. Therefore, thread 1 will print \n5 \u201cx = 10\u201d. \nC / C++ \n6 Example acquire_release.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 int x = 0, y = 0; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 int thrd = omp_get_thread_num(); \nS-10 if (thrd == 0) { \nS-11 x = 10; \nS-12 #pragma omp atomic write release // or seq_cst \nS-13 y = 1; \nS-14 } else { \nS-15 int tmp = 0; \nS-16 while (tmp == 0) { \nS-17 #pragma omp atomic read acquire // or seq_cst \nS-18 tmp = y; \nS-19 } \nS-20 printf(\"x = %d\\n\", x); // always \"x = 10\" \nS-21 } \nS-22 } \nS-23 return 0; \nS-24 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 337 \nFortran \n1 Example acquire_release.2.f90 (omp_5.0) \nS-1 program rel_acq_ex2 \nS-2 use omp_lib \nS-3 integer :: x, y, thrd, tmp \nS-4 x = 0 \nS-5 y = 0 \nS-6 !$omp parallel num_threads(2) private(thrd, tmp) \nS-7 thrd = omp_get_thread_num() \nS-8 if (thrd == 0) then \nS-9 x = 10 \nS-10 !$omp atomic write release ! or seq_cst \nS-11 y = 1 \nS-12 !$omp end atomic \nS-13 else \nS-14 tmp = 0 \nS-15 do while (tmp == 0) \nS-16 !$omp atomic read acquire ! or seq_cst \nS-17 tmp = y \nS-18 !$omp end atomic \nS-19 end do \nS-20 print *, \"x = \", x !! always \"x = 10\" \nS-21 end if \nS-22 !$omp end parallel \nS-23 end program \nFortran \n338 OpenMP Examples Version 5.2.1 - November 2022 \n1 In the third example, atomic constructs that specify relaxed atomic operations are used with \n2 explicit flush directives to enforce memory ordering between the two threads. The explicit \n3 flush directive on thread 0 must specify a release flush and the explicit flush directive on \n4 thread 1 must specify an acquire flush to establish a release/acquire synchronization between the \n5 two threads. The flush and atomic constructs encountered by thread 0 can be replaced by the \n6 atomic construct used in Example 2 for thread 0, and similarly the flush and atomic \n7 constructs encountered by thread 1 can be replaced by the atomic construct used in Example 2 for \n8 thread 1. \nC / C++ \n9 Example acquire_release.3.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 int x = 0, y = 0; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 int thrd = omp_get_thread_num(); \nS-10 if (thrd == 0) { \nS-11 x = 10; \nS-12 #pragma omp flush // or with acq_rel or release clause \nS-13 #pragma omp atomic write // or with relaxed clause \nS-14 y = 1; \nS-15 } else { \nS-16 int tmp = 0; \nS-17 while (tmp == 0) { \nS-18 #pragma omp atomic read // or with relaxed clause \nS-19 tmp = y; \nS-20 } \nS-21 #pragma omp flush // or with acq_rel or acquire clause \nS-22 printf(\"x = %d\\n\", x); // always \"x = 10\" \nS-23 } \nS-24 } \nS-25 return 0; \nS-26 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 339 \nFortran \n1 Example acquire_release.3.f90 (omp_5.0) \nS-1 program rel_acq_ex3 \nS-2 use omp_lib \nS-3 integer :: x, y, thrd, tmp \nS-4 x = 0 \nS-5 y = 0 \nS-6 !$omp parallel num_threads(2) private(thrd, tmp) \nS-7 thrd = omp_get_thread_num() \nS-8 if (thrd == 0) then \nS-9 x = 10 \nS-10 !$omp flush ! or with acq_rel or release clause \nS-11 !$omp atomic write \nS-12 y = 1 \nS-13 !$omp end atomic \nS-14 else \nS-15 tmp = 0 \nS-16 do while (tmp == 0) \nS-17 !$omp atomic read \nS-18 tmp = y \nS-19 !$omp end atomic \nS-20 end do \nS-21 !$omp flush ! or with acq_rel or acquire clause \nS-22 print *, \"x = \", x !! always \"x = 10\" \nS-23 end if \nS-24 !$omp end parallel \nS-25 end program \nFortran \n2 Example 4 will fail to order the write to x on thread 0 before the read from x on thread 1. \n3 Importantly, the implicit release flush on exit from the critical region will not synchronize with \n4 the acquire flush that occurs on the atomic read operation performed by thread 1. This is because \n5 implicit release flushes that occur on a given construct may only synchronize with implicit acquire \n6 flushes on a compatible construct (and vice-versa) that internally makes use of the same \n7 synchronization variable. For a critical construct, this might correspond to a lock object that is \n8 used by a given implementation (for the synchronization semantics of other constructs due to \n9 implicit release and acquire flushes, refer to the Implicit Flushes section of the OpenMP \n10 Specifications document). Either an explicit flush directive that provides a release flush (i.e., a \n11 flush without a list that does not have the acquire clause) must be specified between the \n12 critical construct and the atomic write, or an atomic operation that modifies y and provides \n13 release semantics must be specified. \n340 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example acquire_release_broke.4.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 \nS-7 // !!! THIS CODE WILL FAIL TO PRODUCE CONSISTENT RESULTS !!!!!!! \nS-8 // !!! DO NOT PROGRAM SYNCHRONIZATION THIS WAY !!!!!!! \nS-9 \nS-10 int x = 0, y; \nS-11 #pragma omp parallel num_threads(2) \nS-12 { \nS-13 int thrd = omp_get_thread_num(); \nS-14 if (thrd == 0) { \nS-15 #pragma omp critical \nS-16 { x = 10; } \nS-17 // an explicit flush directive that provides \nS-18 // release semantics is needed here \nS-19 // to complete the synchronization. \nS-20 #pragma omp atomic write \nS-21 y = 1; \nS-22 } else { \nS-23 int tmp = 0; \nS-24 while (tmp == 0) { \nS-25 #pragma omp atomic read acquire // or seq_cst \nS-26 tmp = y; \nS-27 } \nS-28 #pragma omp critical \nS-29 { printf(\"x = %d\\n\", x); } // !! NOT ALWAYS 10 \nS-30 } \nS-31 } \nS-32 return 0; \nS-33 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 341 \nFortran \n1 Example acquire_release_broke.4.f90 (omp_5.0) \nS-1 program rel_acq_ex4 \nS-2 use omp_lib \nS-3 integer :: x, y, thrd \nS-4 integer :: tmp \nS-5 x = 0 \nS-6 \nS-7 !! !!! THIS CODE WILL FAIL TO PRODUCE CONSISTENT RESULTS !!!!!!! \nS-8 !! !!! DO NOT PROGRAM SYNCHRONIZATION THIS WAY !!!!!!! \nS-9 \nS-10 !$omp parallel num_threads(2) private(thrd) private(tmp) \nS-11 thrd = omp_get_thread_num() \nS-12 if (thrd == 0) then \nS-13 !$omp critical \nS-14 x = 10 \nS-15 !$omp end critical \nS-16 ! an explicit flush directive that provides \nS-17 ! release semantics is needed here to \nS-18 ! complete the synchronization. \nS-19 !$omp atomic write \nS-20 y = 1 \nS-21 !$omp end atomic \nS-22 else \nS-23 tmp = 0 \nS-24 do while(tmp == 0) \nS-25 !$omp atomic read acquire ! or seq_cst \nS-26 tmp = x \nS-27 !$omp end atomic \nS-28 end do \nS-29 !$omp critical \nS-30 print *, \"x = \", x !! !! NOT ALWAYS 10 \nS-31 !$omp end critical \nS-32 end if \nS-33 !$omp end parallel \nS-34 end program \nFortran \n342 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.9 ordered Clause and ordered Construct", "chunk": ""}
{"section_title": "9.9 ordered Clause and ordered Construct", "chunk": "2 Ordered constructs are useful for sequentially ordering the output from work that is done in \n3 parallel. The following program prints out the indices in sequential order: \nC / C++ \n4 Example ordered.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 void work(int k) \nS-4 { \nS-5 #pragma omp ordered \nS-6 printf(\" %d\\n\", k); \nS-7 } \nS-8 \nS-9 void ordered_example(int lb, int ub, int stride) \nS-10 { \nS-11 int i; \nS-12 \nS-13 #pragma omp parallel for ordered schedule(dynamic) \nS-14 for (i=lb; i<ub; i+=stride) \nS-15 work(i); \nS-16 } \nS-17 \nS-18 int main() \nS-19 { \nS-20 ordered_example(0, 100, 5); \nS-21 return 0; \nS-22 } \nC / C++ \nFortran \n5 Example ordered.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(K) \nS-2 INTEGER k \nS-3 \nS-4 !$OMP ORDERED \nS-5 WRITE(*,*) K \nS-6 !$OMP END ORDERED \nS-7 \nS-8 END SUBROUTINE WORK \nS-9 \nS-10 SUBROUTINE SUB(LB, UB, STRIDE) \nS-11 INTEGER LB, UB, STRIDE \nS-12 INTEGER I \nCHAPTER 9. SYNCHRONIZATION 343 \nS-13 \nS-14 !$OMP PARALLEL DO ORDERED SCHEDULE(DYNAMIC) \nS-15 DO I=LB,UB,STRIDE \nS-16 CALL WORK(I) \nS-17 END DO \nS-18 !$OMP END PARALLEL DO \nS-19 \nS-20 END SUBROUTINE SUB \nS-21 \nS-22 PROGRAM ORDERED_EXAMPLE \nS-23 CALL SUB(1,100,5) \nS-24 END PROGRAM ORDERED_EXAMPLE \nFortran \n1 It is possible to have multiple ordered constructs within a loop region with the ordered clause \n2 specified. The first example is non-conforming because all iterations execute two ordered \n3 regions. An iteration of a loop must not execute more than one ordered region: \nC / C++ \n4 Example ordered.2.c (pre_omp_3.0) \nS-1 void work(int i) {} \nS-2 \nS-3 void ordered_wrong(int n) \nS-4 { \nS-5 int i; \nS-6 #pragma omp for ordered \nS-7 for (i=0; i<n; i++) { \nS-8 /* incorrect because an iteration may not execute more than one \nS-9 ordered region */ \nS-10 #pragma omp ordered \nS-11 work(i); \nS-12 #pragma omp ordered \nS-13 work(i+1); \nS-14 } \nS-15 } \nC / C++ \n344 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example ordered.2.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I) \nS-2 INTEGER I \nS-3 END SUBROUTINE WORK \nS-4 \nS-5 SUBROUTINE ORDERED_WRONG(N) \nS-6 INTEGER N \nS-7 \nS-8 INTEGER I \nS-9 !$OMP DO ORDERED \nS-10 DO I = 1, N \nS-11 ! incorrect because an iteration may not execute more than one \nS-12 ! ordered region \nS-13 !$OMP ORDERED \nS-14 CALL WORK(I) \nS-15 !$OMP END ORDERED \nS-16 \nS-17 !$OMP ORDERED \nS-18 CALL WORK(I+1) \nS-19 !$OMP END ORDERED \nS-20 END DO \nS-21 END SUBROUTINE ORDERED_WRONG \nFortran \n2 The following is a conforming example with more than one ordered construct. Each iteration \n3 will execute only one ordered region: \nC / C++ \n4 Example ordered.3.c (pre_omp_3.0) \nS-1 void work(int i) {} \nS-2 void ordered_good(int n) \nS-3 { \nS-4 int i; \nS-5 #pragma omp for ordered \nS-6 for (i=0; i<n; i++) { \nS-7 if (i <= 10) { \nS-8 #pragma omp ordered \nS-9 work(i); \nS-10 } \nS-11 if (i > 10) { \nS-12 #pragma omp ordered \nS-13 work(i+1); \nS-14 } \nCHAPTER 9. SYNCHRONIZATION 345 \nS-15 } \nS-16 } \nC / C++ \nFortran \n1 Example ordered.3.f (pre_omp_3.0) \nS-1 SUBROUTINE ORDERED_GOOD(N) \nS-2 INTEGER N \nS-3 \nS-4 !$OMP DO ORDERED \nS-5 DO I = 1,N \nS-6 IF (I <= 10) THEN \nS-7 !$OMP ORDERED \nS-8 CALL WORK(I) \nS-9 !$OMP END ORDERED \nS-10 ENDIF \nS-11 \nS-12 IF (I > 10) THEN \nS-13 !$OMP ORDERED \nS-14 CALL WORK(I+1) \nS-15 !$OMP END ORDERED \nS-16 ENDIF \nS-17 ENDDO \nS-18 END SUBROUTINE ORDERED_GOOD \nFortran \n346 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.10 depobj Construct", "chunk": ""}
{"section_title": "9.10 depobj Construct", "chunk": "2 The stand-alone depobj construct provides a mechanism to create a depend object that expresses \n3 a dependence to be used subsequently in the depend clause of another construct. The dependence \n4 is created from a dependence type and a storage location, within a depend clause of an depobj \n5 construct; and it is stored in the depend object. The depend object is represented by a variable of \n6 type omp_depend_t in C/C++ (by a scalar variable of integer kind omp_depend_kind in \n7 Fortran). \n8 In the example below the stand-alone depobj construct uses the depend, update and \n9 destroy clauses to initialize, update and uninitialize a depend object (obj). \n10 The first depobj construct initializes the obj depend object with an inout dependence type \n11 with a storage location defined by variable a. This dependence is passed into the driver routine via \n12 the obj depend object. \n13 In the first driver routine call, Task 1 uses the dependence of the object (inout), while Task 2 uses \n14 an in dependence specified directly in a depend clause. For these task dependences Task 1 must \n15 execute and complete before Task 2 begins. \n16 Before the second call to driver, obj is updated using the depobj construct to represent an in \n17 dependence. Hence, in the second call to driver, Task 1 will have an in dependence; and Task 1 \n18 and Task 2 can execute simultaneously. Note: in an update clause, only the dependence type can \n19 be (is) updated. \n20 The third depobj construct uses the destroy clause. It frees resources as it puts the depend \n21 object in an uninitialized state\u2013 effectively destroying the depend object. After an object has been \n22 uninitialized it can be initialized again with a new dependence type and a new variable. \nC / C++ \n23 Example depobj.1.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 #define N 100 \nS-5 #define TRUE 1 \nS-6 #define FALSE 0 \nS-7 \nS-8 void driver(int update, float a[], float b[], int n, omp_depend_t *obj); \nS-9 \nS-10 void update_copy(int update, float a[], float b[], int n); \nS-11 void checkpoint(float a[],int n); \nS-12 void init(float a[], int n); \nS-13 \nS-14 \nS-15 int main(){ \nS-16 \nCHAPTER 9. SYNCHRONIZATION 347 \nS-17 float a[N],b[N]; \nS-18 omp_depend_t obj; \nS-19 \nS-20 init(a, N); \nS-21 \nS-22 #pragma omp depobj(obj) depend(inout: a) \nS-23 \nS-24 driver(TRUE, a,b,N, &obj); // updating a occurs \nS-25 \nS-26 #pragma omp depobj(obj) update(in) \nS-27 \nS-28 driver(FALSE, a,b,N, &obj); // no updating of a \nS-29 \nS-30 #pragma omp depobj(obj) destroy(obj) // obj is set to uninitialized \nS-31 // state, resources are freed \nS-32 return 0; \nS-33 \nS-34 } \nS-35 \nS-36 void driver(int update, float a[], float b[], int n, omp_depend_t *obj) \nS-37 { \nS-38 #pragma omp parallel num_threads(2) \nS-39 #pragma omp single \nS-40 { \nS-41 \nS-42 #pragma omp task depend(depobj: *obj) // Task 1, uses depend object \nS-43 update_copy(update, a,b,n); // may update a, always copy a to b \nS-44 \nS-45 #pragma omp task depend(in: a[:n]) // Task 2, only read a \nS-46 checkpoint(a,n); \nS-47 } \nS-48 } \nS-49 \nS-50 void update_copy(int update, float a[], float b[], int n) \nS-51 { \nS-52 if(update) for(int i=0;i<n;i++) a[i]+=1.0f; \nS-53 \nS-54 for(int i=0;i<n;i++) b[i]=a[i]; \nS-55 } \nS-56 \nS-57 void checkpoint(float a[], int n) \nS-58 { \nS-59 for(int i=0;i<n;i++) printf(\" %f \",a[i]); \nS-60 printf(\"\\n\"); \nS-61 } \nS-62 \nS-63 void init(float a[], int n) \n348 OpenMP Examples Version 5.2.1 - November 2022 \nS-64 { \nS-65 for(int i=0;i<n;i++) a[i]=i; \nS-66 } \nC / C++ \nFortran \n1 Example depobj.1.f90 (omp_5.2) \nS-1 program main \nS-2 use omp_lib \nS-3 implicit none \nS-4 \nS-5 integer,parameter :: N=100 \nS-6 real :: a(N),b(N) \nS-7 integer(omp_depend_kind) :: obj \nS-8 \nS-9 call init(a, N) \nS-10 \nS-11 !$omp depobj(obj) depend(inout: a) \nS-12 \nS-13 call driver(.true., a,b,N, obj) !! updating occurs \nS-14 \nS-15 !$omp depobj(obj) update(in) \nS-16 \nS-17 call driver(.false., a,b,N, obj) !! no updating \nS-18 \nS-19 !$omp depobj(obj) destroy(obj) !! obj is set to uninitialized \nS-20 !! state, resources are freed \nS-21 \nS-22 end program \nS-23 \nS-24 subroutine driver(update, a, b, n, obj) \nS-25 use omp_lib \nS-26 implicit none \nS-27 logical :: update \nS-28 real :: a(n), b(n) \nS-29 integer :: n \nS-30 integer(omp_depend_kind) :: obj \nS-31 \nS-32 !$omp parallel num_threads(2) \nS-33 \nS-34 !$omp single \nS-35 \nS-36 !$omp task depend(depobj: obj) !! Task 1, uses depend object \nS-37 call update_copy(update, a,b,n) \nS-38 !! update a or not, always copy a to b \nS-39 !$omp end task \nCHAPTER 9. SYNCHRONIZATION 349 \nS-40 \nS-41 !$omp task depend(in: a) !! Task 2, only read a \nS-42 call checkpoint(a,n) \nS-43 !$omp end task \nS-44 \nS-45 !$omp end single \nS-46 \nS-47 !$omp end parallel \nS-48 \nS-49 end subroutine \nS-50 \nS-51 subroutine update_copy(update, a, b, n) \nS-52 implicit none \nS-53 logical :: update \nS-54 real :: a(n), b(n) \nS-55 integer :: n \nS-56 \nS-57 if (update) a = a + 1.0 \nS-58 \nS-59 b = a \nS-60 \nS-61 end subroutine \nS-62 \nS-63 subroutine checkpoint( a, n) \nS-64 implicit none \nS-65 integer :: n \nS-66 real :: a(n) \nS-67 integer :: i \nS-68 \nS-69 write(*,\u2019( *(f5.0) )\u2019) (a(i), i=1,n) \nS-70 end subroutine \nS-71 \nS-72 subroutine init(a,n) \nS-73 implicit none \nS-74 integer :: n \nS-75 real :: a(n) \nS-76 integer :: i \nS-77 \nS-78 a=[ (i, i=1,n) ] \nS-79 end subroutine \nFortran \n350 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.11 Doacross Loop Nest", "chunk": ""}
{"section_title": "9.11 Doacross Loop Nest", "chunk": "2 An ordered clause can be used on a loop construct with an integer parameter argument to define \n3 the number of associated loops within a doacross loop nest where cross-iteration dependences \n4 exist. A doacross clause on an ordered construct within an ordered loop describes the \n5 dependences of the doacross loops. \n6 In the code below, the doacross(sink:i-1) clause defines an i-1 to i cross-iteration \n7 dependence that specifies a wait point for the completion of computation from iteration i-1 before \n8 proceeding to the subsequent statements. The doacross(source:omp_cur_iteration) \n9 or doacross(source:) clause indicates the completion of computation from the current \n10 iteration (i) to satisfy the cross-iteration dependence that arises from the iteration. The \n11 omp_cur_iteration keyword is optional for the source dependence type. For this example \n12 the same sequential ordering could have been achieved with an ordered clause without a \n13 parameter, on the loop directive, and a single ordered directive without the doacross clause \n14 specified for the statement executing the bar function. \nC / C++ \n15 Example doacross.1.c (omp_5.2) \nS-1 float foo(int i); \nS-2 float bar(float a, float b); \nS-3 float baz(float b); \nS-4 \nS-5 void work( int N, float *A, float *B, float *C ) \nS-6 { \nS-7 int i; \nS-8 \nS-9 #pragma omp for ordered(1) \nS-10 for (i=1; i<N; i++) \nS-11 { \nS-12 A[i] = foo(i); \nS-13 \nS-14 #pragma omp ordered doacross(sink: i-1) \nS-15 B[i] = bar(A[i], B[i-1]); \nS-16 #pragma omp ordered doacross(source: omp_cur_iteration) \nS-17 \nS-18 C[i] = baz(B[i]); \nS-19 } \nS-20 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 351 \nFortran \n1 Example doacross.1.f90 (omp_5.2) \nS-1 subroutine work( N, A, B, C ) \nS-2 integer :: N, i \nS-3 real, dimension(N) :: A, B, C \nS-4 real, external :: foo, bar, baz \nS-5 \nS-6 !$omp do ordered(1) \nS-7 do i=2, N \nS-8 A(i) = foo(i) \nS-9 \nS-10 !$omp ordered doacross(sink: i-1) \nS-11 B(i) = bar(A(i), B(i-1)) \nS-12 !$omp ordered doacross(source: omp_cur_iteration) \nS-13 \nS-14 C(i) = baz(B(i)) \nS-15 end do \nS-16 end subroutine \nFortran \n2 The following code is similar to the previous example but with doacross loop nest extended to two \n3 nested loops, i and j, as specified by the ordered(2) clause on the loop directive. In the C/C++ \n4 code, the i and j loops are the first and second associated loops, respectively, whereas in the Fortran \n5 code, the j and i loops are the first and second associated loops, respectively. The \n6 doacross(sink:i-1,j) and doacross(sink:i,j-1) clauses in the C/C++ code define \n7 cross-iteration dependences in two dimensions from iterations (i-1, j) and (i, j-1) to iteration (i, j). \n8 Likewise, the doacross(sink:j-1,i) and doacross(sink:j,i-1) clauses in the \n9 Fortran code define cross-iteration dependences from iterations (j-1, i) and (j, i-1) to iteration (j, i). \nC / C++ \n10 Example doacross.2.c (omp_5.2) \nS-1 float foo(int i, int j); \nS-2 float bar(float a, float b, float c); \nS-3 float baz(float b); \nS-4 \nS-5 void work( int N, int M, float **A, float **B, float **C ) \nS-6 { \nS-7 int i, j; \nS-8 \nS-9 #pragma omp for ordered(2) \nS-10 for (i=1; i<N; i++) \nS-11 { \nS-12 for (j=1; j<M; j++) \nS-13 { \n352 OpenMP Examples Version 5.2.1 - November 2022 \nS-14 A[i][j] = foo(i, j); \nS-15 \nS-16 #pragma omp ordered doacross(sink: i-1,j) doacross(sink: i,j-1) \nS-17 B[i][j] = bar(A[i][j], B[i-1][j], B[i][j-1]); \nS-18 #pragma omp ordered doacross(source:) \nS-19 \nS-20 C[i][j] = baz(B[i][j]); \nS-21 } \nS-22 } \nS-23 } \nC / C++ \nFortran \n1 Example doacross.2.f90 (omp_5.2) \nS-1 subroutine work( N, M, A, B, C ) \nS-2 integer :: N, M, i, j \nS-3 real, dimension(M,N) :: A, B, C \nS-4 real, external :: foo, bar, baz \nS-5 \nS-6 !$omp do ordered(2) \nS-7 do j=2, N \nS-8 do i=2, M \nS-9 A(i,j) = foo(i, j) \nS-10 \nS-11 !$omp ordered doacross(sink: j-1,i) doacross(sink: j,i-1) \nS-12 B(i,j) = bar(A(i,j), B(i-1,j), B(i,j-1)) \nS-13 !$omp ordered doacross(source:) \nS-14 \nS-15 C(i,j) = baz(B(i,j)) \nS-16 end do \nS-17 end do \nS-18 end subroutine \nFortran \n2 The following example shows the incorrect use of the ordered directive with a doacross \n3 clause. There are two issues with the code. The first issue is a missing \n4 ordered doacross(source:) directive, which could cause a deadlock. The second issue is \n5 the doacross(sink:i+1,j) and doacross(sink:i,j+1) clauses define dependences \n6 on lexicographically later source iterations (i+1, j) and (i, j+1), which could cause a deadlock as \n7 well since they may not start to execute until the current iteration completes. \nCHAPTER 9. SYNCHRONIZATION 353 \nC / C++ \n1 Example doacross.3.c (omp_5.2) \nS-1 #define N 100 \nS-2 \nS-3 void work_wrong(double p[][N][N]) \nS-4 { \nS-5 int i, j, k; \nS-6 \nS-7 #pragma omp parallel for ordered(2) private(i,j,k) \nS-8 for (i=1; i<N-1; i++) \nS-9 { \nS-10 for (j=1; j<N-1; j++) \nS-11 { \nS-12 #pragma omp ordered doacross(sink: i-1,j) doacross(sink: i+1,j) \\ \nS-13 doacross(sink: i,j-1) doacross(sink: i,j+1) \nS-14 for (k=1; k<N-1; k++) \nS-15 { \nS-16 double tmp1 = p[i-1][j][k] + p[i+1][j][k]; \nS-17 double tmp2 = p[i][j-1][k] + p[i][j+1][k]; \nS-18 double tmp3 = p[i][j][k-1] + p[i][j][k+1]; \nS-19 p[i][j][k] = (tmp1 + tmp2 + tmp3) / 6.0; \nS-20 } \nS-21 /* missing #pragma omp ordered doacross(source:) */ \nS-22 } \nS-23 } \nS-24 } \nC / C++ \nFortran \n2 Example doacross.3.f90 (omp_5.2) \nS-1 subroutine work_wrong(N, p) \nS-2 integer :: N \nS-3 real(8), dimension(N,N,N) :: p \nS-4 integer :: i, j, k \nS-5 real(8) :: tmp1, tmp2, tmp3 \nS-6 \nS-7 !$omp parallel do ordered(2) private(i,j,k,tmp1,tmp2,tmp3) \nS-8 do i=2, N-1 \nS-9 do j=2, N-1 \nS-10 !$omp ordered doacross(sink: i-1,j) doacross(sink: i+1,j) & \nS-11 !$omp& doacross(sink: i,j-1) doacross(sink: i,j+1) \nS-12 do k=2, N-1 \nS-13 tmp1 = p(k-1,j,i) + p(k+1,j,i) \nS-14 tmp2 = p(k,j-1,i) + p(k,j+1,i) \nS-15 tmp3 = p(k,j,i-1) + p(k,j,i+1) \n354 OpenMP Examples Version 5.2.1 - November 2022 \nS-16 p(k,j,i) = (tmp1 + tmp2 + tmp3) / 6.0 \nS-17 end do \nS-18 ! missing !$omp ordered doacross(source:) \nS-19 end do \nS-20 end do \nS-21 end subroutine \nFortran \n1 The following example illustrates the use of the collapse clause for a doacross loop nest. The i \n2 and j loops are the associated loops for the collapsed loop as well as for the doacross loop nest. The \n3 example also shows a compliant usage of the dependence source directive placed before the \n4 corresponding sink directive. Checking the completion of computation from previous iterations at \n5 the sink point can occur after the source statement. \nC / C++ \n6 Example doacross.4.c (omp_5.2) \nS-1 double foo(int i, int j); \nS-2 \nS-3 void work( int N, int M, double **A, double **B, double **C ) \nS-4 { \nS-5 int i, j; \nS-6 double alpha = 1.2; \nS-7 \nS-8 #pragma omp for collapse(2) ordered(2) \nS-9 for (i = 1; i < N-1; i++) \nS-10 { \nS-11 for (j = 1; j < M-1; j++) \nS-12 { \nS-13 A[i][j] = foo(i, j); \nS-14 #pragma omp ordered doacross(source:) \nS-15 \nS-16 B[i][j] = alpha * A[i][j]; \nS-17 \nS-18 #pragma omp ordered doacross(sink: i-1,j) doacross(sink: i,j-1) \nS-19 C[i][j] = 0.2 * (A[i-1][j] + A[i+1][j] + \nS-20 A[i][j-1] + A[i][j+1] + A[i][j]); \nS-21 } \nS-22 } \nS-23 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 355 \nFortran \n1 Example doacross.4.f90 (omp_5.2) \nS-1 subroutine work( N, M, A, B, C ) \nS-2 integer :: N, M \nS-3 real(8), dimension(M, N) :: A, B, C \nS-4 real(8), external :: foo \nS-5 integer :: i, j \nS-6 real(8) :: alpha = 1.2 \nS-7 \nS-8 !$omp do collapse(2) ordered(2) \nS-9 do j=2, N-1 \nS-10 do i=2, M-1 \nS-11 A(i,j) = foo(i, j) \nS-12 !$omp ordered doacross(source:) \nS-13 \nS-14 B(i,j) = alpha * A(i,j) \nS-15 \nS-16 !$omp ordered doacross(sink: j,i-1) doacross(sink: j-1,i) \nS-17 C(i,j) = 0.2 * (A(i-1,j) + A(i+1,j) + & \nS-18 A(i,j-1) + A(i,j+1) + A(i,j)) \nS-19 end do \nS-20 end do \nS-21 end subroutine \nFortran \n356 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.12 Lock Routines", "chunk": "2 This section is about the use of lock routines for synchronization. \n"}
{"section_title": "9.12.1 omp_init_lock Routine", "chunk": "4 The following example demonstrates how to initialize an array of locks in a parallel region by \n5 using omp_init_lock. \nC++ \n6 Example init_lock.1.cpp (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 \nS-3 omp_lock_t *new_locks() { \nS-4 int i; \nS-5 omp_lock_t *lock = new omp_lock_t[1000]; \nS-6 \nS-7 #pragma omp parallel for private(i) \nS-8 for (i=0; i<1000; i++) \nS-9 { omp_init_lock(&lock[i]); } \nS-10 \nS-11 return lock; \nS-12 } \nC++ \nFortran \n7 Example init_lock.1.f (pre_omp_3.0) \nS-1 FUNCTION NEW_LOCKS() \nS-2 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-3 INTEGER(OMP_LOCK_KIND), DIMENSION(1000) :: NEW_LOCKS \nS-4 INTEGER I \nS-5 \nS-6 !$OMP PARALLEL DO PRIVATE(I) \nS-7 DO I=1,1000 \nS-8 CALL OMP_INIT_LOCK(NEW_LOCKS(I)) \nS-9 END DO \nS-10 !$OMP END PARALLEL DO \nS-11 \nS-12 END FUNCTION NEW_LOCKS \nFortran \nCHAPTER 9. SYNCHRONIZATION 357 \n"}
{"section_title": "9.12.2 omp_init_lock_with_hint Routine", "chunk": ""}
{"section_title": "9.12.2 omp_init_lock_with_hint Routine", "chunk": "2 The following example demonstrates how to initialize an array of locks in a parallel region by \n3 using omp_init_lock_with_hint. Note, hints are combined with an | or + operator in \n4 C/C++ and a + operator in Fortran. \nC++ \n5 Example init_lock_with_hint.1.cpp (omp_5.0) \nS-1 #include <omp.h> \nS-2 \nS-3 omp_lock_t *new_locks() \nS-4 { \nS-5 int i; \nS-6 omp_lock_t *lock = new omp_lock_t[1000]; \nS-7 \nS-8 #pragma omp parallel for private(i) \nS-9 for (i=0; i<1000; i++) \nS-10 { \nS-11 omp_init_lock_with_hint(&lock[i], \nS-12 static_cast<omp_lock_hint_t>(omp_sync_hint_contended | \nS-13 omp_sync_hint_speculative)); \nS-14 } \nS-15 return lock; \nS-16 } \nC++ \nFortran \n6 Example init_lock_with_hint.1.f (omp_5.0) \nS-1 FUNCTION NEW_LOCKS() \nS-2 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-3 INTEGER(OMP_LOCK_KIND), DIMENSION(1000) :: NEW_LOCKS \nS-4 \nS-5 INTEGER I \nS-6 \nS-7 !$OMP PARALLEL DO PRIVATE(I) \nS-8 DO I=1,1000 \nS-9 CALL OMP_INIT_LOCK_WITH_HINT(NEW_LOCKS(I), \nS-10 & OMP_SYNC_HINT_CONTENDED + OMP_SYNC_HINT_SPECULATIVE) \nS-11 END DO \nS-12 !$OMP END PARALLEL DO \nS-13 \nS-14 END FUNCTION NEW_LOCKS \nFortran \n358 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.12.3 Ownership of Locks", "chunk": ""}
{"section_title": "9.12.3 Ownership of Locks", "chunk": "2 Ownership of locks has changed since OpenMP 2.5. In OpenMP 2.5, locks are owned by threads; \n3 so a lock released by the omp_unset_lock routine must be owned by the same thread executing \n4 the routine. Beginning with OpenMP 3.0, locks are owned by task regions; so a lock released by the \n5 omp_unset_lock routine in a task region must be owned by the same task region. \n6 This change in ownership requires extra care when using locks. The following program is \n7 conforming in OpenMP 2.5 because the thread that releases the lock lck in the parallel region is \n8 the same thread that acquired the lock in the sequential part of the program (primary thread of \n9 parallel region and the initial thread are the same). However, it is not conforming beginning with \n10 OpenMP 3.0, because the task region that releases the lock lck is different from the task region \n11 that acquires the lock. \nC / C++ \n12 Example lock_owner.1.c (omp_5.1) \nS-1 #include <stdlib.h> \nS-2 #include <stdio.h> \nS-3 #include <omp.h> \nS-4 \nS-5 int main() \nS-6 { \nS-7 int x; \nS-8 omp_lock_t lck; \nS-9 \nS-10 omp_init_lock (&lck); \nS-11 omp_set_lock (&lck); \nS-12 x = 0; \nS-13 \nS-14 #pragma omp parallel shared (x) \nS-15 { \nS-16 #pragma omp masked \nS-17 { \nS-18 x = x + 1; \nS-19 omp_unset_lock (&lck); \nS-20 } \nS-21 \nS-22 /* Some more stuff. */ \nS-23 } \nS-24 omp_destroy_lock (&lck); \nS-25 return 0; \nS-26 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 359 \nFortran \n1 Example lock_owner.1.f (omp_5.1) \nS-1 program lock \nS-2 use omp_lib \nS-3 integer :: x \nS-4 integer (kind=omp_lock_kind) :: lck \nS-5 \nS-6 call omp_init_lock (lck) \nS-7 call omp_set_lock(lck) \nS-8 x = 0 \nS-9 \nS-10 !$omp parallel shared (x) \nS-11 !$omp masked \nS-12 x = x + 1 \nS-13 call omp_unset_lock(lck) \nS-14 !$omp end masked \nS-15 \nS-16 ! Some more stuff. \nS-17 !$omp end parallel \nS-18 \nS-19 call omp_destroy_lock(lck) \nS-20 \nS-21 end \nFortran \n"}
{"section_title": "9.12.4 Simple Lock Routines", "chunk": ""}
{"section_title": "9.12.4 Simple Lock Routines", "chunk": "3 In the following example, the lock routines cause the threads to be idle while waiting for entry to \n4 the first critical section, but to do other work while waiting for entry to the second. The \n5 omp_set_lock function blocks, but the omp_test_lock function does not, allowing the work \n6 in skip to be done. \n7 Note that the argument to the lock routines should have type omp_lock_t (or omp_lock_kind \n8 in Fortran), and that there is no need to flush the lock variable (lck). \n360 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example simple_lock.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 void skip(int i) {} \nS-4 void work(int i) {} \nS-5 int main() \nS-6 { \nS-7 omp_lock_t lck; \nS-8 int id; \nS-9 omp_init_lock(&lck); \nS-10 \nS-11 #pragma omp parallel shared(lck) private(id) \nS-12 { \nS-13 id = omp_get_thread_num(); \nS-14 \nS-15 omp_set_lock(&lck); \nS-16 /* only one thread at a time can execute this printf */ \nS-17 printf(\"My thread id is %d.\\n\", id); \nS-18 omp_unset_lock(&lck); \nS-19 \nS-20 while (! omp_test_lock(&lck)) { \nS-21 skip(id); /* we do not yet have the lock, \nS-22 so we must do something else */ \nS-23 } \nS-24 \nS-25 work(id); /* we now have the lock \nS-26 and can do the work */ \nS-27 \nS-28 omp_unset_lock(&lck); \nS-29 } \nS-30 omp_destroy_lock(&lck); \nS-31 \nS-32 return 0; \nS-33 } \nC / C++ \nCHAPTER 9. SYNCHRONIZATION 361 \nFortran \n1 Example simple_lock.1.f (pre_omp_3.0) \nS-1 SUBROUTINE SKIP(ID) \nS-2 END SUBROUTINE SKIP \nS-3 \nS-4 SUBROUTINE WORK(ID) \nS-5 END SUBROUTINE WORK \nS-6 \nS-7 PROGRAM SIMPLELOCK \nS-8 \nS-9 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-10 \nS-11 INTEGER(OMP_LOCK_KIND) LCK \nS-12 INTEGER ID \nS-13 \nS-14 CALL OMP_INIT_LOCK(LCK) \nS-15 \nS-16 !$OMP PARALLEL SHARED(LCK) PRIVATE(ID) \nS-17 ID = OMP_GET_THREAD_NUM() \nS-18 CALL OMP_SET_LOCK(LCK) \nS-19 PRINT *, \u2019My thread id is \u2019, ID \nS-20 CALL OMP_UNSET_LOCK(LCK) \nS-21 \nS-22 DO WHILE (.NOT. OMP_TEST_LOCK(LCK)) \nS-23 CALL SKIP(ID) ! We do not yet have the lock \nS-24 ! so we must do something else \nS-25 END DO \nS-26 \nS-27 CALL WORK(ID) ! We now have the lock \nS-28 ! and can do the work \nS-29 \nS-30 CALL OMP_UNSET_LOCK( LCK ) \nS-31 \nS-32 !$OMP END PARALLEL \nS-33 \nS-34 CALL OMP_DESTROY_LOCK( LCK ) \nS-35 \nS-36 END PROGRAM SIMPLELOCK \nFortran \n362 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "9.12.5 Nestable Lock Routines", "chunk": ""}
{"section_title": "9.12.5 Nestable Lock Routines", "chunk": "2 The following example demonstrates how a nestable lock can be used to synchronize updates both \n3 to a whole structure and to one of its members. \nC / C++ \n4 Example nestable_lock.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 \nS-3 typedef struct { \nS-4 int a,b; \nS-5 omp_nest_lock_t lck; \nS-6 } pair; \nS-7 \nS-8 int work1(); \nS-9 int work2(); \nS-10 int work3(); \nS-11 \nS-12 void incr_a(pair *p, int a) \nS-13 { \nS-14 \nS-15 /* Called only from incr_pair, no need to lock. */ \nS-16 p->a += a; \nS-17 \nS-18 } \nS-19 \nS-20 void incr_b(pair *p, int b) \nS-21 { \nS-22 \nS-23 /* Called both from incr_pair and elsewhere, */ \nS-24 /* so need a nestable lock. */ \nS-25 \nS-26 omp_set_nest_lock(&p->lck); \nS-27 p->b += b; \nS-28 omp_unset_nest_lock(&p->lck); \nS-29 \nS-30 } \nS-31 \nS-32 void incr_pair(pair *p, int a, int b) \nS-33 { \nS-34 \nS-35 omp_set_nest_lock(&p->lck); \nS-36 incr_a(p, a); \nS-37 incr_b(p, b); \nS-38 omp_unset_nest_lock(&p->lck); \nS-39 \nS-40 } \nCHAPTER 9. SYNCHRONIZATION 363 \nS-41 \nS-42 void nestlock(pair *p) \nS-43 { \nS-44 \nS-45 #pragma omp parallel sections \nS-46 { \nS-47 #pragma omp section \nS-48 incr_pair(p, work1(), work2()); \nS-49 #pragma omp section \nS-50 incr_b(p, work3()); \nS-51 } \nS-52 \nS-53 } \nC / C++ \nFortran \n1 Example nestable_lock.1.f (pre_omp_3.0) \nS-1 MODULE DATA \nS-2 USE OMP_LIB, ONLY: OMP_NEST_LOCK_KIND \nS-3 TYPE LOCKED_PAIR \nS-4 INTEGER A \nS-5 INTEGER B \nS-6 INTEGER (OMP_NEST_LOCK_KIND) LCK \nS-7 END TYPE \nS-8 END MODULE DATA \nS-9 \nS-10 SUBROUTINE INCR_A(P, A) \nS-11 ! called only from INCR_PAIR, no need to lock \nS-12 USE DATA \nS-13 TYPE(LOCKED_PAIR) :: P \nS-14 INTEGER A \nS-15 P%A = P%A + A \nS-16 END SUBROUTINE INCR_A \nS-17 \nS-18 SUBROUTINE INCR_B(P, B) \nS-19 ! called from both INCR_PAIR and elsewhere, \nS-20 ! so we need a nestable lock \nS-21 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-22 USE DATA \nS-23 TYPE(LOCKED_PAIR) :: P \nS-24 INTEGER B \nS-25 CALL OMP_SET_NEST_LOCK(P%LCK) \nS-26 P%B = P%B + B \nS-27 CALL OMP_UNSET_NEST_LOCK(P%LCK) \nS-28 END SUBROUTINE INCR_B \nS-29 \n364 OpenMP Examples Version 5.2.1 - November 2022 \nS-30 SUBROUTINE INCR_PAIR(P, A, B) \nS-31 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-32 USE DATA \nS-33 TYPE(LOCKED_PAIR) :: P \nS-34 INTEGER A \nS-35 INTEGER B \nS-36 \nS-37 CALL OMP_SET_NEST_LOCK(P%LCK) \nS-38 CALL INCR_A(P, A) \nS-39 CALL INCR_B(P, B) \nS-40 CALL OMP_UNSET_NEST_LOCK(P%LCK) \nS-41 END SUBROUTINE INCR_PAIR \nS-42 \nS-43 SUBROUTINE NESTLOCK(P) \nS-44 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-45 USE DATA \nS-46 TYPE(LOCKED_PAIR) :: P \nS-47 INTEGER WORK1, WORK2, WORK3 \nS-48 EXTERNAL WORK1, WORK2, WORK3 \nS-49 \nS-50 !$OMP PARALLEL SECTIONS \nS-51 \nS-52 !$OMP SECTION \nS-53 CALL INCR_PAIR(P, WORK1(), WORK2()) \nS-54 !$OMP SECTION \nS-55 CALL INCR_B(P, WORK3()) \nS-56 !$OMP END PARALLEL SECTIONS \nS-57 \nS-58 END SUBROUTINE NESTLOCK \nFortran \nCHAPTER 9. SYNCHRONIZATION 365 \nThis page intentionally left blank \n"}
{"section_title": "10 Data Environment", "chunk": ""}
{"section_title": "10 Data Environment", "chunk": "2 The OpenMP data environment contains data attributes of variables and objects. Many constructs \n3 (such as parallel, simd, task) accept clauses to control data-sharing attributes of referenced \n4 variables in the construct, where data-sharing applies to whether the attribute of the variable is \n5 shared, is private storage, or has special operational characteristics (as found in the \n6 firstprivate, lastprivate, linear, or reduction clause). \n7 The data environment for a device (distinguished as a device data environment) is controlled on the \n8 host by data-mapping attributes, which determine the relationship of the data on the host, the \n9 original data, and the data on the device, the corresponding data. \n10 DATA-SHARING ATTRIBUTES \n11 Data-sharing attributes of variables can be classified as being predetermined, explicitly determined \n12 or implicitly determined. \n13 Certain variables and objects have predetermined attributes. A commonly found case is the loop \n14 iteration variable in associated loops of a for or do construct. It has a private data-sharing \n15 attribute. Variables with predetermined data-sharing attributes cannot be listed in a data-sharing \n16 clause; but there are some exceptions (mainly concerning loop iteration variables). \n17 Variables with explicitly determined data-sharing attributes are those that are referenced in a given \n18 construct and are listed in a data-sharing attribute clause on the construct. Some of the common \n19 data-sharing clauses are: shared, private, firstprivate, lastprivate, linear, and \n20 reduction. \n21 Variables with implicitly determined data-sharing attributes are those that are referenced in a given \n22 construct, do not have predetermined data-sharing attributes, and are not listed in a data-sharing \n23 attribute clause of an enclosing construct. For a complete list of variables and objects with \n24 predetermined and implicitly determined attributes, please refer to the Data-sharing Attribute Rules \n25 for Variables Referenced in a Construct subsection of the OpenMP Specifications document. \n26 DATA-MAPPING ATTRIBUTES \n27 The map clause on a device construct explicitly specifies how the list items in the clause are \n28 mapped from the encountering task\u2019s data environment (on the host) to the corresponding item in \n29 the device data environment (on the device). The common list items are arrays, array sections, \n30 scalars, pointers, and structure elements (members). \n31 Procedures and global variables have predetermined data mapping if they appear within the list or \n32 block of a declare target directive. Also, a C/C++ pointer is mapped as a zero-length array \n33 section, as is a C++ variable that is a reference to a pointer. \n367 \n1 Without explicit mapping, non-scalar and non-pointer variables within the scope of the target \n2 construct are implicitly mapped with a map-type of tofrom. Without explicit mapping, scalar \n3 variables within the scope of the target construct are not mapped, but have an implicit firstprivate \n4 data-sharing attribute. (That is, the value of the original variable is given to a private variable of the \n5 same name on the device.) This behavior can be changed with the defaultmap clause. \n6 The map clause can appear on target, target data and target enter/exit data \n7 constructs. The operations of creation and removal of device storage as well as assignment of the \n8 original list item values to the corresponding list items may be complicated when the list item \n9 appears on multiple constructs or when the host and device storage is shared. In these cases the \n10 item\u2019s reference count, the number of times it has been referenced (+1 on entry and -1 on exited) in \n11 nested (structured) map regions and/or accumulative (unstructured) mappings, determines the \n12 operation. Details of the map clause and reference count operation are specified in the map Clause \n13 subsection of the OpenMP Specifications document. \n368 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.1 threadprivate Directive", "chunk": ""}
{"section_title": "10.1 threadprivate Directive", "chunk": "2 The following examples demonstrate how to use the threadprivate directive to give each \n3 thread a separate counter. \nC / C++ \n4 Example threadprivate.1.c (pre_omp_3.0) \nS-1 int counter = 0; \nS-2 #pragma omp threadprivate(counter) \nS-3 \nS-4 int increment_counter() \nS-5 { \nS-6 counter++; \nS-7 return(counter); \nS-8 } \nC / C++ \nFortran \n5 Example threadprivate.1.f (pre_omp_3.0) \nS-1 INTEGER FUNCTION INCREMENT_COUNTER() \nS-2 COMMON/INC_COMMON/COUNTER \nS-3 !$OMP THREADPRIVATE(/INC_COMMON/) \nS-4 \nS-5 COUNTER = COUNTER +1 \nS-6 INCREMENT_COUNTER = COUNTER \nS-7 RETURN \nS-8 END FUNCTION INCREMENT_COUNTER \nFortran \nC / C++ \n6 The following example uses threadprivate on a static variable: \n7 Example threadprivate.2.c (pre_omp_3.0) \nS-1 int increment_counter_2() \nS-2 { \nS-3 static int counter = 0; \nS-4 #pragma omp threadprivate(counter) \nS-5 counter++; \nS-6 return(counter); \nS-7 } \nCHAPTER 10. DATA ENVIRONMENT 369 \n1 The following example demonstrates unspecified behavior for the initialization of a \n2 threadprivate variable. A threadprivate variable is initialized once at an unspecified \n3 point before its first reference. Because a is constructed using the value of x (which is modified by \n4 the statement x++), the value of a.val at the start of the parallel region could be either 1 or \n5 2. This problem is avoided for b, which uses an auxiliary const variable and a copy-constructor. \n6 Example threadprivate.3.cpp (pre_omp_3.0) \nS-1 class T { \nS-2 public: \nS-3 int val; \nS-4 T (int); \nS-5 T (const T&); \nS-6 }; \nS-7 \nS-8 T :: T (int v){ \nS-9 val = v; \nS-10 } \nS-11 \nS-12 T :: T (const T& t) { \nS-13 val = t.val; \nS-14 } \nS-15 \nS-16 void g(T a, T b){ \nS-17 a.val += b.val; \nS-18 } \nS-19 \nS-20 int x = 1; \nS-21 T a(x); \nS-22 const T b_aux(x); /* Capture value of x = 1 */ \nS-23 T b(b_aux); \nS-24 #pragma omp threadprivate(a, b) \nS-25 \nS-26 void f(int n) { \nS-27 x++; \nS-28 #pragma omp parallel for \nS-29 /* In each thread: \nS-30 * a is constructed from x (with value 1 or 2?) \nS-31 * b is copy-constructed from b_aux \nS-32 */ \nS-33 \nS-34 for (int i=0; i<n; i++) { \nS-35 g(a, b); /* Value of a is unspecified. */ \nS-36 } \nS-37 } \nC / C++ \n370 OpenMP Examples Version 5.2.1 - November 2022 \n1 The following examples show non-conforming uses and correct uses of the threadprivate \n2 directive. \nFortran \n3 The following example is non-conforming because the common block is not declared local to the \n4 subroutine that refers to it: \n5 Example threadprivate.2.f (pre_omp_3.0) \nS-1 MODULE INC_MODULE \nS-2 COMMON /T/ A \nS-3 END MODULE INC_MODULE \nS-4 \nS-5 SUBROUTINE INC_MODULE_WRONG() \nS-6 USE INC_MODULE \nS-7 !$OMP THREADPRIVATE(/T/) \nS-8 !non-conforming because /T/ not declared in INC_MODULE_WRONG \nS-9 END SUBROUTINE INC_MODULE_WRONG \n6 The following example is also non-conforming because the common block is not declared local to \n7 the subroutine that refers to it: \n8 Example threadprivate.3.f (pre_omp_3.0) \nS-1 SUBROUTINE INC_WRONG() \nS-2 COMMON /T/ A \nS-3 !$OMP THREADPRIVATE(/T/) \nS-4 \nS-5 CONTAINS \nS-6 SUBROUTINE INC_WRONG_SUB() \nS-7 !$OMP PARALLEL COPYIN(/T/) \nS-8 !non-conforming because /T/ not declared in INC_WRONG_SUB \nS-9 !$OMP END PARALLEL \nS-10 END SUBROUTINE INC_WRONG_SUB \nS-11 END SUBROUTINE INC_WRONG \n9 The following example is a correct rewrite of the previous example: \n10 Example threadprivate.4.f (pre_omp_3.0) \nS-1 SUBROUTINE INC_GOOD() \nS-2 COMMON /T/ A \nS-3 !$OMP THREADPRIVATE(/T/) \nS-4 \nS-5 CONTAINS \nS-6 SUBROUTINE INC_GOOD_SUB() \nS-7 COMMON /T/ A \nS-8 !$OMP THREADPRIVATE(/T/) \nS-9 \nS-10 !$OMP PARALLEL COPYIN(/T/) \nCHAPTER 10. DATA ENVIRONMENT 371 \nFortran (cont.) \nS-11 !$OMP END PARALLEL \nS-12 END SUBROUTINE INC_GOOD_SUB \nS-13 END SUBROUTINE INC_GOOD \n1 The following is an example of the use of threadprivate for local variables: \n2 Example threadprivate.5.f (pre_omp_3.0) \nS-1 PROGRAM INC_GOOD2 \nS-2 INTEGER, ALLOCATABLE, SAVE :: A(:) \nS-3 INTEGER, POINTER, SAVE :: PTR \nS-4 INTEGER, SAVE :: I \nS-5 INTEGER, TARGET :: TARG \nS-6 LOGICAL :: FIRSTIN = .TRUE. \nS-7 !$OMP THREADPRIVATE(A, I, PTR) \nS-8 \nS-9 ALLOCATE (A(3)) \nS-10 A = (/1,2,3/) \nS-11 PTR => TARG \nS-12 I = 5 \nS-13 \nS-14 !$OMP PARALLEL COPYIN(I, PTR) \nS-15 !$OMP CRITICAL \nS-16 IF (FIRSTIN) THEN \nS-17 TARG = 4 ! Update target of ptr \nS-18 I = I + 10 \nS-19 IF (ALLOCATED(A)) A = A + 10 \nS-20 FIRSTIN = .FALSE. \nS-21 END IF \nS-22 \nS-23 IF (ALLOCATED(A)) THEN \nS-24 PRINT *, \u2019a = \u2019, A \nS-25 ELSE \nS-26 PRINT *, \u2019A is not allocated\u2019 \nS-27 END IF \nS-28 \nS-29 PRINT *, \u2019ptr = \u2019, PTR \nS-30 PRINT *, \u2019i = \u2019, I \nS-31 PRINT * \nS-32 \nS-33 !$OMP END CRITICAL \nS-34 !$OMP END PARALLEL \nS-35 END PROGRAM INC_GOOD2 \n3 The above program, if executed by two threads, will print one of the following two sets of output: \n372 OpenMP Examples Version 5.2.1 - November 2022 \nFortran (cont.) \n1 a = 11 12 13 \n2 ptr = 4 \n3 i = 15 \n4 A is not allocated \n5 ptr = 4 \n6 i = 5 \n7 or \n8 A is not allocated \n9 ptr = 4 \n10 i = 15 \n11 a = 1 2 3 \n12 ptr = 4 \n13 i = 5 \n14 The following is an example of the use of threadprivate for module variables: \n15 Example threadprivate.6.f (pre_omp_3.0) \nS-1 MODULE INC_MODULE_GOOD3 \nS-2 REAL, POINTER :: WORK(:) \nS-3 SAVE WORK \nS-4 !$OMP THREADPRIVATE(WORK) \nS-5 END MODULE INC_MODULE_GOOD3 \nS-6 \nS-7 SUBROUTINE SUB1(N) \nS-8 USE INC_MODULE_GOOD3 \nS-9 !$OMP PARALLEL PRIVATE(THE_SUM) \nS-10 ALLOCATE(WORK(N)) \nS-11 CALL SUB2(THE_SUM) \nS-12 WRITE(*,*)THE_SUM \nS-13 !$OMP END PARALLEL \nS-14 END SUBROUTINE SUB1 \nS-15 \nS-16 SUBROUTINE SUB2(THE_SUM) \nS-17 USE INC_MODULE_GOOD3 \nS-18 WORK(:) = 10 \nS-19 THE_SUM=SUM(WORK) \nS-20 END SUBROUTINE SUB2 \nS-21 \nS-22 PROGRAM INC_GOOD3 \nS-23 N = 10 \nS-24 CALL SUB1(N) \nCHAPTER 10. DATA ENVIRONMENT 373 \nS-25 END PROGRAM INC_GOOD3 \nFortran \nC++ \n1 The following example illustrates initialization of threadprivate variables for class-type T. t1 \n2 is default constructed, t2 is constructed taking a constructor accepting one argument of integer \n3 type, t3 is copy constructed with argument f(): \n4 Example threadprivate.4.cpp (pre_omp_3.0) \nS-1 struct T { T (); T (int); ~T (); int t; }; \nS-2 int f(); \nS-3 static T t1; \nS-4 #pragma omp threadprivate(t1) \nS-5 static T t2( 23 ); \nS-6 #pragma omp threadprivate(t2) \nS-7 static T t3 = f(); \nS-8 #pragma omp threadprivate(t3) \n5 The following example illustrates the use of threadprivate for static class members. The \n6 threadprivate directive for a static class member must be placed inside the class definition. \n7 Example threadprivate.5.cpp (pre_omp_3.0) \nS-1 class T { \nS-2 public: \nS-3 static int i; \nS-4 #pragma omp threadprivate(i) \nS-5 }; \nC++ \n374 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.2 default(none) Clause", "chunk": ""}
{"section_title": "10.2 default(none) Clause", "chunk": "2 The following example distinguishes the variables that are affected by the default(none) \n3 clause from those that are not. \nC / C++ \n4 Beginning with OpenMP 4.0, variables with const-qualified type and no mutable member are no \n5 longer predetermined shared. Thus, these variables (variable c in the example) need to be explicitly \n6 listed in data-sharing attribute clauses when the default(none) clause is specified. \n7 Example default_none.1.c (pre_omp_3.0) \nS-1 #include <omp.h> \nS-2 int x, y, z[1000]; \nS-3 #pragma omp threadprivate(x) \nS-4 \nS-5 void default_none(int a) { \nS-6 const int c = 1; \nS-7 int i = 0; \nS-8 \nS-9 #pragma omp parallel default(none) private(a) shared(z, c) \nS-10 { \nS-11 int j = omp_get_num_threads(); \nS-12 /* O.K. - j is declared within parallel region */ \nS-13 a = z[j]; /* O.K. - a is listed in private clause */ \nS-14 /* - z is listed in shared clause */ \nS-15 x = c; /* O.K. - x is threadprivate */ \nS-16 /* - c has const-qualified type and \nS-17 is listed in shared clause */ \nS-18 z[i] = y; /* Error - cannot reference i or y here */ \nS-19 \nS-20 #pragma omp for firstprivate(y) \nS-21 /* Error - Cannot reference y in the firstprivate clause */ \nS-22 for (i=0; i<10 ; i++) { \nS-23 z[i] = i; /* O.K. - i is the loop iteration variable */ \nS-24 } \nS-25 \nS-26 z[i] = y; /* Error - cannot reference i or y here */ \nS-27 } \nS-28 } \nC / C++ \nCHAPTER 10. DATA ENVIRONMENT 375 \nFortran \n1 Example default_none.1.f (pre_omp_3.0) \nS-1 SUBROUTINE DEFAULT_NONE(A) \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 \nS-4 INTEGER A \nS-5 \nS-6 INTEGER X, Y, Z(1000) \nS-7 COMMON/BLOCKX/X \nS-8 COMMON/BLOCKY/Y \nS-9 COMMON/BLOCKZ/Z \nS-10 !$OMP THREADPRIVATE(/BLOCKX/) \nS-11 \nS-12 INTEGER I, J \nS-13 i = 1 \nS-14 \nS-15 !$OMP PARALLEL DEFAULT(NONE) PRIVATE(A) SHARED(Z) PRIVATE(J) \nS-16 J = OMP_GET_NUM_THREADS(); \nS-17 ! O.K. - J is listed in PRIVATE clause \nS-18 A = Z(J) ! O.K. - A is listed in PRIVATE clause \nS-19 ! - Z is listed in SHARED clause \nS-20 X = 1 ! O.K. - X is THREADPRIVATE \nS-21 Z(I) = Y ! Error - cannot reference I or Y here \nS-22 \nS-23 !$OMP DO firstprivate(y) \nS-24 ! Error - Cannot reference y in the firstprivate clause \nS-25 DO I = 1,10 \nS-26 Z(I) = I ! O.K. - I is the loop iteration variable \nS-27 END DO \nS-28 \nS-29 \nS-30 Z(I) = Y ! Error - cannot reference I or Y here \nS-31 !$OMP END PARALLEL \nS-32 END SUBROUTINE DEFAULT_NONE \nFortran \n376 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.3 private Clause", "chunk": ""}
{"section_title": "10.3 private Clause", "chunk": "2 In the following example, the values of original list items i and j are retained on exit from the \n3 parallel region, while the private list items i and j are modified within the parallel \n4 construct. \nC / C++ \n5 Example private.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 #include <assert.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 int i, j; \nS-7 int *ptr_i, *ptr_j; \nS-8 \nS-9 i = 1; \nS-10 j = 2; \nS-11 \nS-12 ptr_i = &i; \nS-13 ptr_j = &j; \nS-14 \nS-15 #pragma omp parallel private(i) firstprivate(j) \nS-16 { \nS-17 i = 3; \nS-18 j = j + 2; \nS-19 assert (*ptr_i == 1 && *ptr_j == 2); \nS-20 } \nS-21 \nS-22 assert(i == 1 && j == 2); \nS-23 \nS-24 return 0; \nS-25 } \nC / C++ \nCHAPTER 10. DATA ENVIRONMENT 377 \nFortran \n1 Example private.1.f (pre_omp_3.0) \nS-1 PROGRAM PRIV_EXAMPLE \nS-2 INTEGER I, J \nS-3 \nS-4 I = 1 \nS-5 J = 2 \nS-6 \nS-7 !$OMP PARALLEL PRIVATE(I) FIRSTPRIVATE(J) \nS-8 I = 3 \nS-9 J = J + 2 \nS-10 !$OMP END PARALLEL \nS-11 \nS-12 PRINT *, I, J ! I .eq. 1 .and. J .eq. 2 \nS-13 END PROGRAM PRIV_EXAMPLE \nFortran \n2 In the following example, all uses of the variable a within the loop construct in the routine f refer to \n3 a private list item a, while it is unspecified whether references to a in the routine g are to a private \n4 list item or the original list item. \nC / C++ \n5 Example private.2.c (pre_omp_3.0) \nS-1 int a; \nS-2 \nS-3 void g(int k) { \nS-4 a = k; /* Accessed in the region but outside of the construct; \nS-5 * therefore unspecified whether original or private list \nS-6 * item is modified. */ \nS-7 } \nS-8 \nS-9 \nS-10 void f(int n) { \nS-11 int a = 0; \nS-12 \nS-13 #pragma omp parallel for private(a) \nS-14 for (int i=1; i<n; i++) { \nS-15 a = i; \nS-16 g(a*2); /* Private copy of \"a\" */ \nS-17 } \nS-18 } \nC / C++ \n378 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example private.2.f (pre_omp_3.0) \nS-1 MODULE PRIV_EXAMPLE2 \nS-2 REAL A \nS-3 \nS-4 CONTAINS \nS-5 \nS-6 SUBROUTINE G(K) \nS-7 REAL K \nS-8 A = K ! Accessed in the region but outside of the \nS-9 ! construct; therefore unspecified whether \nS-10 ! original or private list item is modified. \nS-11 END SUBROUTINE G \nS-12 \nS-13 SUBROUTINE F(N) \nS-14 INTEGER N \nS-15 REAL A \nS-16 \nS-17 INTEGER I \nS-18 !$OMP PARALLEL DO PRIVATE(A) \nS-19 DO I = 1,N \nS-20 A = I \nS-21 CALL G(A*2) \nS-22 ENDDO \nS-23 !$OMP END PARALLEL DO \nS-24 END SUBROUTINE F \nS-25 \nS-26 END MODULE PRIV_EXAMPLE2 \nFortran \n2 The following example demonstrates that a list item that appears in a private clause in a \n3 parallel construct may also appear in a private clause in an enclosed worksharing construct, \n4 which results in an additional private copy. \nC / C++ \n5 Example private.3.c (pre_omp_3.0) \nS-1 #include <assert.h> \nS-2 void priv_example3() \nS-3 { \nS-4 int i, a; \nS-5 \nS-6 #pragma omp parallel private(a) \nS-7 { \nS-8 a = 1; \nS-9 #pragma omp parallel for private(a) \nCHAPTER 10. DATA ENVIRONMENT 379 \nS-10 for (i=0; i<10; i++) \nS-11 { \nS-12 a = 2; \nS-13 } \nS-14 assert(a == 1); \nS-15 } \nS-16 } \nC / C++ \nFortran \n1 Example private.3.f (pre_omp_3.0) \nS-1 SUBROUTINE PRIV_EXAMPLE3() \nS-2 INTEGER I, A \nS-3 \nS-4 !$OMP PARALLEL PRIVATE(A) \nS-5 A = 1 \nS-6 !$OMP PARALLEL DO PRIVATE(A) \nS-7 DO I = 1, 10 \nS-8 A = 2 \nS-9 END DO \nS-10 !$OMP END PARALLEL DO \nS-11 PRINT *, A ! Outer A still has value 1 \nS-12 !$OMP END PARALLEL \nS-13 END SUBROUTINE PRIV_EXAMPLE3 \nFortran \n380 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.4 Fortran Private Loop Iteration Variables", "chunk": ""}
{"section_title": "10.4 Fortran Private Loop Iteration Variables", "chunk": "Fortran \n2 In general loop iteration variables will be private, when used in the do-loop of a do and \n3 parallel do construct or in sequential loops in a parallel construct (see Section 2.7.1 and \n4 Section 2.14.1 of the OpenMP 4.0 specification). In the following example of a sequential loop in a \n5 parallel construct the loop iteration variable I will be private. \n6 Example fort_loopvar.1.f90 (pre_omp_3.0) \nS-1 SUBROUTINE PLOOP_1(A,N) \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 \nS-4 REAL A(*) \nS-5 INTEGER I, MYOFFSET, N \nS-6 \nS-7 !$OMP PARALLEL PRIVATE(MYOFFSET) \nS-8 MYOFFSET = OMP_GET_THREAD_NUM()*N \nS-9 DO I = 1, N \nS-10 A(MYOFFSET+I) = FLOAT(I) \nS-11 ENDDO \nS-12 !$OMP END PARALLEL \nS-13 \nS-14 END SUBROUTINE PLOOP_1 \n7 In exceptional cases, loop iteration variables can be made shared, as in the following example: \n8 Example fort_loopvar.2.f90 (pre_omp_3.0) \nS-1 SUBROUTINE PLOOP_2(A,B,N,I1,I2) \nS-2 REAL A(*), B(*) \nS-3 INTEGER I1, I2, N \nS-4 \nS-5 !$OMP PARALLEL SHARED(A,B,I1,I2) \nS-6 !$OMP SECTIONS \nS-7 !$OMP SECTION \nS-8 DO I1 = I1, N \nS-9 IF (A(I1).NE.0.0) EXIT \nS-10 ENDDO \nS-11 !$OMP SECTION \nS-12 DO I2 = I2, N \nS-13 IF (B(I2).NE.0.0) EXIT \nS-14 ENDDO \nS-15 !$OMP END SECTIONS \nS-16 !$OMP SINGLE \nS-17 IF (I1.LE.N) PRINT *, \u2019ITEMS IN A UP TO \u2019, I1, \u2019ARE ALL ZERO.\u2019 \nS-18 IF (I2.LE.N) PRINT *, \u2019ITEMS IN B UP TO \u2019, I2, \u2019ARE ALL ZERO.\u2019 \nS-19 !$OMP END SINGLE \nCHAPTER 10. DATA ENVIRONMENT 381 \nS-20 !$OMP END PARALLEL \nS-21 \nS-22 END SUBROUTINE PLOOP_2 \n1 Note however that the use of shared loop iteration variables can easily lead to race conditions. \nFortran \n382 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.5 Fortran Restrictions on shared and private Clauses with Common Blocks", "chunk": ""}
{"section_title": "10.5 Fortran Restrictions on shared and private Clauses with Common Blocks", "chunk": "2 private Clauses with Common Blocks \nFortran \n3 When a named common block is specified in a private, firstprivate, or lastprivate \n4 clause of a construct, none of its members may be declared in another data-sharing attribute clause \n5 on that construct. The following examples illustrate this point. \n6 The following example is conforming: \n7 Example fort_sp_common.1.f (pre_omp_3.0) \nS-1 SUBROUTINE COMMON_GOOD() \nS-2 COMMON /C/ X,Y \nS-3 REAL X, Y \nS-4 \nS-5 !$OMP PARALLEL PRIVATE (/C/) \nS-6 ! do work here \nS-7 !$OMP END PARALLEL \nS-8 !$OMP PARALLEL SHARED (X,Y) \nS-9 ! do work here \nS-10 !$OMP END PARALLEL \nS-11 END SUBROUTINE COMMON_GOOD \n8 The following example is also conforming: \n9 Example fort_sp_common.2.f (pre_omp_3.0) \nS-1 SUBROUTINE COMMON_GOOD2() \nS-2 COMMON /C/ X,Y \nS-3 REAL X, Y \nS-4 INTEGER I \nS-5 !$OMP PARALLEL \nS-6 !$OMP DO PRIVATE(/C/) \nS-7 DO I=1,1000 \nS-8 ! do work here \nS-9 ENDDO \nS-10 !$OMP END DO \nS-11 !$OMP DO PRIVATE(X) \nS-12 DO I=1,1000 \nS-13 ! do work here \nS-14 ENDDO \nS-15 !$OMP END DO \nS-16 !$OMP END PARALLEL \nS-17 END SUBROUTINE COMMON_GOOD2 \nCHAPTER 10. DATA ENVIRONMENT 383 \n1 The following example is conforming: \n2 Example fort_sp_common.3.f (pre_omp_3.0) \nS-1 SUBROUTINE COMMON_GOOD3() \nS-2 COMMON /C/ X,Y \nS-3 !$OMP PARALLEL PRIVATE (/C/) \nS-4 ! do work here \nS-5 !$OMP END PARALLEL \nS-6 !$OMP PARALLEL SHARED (/C/) \nS-7 ! do work here \nS-8 !$OMP END PARALLEL \nS-9 END SUBROUTINE COMMON_GOOD3 \n3 The following example is non-conforming because x is a constituent element of c: \n4 Example fort_sp_common.4.f (pre_omp_3.0) \nS-1 SUBROUTINE COMMON_WRONG() \nS-2 COMMON /C/ X,Y \nS-3 ! Incorrect because X is a constituent element of C \nS-4 !$OMP PARALLEL PRIVATE(/C/), SHARED(X) \nS-5 ! do work here \nS-6 !$OMP END PARALLEL \nS-7 END SUBROUTINE COMMON_WRONG \n5 The following example is non-conforming because a common block may not be declared both \n6 shared and private: \n7 Example fort_sp_common.5.f (pre_omp_3.0) \nS-1 SUBROUTINE COMMON_WRONG2() \nS-2 COMMON /C/ X,Y \nS-3 ! Incorrect: common block C cannot be declared both \nS-4 ! shared and private \nS-5 !$OMP PARALLEL PRIVATE (/C/), SHARED(/C/) \nS-6 ! do work here \nS-7 !$OMP END PARALLEL \nS-8 \nS-9 END SUBROUTINE COMMON_WRONG2 \nFortran \n384 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.6 Fortran Restrictions on Storage Association with the private Clause", "chunk": ""}
{"section_title": "10.6 Fortran Restrictions on Storage Association with the private Clause", "chunk": "2 with the private Clause \nFortran \n3 The following non-conforming examples illustrate the implications of the private clause rules \n4 with regard to storage association. \n5 Example fort_sa_private.1.f (pre_omp_3.0) \nS-1 SUBROUTINE SUB() \nS-2 COMMON /BLOCK/ X \nS-3 PRINT *,X ! X is undefined \nS-4 END SUBROUTINE SUB \nS-5 \nS-6 PROGRAM PRIV_RESTRICT \nS-7 COMMON /BLOCK/ X \nS-8 X = 1.0 \nS-9 !$OMP PARALLEL PRIVATE (X) \nS-10 X = 2.0 \nS-11 CALL SUB() \nS-12 !$OMP END PARALLEL \nS-13 END PROGRAM PRIV_RESTRICT \n6 Example fort_sa_private.2.f (pre_omp_3.0) \nS-1 PROGRAM PRIV_RESTRICT2 \nS-2 COMMON /BLOCK2/ X \nS-3 X = 1.0 \nS-4 \nS-5 !$OMP PARALLEL PRIVATE (X) \nS-6 X = 2.0 \nS-7 CALL SUB() \nS-8 !$OMP END PARALLEL \nS-9 \nS-10 CONTAINS \nS-11 \nS-12 SUBROUTINE SUB() \nS-13 COMMON /BLOCK2/ Y \nS-14 \nS-15 PRINT *,X ! X is undefined \nS-16 PRINT *,Y ! Y is undefined \nS-17 END SUBROUTINE SUB \nS-18 \nS-19 END PROGRAM PRIV_RESTRICT2 \nCHAPTER 10. DATA ENVIRONMENT 385 \nFortran (cont.) \n1 Example fort_sa_private.3.f (pre_omp_3.0) \nS-1 PROGRAM PRIV_RESTRICT3 \nS-2 EQUIVALENCE (X,Y) \nS-3 X = 1.0 \nS-4 \nS-5 !$OMP PARALLEL PRIVATE(X) \nS-6 PRINT *,Y ! Y is undefined \nS-7 Y = 10 \nS-8 PRINT *,X ! X is undefined \nS-9 !$OMP END PARALLEL \nS-10 END PROGRAM PRIV_RESTRICT3 \n2 Example fort_sa_private.4.f (pre_omp_3.0) \nS-1 PROGRAM PRIV_RESTRICT4 \nS-2 INTEGER I, J \nS-3 INTEGER A(100), B(100) \nS-4 EQUIVALENCE (A(51), B(1)) \nS-5 \nS-6 !$OMP PARALLEL DO DEFAULT(PRIVATE) PRIVATE(I,J) LASTPRIVATE(A) \nS-7 DO I=1,100 \nS-8 DO J=1,100 \nS-9 B(J) = J - 1 \nS-10 ENDDO \nS-11 \nS-12 DO J=1,100 \nS-13 A(J) = J ! B becomes undefined at this point \nS-14 ENDDO \nS-15 \nS-16 DO J=1,50 \nS-17 B(J) = B(J) + 1 ! B is undefined \nS-18 ! A becomes undefined at this point \nS-19 ENDDO \nS-20 ENDDO \nS-21 !$OMP END PARALLEL DO ! The LASTPRIVATE write for A has \nS-22 ! undefined results \nS-23 \nS-24 PRINT *, B ! B is undefined since the LASTPRIVATE \nS-25 ! write of A was not defined \nS-26 END PROGRAM PRIV_RESTRICT4 \n3 Example fort_sa_private.5.f (omp_5.1) \nS-1 SUBROUTINE SUB1(X) \nS-2 DIMENSION X(*) \n386 OpenMP Examples Version 5.2.1 - November 2022 \nS-3 \nS-4 ! This use of X does not conform to the \nS-5 ! specification. It would be legal Fortran 90, \nS-6 ! but the OpenMP private directive allows the \nS-7 ! compiler to break the sequence association that \nS-8 ! A had with the rest of the common block. \nS-9 \nS-10 FORALL (I = 1:10) X(I) = I \nS-11 END SUBROUTINE SUB1 \nS-12 \nS-13 PROGRAM PRIV_RESTRICT5 \nS-14 COMMON /BLOCK5/ A \nS-15 \nS-16 DIMENSION A(1),B(10) \nS-17 EQUIVALENCE (A,B(1)) \nS-18 \nS-19 ! the common block has to be at least 10 words \nS-20 A = 0 \nS-21 \nS-22 !$OMP PARALLEL PRIVATE(/BLOCK5/) \nS-23 \nS-24 ! Without the private clause, \nS-25 ! we would be passing a member of a sequence \nS-26 ! that is at least ten elements long. \nS-27 ! With the private clause, A may no longer be \nS-28 ! sequence-associated. \nS-29 \nS-30 CALL SUB1(A) \nS-31 !$OMP MASKED \nS-32 PRINT *, A \nS-33 !$OMP END MASKED \nS-34 \nS-35 !$OMP END PARALLEL \nS-36 END PROGRAM PRIV_RESTRICT5 \nFortran \nCHAPTER 10. DATA ENVIRONMENT 387 \n"}
{"section_title": "10.7 C/C++ Arrays in a firstprivate Clause", "chunk": ""}
{"section_title": "10.7 C/C++ Arrays in a firstprivate Clause", "chunk": "2 Clause \nC / C++ \n3 The following example illustrates the size and value of list items of array or pointer type in a \n4 firstprivate clause . The size of new list items is based on the type of the corresponding \n5 original list item, as determined by the base language. \n6 In this example: \n7 \u2022 The type of A is array of two arrays of two ints. \n8 \u2022 The type of B is adjusted to pointer to array of n ints, because it is a function parameter. \n9 \u2022 The type of C is adjusted to pointer to int, because it is a function parameter. \n10 \u2022 The type of D is array of two arrays of two ints. \n11 \u2022 The type of E is array of n arrays of n ints. \n12 Note that B and E involve variable length array types. \n13 The new items of array type are initialized as if each integer element of the original array is \n14 assigned to the corresponding element of the new array. Those of pointer type are initialized as if \n15 by assignment from the original item to the new item. \n16 Example carrays_fpriv.1.c (pre_omp_3.0) \nS-1 #include <assert.h> \nS-2 \nS-3 int A[2][2] = {1, 2, 3, 4}; \nS-4 \nS-5 void f(int n, int B[n][n], int C[]) \nS-6 { \nS-7 int D[2][2] = {1, 2, 3, 4}; \nS-8 int E[n][n]; \nS-9 \nS-10 assert(n >= 2); \nS-11 E[1][1] = 4; \nS-12 \nS-13 #pragma omp parallel firstprivate(B, C, D, E) \nS-14 { \nS-15 assert(sizeof(B) == sizeof(int (*)[n])); \nS-16 assert(sizeof(C) == sizeof(int*)); \nS-17 assert(sizeof(D) == 4 * sizeof(int)); \nS-18 assert(sizeof(E) == n * n * sizeof(int)); \nS-19 \nS-20 /* Private B and C have values of original B and C. */ \nS-21 assert(&B[1][1] == &A[1][1]); \nS-22 assert(&C[3] == &A[1][1]); \nS-23 assert(D[1][1] == 4); \n388 OpenMP Examples Version 5.2.1 - November 2022 \nS-24 assert(E[1][1] == 4); \nS-25 } \nS-26 } \nS-27 \nS-28 int main() { \nS-29 f(2, A, A[0]); \nS-30 return 0; \nS-31 } \nC / C++ \nCHAPTER 10. DATA ENVIRONMENT 389 \n"}
{"section_title": "10.8 lastprivate Clause", "chunk": ""}
{"section_title": "10.8 lastprivate Clause", "chunk": "2 Correct execution sometimes depends on the value that the last iteration of a loop assigns to a \n3 variable. Such programs must list all such variables in a lastprivate clause so that the values \n4 of the variables are the same as when the loop is executed sequentially. \nC / C++ \n5 Example lastprivate.1.c (pre_omp_3.0) \nS-1 void lastpriv (int n, float *a, float *b) \nS-2 { \nS-3 int i; \nS-4 \nS-5 #pragma omp parallel \nS-6 { \nS-7 #pragma omp for lastprivate(i) \nS-8 for (i=0; i<n-1; i++) \nS-9 a[i] = b[i] + b[i+1]; \nS-10 } \nS-11 \nS-12 a[i]=b[i]; /* i == n-1 here */ \nS-13 } \nC / C++ \nFortran \n6 Example lastprivate.1.f (pre_omp_3.0) \nS-1 SUBROUTINE LASTPRIV(N, A, B) \nS-2 \nS-3 INTEGER N \nS-4 REAL A(*), B(*) \nS-5 INTEGER I \nS-6 !$OMP PARALLEL \nS-7 !$OMP DO LASTPRIVATE(I) \nS-8 \nS-9 DO I=1,N-1 \nS-10 A(I) = B(I) + B(I+1) \nS-11 ENDDO \nS-12 \nS-13 !$OMP END PARALLEL \nS-14 A(I) = B(I) ! I has the value of N here \nS-15 \nS-16 END SUBROUTINE LASTPRIV \nFortran \n390 OpenMP Examples Version 5.2.1 - November 2022 \n1 The next example illustrates the use of the conditional modifier in a lastprivate clause to \n2 return the last value when it may not come from the last iteration of a loop. That is, users can \n3 preserve the serial equivalence semantics of the loop. The conditional lastprivate ensures the final \n4 value of the variable after the loop is as if the loop iterations were executed in a sequential order. \nC / C++ \n5 Example lastprivate.2.c (omp_5.0) \nS-1 #include <math.h> \nS-2 \nS-3 float condlastprivate(float *a, int n) \nS-4 { \nS-5 float x = 0.0f; \nS-6 \nS-7 #pragma omp parallel for simd lastprivate(conditional: x) \nS-8 for (int k = 0; k < n; k++) { \nS-9 if (a[k] < 108.5 || a[k] > 208.5) { \nS-10 x = sinf(a[k]); \nS-11 } \nS-12 } \nS-13 \nS-14 return x; \nS-15 } \nC / C++ \nFortran \n6 Example lastprivate.2.f90 (omp_5.0) \nS-1 function condlastprivate(a, n) result(x) \nS-2 implicit none \nS-3 real a(*), x \nS-4 integer n, k \nS-5 \nS-6 x = 0.0 \nS-7 \nS-8 !$omp parallel do simd lastprivate(conditional: x) \nS-9 do k = 1, n \nS-10 if (a(k) < 108.5 .or. a(k) > 208.5) then \nS-11 x = sin(a(k)) \nS-12 endif \nS-13 end do \nS-14 \nS-15 end function condlastprivate \nFortran \nCHAPTER 10. DATA ENVIRONMENT 391 \n"}
{"section_title": "10.9 Reduction", "chunk": "2 This section covers ways to perform reductions in parallel, task, taskloop, and SIMD regions. \n"}
{"section_title": "10.9.1 reduction Clause", "chunk": ""}
{"section_title": "10.9.1 reduction Clause", "chunk": "4 The following example demonstrates the reduction clause; note that some reductions can be \n5 expressed in the loop in several ways, as shown for the max and min reductions below: \nC / C++ \n6 Example reduction.1.c (omp_3.1) \nS-1 #include <math.h> \nS-2 void reduction1(float *x, int *y, int n) \nS-3 { \nS-4 int i, b, c; \nS-5 float a, d; \nS-6 a = 0.0; \nS-7 b = 0; \nS-8 c = y[0]; \nS-9 d = x[0]; \nS-10 #pragma omp parallel for private(i) shared(x, y, n) \\ \nS-11 reduction(+:a) reduction(^:b) \\ \nS-12 reduction(min:c) reduction(max:d) \nS-13 for (i=0; i<n; i++) { \nS-14 a += x[i]; \nS-15 b ^= y[i]; \nS-16 if (c > y[i]) c = y[i]; \nS-17 d = fmaxf(d,x[i]); \nS-18 } \nS-19 } \nC / C++ \n392 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example reduction.1.f90 (pre_omp_3.0) \nS-1 SUBROUTINE REDUCTION1(A, B, C, D, X, Y, N) \nS-2 REAL :: X(*), A, D \nS-3 INTEGER :: Y(*), N, B, C \nS-4 INTEGER :: I \nS-5 A = 0 \nS-6 B = 0 \nS-7 C = Y(1) \nS-8 D = X(1) \nS-9 !$OMP PARALLEL DO PRIVATE(I) SHARED(X, Y, N) REDUCTION(+:A) & \nS-10 !$OMP& REDUCTION(IEOR:B) REDUCTION(MIN:C) REDUCTION(MAX:D) \nS-11 DO I=1,N \nS-12 A = A + X(I) \nS-13 B = IEOR(B, Y(I)) \nS-14 C = MIN(C, Y(I)) \nS-15 IF (D < X(I)) D = X(I) \nS-16 END DO \nS-17 \nS-18 END SUBROUTINE REDUCTION1 \nFortran \n2 A common implementation of the preceding example is to treat it as if it had been written as \n3 follows: \nC / C++ \n4 Example reduction.2.c (pre_omp_3.0) \nS-1 #include <limits.h> \nS-2 #include <math.h> \nS-3 void reduction2(float *x, int *y, int n) \nS-4 { \nS-5 int i, b, b_p, c, c_p; \nS-6 float a, a_p, d, d_p; \nS-7 a = 0.0f; \nS-8 b = 0; \nS-9 c = y[0]; \nS-10 d = x[0]; \nS-11 #pragma omp parallel shared(a, b, c, d, x, y, n) \\ \nS-12 private(a_p, b_p, c_p, d_p) \nS-13 { \nS-14 a_p = 0.0f; \nS-15 b_p = 0; \nS-16 c_p = INT_MAX; \nS-17 d_p = -HUGE_VALF; \nS-18 #pragma omp for private(i) \nCHAPTER 10. DATA ENVIRONMENT 393 \nS-19 for (i=0; i<n; i++) { \nS-20 a_p += x[i]; \nS-21 b_p ^= y[i]; \nS-22 if (c_p > y[i]) c_p = y[i]; \nS-23 d_p = fmaxf(d_p,x[i]); \nS-24 } \nS-25 #pragma omp critical \nS-26 { \nS-27 a += a_p; \nS-28 b ^= b_p; \nS-29 if( c > c_p ) c = c_p; \nS-30 d = fmaxf(d,d_p); \nS-31 } \nS-32 } \nS-33 } \nC / C++ \nFortran \n1 Example reduction.2.f90 (pre_omp_3.0) \nS-1 SUBROUTINE REDUCTION2(A, B, C, D, X, Y, N) \nS-2 REAL :: X(*), A, D \nS-3 INTEGER :: Y(*), N, B, C \nS-4 REAL :: A_P, D_P \nS-5 INTEGER :: I, B_P, C_P \nS-6 A = 0 \nS-7 B = 0 \nS-8 C = Y(1) \nS-9 D = X(1) \nS-10 !$OMP PARALLEL SHARED(X, Y, A, B, C, D, N) & \nS-11 !$OMP& PRIVATE(A_P, B_P, C_P, D_P) \nS-12 A_P = 0.0 \nS-13 B_P = 0 \nS-14 C_P = HUGE(C_P) \nS-15 D_P = -HUGE(D_P) \nS-16 !$OMP DO PRIVATE(I) \nS-17 DO I=1,N \nS-18 A_P = A_P + X(I) \nS-19 B_P = IEOR(B_P, Y(I)) \nS-20 C_P = MIN(C_P, Y(I)) \nS-21 IF (D_P < X(I)) D_P = X(I) \nS-22 END DO \nS-23 !$OMP CRITICAL \nS-24 A = A + A_P \nS-25 B = IEOR(B, B_P) \nS-26 C = MIN(C, C_P) \nS-27 D = MAX(D, D_P) \n394 OpenMP Examples Version 5.2.1 - November 2022 \nFortran (cont.) \nS-28 !$OMP END CRITICAL \nS-29 !$OMP END PARALLEL \nS-30 END SUBROUTINE REDUCTION2 \n1 The following program is non-conforming because the reduction is on the intrinsic procedure name \n2 MAX but that name has been redefined to be the variable named MAX. \n3 Example reduction.3.f90 (pre_omp_3.0) \nS-1 PROGRAM REDUCTION_WRONG \nS-2 MAX = HUGE(0) \nS-3 M = 0 \nS-4 \nS-5 !$OMP PARALLEL DO REDUCTION(MAX: M) \nS-6 ! MAX is no longer the intrinsic so this is non-conforming \nS-7 DO I = 1, 100 \nS-8 CALL SUB(M,I) \nS-9 END DO \nS-10 \nS-11 END PROGRAM REDUCTION_WRONG \nS-12 \nS-13 SUBROUTINE SUB(M,I) \nS-14 M = MAX(M,I) \nS-15 END SUBROUTINE SUB \n4 The following conforming program performs the reduction using the intrinsic procedure name MAX \n5 even though the intrinsic MAX has been renamed to REN. \n6 Example reduction.4.f90 (pre_omp_3.0) \nS-1 MODULE M \nS-2 INTRINSIC MAX \nS-3 END MODULE M \nS-4 \nS-5 PROGRAM REDUCTION3 \nS-6 USE M, REN => MAX \nS-7 N = 0 \nS-8 !$OMP PARALLEL DO REDUCTION(REN: N) ! still does MAX \nS-9 DO I = 1, 100 \nS-10 N = MAX(N,I) \nS-11 END DO \nS-12 END PROGRAM REDUCTION3 \n7 The following conforming program performs the reduction using intrinsic procedure name MAX \n8 even though the intrinsic MAX has been renamed to MIN. \n9 Example reduction.5.f90 (pre_omp_3.0) \nCHAPTER 10. DATA ENVIRONMENT 395 \nS-1 MODULE MOD \nS-2 INTRINSIC MAX, MIN \nS-3 END MODULE MOD \nS-4 \nS-5 PROGRAM REDUCTION4 \nS-6 USE MOD, MIN=>MAX, MAX=>MIN \nS-7 REAL :: R \nS-8 R = -HUGE(0.0) \nS-9 \nS-10 !$OMP PARALLEL DO REDUCTION(MIN: R) ! still does MAX \nS-11 DO I = 1, 1000 \nS-12 R = MIN(R, SIN(REAL(I))) \nS-13 END DO \nS-14 PRINT *, R \nS-15 END PROGRAM REDUCTION4 \nFortran \n1 The following example is non-conforming because the initialization (a = 0) of the original list \n2 item a is not synchronized with the update of a as a result of the reduction computation in the for \n3 loop. Therefore, the example may print an incorrect value for a. \n4 To avoid this problem, the initialization of the original list item a should complete before any \n5 update of a as a result of the reduction clause. This can be achieved by adding an explicit \n6 barrier after the assignment a = 0, or by enclosing the assignment a = 0 in a single directive \n7 (which has an implied barrier), or by initializing a before the start of the parallel region. \nC / C++ \n8 Example reduction.6.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 \nS-3 int main (void) \nS-4 { \nS-5 int a, i; \nS-6 \nS-7 #pragma omp parallel shared(a) private(i) \nS-8 { \nS-9 #pragma omp masked \nS-10 a = 0; \nS-11 \nS-12 // To avoid race conditions, add a barrier here. \nS-13 \nS-14 #pragma omp for reduction(+:a) \nS-15 for (i = 0; i < 10; i++) { \nS-16 a += i; \nS-17 } \nS-18 \n396 OpenMP Examples Version 5.2.1 - November 2022 \nS-19 #pragma omp single \nS-20 printf (\"Sum is %d\\n\", a); \nS-21 } \nS-22 return 0; \nS-23 } \nC / C++ \nFortran \n1 Example reduction.6.f (omp_5.1) \nS-1 INTEGER A, I \nS-2 \nS-3 !$OMP PARALLEL SHARED(A) PRIVATE(I) \nS-4 \nS-5 !$OMP MASKED \nS-6 A = 0 \nS-7 !$OMP END MASKED \nS-8 \nS-9 ! To avoid race conditions, add a barrier here. \nS-10 \nS-11 !$OMP DO REDUCTION(+:A) \nS-12 DO I= 0, 9 \nS-13 A = A + I \nS-14 END DO \nS-15 \nS-16 !$OMP SINGLE \nS-17 PRINT *, \"Sum is \", A \nS-18 !$OMP END SINGLE \nS-19 \nS-20 !$OMP END PARALLEL \nS-21 \nS-22 END \nFortran \n2 The following example demonstrates the reduction of array a. In C/C++ this is illustrated by the \n3 explicit use of an array section a[0:N] in the reduction clause. The corresponding Fortran \n4 example uses array syntax supported in the base language. As of the OpenMP 4.5 specification the \n5 explicit use of array section in the reduction clause in Fortran is not permitted. But this \n6 oversight has been fixed in the OpenMP 5.0 specification. \nCHAPTER 10. DATA ENVIRONMENT 397 \nC / C++ \n1 Example reduction.7.c (omp_4.5) \nS-1 #include <stdio.h> \nS-2 \nS-3 #define N 100 \nS-4 void init(int n, float (*b)[N]); \nS-5 \nS-6 int main(){ \nS-7 \nS-8 int i,j; \nS-9 float a[N], b[N][N]; \nS-10 \nS-11 init(N,b); \nS-12 \nS-13 for(i=0; i<N; i++) a[i]=0.0e0; \nS-14 \nS-15 #pragma omp parallel for reduction(+:a[0:N]) private(j) \nS-16 for(i=0; i<N; i++){ \nS-17 for(j=0; j<N; j++){ \nS-18 a[j] += b[i][j]; \nS-19 } \nS-20 } \nS-21 printf(\" a[0] a[N-1]: %f %f\\n\", a[0], a[N-1]); \nS-22 \nS-23 return 0; \nS-24 } \nC / C++ \nFortran \n2 Example reduction.7.f90 (pre_omp_3.0) \nS-1 program array_red \nS-2 \nS-3 integer,parameter :: n=100 \nS-4 integer :: j \nS-5 real :: a(n), b(n,n) \nS-6 \nS-7 call init(n,b) \nS-8 \nS-9 a(:) = 0.0e0 \nS-10 \nS-11 !$omp parallel do reduction(+:a) \nS-12 do j = 1, n \nS-13 a(:) = a(:) + b(:,j) \nS-14 end do \nS-15 \n398 OpenMP Examples Version 5.2.1 - November 2022 \nS-16 print*, \" a(1) a(n): \", a(1), a(n) \nS-17 \nS-18 end program \nFortran \n"}
{"section_title": "10.9.2 Task Reduction", "chunk": ""}
{"section_title": "10.9.2 Task Reduction", "chunk": "2 In OpenMP 5.0 the task_reduction clause was created for the taskgroup construct, to \n3 allow reductions among explicit tasks that have an in_reduction clause. \n4 In the task_reduction.1 example below a reduction is performed as the algorithm traverses a linked \n5 list. The reduction statement is assigned to be an explicit task using a task construct and is \n6 specified to be a reduction participant with the in_reduction clause. A taskgroup construct \n7 encloses the tasks participating in the reduction, and specifies, with the task_reduction \n8 clause, that the taskgroup has tasks participating in a reduction. After the taskgroup region the \n9 original variable will contain the final value of the reduction. \n10 Note: The res variable is private in the linked_list_sum routine and is not required to be shared (as \n11 in the case of a parallel construct reduction). \nC / C++ \n12 Example task_reduction.1.c (omp_5.0) \nS-1 #include<stdlib.h> \nS-2 #include<stdio.h> \nS-3 #define N 10 \nS-4 \nS-5 typedef struct node_tag { \nS-6 int val; \nS-7 struct node_tag *next; \nS-8 } node_t; \nS-9 \nS-10 int linked_list_sum(node_t *p) \nS-11 { \nS-12 int res = 0; \nS-13 \nS-14 #pragma omp taskgroup task_reduction(+: res) \nS-15 { \nS-16 node_t* aux = p; \nS-17 while(aux != 0) \nS-18 { \nS-19 #pragma omp task in_reduction(+: res) \nS-20 res += aux->val; \nS-21 \nS-22 aux = aux->next; \nCHAPTER 10. DATA ENVIRONMENT 399 \nS-23 } \nS-24 } \nS-25 return res; \nS-26 } \nS-27 \nS-28 \nS-29 int main() { \nS-30 int i; \nS-31 // Create the root node. \nS-32 node_t* root = (node_t*) malloc(sizeof(node_t)); \nS-33 root->val = 1; \nS-34 \nS-35 node_t* aux = root; \nS-36 \nS-37 // Create N-1 more nodes. \nS-38 for(i=2;i<=N;++i){ \nS-39 aux->next = (node_t*) malloc(sizeof(node_t)); \nS-40 aux = aux->next; \nS-41 aux->val = i; \nS-42 } \nS-43 \nS-44 aux->next = 0; \nS-45 \nS-46 #pragma omp parallel \nS-47 #pragma omp single \nS-48 { \nS-49 int result = linked_list_sum(root); \nS-50 printf( \"Calculated: %d Analytic:%d\\n\", result, (N*(N+1)/2) ); \nS-51 } \nS-52 \nS-53 return 0; \nS-54 } \nC / C++ \nFortran \n1 Example task_reduction.1.f90 (omp_5.0) \nS-1 module m \nS-2 type node_t \nS-3 integer :: val \nS-4 type(node_t), pointer :: next \nS-5 end type \nS-6 end module m \nS-7 \nS-8 function linked_list_sum(p) result(res) \nS-9 use m \nS-10 implicit none \n400 OpenMP Examples Version 5.2.1 - November 2022 \nS-11 type(node_t), pointer :: p \nS-12 type(node_t), pointer :: aux \nS-13 integer :: res \nS-14 \nS-15 res = 0 \nS-16 \nS-17 !$omp taskgroup task_reduction(+: res) \nS-18 aux => p \nS-19 do while (associated(aux)) \nS-20 !$omp task in_reduction(+: res) \nS-21 res = res + aux%val \nS-22 !$omp end task \nS-23 aux => aux%next \nS-24 end do \nS-25 !$omp end taskgroup \nS-26 end function linked_list_sum \nS-27 \nS-28 \nS-29 program main \nS-30 use m \nS-31 implicit none \nS-32 type(node_t), pointer :: root, aux \nS-33 integer :: res, i \nS-34 integer, parameter :: N=10 \nS-35 \nS-36 interface \nS-37 function linked_list_sum(p) result(res) \nS-38 use m \nS-39 implicit none \nS-40 type(node_t), pointer :: p \nS-41 integer :: res \nS-42 end function \nS-43 end interface \nS-44 ! Create the root node. \nS-45 allocate(root) \nS-46 root%val = 1 \nS-47 aux => root \nS-48 \nS-49 ! Create N-1 more nodes. \nS-50 do i = 2,N \nS-51 allocate(aux%next) \nS-52 aux => aux%next \nS-53 aux%val = i \nS-54 end do \nS-55 \nS-56 aux%next => null() \nS-57 \nCHAPTER 10. DATA ENVIRONMENT 401 \nS-58 !$omp parallel \nS-59 !$omp single \nS-60 res = linked_list_sum(root) \nS-61 print *, \"Calculated:\", res, \" Analytic:\", (N*(N+1))/2 \nS-62 !$omp end single \nS-63 !$omp end parallel \nS-64 \nS-65 end program main \nFortran \n1 In OpenMP 5.0 the task reduction-modifier for the reduction clause was introduced to \n2 provide a means of performing reductions among implicit and explicit tasks. \n3 The reduction clause of a parallel or worksharing construct may specify the task \n4 reduction-modifier to include explicit task reductions within their region, provided the reduction \n5 operators (reduction-identifiers) and variables (list items) of the participating tasks match those of \n6 the implicit tasks. \n7 There are 2 reduction use cases (identified by USE CASE #) in the task_reduction.2 example below. \n8 In USE CASE 1 a task modifier in the reduction clause of the parallel construct is used to \n9 include the reductions of any participating tasks, those with an in_reduction clause and \n10 matching reduction-identifiers (+) and list items (x). \n11 Note, a taskgroup construct (with a task_reduction clause) in not necessary to scope the \n12 explicit task reduction (as seen in the example above). Hence, even without the implicit task \n13 reduction statement (without the C x++ and Fortran x=x+1 statements), the task \n14 reduction-modifier in a reduction clause of the parallel construct can be used to avoid \n15 having to create a taskgroup construct (and its task_reduction clause) around the task \n16 generating structure. \n17 In USE CASE 2 tasks participating in the reduction are within a worksharing region (a parallel \n18 worksharing-loop construct). Here, too, no taskgroup is required, and the reduction-identifier \n19 (+) and list item (variable x) match as required. \nC / C++ \n20 Example task_reduction.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 int main(void){ \nS-3 int N=100, M=10; \nS-4 int i, x; \nS-5 \nS-6 // USE CASE 1 explicit-task reduction + parallel reduction clause \nS-7 x=0; \nS-8 #pragma omp parallel num_threads(M) reduction(task,+:x) \nS-9 { \nS-10 \n402 OpenMP Examples Version 5.2.1 - November 2022 \nS-11 x++; // implicit task reduction statement \nS-12 \nS-13 #pragma omp single \nS-14 for(i=0;i<N;i++) \nS-15 #pragma omp task in_reduction(+:x) \nS-16 x++; \nS-17 \nS-18 } \nS-19 printf(\"x=%d =M+N\\n\",x); // x= 110 =M+N \nS-20 \nS-21 \nS-22 // USE CASE 2 task reduction + worksharing reduction clause \nS-23 x=0; \nS-24 #pragma omp parallel for num_threads(M) reduction(task,+:x) \nS-25 for(i=0; i< N; i++){ \nS-26 \nS-27 x++; \nS-28 \nS-29 if( i%2 == 0){ \nS-30 #pragma omp task in_reduction(+:x) \nS-31 x--; \nS-32 } \nS-33 } \nS-34 printf(\"x=%d =N-N/2\\n\",x); // x= 50 =N-N/2 \nS-35 \nS-36 return 0; \nS-37 } \nC / C++ \nFortran \n1 Example task_reduction.2.f90 (omp_5.0) \nS-1 program task_modifier \nS-2 \nS-3 integer :: N=100, M=10 \nS-4 integer :: i, x \nS-5 \nS-6 ! USE CASE 1 explicit-task reduction + parallel reduction clause \nS-7 x=0 \nS-8 !$omp parallel num_threads(M) reduction(task,+:x) \nS-9 \nS-10 x=x+1 !! implicit task reduction statement \nS-11 \nS-12 !$omp single \nS-13 do i = 1,N \nS-14 !$omp task in_reduction(+:x) \nS-15 x=x+1 \nCHAPTER 10. DATA ENVIRONMENT 403 \nS-16 !$omp end task \nS-17 end do \nS-18 !$omp end single \nS-19 \nS-20 !$omp end parallel \nS-21 write(*,\u2019(\"x=\",I0,\" =M+N\")\u2019) x ! x= 110 =M+N \nS-22 \nS-23 \nS-24 ! USE CASE 2 task reduction + worksharing reduction clause \nS-25 x=0 \nS-26 !$omp parallel do num_threads(M) reduction(task,+:x) \nS-27 do i = 1,N \nS-28 \nS-29 x=x+1 \nS-30 \nS-31 if( mod(i,2) == 0) then \nS-32 !$omp task in_reduction(+:x) \nS-33 x=x-1 \nS-34 !$omp end task \nS-35 endif \nS-36 \nS-37 end do \nS-38 write(*,\u2019(\"x=\",I0,\" =N-N/2\")\u2019) x ! x= 50 =N-N/2 \nS-39 \nS-40 end program \nFortran \n"}
{"section_title": "10.9.3 Reduction on Combined Target Constructs", "chunk": ""}
{"section_title": "10.9.3 Reduction on Combined Target Constructs", "chunk": "2 When a reduction clause appears on a combined construct that combines a target construct \n3 with another construct, there is an implicit map of the list items with a tofrom map type for the \n4 target construct. Otherwise, the list items (if they are scalar variables) would be treated as \n5 firstprivate by default in the target construct, which is unlikely to provide the intended behavior \n6 since the result of the reduction that is in the firstprivate variable would be discarded at the end of \n7 the target region. \n8 In the following example, the use of the reduction clause on sum1 or sum2 should, by default, \n9 result in an implicit tofrom map for that variable. So long as neither sum1 nor sum2 were \n10 already present on the device, the mapping behavior ensures the value for sum1 computed in the \n11 first target construct is used in the second target construct. \n404 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example target_reduction.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 int f(int); \nS-3 int g(int); \nS-4 int main() \nS-5 { \nS-6 int sum1=0, sum2=0; \nS-7 const int n = 100; \nS-8 \nS-9 #pragma omp target teams distribute reduction(+:sum1) \nS-10 for (int i = 0; i < n; i++) { \nS-11 sum1 += f(i); \nS-12 } \nS-13 \nS-14 #pragma omp target teams distribute reduction(+:sum2) \nS-15 for (int i = 0; i < n; i++) { \nS-16 sum2 += g(i) * sum1; \nS-17 } \nS-18 \nS-19 printf( \"sum1 = %d, sum2 = %d\\n\", sum1, sum2); \nS-20 //OUTPUT: sum1 = 9900, sum2 = 147015000 \nS-21 return 0; \nS-22 } \nS-23 \nS-24 int f(int res){ return res*2; } \nS-25 int g(int res){ return res*3; } \nC / C++ \nFortran \n2 Example target_reduction.1.f90 (omp_5.0) \nS-1 program target_reduction_ex1 \nS-2 interface \nS-3 function f(res) \nS-4 integer :: f, res \nS-5 end function \nS-6 function g(res) \nS-7 integer :: g, res \nS-8 end function \nS-9 end interface \nS-10 integer :: sum1, sum2, i \nS-11 integer, parameter :: n = 100 \nS-12 sum1 = 0 \nS-13 sum2 = 0 \nS-14 !$omp target teams distribute reduction(+:sum1) \nCHAPTER 10. DATA ENVIRONMENT 405 \nS-15 do i=1,n \nS-16 sum1 = sum1 + f(i) \nS-17 end do \nS-18 !$omp target teams distribute reduction(+:sum2) \nS-19 do i=1,n \nS-20 sum2 = sum2 + g(i)*sum1 \nS-21 end do \nS-22 print *, \"sum1 = \", sum1, \", sum2 = \", sum2 \nS-23 !!OUTPUT: sum1 = 10100 , sum2 = 153015000 \nS-24 end program \nS-25 \nS-26 \nS-27 integer function f(res) \nS-28 integer :: res \nS-29 f = res*2 \nS-30 end function \nS-31 integer function g(res) \nS-32 integer :: res \nS-33 g = res*3 \nS-34 end function \nFortran \n1 In next example, the variables sum1 and sum2 remain on the device for the duration of the \n2 target data region so that it is their device copies that are updated by the reductions. Note the \n3 significance of mapping sum1 on the second target construct; otherwise, it would be treated by \n4 default as firstprivate and the result computed for sum1 in the prior target region may not be \n5 used. Alternatively, a target update construct could be used between the two target \n6 constructs to update the host version of sum1 with the value that is in the corresponding device \n7 version after the completion of the first construct. \nC / C++ \n8 Example target_reduction.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 int f(int); \nS-3 int g(int); \nS-4 int main() \nS-5 { \nS-6 int sum1=0, sum2=0; \nS-7 const int n = 100; \nS-8 \nS-9 #pragma omp target data map(sum1,sum2) \nS-10 { \nS-11 #pragma omp target teams distribute reduction(+:sum1) \nS-12 for (int i = 0; i < n; i++) { \nS-13 sum1 += f(i); \nS-14 } \n406 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 \nS-16 #pragma omp target teams distribute map(sum1) reduction(+:sum2) \nS-17 for (int i = 0; i < n; i++) { \nS-18 sum2 += g(i) * sum1; \nS-19 } \nS-20 } \nS-21 printf( \"sum1 = %d, sum2 = %d\\n\", sum1, sum2); \nS-22 //OUTPUT: sum1 = 9900, sum2 = 147015000 \nS-23 return 0; \nS-24 } \nS-25 \nS-26 int f(int res){ return res*2; } \nS-27 int g(int res){ return res*3; } \nC / C++ \nFortran \n1 Example target_reduction.2.f90 (omp_5.0) \nS-1 program target_reduction_ex2 \nS-2 interface \nS-3 function f(res) \nS-4 integer :: f, res \nS-5 end function \nS-6 function g(res) \nS-7 integer :: g, res \nS-8 end function \nS-9 end interface \nS-10 integer :: sum1, sum2, i \nS-11 integer, parameter :: n = 100 \nS-12 sum1 = 0 \nS-13 sum2 = 0 \nS-14 !$omp target data map(sum1, sum2) \nS-15 !$omp target teams distribute reduction(+:sum1) \nS-16 do i=1,n \nS-17 sum1 = sum1 + f(i) \nS-18 end do \nS-19 !$omp target teams distribute map(sum1) reduction(+:sum2) \nS-20 do i=1,n \nS-21 sum2 = sum2 + g(i)*sum1 \nS-22 end do \nS-23 !$omp end target data \nS-24 print *, \"sum1 = \", sum1, \", sum2 = \", sum2 \nS-25 !!OUTPUT: sum1 = 10100 , sum2 = 153015000 \nS-26 end program \nS-27 \nS-28 \nS-29 integer function f(res) \nCHAPTER 10. DATA ENVIRONMENT 407 \nS-30 integer :: res \nS-31 f = res*2 \nS-32 end function \nS-33 integer function g(res) \nS-34 integer :: res \nS-35 g = res*3 \nS-36 end function \nFortran \n"}
{"section_title": "10.9.4 Task Reduction with Target Constructs", "chunk": ""}
{"section_title": "10.9.4 Task Reduction with Target Constructs", "chunk": "2 The following examples illustrate how task reductions can apply to target tasks that result from a \n3 target construct with the in_reduction clause. Here, the in_reduction clause specifies \n4 that the target task participates in the task reduction defined in the scope of the enclosing \n5 taskgroup construct. Partial results from all tasks participating in the task reduction will be \n6 combined (in some order) into the original variable listed in the task_reduction clause before \n7 exiting the taskgroup region. \nC / C++ \n8 Example target_task_reduction.1.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 void device_compute(int *); \nS-3 #pragma omp declare target enter(device_compute) \nS-4 void host_compute(int *); \nS-5 int main() \nS-6 { \nS-7 int sum = 0; \nS-8 \nS-9 #pragma omp parallel masked \nS-10 #pragma omp taskgroup task_reduction(+:sum) \nS-11 { \nS-12 #pragma omp target in_reduction(+:sum) nowait \nS-13 device_compute(&sum); \nS-14 \nS-15 #pragma omp task in_reduction(+:sum) \nS-16 host_compute(&sum); \nS-17 } \nS-18 printf( \"sum = %d\\n\", sum); \nS-19 //OUTPUT: sum = 2 \nS-20 return 0; \nS-21 } \nS-22 \nS-23 void device_compute(int *sum){ *sum = 1; } \nS-24 void host_compute(int *sum){ *sum = 1; } \n408 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \nFortran \n1 Example target_task_reduction.1.f90 (omp_5.2) \nS-1 program target_task_reduction_ex1 \nS-2 interface \nS-3 subroutine device_compute(res) \nS-4 !$omp declare target enter(device_compute) \nS-5 integer :: res \nS-6 end subroutine device_compute \nS-7 subroutine host_compute(res) \nS-8 integer :: res \nS-9 end subroutine host_compute \nS-10 end interface \nS-11 integer :: sum \nS-12 sum = 0 \nS-13 !$omp parallel masked \nS-14 !$omp taskgroup task_reduction(+:sum) \nS-15 !$omp target in_reduction(+:sum) nowait \nS-16 call device_compute(sum) \nS-17 !$omp end target \nS-18 !$omp task in_reduction(+:sum) \nS-19 call host_compute(sum) \nS-20 !$omp end task \nS-21 !$omp end taskgroup \nS-22 !$omp end parallel masked \nS-23 print *, \"sum = \", sum \nS-24 !!OUTPUT: sum = 2 \nS-25 end program \nS-26 \nS-27 subroutine device_compute(sum) \nS-28 integer :: sum \nS-29 sum = 1 \nS-30 end subroutine \nS-31 subroutine host_compute(sum) \nS-32 integer :: sum \nS-33 sum = 1 \nS-34 end subroutine \nFortran \nCHAPTER 10. DATA ENVIRONMENT 409 \n1 In the next pair of examples, the task reduction is defined by a reduction clause with the task \n2 modifier, rather than a task_reduction clause on a taskgroup construct. Again, the partial \n3 results from the participating tasks will be combined in some order into the original reduction \n4 variable, sum. \nC / C++ \n5 Example target_task_reduction.2a.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 extern void device_compute(int *); \nS-3 #pragma omp declare target enter(device_compute) \nS-4 extern void host_compute(int *); \nS-5 int main() \nS-6 { \nS-7 int sum = 0; \nS-8 \nS-9 #pragma omp parallel sections reduction(task, +:sum) \nS-10 { \nS-11 #pragma omp section \nS-12 { \nS-13 #pragma omp target in_reduction(+:sum) \nS-14 device_compute(&sum); \nS-15 } \nS-16 #pragma omp section \nS-17 { \nS-18 host_compute(&sum); \nS-19 } \nS-20 } \nS-21 printf( \"sum = %d\\n\", sum); \nS-22 //OUTPUT: sum = 2 \nS-23 return 0; \nS-24 } \nS-25 \nS-26 void device_compute(int *sum){ *sum = 1; } \nS-27 void host_compute(int *sum){ *sum = 1; } \nC / C++ \n410 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example target_task_reduction.2a.f90 (omp_5.2) \nS-1 program target_task_reduction_ex2 \nS-2 interface \nS-3 subroutine device_compute(res) \nS-4 !$omp declare target enter(device_compute) \nS-5 integer :: res \nS-6 end subroutine device_compute \nS-7 subroutine host_compute(res) \nS-8 integer :: res \nS-9 end subroutine host_compute \nS-10 end interface \nS-11 integer :: sum \nS-12 sum = 0 \nS-13 !$omp parallel sections reduction(task,+:sum) \nS-14 !$omp section \nS-15 !$omp target in_reduction(+:sum) nowait \nS-16 call device_compute(sum) \nS-17 !$omp end target \nS-18 !$omp section \nS-19 call host_compute(sum) \nS-20 !$omp end parallel sections \nS-21 print *, \"sum = \", sum \nS-22 !!OUTPUT: sum = 2 \nS-23 end program \nS-24 \nS-25 subroutine device_compute(sum) \nS-26 integer :: sum \nS-27 sum = 1 \nS-28 end subroutine \nS-29 subroutine host_compute(sum) \nS-30 integer :: sum \nS-31 sum = 1 \nS-32 end subroutine \nFortran \n2 Next, the task modifier is again used to define a task reduction over participating tasks. This time, \n3 the participating tasks are a target task resulting from a target construct with the \n4 in_reduction clause, and the implicit task (executing on the primary thread) that calls \n5 host_compute. As before, the partial results from these participating tasks are combined in \n6 some order into the original reduction variable. \nCHAPTER 10. DATA ENVIRONMENT 411 \nC / C++ \n1 Example target_task_reduction.2b.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 extern void device_compute(int *); \nS-3 #pragma omp declare target enter(device_compute) \nS-4 extern void host_compute(int *); \nS-5 int main() \nS-6 { \nS-7 int sum = 0; \nS-8 \nS-9 #pragma omp parallel masked reduction(task, +:sum) \nS-10 { \nS-11 #pragma omp target in_reduction(+:sum) nowait \nS-12 device_compute(&sum); \nS-13 \nS-14 host_compute(&sum); \nS-15 } \nS-16 printf( \"sum = %d\\n\", sum); \nS-17 //OUTPUT: sum = 2 \nS-18 return 0; \nS-19 } \nS-20 \nS-21 void device_compute(int *sum){ *sum = 1; } \nS-22 void host_compute(int *sum){ *sum = 1; } \nC / C++ \nFortran \n2 Example target_task_reduction.2b.f90 (omp_5.2) \nS-1 program target_task_reduction_ex2b \nS-2 interface \nS-3 subroutine device_compute(res) \nS-4 !$omp declare target enter(device_compute) \nS-5 integer :: res \nS-6 end subroutine device_compute \nS-7 subroutine host_compute(res) \nS-8 integer :: res \nS-9 end subroutine host_compute \nS-10 end interface \nS-11 integer :: sum \nS-12 sum = 0 \nS-13 !$omp parallel masked reduction(task,+:sum) \nS-14 !$omp target in_reduction(+:sum) nowait \nS-15 call device_compute(sum) \nS-16 !$omp end target \nS-17 call host_compute(sum) \n412 OpenMP Examples Version 5.2.1 - November 2022 \nS-18 !$omp end parallel masked \nS-19 print *, \"sum = \", sum \nS-20 !!OUTPUT: sum = 2 \nS-21 end program \nS-22 \nS-23 \nS-24 subroutine device_compute(sum) \nS-25 integer :: sum \nS-26 sum = 1 \nS-27 end subroutine \nS-28 subroutine host_compute(sum) \nS-29 integer :: sum \nS-30 sum = 1 \nS-31 end subroutine \nFortran \n"}
{"section_title": "10.9.5 Taskloop Reduction", "chunk": ""}
{"section_title": "10.9.5 Taskloop Reduction", "chunk": "2 In the OpenMP 5.0 Specification the taskloop construct was extended to include the reductions. \n3 The following two examples show how to implement a reduction over an array using taskloop \n4 reduction in two different ways. In the first example we apply the reduction clause to the \n5 taskloop construct. As it was explained above in the task reduction examples, a reduction over \n6 tasks is divided in two components: the scope of the reduction, which is defined by a taskgroup \n7 region, and the tasks that participate in the reduction. In this example, the reduction clause \n8 defines both semantics. First, it specifies that the implicit taskgroup region associated with the \n9 taskloop construct is the scope of the reduction, and second, it defines all tasks created by the \n10 taskloop construct as participants of the reduction. About the first property, it is important to \n11 note that if we add the nogroup clause to the taskloop construct the code will be \n12 nonconforming, basically because we have a set of tasks that participate in a reduction that has not \n13 been defined. \nC / C++ \n14 Example taskloop_reduction.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 int array_sum(int n, int *v) { \nS-4 int i; \nS-5 int res = 0; \nS-6 \nS-7 #pragma omp taskloop reduction(+: res) \nS-8 for(i = 0; i < n; ++i) \nS-9 res += v[i]; \nS-10 \nCHAPTER 10. DATA ENVIRONMENT 413 \nS-11 return res; \nS-12 } \nS-13 \nS-14 int main(int argc, char *argv[]) { \nS-15 int n = 10; \nS-16 int v[10] = {1,2,3,4,5,6,7,8,9,10}; \nS-17 \nS-18 #pragma omp parallel \nS-19 #pragma omp single \nS-20 { \nS-21 int res = array_sum(n, v); \nS-22 printf(\"The result is %d\\n\", res); \nS-23 } \nS-24 return 0; \nS-25 } \nC / C++ \nFortran \n1 Example taskloop_reduction.1.f90 (omp_5.0) \nS-1 function array_sum(n, v) result(res) \nS-2 implicit none \nS-3 integer :: n, v(n), res \nS-4 integer :: i \nS-5 \nS-6 res = 0 \nS-7 !$omp taskloop reduction(+: res) \nS-8 do i=1, n \nS-9 res = res + v(i) \nS-10 end do \nS-11 !$omp end taskloop \nS-12 \nS-13 end function array_sum \nS-14 \nS-15 program main \nS-16 implicit none \nS-17 integer :: n, v(10), res \nS-18 integer :: i \nS-19 \nS-20 integer, external :: array_sum \nS-21 \nS-22 n = 10 \nS-23 do i=1, n \nS-24 v(i) = i \nS-25 end do \nS-26 \nS-27 !$omp parallel \n414 OpenMP Examples Version 5.2.1 - November 2022 \nS-28 !$omp single \nS-29 res = array_sum(n, v) \nS-30 print *, \"The result is\", res \nS-31 !$omp end single \nS-32 !$omp end parallel \nS-33 end program main \nFortran \n1 The second example computes exactly the same value as in the preceding taskloop_reduction.1 \n2 code section, but in a very different way. First, in the array_sum function a taskgroup region is \n3 created that defines the scope of a new reduction using the task_reduction clause. After that, \n4 a task and also the tasks generated by a taskloop participate in that reduction by using the \n5 in_reduction clause on the task and taskloop constructs, respectively. Note that the \n6 nogroup clause was added to the taskloop construct. This is allowed because what is \n7 expressed with the in_reduction clause is different from what is expressed with the \n8 reduction clause. In one case the generated tasks are specified to participate in a previously \n9 declared reduction (in_reduction clause) whereas in the other case creation of a new reduction \n10 is specified and also all tasks generated by the taskloop will participate on it. \nC / C++ \n11 Example taskloop_reduction.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 int array_sum(int n, int *v) { \nS-4 int i; \nS-5 int res = 0; \nS-6 \nS-7 #pragma omp taskgroup task_reduction(+: res) \nS-8 { \nS-9 if (n > 0) { \nS-10 #pragma omp task in_reduction(+: res) \nS-11 res = res + v[0]; \nS-12 \nS-13 #pragma omp taskloop in_reduction(+: res) nogroup \nS-14 for(i = 1; i < n; ++i) \nS-15 res += v[i]; \nS-16 } \nS-17 } \nS-18 \nS-19 return res; \nS-20 } \nS-21 \nS-22 int main() { \nS-23 int n = 10; \nS-24 int v[10] = {1,2,3,4,5,6,7,8,9,10}; \nCHAPTER 10. DATA ENVIRONMENT 415 \nS-25 \nS-26 #pragma omp parallel \nS-27 #pragma omp single \nS-28 { \nS-29 int res = array_sum(n, v); \nS-30 printf(\"The result is %d\\n\", res); \nS-31 } \nS-32 return 0; \nS-33 } \nC / C++ \nFortran \n1 Example taskloop_reduction.2.f90 (omp_5.0) \nS-1 function array_sum(n, v) result(res) \nS-2 implicit none \nS-3 integer :: n, v(n), res \nS-4 integer :: i \nS-5 \nS-6 res = 0 \nS-7 !$omp taskgroup task_reduction(+: res) \nS-8 if (n > 0) then \nS-9 !$omp task in_reduction(+: res) \nS-10 res = res + v(1) \nS-11 !$omp end task \nS-12 \nS-13 !$omp taskloop in_reduction(+: res) nogroup \nS-14 do i=2, n \nS-15 res = res + v(i) \nS-16 end do \nS-17 !$omp end taskloop \nS-18 endif \nS-19 !$omp end taskgroup \nS-20 \nS-21 end function array_sum \nS-22 \nS-23 program main \nS-24 implicit none \nS-25 integer :: n, v(10), res \nS-26 integer :: i \nS-27 \nS-28 integer, external :: array_sum \nS-29 \nS-30 n = 10 \nS-31 do i=1, n \nS-32 v(i) = i \nS-33 end do \n416 OpenMP Examples Version 5.2.1 - November 2022 \nS-34 \nS-35 !$omp parallel \nS-36 !$omp single \nS-37 res = array_sum(n, v) \nS-38 print *, \"The result is\", res \nS-39 !$omp end single \nS-40 !$omp end parallel \nS-41 end program main \nFortran \n1 In the OpenMP 5.0 Specification, reduction clauses for the taskloop simd construct were \n2 also added. \n3 The examples below compare reductions for the taskloop and the taskloop simd constructs. \n4 These examples illustrate the use of reduction clauses within \u201cstand-alone\u201d taskloop \n5 constructs, and the use of in_reduction clauses for tasks of taskloops to participate with other \n6 reductions within the scope of a parallel region. \n7 taskloop reductions: \n8 In the taskloop reductions section of the example below, taskloop 1 uses the reduction clause in \n9 a taskloop construct for a sum reduction, accumulated in asum. The behavior is as though a \n10 taskgroup construct encloses the taskloop region with a task_reduction clause, and each \n11 taskloop task has an in_reduction clause with the specifications of the reduction clause. \n12 At the end of the taskloop region asum contains the result of the reduction. \n13 The next taskloop, taskloop 2, illustrates the use of the in_reduction clause to participate in a \n14 previously defined reduction scope of a parallel construct. \n15 The task reductions of task 2 and taskloop 2 are combined across the taskloop construct and the \n16 single task construct, as specified in the reduction(task, +:asum) clause of the \n17 parallel construct. At the end of the parallel region asum contains the combined result of all \n18 reductions. \n19 taskloop simd reductions: \n20 Reductions for the taskloop simd construct are shown in the second half of the code. Since \n21 each component construct, taskloop and simd, can accept a reduction-type clause, the \n22 taskloop simd construct is a composite construct, and the specific application of the reduction \n23 clause is defined within the taskloop simd construct section of the OpenMP 5.0 Specification. \n24 The code below illustrates use cases for these reductions. \n25 In the taskloop simd reduction section of the example below, taskloop simd 3 uses the reduction \n26 clause in a taskloop simd construct for a sum reduction within a loop. For this case a \n27 reduction clause is used, as one would use for a simd construct. The SIMD reductions of each \n28 task are combined, and the results of these tasks are further combined just as in the taskloop \n29 construct with the reduction clause for taskloop 1. At the end of the taskloop region asum \n30 contains the combined result of all reductions. \nCHAPTER 10. DATA ENVIRONMENT 417 \n1 If a taskloop simd construct is to participate in a previously defined reduction scope, the \n2 reduction participation should be specified with a in_reduction clause, as shown in the \n3 parallel region enclosing task 4 and taskloop simd 4 code sections. \n4 Here the taskloop simd construct\u2019s in_reduction clause specifies participation of the \n5 construct\u2019s tasks as a task reduction within the scope of the parallel region. That is, the results of \n6 each task of the taskloop construct component contribute to the reduction in a broader level, just \n7 as in parallel reduction a code section above. Also, each simd-component construct occurs as if it \n8 has a reduction clause, and the SIMD results of each task are combined as though to form a \n9 single result for each task (that participates in the in_reduction clause). At the end of the \n10 parallel region asum contains the combined result of all reductions. \nC / C++ \n11 Example taskloop_simd_reduction.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 \nS-4 int main(){ \nS-5 int i, a[N], asum=0; \nS-6 \nS-7 for(i=0;i<N;i++) a[i]=i; \nS-8 \nS-9 // taskloop reductions \nS-10 \nS-11 #pragma omp parallel masked \nS-12 #pragma omp taskloop reduction(+:asum) // taskloop 1 \nS-13 for(i=0;i<N;i++){ asum += a[i]; } \nS-14 \nS-15 \nS-16 #pragma omp parallel reduction(task, +:asum) // parallel reduction a \nS-17 { \nS-18 #pragma omp masked \nS-19 #pragma omp task in_reduction(+:asum) // task 2 \nS-20 for(i=0;i<N;i++){ asum += a[i]; } \nS-21 \nS-22 #pragma omp masked taskloop in_reduction(+:asum) // taskloop 2 \nS-23 for(i=0;i<N;i++){ asum += a[i]; } \nS-24 } \nS-25 \nS-26 // taskloop simd reductions \nS-27 \nS-28 #pragma omp parallel masked \nS-29 #pragma omp taskloop simd reduction(+:asum) // taskloop simd 3 \nS-30 for(i=0;i<N;i++){ asum += a[i]; } \nS-31 \nS-32 \n418 OpenMP Examples Version 5.2.1 - November 2022 \nS-33 #pragma omp parallel reduction(task, +:asum) // parallel reduction b \nS-34 { \nS-35 #pragma omp masked \nS-36 #pragma omp task in_reduction(+:asum) // task 4 \nS-37 for(i=0;i<N;i++){ asum += a[i]; } \nS-38 \nS-39 #pragma omp masked taskloop simd in_reduction(+:asum) // taskloop \nS-40 for(i=0;i<N;i++){ asum += a[i]; } // simd 4 \nS-41 \nS-42 } \nS-43 \nS-44 printf(\"asum=%d"}
{"section_title": "10.9.5 Taskloop Reduction", "chunk": "2 In the OpenMP 5.0 Specification the taskloop construct was extended to include the reductions. \n3 The following two examples show how to implement a reduction over an array using taskloop \n4 reduction in two different ways. In the first example we apply the reduction clause to the \n5 taskloop construct. As it was explained above in the task reduction examples, a reduction over \n6 tasks is divided in two components: the scope of the reduction, which is defined by a taskgroup \n7 region, and the tasks that participate in the reduction. In this example, the reduction clause \n8 defines both semantics. First, it specifies that the implicit taskgroup region associated with the \n9 taskloop construct is the scope of the reduction, and second, it defines all tasks created by the \n10 taskloop construct as participants of the reduction. About the first property, it is important to \n11 note that if we add the nogroup clause to the taskloop construct the code will be \n12 nonconforming, basically because we have a set of tasks that participate in a reduction that has not \n13 been defined. \nC / C++ \n14 Example taskloop_reduction.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 int array_sum(int n, int *v) { \nS-4 int i; \nS-5 int res = 0; \nS-6 \nS-7 #pragma omp taskloop reduction(+: res) \nS-8 for(i = 0; i < n; ++i) \nS-9 res += v[i]; \nS-10 \nCHAPTER 10. DATA ENVIRONMENT 413 \nS-11 return res; \nS-12 } \nS-13 \nS-14 int main(int argc, char *argv[]) { \nS-15 int n = 10; \nS-16 int v[10] = {1,2,3,4,5,6,7,8,9,10}; \nS-17 \nS-18 #pragma omp parallel \nS-19 #pragma omp single \nS-20 { \nS-21 int res = array_sum(n, v); \nS-22 printf(\"The result is %d\\n\", res); \nS-23 } \nS-24 return 0; \nS-25 } \nC / C++ \nFortran \n1 Example taskloop_reduction.1.f90 (omp_5.0) \nS-1 function array_sum(n, v) result(res) \nS-2 implicit none \nS-3 integer :: n, v(n), res \nS-4 integer :: i \nS-5 \nS-6 res = 0 \nS-7 !$omp taskloop reduction(+: res) \nS-8 do i=1, n \nS-9 res = res + v(i) \nS-10 end do \nS-11 !$omp end taskloop \nS-12 \nS-13 end function array_sum \nS-14 \nS-15 program main \nS-16 implicit none \nS-17 integer :: n, v(10), res \nS-18 integer :: i \nS-19 \nS-20 integer, external :: array_sum \nS-21 \nS-22 n = 10 \nS-23 do i=1, n \nS-24 v(i) = i \nS-25 end do \nS-26 \nS-27 !$omp parallel \n414 OpenMP Examples Version 5.2.1 - November 2022 \nS-28 !$omp single \nS-29 res = array_sum(n, v) \nS-30 print *, \"The result is\", res \nS-31 !$omp end single \nS-32 !$omp end parallel \nS-33 end program main \nFortran \n1 The second example computes exactly the same value as in the preceding taskloop_reduction.1 \n2 code section, but in a very different way. First, in the array_sum function a taskgroup region is \n3 created that defines the scope of a new reduction using the task_reduction clause. After that, \n4 a task and also the tasks generated by a taskloop participate in that reduction by using the \n5 in_reduction clause on the task and taskloop constructs, respectively. Note that the \n6 nogroup clause was added to the taskloop construct. This is allowed because what is \n7 expressed with the in_reduction clause is different from what is expressed with the \n8 reduction clause. In one case the generated tasks are specified to participate in a previously \n9 declared reduction (in_reduction clause) whereas in the other case creation of a new reduction \n10 is specified and also all tasks generated by the taskloop will participate on it. \nC / C++ \n11 Example taskloop_reduction.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 int array_sum(int n, int *v) { \nS-4 int i; \nS-5 int res = 0; \nS-6 \nS-7 #pragma omp taskgroup task_reduction(+: res) \nS-8 { \nS-9 if (n > 0) { \nS-10 #pragma omp task in_reduction(+: res) \nS-11 res = res + v[0]; \nS-12 \nS-13 #pragma omp taskloop in_reduction(+: res) nogroup \nS-14 for(i = 1; i < n; ++i) \nS-15 res += v[i]; \nS-16 } \nS-17 } \nS-18 \nS-19 return res; \nS-20 } \nS-21 \nS-22 int main() { \nS-23 int n = 10; \nS-24 int v[10] = {1,2,3,4,5,6,7,8,9,10}; \nCHAPTER 10. DATA ENVIRONMENT 415 \nS-25 \nS-26 #pragma omp parallel \nS-27 #pragma omp single \nS-28 { \nS-29 int res = array_sum(n, v); \nS-30 printf(\"The result is %d\\n\", res); \nS-31 } \nS-32 return 0; \nS-33 } \nC / C++ \nFortran \n1 Example taskloop_reduction.2.f90 (omp_5.0) \nS-1 function array_sum(n, v) result(res) \nS-2 implicit none \nS-3 integer :: n, v(n), res \nS-4 integer :: i \nS-5 \nS-6 res = 0 \nS-7 !$omp taskgroup task_reduction(+: res) \nS-8 if (n > 0) then \nS-9 !$omp task in_reduction(+: res) \nS-10 res = res + v(1) \nS-11 !$omp end task \nS-12 \nS-13 !$omp taskloop in_reduction(+: res) nogroup \nS-14 do i=2, n \nS-15 res = res + v(i) \nS-16 end do \nS-17 !$omp end taskloop \nS-18 endif \nS-19 !$omp end taskgroup \nS-20 \nS-21 end function array_sum \nS-22 \nS-23 program main \nS-24 implicit none \nS-25 integer :: n, v(10), res \nS-26 integer :: i \nS-27 \nS-28 integer, external :: array_sum \nS-29 \nS-30 n = 10 \nS-31 do i=1, n \nS-32 v(i) = i \nS-33 end do \n416 OpenMP Examples Version 5.2.1 - November 2022 \nS-34 \nS-35 !$omp parallel \nS-36 !$omp single \nS-37 res = array_sum(n, v) \nS-38 print *, \"The result is\", res \nS-39 !$omp end single \nS-40 !$omp end parallel \nS-41 end program main \nFortran \n1 In the OpenMP 5.0 Specification, reduction clauses for the taskloop simd construct were \n2 also added. \n3 The examples below compare reductions for the taskloop and the taskloop simd constructs. \n4 These examples illustrate the use of reduction clauses within \u201cstand-alone\u201d taskloop \n5 constructs, and the use of in_reduction clauses for tasks of taskloops to participate with other \n6 reductions within the scope of a parallel region. \n7 taskloop reductions: \n8 In the taskloop reductions section of the example below, taskloop 1 uses the reduction clause in \n9 a taskloop construct for a sum reduction, accumulated in asum. The behavior is as though a \n10 taskgroup construct encloses the taskloop region with a task_reduction clause, and each \n11 taskloop task has an in_reduction clause with the specifications of the reduction clause. \n12 At the end of the taskloop region asum contains the result of the reduction. \n13 The next taskloop, taskloop 2, illustrates the use of the in_reduction clause to participate in a \n14 previously defined reduction scope of a parallel construct. \n15 The task reductions of task 2 and taskloop 2 are combined across the taskloop construct and the \n16 single task construct, as specified in the reduction(task, +:asum) clause of the \n17 parallel construct. At the end of the parallel region asum contains the combined result of all \n18 reductions. \n19 taskloop simd reductions: \n20 Reductions for the taskloop simd construct are shown in the second half of the code. Since \n21 each component construct, taskloop and simd, can accept a reduction-type clause, the \n22 taskloop simd construct is a composite construct, and the specific application of the reduction \n23 clause is defined within the taskloop simd construct section of the OpenMP 5.0 Specification. \n24 The code below illustrates use cases for these reductions. \n25 In the taskloop simd reduction section of the example below, taskloop simd 3 uses the reduction \n26 clause in a taskloop simd construct for a sum reduction within a loop. For this case a \n27 reduction clause is used, as one would use for a simd construct. The SIMD reductions of each \n28 task are combined, and the results of these tasks are further combined just as in the taskloop \n29 construct with the reduction clause for taskloop 1. At the end of the taskloop region asum \n30 contains the combined result of all reductions. \nCHAPTER 10. DATA ENVIRONMENT 417 \n1 If a taskloop simd construct is to participate in a previously defined reduction scope, the \n2 reduction participation should be specified with a in_reduction clause, as shown in the \n3 parallel region enclosing task 4 and taskloop simd 4 code sections. \n4 Here the taskloop simd construct\u2019s in_reduction clause specifies participation of the \n5 construct\u2019s tasks as a task reduction within the scope of the parallel region. That is, the results of \n6 each task of the taskloop construct component contribute to the reduction in a broader level, just \n7 as in parallel reduction a code section above. Also, each simd-component construct occurs as if it \n8 has a reduction clause, and the SIMD results of each task are combined as though to form a \n9 single result for each task (that participates in the in_reduction clause). At the end of the \n10 parallel region asum contains the combined result of all reductions. \nC / C++ \n11 Example taskloop_simd_reduction.1.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 \nS-4 int main(){ \nS-5 int i, a[N], asum=0; \nS-6 \nS-7 for(i=0;i<N;i++) a[i]=i; \nS-8 \nS-9 // taskloop reductions \nS-10 \nS-11 #pragma omp parallel masked \nS-12 #pragma omp taskloop reduction(+:asum) // taskloop 1 \nS-13 for(i=0;i<N;i++){ asum += a[i]; } \nS-14 \nS-15 \nS-16 #pragma omp parallel reduction(task, +:asum) // parallel reduction a \nS-17 { \nS-18 #pragma omp masked \nS-19 #pragma omp task in_reduction(+:asum) // task 2 \nS-20 for(i=0;i<N;i++){ asum += a[i]; } \nS-21 \nS-22 #pragma omp masked taskloop in_reduction(+:asum) // taskloop 2 \nS-23 for(i=0;i<N;i++){ asum += a[i]; } \nS-24 } \nS-25 \nS-26 // taskloop simd reductions \nS-27 \nS-28 #pragma omp parallel masked \nS-29 #pragma omp taskloop simd reduction(+:asum) // taskloop simd 3 \nS-30 for(i=0;i<N;i++){ asum += a[i]; } \nS-31 \nS-32 \n418 OpenMP Examples Version 5.2.1 - November 2022 \nS-33 #pragma omp parallel reduction(task, +:asum) // parallel reduction b \nS-34 { \nS-35 #pragma omp masked \nS-36 #pragma omp task in_reduction(+:asum) // task 4 \nS-37 for(i=0;i<N;i++){ asum += a[i]; } \nS-38 \nS-39 #pragma omp masked taskloop simd in_reduction(+:asum) // taskloop \nS-40 for(i=0;i<N;i++){ asum += a[i]; } // simd 4 \nS-41 \nS-42 } \nS-43 \nS-44 printf(\"asum=%d\",asum); // output: asum=29700 \nS-45 } \nC / C++ \nFortran \n1 Example taskloop_simd_reduction.1.f90 (omp_5.1) \nS-1 program main \nS-2 \nS-3 use omp_lib \nS-4 integer, parameter :: N=100 \nS-5 integer :: i, a(N), asum=0 \nS-6 \nS-7 a = [( i, i=1,N )] !! initialize \nS-8 \nS-9 !! taskloop reductions \nS-10 \nS-11 !$omp parallel masked \nS-12 !$omp taskloop reduction(+:asum) !! taskloop 1 \nS-13 do i=1,N; asum = asum + a(i); enddo \nS-14 !$omp end taskloop \nS-15 !$omp end parallel masked \nS-16 \nS-17 \nS-18 !$omp parallel reduction(task, +:asum) !! parallel reduction a \nS-19 \nS-20 !$omp masked \nS-21 !$omp task in_reduction(+:asum) !! task 2 \nS-22 do i=1,N; asum = asum + a(i); enddo \nS-23 !$omp end task \nS-24 !$omp end masked \nS-25 \nS-26 !$omp masked taskloop in_reduction(+:asum) !! taskloop 2 \nS-27 do i=1,N; asum = asum + a(i); enddo \nS-28 !$omp end masked taskloop \nS-29 \nCHAPTER 10. DATA ENVIRONMENT 419 \nS-30 !$omp end parallel \nS-31 \nS-32 !! taskloop simd reductions \nS-33 \nS-34 !$omp parallel masked \nS-35 !$omp taskloop simd reduction(+:asum) !! taskloop simd 3 \nS-36 do i=1,N; asum = asum + a(i); enddo \nS-37 !$omp end taskloop simd \nS-38 !$omp end parallel masked \nS-39 \nS-40 \nS-41 !$omp parallel reduction(task, +:asum) !! parallel reduction b \nS-42 \nS-43 !$omp masked \nS-44 !$omp task in_reduction(+:asum) !! task 4 \nS-45 do i=1,N; asum = asum + a(i); enddo \nS-46 !$omp end task \nS-47 !$omp end masked \nS-48 \nS-49 !$omp masked taskloop simd in_reduction(+:asum) !! taskloop simd 4 \nS-50 do i=1,N; asum = asum + a(i); enddo \nS-51 !$omp end masked taskloop simd \nS-52 \nS-53 !$omp end parallel \nS-54 \nS-55 print*,\"asum=\",asum !! output: asum=30300 \nS-56 \nS-57 end program \nFortran \n"}
{"section_title": "10.9.6 Reduction with the scope Construct", "chunk": ""}
{"section_title": "10.9.6 Reduction with the scope Construct", "chunk": "2 The following example illustrates the use of the scope construct to perform a reduction in a \n3 parallel region. The case is useful for producing a reduction and accessing reduction variables \n4 inside a parallel region without using a worksharing-loop construct. \n420 OpenMP Examples Version 5.2.1 - November 2022 \nC++ \n1 Example scope_reduction.1.cpp (omp_5.1) \nS-1 #include <stdio.h> \nS-2 void do_work(int n, float a[], float &s) \nS-3 { \nS-4 float loc_s = 0.0f; // local sum \nS-5 static int nthrs; \nS-6 #pragma omp for \nS-7 for (int i = 0; i < n; i++) \nS-8 loc_s += a[i]; \nS-9 #pragma omp single \nS-10 { \nS-11 s = 0.0f; // total sum \nS-12 nthrs = 0; \nS-13 } \nS-14 #pragma omp scope reduction(+:s,nthrs) \nS-15 { \nS-16 s += loc_s; \nS-17 nthrs++; \nS-18 } \nS-19 #pragma omp masked \nS-20 printf(\"total sum = %f, nthrs = %d\\n\", s, nthrs); \nS-21 } \nS-22 \nS-23 float work(int n, float a[]) \nS-24 { \nS-25 float s; \nS-26 #pragma omp parallel \nS-27 { \nS-28 do_work(n, a, s); \nS-29 } \nS-30 return s; \nS-31 } \nC++ \nCHAPTER 10. DATA ENVIRONMENT 421 \nFortran \n1 Example scope_reduction.1.f90 (omp_5.1) \nS-1 subroutine do_work(n, a, s) \nS-2 implicit none \nS-3 integer n, i \nS-4 real a(*), s, loc_s \nS-5 integer, save :: nthrs \nS-6 \nS-7 loc_s = 0.0 ! local sum \nS-8 !$omp do \nS-9 do i = 1, n \nS-10 loc_s = loc_s + a(i) \nS-11 end do \nS-12 !$omp single \nS-13 s = 0.0 ! total sum \nS-14 nthrs = 0 \nS-15 !$omp end single \nS-16 !$omp scope reduction(+:s,nthrs) \nS-17 s = s + loc_s \nS-18 nthrs = nthrs + 1 \nS-19 !$omp end scope \nS-20 !$omp masked \nS-21 print *, \"total sum = \", s, \", nthrs = \", nthrs \nS-22 !$omp end masked \nS-23 end subroutine \nS-24 \nS-25 function work(n, a) result(s) \nS-26 implicit none \nS-27 integer n \nS-28 real a(*), s \nS-29 \nS-30 !$omp parallel \nS-31 call do_work(n, a, s) \nS-32 !$omp end parallel \nS-33 end function \nFortran \n"}
{"section_title": "10.9.7 User-Defined Reduction", "chunk": ""}
{"section_title": "10.9.7 User-Defined Reduction", "chunk": "3 The declare reduction directive can be used to specify user-defined reductions (UDR) for \n4 user data types. \n5 In the following example, declare reduction directives are used to define min and max \n6 operations for the point data structure for computing the rectangle that encloses a set of 2-D points. \n422 OpenMP Examples Version 5.2.1 - November 2022 \n1 Each declare reduction directive defines new reduction identifiers, min and max, to be used \n2 in a reduction clause. The next item in the declaration list is the data type (struct point) used in \n3 the reduction, followed by the combiner, here the functions minproc and maxproc perform the min \n4 and max operations, respectively, on the user data (of type struct point). In the function argument \n5 list are two special OpenMP variable identifiers, omp_in and omp_out, that denote the two \n6 values to be combined in the \u201creal\u201d function; the omp_out identifier indicates which one is to hold \n7 the result. \n8 The initializer of the declare reduction directive specifies the initial value for the private \n9 variable of each implicit task. The omp_priv identifier is used to denote the private variable. \nC / C++ \n10 Example udr.1.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 #include <limits.h> \nS-3 \nS-4 struct point { \nS-5 int x; \nS-6 int y; \nS-7 }; \nS-8 \nS-9 void minproc ( struct point *out, struct point *in ) \nS-10 { \nS-11 if ( in->x < out->x ) out->x = in->x; \nS-12 if ( in->y < out->y ) out->y = in->y; \nS-13 } \nS-14 \nS-15 void maxproc ( struct point *out, struct point *in ) \nS-16 { \nS-17 if ( in->x > out->x ) out->x = in->x; \nS-18 if ( in->y > out->y ) out->y = in->y; \nS-19 } \nS-20 \nS-21 #pragma omp declare reduction(min : struct point : \\ \nS-22 minproc(&omp_out, &omp_in)) \\ \nS-23 initializer( omp_priv = { INT_MAX, INT_MAX } ) \nS-24 \nS-25 #pragma omp declare reduction(max : struct point : \\ \nS-26 maxproc(&omp_out, &omp_in)) \\ \nS-27 initializer( omp_priv = { 0, 0 } ) \nS-28 \nS-29 void find_enclosing_rectangle ( int n, struct point points[] ) \nS-30 { \nS-31 struct point minp = { INT_MAX, INT_MAX }, maxp = {0,0}; \nS-32 int i; \nS-33 \nCHAPTER 10. DATA ENVIRONMENT 423 \nS-34 #pragma omp parallel for reduction(min:minp) reduction(max:maxp) \nS-35 for ( i = 0; i < n; i++ ) { \nS-36 minproc(&minp, &points[i]); \nS-37 maxproc(&maxp, &points[i]); \nS-38 } \nS-39 printf(\"min = (%d, %d)\\n\", minp.x, minp.y); \nS-40 printf(\"max = (%d, %d)\\n\", maxp.x, maxp.y); \nS-41 } \nC / C++ \n1 The following example shows the corresponding code in Fortran. The declare reduction \n2 directives are specified as part of the declaration in subroutine find_enclosing_rectangle and the \n3 procedures that perform the min and max operations are specified as subprograms. \nFortran \n4 Example udr.1.f90 (omp_4.0) \nS-1 module data_type \nS-2 \nS-3 type :: point \nS-4 integer :: x \nS-5 integer :: y \nS-6 end type \nS-7 \nS-8 end module data_type \nS-9 \nS-10 subroutine find_enclosing_rectangle ( n, points ) \nS-11 use data_type \nS-12 implicit none \nS-13 integer :: n \nS-14 type(point) :: points(*) \nS-15 \nS-16 !$omp declare reduction(min : point : minproc(omp_out, omp_in)) & \nS-17 !$omp& initializer( omp_priv = point( HUGE(0), HUGE(0) ) ) \nS-18 \nS-19 !$omp declare reduction(max : point : maxproc(omp_out, omp_in)) & \nS-20 !$omp& initializer( omp_priv = point( 0, 0 ) ) \nS-21 \nS-22 type(point) :: minp = point( HUGE(0), HUGE(0) ), maxp = point( 0, 0 ) \nS-23 integer :: i \nS-24 \nS-25 !$omp parallel do reduction(min:minp) reduction(max:maxp) \nS-26 do i = 1, n \nS-27 call minproc(minp, points(i)) \nS-28 call maxproc(maxp, points(i)) \nS-29 end do \nS-30 print *, \"min = (\", minp%x, minp%y, \")\" \n424 OpenMP Examples Version 5.2.1 - November 2022 \nS-31 print *, \"max = (\", maxp%x, maxp%y, \")\" \nS-32 \nS-33 contains \nS-34 subroutine minproc ( out, in ) \nS-35 implicit none \nS-36 type(point), intent(inout) :: out \nS-37 type(point), intent(in) :: in \nS-38 \nS-39 out%x = min( out%x, in%x ) \nS-40 out%y = min( out%y, in%y ) \nS-41 end subroutine minproc \nS-42 \nS-43 subroutine maxproc ( out, in ) \nS-44 implicit none \nS-45 type(point), intent(inout) :: out \nS-46 type(point), intent(in) :: in \nS-47 \nS-48 out%x = max( out%x, in%x ) \nS-49 out%y = max( out%y, in%y ) \nS-50 end subroutine maxproc \nS-51 \nS-52 end subroutine \nFortran \n1 The following example shows the same computation as udr.1 but it illustrates that you can craft \n2 complex expressions in the user-defined reduction declaration. In this case, instead of calling the \n3 minproc and maxproc functions we inline the code in a single expression. \nC / C++ \n4 Example udr.2.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 #include <limits.h> \nS-3 \nS-4 struct point { \nS-5 int x; \nS-6 int y; \nS-7 }; \nS-8 \nS-9 #pragma omp declare reduction(min : struct point : \\ \nS-10 omp_out.x = omp_in.x > omp_out.x ? omp_out.x : omp_in.x, \\ \nS-11 omp_out.y = omp_in.y > omp_out.y ? omp_out.y : omp_in.y ) \\ \nS-12 initializer( omp_priv = { INT_MAX, INT_MAX } ) \nS-13 \nS-14 #pragma omp declare reduction(max : struct point : \\ \nS-15 omp_out.x = omp_in.x < omp_out.x ? omp_out.x : omp_in.x, \\ \nS-16 omp_out.y = omp_in.y < omp_out.y ? omp_out.y : omp_in.y ) \\ \nCHAPTER 10. DATA ENVIRONMENT 425 \nS-17 initializer( omp_priv = { 0, 0 } ) \nS-18 \nS-19 void find_enclosing_rectangle ( int n, struct point points[] ) \nS-20 { \nS-21 struct point minp = { INT_MAX, INT_MAX }, maxp = {0,0}; \nS-22 int i; \nS-23 \nS-24 #pragma omp parallel for reduction(min:minp) reduction(max:maxp) \nS-25 for ( i = 0; i < n; i++ ) { \nS-26 if ( points[i].x < minp.x ) minp.x = points[i].x; \nS-27 if ( points[i].y < minp.y ) minp.y = points[i].y; \nS-28 if ( points[i].x > maxp.x ) maxp.x = points[i].x; \nS-29 if ( points[i].y > maxp.y ) maxp.y = points[i].y; \nS-30 } \nS-31 printf(\"min = (%d, %d)\\n\", minp.x, minp.y); \nS-32 printf(\"max = (%d, %d)\\n\", maxp.x, maxp.y); \nS-33 } \nC / C++ \n1 The corresponding code of the same example in Fortran is very similar except that the assignment \n2 expression in the declare reduction directive can only be used for a single variable, in this \n3 case through a type structure constructor point(. . .). \nFortran \n4 Example udr.2.f90 (omp_4.0) \nS-1 module data_type \nS-2 \nS-3 type :: point \nS-4 integer :: x \nS-5 integer :: y \nS-6 end type \nS-7 \nS-8 end module data_type \nS-9 \nS-10 subroutine find_enclosing_rectangle ( n, points ) \nS-11 use data_type \nS-12 implicit none \nS-13 integer :: n \nS-14 type(point) :: points(*) \nS-15 \nS-16 !$omp declare reduction( min : point : & \nS-17 !$omp& omp_out = point(min( omp_out%x, omp_in%x ), & \nS-18 !$omp& min( omp_out%y, omp_in%y )) ) & \nS-19 !$omp& initializer( omp_priv = point( HUGE(0), HUGE(0) ) ) \nS-20 \nS-21 !$omp declare reduction( max : point : & \n426 OpenMP Examples Version 5.2.1 - November 2022 \nS-22 !$omp& omp_out = point(max( omp_out%x, omp_in%x ), & \nS-23 !$omp& max( omp_out%y, omp_in%y )) ) & \nS-24 !$omp& initializer( omp_priv = point( 0, 0 ) ) \nS-25 \nS-26 type(point) :: minp = point( HUGE(0), HUGE(0) ), maxp = point( 0, 0 ) \nS-27 integer :: i \nS-28 \nS-29 !$omp parallel do reduction(min: minp) reduction(max: maxp) \nS-30 do i = 1, n \nS-31 minp%x = min(minp%x, points(i)%x) \nS-32 minp%y = min(minp%y, points(i)%y) \nS-33 maxp%x = max(maxp%x, points(i)%x) \nS-34 maxp%y = max(maxp%y, points(i)%y) \nS-35 end do \nS-36 print *, \"min = (\", minp%x, minp%y, \")\" \nS-37 print *, \"max = (\", maxp%x, maxp%y, \")\" \nS-38 \nS-39 end subroutine \nFortran \n1 The following example shows the use of special variables in arguments for combiner (omp_in and \n2 omp_out) and initializer (omp_priv and omp_orig) routines. This example returns the \n3 maximum value of an array and the corresponding index value. The declare reduction \n4 directive specifies a user-defined reduction operation maxloc for data type struct mx_s. The \n5 function mx_combine is the combiner and the function mx_init is the initializer. \nC / C++ \n6 Example udr.3.c (omp_4.0) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 \nS-4 struct mx_s { \nS-5 float value; \nS-6 int index; \nS-7 }; \nS-8 \nS-9 /* prototype functions for combiner and initializer in \nS-10 the declare reduction */ \nS-11 void mx_combine(struct mx_s *out, struct mx_s *in); \nS-12 void mx_init(struct mx_s *priv, struct mx_s *orig); \nS-13 \nS-14 #pragma omp declare reduction(maxloc: struct mx_s: \\ \nS-15 mx_combine(&omp_out, &omp_in)) \\ \nS-16 initializer(mx_init(&omp_priv, &omp_orig)) \nS-17 \nS-18 void mx_combine(struct mx_s *out, struct mx_s *in) \nCHAPTER 10. DATA ENVIRONMENT 427 \nS-19 { \nS-20 if ( out->value < in->value ) { \nS-21 out->value = in->value; \nS-22 out->index = in->index; \nS-23 } \nS-24 } \nS-25 \nS-26 void mx_init(struct mx_s *priv, struct mx_s *orig) \nS-27 { \nS-28 priv->value = orig->value; \nS-29 priv->index = orig->index; \nS-30 } \nS-31 \nS-32 int main(void) \nS-33 { \nS-34 struct mx_s mx; \nS-35 float val[N], d; \nS-36 int i, count = N; \nS-37 \nS-38 for (i = 0; i < count; i++) { \nS-39 d = (N*0.8f - i); \nS-40 val[i] = N * N - d * d; \nS-41 } \nS-42 \nS-43 mx.value = val[0]; \nS-44 mx.index = 0; \nS-45 #pragma omp parallel for reduction(maxloc: mx) \nS-46 for (i = 1; i < count; i++) { \nS-47 if (mx.value < val[i]) \nS-48 { \nS-49 mx.value = val[i]; \nS-50 mx.index = i; \nS-51 } \nS-52 } \nS-53 \nS-54 printf(\"max value = %g, index = %d\\n\", mx.value, mx.index); \nS-55 /* prints 10000, 80 */ \nS-56 \nS-57 return 0; \nS-58 } \nC / C++ \n1 Below is the corresponding Fortran version of the above example. The declare reduction \n2 directive specifies the user-defined operation maxloc for user-derived type mx_s. The combiner \n3 mx_combine and the initializer mx_init are specified as subprograms. \n428 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example udr.3.f90 (omp_4.0) \nS-1 program max_loc \nS-2 implicit none \nS-3 \nS-4 type :: mx_s \nS-5 real value \nS-6 integer index \nS-7 end type \nS-8 \nS-9 !$omp declare reduction(maxloc: mx_s: & \nS-10 !$omp& mx_combine(omp_out, omp_in)) & \nS-11 !$omp& initializer(mx_init(omp_priv, omp_orig)) \nS-12 \nS-13 integer, parameter :: N = 100 \nS-14 type(mx_s) :: mx \nS-15 real :: val(N), d \nS-16 integer :: i, count \nS-17 \nS-18 count = N \nS-19 do i = 1, count \nS-20 d = N*0.8 - i + 1 \nS-21 val(i) = N * N - d * d \nS-22 enddo \nS-23 \nS-24 mx%value = val(1) \nS-25 mx%index = 1 \nS-26 !$omp parallel do reduction(maxloc: mx) \nS-27 do i = 2, count \nS-28 if (mx%value < val(i)) then \nS-29 mx%value = val(i) \nS-30 mx%index = i \nS-31 endif \nS-32 enddo \nS-33 \nS-34 print *, \u2019max value = \u2019, mx%value, \u2019 index = \u2019, mx%index \nS-35 ! prints 10000, 81 \nS-36 \nS-37 contains \nS-38 \nS-39 subroutine mx_combine(out, in) \nS-40 implicit none \nS-41 type(mx_s), intent(inout) :: out \nS-42 type(mx_s), intent(in) :: in \nS-43 \nS-44 if ( out%value < in%value ) then \nCHAPTER 10. DATA ENVIRONMENT 429 \nS-45 out%value = in%value \nS-46 out%index = in%index \nS-47 endif \nS-48 end subroutine mx_combine \nS-49 \nS-50 subroutine mx_init(priv, orig) \nS-51 implicit none \nS-52 type(mx_s), intent(out) :: priv \nS-53 type(mx_s), intent(in) :: orig \nS-54 \nS-55 priv%value = orig%value \nS-56 priv%index = orig%index \nS-57 end subroutine mx_init \nS-58 \nS-59 end program \nFortran \n1 The following example explains a few details of the user-defined reduction in Fortran through \n2 modules. The declare reduction directive is declared in a module (data_red). The \n3 reduction-identifier .add. is a user-defined operator that is to allow accessibility in the scope that \n4 performs the reduction operation. The user-defined operator .add. and the subroutine dt_init \n5 specified in the initializer clause are defined in the same subprogram. \n6 The reduction operation (that is, the reduction clause) is in the main program. The reduction \n7 identifier .add. is accessible by use association. Since .add. is a user-defined operator, the explicit \n8 interface should also be accessible by use association in the current program unit. Since the \n9 declare reduction associated to this reduction clause has the initializer clause, the \n10 subroutine specified on the clause must be accessible in the current scoping unit. In this case, the \n11 subroutine dt_init is accessible by use association. \nFortran \n12 Example udr.4.f90 (omp_4.0) \nS-1 module data_red \nS-2 ! Declare data type. \nS-3 type dt \nS-4 real :: r1 \nS-5 real :: r2 \nS-6 end type \nS-7 \nS-8 ! Declare the user-defined operator .add. \nS-9 interface operator(.add.) \nS-10 module procedure addc \nS-11 end interface \nS-12 \nS-13 ! Declare the user-defined reduction operator .add. \nS-14 !$omp declare reduction(.add.:dt:omp_out=omp_out.add.omp_in) & \n430 OpenMP Examples Version 5.2.1 - November 2022 \nS-15 !$omp& initializer(dt_init(omp_priv)) \nS-16 \nS-17 contains \nS-18 ! Declare the initialization routine. \nS-19 subroutine dt_init(u) \nS-20 type(dt) :: u \nS-21 u%r1 = 0.0 \nS-22 u%r2 = 0.0 \nS-23 end subroutine \nS-24 \nS-25 ! Declare the specific procedure for the .add. operator. \nS-26 function addc(x1, x2) result(xresult) \nS-27 type(dt), intent(in) :: x1, x2 \nS-28 type(dt) :: xresult \nS-29 xresult%r1 = x1%r1 + x2%r2 \nS-30 xresult%r2 = x1%r2 + x2%r1 \nS-31 end function \nS-32 \nS-33 end module data_red \nS-34 \nS-35 program main \nS-36 use data_red, only : dt, dt_init, operator(.add.) \nS-37 \nS-38 type(dt) :: xdt1, xdt2 \nS-39 integer :: i \nS-40 \nS-41 xdt1 = dt(1.0,2.0) \nS-42 xdt2 = dt(2.0,3.0) \nS-43 \nS-44 ! The reduction operation \nS-45 !$omp parallel do reduction(.add.: xdt1) \nS-46 do i = 1, 10 \nS-47 xdt1 = xdt1 .add. xdt2 \nS-48 end do \nS-49 !$omp end parallel do \nS-50 \nS-51 print *, xdt1 \nS-52 \nS-53 end program \nFortran \n1 The following example uses user-defined reductions to declare a plus (+) reduction for a C++ class. \n2 As the declare reduction directive is inside the context of the V class the expressions in the \n3 declare reduction directive are resolved in the context of the class. Also, note that the \n4 initializer clause uses a copy constructor to initialize the private variables of the reduction \n5 and it uses as parameter to its original variable by using the special variable omp_orig. \nCHAPTER 10. DATA ENVIRONMENT 431 \nC++ \n1 Example udr.5.cpp (omp_4.0) \nS-1 class V { \nS-2 float *p; \nS-3 int n; \nS-4 \nS-5 public: \nS-6 V( int _n ) : n(_n) { p = new float[n]; } \nS-7 V( const V& m ) : n(m.n) { p = new float[n]; } \nS-8 ~V() { delete[] p; } \nS-9 \nS-10 V& operator+= ( const V& ); \nS-11 \nS-12 #pragma omp declare reduction( + : V : omp_out += omp_in ) \\ \nS-13 initializer(omp_priv(omp_orig)) \nS-14 }; \nC++ \n2 The following examples shows how user-defined reductions can be defined for some STL \n3 containers. The first declare reduction defines the plus (+) operation for std::vector<int> by \n4 making use of the std::transform algorithm. The second and third define the merge (or \n5 concatenation) operation for std::vector<int> and std::list<int>. It shows how the user-defined \n6 reduction operation can be applied to specific data types of an STL. \nC++ \n7 Example udr.6.cpp (omp_4.0) \nS-1 #include <algorithm> \nS-2 #include <list> \nS-3 #include <vector> \nS-4 \nS-5 #pragma omp declare reduction( + : std::vector<int> : \\ \nS-6 std::transform (omp_out.begin(), omp_out.end(), \\ \nS-7 omp_in.begin(), omp_in.end(),std::plus<int>())) \nS-8 \nS-9 #pragma omp declare reduction( merge : std::vector<int> : \\ \nS-10 omp_out.insert(omp_out.end(), omp_in.begin(), omp_in.end())) \nS-11 \nS-12 #pragma omp declare reduction( merge : std::list<int> : \\ \nS-13 omp_out.merge(omp_in)) \nC++ \n432 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.10 scan Directive", "chunk": ""}
{"section_title": "10.10 scan Directive", "chunk": "2 The following examples illustrate how to parallelize a loop that saves the prefix sum of a reduction. \n3 This is accomplished by using the inscan modifier in the reduction clause for the input \n4 variable of the scan, and specifying with a scan directive whether the storage statement includes \n5 or excludes the scan input of the present iteration (k). \n6 Basically, the inscan modifier connects a loop and/or SIMD reduction to the scan operation, and \n7 a scan construct with an inclusive or exclusive clause specifies whether the \u201cscan phase\u201d \n8 (lexical block before and after the directive, respectively) is to use an inclusive or exclusive scan \n9 value for the list item (x). \n10 The first example uses the inclusive scan operation on a composite loop-SIMD construct. The \n11 scan directive separates the reduction statement on variable x from the use of x (saving to array \n12 b). The order of the statements in this example indicates that value a[k] (a(k) in Fortran) is \n13 included in the computation of the prefix sum b[k] (b(k) in Fortran) for iteration k. \nC / C++ \n14 Example scan.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 \nS-4 int main(void) \nS-5 { \nS-6 int a[N], b[N]; \nS-7 int x = 0; \nS-8 \nS-9 // initialization \nS-10 for (int k = 0; k < N; k++) \nS-11 a[k] = k + 1; \nS-12 \nS-13 // a[k] is included in the computation of producing results in b[k] \nS-14 #pragma omp parallel for simd reduction(inscan,+: x) \nS-15 for (int k = 0; k < N; k++) { \nS-16 x += a[k]; \nS-17 #pragma omp scan inclusive(x) \nS-18 b[k] = x; \nS-19 } \nS-20 \nS-21 printf(\"x = %d, b[0:3] = %d %d %d\\n\", x, b[0], b[1], b[2]); \nS-22 // 5050, 1 3 6 \nS-23 \nS-24 return 0; \nS-25 } \nC / C++ \nCHAPTER 10. DATA ENVIRONMENT 433 \nFortran \n1 Example scan.1.f90 (omp_5.0) \nS-1 program inclusive_scan \nS-2 implicit none \nS-3 integer, parameter :: n = 100 \nS-4 integer a(n), b(n) \nS-5 integer x, k \nS-6 \nS-7 ! initialization \nS-8 x = 0 \nS-9 do k = 1, n \nS-10 a(k) = k \nS-11 end do \nS-12 \nS-13 ! a(k) is included in the computation of producing results in b(k) \nS-14 !$omp parallel do simd reduction(inscan,+: x) \nS-15 do k = 1, n \nS-16 x = x + a(k) \nS-17 !$omp scan inclusive(x) \nS-18 b(k) = x \nS-19 end do \nS-20 \nS-21 print *,\u2019x =\u2019, x, \u2019, b(1:3) =\u2019, b(1:3) \nS-22 ! 5050, 1 3 6 \nS-23 \nS-24 end program \nFortran \n2 The second example uses the exclusive scan operation on a composite loop-SIMD construct. The \n3 scan directive separates the use of x (saving to array b) from the reduction statement on variable \n4 x. The order of the statements in this example indicates that value a[k] (a(k) in Fortran) is \n5 excluded from the computation of the prefix sum b[k] (b(k) in Fortran) for iteration k. \nC / C++ \n6 Example scan.2.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #define N 100 \nS-3 \nS-4 int main(void) \nS-5 { \nS-6 int a[N], b[N]; \nS-7 int x = 0; \nS-8 \nS-9 // initialization \n434 OpenMP Examples Version 5.2.1 - November 2022 \nS-10 for (int k = 0; k < N; k++) \nS-11 a[k] = k + 1; \nS-12 \nS-13 // a[k] is not included in the computation of producing \nS-14 // results in b[k] \nS-15 #pragma omp parallel for simd reduction(inscan,+: x) \nS-16 for (int k = 0; k < N; k++) { \nS-17 b[k] = x; \nS-18 #pragma omp scan exclusive(x) \nS-19 x += a[k]; \nS-20 } \nS-21 \nS-22 printf(\"x = %d, b[0:3] = %d %d %d\\n\", x, b[0], b[1], b[2]); \nS-23 // 5050, 0 1 3 \nS-24 \nS-25 return 0; \nS-26 } \nC / C++ \nFortran \n1 Example scan.2.f90 (omp_5.0) \nS-1 program exclusive_scan \nS-2 implicit none \nS-3 integer, parameter :: n = 100 \nS-4 integer a(n), b(n) \nS-5 integer x, k \nS-6 \nS-7 ! initialization \nS-8 x = 0 \nS-9 do k = 1, n \nS-10 a(k) = k \nS-11 end do \nS-12 \nS-13 ! a(k) is not included in the computation of producing results in b(k) \nS-14 !$omp parallel do simd reduction(inscan,+: x) \nS-15 do k = 1, n \nS-16 b(k) = x \nS-17 !$omp scan exclusive(x) \nS-18 x = x + a(k) \nS-19 end do \nS-20 \nS-21 print *,\u2019x =\u2019, x, \u2019, b(1:3) =\u2019, b(1:3) \nS-22 ! 5050, 0 1 3 \nS-23 \nS-24 end program \nFortran \nCHAPTER 10. DATA ENVIRONMENT 435 \n"}
{"section_title": "10.11 copyin Clause", "chunk": ""}
{"section_title": "10.11 copyin Clause", "chunk": "2 The copyin clause is used to initialize threadprivate data upon entry to a parallel region. The \n3 value of the threadprivate variable in the primary thread is copied to the threadprivate variable of \n4 each other team member. \nC / C++ \n5 Example copyin.1.c (pre_omp_3.0) \nS-1 #include <stdlib.h> \nS-2 \nS-3 float* work; \nS-4 int size; \nS-5 float tol; \nS-6 \nS-7 #pragma omp threadprivate(work,size,tol) \nS-8 \nS-9 void build() \nS-10 { \nS-11 int i; \nS-12 work = (float*)malloc( sizeof(float)*size ); \nS-13 for( i = 0; i < size; ++i ) work[i] = tol; \nS-14 } \nS-15 \nS-16 void copyin_example( float t, int n ) \nS-17 { \nS-18 tol = t; \nS-19 size = n; \nS-20 #pragma omp parallel copyin(tol,size) \nS-21 { \nS-22 build(); \nS-23 } \nS-24 } \nC / C++ \n436 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example copyin.1.f (pre_omp_3.0) \nS-1 MODULE M \nS-2 REAL, POINTER, SAVE :: WORK(:) \nS-3 INTEGER :: SIZE \nS-4 REAL :: TOL \nS-5 !$OMP THREADPRIVATE(WORK,SIZE,TOL) \nS-6 END MODULE M \nS-7 \nS-8 SUBROUTINE COPYIN_EXAMPLE( T, N ) \nS-9 USE M \nS-10 REAL :: T \nS-11 INTEGER :: N \nS-12 TOL = T \nS-13 SIZE = N \nS-14 !$OMP PARALLEL COPYIN(TOL,SIZE) \nS-15 CALL BUILD \nS-16 !$OMP END PARALLEL \nS-17 END SUBROUTINE COPYIN_EXAMPLE \nS-18 \nS-19 SUBROUTINE BUILD \nS-20 USE M \nS-21 ALLOCATE(WORK(SIZE)) \nS-22 WORK = TOL \nS-23 END SUBROUTINE BUILD \nFortran \nCHAPTER 10. DATA ENVIRONMENT 437 \n"}
{"section_title": "10.12 copyprivate Clause", "chunk": ""}
{"section_title": "10.12 copyprivate Clause", "chunk": "2 The copyprivate clause can be used to broadcast values acquired by a single thread directly to \n3 all instances of the private variables in the other threads. In this example, if the routine is called \n4 from the sequential part, its behavior is not affected by the presence of the directives. If it is called \n5 from a parallel region, then the actual arguments with which a and b are associated must be \n6 private. \n7 The thread that executes the structured block associated with the single construct broadcasts the \n8 values of the private variables a, b, x, and y from its implicit task\u2019s data environment to the data \n9 environments of the other implicit tasks in the thread team. The broadcast completes before any of \n10 the threads have left the barrier at the end of the construct. \nC / C++ \n11 Example copyprivate.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 float x, y; \nS-3 #pragma omp threadprivate(x, y) \nS-4 \nS-5 void init(float a, float b ) { \nS-6 #pragma omp single copyprivate(a,b,x,y) \nS-7 { \nS-8 scanf(\"%f %f %f %f\", &a, &b, &x, &y); \nS-9 } \nS-10 } \nC / C++ \nFortran \n12 Example copyprivate.1.f (pre_omp_3.0) \nS-1 SUBROUTINE INIT(A,B) \nS-2 REAL A, B \nS-3 COMMON /XY/ X,Y \nS-4 !$OMP THREADPRIVATE (/XY/) \nS-5 \nS-6 !$OMP SINGLE \nS-7 READ (11) A,B,X,Y \nS-8 !$OMP END SINGLE COPYPRIVATE (A,B,/XY/) \nS-9 \nS-10 END SUBROUTINE INIT \nFortran \n13 In this example, assume that the input must be performed by the primary thread. Since the masked \n14 construct does not support the copyprivate clause, it cannot broadcast the input value that is \n15 read. However, copyprivate is used to broadcast an address where the input value is stored. \n438 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example copyprivate.2.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 \nS-4 float read_next( ) { \nS-5 float * tmp; \nS-6 float return_val; \nS-7 \nS-8 #pragma omp single copyprivate(tmp) \nS-9 { \nS-10 tmp = (float *) malloc(sizeof(float)); \nS-11 } /* copies the pointer only */ \nS-12 \nS-13 \nS-14 #pragma omp masked \nS-15 { \nS-16 scanf(\"%f\", tmp); \nS-17 } \nS-18 \nS-19 #pragma omp barrier \nS-20 return_val = *tmp; \nS-21 #pragma omp barrier \nS-22 \nS-23 #pragma omp single nowait \nS-24 { \nS-25 free(tmp); \nS-26 } \nS-27 \nS-28 return return_val; \nS-29 } \nC / C++ \nFortran \n2 Example copyprivate.2.f (omp_5.1) \nS-1 REAL FUNCTION READ_NEXT() \nS-2 REAL, POINTER :: TMP \nS-3 \nS-4 !$OMP SINGLE \nS-5 ALLOCATE (TMP) \nS-6 !$OMP END SINGLE COPYPRIVATE (TMP) ! copies the pointer only \nS-7 \nS-8 !$OMP MASKED \nS-9 READ (11) TMP \nS-10 !$OMP END MASKED \nCHAPTER 10. DATA ENVIRONMENT 439 \nS-11 \nS-12 !$OMP BARRIER \nS-13 READ_NEXT = TMP \nS-14 !$OMP BARRIER \nS-15 \nS-16 !$OMP SINGLE \nS-17 DEALLOCATE (TMP) \nS-18 !$OMP END SINGLE NOWAIT \nS-19 END FUNCTION READ_NEXT \nFortran \n1 Suppose that the number of lock variables required within a parallel region cannot easily be \n2 determined prior to entering it. The copyprivate clause can be used to provide access to shared \n3 lock variables that are allocated within that parallel region. \nC / C++ \n4 Example copyprivate.3.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <omp.h> \nS-4 \nS-5 omp_lock_t *new_lock() \nS-6 { \nS-7 omp_lock_t *lock_ptr; \nS-8 \nS-9 #pragma omp single copyprivate(lock_ptr) \nS-10 { \nS-11 lock_ptr = (omp_lock_t *) malloc(sizeof(omp_lock_t)); \nS-12 omp_init_lock( lock_ptr ); \nS-13 } \nS-14 \nS-15 return lock_ptr; \nS-16 } \nC / C++ \n440 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example copyprivate.3.f (pre_omp_3.0) \nS-1 FUNCTION NEW_LOCK() \nS-2 USE OMP_LIB ! or INCLUDE \"omp_lib.h\" \nS-3 INTEGER(OMP_LOCK_KIND), POINTER :: NEW_LOCK \nS-4 \nS-5 !$OMP SINGLE \nS-6 ALLOCATE(NEW_LOCK) \nS-7 CALL OMP_INIT_LOCK(NEW_LOCK) \nS-8 !$OMP END SINGLE COPYPRIVATE(NEW_LOCK) \nS-9 END FUNCTION NEW_LOCK \n2 Note that the effect of the copyprivate clause on a variable with the allocatable attribute \n3 is different than on a variable with the pointer attribute. The value of A is copied (as if by \n4 intrinsic assignment) and the pointer B is copied (as if by pointer assignment) to the corresponding \n5 list items in the other implicit tasks belonging to the parallel region. \n6 Example copyprivate.4.f (pre_omp_3.0) \nS-1 SUBROUTINE S(N) \nS-2 INTEGER N \nS-3 \nS-4 REAL, DIMENSION(:), ALLOCATABLE :: A \nS-5 REAL, DIMENSION(:), POINTER :: B \nS-6 \nS-7 ALLOCATE (A(N)) \nS-8 !$OMP SINGLE \nS-9 ALLOCATE (B(N)) \nS-10 READ (11) A,B \nS-11 !$OMP END SINGLE COPYPRIVATE(A,B) \nS-12 ! Variable A is private and is \nS-13 ! assigned the same value in each thread \nS-14 ! Variable B is shared \nS-15 \nS-16 !$OMP BARRIER \nS-17 !$OMP SINGLE \nS-18 DEALLOCATE (B) \nS-19 !$OMP END SINGLE NOWAIT \nS-20 END SUBROUTINE S \nFortran \nCHAPTER 10. DATA ENVIRONMENT 441 \n"}
{"section_title": "10.13 C++ Reference in Data-Sharing Clauses", "chunk": ""}
{"section_title": "10.13 C++ Reference in Data-Sharing Clauses", "chunk": "C++ \n2 C++ reference types are allowed in data-sharing attribute clauses as of OpenMP 4.5, except for the \n3 threadprivate, copyin and copyprivate clauses. (See the Data-Sharing Attribute \n4 Clauses Section of the 4.5 OpenMP specification.) When a variable with C++ reference type is \n5 privatized, the object the reference refers to is privatized in addition to the reference itself. The \n6 following example shows the use of reference types in data-sharing clauses in the usual way. \n7 Additionally it shows how the data-sharing of formal arguments with a C++ reference type on an \n8 orphaned task generating construct is determined implicitly. (See the Data-sharing Attribute Rules \n9 for Variables Referenced in a Construct Section of the 4.5 OpenMP specification.) \n10 Example cpp_reference.1.cpp (omp_4.5) \nS-1 void task_body (int &); \nS-2 void gen_task (int &x) { // on orphaned task construct reference argument \nS-3 #pragma omp task // x is implicitly determined firstprivate(x) \nS-4 task_body (x); \nS-5 } \nS-6 void test (int &y, int &z) { \nS-7 #pragma omp parallel private(y) \nS-8 { \nS-9 y = z + 2; \nS-10 gen_task (y); // no matter if the argument is determined private \nS-11 gen_task (z); // or shared in the enclosing context. \nS-12 \nS-13 y++; // each thread has its own int object y refers to \nS-14 gen_task (y); \nS-15 } \nS-16 } \nC++ \n442 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "10.14 Fortran ASSOCIATE Construct", "chunk": ""}
{"section_title": "10.14 Fortran ASSOCIATE Construct", "chunk": "Fortran \n2 The following is an invalid example of specifying an associate name on a data-sharing attribute \n3 clause. The constraint in the Data Sharing Attribute Rules section in the OpenMP 4.0 API \n4 Specifications states that an associate name preserves the association with the selector established \n5 at the ASSOCIATE statement. The associate name b is associated with the shared variable a. With \n6 the predetermined data-sharing attribute rule, the associate name b is not allowed to be specified on \n7 the private clause. \n8 Example associate.1.f (omp_4.0) \nS-1 program example_broken \nS-2 real :: a, c \nS-3 associate (b => a) \nS-4 !$omp parallel private(b, c) ! invalid to privatize b \nS-5 c = 2.0*b \nS-6 !$omp end parallel \nS-7 end associate \nS-8 end program \n9 In next example, within the parallel construct, the association name thread_id is associated \n10 with the private copy of i. The print statement should output the unique thread number. \n11 Example associate.2.f (omp_4.0) \nS-1 program example \nS-2 use omp_lib \nS-3 integer i \nS-4 !$omp parallel private(i) \nS-5 i = omp_get_thread_num() \nS-6 associate(thread_id => i) \nS-7 print *, thread_id ! print private i value \nS-8 end associate \nS-9 !$omp end parallel \nS-10 end program \n12 The following example illustrates the effect of specifying a selector name on a data-sharing \n13 attribute clause. The associate name u is associated with v and the variable v is specified on the \n14 private clause of the parallel construct. The construct association is established prior to the \n15 parallel region. The association between u and the original v is retained (see the Data Sharing \n16 Attribute Rules section in the OpenMP 4.0 API Specifications). Inside the parallel region, v \n17 has the value of -1 and u has the value of the original v. \nCHAPTER 10. DATA ENVIRONMENT 443 \nFortran (cont.) \n1 Example associate.3.f90 (omp_4.0) \nS-1 program example \nS-2 integer :: v \nS-3 v = 15 \nS-4 associate(u => v) \nS-5 !$omp parallel private(v) \nS-6 v = -1 \nS-7 print *, \"v=\", v ! private v=-1 \nS-8 print *, \"u=\", u ! original v=15 \nS-9 !$omp end parallel \nS-10 end associate \nS-11 end program \n2 The following example illustrates mapping behavior for a Fortran associate name and its selector \n3 for a target construct. \n4 For the first 3 target constructs the associate name a_aray is associated with the selector aray, \n5 an array. For the target construct of code block TARGET 1 just the selector aray is used and is \n6 implicitly mapped, likewise for the associate name a_aray in the TARGET 2 block. However, \n7 mapping an associate name and its selector is not valid for the same target construct. Hence the \n8 TARGET 3 block is non-conforming. \n9 In TARGET 4, the scalr selector used in the target region has an implicit data-sharing attribute \n10 of firstprivate since it is a scalar. Hence, the assigned value is not returned. In TARGET 5, the \n11 associate name a_scalr is implicitly mapped and the assigned value is returned to the host (default \n12 tofrom mapping behavior). In TARGET 6, the use of the associate name and its selector in the \n13 target region is conforming because the scalar firstprivate behavior of the selector and the \n14 implicit mapping of the associate name are allowed. At the end of the target region only the \n15 associate name\u2019s value is returned to the host. In TARGET 7, the selector and associate name \n16 appear in an explicit mapping for the same target construct, hence the code block is \n17 non-conforming. \n18 Example associate.4.f90 (omp_5.1) \nS-1 program main \nS-2 integer :: scalr, aray(3) \nS-3 scalr = -1 ; aray = -1 \nS-4 \nS-5 associate(a_scalr=>scalr, a_aray=>aray) \nS-6 \nS-7 !$omp target !! TARGET 1 \nS-8 aray = [1,2,3] \nS-9 !$omp end target \nS-10 print *, a_aray, aray !! 1 2 3 1 2 3 \n444 OpenMP Examples Version 5.2.1 - November 2022 \nS-11 \nS-12 !$omp target !! TARGET 2 \nS-13 a_aray = [4,5,6] \nS-14 !$omp end target \nS-15 print *, a_aray, aray !! 4 5 6 4 5 6 \nS-16 \nS-17 !!!$omp target !! TARGET 3 \nS-18 !! !! mapping, in this case implicit, \nS-19 !! !! of aray AND a_aray NOT ALLOWED \nS-20 !! aray = [4,5,6] \nS-21 !! a_aray = [1,2,3] \nS-22 !!!$omp end target \nS-23 \nS-24 \nS-25 !$omp target !! TARGET 4 \nS-26 scalr = 1 !! scalr is firstprivate \nS-27 !$omp end target \nS-28 print *, a_scalr, scalr !! -1 -1 \nS-29 \nS-30 !$omp target !! TARGET 5 \nS-31 a_scalr = 2 !! a_scalr implicitly mapped \nS-32 !$omp end target \nS-33 print *, a_scalr, scalr !! 2 2 \nS-34 \nS-35 !$omp target !! TARGET 6 \nS-36 scalr = 3 !! scalr is firstprivate \nS-37 print *, a_scalr, scalr !! 2 3 \nS-38 a_scalr = 4 !! a_scalr implicitly mapped \nS-39 print *, a_scalr, scalr !! 4 3 \nS-40 !$omp end target \nS-41 print *, a_scalr, scalr !! 4 4 \nS-42 \nS-43 !!!$omp target map(a_scalr,scalr) !! TARGET 7 \nS-44 !! mapping, in this case explicit, \nS-45 !! of scalr AND a_sclar NOT ALLOWED \nS-46 !! scalr = 5 \nS-47 !! a_scalr = 5 \nS-48 !!!$omp end target \nS-49 \nS-50 end associate \nS-51 \nS-52 end program \nFortran \nCHAPTER 10. DATA ENVIRONMENT 445 \nThis page intentionally left blank \n"}
{"section_title": "11 Memory Model", "chunk": ""}
{"section_title": "11 Memory Model", "chunk": "2 OpenMP provides a shared-memory model that allows all threads on a given device shared access \n3 to memory. For a given OpenMP region that may be executed by more than one thread or SIMD \n4 lane, variables in memory may be shared or private with respect to those threads or SIMD lanes. A \n5 variable\u2019s data-sharing attribute indicates whether it is shared (the shared attribute) or private (the \n6 private, firstprivate, lastprivate, linear, and reduction attributes) in the data environment of an \n7 OpenMP region. While private variables in an OpenMP region are new copies of the original \n8 variable (with same name) that may then be concurrently accessed or modified by their respective \n9 threads or SIMD lanes, a shared variable in an OpenMP region is the same as the variable of the \n10 same name in the enclosing region. Concurrent accesses or modifications to a shared variable may \n11 therefore require synchronization to avoid data races. \n12 OpenMP\u2019s memory model also includes a temporary view of memory that is associated with each \n13 thread. Two different threads may see different values for a given variable in their respective \n14 temporary views. Threads may employ flush operations for the purposes of making their temporary \n15 view of a variable consistent with the value of the variable in memory. The effect of a given flush \n16 operation is characterized by its flush properties \u2013 some combination of strong, release, and \n17 acquire \u2013 and, for strong flushes, a flush-set. \n18 A strong flush will force consistency between the temporary view and the memory for all variables \n19 in its flush-set. Furthermore, all strong flushes in a program that have intersecting flush-sets will \n20 execute in some total order, and within a thread strong flushes may not be reordered with respect to \n21 other memory operations on variables in its flush-set. Release and acquire flushes operate in pairs. \n22 A release flush may \u201csynchronize\u201d with an acquire flush, and when it does so the local memory \n23 operations that precede the release flush will appear to have been completed before the local \n24 memory operations on the same variables that follow the acquire flush. \n25 Flush operations arise from explicit flush directives, implicit flush directives, and also from \n26 the execution of atomic constructs. The flush directive forces a consistent view of local \n27 variables of the thread executing the flush. When a list is supplied on the directive, only the items \n28 (variables) in the list are guaranteed to be flushed. Implied flushes exist at prescribed locations of \n29 certain constructs. For the complete list of these locations and associated constructs, please refer to \n30 the flush Construct section of the OpenMP Specifications document. \n31 In this chapter, examples illustrate how race conditions may arise for accesses to variables with a \n32 shared data-sharing attribute when flush operations are not properly employed. A race condition \n33 can exist when two or more threads are involved in accessing a variable and at least one of the \n34 accesses modifies the variable. In particular, a data race will arise when conflicting accesses do not \n35 have a well-defined completion order. The existence of data races in OpenMP programs result in \n36 undefined behavior, and so they should generally be avoided for programs to be correct. The \n37 completion order of accesses to a shared variable is guaranteed in OpenMP through a set of \n447 \n1 memory consistency rules that are described in the OpenMP Memory Consistency section of the \n2 OpenMP Specifications document. \n"}
{"section_title": "11.1 OpenMP Memory Model", "chunk": ""}
{"section_title": "11.1 OpenMP Memory Model", "chunk": "4 The following examples illustrate two major concerns for concurrent thread execution: ordering of \n5 thread execution and memory accesses that may or may not lead to race conditions. \n6 In the following example, at Print 1, the value of xval could be either 2 or 5, depending on the \n7 timing of the threads. The atomic directives are necessary for the accesses to x by threads 1 and 2 \n8 to avoid a data race. If the atomic write completes before the atomic read, thread 1 is guaranteed to \n9 see 5 in xval. Otherwise, thread 1 is guaranteed to see 2 in xval. \n10 The barrier after Print 1 contains implicit flushes on all threads, as well as a thread synchronization, \n11 so the programmer is guaranteed that the value 5 will be printed by both Print 2 and Print 3. Since \n12 neither Print 2 or Print 3 are modifying x, they may concurrently access x without requiring \n13 atomic directives to avoid a data race. \nC / C++ \n14 Example mem_model.1.c (omp_3.1) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main(){ \nS-5 int x; \nS-6 \nS-7 x = 2; \nS-8 #pragma omp parallel num_threads(2) shared(x) \nS-9 { \nS-10 \nS-11 if (omp_get_thread_num() == 0) { \nS-12 #pragma omp atomic write \nS-13 x = 5; \nS-14 } else { \nS-15 int xval; \nS-16 #pragma omp atomic read \nS-17 xval = x; \nS-18 /* Print 1: xval can be 2 or 5 */ \nS-19 printf(\"1: Thread# %d: x = %d\\n\", omp_get_thread_num(), xval); \nS-20 } \nS-21 \nS-22 #pragma omp barrier \nS-23 \nS-24 if (omp_get_thread_num() == 0) { \nS-25 /* Print 2 */ \n448 OpenMP Examples Version 5.2.1 - November 2022 \nS-26 printf(\"2: Thread# %d: x = %d\\n\", omp_get_thread_num(), x); \nS-27 } else { \nS-28 /* Print 3 */ \nS-29 printf(\"3: Thread# %d: x = %d\\n\", omp_get_thread_num(), x); \nS-30 } \nS-31 } \nS-32 return 0; \nS-33 } \nC / C++ \nFortran \n1 Example mem_model.1.f90 (omp_3.1) \nS-1 PROGRAM MEMMODEL \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 INTEGER X, XVAL \nS-4 \nS-5 X = 2 \nS-6 !$OMP PARALLEL NUM_THREADS(2) SHARED(X) \nS-7 \nS-8 IF (OMP_GET_THREAD_NUM() .EQ. 0) THEN \nS-9 !$OMP ATOMIC WRITE \nS-10 X = 5 \nS-11 ELSE \nS-12 !$OMP ATOMIC READ \nS-13 XVAL = X \nS-14 ! PRINT 1: XVAL can be 2 or 5 \nS-15 PRINT *,\"1: THREAD# \", OMP_GET_THREAD_NUM(), \"X = \", XVAL \nS-16 ENDIF \nS-17 \nS-18 !$OMP BARRIER \nS-19 \nS-20 IF (OMP_GET_THREAD_NUM() .EQ. 0) THEN \nS-21 ! PRINT 2 \nS-22 PRINT *,\"2: THREAD# \", OMP_GET_THREAD_NUM(), \"X = \", X \nS-23 ELSE \nS-24 ! PRINT 3 \nS-25 PRINT *,\"3: THREAD# \", OMP_GET_THREAD_NUM(), \"X = \", X \nS-26 ENDIF \nS-27 \nS-28 !$OMP END PARALLEL \nS-29 \nS-30 END PROGRAM MEMMODEL \nFortran \nCHAPTER 11. MEMORY MODEL 449 \n1 The following example demonstrates why synchronization is difficult to perform correctly through \n2 variables. The write to flag on thread 0 and the read from flag in the loop on thread 1 must be \n3 atomic to avoid a data race. When thread 1 breaks out of the loop, flag will have the value of 1. \n4 However, data will still be undefined at the first print statement. Only after the flush of both flag \n5 and data after the first print statement will data have the well-defined value of 42. \nC / C++ \n6 Example mem_model.2.c (omp_3.1) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 int main() \nS-4 { \nS-5 int data; \nS-6 int flag=0; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 if (omp_get_thread_num()==0) \nS-10 { \nS-11 /* Write to the data buffer that will be \nS-12 * read by thread */ \nS-13 data = 42; \nS-14 /* Flush data to thread 1 and strictly order \nS-15 * the write to data relative to the write to the flag */ \nS-16 #pragma omp flush(flag, data) \nS-17 /* Set flag to release thread 1 */ \nS-18 #pragma omp atomic write \nS-19 flag = 1; \nS-20 } \nS-21 else if(omp_get_thread_num()==1) \nS-22 { \nS-23 /* Loop until we see the update to the flag */ \nS-24 #pragma omp flush(flag, data) \nS-25 int flag_val = 0; \nS-26 while (flag_val < 1) \nS-27 { \nS-28 #pragma omp atomic read \nS-29 flag_val = flag; \nS-30 } \nS-31 /* Value of flag is 1; value of data is undefined */ \nS-32 printf(\"flag=%d data=%d\\n\", flag, data); \nS-33 #pragma omp flush(flag, data) \nS-34 /* Value of flag is 1; value of data is 42 */ \nS-35 printf(\"flag=%d data=%d\\n\", flag, data); \nS-36 } \nS-37 } \n450 OpenMP Examples Version 5.2.1 - November 2022 \nS-38 return 0; \nS-39 } \nC / C++ \nFortran \n1 Example mem_model.2.f (omp_3.1) \nS-1 PROGRAM EXAMPLE \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 INTEGER DATA \nS-4 INTEGER FLAG, FLAG_VAL \nS-5 \nS-6 FLAG = 0 \nS-7 !$OMP PARALLEL NUM_THREADS(2) \nS-8 IF(OMP_GET_THREAD_NUM() .EQ. 0) THEN \nS-9 ! Write to the data buffer that will be read by thread 1 \nS-10 DATA = 42 \nS-11 \nS-12 ! Flush DATA to thread 1 and strictly order the write to DATA \nS-13 ! relative to the write to the FLAG \nS-14 !$OMP FLUSH(FLAG, DATA) \nS-15 \nS-16 ! Set FLAG to release thread 1 \nS-17 !$OMP ATOMIC WRITE \nS-18 FLAG = 1 \nS-19 \nS-20 ELSE IF(OMP_GET_THREAD_NUM() .EQ. 1) THEN \nS-21 ! Loop until we see the update to the FLAG \nS-22 !$OMP FLUSH(FLAG, DATA) \nS-23 FLAG_VAL = 0 \nS-24 DO WHILE(FLAG_VAL .LT. 1) \nS-25 !$OMP ATOMIC READ \nS-26 FLAG_VAL = FLAG \nS-27 ENDDO \nS-28 \nS-29 ! Value of FLAG is 1; value of DATA is undefined \nS-30 PRINT *, \u2019FLAG=\u2019, FLAG, \u2019 DATA=\u2019, DATA \nS-31 \nS-32 !$OMP FLUSH(FLAG, DATA) \nS-33 ! Value of FLAG is 1; value of DATA is 42 \nS-34 PRINT *, \u2019FLAG=\u2019, FLAG, \u2019 DATA=\u2019, DATA \nS-35 \nS-36 ENDIF \nS-37 !$OMP END PARALLEL \nS-38 END \nFortran \nCHAPTER 11. MEMORY MODEL 451 \n1 The next example demonstrates why synchronization is difficult to perform correctly through \n2 variables. As in the preceding example, the updates to flag and the reading of flag in the loops \n3 on threads 1 and 2 are performed atomically to avoid data races on flag. However, the code still \n4 contains data race due to the incorrect use of \u201cflush with a list\u201d after the assignment to data1 on \n5 thread 1. By not including flag in the flush-set of that flush directive, the assignment can be \n6 reordered with respect to the subsequent atomic update to flag. Consequentially, data1 is \n7 undefined at the print statement on thread 2. \nC / C++ \n8 Example mem_model.3.c (omp_3.1) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 \nS-4 int data0 = 0, data1 = 0; \nS-5 \nS-6 int main() \nS-7 { \nS-8 int flag=0; \nS-9 \nS-10 #pragma omp parallel num_threads(3) \nS-11 { \nS-12 if(omp_get_thread_num()==0) \nS-13 { \nS-14 data0 = 17; \nS-15 #pragma omp flush \nS-16 /* Set flag to release thread 1 */ \nS-17 #pragma omp atomic update \nS-18 flag++; \nS-19 /* Flush of flag is implied by the atomic directive */ \nS-20 } \nS-21 else if(omp_get_thread_num()==1) \nS-22 { \nS-23 int flag_val = 0; \nS-24 /* Loop until we see that flag reaches 1*/ \nS-25 while(flag_val < 1) \nS-26 { \nS-27 #pragma omp atomic read \nS-28 flag_val = flag; \nS-29 } \nS-30 #pragma omp flush \nS-31 /* data0 is 17 here */ \nS-32 printf(\"Thread 1 awoken (data0 = %d)\\n\", data0); \nS-33 data1 = 42; \nS-34 #pragma omp flush(data1) \nS-35 /* Set flag to release thread 2 */ \nS-36 #pragma omp atomic update \n452 OpenMP Examples Version 5.2.1 - November 2022 \nS-37 flag++; \nS-38 /* Flush of flag is implied by the atomic directive */ \nS-39 } \nS-40 else if(omp_get_thread_num()==2) \nS-41 { \nS-42 int flag_val = 0; \nS-43 /* Loop until we see that flag reaches 2 */ \nS-44 while(flag_val < 2) \nS-45 { \nS-46 #pragma omp atomic read \nS-47 flag_val = flag; \nS-48 } \nS-49 #pragma omp flush(data0,data1) \nS-50 /* there is a data race here; \nS-51 data0 is 17 and data1 is undefined */ \nS-52 printf(\"Thread 2 awoken (data0 = %d, data1 = %d)\\n\", \nS-53 data0, data1); \nS-54 } \nS-55 } \nS-56 return 0; \nS-57 } \nC / C++ \nFortran \n1 Example mem_model.3.f (omp_3.1) \nS-1 PROGRAM EXAMPLE \nS-2 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-3 INTEGER FLAG, FLAG_VAL \nS-4 INTEGER DATA0, DATA1 \nS-5 \nS-6 FLAG = 0 \nS-7 !$OMP PARALLEL NUM_THREADS(3) \nS-8 IF(OMP_GET_THREAD_NUM() .EQ. 0) THEN \nS-9 DATA0 = 17 \nS-10 !$OMP FLUSH \nS-11 \nS-12 ! Set flag to release thread 1 \nS-13 !$OMP ATOMIC UPDATE \nS-14 FLAG = FLAG + 1 \nS-15 ! Flush of FLAG is implied by the atomic directive \nS-16 \nS-17 ELSE IF(OMP_GET_THREAD_NUM() .EQ. 1) THEN \nS-18 ! Loop until we see that FLAG reaches 1 \nS-19 FLAG_VAL = 0 \nS-20 DO WHILE(FLAG_VAL .LT. 1) \nS-21 !$OMP ATOMIC READ \nCHAPTER 11. MEMORY MODEL 453 \nS-22 FLAG_VAL = FLAG \nS-23 ENDDO \nS-24 !$OMP FLUSH \nS-25 \nS-26 ! DATA0 is 17 here \nS-27 PRINT *, \u2019Thread 1 awoken. DATA0 = \u2019, DATA0 \nS-28 \nS-29 DATA1 = 42 \nS-30 !$OMP FLUSH(DATA1) \nS-31 \nS-32 ! Set FLAG to release thread 2 \nS-33 !$OMP ATOMIC UPDATE \nS-34 FLAG = FLAG + 1 \nS-35 ! Flush of FLAG is implied by the atomic directive \nS-36 \nS-37 ELSE IF(OMP_GET_THREAD_NUM() .EQ. 2) THEN \nS-38 ! Loop until we see that FLAG reaches 2 \nS-39 FLAG_VAL = 0 \nS-40 DO WHILE(FLAG_VAL .LT. 2) \nS-41 !$OMP ATOMIC READ \nS-42 FLAG_VAL = FLAG \nS-43 ENDDO \nS-44 !$OMP FLUSH(DATA0, DATA1) \nS-45 \nS-46 ! There is a data race here; data0 is 17 and data1 is undefined \nS-47 PRINT *, \u2019Thread 2 awoken. DATA0 = \u2019, DATA0, \nS-48 & \u2019 and DATA1 = \u2019, DATA1 \nS-49 \nS-50 ENDIF \nS-51 !$OMP END PARALLEL \nS-52 END \nFortran \n1 The following two examples illustrate the ordering properties of the flush operation. The flush \n2 operations are strong flushes that are applied to the specified flush lists. However, use of a flush \n3 construct with a list is extremely error prone and users are strongly discouraged from attempting it. \n4 In the codes the programmer intends to prevent simultaneous execution of the protected section by \n5 the two threads. The atomic directives in the codes ensure that the accesses to shared variables a \n6 and b are atomic write and atomic read operations. Otherwise both examples would contain data \n7 races and automatically result in unspecified behavior. \n8 In the following incorrect code example, operations on variables a and b are not ordered with \n9 respect to each other. For instance, nothing prevents the compiler from moving the flush of b on \n10 thread 0 or the flush of a on thread 1 to a position completely after the protected section (assuming \n11 that the protected section on thread 0 does not reference b and the protected section on thread 1 \n12 does not reference a). If either re-ordering happens, both threads can simultaneously execute the \n454 OpenMP Examples Version 5.2.1 - November 2022 \n1 protected section. Any shared data accessed in the protected section is not guaranteed to be current \n2 or consistent during or after the protected section. \nC / C++ \n3 Example mem_model.4a.c (omp_3.1) \nS-1 #include <omp.h> \nS-2 \nS-3 void flush_incorrect() \nS-4 { \nS-5 int a, b; \nS-6 a = b = 0; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 int myid = omp_get_thread_num(); \nS-10 int tmp; \nS-11 \nS-12 if ( myid == 0 ) { // thread 0 \nS-13 #pragma omp atomic write \nS-14 b = 1; \nS-15 #pragma omp flush(b) // flushes are not ordered \nS-16 #pragma omp flush(a) // compiler may move them around \nS-17 #pragma omp atomic read \nS-18 tmp = a; \nS-19 } \nS-20 else { // thread 1 \nS-21 #pragma omp atomic write \nS-22 a = 1; \nS-23 #pragma omp flush(a) // flushes are not ordered \nS-24 #pragma omp flush(b) // compiler may move them around \nS-25 #pragma omp atomic read \nS-26 tmp = b; \nS-27 } \nS-28 if ( tmp == 0 ) { // exclusive access not guaranteed \nS-29 /* protected section */ \nS-30 } \nS-31 } \nS-32 } \nC / C++ \nCHAPTER 11. MEMORY MODEL 455 \nFortran \n1 Example mem_model.4a.f90 (omp_3.1) \nS-1 subroutine flush_incorrect \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer a, b, tmp \nS-5 integer myid \nS-6 \nS-7 a = 0; b = 0 \nS-8 !$omp parallel private(myid,tmp) num_threads(2) \nS-9 myid = omp_get_thread_num() \nS-10 \nS-11 if ( myid == 0 ) then ! thread 0 \nS-12 !$omp atomic write \nS-13 b = 1 \nS-14 !$omp flush(b) ! flushes are not ordered \nS-15 !$omp flush(a) ! compiler may move them around \nS-16 !$omp atomic read \nS-17 tmp = a \nS-18 else ! thread 1 \nS-19 !$omp atomic write \nS-20 a = 1 \nS-21 !$omp flush(a) ! flushes are not ordered \nS-22 !$omp flush(b) ! compiler may move them around \nS-23 !$omp atomic read \nS-24 tmp = b \nS-25 endif \nS-26 if ( tmp == 0 ) then ! exclusive access not guaranteed \nS-27 !! protected section \nS-28 endif \nS-29 !$omp end parallel \nS-30 end subroutine \nFortran \n2 The following code example correctly ensures that the protected section is executed by only one \n3 thread at a time. Execution of the protected section by neither thread is considered correct in this \n4 example. This occurs if both flushes complete prior to either thread executing its if statement for \n5 the protected section. The compiler is prohibited from moving the flush at all for either thread, \n6 ensuring that the respective assignment is complete and the data is flushed before the if statement \n7 is executed. \n456 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example mem_model.4b.c (omp_3.1) \nS-1 #include <omp.h> \nS-2 \nS-3 void flush_correct() \nS-4 { \nS-5 int a, b; \nS-6 a = b = 0; \nS-7 #pragma omp parallel num_threads(2) \nS-8 { \nS-9 int myid = omp_get_thread_num(); \nS-10 int tmp; \nS-11 \nS-12 if ( myid == 0 ) { // thread 0 \nS-13 #pragma omp atomic write \nS-14 b = 1; \nS-15 #pragma omp flush(a,b) // flushes are ordered \nS-16 #pragma omp atomic read \nS-17 tmp = a; \nS-18 } \nS-19 else { // thread 1 \nS-20 #pragma omp atomic write \nS-21 a = 1; \nS-22 #pragma omp flush(a,b) // flushes are ordered \nS-23 #pragma omp atomic read \nS-24 tmp = b; \nS-25 } \nS-26 if ( tmp == 0 ) { // access by single thread \nS-27 /* protected section */ \nS-28 } \nS-29 } \nS-30 } \nC / C++ \nFortran \n2 Example mem_model.4b.f90 (omp_3.1) \nS-1 subroutine flush_correct \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer a, b, tmp \nS-5 integer myid \nS-6 \nS-7 a = 0; b = 0 \nS-8 !$omp parallel private(myid,tmp) num_threads(2) \nS-9 myid = omp_get_thread_num() \nCHAPTER 11. MEMORY MODEL 457 \nS-10 \nS-11 if ( myid == 0 ) then ! thread 0 \nS-12 !$omp atomic write \nS-13 b = 1 \nS-14 !$omp flush(a,b) ! flushes are ordered \nS-15 !$omp atomic read \nS-16 tmp = a \nS-17 else ! thread 1 \nS-18 !$omp atomic write \nS-19 a = 1 \nS-20 !$omp flush(a,b) ! flushes are ordered \nS-21 !$omp atomic read \nS-22 tmp = b \nS-23 endif \nS-24 if ( tmp == 0 ) then ! access by single thread \nS-25 !! protected section \nS-26 endif \nS-27 !$omp end parallel \nS-28 end subroutine \nFortran \n458 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "11.2 Memory Allocators", "chunk": ""}
{"section_title": "11.2 Memory Allocators", "chunk": "2 OpenMP memory allocators can be used to allocate memory with specific allocator traits. In the \n3 following example an OpenMP allocator is used to specify an alignment for arrays x and y. The \n4 general approach for attributing traits to variables allocated by OpenMP is to create or specify a \n5 pre-defined memory space, create an array of traits, and then form an allocator from the memory \n6 space and trait. The allocator is then specified in an OpenMP allocation (using an API omp_alloc() \n7 function for C/C++ code and an allocators directive for Fortran code in the allocators.1 \n8 example). \n9 In the example below the xy_memspace variable is declared and assigned the default memory space \n10 (omp_default_mem_space). Next, an array for traits is created. Since only one trait will be used, \n11 the array size is 1. A trait is a structure in C/C++ and a derived type in Fortran, containing 2 \n12 components: a key and a corresponding value (key-value pair). The trait key used here is \n13 omp_atk_alignment (an enum for C/C++ and a parameter for Fortran) and the trait value of 64 is \n14 specified in the xy_traits declaration. These declarations are followed by a call to the \n15 omp_init_allocator() function to combine the memory space (xy_memspace) and the traits \n16 (xy_traits) to form an allocator (xy_alloc). \n17 In the C/C++ code the API omp_allocate() function is used to allocate space, similar to malloc, \n18 except that the allocator is specified as the second argument. In Fortran an allocators directive \n19 is used to specify an allocator for the following Fortran allocate statement. A variable list in the \n20 allocate clause may be supplied if the allocator is to be applied to a subset of variables in the \n21 Fortran allocate statement. Here, the xy_alloc allocator is specified in the modifier of the \n22 allocator clause, and the set of all variables used in the allocate statement is specified in the list. \nC / C++ \n23 Example allocators.1.c (omp_5.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 #include <stdlib.h> \nS-4 #include <stdint.h> \nS-5 #define N 1000 \nS-6 \nS-7 int main() \nS-8 { \nS-9 float *x, *y; \nS-10 float s=2.0; \nS-11 \nS-12 omp_memspace_handle_t xy_memspace = omp_default_mem_space; \nS-13 omp_alloctrait_t xy_traits[1]= {omp_atk_alignment, 64}; \nS-14 omp_allocator_handle_t xy_alloc = \nS-15 omp_init_allocator(xy_memspace,1,xy_traits); \nS-16 \nS-17 \nCHAPTER 11. MEMORY MODEL 459 \nS-18 x=(float *)omp_alloc(N*sizeof(float), xy_alloc); \nS-19 y=(float *)omp_alloc(N*sizeof(float), xy_alloc); \nS-20 \nS-21 if( ((intptr_t)(y))%64 != 0 || ((intptr_t)(x))%64 != 0 ) \nS-22 { printf(\"ERROR: x|y not 64-Byte aligned\\n\"); exit(1); } \nS-23 \nS-24 #pragma omp parallel \nS-25 { \nS-26 #pragma omp for simd simdlen(16) aligned(x,y:64) \nS-27 for(int i=0; i<N; i++){ x[i]=i+1; y[i]=i+1; } // initialize \nS-28 \nS-29 #pragma omp for simd simdlen(16) aligned(x,y:64) \nS-30 for(int i=0; i<N; i++) y[i] = s*x[i] + y[i]; \nS-31 } \nS-32 \nS-33 printf(\"y[0],y[N-1]: %5.0f %5.0f\\n\",y[0],y[N-1]); \nS-34 // output y[0],y[N-1]: 3 3000 \nS-35 \nS-36 omp_free(x, xy_alloc); \nS-37 omp_free(y, xy_alloc); \nS-38 omp_destroy_allocator(xy_alloc); \nS-39 \nS-40 return 0; \nS-41 } \nC / C++ \nFortran \n1 Example allocators.1.f90 (omp_5.2) \nS-1 program main \nS-2 use omp_lib \nS-3 \nS-4 integer, parameter :: N=1000 \nS-5 real, allocatable :: x(:),y(:) \nS-6 real :: s = 2.0e0 \nS-7 integer :: i \nS-8 \nS-9 integer(omp_memspace_handle_kind ) :: xy_memspace = omp_default_mem_space \nS-10 type( omp_alloctrait ) :: xy_traits(1) = & \nS-11 [omp_alloctrait(omp_atk_alignment,64)] \nS-12 integer(omp_allocator_handle_kind) :: xy_alloc \nS-13 \nS-14 xy_alloc = omp_init_allocator( xy_memspace, 1, xy_traits) \nS-15 \nS-16 !$omp allocators allocate(allocator(xy_alloc): x, y) \nS-17 allocate(x(N),y(N)) \nS-18 !! loc is non-standard, but found everywhere \n460 OpenMP Examples Version 5.2.1 - November 2022 \nS-19 !! remove these lines if not available \nS-20 if(modulo(loc(x),64) /= 0 .and. modulo(loc(y),64) /=0 ) then \nS-21 print*,\"ERROR: x|y not 64-byte aligned\"; stop \nS-22 endif \nS-23 \nS-24 !$omp parallel \nS-25 \nS-26 !$omp do simd simdlen(16) aligned(x,y: 64) !! 64B aligned \nS-27 do i=1,N !! initialize \nS-28 x(i)=i \nS-29 y(i)=i \nS-30 end do \nS-31 \nS-32 !$omp do simd simdlen(16) aligned(x,y: 64) !! 64B aligned \nS-33 do i = 1,N \nS-34 y(i) = s*x(i) + y(i) \nS-35 end do \nS-36 \nS-37 !$omp end parallel \nS-38 \nS-39 write(*,\u2019(\"y(1),y(N):\",2f6.0)\u2019) y(1),y(N) !!output: y... 3. 3000. \nS-40 \nS-41 deallocate(x,y) \nS-42 call omp_destroy_allocator(xy_alloc) \nS-43 \nS-44 end program \nFortran \n1 When using the allocators construct with optional clauses in Fortran code, users should be \n2 aware of the behavior of a reallocation. \n3 In the following example, the a variable is allocated with 64-byte alignment through the align \n4 clause of the allocators construct. The alignment of the newly allocated object, a, in the \n5 (reallocation) assignment a = b will not be reallocated with the 64-byte alignment, but with the \n6 32-byte alignment prescribed by the trait of the my_alloctr allocator. It is best to avoid this problem \n7 by constructing and using an allocator (not the align clause) with the required alignment in the \n8 allocators construct. Note that in the subsequent deallocation of a the deallocation must \n9 precede the destruction of the allocator used in the allocation of a. \nCHAPTER 11. MEMORY MODEL 461 \nFortran \n1 Example allocators.2.f90 (omp_5.2) \nS-1 program main \nS-2 use omp_lib \nS-3 implicit none \nS-4 \nS-5 integer, parameter :: align_32=32 \nS-6 real, allocatable :: a(:,:) \nS-7 real :: b(10,10) \nS-8 \nS-9 integer(omp_memspace_handle_kind ) :: my_memspace \nS-10 type( omp_alloctrait ) :: my_traits(1) \nS-11 integer(omp_allocator_handle_kind) :: my_alloctr \nS-12 \nS-13 my_memspace = omp_default_mem_space \nS-14 my_traits = [omp_alloctrait(omp_atk_alignment,align_32)] \nS-15 ! allocator alignment ^^ \nS-16 my_alloctr = omp_init_allocator(my_memspace, 1, my_traits) \nS-17 \nS-18 !$omp allocators allocate(allocator(my_alloctr), align(64): a) \nS-19 allocate(a(5,5)) ! 64-byte aligned by clause <---------^^ \nS-20 \nS-21 a = b ! reallocation occurs with 32-byte alignment \nS-22 ! uses just my_alloctr (32-byte align from allocator) \nS-23 \nS-24 deallocate(a) ! Uses my_alloctr in deallocation. \nS-25 call omp_destroy_allocator(my_alloctr) \nS-26 \nS-27 end program main \nFortran \n2 When creating and using an allocators construct within a Fortran procedure for allocating \n3 storage (and subsequently freeing the allocator storage with an omp_destroy_allocator \n4 construct), users should be aware of the necessity of using an explicit Fortran deallocation instead \n5 of relying on auto-deallocation. \n6 In the following example, a user-defined allocator is used in the allocation of the c variable, and \n7 then the allocator is destroyed. Auto-deallocation at the end of the broken_auto_deallocation \n8 procedure will fail without the allocator, hence an explicit deallocation should be used (before the \n9 omp_destroy_allocator construct). Note that an allocator may be specified directly in the \n10 allocate clause without using the allocator complex modifier, so long as no other modifier \n11 is specified in the clause. \n462 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example allocators.3.f90 (omp_5.2) \nS-1 subroutine broken_auto_deallocation \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer, parameter :: align_32=32 \nS-5 real, allocatable :: c(:) \nS-6 \nS-7 integer(omp_memspace_handle_kind ) :: my_memspace \nS-8 type( omp_alloctrait ) :: my_traits(1) \nS-9 integer(omp_allocator_handle_kind) :: my_alloctr \nS-10 \nS-11 my_memspace = omp_default_mem_space \nS-12 my_traits = [omp_alloctrait(omp_atk_alignment,align_32)] \nS-13 my_alloctr = omp_init_allocator(my_memspace, 1, my_traits) \nS-14 \nS-15 !$omp allocators allocate(my_alloctr: c) \nS-16 allocate(c(100)) \nS-17 \nS-18 !... \nS-19 \nS-20 call omp_destroy_allocator(my_alloctr) \nS-21 ! Auto-deallocation of c fails, \nS-22 ! because my_alloctr is no longer available. \nS-23 \nS-24 end subroutine \nFortran \n2 The allocate directive is a convenient way to apply an OpenMP allocator to the allocation of \n3 declared variables. \n4 This example illustrates the allocation of specific types of storage in a program for use in libraries, \n5 privatized variables, and with offloading. \n6 Two groups of variables, {v1, v2} and {v3, v4}, are used with the allocate directive, and the \n7 {v5, v6} pair is used with the allocate clause. Here we explicitly use predefined allocators \n8 omp_high_bw_mem_alloc and omp_default_mem_alloc with the allocate directive \n9 in CASE 1. Similar effects are achieved for private variables of a task by using the allocate \n10 clause, as shown in CASE 2. \n11 Note, when the allocate directive does not specify an allocator clause, an \n12 implementation-defined default, stored in the def-allocator-var ICV, is used (not illustrated here). \n13 Users can set and get the default allocator with the omp_set_default_allocator and \n14 omp_get_default_allocator API routines. \nCHAPTER 11. MEMORY MODEL 463 \nC / C++ \n1 Example allocators.4.c (omp_5.1) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 \nS-4 void my_init(double *,double *,int, double *,double *,int, \\ \nS-5 double *,double *,int); \nS-6 void lib_saxpy(double *,double *,double,int); \nS-7 void my_gather(double *,double *,int); \nS-8 \nS-9 #pragma omp begin declare target \nS-10 void my_gpu_vxv(double *, double *, int); \nS-11 #pragma omp end declare target \nS-12 \nS-13 #define Nhb 1024*1024 // high bandwith \nS-14 #define Nbg 1024*1024*64 // big memory, default \nS-15 #define Nll 1024*1024 // low latency memory \nS-16 \nS-17 void test_allocate() { \nS-18 \nS-19 double v1[Nhb], v2[Nhb]; \nS-20 double v3[Nbg], v4[Nbg]; \nS-21 double v5[Nll], v6[Nll]; \nS-22 \nS-23 /*** CASE 1: USING ALLOCATE DIRECTIVE ***/ \nS-24 #pragma omp allocate(v1,v2) allocator(omp_high_bw_mem_alloc) \nS-25 #pragma omp allocate(v3,v4) allocator(omp_default_mem_alloc) \nS-26 \nS-27 my_init(v1,v2,Nhb, v3,v4,Nbg, v5,v6,Nll); \nS-28 \nS-29 lib_saxpy(v1,v2,5.0,Nhb); \nS-30 \nS-31 #pragma omp target map(to: v3[0:Nbg], v4[0:Nbg]) map(from:v3[0:Nbg]) \nS-32 my_gpu_vxv(v3,v4,Nbg); \nS-33 \nS-34 /*** CASE 2: USING ALLOCATE CLAUSE ***/ \nS-35 #pragma omp task private(v5,v6) \\ \nS-36 allocate(allocator(omp_low_lat_mem_alloc): v5,v6) \nS-37 { \nS-38 my_gather(v5,v6,Nll); \nS-39 } \nS-40 \nS-41 } \nC / C++ \n464 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example allocators.4.f90 (omp_5.1) \nS-1 subroutine test_allocate \nS-2 use omp_lib \nS-3 \nS-4 interface \nS-5 subroutine my_gpu_vxv(va,vb,n) \nS-6 !$omp declare target \nS-7 integer :: n \nS-8 double precision :: va(n), vb(n) \nS-9 end subroutine \nS-10 end interface \nS-11 \nS-12 integer,parameter :: Nhb=1024*1024, & !! high bandwith \nS-13 Nbg=1024*1024*64,& !! big memory, default \nS-14 Nll=1024*1024 !! low latency memory \nS-15 \nS-16 double precision :: v1(Nhb), v2(Nhb) \nS-17 double precision :: v3(Nbg), v4(Nbg) \nS-18 double precision :: v5(Nll), v6(Nll) \nS-19 \nS-20 !*** CASE 1: USING ALLOCATE DIRECTIVE ***! \nS-21 !$omp allocate(v1,v2) allocator(omp_high_bw_mem_alloc) \nS-22 !$omp allocate(v3,v4) allocator(omp_default_mem_alloc) \nS-23 \nS-24 call my_init(v1,v2,Nhb, v3,v4,Nbg, v5,v6,Nll) \nS-25 \nS-26 call lib_saxpy(v1,v2,5.0,Nhb) \nS-27 \nS-28 !$omp target map(to: v3, v4) map(from:v3) \nS-29 call my_gpu_vxv(v3,v4,Nbg) \nS-30 !$omp end target \nS-31 \nS-32 !*** CASE 2: USING ALLOCATE CLAUSE ***! \nS-33 !$omp task private(v5,v6) & \nS-34 !$omp& allocate(allocator(omp_low_lat_mem_alloc): v5,v6) \nS-35 call my_gather(v5,v6,Nll) \nS-36 !$omp end task \nS-37 \nS-38 end subroutine test_allocate \nFortran \nCHAPTER 11. MEMORY MODEL 465 \n1 The use of allocators in target regions is facilitated by the uses_allocators clause as \n2 shown in the cases below. \n3 In CASE 1, the predefined omp_cgroup_mem_alloc allocator is made available on the device \n4 in the first target construct as specified in the uses_allocators clause. The allocator is \n5 then used in the allocate clause of the teams construct to allocate a private array for each team \n6 (contention group). The private xbuf arrays that are filled by each team are reduced as specified in \n7 the reduction clause on the teams construct. \n8 In CASE 2, user-defined traits are specified in the cgroup_traits variable. An allocator is initialized \n9 for the target region in the uses_allocators clause, and the traits specified in cgroup_traits \n10 are included by the traits modifier. \n11 In CASE 3, the cgroup_alloc variable is initialized on the host with traits and a memory space. \n12 However, these are ignored by the uses_allocators clause and a new allocator for the \n13 target region is initialized with default traits. \nC / C++ \n14 Example allocators.5.c (omp_5.2) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 \nS-4 int calc(int i, int j) { return i * j;} \nS-5 #pragma omp declare target(calc) \nS-6 \nS-7 int main() \nS-8 { \nS-9 #define N 256 \nS-10 int sum; \nS-11 int xbuf[N]; \nS-12 \nS-13 omp_allocator_handle_t cgroup_alloc; \nS-14 const omp_alloctrait_t cgroup_traits[1]= \nS-15 {{omp_atk_access,omp_atv_cgroup}}; \nS-16 \nS-17 for (int i = 0; i < N; i++) { xbuf[i] = 0; } \nS-18 \nS-19 /*** CASE 1: USING ALLOCATE DIRECTIVE ***/ \nS-20 // uses predefined allocator omp_cgroup_mem_alloc \nS-21 #pragma omp target uses_allocators(omp_cgroup_mem_alloc) \nS-22 #pragma omp teams reduction(+:xbuf) thread_limit(N) \\ \nS-23 allocate(omp_cgroup_mem_alloc:xbuf) num_teams(4) \nS-24 { \nS-25 #pragma omp parallel for \nS-26 for (int i = 0; i < N; i++) { \nS-27 xbuf[i] += calc(i,omp_get_team_num()); \nS-28 } \n466 OpenMP Examples Version 5.2.1 - November 2022 \nS-29 } \nS-30 \nS-31 sum = 0; \nS-32 #pragma omp parallel for reduction(+:sum) \nS-33 for (int i = 0; i < N; i++) { \nS-34 sum += xbuf[i]; \nS-35 } \nS-36 if(sum == 3*(N-1)*N) printf(\"PASSED 1 of 3\\n\"); \nS-37 \nS-38 /*** CASE 2: ***/ \nS-39 \nS-40 for (int i = 0; i < N; i++) { xbuf[i] = 0; } \nS-41 \nS-42 cgroup_alloc = omp_null_allocator; \nS-43 \nS-44 // uses custom allocator with specified traits \nS-45 #pragma omp target uses_allocators(traits(cgroup_traits): cgroup_alloc) \nS-46 #pragma omp teams reduction(+:xbuf) thread_limit(N) \\ \nS-47 allocate(cgroup_alloc:xbuf) num_teams(4) \nS-48 { \nS-49 #pragma omp parallel for \nS-50 for (int i = 0; i < N; i++) { \nS-51 xbuf[i] += calc(i,omp_get_team_num()); \nS-52 } \nS-53 } \nS-54 \nS-55 sum = 0; \nS-56 #pragma omp parallel for reduction(+:sum) \nS-57 for (int i = 0; i < N; i++) { \nS-58 sum += xbuf[i]; \nS-59 } \nS-60 if(sum == 3*(N-1)*N) printf(\"PASSED 2 of 3\\n\"); \nS-61 \nS-62 \nS-63 /*** CASE 3: ***/ \nS-64 \nS-65 for (int i = 0; i < N; i++) { xbuf[i] = 0; } \nS-66 \nS-67 cgroup_alloc = omp_init_allocator( \nS-68 omp_default_mem_space, 1, cgroup_traits); \nS-69 \nS-70 // WARNING: uses custom allocator but with DEFAULT traits \nS-71 #pragma omp target uses_allocators(cgroup_alloc) \nS-72 #pragma omp teams reduction(+:xbuf) thread_limit(N) \\ \nS-73 allocate(cgroup_alloc:xbuf) num_teams(4) \nS-74 { \nS-75 #pragma omp parallel for \nCHAPTER 11. MEMORY MODEL 467 \nS-76 for (int i = 0; i < N; i++) { \nS-77 xbuf[i] += calc(i,omp_get_team_num()); \nS-78 } \nS-79 } \nS-80 omp_destroy_allocator(cgroup_alloc); \nS-81 \nS-82 sum = 0; \nS-83 #pragma omp parallel for reduction(+:sum) \nS-84 for (int i = 0; i < N; i++) { \nS-85 sum += xbuf[i]; \nS-86 } \nS-87 if(sum == 3*(N-1)*N) printf(\"PASSED 3 of 3\\n\"); \nS-88 \nS-89 return 0; \nS-90 } \nC / C++ \nFortran \n1 Example allocators.5.f90 (omp_5.2) \nS-1 module functions \nS-2 contains \nS-3 function calc(i,j) result(ii) \nS-4 implicit none \nS-5 integer :: i,j,ii \nS-6 !$omp declare target(calc) \nS-7 \nS-8 ii = i*j \nS-9 end function \nS-10 end module \nS-11 \nS-12 program main \nS-13 \nS-14 use omp_lib \nS-15 use functions \nS-16 implicit none \nS-17 integer, parameter :: N=256 \nS-18 integer :: sum, i \nS-19 integer :: xbuf(N) \nS-20 \nS-21 integer( omp_allocator_handle_kind ) :: cgroup_alloc \nS-22 type(omp_alloctrait),parameter :: cgroup_traits(1)= & \nS-23 [omp_alloctrait(omp_atk_access,omp_atv_cgroup)] \nS-24 \nS-25 do i=1,N; xbuf(i)=0; end do \nS-26 \nS-27 !*** CASE 1: USING ALLOCATE DIRECTIVE ***! \n468 OpenMP Examples Version 5.2.1 - November 2022 \nS-28 \nS-29 !! uses predefined allocator omp_cgroup_mem_alloc \nS-30 \nS-31 !$omp target uses_allocators(omp_cgroup_mem_alloc) \nS-32 !$omp teams reduction(+:xbuf) thread_limit(N) & \nS-33 !$omp& allocate(omp_cgroup_mem_alloc:xbuf) num_teams(4) \nS-34 \nS-35 !$omp parallel do \nS-36 do i = 1,N \nS-37 xbuf(i) = xbuf(i) + calc(i, omp_get_team_num()) \nS-38 enddo \nS-39 \nS-40 !$omp end teams \nS-41 !$omp end target \nS-42 \nS-43 sum = 0 \nS-44 !$omp parallel do reduction(+:sum) \nS-45 do i = 1,N \nS-46 sum = sum + xbuf(i) \nS-47 enddo \nS-48 if(sum == 3*(N+1)*N) print*, \"PASSED 1 of 3\" \nS-49 \nS-50 !*** CASE 2: ***! \nS-51 \nS-52 do i=1,N; xbuf(i)=0; end do \nS-53 \nS-54 cgroup_alloc = omp_null_allocator \nS-55 \nS-56 !! uses custom allocator with specified traits \nS-57 !$omp target uses_allocators(traits(cgroup_traits): cgroup_alloc) \nS-58 !$omp teams reduction(+:xbuf) thread_limit(N) & \nS-59 !$omp& allocate(cgroup_alloc:xbuf) num_teams(4) \nS-60 \nS-61 !$omp parallel do \nS-62 do i = 1,N \nS-63 xbuf(i) = xbuf(i) + calc(i,omp_get_team_num()) \nS-64 enddo \nS-65 \nS-66 !$omp end teams \nS-67 !$omp end target \nS-68 \nS-69 sum = 0 \nS-70 !$omp parallel do reduction(+:sum) \nS-71 do i = 1,N \nS-72 sum = sum + xbuf(i) \nS-73 enddo \nS-74 if(sum == 3*(N+1)*N) print*, \"PASSED 2 of 3\" \nCHAPTER 11. MEMORY MODEL 469 \nS-75 \nS-76 !*** CASE 3: ***! \nS-77 \nS-78 do i=1,N; xbuf(i)=0; end do \nS-79 \nS-80 cgroup_alloc = omp_init_allocator(omp_default_mem_space, 1, & \nS-81 cgroup_traits) \nS-82 \nS-83 !! WARNING: uses custom allocator but with DEFAULT traits \nS-84 !$omp target uses_allocators(cgroup_alloc) \nS-85 !$omp teams reduction(+:xbuf) thread_limit(N) & \nS-86 !$omp& allocate(cgroup_alloc:xbuf) num_teams(4) \nS-87 \nS-88 !$omp parallel do \nS-89 do i = 1,N \nS-90 xbuf(i) = xbuf(i) + calc(i,omp_get_team_num()) \nS-91 enddo \nS-92 \nS-93 !$omp end teams \nS-94 !$omp end target \nS-95 \nS-96 call omp_destroy_allocator(cgroup_alloc) \nS-97 \nS-98 sum = 0 \nS-99 !$omp parallel do reduction(+:sum) \nS-100 do i = 1,N \nS-101 sum = sum + xbuf(i) \nS-102 enddo \nS-103 if(sum == 3*(N+1)*N) print*, \"PASSED 3 of 3\" \nS-104 \nS-105 end program main \nFortran \n1 The following example shows how to make an allocator available in a target region without \n2 specifying a uses_allocators clause. \n3 In CASE 1, the predefined omp_cgroup_mem_alloc allocator is used in the target region as \n4 in CASE 1 of the previous example, but without specifying a uses_allocators clause. This is \n5 accomplished by specifying the requires directive with a dynamic_allocators clause in \n6 the same compilation unit, to remove restrictions on allocator usage in target regions. \n7 CASE 2 also uses the dynamic_allocators clause to remove allocator restrictions in \n8 target regions. Here, an allocator is initialized by calling the omp_init_allocator routine \n9 in the target region. The allocator is then used for the allocations of array xbuf in an \n10 allocate clause of the target teams construct for each team and destroyed after its use. The \n11 use of separate target regions is needed here since no statement is allowed between a target \n12 directive and its nested teams construct. \n470 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example allocators.6.c (omp_5.2) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 \nS-4 #pragma omp requires dynamic_allocators \nS-5 \nS-6 int calc(int i, int j) { return i*j;} \nS-7 #pragma omp declare target(calc) \nS-8 \nS-9 int main() \nS-10 { \nS-11 #define N 256 \nS-12 int sum; \nS-13 int xbuf[N]; \nS-14 \nS-15 static omp_allocator_handle_t cgroup_alloc; \nS-16 #pragma omp declare target(cgroup_alloc) \nS-17 const omp_alloctrait_t cgroup_traits[1] = \nS-18 {{omp_atk_access, omp_atv_cgroup}}; \nS-19 \nS-20 /*** CASE 1: ***/ \nS-21 \nS-22 for (int i = 0; i < N; i++) { xbuf[i] = 0;} \nS-23 \nS-24 // uses predefined allocator, no need to declare it in uses_allocators \nS-25 #pragma omp target teams reduction(+:xbuf) thread_limit(N) \\ \nS-26 allocate(omp_cgroup_mem_alloc:xbuf) num_teams(4) \nS-27 { \nS-28 #pragma omp parallel for \nS-29 for (int i = 0; i < N; i++) { \nS-30 xbuf[i] += calc(i,omp_get_team_num()); \nS-31 } \nS-32 } \nS-33 \nS-34 sum = 0; \nS-35 #pragma omp parallel for reduction(+:sum) \nS-36 for (int i = 0; i < N; i++) { \nS-37 sum += xbuf[i]; \nS-38 } \nS-39 if(sum == 3*(N-1)*N) printf(\"PASSED 1 of 2\\n\"); \nS-40 \nS-41 \nS-42 /*** CASE 2: ***/ \nS-43 \nS-44 for (int i = 0; i < N; i++) { xbuf[i] = 0; } \nCHAPTER 11. MEMORY MODEL 471 \nS-45 \nS-46 // initializes the allocator in target region \nS-47 #pragma omp target \nS-48 cgroup_alloc = omp_init_allocator( \nS-49 omp_default_mem_space, 1, cgroup_traits); \nS-50 \nS-51 // uses the initialized allocator \nS-52 #pragma omp target \nS-53 #pragma omp teams reduction(+:xbuf) thread_limit(N) \\ \nS-54 allocate(cgroup_alloc:xbuf) num_teams(4) \nS-55 { \nS-56 #pragma omp parallel for \nS-57 for (int i = 0; i < N; i++) { \nS-58 xbuf[i] += calc(i,omp_get_team_num()); \nS-59 } \nS-60 } \nS-61 \nS-62 // destroys the allocator after its use \nS-63 #pragma omp target \nS-64 omp_destroy_allocator(cgroup_alloc); \nS-65 \nS-66 sum = 0; \nS-67 #pragma omp parallel for reduction(+:sum) \nS-68 for (int i = 0; i < N; i++) { \nS-69 sum += xbuf[i]; \nS-70 } \nS-71 if(sum == 3*(N-1)*N) printf(\"PASSED 2 of 2\\n\"); \nS-72 \nS-73 return 0; \nS-74 } \nC / C++ \nFortran \n1 Example allocators.6.f90 (omp_5.2) \nS-1 module functions \nS-2 contains \nS-3 function calc(i,j) result(ii) \nS-4 implicit none \nS-5 integer :: i,j,ii \nS-6 !$omp declare target(calc) \nS-7 \nS-8 ii = i*j \nS-9 end function \nS-10 end module \nS-11 \nS-12 program main \n472 OpenMP Examples Version 5.2.1 - November 2022 \nS-13 \nS-14 use omp_lib \nS-15 use functions \nS-16 implicit none \nS-17 integer, parameter :: N=256 \nS-18 integer :: sum, i \nS-19 integer :: xbuf(N) \nS-20 \nS-21 !$omp requires dynamic_allocators \nS-22 \nS-23 integer(omp_allocator_handle_kind),save :: cgroup_alloc \nS-24 !$omp declare target(cgroup_alloc) \nS-25 type(omp_alloctrait),parameter :: cgroup_traits(1)= & \nS-26 [omp_alloctrait(omp_atk_access,omp_atv_cgroup)] \nS-27 \nS-28 !*** CASE 1: ***! \nS-29 \nS-30 do i=1,N; xbuf(i)=0; end do \nS-31 \nS-32 !! uses predefined allocator, no need to declare it in uses_allocators \nS-33 !$omp target teams reduction(+:xbuf) thread_limit(N) & \nS-34 !$omp& allocate(omp_cgroup_mem_alloc:xbuf) num_teams(4) \nS-35 \nS-36 !$omp parallel do \nS-37 do i = 1,N \nS-38 xbuf(i) = xbuf(i) + calc(i,omp_get_team_num()) \nS-39 enddo \nS-40 \nS-41 !$omp end target teams \nS-42 \nS-43 sum = 0 \nS-44 !$omp parallel do reduction(+:sum) \nS-45 do i = 1,N \nS-46 sum = sum + xbuf(i) \nS-47 enddo \nS-48 if(sum == 3*(N+1)*N) print*, \"PASSED 1 of 2\" \nS-49 \nS-50 !*** CASE 2: ***! \nS-51 \nS-52 do i=1,N; xbuf(i)=0; end do \nS-53 \nS-54 !! initializes allocator in the target region \nS-55 !$omp target \nS-56 cgroup_alloc = omp_init_allocator(omp_default_mem_space, 1, & \nS-57 cgroup_traits) \nS-58 !$omp end target \nS-59 \nCHAPTER 11. MEMORY MODEL 473 \nS-60 !! uses the initialized allocator \nS-61 !$omp target \nS-62 !$omp teams reduction(+:xbuf) thread_limit(N) & \nS-63 !$omp& allocate(cgroup_alloc:xbuf) num_teams(4) \nS-64 \nS-65 !$omp parallel do \nS-66 do i = 1,N \nS-67 xbuf(i) = xbuf(i) + calc(i,omp_get_team_num()) \nS-68 enddo \nS-69 \nS-70 !$omp end teams \nS-71 !$omp end target \nS-72 \nS-73 !! destroys the allocator after its use \nS-74 !$omp target \nS-75 call omp_destroy_allocator(cgroup_alloc) \nS-76 !$omp end target \nS-77 \nS-78 sum = 0 \nS-79 !$omp parallel do reduction(+:sum) \nS-80 do i = 1,N \nS-81 sum = sum + xbuf(i) \nS-82 enddo \nS-83 if(sum == 3*(N+1)*N) print*, \"PASSED 2 of 2\" \nS-84 \nS-85 end program main \nFortran \n474 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "11.3 Race Conditions Caused by Implied Copies of Shared Variables in Fortran", "chunk": ""}
{"section_title": "11.3 Race Conditions Caused by Implied Copies of Shared Variables in Fortran", "chunk": "2 Copies of Shared Variables in Fortran \nFortran \n3 The following example contains a race condition, because the shared variable, which is an array \n4 section, is passed as an actual argument to a routine that has an assumed-size array as its dummy \n5 argument. The subroutine call passing an array section argument may cause the compiler to copy \n6 the argument into a temporary location prior to the call and copy from the temporary location into \n7 the original variable when the subroutine returns. This copying would cause races in the \n8 parallel region. \n9 Example fort_race.1.f90 (pre_omp_3.0) \nS-1 SUBROUTINE SHARED_RACE \nS-2 \nS-3 INCLUDE \"omp_lib.h\" ! or USE OMP_LIB \nS-4 \nS-5 REAL A(20) \nS-6 INTEGER MYTHREAD \nS-7 \nS-8 !$OMP PARALLEL SHARED(A) PRIVATE(MYTHREAD) \nS-9 \nS-10 MYTHREAD = OMP_GET_THREAD_NUM() \nS-11 IF (MYTHREAD .EQ. 0) THEN \nS-12 CALL SUB(A(1:10)) ! compiler may introduce writes to A(6:10) \nS-13 ELSE \nS-14 A(6:10) = 12 \nS-15 ENDIF \nS-16 \nS-17 !$OMP END PARALLEL \nS-18 \nS-19 END SUBROUTINE SHARED_RACE \nS-20 \nS-21 SUBROUTINE SUB(X) \nS-22 REAL X(*) \nS-23 X(1:5) = 4 \nS-24 END SUBROUTINE SUB \nFortran \nCHAPTER 11. MEMORY MODEL 475 \nThis page intentionally left blank \n"}
{"section_title": "12 Program Control", "chunk": ""}
{"section_title": "12 Program Control", "chunk": "2 Basic concepts and mechanisms for directing and controlling a program compilation and execution \n3 are provided in this introduction and illustrated in subsequent examples. \n4 CONDITIONAL COMPILATION and EXECUTION \n5 Conditional compilation can be performed with conventional #ifdef directives in C, C++, and \n6 Fortran, and additionally with OpenMP sentinel (!$) in Fortran. The if clause on some directives \n7 can direct the runtime to ignore or alter the behavior of the construct. Of course, the base-language \n8 if statements can be used to control the execution of stand-alone directives (such as flush, \n9 barrier, taskwait, and taskyield). However, the directives must appear in a block \n10 structure, and not as a substatement. The metadirective and declare variant directives \n11 provide conditional selection of directives and routines for compilation (and use), respectively. The \n12 assume and requires directives provide invariants for optimizing compilation, and essential \n13 features for compilation and correct execution, respectively. \n14 CANCELLATION \n15 Cancellation (termination) of the normal sequence of execution for the threads in an OpenMP \n16 region can be accomplished with the cancel construct. The construct uses a \n17 construct-type-clause to set the region-type to activate for the cancellation. That is, inclusion of one \n18 of the construct-type-clause names parallel, for, do, sections or taskgroup on the \n19 directive line activates the corresponding region. The cancel construct is activated by the first \n20 encountering thread, and it continues execution at the end of the named region. The cancel \n21 construct is also a cancellation point for any other thread of the team to also continue execution at \n22 the end of the named region. \n23 Also, once the specified region has been activated for cancellation any thread that encounters a \n24 cancellation point construct with the same named region (construct-type-clause), continues \n25 execution at the end of the region. \n26 For an activated cancel taskgroup construct, the tasks that belong to the taskgroup set of the \n27 innermost enclosing taskgroup region will be canceled. \n28 A task that encounters a cancel taskgroup construct continues execution at the end of its task \n29 region. Any task of the taskgroup that has already begun execution will run to completion, unless it \n30 encounters a cancellation point; tasks that have not begun execution may be discarded as \n31 completed tasks. \n32 CONTROL VARIABLES \n477 \n1 Internal control variables (ICV) are used by implementations to hold values which control the \n2 execution of OpenMP regions. Control (and hence the ICVs) may be set as implementation \n3 defaults, or set and adjusted through environment variables, clauses, and API functions. Initial ICV \n4 values are reported by the runtime if the OMP_DISPLAY_ENV environment variable has been set \n5 to TRUE or VERBOSE. \n6 NESTED CONSTRUCTS \n7 Certain combinations of nested constructs are permitted, giving rise to combined constructs \n8 consisting of two or more directives. These can be used when the two (or several) constructs would \n9 be used immediately in succession (closely nested). A combined construct can use the clauses of \n10 the component constructs without restrictions. A composite construct is a combined construct \n11 which has one or more clauses with (an often obviously) modified or restricted meaning, relative to \n12 when the constructs are uncombined. \n13 Certain nestings are forbidden, and often the reasoning is obvious. For example, worksharing \n14 constructs cannot be nested, and the barrier construct cannot be nested inside a worksharing \n15 construct, or a critical construct. Also, target constructs cannot be nested, unless the nested \n16 target is a reverse offload. \n17 The parallel construct can be nested, as well as the task construct. The parallel execution in \n18 the nested parallel construct(s) is controlled by the OMP_MAX_ACTIVE_LEVELS environment \n19 variable, and the omp_set_max_active_levels routine. Use the \n20 omp_get_max_active_levels routine to determine the maximum levels provided by an \n21 implementation. As of OpenMP 5.0, use of the OMP_NESTED environment variable and the \n22 omp_set_nested routine has been deprecated. \n23 More details on nesting can be found in the Nesting of Regions of the Directives chapter in the \n24 OpenMP Specifications document. \n478 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "12.1 Conditional Compilation", "chunk": "C / C++ \n2 The following example illustrates the use of conditional compilation using the OpenMP macro \n3 _OPENMP. With OpenMP compilation, the _OPENMP macro becomes defined. \n4 Example cond_comp.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 int main() \nS-4 { \nS-5 \nS-6 # ifdef _OPENMP \nS-7 printf(\"Compiled by an OpenMP-compliant implementation.\\n\"); \nS-8 # endif \nS-9 \nS-10 return 0; \nS-11 } \nC / C++ \nFortran \n5 The following example illustrates the use of the conditional compilation sentinel. With OpenMP \n6 compilation, the conditional compilation sentinel !$ is recognized and treated as two spaces. In \n7 fixed form source, statements guarded by the sentinel must start after column 6. \n8 Example cond_comp.1.f (pre_omp_3.0) \nS-1 PROGRAM EXAMPLE \nS-2 \nS-3 C234567890 \nS-4 !$ PRINT *, \"Compiled by an OpenMP-compliant implementation.\" \nS-5 \nS-6 END PROGRAM EXAMPLE \nFortran \nCHAPTER 12. PROGRAM CONTROL 479 \n"}
{"section_title": "12.2 Internal Control Variables (ICVs)", "chunk": ""}
{"section_title": "12.2 Internal Control Variables (ICVs)", "chunk": "2 According to Section 2.3 of the OpenMP 4.0 specification, an OpenMP implementation must act as \n3 if there are ICVs that control the behavior of the program. This example illustrates two ICVs, \n4 nthreads-var and max-active-levels-var. The nthreads-var ICV controls the number of threads \n5 requested for encountered parallel regions; there is one copy of this ICV per task. The \n6 max-active-levels-var ICV controls the maximum number of nested active parallel regions; there is \n7 one copy of this ICV for the whole program. \n8 In the following example, the nest-var, max-active-levels-var, dyn-var, and nthreads-var ICVs are \n9 modified through calls to the runtime library routines omp_set_nested, \n10 omp_set_max_active_levels, omp_set_dynamic, and omp_set_num_threads \n11 respectively. These ICVs affect the operation of parallel regions. Each implicit task generated \n12 by a parallel region has its own copy of the nest-var, dyn-var, and nthreads-var ICVs. \n13 In the following example, the new value of nthreads-var applies only to the implicit tasks that \n14 execute the call to omp_set_num_threads. There is one copy of the max-active-levels-var \n15 ICV for the whole program and its value is the same for all tasks. This example assumes that nested \n16 parallelism is supported. \n17 The outer parallel region creates a team of two threads; each of the threads will execute one of \n18 the two implicit tasks generated by the outer parallel region. \n19 Each implicit task generated by the outer parallel region calls omp_set_num_threads(3), \n20 assigning the value 3 to its respective copy of nthreads-var. Then each implicit task encounters an \n21 inner parallel region that creates a team of three threads; each of the threads will execute one of \n22 the three implicit tasks generated by that inner parallel region. \n23 Since the outer parallel region is executed by 2 threads, and the inner by 3, there will be a total \n24 of 6 implicit tasks generated by the two inner parallel regions. \n25 Each implicit task generated by an inner parallel region will execute the call to \n26 omp_set_num_threads(4), assigning the value 4 to its respective copy of nthreads-var. \n27 The print statement in the outer parallel region is executed by only one of the threads in the \n28 team. So it will be executed only once. \n29 The print statement in an inner parallel region is also executed by only one of the threads in the \n30 team. Since we have a total of two inner parallel regions, the print statement will be executed \n31 twice \u2013 once per inner parallel region. \n480 OpenMP Examples Version 5.2.1 - November 2022 \nC / C++ \n1 Example icv.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main (void) \nS-5 { \nS-6 omp_set_nested(1); \nS-7 omp_set_max_active_levels(8); \nS-8 omp_set_dynamic(0); \nS-9 omp_set_num_threads(2); \nS-10 #pragma omp parallel \nS-11 { \nS-12 omp_set_num_threads(3); \nS-13 \nS-14 #pragma omp parallel \nS-15 { \nS-16 omp_set_num_threads(4); \nS-17 #pragma omp single \nS-18 { \nS-19 // The following should print: \nS-20 // Inner: max_act_lev=8, num_thds=3, max_thds=4 \nS-21 // Inner: max_act_lev=8, num_thds=3, max_thds=4 \nS-22 printf (\"Inner: max_act_lev=%d, num_thds=%d, max_thds=%d\\n\", \nS-23 omp_get_max_active_levels(), omp_get_num_threads(), \nS-24 omp_get_max_threads()); \nS-25 } \nS-26 } \nS-27 \nS-28 #pragma omp barrier \nS-29 #pragma omp single \nS-30 { \nS-31 // The following should print: \nS-32 // Outer: max_act_lev=8, num_thds=2, max_thds=3 \nS-33 printf (\"Outer: max_act_lev=%d, num_thds=%d, max_thds=%d\\n\", \nS-34 omp_get_max_active_levels(), omp_get_num_threads(), \nS-35 omp_get_max_threads()); \nS-36 } \nS-37 } \nS-38 return 0; \nS-39 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 481 \nFortran \n1 Example icv.1.f (pre_omp_3.0) \nS-1 program icv \nS-2 use omp_lib \nS-3 \nS-4 call omp_set_nested(.true.) \nS-5 call omp_set_max_active_levels(8) \nS-6 call omp_set_dynamic(.false.) \nS-7 call omp_set_num_threads(2) \nS-8 \nS-9 !$omp parallel \nS-10 call omp_set_num_threads(3) \nS-11 \nS-12 !$omp parallel \nS-13 call omp_set_num_threads(4) \nS-14 !$omp single \nS-15 ! The following should print: \nS-16 ! Inner: max_act_lev= 8 , num_thds= 3 , max_thds= 4 \nS-17 ! Inner: max_act_lev= 8 , num_thds= 3 , max_thds= 4 \nS-18 print *, \"Inner: max_act_lev=\", omp_get_max_active_levels(), \nS-19 & \", num_thds=\", omp_get_num_threads(), \nS-20 & \", max_thds=\", omp_get_max_threads() \nS-21 !$omp end single \nS-22 !$omp end parallel \nS-23 \nS-24 !$omp barrier \nS-25 !$omp single \nS-26 ! The following should print: \nS-27 ! Outer: max_act_lev= 8 , num_thds= 2 , max_thds= 3 \nS-28 print *, \"Outer: max_act_lev=\", omp_get_max_active_levels(), \nS-29 & \", num_thds=\", omp_get_num_threads(), \nS-30 & \", max_thds=\", omp_get_max_threads() \nS-31 !$omp end single \nS-32 !$omp end parallel \nS-33 end \nFortran \n482 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "12.3 Placement of flush, barrier, taskwait and taskyield Directives", "chunk": ""}
{"section_title": "12.3 Placement of flush, barrier, taskwait and taskyield Directives", "chunk": "2 and taskyield Directives \n3 The following example is non-conforming, because the flush, barrier, taskwait, and \n4 taskyield directives are stand-alone directives and cannot be the immediate substatement of an \n5 if statement. \nC / C++ \n6 Example standalone.1.c (omp_3.1) \nS-1 void standalone_wrong() \nS-2 { \nS-3 int a = 1; \nS-4 \nS-5 if (a != 0) \nS-6 #pragma omp flush(a) \nS-7 /* incorrect as flush cannot be immediate substatement \nS-8 of if statement */ \nS-9 \nS-10 if (a != 0) \nS-11 #pragma omp barrier \nS-12 /* incorrect as barrier cannot be immediate substatement \nS-13 of if statement */ \nS-14 \nS-15 if (a!=0) \nS-16 #pragma omp taskyield \nS-17 /* incorrect as taskyield cannot be immediate substatement of if statement \nS-18 */ \nS-19 \nS-20 if (a != 0) \nS-21 #pragma omp taskwait \nS-22 /* incorrect as taskwait cannot be immediate substatement \nS-23 of if statement */ \nS-24 \nS-25 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 483 \n1 The following example is non-conforming, because the flush, barrier, taskwait, and \n2 taskyield directives are stand-alone directives and cannot be the action statement of an if \n3 statement or a labeled branch target. \nFortran \n4 Example standalone.1.f90 (omp_3.1) \nS-1 SUBROUTINE STANDALONE_WRONG() \nS-2 \nS-3 INTEGER A \nS-4 \nS-5 A = 1 \nS-6 \nS-7 ! the FLUSH directive must not be the action statement \nS-8 ! in an IF statement \nS-9 IF (A .NE. 0) !$OMP FLUSH(A) \nS-10 \nS-11 ! the BARRIER directive must not be the action statement \nS-12 ! in an IF statement \nS-13 IF (A .NE. 0) !$OMP BARRIER \nS-14 \nS-15 ! the TASKWAIT directive must not be the action statement \nS-16 ! in an IF statement \nS-17 IF (A .NE. 0) !$OMP TASKWAIT \nS-18 \nS-19 ! the TASKYIELD directive must not be the action statement \nS-20 ! in an IF statement \nS-21 IF (A .NE. 0) !$OMP TASKYIELD \nS-22 \nS-23 GOTO 100 \nS-24 \nS-25 ! the FLUSH directive must not be a labeled branch target \nS-26 ! statement \nS-27 100 !$OMP FLUSH(A) \nS-28 GOTO 200 \nS-29 \nS-30 ! the BARRIER directive must not be a labeled branch target \nS-31 ! statement \nS-32 200 !$OMP BARRIER \nS-33 GOTO 300 \nS-34 \nS-35 ! the TASKWAIT directive must not be a labeled branch target \nS-36 ! statement \nS-37 300 !$OMP TASKWAIT \nS-38 GOTO 400 \nS-39 \nS-40 ! the TASKYIELD directive must not be a labeled branch target \n484 OpenMP Examples Version 5.2.1 - November 2022 \nS-41 ! statement \nS-42 400 !$OMP TASKYIELD \nS-43 \nS-44 END SUBROUTINE \nFortran \n1 The following version of the above example is conforming because the flush, barrier, \n2 taskwait, and taskyield directives are enclosed in a compound statement. \nC / C++ \n3 Example standalone.2.c (omp_3.1) \nS-1 void standalone_ok() \nS-2 { \nS-3 int a = 1; \nS-4 \nS-5 #pragma omp parallel \nS-6 { \nS-7 if (a != 0) { \nS-8 #pragma omp flush(a) \nS-9 } \nS-10 if (a != 0) { \nS-11 #pragma omp barrier \nS-12 } \nS-13 if (a != 0) { \nS-14 #pragma omp taskwait \nS-15 } \nS-16 if (a != 0) { \nS-17 #pragma omp taskyield \nS-18 } \nS-19 } \nS-20 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 485 \n1 The following example is conforming because the flush, barrier, taskwait, and \n2 taskyield directives are enclosed in an if construct or follow the labeled branch target. \nFortran \n3 Example standalone.2.f90 (omp_3.1) \nS-1 SUBROUTINE STANDALONE_OK() \nS-2 INTEGER A \nS-3 A = 1 \nS-4 IF (A .NE. 0) THEN \nS-5 !$OMP FLUSH(A) \nS-6 ENDIF \nS-7 IF (A .NE. 0) THEN \nS-8 !$OMP BARRIER \nS-9 ENDIF \nS-10 IF (A .NE. 0) THEN \nS-11 !$OMP TASKWAIT \nS-12 ENDIF \nS-13 IF (A .NE. 0) THEN \nS-14 !$OMP TASKYIELD \nS-15 ENDIF \nS-16 GOTO 100 \nS-17 100 CONTINUE \nS-18 !$OMP FLUSH(A) \nS-19 GOTO 200 \nS-20 200 CONTINUE \nS-21 !$OMP BARRIER \nS-22 GOTO 300 \nS-23 300 CONTINUE \nS-24 !$OMP TASKWAIT \nS-25 GOTO 400 \nS-26 400 CONTINUE \nS-27 !$OMP TASKYIELD \nS-28 END SUBROUTINE \nFortran \n486 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "12.4 Cancellation Constructs", "chunk": ""}
{"section_title": "12.4 Cancellation Constructs", "chunk": "2 The following example shows how the cancel directive can be used to terminate an OpenMP \n3 region. Although the cancel construct terminates the OpenMP worksharing region, programmers \n4 must still track the exception through the pointer ex and issue a cancellation for the parallel \n5 region if an exception has been raised. The primary thread checks the exception pointer to make \n6 sure that the exception is properly handled in the sequential part. If cancellation of the parallel \n7 region has been requested, some threads might have executed phase_1(). However, it is \n8 guaranteed that none of the threads executed phase_2(). \nC++ \n9 Example cancellation.1.cpp (omp_4.0) \nS-1 #include <iostream> \nS-2 #include <exception> \nS-3 #include <cstddef> \nS-4 \nS-5 #define N 10000 \nS-6 \nS-7 extern void causes_an_exception(); \nS-8 extern void phase_1(); \nS-9 extern void phase_2(); \nS-10 \nS-11 void example() { \nS-12 std::exception *ex = NULL; \nS-13 #pragma omp parallel shared(ex) \nS-14 { \nS-15 #pragma omp for \nS-16 for (int i = 0; i < N; i++) { \nS-17 // no \u2019if\u2019 that prevents compiler optimizations \nS-18 try { \nS-19 causes_an_exception(); \nS-20 } \nS-21 catch (std::exception *e) { \nS-22 // still must remember exception for later handling \nS-23 #pragma omp atomic write \nS-24 ex = e; \nS-25 // cancel worksharing construct \nS-26 #pragma omp cancel for \nS-27 } \nS-28 } \nS-29 // if an exception has been raised, cancel parallel region \nS-30 if (ex) { \nS-31 #pragma omp cancel parallel \nS-32 } \nS-33 phase_1(); \nS-34 #pragma omp barrier \nCHAPTER 12. PROGRAM CONTROL 487 \nS-35 phase_2(); \nS-36 } \nS-37 // continue here if an exception has been thrown in \nS-38 // the worksharing loop \nS-39 if (ex) { \nS-40 // handle exception stored in ex \nS-41 } \nS-42 } \nC++ \n1 The following example illustrates the use of the cancel construct in error handling. If there is an \n2 error condition from the allocate statement, the cancellation is activated. The encountering \n3 thread sets the shared variable err and other threads of the binding thread set proceed to the end of \n4 the worksharing construct after the cancellation has been activated. \nFortran \n5 Example cancellation.1.f90 (omp_4.0) \nS-1 subroutine example(n, dim) \nS-2 integer, intent(in) :: n, dim(n) \nS-3 integer :: i, s, err \nS-4 real, allocatable :: B(:) \nS-5 err = 0 \nS-6 !$omp parallel shared(err) \nS-7 ! ... \nS-8 !$omp do private(s, B) \nS-9 do i=1, n \nS-10 !$omp cancellation point do \nS-11 allocate(B(dim(i)), stat=s) \nS-12 if (s .gt. 0) then \nS-13 !$omp atomic write \nS-14 err = s \nS-15 !$omp cancel do \nS-16 endif \nS-17 ! ... \nS-18 ! deallocate private array B \nS-19 if (allocated(B)) then \nS-20 deallocate(B) \nS-21 endif \nS-22 enddo \nS-23 !$omp end parallel \nS-24 end subroutine \nFortran \n488 OpenMP Examples Version 5.2.1 - November 2022 \n1 The following example shows how to cancel a parallel search on a binary tree as soon as the search \n2 value has been detected. The code creates a task to descend into the child nodes of the current tree \n3 node. If the search value has been found, the code remembers the tree node with the found value \n4 through an atomic write to the result variable and then cancels execution of all search tasks. The \n5 function search_tree_parallel groups all search tasks into a single task group to control \n6 the effect of the cancel taskgroup directive. The level argument is used to create undeferred \n7 tasks after the first ten levels of the tree. \nC / C++ \n8 Example cancellation.2.c (omp_5.1) \nS-1 #include <stddef.h> \nS-2 \nS-3 typedef struct binary_tree_s { \nS-4 int value; \nS-5 struct binary_tree_s *left, *right; \nS-6 } binary_tree_t; \nS-7 \nS-8 binary_tree_t *search_tree(binary_tree_t *tree, int value, int level) { \nS-9 binary_tree_t *found = NULL; \nS-10 if (tree) { \nS-11 if (tree->value == value) { \nS-12 found = tree; \nS-13 } \nS-14 else { \nS-15 #pragma omp task shared(found) if(level < 10) \nS-16 { \nS-17 binary_tree_t *found_left = NULL; \nS-18 found_left = search_tree(tree->left, value, level + 1); \nS-19 if (found_left) { \nS-20 #pragma omp atomic write \nS-21 found = found_left; \nS-22 #pragma omp cancel taskgroup \nS-23 } \nS-24 } \nS-25 #pragma omp task shared(found) if(level < 10) \nS-26 { \nS-27 binary_tree_t *found_right = NULL; \nS-28 found_right = search_tree(tree->right, value, level + 1); \nS-29 if (found_right) { \nS-30 #pragma omp atomic write \nS-31 found = found_right; \nS-32 #pragma omp cancel taskgroup \nS-33 } \nS-34 } \nS-35 #pragma omp taskwait \nS-36 } \nCHAPTER 12. PROGRAM CONTROL 489 \nS-37 } \nS-38 return found; \nS-39 } \nS-40 binary_tree_t *search_tree_parallel(binary_tree_t *tree, int value) { \nS-41 binary_tree_t *found = NULL; \nS-42 #pragma omp parallel shared(found, tree, value) \nS-43 { \nS-44 #pragma omp masked \nS-45 { \nS-46 #pragma omp taskgroup \nS-47 { \nS-48 found = search_tree(tree, value, 0); \nS-49 } \nS-50 } \nS-51 } \nS-52 return found; \nS-53 } \nC / C++ \n1 The following is the equivalent parallel search example in Fortran. \nFortran \n2 Example cancellation.2.f90 (omp_5.1) \nS-1 module parallel_search \nS-2 type binary_tree \nS-3 integer :: value \nS-4 type(binary_tree), pointer :: right \nS-5 type(binary_tree), pointer :: left \nS-6 end type \nS-7 \nS-8 contains \nS-9 recursive subroutine search_tree(tree, value, level, found) \nS-10 type(binary_tree), intent(in), pointer :: tree \nS-11 integer, intent(in) :: value, level \nS-12 type(binary_tree), pointer :: found \nS-13 type(binary_tree), pointer :: found_left => NULL(), & \nS-14 found_right => NULL() \nS-15 \nS-16 if (associated(tree)) then \nS-17 if (tree%value .eq. value) then \nS-18 found => tree \nS-19 else \nS-20 !$omp task shared(found) if(level<10) \nS-21 call search_tree(tree%left, value, level+1, found_left) \nS-22 if (associated(found_left)) then \nS-23 !$omp critical \n490 OpenMP Examples Version 5.2.1 - November 2022 \nS-24 found => found_left \nS-25 !$omp end critical \nS-26 \nS-27 !$omp cancel taskgroup \nS-28 endif \nS-29 !$omp end task \nS-30 \nS-31 !$omp task shared(found) if(level<10) \nS-32 call search_tree(tree%right, value, level+1, found_right) \nS-33 if (associated(found_right)) then \nS-34 !$omp critical \nS-35 found => found_right \nS-36 !$omp end critical \nS-37 \nS-38 !$omp cancel taskgroup \nS-39 endif \nS-40 !$omp end task \nS-41 \nS-42 !$omp taskwait \nS-43 endif \nS-44 endif \nS-45 end subroutine \nS-46 \nS-47 subroutine search_tree_parallel(tree, value, found) \nS-48 type(binary_tree), intent(in), pointer :: tree \nS-49 integer, intent(in) :: value \nS-50 type(binary_tree), pointer :: found \nS-51 \nS-52 found => NULL() \nS-53 !$omp parallel shared(found, tree, value) \nS-54 !$omp masked \nS-55 !$omp taskgroup \nS-56 call search_tree(tree, value, 0, found) \nS-57 !$omp end taskgroup \nS-58 !$omp end masked \nS-59 !$omp end parallel \nS-60 end subroutine \nS-61 \nS-62 end module parallel_search \nFortran \nCHAPTER 12. PROGRAM CONTROL 491 \n"}
{"section_title": "12.5 requires Directive", "chunk": ""}
{"section_title": "12.5 requires Directive", "chunk": "2 The declarative requires directive can be used to specify features that an implementation must \n3 provide to compile and execute correctly. \n4 In the following example the unified_shared_memory clause of the requires directive \n5 ensures that the host and all devices accessible through OpenMP provide a unified address space \n6 for memory that is shared by all devices. \n7 The example illustrates the use of the requires directive specifying unified shared memory in \n8 file scope, before any device directives or device routines. No map clause is needed for the p \n9 structure on the device (and its address &p, for the C++ code, is the same address on the host and \n10 device). However, scalar variables referenced within the target construct still have a default \n11 data-sharing attribute of firstprivate. The q scalar is incremented on the device, and its change is \n12 not updated on the host. \nC++ \n13 Example requires.1.cpp (omp_5.0) \nS-1 #include <iostream> \nS-2 using namespace std; \nS-3 \nS-4 #pragma omp requires unified_shared_memory \nS-5 \nS-6 typedef struct mypoints \nS-7 { \nS-8 double res; \nS-9 double data[500]; \nS-10 } mypoints_t; \nS-11 \nS-12 void do_something_with_p(mypoints_t *p, int q); \nS-13 \nS-14 int main() \nS-15 { \nS-16 mypoints_t p; \nS-17 int q=0; \nS-18 \nS-19 #pragma omp target // no map clauses needed \nS-20 { // q is firstprivate \nS-21 q++; \nS-22 do_something_with_p(&p,q); \nS-23 } \nS-24 cout<< p.res << \" \" << q << endl; // output 1 0 \nS-25 return 0; \nS-26 } \nS-27 void do_something_with_p(mypoints_t *p, int q) \nS-28 { \nS-29 p->res = q; \n492 OpenMP Examples Version 5.2.1 - November 2022 \nS-30 for(int i=0;i<sizeof(p->data)/sizeof(double);i++) \nS-31 p->data[i]=q*i; \nS-32 } \nC++ \nFortran \n1 Example requires.1.f90 (omp_5.0) \nS-1 module data \nS-2 !$omp requires unified_shared_memory \nS-3 type,public :: mypoints \nS-4 double precision :: res \nS-5 double precision :: data(500) \nS-6 end type \nS-7 end module \nS-8 \nS-9 program main \nS-10 use data \nS-11 type(mypoints) :: p \nS-12 integer :: q=0 \nS-13 \nS-14 !$omp target !! no map clauses needed \nS-15 q = q + 1 !! q is firstprivate \nS-16 call do_something_with_p(p,q) \nS-17 !$omp end target \nS-18 \nS-19 write(*,\u2019(f5.0,i5)\u2019) p%res, q !! output 1. 0 \nS-20 \nS-21 end program \nS-22 \nS-23 subroutine do_something_with_p(p,q) \nS-24 use data \nS-25 type(mypoints) :: p \nS-26 integer :: q \nS-27 \nS-28 p%res = q; \nS-29 do i=1,size(p%data) \nS-30 p%data(i)=q*i \nS-31 enddo \nS-32 \nS-33 end subroutine \nFortran \nCHAPTER 12. PROGRAM CONTROL 493 \n"}
{"section_title": "12.6 declare variant Directive", "chunk": ""}
{"section_title": "12.6 declare variant Directive", "chunk": "2 A declare variant directive specifies an alternate function, function variant, to be used in \n3 place of the base function when the trait within the match clause matches the OpenMP context at a \n4 given call site. The base function follows the directive in the C and C++ languages. In Fortran, \n5 either a subroutine or function may be used as the base function, and the declare variant \n6 directive must be in the specification part of a subroutine or function (unless a base-proc-name \n7 modifier is used, as in the case of a procedure declaration statement). See the OpenMP 5.0 \n8 Specification for details on the modifier. \n9 When multiple declare variant directives are used a function variant becomes a candidate for \n10 replacing the base function if the context at the base function call matches the traits of all selectors \n11 in the match clause. If there are multiple candidates, a score is assigned with rules for each of the \n12 selector traits. The scoring algorithm can be found in the OpenMP 5.0 Specification. \n13 In the first example the vxv() function is called within a parallel region, a target region, and \n14 in a sequential part of the program. Two function variants, p_vxv() and t_vxv(), are defined for the \n15 first two regions by using parallel and target selectors (within the construct trait set) in a match \n16 clause. The p_vxv() function variant includes a for construct (do construct for Fortran) for the \n17 parallel region, while t_vxv() includes a distribute simd construct for the target \n18 region. The t_vxv() function is explicitly compiled for the device using a declare target directive. \n19 Since the two declare variant directives have no selectors that match traits for the context of \n20 the base function call in the sequential part of the program, the base vxv() function is used there, as \n21 expected. (The vectors in the p_vxv and t_vxv functions have been multiplied by 3 and 2, \n22 respectively, for checking the validity of the replacement. Normally the purpose of a function \n23 variant is to produce the same results by a different method.) \nC / C++ \n24 Example declare_variant.1.c (omp_5.1) \nS-1 #define N 100 \nS-2 #include <stdio.h> \nS-3 #include <omp.h> \nS-4 \nS-5 void p_vxv(int *v1,int *v2,int *v3,int n); \nS-6 void t_vxv(int *v1,int *v2,int *v3,int n); \nS-7 \nS-8 #pragma omp declare variant( p_vxv ) match( construct={parallel} ) \nS-9 #pragma omp declare variant( t_vxv ) match( construct={target} ) \nS-10 void vxv(int *v1,int *v2,int *v3,int n) // base function \nS-11 { \nS-12 for (int i= 0; i< n; i++) v3[i] = v1[i] * v2[i]; \nS-13 } \nS-14 \nS-15 void p_vxv(int *v1,int *v2,int *v3,int n) // function variant \nS-16 { \n494 OpenMP Examples Version 5.2.1 - November 2022 \nS-17 #pragma omp for \nS-18 for (int i= 0; i< n; i++) v3[i] = v1[i] * v2[i]*3; \nS-19 } \nS-20 \nS-21 #pragma omp begin declare target \nS-22 void t_vxv(int *v1,int *v2,int *v3,int n) // function variant \nS-23 { \nS-24 #pragma omp distribute simd \nS-25 for (int i= 0; i< n; i++) v3[i] = v1[i] * v2[i]*2; \nS-26 } \nS-27 #pragma omp end declare target \nS-28 \nS-29 int main() \nS-30 { \nS-31 int v1[N], v2[N], v3[N]; \nS-32 for(int i=0; i<N; i++){ v1[i]=(i+1); v2[i]=-(i+1); v3[i]=0; } //init \nS-33 \nS-34 #pragma omp parallel \nS-35 { \nS-36 vxv(v1,v2,v3,N); \nS-37 } \nS-38 printf(\" %d %d\\n\",v3[0],v3[N-1]); //from p_vxv -- output: -3 -30000 \nS-39 \nS-40 #pragma omp target teams map(to: v1[:N],v2[:N]) map(from: v3[:N]) \nS-41 { \nS-42 vxv(v1,v2,v3,N); \nS-43 } \nS-44 printf(\" %d %d\\n\",v3[0],v3[N-1]); //from t_vxv -- output: -2 -20000 \nS-45 \nS-46 vxv(v1,v2,v3,N); \nS-47 printf(\" %d %d\\n\",v3[0],v3[N-1]); //from vxv -- output: -1 -10000 \nS-48 \nS-49 return 0; \nS-50 } \nC / C++ \nFortran \n1 Example declare_variant.1.f90 (omp_5.0) \nS-1 module subs \nS-2 use omp_lib \nS-3 contains \nS-4 subroutine vxv(v1, v2, v3) !! base function \nS-5 integer,intent(in) :: v1(:),v2(:) \nS-6 integer,intent(out) :: v3(:) \nS-7 integer :: i,n \nS-8 !$omp declare variant( p_vxv ) match( construct={parallel} ) \nCHAPTER 12. PROGRAM CONTROL 495 \nS-9 !$omp declare variant( t_vxv ) match( construct={target} ) \nS-10 \nS-11 n=size(v1) \nS-12 do i = 1,n; v3(i) = v1(i) * v2(i); enddo \nS-13 \nS-14 end subroutine \nS-15 \nS-16 subroutine p_vxv(v1, v2, v3) !! function variant \nS-17 integer,intent(in) :: v1(:),v2(:) \nS-18 integer,intent(out) :: v3(:) \nS-19 integer :: i,n \nS-20 n=size(v1) \nS-21 \nS-22 !$omp do \nS-23 do i = 1,n; v3(i) = v1(i) * v2(i) * 3; enddo \nS-24 \nS-25 end subroutine \nS-26 \nS-27 subroutine t_vxv(v1, v2, v3) !! function variant \nS-28 integer,intent(in) :: v1(:),v2(:) \nS-29 integer,intent(out) :: v3(:) \nS-30 integer :: i,n \nS-31 !$omp declare target \nS-32 n=size(v1) \nS-33 \nS-34 !$omp distribute simd \nS-35 do i = 1,n; v3(i) = v1(i) * v2(i) * 2; enddo \nS-36 \nS-37 end subroutine \nS-38 \nS-39 end module subs \nS-40 \nS-41 \nS-42 program main \nS-43 use omp_lib \nS-44 use subs \nS-45 integer,parameter :: N = 100 \nS-46 integer :: v1(N), v2(N), v3(N) \nS-47 \nS-48 do i= 1,N; v1(i)= i; v2(i)= -i; v3(i)= 0; enddo !! init \nS-49 \nS-50 !$omp parallel \nS-51 call vxv(v1,v2,v3) \nS-52 !$omp end parallel \nS-53 print *, v3(1),v3(N) !! from p_vxv -- output: -3 -30000 \nS-54 \nS-55 !$omp target teams map(to: v1,v2) map(from: v3) \n496 OpenMP Examples Version 5.2.1 - November 2022 \nS-56 call vxv(v1,v2,v3) \nS-57 !$omp end target teams \nS-58 print *, v3(1),v3(N) !! from t_vxv -- output: -2 -20000 \nS-59 \nS-60 call vxv(v1,v2,v3) \nS-61 print *, v3(1),v3(N) !! from vxv -- output: -1 -10000 \nS-62 \nS-63 end program \nFortran \n1 In this example, traits from the device set are used to select a function variant. In the \n2 declare variant directive, an isa selector specifies that if the implementation of the \n3 \u201ccore-avx512\u201d instruction set is detected at compile time the avx512_saxpy() variant function is \n4 used for the call to base_saxpy(). \n5 A compilation of avx512_saxpy() is aware of the AVX-512 instruction set that supports 512-bit \n6 vector extensions (for Xeon or Xeon Phi architectures). Within avx512_saxpy(), the \n7 parallel for simd construct performs parallel execution, and takes advantage of 64-byte data \n8 alignment. When the avx512_saxpy() function variant is not selected, the base base_saxpy() \n9 function variant containing only a basic parallel for construct is used for the call to \n10 base_saxpy(). \nC / C++ \n11 Example declare_variant.2.c (omp_5.0) \nS-1 #include <omp.h> \nS-2 \nS-3 void base_saxpy(int, float, float *, float *); \nS-4 void avx512_saxpy(int, float, float *, float *); \nS-5 \nS-6 #pragma omp declare variant( avx512_saxpy ) \\ \nS-7 match( device={isa(\"core-avx512\")} ) \nS-8 void base_saxpy(int n, float s, float *x, float *y) // base function \nS-9 { \nS-10 #pragma omp parallel for \nS-11 for(int i=0; i<n; i++) y[i] = s*x[i] + y[i]; \nS-12 } \nS-13 \nS-14 void avx512_saxpy(int n, float s, float *x, float *y) //function variant \nS-15 { \nS-16 //assume 64-byte alignment for AVX-512 \nS-17 #pragma omp parallel for simd simdlen(16) aligned(x,y:64) \nS-18 for(int i=0; i<n; i++) y[i] = s*x[i] + y[i]; \nS-19 } \nS-20 \nS-21 // Above may be in another file scope. \nS-22 \nCHAPTER 12. PROGRAM CONTROL 497 \nS-23 #include <stdio.h> \nS-24 #include <stdlib.h> \nS-25 #include <stdint.h> \nS-26 #define N 1000 \nS-27 \nS-28 int main() \nS-29 { \nS-30 static float x[N],y[N] __attribute__ ((aligned(64))); \nS-31 float s=2.0; \nS-32 // Check for 64-byte aligned \nS-33 if( ((intptr_t)y)%64 != 0 || ((intptr_t)x)%64 != 0 ) \nS-34 { printf(\"ERROR: x|y not 64-Byte aligned\\n\"); exit(1); } \nS-35 \nS-36 for(int i=0; i<N; i++){ x[i]=i+1; y[i]=i+1; } // initialize \nS-37 \nS-38 base_saxpy(N,s,x,y); \nS-39 \nS-40 printf(\"y[0],y[N-1]: %5.0f %5.0f\\n\",y[0],y[N-1]); \nS-41 //output: y[0],y[N-1]: 3 3000 \nS-42 \nS-43 return 0; \nS-44 } \nC / C++ \nFortran \n1 Example declare_variant.2.f90 (omp_5.0) \nS-1 module subs \nS-2 use omp_lib \nS-3 contains \nS-4 \nS-5 subroutine base_saxpy(s,x,y) !! base function \nS-6 real,intent(inout) :: s,x(:),y(:) \nS-7 !$omp declare variant( avx512_saxpy ) & \nS-8 !$omp& match( device={isa(\"core-avx512\")} ) \nS-9 \nS-10 y = s*x + y \nS-11 \nS-12 end subroutine \nS-13 \nS-14 subroutine avx512_saxpy(s,x,y) !! function variant \nS-15 real,intent(inout) :: s,x(:),y(:) \nS-16 integer :: i,n \nS-17 n=size(x) \nS-18 !!assume 64-byte alignment for AVX-512 \nS-19 !$omp parallel do simd simdlen(16) aligned(x,y: 64) \nS-20 do i = 1,n \n498 OpenMP Examples Version 5.2.1 - November 2022 \nS-21 y(i) = s*x(i) + y(i) \nS-22 end do \nS-23 \nS-24 end subroutine \nS-25 \nS-26 end module subs \nS-27 \nS-28 \nS-29 program main \nS-30 use omp_lib \nS-31 use subs \nS-32 \nS-33 integer, parameter :: N=1000, align=64 \nS-34 real, allocatable :: x(:),y(:) \nS-35 real :: s = 2.0e0 \nS-36 integer :: i \nS-37 \nS-38 allocate(x(N),y(N)) !! Assumes allocation is 64-byte aligned \nS-39 !! (using compiler options, or another \nS-40 !! allocation method). \nS-41 \nS-42 !! loc is non-standard, but found everywhere \nS-43 !! remove these lines if not available \nS-44 if(modulo(loc(x),align) /= 0 .and. modulo(loc(y),align) /=0 ) then \nS-45 print*,\"ERROR: x|y not 64-byte aligned\"; stop \nS-46 endif \nS-47 \nS-48 do i=1,N !! initialize \nS-49 x(i)=i \nS-50 y(i)=i \nS-51 end do \nS-52 \nS-53 call base_saxpy(s,x,y) \nS-54 \nS-55 write(*,\u2019(\"y(1),y(N):\",2f6.0)\u2019) y(1),y(N) !!output: y... 3. 3000. \nS-56 \nS-57 deallocate(x,y) \nS-58 \nS-59 end program \nFortran \nCHAPTER 12. PROGRAM CONTROL 499 \n"}
{"section_title": "12.7 Metadirectives", "chunk": ""}
{"section_title": "12.7 Metadirectives", "chunk": "2 A metadirective directive provides a mechanism to select a directive in a when clause to be \n3 used, depending upon one or more contexts: implementation, available devices and the present \n4 enclosing construct. The directive in an otherwise clause is used when a directive of the when \n5 clause is not selected. \n6 In the when clause the context selector (or just selector) defines traits that are evaluated for \n7 selection of the directive that follows the selector. This \u201cselectables\u201d directive is called a directive \n8 variant. Traits are grouped by construct, implementation and device sets to be used by a selector of \n9 the same name. \n10 In the first example the architecture trait arch of the device selector set specifies that if an nvptx \n11 architecture is active in the OpenMP context, then the teams loop directive variant is selected as \n12 the directive; otherwise, the parallel loop directive variant of the otherwise clause is \n13 selected as the directive. That is, if a device of nvptx architecture is supported by the \n14 implementation within the enclosing target construct, its directive variant is selected. The \n15 architecture names, such as nvptx, are implementation defined. Also, note that device as used in a \n16 target construct specifies a device number, while device, as used in the metadirective \n17 directive as selector set, has traits of kind, isa and arch. \nC / C++ \n18 Example metadirective.1.c (omp_5.2) \nS-1 #define N 100 \nS-2 #include <stdio.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 int v1[N], v2[N], v3[N]; \nS-7 for(int i=0; i<N; i++){ v1[i]=(i+1); v2[i]=-(i+1); } \nS-8 \nS-9 #pragma omp target map(to:v1,v2) map(from:v3) device(0) \nS-10 #pragma omp metadirective \\ \nS-11 when( device={arch(\"nvptx\")}: teams loop) \\ \nS-12 otherwise( parallel loop) \nS-13 for (int i= 0; i< N; i++) v3[i] = v1[i] * v2[i]; \nS-14 \nS-15 printf(\" %d %d\\n\",v3[0],v3[N-1]); //output: -1 -10000 \nS-16 \nS-17 return 0; \nS-18 } \nC / C++ \n500 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example metadirective.1.f90 (omp_5.2) \nS-1 program main \nS-2 integer, parameter :: N= 100 \nS-3 integer :: v1(N), v2(N), v3(N); \nS-4 \nS-5 do i=1,N; v1(i)=i; v2(i)=-i; enddo ! initialize \nS-6 \nS-7 !$omp target map(to:v1,v2) map(from:v3) device(0) \nS-8 !$omp metadirective & \nS-9 !$omp& when( device={arch(\"nvptx\")}: teams loop) & \nS-10 !$omp& otherwise( parallel loop) \nS-11 do i= 1,N; v3(i) = v1(i) * v2(i); enddo \nS-12 !$omp end target \nS-13 \nS-14 print *, v3(1),v3(N) !!output: -1 -10000 \nS-15 end program \nFortran \n2 In the second example, the implementation selector set is specified in the when clause to distinguish \n3 between platforms. Additionally, specific architectures are specified with the device selector set. \n4 In the code, different teams constructs are employed as determined by the metadirective \n5 directive. The number of teams is restricted by a num_teams clause and a thread limit is also set \n6 by a thread_limit clause for vendor platforms and specific architecture traits. Otherwise, just \n7 the teams construct is used without any clauses, as prescribed by the otherwise clause. \nC / C++ \n8 Example metadirective.2.c (omp_5.2) \nS-1 #define N 100 \nS-2 #include <stdio.h> \nS-3 #include <omp.h> \nS-4 \nS-5 void work_on_chunk(int idev, int i); \nS-6 \nS-7 int main() //Driver \nS-8 { \nS-9 int i,idev; \nS-10 \nS-11 for (idev=0; idev<omp_get_num_devices(); idev++) \nS-12 { \nS-13 #pragma omp target device(idev) \nS-14 #pragma omp metadirective \\ \nS-15 when( implementation={vendor(nvidia)}, \\ \nS-16 device={arch(\"kepler\")}: \\ \nCHAPTER 12. PROGRAM CONTROL 501 \nS-17 teams num_teams(512) thread_limit(32) ) \\ \nS-18 when( implementation={vendor(amd)}, \\ \nS-19 device={arch(\"fiji\" )}: \\ \nS-20 teams num_teams(512) thread_limit(64) ) \\ \nS-21 otherwise( \\ \nS-22 teams) \nS-23 #pragma omp distribute parallel for \nS-24 for (i=0; i<N; i++) work_on_chunk(idev,i); \nS-25 } \nS-26 return 0; \nS-27 } \nC / C++ \nFortran \n1 Example metadirective.2.f90 (omp_5.2) \nS-1 program main !!Driver \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer, parameter :: N=1000 \nS-5 external :: work_on_chunk \nS-6 integer :: i,idev \nS-7 \nS-8 do idev=0,omp_get_num_devices()-1 \nS-9 \nS-10 !$omp target device(idev) \nS-11 !$omp begin metadirective & \nS-12 !$omp& when( implementation={vendor(nvidia)}, & \nS-13 !$omp& device={arch(\"kepler\")}: & \nS-14 !$omp& teams num_teams(512) thread_limit(32) ) & \nS-15 !$omp& when( implementation={vendor(amd)}, & \nS-16 !$omp& device={arch(\"fiji\" )}: & \nS-17 !$omp& teams num_teams(512) thread_limit(64) ) & \nS-18 !$omp& otherwise( teams ) \nS-19 !$omp distribute parallel do \nS-20 do i=1,N \nS-21 call work_on_chunk(idev,i) \nS-22 end do \nS-23 !$omp end metadirective \nS-24 !$omp end target \nS-25 \nS-26 end do \nS-27 \nS-28 end program \nFortran \n502 OpenMP Examples Version 5.2.1 - November 2022 \n1 In the third example, a construct selector set is specified in the when clause. Here, a \n2 metadirective directive is used within a function that is also compiled as a function for a \n3 target device as directed by a declare target directive. The target directive name of the \n4 construct selector ensures that the distribute parallel for/do construct is employed \n5 for the target compilation. Otherwise, for the host-compiled version the \n6 parallel for/do simd construct is used. \n7 In the first call to the exp_pi_diff() routine the context is a target teams construct and the \n8 distribute parallel for/do construct version of the function is invoked, while in the \n9 second call the parallel for/do simd construct version is used. \n10 This case illustrates an important point for users that may want to hoist the target directive out of \n11 a function that contains the usual target teams distribute parallel for/do construct \n12 (for providing alternate constructs through the metadirective directive as here). While this \n13 combined construct can be decomposed into a target and teams distribute parallel \n14 for/do constructs, the OpenMP 5.0 specification has the restriction: \u201cIf a teams construct is \n15 nested within a target construct, that target construct must contain no statements, declarations \n16 or directives outside of the teams construct\u201d. So, the teams construct must immediately follow \n17 the target construct without any intervening code statements (which includes function calls). \n18 Since the target construct alone cannot be hoisted out of a function, the target teams \n19 construct has been hoisted out of the function, and the distribute parallel for/do \n20 construct is used as the variant directive of the metadirective directive within the function. \nC / C++ \n21 Example metadirective.3.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 #include <math.h> \nS-3 #define N 1000 \nS-4 \nS-5 #pragma omp begin declare target \nS-6 void exp_pi_diff(double *d, double my_pi){ \nS-7 #pragma omp metadirective \\ \nS-8 when( construct={target}: distribute parallel for ) \\ \nS-9 otherwise( parallel for simd ) \nS-10 for(int i = 0; i<N; i++) d[i] = exp( (M_PI-my_pi)*i ); \nS-11 } \nS-12 #pragma omp end declare target \nS-13 \nS-14 int main() \nS-15 { \nS-16 //Calculates sequence of exponentials: (M_PI-my_pi) * index \nS-17 //M_PI is from math.h, and my_pi is user provided. \nS-18 \nS-19 double d[N]; \nS-20 double my_pi=3.14159265358979e0; \nS-21 \nCHAPTER 12. PROGRAM CONTROL 503 \nS-22 #pragma omp target teams map(tofrom: d[0:N]) \nS-23 exp_pi_diff(d,my_pi); \nS-24 // value should be near 1 \nS-25 printf(\"d[N-1] = %20.14f\\n\",d[N-1]); // ...= 1.00000000000311 \nS-26 \nS-27 exp_pi_diff(d,my_pi); // value should be near 1 \nS-28 printf(\"d[N-1] = %20.14f\\n\",d[N-1]); // ...= 1.00000000000311 \nS-29 } \nC / C++ \nFortran \n1 Example metadirective.3.f90 (omp_5.2) \nS-1 module params \nS-2 integer, parameter :: N=1000 \nS-3 DOUBLE PRECISION, PARAMETER::M_PI=4.0d0*DATAN(1.0d0) \nS-4 ! 3.1415926535897932_8 \nS-5 end module \nS-6 \nS-7 \nS-8 subroutine exp_pi_diff(d, my_pi) \nS-9 use params \nS-10 implicit none \nS-11 integer :: i \nS-12 double precision :: d(N), my_pi \nS-13 !$omp declare target \nS-14 \nS-15 !$omp metadirective & \nS-16 !$omp& when( construct={target}: distribute parallel do ) & \nS-17 !$omp& otherwise( parallel do simd ) \nS-18 \nS-19 do i = 1,size(d) \nS-20 d(i) = exp( (M_PI-my_pi)*i ) \nS-21 end do \nS-22 \nS-23 end subroutine \nS-24 \nS-25 program main \nS-26 ! Calculates sequence of exponentials: (M_PI-my_pi) * index \nS-27 ! M_PI is from usual way, and my_pi is user provided. \nS-28 ! Fortran Standard does not provide PI \nS-29 \nS-30 use params \nS-31 implicit none \nS-32 double precision :: d(N) \nS-33 double precision :: my_pi=3.14159265358979d0 \nS-34 \n504 OpenMP Examples Version 5.2.1 - November 2022 \nS-35 !$omp target teams map(from: d) \nS-36 call exp_pi_diff(d,my_pi) \nS-37 !$omp end target teams \nS-38 ! value should be near 1 \nS-39 print*, \"d(N) = \",d(N) ! 1.00000000000311 \nS-40 \nS-41 call exp_pi_diff(d,my_pi) ! value should be near 1 \nS-42 print*, \"d(N) = \",d(N) ! 1.00000000000311 \nS-43 \nS-44 end program \nFortran \n1 The user selector set can be used in a metadirective to select directives at execution time when the \n2 condition( boolean-expr ) selector expression is not a constant expression. In this case it is a \n3 dynamic trait set, and the selection is made at run time, rather than at compile time. \n4 In the following example the foo function employs the condition selector to choose a device for \n5 execution at run time. In the bar routine metadirectives are nested. At the outer level a selection \n6 between serial and parallel execution in performed at run time, followed by another run time \n7 selection on the schedule kind in the inner level when the active construct trait is parallel. \n8 (Note, the variable b in two of the \u201cselected\u201d constructs is declared private for the sole purpose of \n9 detecting and reporting that the construct is used. Since the variable is private, its value is \n10 unchanged outside of the construct region, whereas it is changed if the \u201cunselected\u201d construct is \n11 used.) \nC / C++ \n12 Example metadirective.4.c (omp_5.2) \nS-1 #define N 100 \nS-2 #include <stdbool.h> \nS-3 #include <stdlib.h> \nS-4 #include <stdio.h> \nS-5 #include <omp.h> \nS-6 \nS-7 void foo(int *a, int n, bool use_gpu) \nS-8 { \nS-9 int b=0; // use b to detect if run on gpu \nS-10 \nS-11 #pragma omp metadirective \\ \nS-12 when( user={condition(use_gpu)}: \\ \nS-13 target teams distribute parallel for \\ \nS-14 private(b) map(from:a[0:n]) ) \\ \nS-15 otherwise( \\ \nS-16 parallel for ) \nS-17 for (int i=0; i<n; i++) {a[i]=i; if(i==n-1) b=1;} \nS-18 \nCHAPTER 12. PROGRAM CONTROL 505 \nS-19 if(b==0) printf(\"PASSED 1 of 3\\n\"); \nS-20 } \nS-21 \nS-22 void bar (int *a, int n, bool run_parallel, bool unbalanced) \nS-23 { \nS-24 int b=0; \nS-25 #pragma omp metadirective \\ \nS-26 when(user={condition(run_parallel)}: parallel) \nS-27 { \nS-28 if(omp_in_parallel() && omp_get_thread_num() == 0) \nS-29 printf(\"PASSED 2 of 3\\n\"); \nS-30 \nS-31 #pragma omp metadirective \\ \nS-32 when( construct={parallel}, \\ \nS-33 user={condition(unbalanced)}: for schedule(guided) \\ \nS-34 private(b)) \\ \nS-35 when( construct={parallel} : for schedule(static)) \nS-36 for (int i=0; i<n; i++) {a[i]=i; if(i==n-1) b=1;} \nS-37 } \nS-38 // if guided b=0, because b is private \nS-39 if(b==0) printf(\"PASSED 3 of 3\\n\"); \nS-40 } \nS-41 \nS-42 void foo(int *a, int n, bool use_gpu); \nS-43 void bar(int *a, int n, bool run_parallel, bool unbalanced); \nS-44 \nS-45 int main(){ \nS-46 \nS-47 int p[N]; \nS-48 // App normally sets these, dependent on input parameters \nS-49 bool use_gpu=true, run_parallel=true, unbalanced=true; \nS-50 \nS-51 // Testing: set Env Var MK_FAIL to anything to fail tests \nS-52 if(getenv(\"MK_FAIL\")!=NULL) { \nS-53 use_gpu=false; run_parallel=false; unbalanced=false; \nS-54 } \nS-55 \nS-56 foo(p, N, use_gpu); \nS-57 bar(p, N, run_parallel,unbalanced); \nS-58 \nS-59 return 0; \nS-60 } \nC / C++ \n506 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example metadirective.4.f90 (omp_5.2) \nS-1 subroutine foo(a, n, use_gpu) \nS-2 integer :: n, a(n) \nS-3 logical :: use_gpu \nS-4 \nS-5 integer :: b=0 !! use b to detect if run on gpu \nS-6 \nS-7 !$omp metadirective & \nS-8 !$omp& when(user={condition(use_gpu)}: & \nS-9 !$omp& target teams distribute parallel for & \nS-10 !$omp& private(b) map(from:a(1:n)) ) & \nS-11 !$omp& otherwise( & \nS-12 !$omp& parallel do) \nS-13 do i = 1,n; a(i)=i; if(i==n) b=1; end do \nS-14 \nS-15 if(b==0) print *, \"PASSED 1 of 3\" ! bc b is firstprivate for gpu run \nS-16 end subroutine \nS-17 \nS-18 subroutine bar (a, n, run_parallel, unbalanced) \nS-19 use omp_lib, only : omp_get_thread_num, omp_in_parallel \nS-20 integer :: n, a(n) \nS-21 logical :: run_parallel, unbalanced \nS-22 \nS-23 integer :: b=0 \nS-24 !$omp begin metadirective when(user={condition(run_parallel)}: parallel) \nS-25 \nS-26 if(omp_in_parallel() .and. omp_get_thread_num() == 0) & \nS-27 print *,\"PASSED 2 of 3\" \nS-28 \nS-29 !$omp metadirective & \nS-30 !$omp& when(construct={parallel}, user={condition(unbalanced)}: & \nS-31 !$omp& for schedule(guided) private(b)) & \nS-32 !$omp& when(construct={parallel}: for schedule(static)) \nS-33 do i = 1,n; a(i)=i; if(i==n) b=1; end do \nS-34 \nS-35 !$omp end metadirective \nS-36 \nS-37 if(b==0) print *, \"PASSED 3 of 3\" !!if guided, b=0 since b is private \nS-38 end subroutine \nS-39 \nS-40 program meta \nS-41 use omp_lib \nS-42 integer, parameter :: N=100 \nS-43 integer :: p(N) \nS-44 integer :: env_stat \nCHAPTER 12. PROGRAM CONTROL 507 \nS-45 !! App normally sets these, dependent on input parameters \nS-46 logical :: use_gpu=.true., run_parallel=.true., unbalanced=.true. \nS-47 \nS-48 !! Testing: set Env Var MK_FAIL to anything to fail tests \nS-49 call get_environment_variable(\u2019MK_FAIL\u2019,status=env_stat) \nS-50 if(env_stat /= 1) then ! status =1 when not set! \nS-51 use_gpu=.false.; run_parallel=.false.; unbalanced=.false. \nS-52 endif \nS-53 \nS-54 \nS-55 call foo(p, N, use_gpu) \nS-56 call bar(p, N, run_parallel,unbalanced) \nS-57 \nS-58 end program \nFortran \n1 Metadirectives can be used in conjunction with templates as shown in the C++ code below. Here \n2 the template definition generates two versions of the Fibonacci function. The tasking boolean is \n3 used in the condition selector to enable tasking. The true form implements a parallel version \n4 with task and taskwait constructs as in the tasking.4.c code in Section 5.1. The false form \n5 implements a serial version without any tasking constructs. Note that the serial version is used in \n6 the parallel function for optimally processing numbers less than 8. \nC++ \n7 Example metadirective.5.cpp (omp_5.0) \nS-1 #include <stdio.h> \nS-2 \nS-3 // revised Fibonacci from tasking.4.c example \nS-4 \nS-5 template <bool tasking> \nS-6 int fib(int n) { \nS-7 int i, j; \nS-8 if (n<2) { \nS-9 return n; \nS-10 } else if ( tasking && n<8 ) { // serial/taskless cutoff for n<8 \nS-11 return fib<false>(n); \nS-12 } else { \nS-13 #pragma omp metadirective \\ \nS-14 when(user={condition(tasking)}: task shared(i)) \nS-15 { \nS-16 i=fib<tasking>(n-1); \nS-17 } \nS-18 #pragma omp metadirective \\ \nS-19 when(user={condition(tasking)}: task shared(j)) \nS-20 { \nS-21 j=fib<tasking>(n-2); \n508 OpenMP Examples Version 5.2.1 - November 2022 \nS-22 } \nS-23 #pragma omp metadirective \\ \nS-24 when(user={condition(tasking)}: taskwait) \nS-25 return i+j; \nS-26 } \nS-27 } \nS-28 \nS-29 int main(int argc, char** argv) { \nS-30 int n = 15; \nS-31 #pragma omp parallel \nS-32 #pragma omp single \nS-33 { \nS-34 printf(\"fib(%i) = %i\\n\", n, fib<true>(n)); \nS-35 } \nS-36 return 0; \nS-37 } \nS-38 // OUTPUT: \nS-39 // fib(15) = 610 \nC++ \nCHAPTER 12. PROGRAM CONTROL 509 \n"}
{"section_title": "12.8 Nested Loop Constructs", "chunk": ""}
{"section_title": "12.8 Nested Loop Constructs", "chunk": "2 The following example of loop construct nesting is conforming because the inner and outer loop \n3 regions bind to different parallel regions: \nC / C++ \n4 Example nested_loop.1.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 \nS-3 void good_nesting(int n) \nS-4 { \nS-5 int i, j; \nS-6 #pragma omp parallel default(shared) \nS-7 { \nS-8 #pragma omp for \nS-9 for (i=0; i<n; i++) { \nS-10 #pragma omp parallel shared(i, n) \nS-11 { \nS-12 #pragma omp for \nS-13 for (j=0; j < n; j++) \nS-14 work(i, j); \nS-15 } \nS-16 } \nS-17 } \nS-18 } \nC / C++ \nFortran \n5 Example nested_loop.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I, J) \nS-2 INTEGER I, J \nS-3 END SUBROUTINE WORK \nS-4 \nS-5 SUBROUTINE GOOD_NESTING(N) \nS-6 INTEGER N \nS-7 \nS-8 INTEGER I \nS-9 !$OMP PARALLEL DEFAULT(SHARED) \nS-10 !$OMP DO \nS-11 DO I = 1, N \nS-12 !$OMP PARALLEL SHARED(I,N) \nS-13 !$OMP DO \nS-14 DO J = 1, N \nS-15 CALL WORK(I,J) \nS-16 END DO \n510 OpenMP Examples Version 5.2.1 - November 2022 \nS-17 !$OMP END PARALLEL \nS-18 END DO \nS-19 !$OMP END PARALLEL \nS-20 END SUBROUTINE GOOD_NESTING \nFortran \n1 The following variation of the preceding example is also conforming: \nC / C++ \n2 Example nested_loop.2.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 \nS-3 \nS-4 void work1(int i, int n) \nS-5 { \nS-6 int j; \nS-7 #pragma omp parallel default(shared) \nS-8 { \nS-9 #pragma omp for \nS-10 for (j=0; j<n; j++) \nS-11 work(i, j); \nS-12 } \nS-13 } \nS-14 \nS-15 \nS-16 void good_nesting2(int n) \nS-17 { \nS-18 int i; \nS-19 #pragma omp parallel default(shared) \nS-20 { \nS-21 #pragma omp for \nS-22 for (i=0; i<n; i++) \nS-23 work1(i, n); \nS-24 } \nS-25 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 511 \nFortran \n1 Example nested_loop.2.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I, J) \nS-2 INTEGER I, J \nS-3 END SUBROUTINE WORK \nS-4 \nS-5 SUBROUTINE WORK1(I, N) \nS-6 INTEGER J \nS-7 !$OMP PARALLEL DEFAULT(SHARED) \nS-8 !$OMP DO \nS-9 DO J = 1, N \nS-10 CALL WORK(I,J) \nS-11 END DO \nS-12 !$OMP END PARALLEL \nS-13 END SUBROUTINE WORK1 \nS-14 \nS-15 SUBROUTINE GOOD_NESTING2(N) \nS-16 INTEGER N \nS-17 !$OMP PARALLEL DEFAULT(SHARED) \nS-18 !$OMP DO \nS-19 DO I = 1, N \nS-20 CALL WORK1(I, N) \nS-21 END DO \nS-22 !$OMP END PARALLEL \nS-23 END SUBROUTINE GOOD_NESTING2 \nFortran \n512 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "12.9 Restrictions on Nesting of Regions", "chunk": ""}
{"section_title": "12.9 Restrictions on Nesting of Regions", "chunk": "2 The examples in this section illustrate the region nesting rules. \n3 The following example is non-conforming because the inner and outer loop regions are closely \n4 nested: \nC / C++ \n5 Example nesting_restrict.1.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 \nS-3 void wrong1(int n) \nS-4 { \nS-5 \nS-6 #pragma omp parallel default(shared) \nS-7 { \nS-8 int i, j; \nS-9 #pragma omp for \nS-10 for (i=0; i<n; i++) { \nS-11 /* incorrect nesting of loop regions */ \nS-12 #pragma omp for \nS-13 for (j=0; j<n; j++) \nS-14 work(i, j); \nS-15 } \nS-16 } \nS-17 \nS-18 } \nC / C++ \nFortran \n6 Example nesting_restrict.1.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK(I, J) \nS-2 INTEGER I, J \nS-3 \nS-4 END SUBROUTINE WORK \nS-5 \nS-6 SUBROUTINE WRONG1(N) \nS-7 \nS-8 INTEGER N \nS-9 INTEGER I,J \nS-10 !$OMP PARALLEL DEFAULT(SHARED) \nS-11 !$OMP DO \nS-12 DO I = 1, N \nS-13 !$OMP DO ! incorrect nesting of loop regions \nS-14 DO J = 1, N \nS-15 CALL WORK(I,J) \nCHAPTER 12. PROGRAM CONTROL 513 \nS-16 END DO \nS-17 END DO \nS-18 !$OMP END PARALLEL \nS-19 \nS-20 END SUBROUTINE WRONG1 \nFortran \n1 The following orphaned version of the preceding example is also non-conforming: \nC / C++ \n2 Example nesting_restrict.2.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 void work1(int i, int n) \nS-3 { \nS-4 int j; \nS-5 /* incorrect nesting of loop regions */ \nS-6 #pragma omp for \nS-7 for (j=0; j<n; j++) \nS-8 work(i, j); \nS-9 } \nS-10 \nS-11 void wrong2(int n) \nS-12 { \nS-13 #pragma omp parallel default(shared) \nS-14 { \nS-15 int i; \nS-16 #pragma omp for \nS-17 for (i=0; i<n; i++) \nS-18 work1(i, n); \nS-19 } \nS-20 } \nC / C++ \n514 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example nesting_restrict.2.f (pre_omp_3.0) \nS-1 SUBROUTINE WORK1(I,N) \nS-2 INTEGER I, N \nS-3 INTEGER J \nS-4 !$OMP DO ! incorrect nesting of loop regions \nS-5 DO J = 1, N \nS-6 CALL WORK(I,J) \nS-7 END DO \nS-8 END SUBROUTINE WORK1 \nS-9 SUBROUTINE WRONG2(N) \nS-10 INTEGER N \nS-11 INTEGER I \nS-12 !$OMP PARALLEL DEFAULT(SHARED) \nS-13 !$OMP DO \nS-14 DO I = 1, N \nS-15 CALL WORK1(I,N) \nS-16 END DO \nS-17 !$OMP END PARALLEL \nS-18 END SUBROUTINE WRONG2 \nFortran \n2 The following example is non-conforming because the loop and single regions are closely nested: \nC / C++ \n3 Example nesting_restrict.3.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 void wrong3(int n) \nS-3 { \nS-4 #pragma omp parallel default(shared) \nS-5 { \nS-6 int i; \nS-7 #pragma omp for \nS-8 for (i=0; i<n; i++) { \nS-9 /* incorrect nesting of regions */ \nS-10 #pragma omp single \nS-11 work(i, 0); \nS-12 } \nS-13 } \nS-14 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 515 \nFortran \n1 Example nesting_restrict.3.f (pre_omp_3.0) \nS-1 SUBROUTINE WRONG3(N) \nS-2 INTEGER N \nS-3 \nS-4 INTEGER I \nS-5 !$OMP PARALLEL DEFAULT(SHARED) \nS-6 !$OMP DO \nS-7 DO I = 1, N \nS-8 !$OMP SINGLE ! incorrect nesting of regions \nS-9 CALL WORK(I, 1) \nS-10 !$OMP END SINGLE \nS-11 END DO \nS-12 !$OMP END PARALLEL \nS-13 END SUBROUTINE WRONG3 \nFortran \n2 The following example is non-conforming because a barrier region cannot be closely nested \n3 inside a loop region: \nC / C++ \n4 Example nesting_restrict.4.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 void wrong4(int n) \nS-3 { \nS-4 \nS-5 #pragma omp parallel default(shared) \nS-6 { \nS-7 int i; \nS-8 #pragma omp for \nS-9 for (i=0; i<n; i++) { \nS-10 work(i, 0); \nS-11 /* incorrect nesting of barrier region in a loop region */ \nS-12 #pragma omp barrier \nS-13 work(i, 1); \nS-14 } \nS-15 } \nS-16 } \nC / C++ \n516 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example nesting_restrict.4.f (pre_omp_3.0) \nS-1 SUBROUTINE WRONG4(N) \nS-2 INTEGER N \nS-3 \nS-4 INTEGER I \nS-5 !$OMP PARALLEL DEFAULT(SHARED) \nS-6 !$OMP DO \nS-7 DO I = 1, N \nS-8 CALL WORK(I, 1) \nS-9 ! incorrect nesting of barrier region in a loop region \nS-10 !$OMP BARRIER \nS-11 CALL WORK(I, 2) \nS-12 END DO \nS-13 !$OMP END PARALLEL \nS-14 END SUBROUTINE WRONG4 \nFortran \n2 The following example is non-conforming because the barrier region cannot be closely nested \n3 inside the critical region. If this were permitted, it would result in deadlock due to the fact that \n4 only one thread at a time can enter the critical region: \nC / C++ \n5 Example nesting_restrict.5.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 void wrong5(int n) \nS-3 { \nS-4 #pragma omp parallel \nS-5 { \nS-6 #pragma omp critical \nS-7 { \nS-8 work(n, 0); \nS-9 /* incorrect nesting of barrier region in a critical region */ \nS-10 #pragma omp barrier \nS-11 work(n, 1); \nS-12 } \nS-13 } \nS-14 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 517 \nFortran \n1 Example nesting_restrict.5.f (pre_omp_3.0) \nS-1 SUBROUTINE WRONG5(N) \nS-2 INTEGER N \nS-3 \nS-4 !$OMP PARALLEL DEFAULT(SHARED) \nS-5 !$OMP CRITICAL \nS-6 CALL WORK(N,1) \nS-7 ! incorrect nesting of barrier region in a critical region \nS-8 !$OMP BARRIER \nS-9 CALL WORK(N,2) \nS-10 !$OMP END CRITICAL \nS-11 !$OMP END PARALLEL \nS-12 END SUBROUTINE WRONG5 \nFortran \n2 The following example is non-conforming because the barrier region cannot be closely nested \n3 inside the single region. If this were permitted, it would result in deadlock due to the fact that \n4 only one thread executes the single region: \nC / C++ \n5 Example nesting_restrict.6.c (pre_omp_3.0) \nS-1 void work(int i, int j) {} \nS-2 void wrong6(int n) \nS-3 { \nS-4 #pragma omp parallel \nS-5 { \nS-6 #pragma omp single \nS-7 { \nS-8 work(n, 0); \nS-9 /* incorrect nesting of barrier region in a single region */ \nS-10 #pragma omp barrier \nS-11 work(n, 1); \nS-12 } \nS-13 } \nS-14 } \nC / C++ \n518 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example nesting_restrict.6.f (pre_omp_3.0) \nS-1 SUBROUTINE WRONG6(N) \nS-2 INTEGER N \nS-3 \nS-4 !$OMP PARALLEL DEFAULT(SHARED) \nS-5 !$OMP SINGLE \nS-6 CALL WORK(N,1) \nS-7 ! incorrect nesting of barrier region in a single region \nS-8 !$OMP BARRIER \nS-9 CALL WORK(N,2) \nS-10 !$OMP END SINGLE \nS-11 !$OMP END PARALLEL \nS-12 END SUBROUTINE WRONG6 \nFortran \nCHAPTER 12. PROGRAM CONTROL 519 \n"}
{"section_title": "12.10 Target Offload", "chunk": ""}
{"section_title": "12.10 Target Offload", "chunk": "2 In the OpenMP 5.0 implementation the OMP_TARGET_OFFLOAD environment variable was \n3 defined to change default offload behavior. By default the target code (region) is executed on the \n4 host if the target device does not exist or the implementation does not support the target device. \n5 In an OpenMP 5.0 compliant implementation, setting the OMP_TARGET_OFFLOAD variable to \n6 MANDATORY will force the program to terminate execution when a target construct is \n7 encountered and the target device is not supported or is not available. With a value DEFAULT the \n8 target region will execute on a device if the device exists and is supported by the implementation, \n9 otherwise it will execute on the host. Support for the DISABLED value is optional; when it is \n10 supported the behavior is as if only the host device exists (other devices are considered non-existent \n11 to the runtime), and target regions are executed on the host. \n12 The following example reports execution behavior for different values of the \n13 OMP_TARGET_OFFLOAD variable. A handy routine for extracting the OMP_TARGET_OFFLOAD \n14 environment variable value is deployed here, because the OpenMP API does not have a routine for \n15 obtaining the value. \n16 Note: The example issues a warning when a pre-5.0 implementation is used, indicating that the \n17 OMP_TARGET_OFFLOAD is ignored. The value of the OMP_TARGET_OFFLOAD variable is \n18 reported when the OMP_DISPLAY_ENV environment variable is set to TRUE or VERBOSE. \nC / C++ \n19 Example target_offload_control.1.c (omp_5.0) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 #include <ctype.h> \nS-4 #include <stdlib.h> \nS-5 #include <string.h> \nS-6 \nS-7 typedef enum offload_policy \nS-8 {MANDATORY, DISABLED, DEFAULT, UNKNOWN, NOTSET} offload_policy_t; \nS-9 \nS-10 \nS-11 offload_policy_t get_offload_policy() \nS-12 { \nS-13 char *env, *end; \nS-14 size_t n; \nS-15 \nS-16 env = getenv(\"OMP_TARGET_OFFLOAD\"); \nS-17 if(env == NULL) return NOTSET; \nS-18 \nS-19 end = env + strlen(env); //Find trimmed beginning/end \nS-20 while ( *env && isspace(*(env )) ) env++; \nS-21 while (end != env && isspace(*(end-1)) ) end--; \n520 OpenMP Examples Version 5.2.1 - November 2022 \nS-22 n = (int)(end - env); \nS-23 \nS-24 //Find ONLY string -nothing more, case insensitive \nS-25 if (n == 9 && !strncasecmp(env, \"MANDATORY\",n)) return MANDATORY; \nS-26 else if (n == 8 && !strncasecmp(env, \"DISABLED\" ,n)) return DISABLED ; \nS-27 else if (n == 7 && !strncasecmp(env, \"DEFAULT\" ,n)) return DEFAULT ; \nS-28 else return UNKNOWN ; \nS-29 } \nS-30 \nS-31 \nS-32 int main() \nS-33 { \nS-34 int device_num, on_init_dev; \nS-35 \nS-36 // get policy from OMP_TARGET_OFFLOAD variable \nS-37 offload_policy_t policy = get_offload_policy(); \nS-38 \nS-39 if(_OPENMP< 201811) \nS-40 { \nS-41 printf(\"Warning: OMP_TARGET_OFFLOAD NOT supported, version %d\\n\", \nS-42 _OPENMP ); \nS-43 printf(\" If OMP_TARGET_OFFLOAD is set, \" \nS-44 \"it will be ignored.\\n\"); \nS-45 } \nS-46 \nS-47 // Set target device number to an unavailable \nS-48 // device to test offload policy. \nS-49 device_num = omp_get_num_devices() + 1; \nS-50 \nS-51 // Policy: \nS-52 printf(\"OMP_TARGET_OFFLOAD Policy: \"); \nS-53 if (policy==MANDATORY) \nS-54 printf(\"MANDATORY-Terminate if dev. not avail\\n\"); \nS-55 else if(policy==DISABLED ) \nS-56 printf(\"DISABLED -(if supported) Only on Host\\n\"); \nS-57 else if(policy==DEFAULT ) \nS-58 printf(\"DEFAULT -On host if device not avail\\n\"); \nS-59 else if(policy==UNKNOWN ) \nS-60 printf(\"OMP_TARGET_OFFLOAD has unknown value\\n\" ); \nS-61 else if(policy==NOTSET ) \nS-62 printf(\"OMP_TARGET_OFFLOAD not set\\n\" ); \nS-63 \nS-64 \nS-65 on_init_dev = 1; \nS-66 // device# out of range--not supported \nS-67 #pragma omp target device(device_num) map(tofrom: on_init_dev) \nS-68 on_init_dev=omp_is_initial_device(); \nCHAPTER 12. PROGRAM CONTROL 521 \nS-69 \nS-70 if (policy == MANDATORY && _OPENMP >= 201811) \nS-71 printf(\"ERROR: OpenMP implementation ignored MANDATORY policy.\\n\"); \nS-72 \nS-73 printf(\"Target region executed on init dev %s\\n\", \nS-74 on_init_dev ? \"TRUE\":\"FALSE\"); \nS-75 \nS-76 return 0; \nS-77 } \nC / C++ \nFortran \n1 Example target_offload_control.1.f90 (omp_5.0) \nS-1 module offload_policy \nS-2 implicit none \nS-3 integer, parameter :: LEN_POLICY=10 \nS-4 contains \nS-5 character(LEN_POLICY) function get_offload_policy() \nS-6 character(64) :: env \nS-7 integer :: length, i \nS-8 env=repeat(\u2019 \u2019,len(env)) \nS-9 !policy is blank if not found * \nS-10 call get_environment_variable(\"OMP_TARGET_OFFLOAD\",env,length) \nS-11 \nS-12 do i = 1,len(env) !Makes a-z upper case \nS-13 if(iachar(env(i:i))>96) env(i:i)=achar(iachar(env(i:i))-32) \nS-14 end do \nS-15 \nS-16 get_offload_policy = trim(adjustl(env)) !remove peripheral spaces \nS-17 \nS-18 if(length==0) get_offload_policy=\"NOTSET\" \nS-19 \nS-20 return \nS-21 \nS-22 end function \nS-23 \nS-24 end module \nS-25 \nS-26 program policy_test \nS-27 \nS-28 use omp_lib \nS-29 use offload_policy \nS-30 \nS-31 integer :: i, device_num \nS-32 logical :: on_init_dev \nS-33 character(LEN_POLICY) :: policy \n522 OpenMP Examples Version 5.2.1 - November 2022 \nS-34 \nS-35 policy = get_offload_policy() !!Get OMP_TARGET_OFFLOAD value \nS-36 \nS-37 if (OPENMP_VERSION < 201811) then \nS-38 print*,\"Warning: OMP_TARGET_OFFLOAD NOT supported by VER.\", & \nS-39 OPENMP_VERSION \nS-40 print*,\" If OMP_TARGET_OFFLOAD is set, it will be ignored.\" \nS-41 endif \nS-42 \nS-43 ! Set target device number to an unavailable device \nS-44 ! to test offload policy. \nS-45 device_num = omp_get_num_devices() + 1 \nS-46 \nS-47 !! Report OMP_TARGET_OFFOAD value \nS-48 select CASE (policy) \nS-49 case(\"MANDATORY\") \nS-50 print*,\"Policy: MANDATORY-Terminate if dev. not avail.\" \nS-51 case(\"DISABLED\") \nS-52 print*,\"Policy: DISABLED-(if supported) Only on Host.\" \nS-53 case(\"DEFAULT\") \nS-54 print*,\"Policy: DEFAULT On host if device not avail.\" \nS-55 case(\"NOTSET\") \nS-56 print*,\" OMP_TARGET_OFFLOAD is not set.\" \nS-57 case DEFAULT \nS-58 print*,\" OMP_TARGET_OFFLOAD has unknown value.\" \nS-59 print*,\" UPPER CASE VALUE=\",policy \nS-60 end select \nS-61 \nS-62 \nS-63 on_init_dev = .FALSE. \nS-64 !! device# out of range--not supported \nS-65 !$omp target device(device_num) map(tofrom: on_init_dev) \nS-66 on_init_dev=omp_is_initial_device() \nS-67 !$omp end target \nS-68 \nS-69 if (policy==\"MANDATORY\" .and. OPENMP_VERSION>=201811) then \nS-70 print*,\"OMP ERROR: \", & \nS-71 \"OpenMP 5.0 implementation ignored MANDATORY policy.\" \nS-72 print*,\" Termination should have occurred\", & \nS-73 \" at target directive.\" \nS-74 endif \nS-75 \nS-76 print*, \"Target executed on init dev (T|F): \", on_init_dev \nS-77 \nS-78 end program policy_test \nFortran \nCHAPTER 12. PROGRAM CONTROL 523 \n"}
{"section_title": "12.11 omp_pause_resource and  omp_pause_resource_all Routines", "chunk": ""}
{"section_title": "12.11 omp_pause_resource and  omp_pause_resource_all Routines", "chunk": "2 omp_pause_resource_all Routines \n3 Sometimes, it is necessary to relinquish resources created or allocated for the OpenMP runtime \n4 environment to avoid interference with subsequent actions as illustrated by the following example. \n5 In the beginning either a call to the omp_get_max_threads routine or the subsequent \n6 parallel construct may trigger resource allocation by the OpenMP runtime, which may cause \n7 unexpected side effects for the subsequent fork call. It is desirable to relinquish OpenMP resources \n8 allocated before the fork by using the omp_pause_resource routine for a given device, in this \n9 case the host device. The host device number is returned by the omp_get_initial_device \n10 routine. The omp_pause_hard value is used here to free as many OpenMP resources as \n11 possible. After the fork, the child process will initialize its OpenMP runtime environment when \n12 encountering the parallel construct. \nC / C++ \n13 Example pause_resource.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <stdlib.h> \nS-3 #include <unistd.h> \nS-4 #include <sys/wait.h> \nS-5 #include <omp.h> \nS-6 \nS-7 int main() \nS-8 { \nS-9 pid_t pid; \nS-10 int nt = omp_get_max_threads(); \nS-11 \nS-12 #pragma omp parallel \nS-13 { \nS-14 #pragma omp single \nS-15 printf(\"number of threads = %d (max = %d)\\n\", \nS-16 omp_get_num_threads(), nt); \nS-17 } \nS-18 \nS-19 /* clean up thread environment before fork */ \nS-20 omp_pause_resource(omp_pause_hard, omp_get_initial_device()); \nS-21 \nS-22 pid = fork(); \nS-23 if (pid < 0) { \nS-24 printf(\"fork failed\\n\"); \nS-25 exit(1); \nS-26 } \nS-27 else if (pid == 0) { \nS-28 /* child process */ \nS-29 #pragma omp parallel num_threads(nt) \n524 OpenMP Examples Version 5.2.1 - November 2022 \nS-30 { \nS-31 int myid = omp_get_thread_num(); \nS-32 printf(\"child: myid %d of %d\\n\", myid, nt); \nS-33 } \nS-34 exit(0); \nS-35 } \nS-36 else { \nS-37 /* parent process */ \nS-38 int s; \nS-39 printf(\"parent process - waiting pid %d\\n\", pid); \nS-40 waitpid(pid, &s, 0); \nS-41 } \nS-42 return 0; \nS-43 } \nC / C++ \n1 The following example illustrates a different use case. After executing the first parallel code \n2 (parallel region 1), the relinquish program switches to executing an external parallel program \n3 (called subprogram, which is compiled from pause_resource.2b). In order to make resources \n4 available for the external subprogram, relinquish calls omp_pause_resource_all to \n5 relinquish OpenMP resources used by the current program before calling \n6 execute_command_line to execute subprogram. The omp_pause_soft value is used here \n7 to allow subsequent OpenMP regions (parallel region 2) to restart more quickly. \nFortran \n8 Example pause_resource.2a.f90 (omp_5.0) \nS-1 program relinquish \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer :: err \nS-5 \nS-6 write (*,*) \u2019In relinquish\u2019 \nS-7 \nS-8 !$omp parallel \nS-9 write (*,*) \u2019In parallel region 1\u2019 \nS-10 !$omp end parallel \nS-11 \nS-12 err = omp_pause_resource_all(omp_pause_soft) \nS-13 \nS-14 ! execute the external subprogram produced from pause_resource.2b \nS-15 call execute_command_line(command=\u2019./subprogram\u2019, wait=.true., & \nS-16 cmdstat=err) \nS-17 if (err /= 0) write (*,*) \u2019Warning: subprogram failed to execute\u2019 \nS-18 \nS-19 !$omp parallel \nS-20 write (*,*) \u2019In parallel region 2\u2019 \nCHAPTER 12. PROGRAM CONTROL 525 \nS-21 !$omp end parallel \nS-22 end program relinquish \nFortran \nFortran \n1 Example pause_resource.2b.f90 (pre_omp_3.0) \nS-1 ! This program compiles to an executable \"subprogram\" \nS-2 subroutine compute(i, j, k, r) \nS-3 implicit none \nS-4 integer :: i, j, k \nS-5 real(8) :: r \nS-6 r = i + j + k \nS-7 end subroutine compute \nS-8 \nS-9 program subprogram \nS-10 implicit none \nS-11 integer :: i, j, k \nS-12 real(8) :: s, r \nS-13 integer, parameter :: n = 500 \nS-14 \nS-15 write (*,*) \u2019In subprogram\u2019 \nS-16 s = 0.d0 \nS-17 !$omp parallel do private(r) reduction(+:s) \nS-18 do i = 1, n \nS-19 do j = 1, n \nS-20 do k = 1, n \nS-21 call compute(i, j, k, r) \nS-22 s = s + r \nS-23 end do \nS-24 end do \nS-25 end do \nS-26 !$omp end parallel do \nS-27 \nS-28 write (*,*) \u2019Sum = \u2019,s \nS-29 end program subprogram \nFortran \n526 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "12.12 Controlling Concurrency and Reproducibility with the order Clause", "chunk": ""}
{"section_title": "12.12 Controlling Concurrency and Reproducibility with the order Clause", "chunk": "2 with the order Clause \n3 The order clause is used for controlling the parallel execution of loop iterations for one or more \n4 loops that are associated with a directive. It is specified with a clause argument and optional \n5 modifier. The only supported argument, introduced in OpenMP 5.0, is the keyword concurrent \n6 which indicates that the loop iterations may execute concurrently, including iterations in the same \n7 chunk per the loop schedule. Because of the relaxed execution permitted with an \n8 order(concurrent) clause, codes must not assume that any cross-iteration data dependences \n9 would be preserved or that any two iterations may execute on the same thread. \n10 The following example in this section demonstrates the use of the order(concurrent) clause, \n11 without any modifiers, for controlling the parallel execution of loop iterations. The \n12 order(concurrent) clause cannot be used for the second and third parallel for/do \n13 constructs because of either having data dependences or accessing threadprivate variables. \nC / C++ \n14 Example reproducible.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main() \nS-5 { \nS-6 const int n = 1000; \nS-7 int v[n], u[n]; \nS-8 static int sum; \nS-9 #pragma omp threadprivate(sum) \nS-10 \nS-11 // no data dependences, so can execute concurrently \nS-12 #pragma omp parallel for order(concurrent) \nS-13 for (int i = 0; i < n; i++) { \nS-14 u[i] = i; \nS-15 v[i] = i; \nS-16 v[i] += u[i] * u[i]; \nS-17 } \nS-18 \nS-19 // with data dependences, so cannot execute iterations \nS-20 // concurrently with the order(concurrent) clause \nS-21 #pragma omp parallel for ordered \nS-22 for (int i = 1; i < n; i++) { \nS-23 v[i] += u[i] * u[i]; \nS-24 #pragma omp ordered \nS-25 v[i] += v[i-1]; \nS-26 } \nS-27 \nCHAPTER 12. PROGRAM CONTROL 527 \nS-28 sum = 0; \nS-29 // accessing a threadprivate variable, which would not be \nS-30 // permitted if the order(concurrent) clause was present \nS-31 #pragma omp parallel for copyin(sum) \nS-32 for (int i = 0; i < n; i++) { \nS-33 sum += v[i]; \nS-34 } \nS-35 \nS-36 #pragma omp parallel \nS-37 { \nS-38 printf(\"sum = %d on thread %d\\n\", sum, omp_get_thread_num()); \nS-39 } \nS-40 \nS-41 return 0; \nS-42 } \nC / C++ \nFortran \n1 Example reproducible.1.f90 (omp_5.0) \nS-1 program main \nS-2 use omp_lib \nS-3 implicit none \nS-4 integer, parameter :: n = 1000 \nS-5 integer :: v(n), u(n) \nS-6 integer :: i \nS-7 integer, save :: sum \nS-8 !$omp threadprivate(sum) \nS-9 \nS-10 !! no data dependences, so can execute concurrently \nS-11 !$omp parallel do order(concurrent) \nS-12 do i = 1, n \nS-13 u(i) = i \nS-14 v(i) = i \nS-15 v(i) = v(i) + u(i) * u(i) \nS-16 end do \nS-17 \nS-18 !! with data dependences, so cannot execute iterations \nS-19 !! concurrently with the order(concurrent) clause \nS-20 !$omp parallel do ordered \nS-21 do i = 2, n \nS-22 v(i) = v(i) + u(i) * u(i) \nS-23 !$omp ordered \nS-24 v(i) = v(i) + v(i-1) \nS-25 !$omp end ordered \nS-26 end do \nS-27 \n528 OpenMP Examples Version 5.2.1 - November 2022 \nS-28 sum = 0 \nS-29 !! accessing a threadprivate variable, which would not be \nS-30 !! permitted if the order(concurrent) clause was present \nS-31 !$omp parallel do copyin(sum) \nS-32 do i = 2, n \nS-33 sum = sum + v(i) \nS-34 end do \nS-35 \nS-36 !$omp parallel \nS-37 print *,\"sum = \",sum,\" on thread \", omp_get_thread_num() \nS-38 !$omp end parallel \nS-39 \nS-40 end program \nFortran \n1 Modifiers to the order clause, introduced in OpenMP 5.1, may be specified to control the \n2 reproducibility of the loop schedule for the associated loop(s). A reproducible loop schedule will \n3 consistently yield the same mapping of iterations to threads (or SIMD lanes) if the directive name, \n4 loop schedule, iteration space, and binding region remain the same. The reproducible \n5 modifier indicates the loop schedule must be reproducible, while the unconstrained modifier \n6 indicates that the loop schedule is not reproducible. If a modifier is not specified, then the order \n7 clause does not affect the reproducibility of the loop schedule. \n8 The next example demonstrates the use of the order(concurrent) clause with modifiers for \n9 additionally controlling the reproducibility of a loop\u2019s schedule. The two worksharing-loop \n10 constructs in the first parallel construct specify that the loops have reproducible schedules, thus \n11 memory effects from iteration i from the first loop will be observable to iteration i in the second \n12 loop. In the second parallel construct, the order clause does not control reproducibility for \n13 the loop schedules. However, since both loops specify the same static schedules, the schedules are \n14 reproducible and the data dependences between the loops are preserved by the execution. In the \n15 third parallel construct, the order clause indicates that the loops are not reproducible, \n16 overriding the default reproducibility prescribed by the specified static schedule. Consequentially, \n17 the nowait clause on the first worksharing-loop construct should not be used to ensure that the \n18 data dependences are preserved by the execution. \nC / C++ \n19 Example reproducible.2.c (omp_5.1) \nS-1 #include <stdio.h> \nS-2 \nS-3 int main() \nS-4 { \nS-5 const int n = 1000; \nS-6 int v[n], u[n]; \nS-7 \nS-8 #pragma omp parallel \nCHAPTER 12. PROGRAM CONTROL 529 \nS-9 { \nS-10 // reproducible schedules are used for the following two constructs \nS-11 #pragma omp for order(reproducible: concurrent) nowait \nS-12 for (int i = 0; i < n; i++) { \nS-13 u[i] = i; \nS-14 v[i] = i; \nS-15 } \nS-16 #pragma omp for order(reproducible: concurrent) \nS-17 for (int i = 0; i < n; i++) { \nS-18 v[i] += u[i] * u[i]; \nS-19 } \nS-20 } \nS-21 \nS-22 #pragma omp parallel \nS-23 { \nS-24 // static schedules preserve data dependences between the loops \nS-25 #pragma omp for schedule(static) order(concurrent) nowait \nS-26 for (int i = 0; i < n; i++) { \nS-27 u[i] = i; \nS-28 v[i] = i; \nS-29 } \nS-30 #pragma omp for schedule(static) order(concurrent) \nS-31 for (int i = 0; i < n; i++) { \nS-32 v[i] += u[i] * u[i]; \nS-33 } \nS-34 } \nS-35 \nS-36 #pragma omp parallel \nS-37 { \nS-38 // the default reproducibility by the static schedule is not \nS-39 // preserved due to the unconstrained order clause. \nS-40 // use of nowait here could result in data race. \nS-41 #pragma omp for schedule(static) order(unconstrained: concurrent) \nS-42 for (int i = 0; i < n; i++) { \nS-43 u[i] = i; \nS-44 v[i] = i; \nS-45 } \nS-46 #pragma omp for schedule(static) order(unconstrained: concurrent) \nS-47 for (int i = 0; i < n; i++) { \nS-48 v[i] += u[i] * u[i]; \nS-49 } \nS-50 } \nS-51 \nS-52 return 0; \nS-53 } \nC / C++ \n530 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example reproducible.2.f90 (omp_5.1) \nS-1 program main \nS-2 implicit none \nS-3 integer, parameter :: n = 1000 \nS-4 integer :: v(n), u(n) \nS-5 integer :: i \nS-6 \nS-7 !$omp parallel \nS-8 !! reproducible schedules are used the following two constructs \nS-9 !$omp do order(reproducible: concurrent) \nS-10 do i = 1, n \nS-11 u(i) = i \nS-12 v(i) = i \nS-13 end do \nS-14 !$omp end do nowait \nS-15 !$omp do order(reproducible: concurrent) \nS-16 do i = 1, n \nS-17 v(i) = v(i) + u(i) * u(i) \nS-18 end do \nS-19 !$omp end parallel \nS-20 \nS-21 !$omp parallel \nS-22 !! static schedules preserve data dependences between the loops \nS-23 !$omp do schedule(static) order(concurrent) \nS-24 do i = 1, n \nS-25 u(i) = i \nS-26 v(i) = i \nS-27 end do \nS-28 !$omp end do nowait \nS-29 !$omp do schedule(static) order(concurrent) \nS-30 do i = 1, n \nS-31 v(i) = v(i) + u(i) * u(i) \nS-32 end do \nS-33 !$omp end parallel \nS-34 \nS-35 !$omp parallel \nS-36 !! the default reproducibility by the static schedule is not \nS-37 !! preserved due to the unconstrained order clause. \nS-38 !! use of nowait here could result in data race. \nS-39 !$omp do schedule(static) order(unconstrained: concurrent) \nS-40 do i = 1, n \nS-41 u(i) = i \nS-42 v(i) = i \nS-43 end do \nS-44 !$omp do schedule(static) order(unconstrained: concurrent) \nCHAPTER 12. PROGRAM CONTROL 531 \nS-45 do i = 1, n \nS-46 v(i) = v(i) + u(i) * u(i) \nS-47 end do \nS-48 !$omp end parallel \nS-49 \nS-50 end program \nFortran \n532 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "12.13 interop Construct", "chunk": ""}
{"section_title": "12.13 interop Construct", "chunk": "2 The interop construct allows OpenMP to interoperate with foreign runtime environments. In the \n3 example below, asynchronous cuda memory copies and a cublasDaxpy routine are executed in a \n4 cuda stream. Also, an asynchronous target task execution (having a nowait clause) and two \n5 explicit tasks are executed through OpenMP directives. Scheduling dependences (synchronization) \n6 are imposed on the foreign stream and the OpenMP tasks through depend clauses. \n7 First, an interop object, obj, is initialized for synchronization by including the targetsync \n8 interop-type in the interop init clause (init( targetsync,obj )). The object provides \n9 access to the foreign runtime. The depend clause provides a dependence behavior for foreign \n10 tasks associated with a valid object. \n11 Next, the omp_get_interop_int routine is used to extract the foreign runtime id \n12 (omp_ipr_fr_id), and a test in the next statement ensures that the cuda runtime \n13 (omp_ifr_cuda) is available. \n14 Within the block for executing the cublasDaxpy routine, a stream is acquired with the \n15 omp_get_interop_ptr routine, which returns a cuda stream (s). The stream is included in the \n16 cublas handle, and used directly in the asynchronous memory routines. The following interop \n17 construct, with the destroy clause, ensures that the foreign tasks have completed. \nC / C++ \n18 Example interop.1.c (omp_5.1) \nS-1 #include <omp.h> \nS-2 #include <stdio.h> \nS-3 #include <stdlib.h> \nS-4 #include <cublas_v2.h> \nS-5 #include <cuda_runtime_api.h> \nS-6 \nS-7 #define N 16384 \nS-8 \nS-9 void myVectorSet(int n, double s, double *x) \nS-10 { \nS-11 for(int i=0; i<n; ++i) x[i] = s*(i+1); \nS-12 } \nS-13 void myDaxpy(int n, double s, double *x, double *y) \nS-14 { \nS-15 for(int i=0; i<n; ++i) y[i] = s*x[i]+y[i]; \nS-16 } \nS-17 void myDscal(int n, double s, double *x) \nS-18 { \nS-19 for(int i=0; i<n; ++i) x[i] = s*x[i]; \nS-20 } \nS-21 \nS-22 \nCHAPTER 12. PROGRAM CONTROL 533 \nS-23 int main(){ \nS-24 const double scalar=2.0; \nS-25 double *x, *y, *d_x, *d_y; \nS-26 int dev; \nS-27 \nS-28 omp_interop_t obj=omp_interop_none; \nS-29 intptr_t type; \nS-30 \nS-31 // Async Memcpy requires pinned memory \nS-32 cudaMallocHost( (void**)&x, N*sizeof(double) ); \nS-33 cudaMallocHost( (void**)&y, N*sizeof(double) ); \nS-34 cudaMalloc( (void**)&d_x, N*sizeof(double) ); \nS-35 cudaMalloc( (void**)&d_y, N*sizeof(double) ); \nS-36 \nS-37 dev = omp_get_default_device(); \nS-38 omp_target_associate_ptr(&x[0], d_x, sizeof(double)*N, 0, dev); \nS-39 omp_target_associate_ptr(&y[0], d_y, sizeof(double)*N, 0, dev); \nS-40 \nS-41 #pragma omp target nowait depend(out: x[0:N]) \\ \nS-42 map(from: x[0:N]) device(dev) \nS-43 myVectorSet(N, 1.0, x); \nS-44 \nS-45 #pragma omp task depend(out: y[0:N]) \nS-46 myVectorSet(N, -1.0, y); \nS-47 \nS-48 // get obj for syncing \nS-49 #pragma omp interop init(targetsync: obj) device(dev) \\ \nS-50 depend(in: x[0:N]) depend(inout: y[0:N]) \nS-51 \nS-52 //foreign rt id and string name \nS-53 int id = (int )omp_get_interop_int(obj, omp_ipr_fr_id, NULL); \nS-54 char* rt_name = (char*)omp_get_interop_str(obj, omp_ipr_fr_name, NULL); \nS-55 \nS-56 if(obj != omp_interop_none && id == omp_ifr_cuda) { \nS-57 \nS-58 printf(\" OpenMP working with %s runtime to execute cublas daxpy.\\n\", \nS-59 rt_name); \nS-60 cublasHandle_t handle; \nS-61 int rc; \nS-62 cublasCreate(&handle); \nS-63 \nS-64 cudaStream_t s= \nS-65 (cudaStream_t)omp_get_interop_ptr(obj, omp_ipr_targetsync, &rc); \nS-66 if(rc != omp_irc_success) { \nS-67 fprintf(stderr,\"ERROR: Failed to get %s stream, rt error= %d.\\n\", \nS-68 rt_name, rc); \nS-69 if(rc == omp_irc_no_value) \n534 OpenMP Examples Version 5.2.1 - November 2022 \nS-70 fprintf(stderr, \nS-71 \"Parameters valid, no meaningful value available.\"); \nS-72 exit(1); \nS-73 } \nS-74 \nS-75 cublasSetStream( handle,s ); \nS-76 cudaMemcpyAsync( d_x, x, N*sizeof(double), \nS-77 cudaMemcpyHostToDevice, s ); \nS-78 cudaMemcpyAsync( d_y, y, N*sizeof(double), \nS-79 cudaMemcpyHostToDevice, s ); \nS-80 cublasDaxpy( handle, N, &scalar, &d_x[0], 1, &d_y[0], 1 ) ; \nS-81 cudaMemcpyAsync( y, d_y, N*sizeof(double), \nS-82 cudaMemcpyDeviceToHost, s ); \nS-83 \nS-84 } else { // Execute as OpenMP offload. \nS-85 \nS-86 printf(\" Notice: Offloading myDaxpy to perform daxpy calculation.\\n\"); \nS-87 \nS-88 #pragma omp target depend(inout: y[0:N]) depend(in: x[0:N]) nowait \\ \nS-89 map(to: x[0:N]) map(tofrom: y[0:N]) device(dev) \nS-90 myDaxpy(N, scalar, x, y); \nS-91 \nS-92 } \nS-93 \nS-94 // This also ensures foreign tasks complete. \nS-95 #pragma omp interop destroy(obj) nowait depend(out: y[0:N]) \nS-96 \nS-97 #pragma omp target depend(inout: x[0:N]) \nS-98 myDscal(N, scalar, x); \nS-99 \nS-100 #pragma omp taskwait \nS-101 printf(\"(-1:-16384) %f:%f\\n\", y[0], y[N-1]); \nS-102 printf(\"(-2:-32768) %f:%f\\n\", x[0], x[N-1]); \nS-103 \nS-104 } \nC / C++ \nCHAPTER 12. PROGRAM CONTROL 535 \n"}
{"section_title": "12.14 Utilities", "chunk": "2 This section contains examples of utility routines and features. \n"}
{"section_title": "12.14.1 Timing Routines", "chunk": ""}
{"section_title": "12.14.1 Timing Routines", "chunk": "4 The omp_get_wtime routine can be used to measure the elapsed wall clock time (in seconds) of \n5 code execution in a program. The routine is thread safe and can be executed by multiple threads \n6 concurrently. The precision of the timer can be obtained by a call to the omp_get_wtick \n7 routine. The following example shows a use case. \nC / C++ \n8 Example get_wtime.1.c (pre_omp_3.0) \nS-1 #include <stdio.h> \nS-2 #include <unistd.h> \nS-3 #include <omp.h> \nS-4 \nS-5 void work_to_be_timed() \nS-6 { \nS-7 sleep(2); \nS-8 } \nS-9 \nS-10 int main() \nS-11 { \nS-12 double start, end; \nS-13 \nS-14 start = omp_get_wtime(); \nS-15 work_to_be_timed(); // any parallel or serial codes \nS-16 end = omp_get_wtime(); \nS-17 \nS-18 printf(\"Work took %f seconds\\n\", end - start); \nS-19 printf(\"Precision of the timer is %f (sec)\\n\", omp_get_wtick()); \nS-20 return 0; \nS-21 } \nC / C++ \n536 OpenMP Examples Version 5.2.1 - November 2022 \nFortran \n1 Example get_wtime.1.f90 (pre_omp_3.0) \nS-1 subroutine work_to_be_timed \nS-2 use, intrinsic :: iso_c_binding, only: c_int \nS-3 interface \nS-4 subroutine fsleep(sec) bind(C, name=\"sleep\") \nS-5 import c_int \nS-6 integer(c_int), value :: sec \nS-7 end subroutine \nS-8 end interface \nS-9 call fsleep(2) \nS-10 end subroutine \nS-11 \nS-12 program do_work \nS-13 use omp_lib \nS-14 implicit none \nS-15 double precision :: start, end \nS-16 \nS-17 start = omp_get_wtime() \nS-18 call work_to_be_timed ! any parallel or serial codes \nS-19 end = omp_get_wtime() \nS-20 \nS-21 print *, \"Work took\", end - start, \"seconds\" \nS-22 print *, \"Precision of the timer is\", omp_get_wtick(), \"(sec)\" \nS-23 end program \nFortran \n"}
{"section_title": "12.14.2 Environment Display", "chunk": ""}
{"section_title": "12.14.2 Environment Display", "chunk": "3 The OpenMP version number and the values of ICVs associated with the relevant environment \n4 variables can be displayed at runtime by setting the OMP_DISPLAY_ENV environment variable to \n5 either TRUE or VERBOSE. The information is displayed once by the runtime. \n6 A more flexible or controllable approach is to call the omp_display_env API routine at any \n7 desired point of a code to display the same information. This OpenMP 5.1 API routine takes a \n8 single verbose argument. A value of 0 or .false. (for C/C++ or Fortran) indicates the required \n9 OpenMP ICVs associated with environment variables be displayed, and a value of 1 or .true. (for \n10 C/C++ or Fortran) will include vendor-specific ICVs that can be modified by environment variables. \n11 The following example illustrates the conditional execution of the API omp_display_env \n12 routine. Typically it would be invoked in various debug modes of an application. An important use \n13 case is to have a single MPI process (e.g., rank = 0) of a hybrid (MPI+OpenMP) code execute the \n14 routine, instead of all MPI processes, as would be done by setting the OMP_DISPLAY_ENV to \n15 TRUE or VERBOSE. \nCHAPTER 12. PROGRAM CONTROL 537 \nC / C++ \n1 Example display_env.1.c (omp_5.1) \nS-1 #include <omp.h> \nS-2 \nS-3 //implementers: customize debug routines for app debugging \nS-4 int debug(){ return 1; } \nS-5 int debug_omp_verbose(){ return 0; } \nS-6 \nS-7 int main() \nS-8 { \nS-9 if( debug() ) omp_display_env( debug_omp_verbose() ); \nS-10 // ... \nS-11 return 0; \nS-12 } \nC / C++ \nFortran \n2 Example display_env.1.f90 (omp_5.1) \nS-1 !implementers: customize debug routines for app debugging \nS-2 function debug() \nS-3 logical :: debug \nS-4 debug = .true. \nS-5 end function \nS-6 \nS-7 function debug_omp_verbose() \nS-8 logical :: debug_omp_verbose \nS-9 debug_omp_verbose = .false. \nS-10 end function \nS-11 \nS-12 program display_omp_environment \nS-13 use omp_lib \nS-14 logical :: debug, debug_omp_verbose \nS-15 \nS-16 if( debug() ) call omp_display_env( debug_omp_verbose() ) \nS-17 !! ... \nS-18 end program \nFortran \n538 OpenMP Examples Version 5.2.1 - November 2022 \n1 A sample output from the execution of the code might look like: \n2 OPENMP DISPLAY ENVIRONMENT BEGIN \n3 _OPENMP=\u2019202011\u2019 \n4 [host] OMP_AFFINITY_FORMAT=\u2019(null)\u2019 \n5 [host] OMP_ALLOCATOR=\u2019omp_default_mem_alloc\u2019 \n6 [host] OMP_CANCELLATION=\u2019FALSE\u2019 \n7 [host] OMP_DEFAULT_DEVICE=\u20190\u2019 \n8 [host] OMP_DISPLAY_AFFINITY=\u2019FALSE\u2019 \n9 [host] OMP_DISPLAY_ENV=\u2019FALSE\u2019 \n10 [host] OMP_DYNAMIC=\u2019FALSE\u2019 \n11 [host] OMP_MAX_ACTIVE_LEVELS=\u20191\u2019 \n12 [host] OMP_MAX_TASK_PRIORITY=\u20190\u2019 \n13 [host] OMP_NESTED: deprecated; max-active-levels-var=1 \n14 [host] OMP_NUM_THREADS: value is not defined \n15 [host] OMP_PLACES: value is not defined \n16 [host] OMP_PROC_BIND: value is not defined \n17 [host] OMP_SCHEDULE=\u2019static\u2019 \n18 [host] OMP_STACKSIZE=\u20194M\u2019 \n19 [host] OMP_TARGET_OFFLOAD=DEFAULT \n20 [host] OMP_THREAD_LIMIT=\u20190\u2019 \n21 [host] OMP_TOOL=\u2019enabled\u2019 \n22 [host] OMP_TOOL_LIBRARIES: value is not defined \n23 OPENMP DISPLAY ENVIRONMENT END \n"}
{"section_title": "12.14.3 error Directive", "chunk": ""}
{"section_title": "12.14.3 error Directive", "chunk": "25 The error directive provides a consistent method for C, C++, and Fortran to emit a fatal or \n26 warning message at compilation or execution time, as determined by a severity or an at \n27 clause, respectively. When severity(fatal) is present, the compilation or execution is \n28 aborted. Without any clauses the default behavior is as if at(compilation) and \n29 severity(fatal) were specified. \n30 The C, C++, and Fortran examples below show all the cases for reporting messages. \nC / C++ \n31 Example error.1.c (omp_5.2) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 \nS-4 int main(){ \nS-5 \nS-6 #pragma omp metadirective \\ \nS-7 when(implementation={vendor(gnu)}: nothing ) \\ \nS-8 otherwise(error at(compilation) severity(fatal) \\ \nCHAPTER 12. PROGRAM CONTROL 539 \nS-9 message(\"GNU compiler required.\")) \nS-10 \nS-11 if( omp_get_num_procs() < 3 ){ \nS-12 #pragma omp error at(execution) severity(fatal) \\ \nS-13 message(\"3 or more procs required.\") \nS-14 } \nS-15 \nS-16 #pragma omp parallel master \nS-17 { \nS-18 // Give notice about master deprecation at compile time and run time. \nS-19 #pragma omp error at(compilation) severity(warning) \\ \nS-20 message(\"Notice: master is deprecated.\") \nS-21 #pragma omp error at(execution) severity(warning) \\ \nS-22 message(\"Notice: masked used next release.\") \nS-23 \nS-24 printf(\" Hello from thread number 0.\\n\"); \nS-25 } \nS-26 \nS-27 } \nC / C++ \nFortran \n1 Example error.1.f90 (omp_5.2) \nS-1 program main \nS-2 use omp_lib \nS-3 \nS-4 !$omp metadirective & \nS-5 !$omp& when( implementation={vendor(gnu)}: nothing ) & \nS-6 !$omp& otherwise( error at(compilation) severity(fatal) & \nS-7 !$omp& message( \"GNU compiler required.\" ) ) \nS-8 \nS-9 \nS-10 if( omp_get_num_procs() < 3 ) then \nS-11 !$omp error at(execution) severity(fatal) & \nS-12 !$omp& message(\"3 or more procs required.\") \nS-13 endif \nS-14 \nS-15 !$omp parallel master \nS-16 \nS-17 !! Give notice about master deprecation at compile time and run time. \nS-18 !$omp error at(compilation) severity(warning) & \nS-19 !$omp& message(\"Notice: master is deprecated.\") \nS-20 !$omp error at(execution) severity(warning) & \nS-21 !$omp& message(\"Notice: masked to be used in next release.\") \nS-22 \nS-23 print*,\" Hello from thread number 0.\" \n540 OpenMP Examples Version 5.2.1 - November 2022 \nS-24 \nS-25 !$omp end parallel master \nS-26 \nS-27 end program \nFortran \nCHAPTER 12. PROGRAM CONTROL 541 \nThis page intentionally left blank \n"}
{"section_title": "13 OMPT Interface", "chunk": "2 OMPT defines mechanisms and an API for interfacing with tools in the OpenMP program. \n3 The OMPT API provides the following functionality: \n4 \u2022 examines the state associated with an OpenMP thread \n5 \u2022 interprets the call stack of an OpenMP thread \n6 \u2022 receives notification about OpenMP events \n7 \u2022 traces activity on OpenMP target devices \n8 \u2022 assesses implementation-dependent details \n9 \u2022 controls a tool from an OpenMP application \n10 The following sections will illustrate basic mechanisms and operations of the OMPT API. \n543 \n"}
{"section_title": "13.1 OMPT Start", "chunk": ""}
{"section_title": "13.1 OMPT Start", "chunk": "2 There are three steps an OpenMP implementation takes to activate a tool. This section explains how \n3 the tool and an OpenMP implementation interact to accomplish tool activation. \n4 Step 1. Determine Whether to Initialize \n5 A tool is activated by the OMPT interface when it returns a non-NULL pointer to an \n6 ompt_start_tool_result_t structure on a call to ompt_start_tool by the \n7 OpenMP implementation. There are three ways that a tool can provide a definition of \n8 ompt_start_tool to an OpenMP implementation: (1) Statically linking the tool\u2019s \n9 definition of ompt_start_tool into an OpenMP application. (2) Introducing a \n10 dynamically linked library that includes the tool\u2019s definition of ompt_start_tool into \n11 the application\u2019s address space. (3) Providing the name of a dynamically linked library \n12 appropriate for the architecture and operating system used by the application in the \n13 tool-libraries-var ICV. \n14 Step 2. Initializing a First-Party tool \n15 If a tool-provided implementation of ompt_start_tool returns a non-NULL pointer to \n16 an ompt_start_tool_result_t structure, the OpenMP implementation will invoke \n17 the tool initializer specified in this structure prior to the occurrence of any OpenMP event. \n18 Step 3. Monitoring Activity on the Host \n19 To monitor execution of an OpenMP program on the host device, a tool\u2019s initializer must \n20 register to receive notification of events that occur as an OpenMP program executes. A tool \n21 can register callbacks for OpenMP events using the runtime entry point known as \n22 ompt_set_callback, which has the following possible return codes: \n23 ompt_set_error, ompt_set_never, ompt_set_impossible, \n24 ompt_set_sometimes, ompt_set_sometimes_paired, ompt_set_always. \n25 If the ompt_set_callback runtime entry point is called outside a tool\u2019s initializer, \n26 registration of supported callbacks may fail with a return code of ompt_set_error. All \n27 callbacks registered with ompt_set_callback or returned by ompt_get_callback \n28 use the dummy type signature ompt_callback_t. While this is a compromise, it is better \n29 than providing unique runtime entry points with precise type signatures to set and get the \n30 callback for each unique runtime entry point type signature. \n31 \u2014\u2014\u2014\u2014\u2014- \n32 To use the OMPT interface a tool must provide a globally-visible implementation of the \n33 ompt_start_tool function. The function returns a pointer to an \n34 ompt_start_tool_result_t structure that contains callback pointers for tool initialization \n35 and finalization as well as a data word, tool_data, that is to be passed by reference to these callbacks. \n36 A NULL return indicates the tool will not use the OMPT interface. The runtime execution of \n37 ompt_start_tool is triggered by the first OpenMP directive or OpenMP API routine call. \n544 OpenMP Examples Version 5.2.1 - November 2022 \n1 In the example below, the user-provided ompt_start_tool function performs a check to make \n2 sure the runtime OpenMP version that OMPT supports (provided by the omp_version \n3 argument) is identical to the OpenMP implementation (compile-time) version. Also, a NULL is \n4 returned to indicate that the OMPT interface is not used (no callbacks and tool data are specified). \n5 Note: The omp-tools.h file is included. \nC / C++ \n6 Example ompt_start.1.c (omp_5.0) \nS-1 #include <stdio.h> \nS-2 #include <omp.h> \nS-3 #include <omp-tools.h> \nS-4 \nS-5 ompt_start_tool_result_t *ompt_start_tool( \nS-6 unsigned int omp_version, \nS-7 const char *runtime_version \nS-8 ){ \nS-9 if(omp_version != _OPENMP) \nS-10 printf(\"Warning: OpenMP runtime version (%i) \" \nS-11 \"does not match the compile time version (%i)\" \nS-12 \" for runtime identifying as %s\\n\", \nS-13 omp_version, _OPENMP, runtime_version); \nS-14 // Returning NULL will disable this as an OMPT tool, \nS-15 // allowing other tools to be loaded \nS-16 return NULL; \nS-17 } \nS-18 \nS-19 int main(void){ \nS-20 printf(\"Running with %i threads\\n\", omp_get_max_threads()); \nS-21 return 0; \nS-22 } \nC / C++ \nCHAPTER 13. OMPT INTERFACE 545 \nThis page intentionally left blank \n"}
{"section_title": "A Feature Deprecations and Updates in Examples", "chunk": ""}
{"section_title": "A Feature Deprecations and Updates in Examples", "chunk": "2 in Examples \n3 Deprecation of features began in OpenMP 5.0. Examples that use a deprecated feature have been \n4 updated with an equivalent replacement feature. \n5 Table A.1 summarizes deprecated features and their replacements in each version. Affected \n6 examples are updated accordingly and listed in Section A.1. \nTABLE A.1: Deprecated Features and Their Replacements \nVersion Deprecated Feature Replacement \n5.2 default clause on metadirectives otherwise clause \n5.2 delimited declare target directive \nfor C/C++ \nbegin declare target directive \n5.2 to clause on declare target \ndirective \nenter clause \n5.2 non-argument destroy clause on \ndepobj construct \ndestroy(argument) \n5.2 allocate construct for Fortran \nALLOCATE statements \nallocators construct \n5.2 depend clause on ordered construct doacross clause \n5.2 linear(modifier(list): linear-step) \nclause \nlinear(list: step(linear-step), \nmodifier) clause \n5.1 master construct masked construct \n5.1 master affinity policy primary affinity policy \n5.0 omp_lock_hint_* constants omp_sync_hint_* constants \n7 These replacements appear in examples that illustrate, otherwise, earlier features. When using a \n8 compiler that is compliant with a version prior to the indicated version, the earlier form of an \n9 example for a previous version is listed as a reference. \n547 \n"}
{"section_title": "A.1 Updated Examples for Different Versions", "chunk": ""}
{"section_title": "A.1 Updated Examples for Different Versions", "chunk": "2 The following tables list the updated examples for different versions as a result of feature \n3 deprecation. The Earlier Version column of the tables shows the version tag of the earlier version. \n4 It also shows the prior name of an example when it has been renamed. \n5 Table A.2 lists the updated examples for OpenMP 5.2 in the Examples Document Version 5.2. The \n6 Earlier Version column of the table lists the earlier version tags of the examples that can be found in \n7 the Examples Document Version 5.1. \nTABLE A.2: Updated Examples for Version 5.2 \nExample Name Earlier Version Feature Updated \nerror.1.c, f90 5.1 default clause on metadirectives \nmetadirective.1.c, f90 5.0 replaced with otherwise clause \nmetadirective.2.c, f90 5.0 \nmetadirective.3.c, f90 5.0 \nmetadirective.4.c, f90 5.1 \ntarget_ptr_map.4.c 5.1 \ntarget_ptr_map.5.c, f90 5.1 \narray_shaping.1.f90 5.0 to clause on declare target \ntarget_reverse_offload.7.c 5.0 directive replaced with enter clause \ntarget_task_reduction.1.c, f90 5.1 \ntarget_task_reduction.2a.c, f90 5.0 \ntarget_task_reduction.2b.c, f90 5.1 \narray_shaping.1.c 5.0 delimited declare target \nasync_target.1.c 4.0 directive replaced with \nasync_target.2.c 4.0 begin declare target \ndeclare_target.1.c 4.0 directive for C/C++ \ndeclare_target.2c.cpp 4.0 \ndeclare_target.3.c 4.0 \ndeclare_target.4.c 4.0 \ndeclare_target.5.c 4.0 \ndeclare_target.6.c 4.0 \ndeclare_variant.1.c 5.0 \ndevice.1.c 4.0 \nmetadirective.3.c 5.0 \ntarget_ptr_map.2.c 5.0 \ntarget_ptr_map.3a.c 5.0 \ntarget_ptr_map.3b.c 5.0 \ntarget_struct_map.1.c 5.0 \ntable continued on next page \n548 OpenMP Examples Version 5.2.1 - November 2022 \ntable continued from previous page \nExample Name Earlier Version Feature Updated \ntarget_struct_map.2.cpp 5.0 \ntarget_struct_map.3.c 5.0 \ntarget_struct_map.4.c 5.0 \ndoacross.1.c, f90 4.5 depend clause on ordered \ndoacross.2.c, f90 4.5 construct replaced with doacross \ndoacross.3.c, f90 4.5 clause \ndoacross.4.c, f90 4.5 \nlinear_modifier.1.cpp, f90 4.5 modifier syntax change for linear \nlinear_modifier.2.cpp, f90 4.5 clause on declare simd directive \nlinear_modifier.3.c, f90 4.5 \nallocators.1.f90 5.0 allocate construct replaced with \nallocators construct for Fortran \nallocate statements \ndepobj.1.c, f90 5.0 argument added to destroy clause \non depobj construct \n1 Table A.3 lists the updated examples for OpenMP 5.1 in the Examples Document Version 5.1. The \n2 Earlier Version column of the table lists the earlier version tags and prior names of the examples \n3 that can be found in the Examples Document Version 5.0.1. \nTABLE A.3: Updated Examples for Version 5.1 \nExample Name Earlier Version Feature Updated \naffinity.5.c, f 4.0 master affinity policy \nreplaced with primary policy \nasync_target.3.c, f90 5.0 master construct replaced \ncancellation.2.c, f90 4.0 with masked construct \ncopyprivate.2.c, f 3.0 \nfort_sa_private.5.f 3.0 \nlock_owner.1.c, f 3.0 \nmasked.1.c, f 3.0: master.1.c, f \nparallel_masked_taskloop.1.c, f90 5.0: parallel_master_taskloop.1.c, f90 \nreduction.6.c, f 3.0 \ntarget_task_reduction.1.c, f90 5.0 \ntarget_task_reduction.2b.c, f90 5.0 \ntaskloop_simd_reduction.1.c, f90 5.0 \ntable continued on next page \nAPPENDIX A. FEATURE DEPRECATIONS AND UPDATES IN EXAMPLES 549 \ntable continued from previous page \nExample Name Earlier Version Feature Updated \ntask_detach.1.c, f90 5.0 \n1 Table A.4 lists the updated examples for OpenMP 5.0 in the Examples Document Version 5.1. The \n2 Earlier Version column of the table lists the earlier version tags of the examples that can be found in \n3 the Examples Document Version 5.0.1. \nTABLE A.4: Updated Examples for Version 5.0 \nExample Name Earlier Version Feature Updated \ncritical.2.c, f 4.5 omp_lock_hint_* constants \ninit_lock_with_hint.1.cpp, f 4.5 replaced with omp_sync_hint_* \nconstants \n550 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "B Document Revision History", "chunk": ""}
{"section_title": "B.1 Changes from 5.2 to 5.2.1", "chunk": ""}
{"section_title": "B.1 Changes from 5.2 to 5.2.1", "chunk": "3 \u2022 General changes: \n4 \u2013 Updated source metadata tags for all examples to use an improved form (see \n5 https://github.com/OpenMP/Examples/blob/v5.2.1/Contributions.md) \n6 \u2013 Explicitly included the version tag (pre_omp_3.0) in those examples that did not contain a \n7 version tag previously \n8 \u2022 Added the following examples for the 5.2 features: \n9 \u2013 uses_allocators clause for the use of allocators in target regions (Section 11.2 on \n10 page 459) \n11 \u2022 Added the following examples for the 5.1 features: \n12 \u2013 The inoutset dependence type (Section 5.3.4 on page 108) \n13 \u2013 Atomic compare and capture (Section 9.5 on page 326) \n14 \u2022 Added the following examples for the 5.0 features: \n15 \u2013 declare target directive with device_type(nohost) clause (Section 6.13.6 on \n16 page 231) \n17 \u2013 omp_pause_resource and omp_pause_resource_all routines (Section 12.11 on \n18 page 524) \n19 \u2022 Miscellaneous fixes: \n20 \u2013 Fixed an inconsistent use of mapper in Example target_mapper.3.f90 (Section 6.9 on page 191) \n21 \u2013 Fixed mismatched argument list in Example fort_sa_private.5.f (Section 10.6 on page 385) \n22 \u2013 Moved the placement of declare target enter directive after function declaration \n23 (Section 10.9.4 on page 408) \n24 \u2013 Fixed an incorrect use of omp_in_parallel routine in Example metadirective.4 \n25 (Section 12.7 on page 500) \n26 \u2013 Fixed an incorrect value for at clause (Section 12.14.3 on page 539) \n551 \n"}
{"section_title": "B.2 Changes from 5.1 to 5.2", "chunk": ""}
{"section_title": "B.2 Changes from 5.1 to 5.2", "chunk": "2 \u2022 General changes: \n3 \u2013 Included a description of the semantics for OpenMP directive syntax (see Section 2 on page 3) \n4 \u2013 Reorganized the Introduction Chapter and moved the Feature Deprecation Chapter to \n5 Appendix A \n6 \u2013 Included a list of examples that were updated for feature deprecation and replacement in each \n7 version (see Appendix A.1) \n8 \u2013 Added Index entries \n9 \u2022 Updated the examples for feature deprecation and replacement in OpenMP 5.2. See Table A.1 \n10 and Table A.2 for details. \n11 \u2022 Added the following examples for the 5.2 features: \n12 \u2013 Mapping class objects with virtual functions (Section 6.7 on page 186) \n13 \u2013 allocators construct for Fortran allocate statement (Section 11.2 on page 459) \n14 \u2013 Behavior of reallocation of variables through OpenMP allocator in Fortran (Section 11.2 on \n15 page 459) \n16 \u2022 Added the following examples for the 5.1 features: \n17 \u2013 Clarification of optional end directive for strictly structured block in Fortran (Section 2.4 on \n18 page 9) \n19 \u2013 filter clause on masked construct (Section 3.14 on page 49) \n20 \u2013 omp_all_memory reserved locator for specifying task dependences (Section 5.3.9 on \n21 page 124) \n22 \u2013 Behavior of Fortran allocatable variables in target regions (Section 6.5 on page 178) \n23 \u2013 Device memory routines in Fortran (Section 6.17.5 on page 262) \n24 \u2013 Partial tiles from tile construct (Section 8.3 on page 303) \n25 \u2013 Fortran associate names and selectors in target region (Section 10.14 on page 444) \n26 \u2013 allocate directive for variable declarations and allocate clause on task constructs \n27 (Section 11.2 on page 459) \n28 \u2013 Controlling concurrency and reproducibility with order clause (Section 12.12 on page 527) \n29 \u2022 Added other examples: \n30 \u2013 Using lambda expressions with target constructs (Section 6.14 on page 234) \n31 \u2013 Target memory and device pointer routines (Section 6.17.5 on page 262) \n552 OpenMP Examples Version 5.2.1 - November 2022 \n1 \u2013 Examples to illustrate the ordering properties of the flush operation (Section 11.1 on page 448) \n2 \u2013 User selector in the metadirective directive (Section 12.7 on page 500) \n"}
{"section_title": "B.3 Changes from 5.0.1 to 5.1", "chunk": ""}
{"section_title": "B.3 Changes from 5.0.1 to 5.1", "chunk": "4 \u2022 General changes: \n5 \u2013 Replaced master construct example with equivalent masked construct example \n6 (Section 3.14 on page 49) \n7 \u2013 Primary thread is now used to describe thread number 0 in the current team \n8 \u2013 primary thread affinity policy is now used to specify that every thread in the team is \n9 assigned to the same place as the primary thread (Section 4.1.3 on page 65) \n10 \u2013 The omp_lock_hint_* constants have been renamed omp_sync_hint_* (Section 9.1 \n11 on page 313, Section 9.12 on page 357) \n12 \u2022 Added the following new chapters: \n13 \u2013 Deprecated Features (on page 547) \n14 \u2013 Directive Syntax (Section 2 on page 3) \n15 \u2013 Loop Transformations (Section 8 on page 289) \n16 \u2013 OMPT Interface (Section 13 on page 543) \n17 \u2022 Added the following examples for the 5.1 features: \n18 \u2013 OpenMP directives in C++ attribute specifiers (Section 2.2 on page 5) \n19 \u2013 Directive syntax adjustment to allow Fortran BLOCK ... END BLOCK as a structured block \n20 (Section 2.4 on page 9) \n21 \u2013 omp_target_is_accessible API routine (Section 6.3 on page 162) \n22 \u2013 Fortran allocatable array mapping in target regions (Section 6.5 on page 178) \n23 \u2013 begin declare target (with end declare target) directive (Section 6.13.2 on \n24 page 220) \n25 \u2013 tile construct (Section 8.1 on page 289) \n26 \u2013 unroll construct (Section 8.2 on page 293) \n27 \u2013 Reduction with the scope construct (Section 10.9.6 on page 420) \n28 \u2013 metadirective directive with dynamic condition selector (Section 12.7 on page 500) \n29 \u2013 interop construct (Section 12.13 on page 533) \nAPPENDIX B. DOCUMENT REVISION HISTORY 553 \n1 \u2013 Environment display with the omp_display_env routine (Section 12.14.2 on page 537) \n2 \u2013 error directive (Section 12.14.3 on page 539) \n3 \u2022 Included additional examples for the 5.0 features: \n4 \u2013 collapse clause for non-rectangular loop nest (Section 3.8 on page 32) \n5 \u2013 detach clause for tasks (Section 5.4 on page 128) \n6 \u2013 Pointer attachment for a structure member (Section 6.4 on page 171) \n7 \u2013 Host and device pointer association with the omp_target_associate_ptr routine \n8 (Section 6.17.4 on page 259) \n9 \u2013 Sample code on activating the tool interface (Section 13.1 on page 544) \n10 \u2022 Added other examples: \n11 \u2013 The omp_get_wtime routine (Section 12.14.1 on page 536) \n"}
{"section_title": "B.4 Changes from 5.0.0 to 5.0.1", "chunk": ""}
{"section_title": "B.4 Changes from 5.0.0 to 5.0.1", "chunk": "13 \u2022 Added version tags (omp_x.y) in example labels and the corresponding source codes for all \n14 examples that feature OpenMP 3.0 and later. \n15 \u2022 Included additional examples for the 5.0 features: \n16 \u2013 Extension to the defaultmap clause (Section 6.2 on page 157) \n17 \u2013 Transferring noncontiguous data with the target update directive in Fortran (Section 6.8 \n18 on page 187) \n19 \u2013 conditional modifier for the lastprivate clause (Section 10.8 on page 390) \n20 \u2013 task modifier for the reduction clause (Section 10.9.2 on page 399) \n21 \u2013 Reduction on combined target constructs (Section 10.9.3 on page 404) \n22 \u2013 Task reduction with target constructs (Section 10.9.4 on page 408) \n23 \u2013 scan directive for returning the prefix sum of a reduction (Section 10.10 on page 433) \n24 \u2022 Included additional examples for the 4.x features: \n25 \u2013 Dependence for undeferred tasks (Section 5.3.9 on page 124) \n26 \u2013 ref, val, uval modifiers for linear clause (Section 7.4 on page 281) \n27 \u2022 Clarified the description of pointer mapping and pointer attachment in Section 6.3 on page 162. \n28 \u2022 Clarified the description of memory model examples in Section 11.1 on page 448. \n554 OpenMP Examples Version 5.2.1 - November 2022 \n"}
{"section_title": "B.5 Changes from 4.5.0 to 5.0.0", "chunk": ""}
{"section_title": "B.5 Changes from 4.5.0 to 5.0.0", "chunk": "2 \u2022 Added the following examples for the 5.0 features: \n3 \u2013 Extended teams construct for host execution (Section 3.3 on page 18) \n4 \u2013 loop and teams loop constructs specify loop iterations that can execute concurrently \n5 (Section 3.15 on page 52) \n6 \u2013 Task data affinity is indicated by affinity clause of task construct (Section 4.2 on \n7 page 66) \n8 \u2013 Display thread affinity with OMP_DISPLAY_AFFINITY environment variable or \n9 omp_display_affinity() API routine (Section 4.3 on page 67) \n10 \u2013 taskwait with dependences (Section 5.3.6 on page 113) \n11 \u2013 mutexinoutset task dependences (Section 5.3.7 on page 119) \n12 \u2013 Multidependence Iterators (in depend clauses) (Section 5.3.8 on page 122) \n13 \u2013 Combined constructs: parallel master taskloop and \n14 parallel master taskloop simd (Section 5.8 on page 142) \n15 \u2013 Reverse Offload through ancestor modifier of device clause. (Section 6.1.6 on page 154) \n16 \u2013 Pointer Mapping - behavior of mapped pointers (Section 6.3 on page 162) \n17 \u2013 Structure Mapping - behavior of mapped structures (Section 6.4 on page 171) \n18 \u2013 Array Shaping with the shape-operator (Section 6.8 on page 187) \n19 \u2013 The declare mapper directive (Section 6.9 on page 191) \n20 \u2013 Acquire and Release Semantics Synchronization: Memory ordering clauses acquire, \n21 release, and acq_rel were added to flush and atomic constructs (Section 9.8 on page 335) \n22 \u2013 depobj construct provides dependence objects for subsequent use in depend clauses \n23 (Section 9.10 on page 347) \n24 \u2013 reduction clause for task construct (Section 10.9.2 on page 399) \n25 \u2013 reduction clause for taskloop construct (Section 10.9.5 on page 413) \n26 \u2013 reduction clause for taskloop simd construct (Section 10.9.5 on page 413) \n27 \u2013 Memory Allocators for making OpenMP memory requests with traits (Section 11.2 on \n28 page 459) \n29 \u2013 requires directive specifies required features of implementation (Section 12.5 on page 492) \n30 \u2013 declare variant directive - for function variants (Section 12.6 on page 494) \n31 \u2013 metadirective directive - for directive variants (Section 12.7 on page 500) \nAPPENDIX B. DOCUMENT REVISION HISTORY 555 \n1 \u2013 OMP_TARGET_OFFLOAD Environment Variable - controls offload behavior (Section 12.10 \n2 on page 520) \n3 \u2022 Included the following additional examples for the 4.x features: \n4 \u2013 more taskloop examples (Section 5.7 on page 138) \n5 \u2013 user-defined reduction (UDR) (Section 10.9.7 on page 422) \n"}
{"section_title": "B.6 Changes from 4.0.2 to 4.5.0", "chunk": ""}
{"section_title": "B.6 Changes from 4.0.2 to 4.5.0", "chunk": "7 \u2022 Reorganized into chapters of major topics \n8 \u2022 Included file extensions in example labels to indicate source type \n9 \u2022 Applied the explicit map(tofrom) for scalar variables in a number of examples to comply \n10 with the change of the default behavior for scalar variables from map(tofrom) to \n11 firstprivate in the 4.5 specification \n12 \u2022 Added the following new examples: \n13 \u2013 linear clause in loop constructs (Section 3.9 on page 38) \n14 \u2013 priority clause for task construct (Section 5.2 on page 103) \n15 \u2013 taskloop construct (Section 5.7 on page 138) \n16 \u2013 directive-name modifier in multiple if clauses on a combined construct (Section 6.1.5 on \n17 page 151) \n18 \u2013 unstructured data mapping (Section 6.11 on page 211) \n19 \u2013 link clause for declare target directive (Section 6.13.5 on page 229) \n20 \u2013 asynchronous target execution with nowait clause (Section 6.16 on page 246) \n21 \u2013 device memory routines and device pointers (Section 6.17.5 on page 262) \n22 \u2013 doacross loop nest (Section 9.11 on page 351) \n23 \u2013 locks with hints (Section 9.12 on page 357) \n24 \u2013 C/C++ array reduction (Section 10.9.1 on page 392) \n25 \u2013 C++ reference types in data sharing clauses (Section 10.13 on page 442) \n"}
{"section_title": "B.7 Changes from 4.0.1 to 4.0.2", "chunk": "27 \u2022 Names of examples were changed from numbers to mnemonics \n556 OpenMP Examples Version 5.2.1 - November 2022 \n1 \u2022 Added SIMD examples (Section 7.1 on page 267) \n2 \u2022 Applied miscellaneous fixes in several source codes \n3 \u2022 Added the revision history \n"}
{"section_title": "B.8 Changes from 4.0 to 4.0.1", "chunk": "5 Added the following new examples: \n6 \u2022 the proc_bind clause (Section 4.1 on page 60) \n7 \u2022 the taskgroup construct (Section 5.5 on page 133) \n"}
{"section_title": "B.9 Changes from 3.1 to 4.0", "chunk": "9 \u2022 Beginning with OpenMP 4.0, examples were placed in a separate document from the \n10 specification document. \n11 \u2022 Version 4.0 added the following new examples: \n12 \u2013 task dependences (Section 5.3 on page 105) \n13 \u2013 target construct (Section 6.1 on page 146) \n14 \u2013 array sections in device constructs (Section 6.6 on page 182) \n15 \u2013 target data construct (Section 6.10 on page 198) \n16 \u2013 target update construct (Section 6.12 on page 214) \n17 \u2013 declare target directive (Section 6.13 on page 218) \n18 \u2013 teams constructs (Section 6.15 on page 237) \n19 \u2013 asynchronous execution of a target region using tasks (Section 6.16.1 on page 246) \n20 \u2013 device runtime routines (Section 6.17 on page 255) \n21 \u2013 Fortran ASSOCIATE construct (Section 10.14 on page 443) \n22 \u2013 cancellation constructs (Section 12.4 on page 487) \nAPPENDIX B. DOCUMENT REVISION HISTORY 557 \n"}
