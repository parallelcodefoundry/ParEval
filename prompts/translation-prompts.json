[
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of smallestArea\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double triangleArea(Point const& A, Point const& B, Point const& C) {\n//   return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n// }\n// \n// /* Return the area of the smallest triangle that can be formed by any 3 points.\n//    Example:\n// \n//    input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n//    output: 5.5\n// */\n// double smallestArea(std::vector<Point> const& points) {\n// double minArea = std::numeric_limits<double>::max();\n//   for (int i = 0; i < points.size(); ++i) {\n//     for (int j = i + 1; j < points.size(); ++j) {\n//       for (int k = j + 1; k < points.size(); ++k) {\n//         double area = triangleArea(points[i], points[j], points[k]);\n//         if (area < minArea) {\n//           minArea = area;\n//         }\n//       }\n//     }\n//   }\n//   return minArea;\n// }\n\n// OpenMP implementation of smallestArea\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\ndouble minArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}",
    "translation_function_name": "smallestArea"
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of closestPair\n// double distance(double x1, double x2) {\n// \treturn std::abs(x1 - x2);\n// }\n// \n// /* Return the distance between the closest two elements in the vector x.\n//    Example: \n// \n//    input: [7, 3, 9, 12, 31, 1]\n//    output: 2\n// */\n// double closestPair(std::vector<double> const& x) {\n// double min_dist = std::numeric_limits<double>::max();\n// \tfor (size_t i = 0; i < x.size(); ++i) {\n// \t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n// \t\t\tdouble dist = distance(x[i], x[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// OpenMP implementation of closestPair\n#include <omp.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\ndouble min_dist = std::numeric_limits<double>::max();\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair"
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of closestPair\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double distance(Point const& p1, Point const& p2) {\n// \treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n// }\n// \n// /* Return the distance between the closest two points in the vector points.\n//    Example: \n// \n//    input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n//    output: 1.41421\n// */\n// double closestPair(std::vector<Point> const& points) {\n// if (points.size() < 2) {\n// \t\treturn 0;\n// \t}\n// \tdouble min_dist = std::numeric_limits<double>::max();\n// \tfor (int i = 0; i < points.size(); ++i) {\n// \t\tfor (int j = i + 1; j < points.size(); ++j) {\n// \t\t\tdouble dist = distance(points[i], points[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// OpenMP implementation of closestPair\n#include <omp.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use OpenMP to compute in parallel.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\nif (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of largestComponent\n// /* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int largestComponent(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//   std::vector<int> component(N, 0);\n//   int max_component = 0;\n// \n//   for (size_t i = 0; i < N; i++) {\n//     if (!visited[i]) {\n//       int size = 0;\n//       std::queue<int> q;\n//       q.push(i);\n//       visited[i] = true;\n//       while (!q.empty()) {\n//         int v = q.front();\n//         q.pop();\n//         size++;\n//         for (size_t j = 0; j < N; j++) {\n//           if (A[v * N + j] == 1 &&!visited[j]) {\n//             q.push(j);\n//             visited[j] = true;\n//           }\n//         }\n//       }\n//       component[i] = size;\n//       max_component = std::max(max_component, size);\n//     }\n//   }\n// \n//   return max_component;\n// }\n\n// OpenMP implementation of largestComponent\n#include <omp.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}",
    "translation_function_name": "largestComponent"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of componentCount\n// /* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int componentCount(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//     int count = 0;\n// \n//     for (size_t i = 0; i < N; ++i) {\n//         if (!visited[i]) {\n//             ++count;\n//             std::stack<size_t> stack;\n//             stack.push(i);\n//             visited[i] = true;\n// \n//             while (!stack.empty()) {\n//                 size_t node = stack.top();\n//                 stack.pop();\n// \n//                 for (size_t j = 0; j < N; ++j) {\n//                     if (A[node * N + j] &&!visited[j]) {\n//                         stack.push(j);\n//                         visited[j] = true;\n//                     }\n//                 }\n//             }\n//         }\n//     }\n// \n//     return count;\n// }\n\n// OpenMP implementation of componentCount\n#include <omp.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] &&!visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}",
    "translation_function_name": "componentCount"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of edgeCount\n// /* Count the number of edges in the directed graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int edgeCount(std::vector<int> const& A, size_t N) {\n// int count = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++count;\n//       }\n//     }\n//   }\n//   return count;\n// }\n\n// OpenMP implementation of edgeCount\n#include <omp.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\nint count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
    "translation_function_name": "edgeCount"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of maxDegree\n// /* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int maxDegree(std::vector<int> const& A, size_t N) {\n// int max_degree = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     int degree = 0;\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++degree;\n//       }\n//     }\n//     if (degree > max_degree) {\n//       max_degree = degree;\n//     }\n//   }\n//   return max_degree;\n// }\n\n// OpenMP implementation of maxDegree\n#include <omp.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\nint max_degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}",
    "translation_function_name": "maxDegree"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "translation_prompt": "// Serial implementation of shortestPathLength\n// /* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n//    output: 2\n// */\n// int shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n// std::vector<int> dist(N, INT_MAX);\n// \tstd::vector<int> prev(N, -1);\n// \tstd::queue<int> q;\n// \tq.push(source);\n// \tdist[source] = 0;\n// \twhile (!q.empty()) {\n// \t\tint u = q.front();\n// \t\tq.pop();\n// \t\tfor (int v = 0; v < N; ++v) {\n// \t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n// \t\t\t\tdist[v] = dist[u] + 1;\n// \t\t\t\tprev[v] = u;\n// \t\t\t\tq.push(v);\n// \t\t\t}\n// \t\t}\n// \t}\n// \tif (dist[dest] == INT_MAX) {\n// \t\treturn -1;\n// \t}\n// \tint path_len = 0;\n// \twhile (prev[dest]!= -1) {\n// \t\tpath_len++;\n// \t\tdest = prev[dest];\n// \t}\n// \treturn path_len;\n// }\n\n// OpenMP implementation of shortestPathLength\n#include <omp.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use OpenMP to compute in parallel.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\nstd::vector<int> dist(N, INT_MAX);\n\tstd::vector<int> prev(N, -1);\n\tstd::queue<int> q;\n\tq.push(source);\n\tdist[source] = 0;\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tprev[v] = u;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == INT_MAX) {\n\t\treturn -1;\n\t}\n\tint path_len = 0;\n\twhile (prev[dest]!= -1) {\n\t\tpath_len++;\n\t\tdest = prev[dest];\n\t}\n\treturn path_len;\n}",
    "translation_function_name": "shortestPathLength"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of smallestOdd\n// /* Return the value of the smallest odd number in the vector x.\n//    Examples:\n// \n//    input: [7, 9, 5, 2, 8, 16, 4, 1]\n//    output: 1\n// \n//    input: [8, 36, 7, 2, 11]\n//    output: 7\n// */\n// int smallestOdd(std::vector<int> const& x) {\n// int smallest = x[0];\n//   for (int i = 1; i < x.size(); i++) {\n//     if (x[i] % 2 == 1 && x[i] < smallest) {\n//       smallest = x[i];\n//     }\n//   }\n//   return smallest;\n// }\n\n// OpenMP implementation of smallestOdd\n#include <omp.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\nint smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}",
    "translation_function_name": "smallestOdd"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of productWithInverses\n// /* Return the product of the vector x with every odd indexed element inverted.\n//    i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n//    Example:\n// \n//    input: [4, 2, 10, 4, 5]\n//    output: 25\n// */\n// double productWithInverses(std::vector<double> const& x) {\n// double result = 1;\n//   for (int i = 0; i < x.size(); i++) {\n//     if (i % 2 == 0) {\n//       result *= x[i];\n//     } else {\n//       result *= 1 / x[i];\n//     }\n//   }\n//   return result;\n// }\n\n// OpenMP implementation of productWithInverses\n#include <omp.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use OpenMP to compute product in parallel.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\ndouble result = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
    "translation_function_name": "productWithInverses"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "translation_prompt": "// Serial implementation of sumOfMinimumElements\n// /* Return the sum of the minimum value at each index of vectors x and y for all indices.\n//    i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n//    Example:\n// \n//    input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n//    output: 10\n// */\n// double sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n// double sum = 0;\n//   for (int i = 0; i < x.size(); i++) {\n//     sum += std::min(x[i], y[i]);\n//   }\n//   return sum;\n// }\n\n// OpenMP implementation of sumOfMinimumElements\n#include <omp.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use OpenMP to sum in parallel.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\ndouble sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}",
    "translation_function_name": "sumOfMinimumElements"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of average\n// /* Return the average of the vector x.\n//    Examples:\n// \t\t\n// \t input: [1, 8, 4, 5, 1]\n//    output: 3.8\n// \n//    input: [2, 2, 2, 3]\n//    output: 2.25\n// */\n// double average(std::vector<double> const& x) {\n// double sum = 0;\n// \tfor (double i : x) {\n// \t\tsum += i;\n// \t}\n// \treturn sum / x.size();\n// }\n\n// OpenMP implementation of average\n#include <omp.h>\n\n/* Return the average of the vector x.\n   Use OpenMP to compute in parallel.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\ndouble sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
    "translation_function_name": "average"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "translation_prompt": "// Serial implementation of reduceLogicalXOR\n// /* Return the logical XOR reduction of the vector of bools x.\n//    Example:\n// \n//    input: [false, false, false, true]\n//    output: true\n// */\n// bool reduceLogicalXOR(std::vector<bool> const& x) {\n// bool result = false;\n//   for (auto const& i : x) {\n//     result ^= i;\n//   }\n//   return result;\n// }\n\n// OpenMP implementation of reduceLogicalXOR\n#include <omp.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use OpenMP to reduce in parallel.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\nbool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}",
    "translation_function_name": "reduceLogicalXOR"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of sumOfPrefixSum\n// /* Compute the prefix sum array of the vector x and return its sum.\n//    Example:\n// \n//    input: [-7, 2, 1, 9, 4, 8]\n//    output: 15\n// */\n// double sumOfPrefixSum(std::vector<double> const& x) {\n// double sum = 0;\n//     std::vector<double> prefix_sum(x.size());\n// \n//     for (size_t i = 0; i < x.size(); ++i) {\n//         if (i == 0) {\n//             prefix_sum[i] = x[i];\n//         } else {\n//             prefix_sum[i] = prefix_sum[i - 1] + x[i];\n//         }\n//         sum += prefix_sum[i];\n//     }\n// \n//     return sum;\n// }\n\n// OpenMP implementation of sumOfPrefixSum\n#include <omp.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\ndouble sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}",
    "translation_function_name": "sumOfPrefixSum"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "translation_prompt": "// Serial implementation of partialMinimums\n// /* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n//    Examples:\n// \n//    input: [8, 6, -1, 7, 3, 4, 4]\n//    output: [8, 6, -1, -1, -1, -1, -1]\n// \n//    input: [5, 4, 6, 4, 3, 6, 1, 1]\n//    output: [5, 4, 4, 4, 3, 3, 1, 1]\n// */\n// void partialMinimums(std::vector<float> &x) {\n// float min = x[0];\n//     for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < min) {\n//             min = x[i];\n//         }\n//         x[i] = min;\n//     }\n// }\n\n// OpenMP implementation of partialMinimums\n#include <omp.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\nfloat min = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n}",
    "translation_function_name": "partialMinimums"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of prefixSum\n// /* Compute the prefix sum of the vector x into output.\n//    Example:\n//    \n//    input: [1, 7, 4, 6, 6, 2]\n//    output: [1, 8, 12, 18, 24, 26]\n// */\n// void prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n// output.resize(x.size());\n//   output[0] = x[0];\n//   for (int i = 1; i < x.size(); ++i) {\n//     output[i] = output[i-1] + x[i];\n//   }\n// }\n\n// OpenMP implementation of prefixSum\n#include <omp.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\noutput.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}",
    "translation_function_name": "prefixSum"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of negateOddsAndHalveEvens\n// /* In the vector x negate the odd values and divide the even values by 2.\n//    Example:\n// \n//    input: [16, 11, 12, 14, 1, 0, 5]\n//    output: [8, -11, 6, 7, -1, 0, -5]\n// */\n// void negateOddsAndHalveEvens(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] % 2 == 0) {\n//             x[i] /= 2;\n//         } else {\n//             x[i] = -x[i];\n//         }\n//     }\n// }\n\n// OpenMP implementation of negateOddsAndHalveEvens\n#include <omp.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
    "translation_function_name": "negateOddsAndHalveEvens"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "translation_prompt": "// Serial implementation of mapPowersOfTwo\n// bool isPowerOfTwo(int x) {\n// \treturn (x > 0) && !(x & (x - 1));\n// }\n// \n// /* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n//    Example:\n// \n//    input: [8, 0, 9, 7, 15, 64, 3]\n//    output: [true, false, false, false, false, true, false]\n// */\n// void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n// for (int i = 0; i < x.size(); i++) {\n// \t\tmask[i] = isPowerOfTwo(x[i]);\n// \t}\n// }\n\n// OpenMP implementation of mapPowersOfTwo\n#include <omp.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\nfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
    "translation_function_name": "mapPowersOfTwo"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of oneMinusInverse\n// /* Replace every element of the vector x with 1-1/x.\n//    Example:\n// \n//    input: [2, 4, 1, 12, -2]\n//    output: [0.5, 0.75, 0, 0.91666666, 1.5]\n// */\n// void oneMinusInverse(std::vector<double> &x) {\n// for (auto &e : x) {\n//     e = 1.0 - 1.0 / e;\n//   }\n// }\n\n// OpenMP implementation of oneMinusInverse\n#include <omp.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\nfor (auto &e : x) {\n    e = 1.0 - 1.0 / e;\n  }\n}",
    "translation_function_name": "oneMinusInverse"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of relu\n// /* Compute the ReLU function on every element of x. Elements less than zero become zero,\n//    while elements greater than zero stay the same.\n//    Example:\n// \n//    input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n//    output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n// */\n// void relu(std::vector<double> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < 0) {\n//             x[i] = 0;\n//         }\n//     }\n// }\n\n// OpenMP implementation of relu\n#include <omp.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
    "translation_function_name": "relu"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of squareEach\n// /* Replace every element of x with the square of its value.\n//    Example:\n// \n//    input: [5, 1, 2, -4, 8]\n//    output: [25, 1, 4, 16, 64]\n// */\n// void squareEach(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         x[i] = x[i] * x[i];\n//     }\n// }\n\n// OpenMP implementation of squareEach\n#include <omp.h>\n\n/* Replace every element of x with the square of its value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
    "translation_function_name": "squareEach"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of spmv\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n//    and A is a sparse matrix stored in COO format.\n//    A has dimensions MxN, x has N values, and y has M values.\n//    Example:\n// \n//    input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n//    output: y=[2, 3]\n// */\n// void spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//       y[i] *= beta;\n//    }\n// \n//    for (size_t i = 0; i < A.size(); ++i) {\n//       y[A[i].row] += alpha * A[i].value * x[A[i].column];\n//    }\n// }\n\n// OpenMP implementation of spmv\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use OpenMP to parallelize.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}",
    "translation_function_name": "spmv"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of spmm\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n//    X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n//    Example:\n// \n//    input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n//    output: Y=[{-1,-4}, {1,0}]\n// */\n// void spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n// std::vector<double> Ytemp(M*N, 0.0);\n//    for (auto const& a : A) {\n//       for (auto const& x : X) {\n//          if (a.column == x.row) {\n//             Ytemp[a.row*N + x.column] += a.value * x.value;\n//          }\n//       }\n//    }\n//    Y = std::move(Ytemp);\n// }\n\n// OpenMP implementation of spmm\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\nstd::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}",
    "translation_function_name": "spmm"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    A is a sparse NxN matrix stored in COO format.\n//    Example:\n// \n//    input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n//    output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n// */\n// void luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n// \n// // Assuming A is sorted by row and then by column\n// L.clear();\n// U.clear();\n// \n// // Initialize L and U with zeros\n// L.resize(N * N, 0);\n// U.resize(N * N, 0);\n// \n// // Create a map for quick access to A's elements\n// std::map<std::pair<size_t, size_t>, double> A_map;\n// for (const auto& elem : A) {\n//     A_map[{elem.row, elem.column}] = elem.value;\n// }\n// \n// // LU decomposition algorithm\n// for (size_t i = 0; i < N; ++i) {\n//     // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n//     for (size_t k = i; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[i * N + j] * U[j * N + k];\n//         }\n//         U[i * N + k] = A_map[{i, k}] - sum;\n//     }\n// \n//     // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n//     for (size_t k = i + 1; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[k * N + j] * U[j * N + i];\n//         }\n//         L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n//     }\n// \n//     // L(i, i) = 1\n//     L[i * N + i] = 1.0;\n// }\n// }\n\n// OpenMP implementation of luFactorize\n#include <omp.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n\n// Assuming A is sorted by row and then by column\nL.clear();\nU.clear();\n\n// Initialize L and U with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Create a map for quick access to A's elements\nstd::map<std::pair<size_t, size_t>, double> A_map;\nfor (const auto& elem : A) {\n    A_map[{elem.row, elem.column}] = elem.value;\n}\n\n// LU decomposition algorithm\nfor (size_t i = 0; i < N; ++i) {\n    // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[i * N + j] * U[j * N + k];\n        }\n        U[i * N + k] = A_map[{i, k}] - sum;\n    }\n\n    // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[k * N + j] * U[j * N + i];\n        }\n        L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n    }\n\n    // L(i, i) = 1\n    L[i * N + i] = 1.0;\n}\n}",
    "translation_function_name": "luFactorize"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "translation_prompt": "// Serial implementation of firstLetterCounts\n// /* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n//    Assume all strings are in lower case. Store the output in `bins` array.\n//    Example:\n// \n//    input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n//    output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n// */\n// void firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n// for (auto const& str : s) {\n//         bins[str[0] - 'a']++;\n//     }\n// }\n\n// OpenMP implementation of firstLetterCounts\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\nfor (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
    "translation_function_name": "firstLetterCounts"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "translation_prompt": "// Serial implementation of binsBy10Count\n// /* Vector x contains values between 0 and 100, inclusive. Count the number of\n//    values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n//    Example:\n// \n//    input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n//    output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n// */\n// void binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n// for (auto const& value : x) {\n//     if (value < 10) {\n//       bins[0]++;\n//     } else if (value < 20) {\n//       bins[1]++;\n//     } else if (value < 30) {\n//       bins[2]++;\n//     } else if (value < 40) {\n//       bins[3]++;\n//     } else if (value < 50) {\n//       bins[4]++;\n//     } else if (value < 60) {\n//       bins[5]++;\n//     } else if (value < 70) {\n//       bins[6]++;\n//     } else if (value < 80) {\n//       bins[7]++;\n//     } else if (value < 90) {\n//       bins[8]++;\n//     } else {\n//       bins[9]++;\n//     }\n//   }\n// }\n\n// OpenMP implementation of binsBy10Count\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\nfor (auto const& value : x) {\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
    "translation_function_name": "binsBy10Count"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuadrants\n// struct Point {\n//    double x, y;\n// };\n// \n// /* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n//    Store the counts in `bins`.\n//    Example:\n// \n//    input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n//    output: [3, 1, 0, 2]\n// */\n// void countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n// for (auto const& point : points) {\n//       if (point.x > 0 && point.y > 0) {\n//          bins[0]++;\n//       } else if (point.x < 0 && point.y > 0) {\n//          bins[1]++;\n//       } else if (point.x < 0 && point.y < 0) {\n//          bins[2]++;\n//       } else {\n//          bins[3]++;\n//       }\n//    }\n// }\n\n// OpenMP implementation of countQuadrants\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\nfor (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
    "translation_function_name": "countQuadrants"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuartiles\n// /* Count the number of doubles in the vector x that have a fractional part \n//    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n//    Examples:\n// \n//    input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n//    output: [2, 1, 2, 2]\n// \n//    input: [1.9, 0.2, 0.6, 10.1, 7.4]\n//    output: [2, 1, 1, 1]\n// */\n// void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n// for (double num : x) {\n//         double fractional_part = num - std::floor(num);\n//         if (fractional_part < 0.25) {\n//             bins[0]++;\n//         } else if (fractional_part < 0.5) {\n//             bins[1]++;\n//         } else if (fractional_part < 0.75) {\n//             bins[2]++;\n//         } else {\n//             bins[3]++;\n//         }\n//     }\n// }\n\n// OpenMP implementation of countQuartiles\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\nfor (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
    "translation_function_name": "countQuartiles"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "translation_prompt": "// Serial implementation of pixelCounts\n// /* Count the number of pixels in image with each grayscale intensity.\n//    The vector `image` is a grayscale image with values 0-255.\n//    Store the results in `bins`.\n//    Example:\n//    \n//    input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n//    output: [0, 0, 2, 0, 1, ...]\n// */\n//  void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n// for (auto const& pixel : image) {\n//         bins[pixel]++;\n//     }\n// }\n\n// OpenMP implementation of pixelCounts\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\nfor (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}",
    "translation_function_name": "pixelCounts"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of findFirstEven\n// /* Return the index of the first even number in the vector x.\n//    Examples:\n// \n//    input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n//    output: 6\n// \n//    input: [3, 8, 9, 9, 3, 4, 8, 6]\n//    output: 1\n// */\n// size_t findFirstEven(std::vector<int> const& x) {\n// for (size_t i = 0; i < x.size(); ++i) {\n//     if (x[i] % 2 == 0) {\n//       return i;\n//     }\n//   }\n//   return x.size();\n// }\n\n// OpenMP implementation of findFirstEven\n#include <omp.h>\n\n/* Return the index of the first even number in the vector x.\n   Use OpenMP to parallelize the search.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}",
    "translation_function_name": "findFirstEven"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "translation_prompt": "// Serial implementation of contains\n// /* Return true if the vector x contains the value `target`. Return false otherwise.\n//    Examples:\n// \n//    input: x=[1, 8, 2, 6, 4, 6], target=3\n//    output: false\n//    \n//    input: x=[1, 8, 2, 6, 4, 6], target=8\n//    output: true\n// */\n// bool contains(std::vector<int> const& x, int target) {\n// return std::find(x.begin(), x.end(), target)!= x.end();\n// }\n\n// OpenMP implementation of contains\n#include <omp.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\nreturn std::find(x.begin(), x.end(), target)!= x.end();\n}",
    "translation_function_name": "contains"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "translation_prompt": "// Serial implementation of xorContains\n// /* Return true if `val` is only in one of vectors x or y.\n//    Return false if it is in both or neither.\n//    Examples:\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n//    output: true\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n//    output: false\n// */\n// bool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n// return std::find(x.begin(), x.end(), val)!= x.end() ^\n//          std::find(y.begin(), y.end(), val)!= y.end();\n// }\n\n// OpenMP implementation of xorContains\n#include <omp.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use OpenMP to search in parallel.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\nreturn std::find(x.begin(), x.end(), val)!= x.end() ^\n         std::find(y.begin(), y.end(), val)!= y.end();\n}",
    "translation_function_name": "xorContains"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "translation_prompt": "// Serial implementation of findLastShortBook\n// struct Book {\n//    std::string title;\n//    int pages;\n// };\n// \n// /* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n// \t Example:\n// \n//    input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n//    output: 2\n// */\n// size_t findLastShortBook(std::vector<Book> const& books) {\n// size_t last_short_book_index = 0;\n//    for (size_t i = 0; i < books.size(); ++i) {\n//       if (books[i].pages < 100) {\n//          last_short_book_index = i;\n//       }\n//    }\n//    return last_short_book_index;\n// }\n\n// OpenMP implementation of findLastShortBook\n#include <omp.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use OpenMP to search in parallel.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\nsize_t last_short_book_index = 0;\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
    "translation_function_name": "findLastShortBook"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of findClosestToPi\n// /* Return the index of the value in the vector x that is closest to the math constant PI.\n//    Use M_PI for the value of PI.\n//    Example:\n// \n//    input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n//    output: 1\n// */\n// size_t findClosestToPi(std::vector<double> const& x) {\n// double min_distance = std::numeric_limits<double>::max();\n//   size_t min_index = 0;\n//   for (size_t i = 0; i < x.size(); ++i) {\n//     double distance = std::abs(x[i] - M_PI);\n//     if (distance < min_distance) {\n//       min_distance = distance;\n//       min_index = i;\n//     }\n//   }\n//   return min_index;\n// }\n\n// OpenMP implementation of findClosestToPi\n#include <omp.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use OpenMP to search in parallel.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\ndouble min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < min_distance) {\n      min_distance = distance;\n      min_index = i;\n    }\n  }\n  return min_index;\n}",
    "translation_function_name": "findClosestToPi"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n// int n = x.size();\n//     if (n == 0) return;\n//     if (n == 1) {\n//         r[0] = x[0].real();\n//         i[0] = x[0].imag();\n//         return;\n//     }\n//     std::vector<std::complex<double>> x_even(n / 2);\n//     std::vector<std::complex<double>> x_odd(n / 2);\n//     for (int i = 0; i < n / 2; i++) {\n//         x_even[i] = x[2 * i];\n//         x_odd[i] = x[2 * i + 1];\n//     }\n//     std::vector<double> r_even(n / 2);\n//     std::vector<double> i_even(n / 2);\n//     std::vector<double> r_odd(n / 2);\n//     std::vector<double> i_odd(n / 2);\n//     fft(x_even, r_even, i_even);\n//     fft(x_odd, r_odd, i_odd);\n//     for (int k = 0; k < n / 2; k++) {\n//         std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n//         r[k] = r_even[k] + z.real();\n//         i[k] = i_even[k] + z.imag();\n//         r[k + n / 2] = r_even[k] - z.real();\n//         i[k + n / 2] = i_even[k] - z.imag();\n//     }\n// }\n\n// OpenMP implementation of fft\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\nint n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}",
    "translation_function_name": "fft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of fftConjugate\n// /* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fftConjugate(std::vector<std::complex<double>> &x) {\n// int n = x.size();\n//     int m = 0;\n//     for (int i = 0; i < n; i++) {\n//         int j = i;\n//         for (int k = 0; k < m; k++) {\n//             j = j ^ (1 << k);\n//         }\n//         if (i < j) {\n//             std::complex<double> temp = x[i];\n//             x[i] = x[j];\n//             x[j] = temp;\n//         }\n//         m = std::max(m, __builtin_ctz(j));\n//     }\n//     for (int i = 1; i < n; i *= 2) {\n//         std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n//         for (int j = 0; j < n; j += 2 * i) {\n//             for (int k = 0; k < i; k++) {\n//                 std::complex<double> u = x[j + k];\n//                 std::complex<double> t = w * x[j + k + i];\n//                 x[j + k] = u + t;\n//                 x[j + k + i] = u - t;\n//             }\n//             w = w * w;\n//         }\n//     }\n// }\n\n// OpenMP implementation of fftConjugate\n#include <omp.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\nint n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}",
    "translation_function_name": "fftConjugate"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of ifft\n// /* forward declare fft. computes fourier transform in-place */\n// void fft(std::vector<std::complex<double>> &x);\n// \n// /* Compute the inverse fourier transform of x in-place.\n//    Example:\n//    \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n// */\n// void ifft(std::vector<std::complex<double>> &x) {\n// \n// // Take conjugate of the complex numbers\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Apply fft\n// fft(x);\n// \n// // Take conjugate again\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Scale the numbers\n// for (auto &elem : x) {\n//     elem /= static_cast<double>(x.size());\n// }\n// }\n\n// OpenMP implementation of ifft\n#include <omp.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n\n// Take conjugate of the complex numbers\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Apply fft\nfft(x);\n\n// Take conjugate again\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Scale the numbers\nfor (auto &elem : x) {\n    elem /= static_cast<double>(x.size());\n}\n}",
    "translation_function_name": "ifft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of dft\n// /* Compute the discrete fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1, 4, 9, 16]\n//    output: [30+0i, -8-12i, -10-0i, -8+12i]\n// */\n// void dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// // TODO: implement this function\n//     output.resize(x.size());\n//     for (int i = 0; i < x.size(); i++) {\n//         output[i] = 0;\n//         for (int j = 0; j < x.size(); j++) {\n//             output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n//         }\n//     }\n// }\n\n// OpenMP implementation of dft\n#include <omp.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// TODO: implement this function\n    output.resize(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}",
    "translation_function_name": "dft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n// int n = x.size();\n//     output.resize(n);\n// \n//     // base case\n//     if (n == 1) {\n//         output[0] = x[0];\n//         return;\n//     }\n// \n//     // radix 2 Cooley-Tukey FFT\n//     // decompose x into even and odd parts\n//     std::vector<std::complex<double>> x_even(n/2);\n//     std::vector<std::complex<double>> x_odd(n/2);\n//     for (int k = 0; k < n/2; k++) {\n//         x_even[k] = x[2*k];\n//         x_odd[k] = x[2*k + 1];\n//     }\n// \n//     // recursively compute FFTs of even and odd parts\n//     std::vector<std::complex<double>> y_even(n/2);\n//     std::vector<std::complex<double>> y_odd(n/2);\n//     fft(x_even, y_even);\n//     fft(x_odd, y_odd);\n// \n//     // combine results\n//     for (int k = 0; k < n/2; k++) {\n//         double kth = -2 * M_PI * k / n;\n//         output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//         output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//     }\n// }\n\n// OpenMP implementation of fft\n#include <omp.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\nint n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}",
    "translation_function_name": "fft"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "translation_prompt": "// Serial implementation of axpy\n// /* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n//    Example:\n//    \n//    input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n//    output: z=[2, -6, 5, 17]\n// */\n// void axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n// for (int i = 0; i < x.size(); ++i) {\n//     z[i] = alpha*x[i] + y[i];\n//   }\n// }\n\n// OpenMP implementation of axpy\n#include <omp.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\nfor (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}",
    "translation_function_name": "axpy"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "translation_prompt": "// Serial implementation of solveLinearSystem\n// /* Solve the linear system Ax=b for x.\n//    A is an NxN matrix in row-major. x and b have N elements.\n//    Example:\n//    \n//    input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n//    output: x=[3, 1, 2]\n// */\n// void solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// // check if the input is valid\n//     if (A.size()!= N * N) {\n//         throw std::invalid_argument(\"A must be a NxN matrix\");\n//     }\n//     if (b.size()!= N) {\n//         throw std::invalid_argument(\"b must have N elements\");\n//     }\n//     // create a copy of A\n//     std::vector<double> A_copy(A);\n//     // create a copy of b\n//     std::vector<double> b_copy(b);\n//     // perform Gaussian elimination\n//     for (size_t i = 0; i < N; i++) {\n//         // find the pivot row and swap it with the current row\n//         size_t pivot_row = i;\n//         for (size_t j = i + 1; j < N; j++) {\n//             if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n//                 pivot_row = j;\n//             }\n//         }\n//         if (pivot_row!= i) {\n//             // swap the current row with the pivot row\n//             for (size_t j = 0; j < N; j++) {\n//                 std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n//             }\n//             std::swap(b_copy[i], b_copy[pivot_row]);\n//         }\n//         // eliminate the current row\n//         for (size_t j = i + 1; j < N; j++) {\n//             double factor = A_copy[j * N + i] / A_copy[i * N + i];\n//             for (size_t k = 0; k < N; k++) {\n//                 A_copy[j * N + k] -= factor * A_copy[i * N + k];\n//             }\n//             b_copy[j] -= factor * b_copy[i];\n//         }\n//     }\n//     // back substitution\n//     x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n//     for (size_t i = N - 2; i < N; i--) {\n//         double sum = 0;\n//         for (size_t j = i + 1; j < N; j++) {\n//             sum += A_copy[i * N + j] * x[j];\n//         }\n//         x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n//     }\n// }\n\n// OpenMP implementation of solveLinearSystem\n#include <omp.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}",
    "translation_function_name": "solveLinearSystem"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of gemm\n// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n//    output: C=[[9, 5], [4, 2]]\n// */\n// void gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             double sum = 0;\n//             for (size_t k = 0; k < K; ++k) {\n//                 sum += A[i * K + k] * B[k * N + j];\n//             }\n//             C[i * N + j] = sum;\n//         }\n//     }\n// }\n\n// OpenMP implementation of gemm\n#include <omp.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
    "translation_function_name": "gemm"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of gemv\n// /* Multiply the matrix A by the vector x. Store the results in the vector y.\n//    A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n//    output: y=[1, -3]\n// */\n// void gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         double sum = 0;\n//         for (size_t j = 0; j < N; ++j) {\n//             sum += A[i * N + j] * x[j];\n//         }\n//         y[i] = sum;\n//     }\n// }\n\n// OpenMP implementation of gemv\n#include <omp.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
    "translation_function_name": "gemv"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// /* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    Store the results for L and U into the original matrix A. \n//    A is an NxN matrix stored in row-major.\n//    Example:\n// \n//    input: [[4, 3], [6, 3]]\n//    output: [[4, 3], [1.5, -1.5]]\n// */\n// void luFactorize(std::vector<double> &A, size_t N) {\n// for (size_t i = 0; i < N; i++) {\n//     for (size_t j = i + 1; j < N; j++) {\n//       A[j * N + i] = A[j * N + i] / A[i * N + i];\n//       for (size_t k = i + 1; k < N; k++) {\n//         A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n//       }\n//     }\n//   }\n// }\n\n// OpenMP implementation of luFactorize\n#include <omp.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\nfor (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}",
    "translation_function_name": "luFactorize"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "translation_prompt": "// Serial implementation of ranks\n// /* For each value in the vector x compute its index in the sorted vector.\n//    Store the results in `ranks`.\n//    Examples:\n// \n//    input: [3.1, 2.8, 9.1, 0.4, 3.14]\n//    output: [2, 1, 4, 0, 3]\n//  \n//    input: [100, 7.6, 16.1, 18, 7.6]\n//    output: [4, 0, 1, 2, 3]\n// */\n// void ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n// std::vector<float> sorted = x;\n//     std::sort(sorted.begin(), sorted.end());\n//     ranks.resize(x.size());\n//     for (size_t i = 0; i < x.size(); ++i) {\n//         ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n//     }\n// }\n\n// OpenMP implementation of ranks\n#include <omp.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\nstd::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}",
    "translation_function_name": "ranks"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of sortIgnoreZero\n// /* Sort the vector x in ascending order ignoring elements with value 0.\n//    Leave zero valued elements in-place.\n//    Example:\n// \n// \t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n//    output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n// */\n// void sortIgnoreZero(std::vector<int> &x) {\n// int n = x.size();\n//   int i = 0;\n//   int j = 0;\n//   int k = 0;\n// \n//   while (i < n) {\n//     if (x[i] == 0) {\n//       i++;\n//     } else {\n//       j = i + 1;\n//       while (j < n) {\n//         if (x[j] == 0) {\n//           j++;\n//         } else {\n//           if (x[i] > x[j]) {\n//             k = x[i];\n//             x[i] = x[j];\n//             x[j] = k;\n//           }\n//           j++;\n//         }\n//       }\n//       i++;\n//     }\n//   }\n// }\n\n// OpenMP implementation of sortIgnoreZero\n#include <omp.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use OpenMP to sort x in parallel. \n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\nint n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < n) {\n    if (x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < n) {\n        if (x[j] == 0) {\n          j++;\n        } else {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n}",
    "translation_function_name": "sortIgnoreZero"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "translation_prompt": "// Serial implementation of findKthSmallest\n// /* Find the k-th smallest element of the vector x.\n//    Example:\n//    \n//    input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n//    output: 6\n// */\n// int findKthSmallest(std::vector<int> const& x, int k) {\n// int n = x.size();\n//     std::vector<int> y(x);\n//     std::sort(y.begin(), y.end());\n//     return y[k-1];\n// }\n\n// OpenMP implementation of findKthSmallest\n#include <omp.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use OpenMP to compute in parallel.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\nint n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
    "translation_function_name": "findKthSmallest"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "translation_prompt": "// Serial implementation of sortByStartTime\n// struct Result {\n//    int startTime, duration;\n//    float value;\n// };\n// \n// /* Sort vector of Result structs by start time in ascending order.\n//    Example:\n//    \n//    input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n//    output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n// */\n// void sortByStartTime(std::vector<Result> &results) {\n// std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n//       return a.startTime < b.startTime;\n//    });\n// }\n\n// OpenMP implementation of sortByStartTime\n#include <omp.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\nstd::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
    "translation_function_name": "sortByStartTime"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of sortComplexByMagnitude\n// /* Sort the vector x of complex numbers by their magnitude in ascending order.\n//    Example:\n//    \n//    input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n//    output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n// */\n// void sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n// std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n//         return std::abs(a) < std::abs(b);\n//     });\n// }\n\n// OpenMP implementation of sortComplexByMagnitude\n#include <omp.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use OpenMP to sort in parallel.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\nstd::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
    "translation_function_name": "sortComplexByMagnitude"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of cellsXOR\n// /* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n//    Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n//    input and output are NxN grids of ints in row-major.\n//    Example:\n// \n//    input: [[0, 1, 1, 0],\n//            [1, 0, 0, 0],\n//            [0, 0, 0, 0],\n//            [0, 1, 0, 0]\n//    output: [[0, 0, 1, 1],\n//             [1, 0, 0, 1],\n//             [0, 0, 1, 0],\n//             [1, 0, 1, 0]]\n// */\n// void cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int count = 0;\n//             if (i > 0 && input[i - 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (i < N - 1 && input[i + 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (j > 0 && input[i + (j - 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (count == 1) {\n//                 output[i + j * N] = 1;\n//             } else {\n//                 output[i + j * N] = 0;\n//             }\n//         }\n//     }\n// }\n\n// OpenMP implementation of cellsXOR\n#include <omp.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
    "translation_function_name": "cellsXOR"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of gameOfLife\n// /* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n//    A cell is 1 if it is alive and 0 if it is dead.\n//    If a live cell has fewer than 2 live neighbors then it dies.\n//    If a live cell has 2 or 3 live neighbors then it lives on.\n//    If a live cell has more than 3 live neighbords then it dies.\n//    If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input:  [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 0, 0, 0],\n//             [0, 1, 1, 0, 0],\n//             [0, 0, 1, 1, 0],\n//             [0, 1, 0, 0, 0]]\n//    output: [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 1, 0, 0],\n//             [0, 1, 0, 1, 0],\n//             [0, 0, 0, 1, 0],\n//             [0, 0, 1, 0, 0]]\n// */\n// void gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// std::vector<int> next(input.size(), 0);\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       int count = 0;\n//       for (int x = -1; x <= 1; ++x) {\n//         for (int y = -1; y <= 1; ++y) {\n//           if (x == 0 && y == 0) {\n//             continue;\n//           }\n//           size_t r = (i + x + N) % N;\n//           size_t c = (j + y + N) % N;\n//           count += input[r * N + c];\n//         }\n//       }\n//       if (input[i * N + j] == 1) {\n//         next[i * N + j] = (count == 2 || count == 3);\n//       } else {\n//         next[i * N + j] = (count == 3);\n//       }\n//     }\n//   }\n//   output = next;\n// }\n\n// OpenMP implementation of gameOfLife\n#include <omp.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nstd::vector<int> next(input.size(), 0);\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}",
    "translation_function_name": "gameOfLife"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of jacobi1D\n// /* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n//    i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n//    Replace with 0 when reading past the boundaries of `input`.\n//    Example:\n// \n//    input: [9, -6, -1, 2, 3]\n//    output: [1, 2/3, -5/3, 4/3, 5/3]\n// */\n// void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// // YOUR CODE HERE\n//     for (int i = 1; i < input.size() - 1; i++) {\n//         output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n//     }\n// }\n\n// OpenMP implementation of jacobi1D\n#include <omp.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// YOUR CODE HERE\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
    "translation_function_name": "jacobi1D"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "translation_prompt": "// Serial implementation of convolveKernel\n// const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n// \n// /* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n//    the dot product of itself and its neighbors with the edge kernel.\n//    Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n//    imageIn and imageOut are NxN grayscale images stored in row-major.\n//    Store the output of the computation in imageOut.\n//    Example:\n// \n//    input: [[112, 118, 141, 152],\n//            [93, 101, 119, 203],\n//            [45, 17, 16, 232],\n//            [82, 31, 49, 101]]\n//    output: [[255, 255, 255, 255],\n//             [255, 147, 0, 255],\n//             [36, 0, 0, 255],\n//             [255, 39, 0, 255]]\n// */\n// void convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int sum = 0;\n//             for (int k = -1; k <= 1; ++k) {\n//                 for (int l = -1; l <= 1; ++l) {\n//                     int x = i + k;\n//                     int y = j + l;\n//                     if (x >= 0 && x < N && y >= 0 && y < N) {\n//                         sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n//                     }\n//                 }\n//             }\n//             imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n//         }\n//     }\n// }\n\n// OpenMP implementation of convolveKernel\n#include <omp.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}",
    "translation_function_name": "convolveKernel"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "omp",
    "prompt": "#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of jacobi2D\n// /* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n//    i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n//    Replace with 0 when reading past the boundaries of `input`.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n//    output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n// */\n// void jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n// for (size_t i = 1; i < N - 1; ++i) {\n//     for (size_t j = 1; j < N - 1; ++j) {\n//       output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n//     }\n//   }\n// }\n\n// OpenMP implementation of jacobi2D\n#include <omp.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "omp",
    "translation_src_example": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}",
    "translation_function_name": "jacobi2D"
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "12_geometry_smallest_triangle",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of smallestArea\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double triangleArea(Point const& A, Point const& B, Point const& C) {\n//   return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n// }\n// \n// /* Return the area of the smallest triangle that can be formed by any 3 points.\n//    Example:\n// \n//    input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n//    output: 5.5\n// */\n// double smallestArea(std::vector<Point> const& points) {\n// double minArea = std::numeric_limits<double>::max();\n//   for (int i = 0; i < points.size(); ++i) {\n//     for (int j = i + 1; j < points.size(); ++j) {\n//       for (int k = j + 1; k < points.size(); ++k) {\n//         double area = triangleArea(points[i], points[j], points[k]);\n//         if (area < minArea) {\n//           minArea = area;\n//         }\n//       }\n//     }\n//   }\n//   return minArea;\n// }\n\n// MPI implementation of smallestArea\n#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble triangleArea(Point const& A, Point const& B, Point const& C) {\n  return 0.5 * std::abs( A.x*(B.y-C.y) + B.x*(C.y-A.y) + C.x*(A.y-B.y) );\n}\n\n/* Return the area of the smallest triangle that can be formed by any 3 points.\n   Example:\n\n   input: [{0, 10}, {5, 5}, {1,0}, {-1, 1}, {-10, 0}]\n   output: 5.5\n*/\ndouble smallestArea(std::vector<Point> const& points) {\ndouble minArea = std::numeric_limits<double>::max();\n  for (int i = 0; i < points.size(); ++i) {\n    for (int j = i + 1; j < points.size(); ++j) {\n      for (int k = j + 1; k < points.size(); ++k) {\n        double area = triangleArea(points[i], points[j], points[k]);\n        if (area < minArea) {\n          minArea = area;\n        }\n      }\n    }\n  }\n  return minArea;\n}",
    "translation_function_name": "smallestArea"
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of closestPair\n// double distance(double x1, double x2) {\n// \treturn std::abs(x1 - x2);\n// }\n// \n// /* Return the distance between the closest two elements in the vector x.\n//    Example: \n// \n//    input: [7, 3, 9, 12, 31, 1]\n//    output: 2\n// */\n// double closestPair(std::vector<double> const& x) {\n// double min_dist = std::numeric_limits<double>::max();\n// \tfor (size_t i = 0; i < x.size(); ++i) {\n// \t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n// \t\t\tdouble dist = distance(x[i], x[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// MPI implementation of closestPair\n#include <mpi.h>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "double distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(std::vector<double> const& x) {\ndouble min_dist = std::numeric_limits<double>::max();\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tfor (size_t j = i + 1; j < x.size(); ++j) {\n\t\t\tdouble dist = distance(x[i], x[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair"
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "13_geometry_closest_pair_2d",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {",
    "translation_prompt": "// Serial implementation of closestPair\n// struct Point {\n// \tdouble x, y;\n// };\n// \n// double distance(Point const& p1, Point const& p2) {\n// \treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n// }\n// \n// /* Return the distance between the closest two points in the vector points.\n//    Example: \n// \n//    input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n//    output: 1.41421\n// */\n// double closestPair(std::vector<Point> const& points) {\n// if (points.size() < 2) {\n// \t\treturn 0;\n// \t}\n// \tdouble min_dist = std::numeric_limits<double>::max();\n// \tfor (int i = 0; i < points.size(); ++i) {\n// \t\tfor (int j = i + 1; j < points.size(); ++j) {\n// \t\t\tdouble dist = distance(points[i], points[j]);\n// \t\t\tif (dist < min_dist) {\n// \t\t\t\tmin_dist = dist;\n// \t\t\t}\n// \t\t}\n// \t}\n// \treturn min_dist;\n// }\n\n// MPI implementation of closestPair\n#include <mpi.h>\n\nstruct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The final result is returned on rank 0.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Point {\n\tdouble x, y;\n};\n\ndouble distance(Point const& p1, Point const& p2) {\n\treturn std::sqrt(std::pow(p2.x-p1.x, 2) + std::pow(p2.y-p1.y, 2));\n}\n\n/* Return the distance between the closest two points in the vector points.\n   Example: \n\n   input: [{2, 3}, {12, 30}, {40, 50}, {5, 1}, {12, 10}, {3, 4}]\n   output: 1.41421\n*/\ndouble closestPair(std::vector<Point> const& points) {\nif (points.size() < 2) {\n\t\treturn 0;\n\t}\n\tdouble min_dist = std::numeric_limits<double>::max();\n\tfor (int i = 0; i < points.size(); ++i) {\n\t\tfor (int j = i + 1; j < points.size(); ++j) {\n\t\t\tdouble dist = distance(points[i], points[j]);\n\t\t\tif (dist < min_dist) {\n\t\t\t\tmin_dist = dist;\n\t\t\t}\n\t\t}\n\t}\n\treturn min_dist;\n}",
    "translation_function_name": "closestPair"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "16_graph_largest_component",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of largestComponent\n// /* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int largestComponent(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//   std::vector<int> component(N, 0);\n//   int max_component = 0;\n// \n//   for (size_t i = 0; i < N; i++) {\n//     if (!visited[i]) {\n//       int size = 0;\n//       std::queue<int> q;\n//       q.push(i);\n//       visited[i] = true;\n//       while (!q.empty()) {\n//         int v = q.front();\n//         q.pop();\n//         size++;\n//         for (size_t j = 0; j < N; j++) {\n//           if (A[v * N + j] == 1 &&!visited[j]) {\n//             q.push(j);\n//             visited[j] = true;\n//           }\n//         }\n//       }\n//       component[i] = size;\n//       max_component = std::max(max_component, size);\n//     }\n//   }\n// \n//   return max_component;\n// }\n\n// MPI implementation of largestComponent\n#include <mpi.h>\n\n/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the number of vertices in the largest component of the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint largestComponent(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n  std::vector<int> component(N, 0);\n  int max_component = 0;\n\n  for (size_t i = 0; i < N; i++) {\n    if (!visited[i]) {\n      int size = 0;\n      std::queue<int> q;\n      q.push(i);\n      visited[i] = true;\n      while (!q.empty()) {\n        int v = q.front();\n        q.pop();\n        size++;\n        for (size_t j = 0; j < N; j++) {\n          if (A[v * N + j] == 1 &&!visited[j]) {\n            q.push(j);\n            visited[j] = true;\n          }\n        }\n      }\n      component[i] = size;\n      max_component = std::max(max_component, size);\n    }\n  }\n\n  return max_component;\n}",
    "translation_function_name": "largestComponent"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "18_graph_count_components",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of componentCount\n// /* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n//    output: 2\n// */\n// int componentCount(std::vector<int> const& A, size_t N) {\n// std::vector<bool> visited(N, false);\n//     int count = 0;\n// \n//     for (size_t i = 0; i < N; ++i) {\n//         if (!visited[i]) {\n//             ++count;\n//             std::stack<size_t> stack;\n//             stack.push(i);\n//             visited[i] = true;\n// \n//             while (!stack.empty()) {\n//                 size_t node = stack.top();\n//                 stack.pop();\n// \n//                 for (size_t j = 0; j < N; ++j) {\n//                     if (A[node * N + j] &&!visited[j]) {\n//                         stack.push(j);\n//                         visited[j] = true;\n//                     }\n//                 }\n//             }\n//         }\n//     }\n// \n//     return count;\n// }\n\n// MPI implementation of componentCount\n#include <mpi.h>\n\n/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n\t Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The component count is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of connected components in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 0, 1, 0]]\n   output: 2\n*/\nint componentCount(std::vector<int> const& A, size_t N) {\nstd::vector<bool> visited(N, false);\n    int count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n        if (!visited[i]) {\n            ++count;\n            std::stack<size_t> stack;\n            stack.push(i);\n            visited[i] = true;\n\n            while (!stack.empty()) {\n                size_t node = stack.top();\n                stack.pop();\n\n                for (size_t j = 0; j < N; ++j) {\n                    if (A[node * N + j] &&!visited[j]) {\n                        stack.push(j);\n                        visited[j] = true;\n                    }\n                }\n            }\n        }\n    }\n\n    return count;\n}",
    "translation_function_name": "componentCount"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of edgeCount\n// /* Count the number of edges in the directed graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int edgeCount(std::vector<int> const& A, size_t N) {\n// int count = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++count;\n//       }\n//     }\n//   }\n//   return count;\n// }\n\n// MPI implementation of edgeCount\n#include <mpi.h>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A represents a directed graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(std::vector<int> const& A, size_t N) {\nint count = 0;\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++count;\n      }\n    }\n  }\n  return count;\n}",
    "translation_function_name": "edgeCount"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {",
    "translation_prompt": "// Serial implementation of maxDegree\n// /* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// int maxDegree(std::vector<int> const& A, size_t N) {\n// int max_degree = 0;\n//   for (size_t i = 0; i < N; ++i) {\n//     int degree = 0;\n//     for (size_t j = 0; j < N; ++j) {\n//       if (A[i * N + j] == 1) {\n//         ++degree;\n//       }\n//     }\n//     if (degree > max_degree) {\n//       max_degree = degree;\n//     }\n//   }\n//   return max_degree;\n// }\n\n// MPI implementation of maxDegree\n#include <mpi.h>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The result is returned on rank 0.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(std::vector<int> const& A, size_t N) {\nint max_degree = 0;\n  for (size_t i = 0; i < N; ++i) {\n    int degree = 0;\n    for (size_t j = 0; j < N; ++j) {\n      if (A[i * N + j] == 1) {\n        ++degree;\n      }\n    }\n    if (degree > max_degree) {\n      max_degree = degree;\n    }\n  }\n  return max_degree;\n}",
    "translation_function_name": "maxDegree"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {",
    "translation_prompt": "// Serial implementation of shortestPathLength\n// /* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Example:\n// \n// \t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n//    output: 2\n// */\n// int shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n// std::vector<int> dist(N, INT_MAX);\n// \tstd::vector<int> prev(N, -1);\n// \tstd::queue<int> q;\n// \tq.push(source);\n// \tdist[source] = 0;\n// \twhile (!q.empty()) {\n// \t\tint u = q.front();\n// \t\tq.pop();\n// \t\tfor (int v = 0; v < N; ++v) {\n// \t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n// \t\t\t\tdist[v] = dist[u] + 1;\n// \t\t\t\tprev[v] = u;\n// \t\t\t\tq.push(v);\n// \t\t\t}\n// \t\t}\n// \t}\n// \tif (dist[dest] == INT_MAX) {\n// \t\treturn -1;\n// \t}\n// \tint path_len = 0;\n// \twhile (prev[dest]!= -1) {\n// \t\tpath_len++;\n// \t\tdest = prev[dest];\n// \t}\n// \treturn path_len;\n// }\n\n// MPI implementation of shortestPathLength\n#include <mpi.h>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. The shortest path length is returned on rank 0.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(std::vector<int> const& A, size_t N, int source, int dest) {\nstd::vector<int> dist(N, INT_MAX);\n\tstd::vector<int> prev(N, -1);\n\tstd::queue<int> q;\n\tq.push(source);\n\tdist[source] = 0;\n\twhile (!q.empty()) {\n\t\tint u = q.front();\n\t\tq.pop();\n\t\tfor (int v = 0; v < N; ++v) {\n\t\t\tif (A[u * N + v] == 1 && dist[v] == INT_MAX) {\n\t\t\t\tdist[v] = dist[u] + 1;\n\t\t\t\tprev[v] = u;\n\t\t\t\tq.push(v);\n\t\t\t}\n\t\t}\n\t}\n\tif (dist[dest] == INT_MAX) {\n\t\treturn -1;\n\t}\n\tint path_len = 0;\n\twhile (prev[dest]!= -1) {\n\t\tpath_len++;\n\t\tdest = prev[dest];\n\t}\n\treturn path_len;\n}",
    "translation_function_name": "shortestPathLength"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of smallestOdd\n// /* Return the value of the smallest odd number in the vector x.\n//    Examples:\n// \n//    input: [7, 9, 5, 2, 8, 16, 4, 1]\n//    output: 1\n// \n//    input: [8, 36, 7, 2, 11]\n//    output: 7\n// */\n// int smallestOdd(std::vector<int> const& x) {\n// int smallest = x[0];\n//   for (int i = 1; i < x.size(); i++) {\n//     if (x[i] % 2 == 1 && x[i] < smallest) {\n//       smallest = x[i];\n//     }\n//   }\n//   return smallest;\n// }\n\n// MPI implementation of smallestOdd\n#include <mpi.h>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the value of the smallest odd number in the vector x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(std::vector<int> const& x) {\nint smallest = x[0];\n  for (int i = 1; i < x.size(); i++) {\n    if (x[i] % 2 == 1 && x[i] < smallest) {\n      smallest = x[i];\n    }\n  }\n  return smallest;\n}",
    "translation_function_name": "smallestOdd"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "26_reduce_product_of_inverses",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of productWithInverses\n// /* Return the product of the vector x with every odd indexed element inverted.\n//    i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n//    Example:\n// \n//    input: [4, 2, 10, 4, 5]\n//    output: 25\n// */\n// double productWithInverses(std::vector<double> const& x) {\n// double result = 1;\n//   for (int i = 0; i < x.size(); i++) {\n//     if (i % 2 == 0) {\n//       result *= x[i];\n//     } else {\n//       result *= 1 / x[i];\n//     }\n//   }\n//   return result;\n// }\n\n// MPI implementation of productWithInverses\n#include <mpi.h>\n\n/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Use MPI to compute product in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the product on all ranks.\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the product of the vector x with every odd indexed element inverted.\n   i.e. x_0 * 1/x_1 * x_2 * 1/x_3 * x_4 ...\n   Example:\n\n   input: [4, 2, 10, 4, 5]\n   output: 25\n*/\ndouble productWithInverses(std::vector<double> const& x) {\ndouble result = 1;\n  for (int i = 0; i < x.size(); i++) {\n    if (i % 2 == 0) {\n      result *= x[i];\n    } else {\n      result *= 1 / x[i];\n    }\n  }\n  return result;\n}",
    "translation_function_name": "productWithInverses"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "29_reduce_sum_of_min_of_pairs",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {",
    "translation_prompt": "// Serial implementation of sumOfMinimumElements\n// /* Return the sum of the minimum value at each index of vectors x and y for all indices.\n//    i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n//    Example:\n// \n//    input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n//    output: 10\n// */\n// double sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n// double sum = 0;\n//   for (int i = 0; i < x.size(); i++) {\n//     sum += std::min(x[i], y[i]);\n//   }\n//   return sum;\n// }\n\n// MPI implementation of sumOfMinimumElements\n#include <mpi.h>\n\n/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Use MPI to sum in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the sum on all ranks.\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the sum of the minimum value at each index of vectors x and y for all indices.\n   i.e. sum = min(x_0, y_0) + min(x_1, y_1) + min(x_2, y_2) + ...\n   Example:\n\n   input: x=[3, 4, 0, 2, 3], y=[2, 5, 3, 1, 7]\n   output: 10\n*/\ndouble sumOfMinimumElements(std::vector<double> const& x, std::vector<double> const& y) {\ndouble sum = 0;\n  for (int i = 0; i < x.size(); i++) {\n    sum += std::min(x[i], y[i]);\n  }\n  return sum;\n}",
    "translation_function_name": "sumOfMinimumElements"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of average\n// /* Return the average of the vector x.\n//    Examples:\n// \t\t\n// \t input: [1, 8, 4, 5, 1]\n//    output: 3.8\n// \n//    input: [2, 2, 2, 3]\n//    output: 2.25\n// */\n// double average(std::vector<double> const& x) {\n// double sum = 0;\n// \tfor (double i : x) {\n// \t\tsum += i;\n// \t}\n// \treturn sum / x.size();\n// }\n\n// MPI implementation of average\n#include <mpi.h>\n\n/* Return the average of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the average on all ranks.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the average of the vector x.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(std::vector<double> const& x) {\ndouble sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\treturn sum / x.size();\n}",
    "translation_function_name": "average"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "25_reduce_xor",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {",
    "translation_prompt": "// Serial implementation of reduceLogicalXOR\n// /* Return the logical XOR reduction of the vector of bools x.\n//    Example:\n// \n//    input: [false, false, false, true]\n//    output: true\n// */\n// bool reduceLogicalXOR(std::vector<bool> const& x) {\n// bool result = false;\n//   for (auto const& i : x) {\n//     result ^= i;\n//   }\n//   return result;\n// }\n\n// MPI implementation of reduceLogicalXOR\n#include <mpi.h>\n\n/* Return the logical XOR reduction of the vector of bools x.\n   Use MPI to reduce in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on all ranks.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the logical XOR reduction of the vector of bools x.\n   Example:\n\n   input: [false, false, false, true]\n   output: true\n*/\nbool reduceLogicalXOR(std::vector<bool> const& x) {\nbool result = false;\n  for (auto const& i : x) {\n    result ^= i;\n  }\n  return result;\n}",
    "translation_function_name": "reduceLogicalXOR"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "32_scan_sum_of_prefix_sum_array",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of sumOfPrefixSum\n// /* Compute the prefix sum array of the vector x and return its sum.\n//    Example:\n// \n//    input: [-7, 2, 1, 9, 4, 8]\n//    output: 15\n// */\n// double sumOfPrefixSum(std::vector<double> const& x) {\n// double sum = 0;\n//     std::vector<double> prefix_sum(x.size());\n// \n//     for (size_t i = 0; i < x.size(); ++i) {\n//         if (i == 0) {\n//             prefix_sum[i] = x[i];\n//         } else {\n//             prefix_sum[i] = prefix_sum[i - 1] + x[i];\n//         }\n//         sum += prefix_sum[i];\n//     }\n// \n//     return sum;\n// }\n\n// MPI implementation of sumOfPrefixSum\n#include <mpi.h>\n\n/* Compute the prefix sum array of the vector x and return its sum.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the prefix sum array of the vector x and return its sum.\n   Example:\n\n   input: [-7, 2, 1, 9, 4, 8]\n   output: 15\n*/\ndouble sumOfPrefixSum(std::vector<double> const& x) {\ndouble sum = 0;\n    std::vector<double> prefix_sum(x.size());\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (i == 0) {\n            prefix_sum[i] = x[i];\n        } else {\n            prefix_sum[i] = prefix_sum[i - 1] + x[i];\n        }\n        sum += prefix_sum[i];\n    }\n\n    return sum;\n}",
    "translation_function_name": "sumOfPrefixSum"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {",
    "translation_prompt": "// Serial implementation of partialMinimums\n// /* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n//    Examples:\n// \n//    input: [8, 6, -1, 7, 3, 4, 4]\n//    output: [8, 6, -1, -1, -1, -1, -1]\n// \n//    input: [5, 4, 6, 4, 3, 6, 1, 1]\n//    output: [5, 4, 4, 4, 3, 3, 1, 1]\n// */\n// void partialMinimums(std::vector<float> &x) {\n// float min = x[0];\n//     for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < min) {\n//             min = x[i];\n//         }\n//         x[i] = min;\n//     }\n// }\n\n// MPI implementation of partialMinimums\n#include <mpi.h>\n\n/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(std::vector<float> &x) {\nfloat min = x[0];\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        x[i] = min;\n    }\n}",
    "translation_function_name": "partialMinimums"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of prefixSum\n// /* Compute the prefix sum of the vector x into output.\n//    Example:\n//    \n//    input: [1, 7, 4, 6, 6, 2]\n//    output: [1, 8, 12, 18, 24, 26]\n// */\n// void prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n// output.resize(x.size());\n//   output[0] = x[0];\n//   for (int i = 1; i < x.size(); ++i) {\n//     output[i] = output[i-1] + x[i];\n//   }\n// }\n\n// MPI implementation of prefixSum\n#include <mpi.h>\n\n/* Compute the prefix sum of the vector x into output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in output on rank 0.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the prefix sum of the vector x into output.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(std::vector<double> const& x, std::vector<double> &output) {\noutput.resize(x.size());\n  output[0] = x[0];\n  for (int i = 1; i < x.size(); ++i) {\n    output[i] = output[i-1] + x[i];\n  }\n}",
    "translation_function_name": "prefixSum"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of negateOddsAndHalveEvens\n// /* In the vector x negate the odd values and divide the even values by 2.\n//    Example:\n// \n//    input: [16, 11, 12, 14, 1, 0, 5]\n//    output: [8, -11, 6, 7, -1, 0, -5]\n// */\n// void negateOddsAndHalveEvens(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] % 2 == 0) {\n//             x[i] /= 2;\n//         } else {\n//             x[i] = -x[i];\n//         }\n//     }\n// }\n\n// MPI implementation of negateOddsAndHalveEvens\n#include <mpi.h>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* In the vector x negate the odd values and divide the even values by 2.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] % 2 == 0) {\n            x[i] /= 2;\n        } else {\n            x[i] = -x[i];\n        }\n    }\n}",
    "translation_function_name": "negateOddsAndHalveEvens"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {",
    "translation_prompt": "// Serial implementation of mapPowersOfTwo\n// bool isPowerOfTwo(int x) {\n// \treturn (x > 0) && !(x & (x - 1));\n// }\n// \n// /* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n//    Example:\n// \n//    input: [8, 0, 9, 7, 15, 64, 3]\n//    output: [true, false, false, false, false, true, false]\n// */\n// void mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n// for (int i = 0; i < x.size(); i++) {\n// \t\tmask[i] = isPowerOfTwo(x[i]);\n// \t}\n// }\n\n// MPI implementation of mapPowersOfTwo\n#include <mpi.h>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored in mask on rank 0.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(std::vector<int> const& x, std::vector<bool> &mask) {\nfor (int i = 0; i < x.size(); i++) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
    "translation_function_name": "mapPowersOfTwo"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of oneMinusInverse\n// /* Replace every element of the vector x with 1-1/x.\n//    Example:\n// \n//    input: [2, 4, 1, 12, -2]\n//    output: [0.5, 0.75, 0, 0.91666666, 1.5]\n// */\n// void oneMinusInverse(std::vector<double> &x) {\n// for (auto &e : x) {\n//     e = 1.0 - 1.0 / e;\n//   }\n// }\n\n// MPI implementation of oneMinusInverse\n#include <mpi.h>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Replace every element of the vector x with 1-1/x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(std::vector<double> &x) {\nfor (auto &e : x) {\n    e = 1.0 - 1.0 / e;\n  }\n}",
    "translation_function_name": "oneMinusInverse"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {",
    "translation_prompt": "// Serial implementation of relu\n// /* Compute the ReLU function on every element of x. Elements less than zero become zero,\n//    while elements greater than zero stay the same.\n//    Example:\n// \n//    input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n//    output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n// */\n// void relu(std::vector<double> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         if (x[i] < 0) {\n//             x[i] = 0;\n//         }\n//     }\n// }\n\n// MPI implementation of relu\n#include <mpi.h>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(std::vector<double> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        if (x[i] < 0) {\n            x[i] = 0;\n        }\n    }\n}",
    "translation_function_name": "relu"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of squareEach\n// /* Replace every element of x with the square of its value.\n//    Example:\n// \n//    input: [5, 1, 2, -4, 8]\n//    output: [25, 1, 4, 16, 64]\n// */\n// void squareEach(std::vector<int> &x) {\n// for (int i = 0; i < x.size(); i++) {\n//         x[i] = x[i] * x[i];\n//     }\n// }\n\n// MPI implementation of squareEach\n#include <mpi.h>\n\n/* Replace every element of x with the square of its value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Replace every element of x with the square of its value.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(std::vector<int> &x) {\nfor (int i = 0; i < x.size(); i++) {\n        x[i] = x[i] * x[i];\n    }\n}",
    "translation_function_name": "squareEach"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of spmv\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n//    and A is a sparse matrix stored in COO format.\n//    A has dimensions MxN, x has N values, and y has M values.\n//    Example:\n// \n//    input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n//    output: y=[2, 3]\n// */\n// void spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//       y[i] *= beta;\n//    }\n// \n//    for (size_t i = 0; i < A.size(); ++i) {\n//       y[A[i].row] += alpha * A[i].value * x[A[i].column];\n//    }\n// }\n\n// MPI implementation of spmv\n#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use MPI to parallelize. Assume MPI has already been initialized.\n   Every rank has a complete copy of A, x, and y. Store the result in y on rank 0.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, std::vector<COOElement> const& A, std::vector<double> const& x, double beta, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n      y[i] *= beta;\n   }\n\n   for (size_t i = 0; i < A.size(); ++i) {\n      y[A[i].row] += alpha * A[i].value * x[A[i].column];\n   }\n}",
    "translation_function_name": "spmv"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of spmm\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n//    X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n//    Example:\n// \n//    input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n//    output: Y=[{-1,-4}, {1,0}]\n// */\n// void spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n// std::vector<double> Ytemp(M*N, 0.0);\n//    for (auto const& a : A) {\n//       for (auto const& x : X) {\n//          if (a.column == x.row) {\n//             Ytemp[a.row*N + x.column] += a.value * x.value;\n//          }\n//       }\n//    }\n//    Y = std::move(Ytemp);\n// }\n\n// MPI implementation of spmm\n#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and X. Store the result in Y on rank 0.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix in row-major.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(std::vector<COOElement> const& A, std::vector<COOElement> const& X, std::vector<double> &Y, size_t M, size_t K, size_t N) {\nstd::vector<double> Ytemp(M*N, 0.0);\n   for (auto const& a : A) {\n      for (auto const& x : X) {\n         if (a.column == x.row) {\n            Ytemp[a.row*N + x.column] += a.value * x.value;\n         }\n      }\n   }\n   Y = std::move(Ytemp);\n}",
    "translation_function_name": "spmm"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "49_sparse_la_sparse_lu_decomp",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    A is a sparse NxN matrix stored in COO format.\n//    Example:\n// \n//    input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n//    output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n// */\n// void luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n// \n// // Assuming A is sorted by row and then by column\n// L.clear();\n// U.clear();\n// \n// // Initialize L and U with zeros\n// L.resize(N * N, 0);\n// U.resize(N * N, 0);\n// \n// // Create a map for quick access to A's elements\n// std::map<std::pair<size_t, size_t>, double> A_map;\n// for (const auto& elem : A) {\n//     A_map[{elem.row, elem.column}] = elem.value;\n// }\n// \n// // LU decomposition algorithm\n// for (size_t i = 0; i < N; ++i) {\n//     // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n//     for (size_t k = i; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[i * N + j] * U[j * N + k];\n//         }\n//         U[i * N + k] = A_map[{i, k}] - sum;\n//     }\n// \n//     // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n//     for (size_t k = i + 1; k < N; ++k) {\n//         double sum = 0.0;\n//         for (size_t j = 0; j < i; ++j) {\n//             sum += L[k * N + j] * U[j * N + i];\n//         }\n//         L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n//     }\n// \n//     // L(i, i) = 1\n//     L[i * N + i] = 1.0;\n// }\n// }\n\n// MPI implementation of luFactorize\n#include <mpi.h>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in L and U on rank 0.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Factorize the sparse matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   A is a sparse NxN matrix stored in COO format.\n   Example:\n\n   input: A=[{0,0,4}, {0,1,3}, {1,0,6}, {1,1,3}]\n   output: L=[{0,0,1},{1,0,1.5}, {1,1,1}] U=[{0,0,4}, {0,1,3}, {1,1,-1.5}]\n*/\nvoid luFactorize(std::vector<COOElement> const& A, std::vector<double> &L, std::vector<double> &U, size_t N) {\n\n// Assuming A is sorted by row and then by column\nL.clear();\nU.clear();\n\n// Initialize L and U with zeros\nL.resize(N * N, 0);\nU.resize(N * N, 0);\n\n// Create a map for quick access to A's elements\nstd::map<std::pair<size_t, size_t>, double> A_map;\nfor (const auto& elem : A) {\n    A_map[{elem.row, elem.column}] = elem.value;\n}\n\n// LU decomposition algorithm\nfor (size_t i = 0; i < N; ++i) {\n    // U(i, k) = A(i, k) - sum(L(i, j) * U(j, k))\n    for (size_t k = i; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[i * N + j] * U[j * N + k];\n        }\n        U[i * N + k] = A_map[{i, k}] - sum;\n    }\n\n    // L(k, i) = (A(k, i) - sum(L(k, j) * U(j, i))) / U(i, i)\n    for (size_t k = i + 1; k < N; ++k) {\n        double sum = 0.0;\n        for (size_t j = 0; j < i; ++j) {\n            sum += L[k * N + j] * U[j * N + i];\n        }\n        L[k * N + i] = (A_map[{k, i}] - sum) / U[i * N + i];\n    }\n\n    // L(i, i) = 1\n    L[i * N + i] = 1.0;\n}\n}",
    "translation_function_name": "luFactorize"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
    "translation_prompt": "// Serial implementation of firstLetterCounts\n// /* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n//    Assume all strings are in lower case. Store the output in `bins` array.\n//    Example:\n// \n//    input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n//    output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n// */\n// void firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n// for (auto const& str : s) {\n//         bins[str[0] - 'a']++;\n//     }\n// }\n\n// MPI implementation of firstLetterCounts\n#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\nfor (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
    "translation_function_name": "firstLetterCounts"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
    "translation_prompt": "// Serial implementation of binsBy10Count\n// /* Vector x contains values between 0 and 100, inclusive. Count the number of\n//    values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n//    Example:\n// \n//    input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n//    output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n// */\n// void binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n// for (auto const& value : x) {\n//     if (value < 10) {\n//       bins[0]++;\n//     } else if (value < 20) {\n//       bins[1]++;\n//     } else if (value < 30) {\n//       bins[2]++;\n//     } else if (value < 40) {\n//       bins[3]++;\n//     } else if (value < 50) {\n//       bins[4]++;\n//     } else if (value < 60) {\n//       bins[5]++;\n//     } else if (value < 70) {\n//       bins[6]++;\n//     } else if (value < 80) {\n//       bins[7]++;\n//     } else if (value < 90) {\n//       bins[8]++;\n//     } else {\n//       bins[9]++;\n//     }\n//   }\n// }\n\n// MPI implementation of binsBy10Count\n#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {\nfor (auto const& value : x) {\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
    "translation_function_name": "binsBy10Count"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuadrants\n// struct Point {\n//    double x, y;\n// };\n// \n// /* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n//    Store the counts in `bins`.\n//    Example:\n// \n//    input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n//    output: [3, 1, 0, 2]\n// */\n// void countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n// for (auto const& point : points) {\n//       if (point.x > 0 && point.y > 0) {\n//          bins[0]++;\n//       } else if (point.x < 0 && point.y > 0) {\n//          bins[1]++;\n//       } else if (point.x < 0 && point.y < 0) {\n//          bins[2]++;\n//       } else {\n//          bins[3]++;\n//       }\n//    }\n// }\n\n// MPI implementation of countQuadrants\n#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {\nfor (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
    "translation_function_name": "countQuadrants"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
    "translation_prompt": "// Serial implementation of countQuartiles\n// /* Count the number of doubles in the vector x that have a fractional part \n//    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n//    Examples:\n// \n//    input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n//    output: [2, 1, 2, 2]\n// \n//    input: [1.9, 0.2, 0.6, 10.1, 7.4]\n//    output: [2, 1, 1, 1]\n// */\n// void countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n// for (double num : x) {\n//         double fractional_part = num - std::floor(num);\n//         if (fractional_part < 0.25) {\n//             bins[0]++;\n//         } else if (fractional_part < 0.5) {\n//             bins[1]++;\n//         } else if (fractional_part < 0.75) {\n//             bins[2]++;\n//         } else {\n//             bins[3]++;\n//         }\n//     }\n// }\n\n// MPI implementation of countQuartiles\n#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {\nfor (double num : x) {\n        double fractional_part = num - std::floor(num);\n        if (fractional_part < 0.25) {\n            bins[0]++;\n        } else if (fractional_part < 0.5) {\n            bins[1]++;\n        } else if (fractional_part < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
    "translation_function_name": "countQuartiles"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
    "translation_prompt": "// Serial implementation of pixelCounts\n// /* Count the number of pixels in image with each grayscale intensity.\n//    The vector `image` is a grayscale image with values 0-255.\n//    Store the results in `bins`.\n//    Example:\n//    \n//    input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n//    output: [0, 0, 2, 0, 1, ...]\n// */\n//  void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n// for (auto const& pixel : image) {\n//         bins[pixel]++;\n//     }\n// }\n\n// MPI implementation of pixelCounts\n#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {\nfor (auto const& pixel : image) {\n        bins[pixel]++;\n    }\n}",
    "translation_function_name": "pixelCounts"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "38_search_find_the_first_even_number",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {",
    "translation_prompt": "// Serial implementation of findFirstEven\n// /* Return the index of the first even number in the vector x.\n//    Examples:\n// \n//    input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n//    output: 6\n// \n//    input: [3, 8, 9, 9, 3, 4, 8, 6]\n//    output: 1\n// */\n// size_t findFirstEven(std::vector<int> const& x) {\n// for (size_t i = 0; i < x.size(); ++i) {\n//     if (x[i] % 2 == 0) {\n//       return i;\n//     }\n//   }\n//   return x.size();\n// }\n\n// MPI implementation of findFirstEven\n#include <mpi.h>\n\n/* Return the index of the first even number in the vector x.\n   Use MPI to parallelize the search. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the index of the first even number in the vector x.\n   Examples:\n\n   input: [7, 3, 9, 5, 5, 7, 2, 9, 12, 11]\n   output: 6\n\n   input: [3, 8, 9, 9, 3, 4, 8, 6]\n   output: 1\n*/\nsize_t findFirstEven(std::vector<int> const& x) {\nfor (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] % 2 == 0) {\n      return i;\n    }\n  }\n  return x.size();\n}",
    "translation_function_name": "findFirstEven"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {",
    "translation_prompt": "// Serial implementation of contains\n// /* Return true if the vector x contains the value `target`. Return false otherwise.\n//    Examples:\n// \n//    input: x=[1, 8, 2, 6, 4, 6], target=3\n//    output: false\n//    \n//    input: x=[1, 8, 2, 6, 4, 6], target=8\n//    output: true\n// */\n// bool contains(std::vector<int> const& x, int target) {\n// return std::find(x.begin(), x.end(), target)!= x.end();\n// }\n\n// MPI implementation of contains\n#include <mpi.h>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(std::vector<int> const& x, int target) {\nreturn std::find(x.begin(), x.end(), target)!= x.end();\n}",
    "translation_function_name": "contains"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {",
    "translation_prompt": "// Serial implementation of xorContains\n// /* Return true if `val` is only in one of vectors x or y.\n//    Return false if it is in both or neither.\n//    Examples:\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n//    output: true\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n//    output: false\n// */\n// bool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n// return std::find(x.begin(), x.end(), val)!= x.end() ^\n//          std::find(y.begin(), y.end(), val)!= y.end();\n// }\n\n// MPI implementation of xorContains\n#include <mpi.h>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Return the result on rank 0.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(std::vector<int> const& x, std::vector<int> const& y, int val) {\nreturn std::find(x.begin(), x.end(), val)!= x.end() ^\n         std::find(y.begin(), y.end(), val)!= y.end();\n}",
    "translation_function_name": "xorContains"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {",
    "translation_prompt": "// Serial implementation of findLastShortBook\n// struct Book {\n//    std::string title;\n//    int pages;\n// };\n// \n// /* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n// \t Example:\n// \n//    input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n//    output: 2\n// */\n// size_t findLastShortBook(std::vector<Book> const& books) {\n// size_t last_short_book_index = 0;\n//    for (size_t i = 0; i < books.size(); ++i) {\n//       if (books[i].pages < 100) {\n//          last_short_book_index = i;\n//       }\n//    }\n//    return last_short_book_index;\n// }\n\n// MPI implementation of findLastShortBook\n#include <mpi.h>\n\nstruct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use MPI to search in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of books. Return the result on rank 0.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Book {\n   std::string title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(std::vector<Book> const& books) {\nsize_t last_short_book_index = 0;\n   for (size_t i = 0; i < books.size(); ++i) {\n      if (books[i].pages < 100) {\n         last_short_book_index = i;\n      }\n   }\n   return last_short_book_index;\n}",
    "translation_function_name": "findLastShortBook"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "37_search_find_the_closest_number_to_pi",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {",
    "translation_prompt": "// Serial implementation of findClosestToPi\n// /* Return the index of the value in the vector x that is closest to the math constant PI.\n//    Use M_PI for the value of PI.\n//    Example:\n// \n//    input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n//    output: 1\n// */\n// size_t findClosestToPi(std::vector<double> const& x) {\n// double min_distance = std::numeric_limits<double>::max();\n//   size_t min_index = 0;\n//   for (size_t i = 0; i < x.size(); ++i) {\n//     double distance = std::abs(x[i] - M_PI);\n//     if (distance < min_distance) {\n//       min_distance = distance;\n//       min_index = i;\n//     }\n//   }\n//   return min_index;\n// }\n\n// MPI implementation of findClosestToPi\n#include <mpi.h>\n\n/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Use MPI to search in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Return the index of the value in the vector x that is closest to the math constant PI.\n   Use M_PI for the value of PI.\n   Example:\n\n   input: [9.18, 3.05, 7.24, 11.3, -166.49, 2.1]\n   output: 1\n*/\nsize_t findClosestToPi(std::vector<double> const& x) {\ndouble min_distance = std::numeric_limits<double>::max();\n  size_t min_index = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    double distance = std::abs(x[i] - M_PI);\n    if (distance < min_distance) {\n      min_distance = distance;\n      min_index = i;\n    }\n  }\n  return min_index;\n}",
    "translation_function_name": "findClosestToPi"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "08_fft_split_fft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n// int n = x.size();\n//     if (n == 0) return;\n//     if (n == 1) {\n//         r[0] = x[0].real();\n//         i[0] = x[0].imag();\n//         return;\n//     }\n//     std::vector<std::complex<double>> x_even(n / 2);\n//     std::vector<std::complex<double>> x_odd(n / 2);\n//     for (int i = 0; i < n / 2; i++) {\n//         x_even[i] = x[2 * i];\n//         x_odd[i] = x[2 * i + 1];\n//     }\n//     std::vector<double> r_even(n / 2);\n//     std::vector<double> i_even(n / 2);\n//     std::vector<double> r_odd(n / 2);\n//     std::vector<double> i_odd(n / 2);\n//     fft(x_even, r_even, i_even);\n//     fft(x_odd, r_odd, i_odd);\n//     for (int k = 0; k < n / 2; k++) {\n//         std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n//         r[k] = r_even[k] + z.real();\n//         i[k] = i_even[k] + z.imag();\n//         r[k + n / 2] = r_even[k] - z.real();\n//         i[k + n / 2] = i_even[k] - z.imag();\n//     }\n// }\n\n// MPI implementation of fft\n#include <mpi.h>\n\n/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the fourier transform of x. Store real part of results in r and imaginary in i.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: r: [4, 1, 0, 1, 0, 1, 0, 1] i: [0, -2.41421, 0, -0.414214, 0, 0.414214, 0, 2.41421]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<double> &r, std::vector<double> &i) {\nint n = x.size();\n    if (n == 0) return;\n    if (n == 1) {\n        r[0] = x[0].real();\n        i[0] = x[0].imag();\n        return;\n    }\n    std::vector<std::complex<double>> x_even(n / 2);\n    std::vector<std::complex<double>> x_odd(n / 2);\n    for (int i = 0; i < n / 2; i++) {\n        x_even[i] = x[2 * i];\n        x_odd[i] = x[2 * i + 1];\n    }\n    std::vector<double> r_even(n / 2);\n    std::vector<double> i_even(n / 2);\n    std::vector<double> r_odd(n / 2);\n    std::vector<double> i_odd(n / 2);\n    fft(x_even, r_even, i_even);\n    fft(x_odd, r_odd, i_odd);\n    for (int k = 0; k < n / 2; k++) {\n        std::complex<double> z = std::polar(1.0, -2 * M_PI * k / n) * std::complex<double>(r_odd[k], i_odd[k]);\n        r[k] = r_even[k] + z.real();\n        i[k] = i_even[k] + z.imag();\n        r[k + n / 2] = r_even[k] - z.real();\n        i[k + n / 2] = i_even[k] - z.imag();\n    }\n}",
    "translation_function_name": "fft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "07_fft_fft_conjugate",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of fftConjugate\n// /* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fftConjugate(std::vector<std::complex<double>> &x) {\n// int n = x.size();\n//     int m = 0;\n//     for (int i = 0; i < n; i++) {\n//         int j = i;\n//         for (int k = 0; k < m; k++) {\n//             j = j ^ (1 << k);\n//         }\n//         if (i < j) {\n//             std::complex<double> temp = x[i];\n//             x[i] = x[j];\n//             x[j] = temp;\n//         }\n//         m = std::max(m, __builtin_ctz(j));\n//     }\n//     for (int i = 1; i < n; i *= 2) {\n//         std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n//         for (int j = 0; j < n; j += 2 * i) {\n//             for (int k = 0; k < i; k++) {\n//                 std::complex<double> u = x[j + k];\n//                 std::complex<double> t = w * x[j + k + i];\n//                 x[j + k] = u + t;\n//                 x[j + k + i] = u - t;\n//             }\n//             w = w * w;\n//         }\n//     }\n// }\n\n// MPI implementation of fftConjugate\n#include <mpi.h>\n\n/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the fourier transform of x in-place. Return the imaginary conjugate of each value.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.41421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fftConjugate(std::vector<std::complex<double>> &x) {\nint n = x.size();\n    int m = 0;\n    for (int i = 0; i < n; i++) {\n        int j = i;\n        for (int k = 0; k < m; k++) {\n            j = j ^ (1 << k);\n        }\n        if (i < j) {\n            std::complex<double> temp = x[i];\n            x[i] = x[j];\n            x[j] = temp;\n        }\n        m = std::max(m, __builtin_ctz(j));\n    }\n    for (int i = 1; i < n; i *= 2) {\n        std::complex<double> w = std::polar(1.0, -2 * M_PI / i);\n        for (int j = 0; j < n; j += 2 * i) {\n            for (int k = 0; k < i; k++) {\n                std::complex<double> u = x[j + k];\n                std::complex<double> t = w * x[j + k + i];\n                x[j + k] = u + t;\n                x[j + k + i] = u - t;\n            }\n            w = w * w;\n        }\n    }\n}",
    "translation_function_name": "fftConjugate"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "05_fft_inverse_fft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of ifft\n// /* forward declare fft. computes fourier transform in-place */\n// void fft(std::vector<std::complex<double>> &x);\n// \n// /* Compute the inverse fourier transform of x in-place.\n//    Example:\n//    \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n// */\n// void ifft(std::vector<std::complex<double>> &x) {\n// \n// // Take conjugate of the complex numbers\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Apply fft\n// fft(x);\n// \n// // Take conjugate again\n// for (auto &elem : x) {\n//     elem = std::conj(elem);\n// }\n// \n// // Scale the numbers\n// for (auto &elem : x) {\n//     elem /= static_cast<double>(x.size());\n// }\n// }\n\n// MPI implementation of ifft\n#include <mpi.h>\n\n/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The final result is stored on rank 0.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* forward declare fft. computes fourier transform in-place */\nvoid fft(std::vector<std::complex<double>> &x);\n\n/* Compute the inverse fourier transform of x in-place.\n   Example:\n   \n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{0.5,0}, {0.125,0.301777}, {0,-0}, {0.125,0.0517767}, {0,-0}, {0.125,-0.0517767}, {0,-0}, {0.125,-0.301777}]\n*/\nvoid ifft(std::vector<std::complex<double>> &x) {\n\n// Take conjugate of the complex numbers\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Apply fft\nfft(x);\n\n// Take conjugate again\nfor (auto &elem : x) {\n    elem = std::conj(elem);\n}\n\n// Scale the numbers\nfor (auto &elem : x) {\n    elem /= static_cast<double>(x.size());\n}\n}",
    "translation_function_name": "ifft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of dft\n// /* Compute the discrete fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1, 4, 9, 16]\n//    output: [30+0i, -8-12i, -10-0i, -8+12i]\n// */\n// void dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// // TODO: implement this function\n//     output.resize(x.size());\n//     for (int i = 0; i < x.size(); i++) {\n//         output[i] = 0;\n//         for (int j = 0; j < x.size(); j++) {\n//             output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n//         }\n//     }\n// }\n\n// MPI implementation of dft\n#include <mpi.h>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(std::vector<double> const& x, std::vector<std::complex<double>> &output) {\n// TODO: implement this function\n    output.resize(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        output[i] = 0;\n        for (int j = 0; j < x.size(); j++) {\n            output[i] += x[j] * std::exp(std::complex<double>(0, -2 * M_PI * i * j / x.size()));\n        }\n    }\n}",
    "translation_function_name": "dft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {",
    "translation_prompt": "// Serial implementation of fft\n// /* Compute the fourier transform of x. Store the result in output.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// void fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n// int n = x.size();\n//     output.resize(n);\n// \n//     // base case\n//     if (n == 1) {\n//         output[0] = x[0];\n//         return;\n//     }\n// \n//     // radix 2 Cooley-Tukey FFT\n//     // decompose x into even and odd parts\n//     std::vector<std::complex<double>> x_even(n/2);\n//     std::vector<std::complex<double>> x_odd(n/2);\n//     for (int k = 0; k < n/2; k++) {\n//         x_even[k] = x[2*k];\n//         x_odd[k] = x[2*k + 1];\n//     }\n// \n//     // recursively compute FFTs of even and odd parts\n//     std::vector<std::complex<double>> y_even(n/2);\n//     std::vector<std::complex<double>> y_odd(n/2);\n//     fft(x_even, y_even);\n//     fft(x_odd, y_odd);\n// \n//     // combine results\n//     for (int k = 0; k < n/2; k++) {\n//         double kth = -2 * M_PI * k / n;\n//         output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//         output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n//     }\n// }\n\n// MPI implementation of fft\n#include <mpi.h>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute the fourier transform of x. Store the result in output.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(std::vector<std::complex<double>> const& x, std::vector<std::complex<double>> &output) {\nint n = x.size();\n    output.resize(n);\n\n    // base case\n    if (n == 1) {\n        output[0] = x[0];\n        return;\n    }\n\n    // radix 2 Cooley-Tukey FFT\n    // decompose x into even and odd parts\n    std::vector<std::complex<double>> x_even(n/2);\n    std::vector<std::complex<double>> x_odd(n/2);\n    for (int k = 0; k < n/2; k++) {\n        x_even[k] = x[2*k];\n        x_odd[k] = x[2*k + 1];\n    }\n\n    // recursively compute FFTs of even and odd parts\n    std::vector<std::complex<double>> y_even(n/2);\n    std::vector<std::complex<double>> y_odd(n/2);\n    fft(x_even, y_even);\n    fft(x_odd, y_odd);\n\n    // combine results\n    for (int k = 0; k < n/2; k++) {\n        double kth = -2 * M_PI * k / n;\n        output[k] = y_even[k] + std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n        output[k + n/2] = y_even[k] - std::complex<double>(cos(kth), sin(kth)) * y_odd[k];\n    }\n}",
    "translation_function_name": "fft"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {",
    "translation_prompt": "// Serial implementation of axpy\n// /* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n//    Example:\n//    \n//    input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n//    output: z=[2, -6, 5, 17]\n// */\n// void axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n// for (int i = 0; i < x.size(); ++i) {\n//     z[i] = alpha*x[i] + y[i];\n//   }\n// }\n\n// MPI implementation of axpy\n#include <mpi.h>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x and y. Store the result in z on rank 0.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, std::vector<double> const& x, std::vector<double> const& y, std::vector<double> &z) {\nfor (int i = 0; i < x.size(); ++i) {\n    z[i] = alpha*x[i] + y[i];\n  }\n}",
    "translation_function_name": "axpy"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "01_dense_la_solve",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {",
    "translation_prompt": "// Serial implementation of solveLinearSystem\n// /* Solve the linear system Ax=b for x.\n//    A is an NxN matrix in row-major. x and b have N elements.\n//    Example:\n//    \n//    input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n//    output: x=[3, 1, 2]\n// */\n// void solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// // check if the input is valid\n//     if (A.size()!= N * N) {\n//         throw std::invalid_argument(\"A must be a NxN matrix\");\n//     }\n//     if (b.size()!= N) {\n//         throw std::invalid_argument(\"b must have N elements\");\n//     }\n//     // create a copy of A\n//     std::vector<double> A_copy(A);\n//     // create a copy of b\n//     std::vector<double> b_copy(b);\n//     // perform Gaussian elimination\n//     for (size_t i = 0; i < N; i++) {\n//         // find the pivot row and swap it with the current row\n//         size_t pivot_row = i;\n//         for (size_t j = i + 1; j < N; j++) {\n//             if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n//                 pivot_row = j;\n//             }\n//         }\n//         if (pivot_row!= i) {\n//             // swap the current row with the pivot row\n//             for (size_t j = 0; j < N; j++) {\n//                 std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n//             }\n//             std::swap(b_copy[i], b_copy[pivot_row]);\n//         }\n//         // eliminate the current row\n//         for (size_t j = i + 1; j < N; j++) {\n//             double factor = A_copy[j * N + i] / A_copy[i * N + i];\n//             for (size_t k = 0; k < N; k++) {\n//                 A_copy[j * N + k] -= factor * A_copy[i * N + k];\n//             }\n//             b_copy[j] -= factor * b_copy[i];\n//         }\n//     }\n//     // back substitution\n//     x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n//     for (size_t i = N - 2; i < N; i--) {\n//         double sum = 0;\n//         for (size_t j = i + 1; j < N; j++) {\n//             sum += A_copy[i * N + j] * x[j];\n//         }\n//         x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n//     }\n// }\n\n// MPI implementation of solveLinearSystem\n#include <mpi.h>\n\n/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and b. Store the result in x on rank 0.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Solve the linear system Ax=b for x.\n   A is an NxN matrix in row-major. x and b have N elements.\n   Example:\n   \n   input: A=[[1,4,2], [1,2,3], [2,1,3]] b=[11, 11, 13]\n   output: x=[3, 1, 2]\n*/\nvoid solveLinearSystem(std::vector<double> const& A, std::vector<double> const& b, std::vector<double> &x, size_t N) {\n// check if the input is valid\n    if (A.size()!= N * N) {\n        throw std::invalid_argument(\"A must be a NxN matrix\");\n    }\n    if (b.size()!= N) {\n        throw std::invalid_argument(\"b must have N elements\");\n    }\n    // create a copy of A\n    std::vector<double> A_copy(A);\n    // create a copy of b\n    std::vector<double> b_copy(b);\n    // perform Gaussian elimination\n    for (size_t i = 0; i < N; i++) {\n        // find the pivot row and swap it with the current row\n        size_t pivot_row = i;\n        for (size_t j = i + 1; j < N; j++) {\n            if (std::abs(A_copy[j * N + i]) > std::abs(A_copy[pivot_row * N + i])) {\n                pivot_row = j;\n            }\n        }\n        if (pivot_row!= i) {\n            // swap the current row with the pivot row\n            for (size_t j = 0; j < N; j++) {\n                std::swap(A_copy[i * N + j], A_copy[pivot_row * N + j]);\n            }\n            std::swap(b_copy[i], b_copy[pivot_row]);\n        }\n        // eliminate the current row\n        for (size_t j = i + 1; j < N; j++) {\n            double factor = A_copy[j * N + i] / A_copy[i * N + i];\n            for (size_t k = 0; k < N; k++) {\n                A_copy[j * N + k] -= factor * A_copy[i * N + k];\n            }\n            b_copy[j] -= factor * b_copy[i];\n        }\n    }\n    // back substitution\n    x[N - 1] = b_copy[N - 1] / A_copy[(N - 1) * N + (N - 1)];\n    for (size_t i = N - 2; i < N; i--) {\n        double sum = 0;\n        for (size_t j = i + 1; j < N; j++) {\n            sum += A_copy[i * N + j] * x[j];\n        }\n        x[i] = (b_copy[i] - sum) / A_copy[i * N + i];\n    }\n}",
    "translation_function_name": "solveLinearSystem"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// Serial implementation of gemm\n// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n//    output: C=[[9, 5], [4, 2]]\n// */\n// void gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             double sum = 0;\n//             for (size_t k = 0; k < K; ++k) {\n//                 sum += A[i * K + k] * B[k * N + j];\n//             }\n//             C[i * N + j] = sum;\n//         }\n//     }\n// }\n\n// MPI implementation of gemm\n#include <mpi.h>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and B. Store the result in C on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(std::vector<double> const& A, std::vector<double> const& B, std::vector<double> &C, size_t M, size_t K, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            double sum = 0;\n            for (size_t k = 0; k < K; ++k) {\n                sum += A[i * K + k] * B[k * N + j];\n            }\n            C[i * N + j] = sum;\n        }\n    }\n}",
    "translation_function_name": "gemm"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {",
    "translation_prompt": "// Serial implementation of gemv\n// /* Multiply the matrix A by the vector x. Store the results in the vector y.\n//    A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n//    output: y=[1, -3]\n// */\n// void gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n// for (size_t i = 0; i < M; ++i) {\n//         double sum = 0;\n//         for (size_t j = 0; j < N; ++j) {\n//             sum += A[i * N + j] * x[j];\n//         }\n//         y[i] = sum;\n//     }\n// }\n\n// MPI implementation of gemv\n#include <mpi.h>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A and x. Store the result in y on rank 0.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(std::vector<double> const& A, std::vector<double> const& x, std::vector<double> &y, size_t M, size_t N) {\nfor (size_t i = 0; i < M; ++i) {\n        double sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            sum += A[i * N + j] * x[j];\n        }\n        y[i] = sum;\n    }\n}",
    "translation_function_name": "gemv"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "00_dense_la_lu_decomp",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {",
    "translation_prompt": "// Serial implementation of luFactorize\n// /* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n//    Store the results for L and U into the original matrix A. \n//    A is an NxN matrix stored in row-major.\n//    Example:\n// \n//    input: [[4, 3], [6, 3]]\n//    output: [[4, 3], [1.5, -1.5]]\n// */\n// void luFactorize(std::vector<double> &A, size_t N) {\n// for (size_t i = 0; i < N; i++) {\n//     for (size_t j = i + 1; j < N; j++) {\n//       A[j * N + i] = A[j * N + i] / A[i * N + i];\n//       for (size_t k = i + 1; k < N; k++) {\n//         A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n//       }\n//     }\n//   }\n// }\n\n// MPI implementation of luFactorize\n#include <mpi.h>\n\n/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of A. Store the result in A on rank 0.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Factorize the matrix A into A=LU where L is a lower triangular matrix and U is an upper triangular matrix.\n   Store the results for L and U into the original matrix A. \n   A is an NxN matrix stored in row-major.\n   Example:\n\n   input: [[4, 3], [6, 3]]\n   output: [[4, 3], [1.5, -1.5]]\n*/\nvoid luFactorize(std::vector<double> &A, size_t N) {\nfor (size_t i = 0; i < N; i++) {\n    for (size_t j = i + 1; j < N; j++) {\n      A[j * N + i] = A[j * N + i] / A[i * N + i];\n      for (size_t k = i + 1; k < N; k++) {\n        A[j * N + k] = A[j * N + k] - A[j * N + i] * A[i * N + k];\n      }\n    }\n  }\n}",
    "translation_function_name": "luFactorize"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {",
    "translation_prompt": "// Serial implementation of ranks\n// /* For each value in the vector x compute its index in the sorted vector.\n//    Store the results in `ranks`.\n//    Examples:\n// \n//    input: [3.1, 2.8, 9.1, 0.4, 3.14]\n//    output: [2, 1, 4, 0, 3]\n//  \n//    input: [100, 7.6, 16.1, 18, 7.6]\n//    output: [4, 0, 1, 2, 3]\n// */\n// void ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n// std::vector<float> sorted = x;\n//     std::sort(sorted.begin(), sorted.end());\n//     ranks.resize(x.size());\n//     for (size_t i = 0; i < x.size(); ++i) {\n//         ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n//     }\n// }\n\n// MPI implementation of ranks\n#include <mpi.h>\n\n/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every process has a complete copy of x. Store the result in ranks on process 0.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(std::vector<float> const& x, std::vector<size_t> &ranks) {\nstd::vector<float> sorted = x;\n    std::sort(sorted.begin(), sorted.end());\n    ranks.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n        ranks[i] = std::distance(sorted.begin(), std::lower_bound(sorted.begin(), sorted.end(), x[i]));\n    }\n}",
    "translation_function_name": "ranks"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "44_sort_sort_non-zero_elements",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {",
    "translation_prompt": "// Serial implementation of sortIgnoreZero\n// /* Sort the vector x in ascending order ignoring elements with value 0.\n//    Leave zero valued elements in-place.\n//    Example:\n// \n// \t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n//    output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n// */\n// void sortIgnoreZero(std::vector<int> &x) {\n// int n = x.size();\n//   int i = 0;\n//   int j = 0;\n//   int k = 0;\n// \n//   while (i < n) {\n//     if (x[i] == 0) {\n//       i++;\n//     } else {\n//       j = i + 1;\n//       while (j < n) {\n//         if (x[j] == 0) {\n//           j++;\n//         } else {\n//           if (x[i] > x[j]) {\n//             k = x[i];\n//             x[i] = x[j];\n//             x[j] = k;\n//           }\n//           j++;\n//         }\n//       }\n//       i++;\n//     }\n//   }\n// }\n\n// MPI implementation of sortIgnoreZero\n#include <mpi.h>\n\n/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place. \n\t Use MPI to sort x in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n\n   input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Sort the vector x in ascending order ignoring elements with value 0.\n   Leave zero valued elements in-place.\n   Example:\n\n\t input: [8, 4, 0, 9, 8, 0, 1, -1, 7]\n   output: [-1, 1, 0, 4, 7, 0, 8, 8, 9]\n*/\nvoid sortIgnoreZero(std::vector<int> &x) {\nint n = x.size();\n  int i = 0;\n  int j = 0;\n  int k = 0;\n\n  while (i < n) {\n    if (x[i] == 0) {\n      i++;\n    } else {\n      j = i + 1;\n      while (j < n) {\n        if (x[j] == 0) {\n          j++;\n        } else {\n          if (x[i] > x[j]) {\n            k = x[i];\n            x[i] = x[j];\n            x[j] = k;\n          }\n          j++;\n        }\n      }\n      i++;\n    }\n  }\n}",
    "translation_function_name": "sortIgnoreZero"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "41_sort_k-th_smallest_element",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {",
    "translation_prompt": "// Serial implementation of findKthSmallest\n// /* Find the k-th smallest element of the vector x.\n//    Example:\n//    \n//    input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n//    output: 6\n// */\n// int findKthSmallest(std::vector<int> const& x, int k) {\n// int n = x.size();\n//     std::vector<int> y(x);\n//     std::sort(y.begin(), y.end());\n//     return y[k-1];\n// }\n\n// MPI implementation of findKthSmallest\n#include <mpi.h>\n\n/* Find the k-th smallest element of the vector x.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Return the result on rank 0.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Find the k-th smallest element of the vector x.\n   Example:\n   \n   input: x=[1, 7, 6, 0, 2, 2, 10, 6], k=4\n   output: 6\n*/\nint findKthSmallest(std::vector<int> const& x, int k) {\nint n = x.size();\n    std::vector<int> y(x);\n    std::sort(y.begin(), y.end());\n    return y[k-1];\n}",
    "translation_function_name": "findKthSmallest"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "43_sort_sort_an_array_of_structs_by_key",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {",
    "translation_prompt": "// Serial implementation of sortByStartTime\n// struct Result {\n//    int startTime, duration;\n//    float value;\n// };\n// \n// /* Sort vector of Result structs by start time in ascending order.\n//    Example:\n//    \n//    input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n//    output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n// */\n// void sortByStartTime(std::vector<Result> &results) {\n// std::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n//       return a.startTime < b.startTime;\n//    });\n// }\n\n// MPI implementation of sortByStartTime\n#include <mpi.h>\n\nstruct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of results. Store the output in results on rank 0.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "struct Result {\n   int startTime, duration;\n   float value;\n};\n\n/* Sort vector of Result structs by start time in ascending order.\n   Example:\n   \n   input: [{startTime=8, duration=4, value=-1.22}, {startTime=2, duration=10, value=1.0}, {startTime=10, duration=3, value=0.0}]\n   output: [{startTime=2, duration=10, value=1.0}, {startTime=8, duration=4, value=-1.22}, {startTime=10, duration=3, value=0.0}]\n*/\nvoid sortByStartTime(std::vector<Result> &results) {\nstd::sort(results.begin(), results.end(), [](const Result &a, const Result &b) {\n      return a.startTime < b.startTime;\n   });\n}",
    "translation_function_name": "sortByStartTime"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "40_sort_sort_an_array_of_complex_numbers_by_magnitude",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {",
    "translation_prompt": "// Serial implementation of sortComplexByMagnitude\n// /* Sort the vector x of complex numbers by their magnitude in ascending order.\n//    Example:\n//    \n//    input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n//    output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n// */\n// void sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n// std::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n//         return std::abs(a) < std::abs(b);\n//     });\n// }\n\n// MPI implementation of sortComplexByMagnitude\n#include <mpi.h>\n\n/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Use MPI to sort in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. Store the result in x on rank 0.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Sort the vector x of complex numbers by their magnitude in ascending order.\n   Example:\n   \n   input: [3.0-1.0i, 4.5+2.1i, 0.0-1.0i, 1.0-0.0i, 0.5+0.5i]\n   output: [0.5+0.5i, 0.0-1.0i, 1.0-0.0i, 3.0-1.0i, 4.5+2.1i]\n*/\nvoid sortComplexByMagnitude(std::vector<std::complex<double>> &x) {\nstd::sort(x.begin(), x.end(), [](std::complex<double> a, std::complex<double> b) {\n        return std::abs(a) < std::abs(b);\n    });\n}",
    "translation_function_name": "sortComplexByMagnitude"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of cellsXOR\n// /* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n//    Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n//    input and output are NxN grids of ints in row-major.\n//    Example:\n// \n//    input: [[0, 1, 1, 0],\n//            [1, 0, 0, 0],\n//            [0, 0, 0, 0],\n//            [0, 1, 0, 0]\n//    output: [[0, 0, 1, 1],\n//             [1, 0, 0, 1],\n//             [0, 0, 1, 0],\n//             [1, 0, 1, 0]]\n// */\n// void cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int count = 0;\n//             if (i > 0 && input[i - 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (i < N - 1 && input[i + 1 + j * N] == 1) {\n//                 ++count;\n//             }\n//             if (j > 0 && input[i + (j - 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n//                 ++count;\n//             }\n//             if (count == 1) {\n//                 output[i + j * N] = 1;\n//             } else {\n//                 output[i + j * N] = 0;\n//             }\n//         }\n//     }\n// }\n\n// MPI implementation of cellsXOR\n#include <mpi.h>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int count = 0;\n            if (i > 0 && input[i - 1 + j * N] == 1) {\n                ++count;\n            }\n            if (i < N - 1 && input[i + 1 + j * N] == 1) {\n                ++count;\n            }\n            if (j > 0 && input[i + (j - 1) * N] == 1) {\n                ++count;\n            }\n            if (j < N - 1 && input[i + (j + 1) * N] == 1) {\n                ++count;\n            }\n            if (count == 1) {\n                output[i + j * N] = 1;\n            } else {\n                output[i + j * N] = 0;\n            }\n        }\n    }\n}",
    "translation_function_name": "cellsXOR"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of gameOfLife\n// /* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n//    A cell is 1 if it is alive and 0 if it is dead.\n//    If a live cell has fewer than 2 live neighbors then it dies.\n//    If a live cell has 2 or 3 live neighbors then it lives on.\n//    If a live cell has more than 3 live neighbords then it dies.\n//    If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input:  [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 0, 0, 0],\n//             [0, 1, 1, 0, 0],\n//             [0, 0, 1, 1, 0],\n//             [0, 1, 0, 0, 0]]\n//    output: [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 1, 0, 0],\n//             [0, 1, 0, 1, 0],\n//             [0, 0, 0, 1, 0],\n//             [0, 0, 1, 0, 0]]\n// */\n// void gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n// std::vector<int> next(input.size(), 0);\n//   for (size_t i = 0; i < N; ++i) {\n//     for (size_t j = 0; j < N; ++j) {\n//       int count = 0;\n//       for (int x = -1; x <= 1; ++x) {\n//         for (int y = -1; y <= 1; ++y) {\n//           if (x == 0 && y == 0) {\n//             continue;\n//           }\n//           size_t r = (i + x + N) % N;\n//           size_t c = (j + y + N) % N;\n//           count += input[r * N + c];\n//         }\n//       }\n//       if (input[i * N + j] == 1) {\n//         next[i * N + j] = (count == 2 || count == 3);\n//       } else {\n//         next[i * N + j] = (count == 3);\n//       }\n//     }\n//   }\n//   output = next;\n// }\n\n// MPI implementation of gameOfLife\n#include <mpi.h>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI is already initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(std::vector<int> const& input, std::vector<int> &output, size_t N) {\nstd::vector<int> next(input.size(), 0);\n  for (size_t i = 0; i < N; ++i) {\n    for (size_t j = 0; j < N; ++j) {\n      int count = 0;\n      for (int x = -1; x <= 1; ++x) {\n        for (int y = -1; y <= 1; ++y) {\n          if (x == 0 && y == 0) {\n            continue;\n          }\n          size_t r = (i + x + N) % N;\n          size_t c = (j + y + N) % N;\n          count += input[r * N + c];\n        }\n      }\n      if (input[i * N + j] == 1) {\n        next[i * N + j] = (count == 2 || count == 3);\n      } else {\n        next[i * N + j] = (count == 3);\n      }\n    }\n  }\n  output = next;\n}",
    "translation_function_name": "gameOfLife"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {",
    "translation_prompt": "// Serial implementation of jacobi1D\n// /* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n//    i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n//    Replace with 0 when reading past the boundaries of `input`.\n//    Example:\n// \n//    input: [9, -6, -1, 2, 3]\n//    output: [1, 2/3, -5/3, 4/3, 5/3]\n// */\n// void jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// // YOUR CODE HERE\n//     for (int i = 1; i < input.size() - 1; i++) {\n//         output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n//     }\n// }\n\n// MPI implementation of jacobi1D\n#include <mpi.h>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(std::vector<double> const& input, std::vector<double> &output) {\n// YOUR CODE HERE\n    for (int i = 1; i < input.size() - 1; i++) {\n        output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n    }\n}",
    "translation_function_name": "jacobi1D"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {",
    "translation_prompt": "// Serial implementation of convolveKernel\n// const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n// \n// /* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n//    the dot product of itself and its neighbors with the edge kernel.\n//    Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n//    imageIn and imageOut are NxN grayscale images stored in row-major.\n//    Store the output of the computation in imageOut.\n//    Example:\n// \n//    input: [[112, 118, 141, 152],\n//            [93, 101, 119, 203],\n//            [45, 17, 16, 232],\n//            [82, 31, 49, 101]]\n//    output: [[255, 255, 255, 255],\n//             [255, 147, 0, 255],\n//             [36, 0, 0, 255],\n//             [255, 39, 0, 255]]\n// */\n// void convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n// for (size_t i = 0; i < N; ++i) {\n//         for (size_t j = 0; j < N; ++j) {\n//             int sum = 0;\n//             for (int k = -1; k <= 1; ++k) {\n//                 for (int l = -1; l <= 1; ++l) {\n//                     int x = i + k;\n//                     int y = j + l;\n//                     if (x >= 0 && x < N && y >= 0 && y < N) {\n//                         sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n//                     }\n//                 }\n//             }\n//             imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n//         }\n//     }\n// }\n\n// MPI implementation of convolveKernel\n#include <mpi.h>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of imageIn. The result is stored in imageOut on rank 0.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "const int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(std::vector<int> const& imageIn, std::vector<int> &imageOut, size_t N) {\nfor (size_t i = 0; i < N; ++i) {\n        for (size_t j = 0; j < N; ++j) {\n            int sum = 0;\n            for (int k = -1; k <= 1; ++k) {\n                for (int l = -1; l <= 1; ++l) {\n                    int x = i + k;\n                    int y = j + l;\n                    if (x >= 0 && x < N && y >= 0 && y < N) {\n                        sum += imageIn[x * N + y] * edgeKernel[k + 1][l + 1];\n                    }\n                }\n            }\n            imageOut[i * N + j] = std::max(std::min(sum, 255), 0);\n        }\n    }\n}",
    "translation_function_name": "convolveKernel"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "mpi",
    "prompt": "#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {",
    "translation_prompt": "// Serial implementation of jacobi2D\n// /* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n//    i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n//    Replace with 0 when reading past the boundaries of `input`.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Example:\n// \n//    input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n//    output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n// */\n// void jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n// for (size_t i = 1; i < N - 1; ++i) {\n//     for (size_t j = 1; j < N - 1; ++j) {\n//       output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n//     }\n//   }\n// }\n\n// MPI implementation of jacobi2D\n#include <mpi.h>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of input. The result is stored in output on rank 0.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\n",
    "translation_src_model": "serial",
    "translation_dst_model": "mpi",
    "translation_src_example": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(std::vector<double> const& input, std::vector<double> &output, size_t N) {\nfor (size_t i = 1; i < N - 1; ++i) {\n    for (size_t j = 1; j < N - 1; ++j) {\n      output[i * N + j] = (input[(i - 1) * N + j] + input[(i + 1) * N + j] + input[i * N + j - 1] + input[i * N + j + 1] + input[i * N + j]) / 5;\n    }\n  }\n}",
    "translation_function_name": "jacobi2D"
  },
  {
    "problem_type": "geometry",
    "language": "cpp",
    "name": "14_geometry_closest_pair_1d",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(Kokkos::View<const double*> &x) {",
    "translation_prompt": "// CUDA implementation of closestPair\n// __device__ double distanceBetweenPoints(double x1, double x2) {\n// \treturn fabs(x1 - x2);\n// }\n// \n// /* Compute the distance between the closest two elements in the vector x.\n//    Store the result in distance.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [7, 3, 9, 12, 31, 1]\n//    output: 2\n// */\n// __global__ void closestPair(const double *x, size_t N, double *distance) {\n// \n//     int tid = blockIdx.x * blockDim.x + threadIdx.x;\n//     double minDist = DBL_MAX;\n// \n//     for (int i = 0; i < N; i++) {\n//         for (int j = i + 1; j < N; j++) {\n//             double dist = distanceBetweenPoints(x[i], x[j]);\n//             if (dist < minDist) {\n//                 minDist = dist;\n//             }\n//         }\n//     }\n// \n//     distance[tid] = minDist;\n// }\n\n// Kokkos implementation of closestPair\n#include <Kokkos_Core.hpp>\n\ndouble distance(double x1, double x2) {\n\treturn std::abs(x1 - x2);\n}\n\n/* Return the distance between the closest two elements in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example: \n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\ndouble closestPair(Kokkos::View<const double*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "__device__ double distanceBetweenPoints(double x1, double x2) {\n\treturn fabs(x1 - x2);\n}\n\n/* Compute the distance between the closest two elements in the vector x.\n   Store the result in distance.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [7, 3, 9, 12, 31, 1]\n   output: 2\n*/\n__global__ void closestPair(const double *x, size_t N, double *distance) {\n\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double minDist = DBL_MAX;\n\n    for (int i = 0; i < N; i++) {\n        for (int j = i + 1; j < N; j++) {\n            double dist = distanceBetweenPoints(x[i], x[j]);\n            if (dist < minDist) {\n                minDist = dist;\n            }\n        }\n    }\n\n    distance[tid] = minDist;\n}",
    "translation_function_name": "closestPair"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "15_graph_edge_count",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A represents a directed graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {",
    "translation_prompt": "// CUDA implementation of edgeCount\n// /* Count the number of edges in the directed graph defined by the adjacency matrix A.\n//    Store the result in numEdges. A represents a directed graph.\n//    A is an NxN adjacency matrix stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// __global__ void edgeCount(const int *A, size_t N, int *numEdges) {\n// int tid = threadIdx.x;\n// \tint bid = blockIdx.x;\n// \tint numThreads = blockDim.x;\n// \tint numBlocks = gridDim.x;\n// \tint start = bid * numThreads + tid;\n// \tint end = (bid + 1) * numThreads;\n// \tint count = 0;\n// \tfor (int i = start; i < end; i++) {\n// \t\tfor (int j = 0; j < N; j++) {\n// \t\t\tif (A[i * N + j] == 1) {\n// \t\t\t\tcount++;\n// \t\t\t}\n// \t\t}\n// \t}\n// \t__syncthreads();\n// \tif (tid == 0) {\n// \t\tatomicAdd(numEdges, count);\n// \t}\n// }\n\n// Kokkos implementation of edgeCount\n#include <Kokkos_Core.hpp>\n\n/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A represents a directed graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint edgeCount(Kokkos::View<const int**> &A, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Count the number of edges in the directed graph defined by the adjacency matrix A.\n   Store the result in numEdges. A represents a directed graph.\n   A is an NxN adjacency matrix stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void edgeCount(const int *A, size_t N, int *numEdges) {\nint tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numThreads = blockDim.x;\n\tint numBlocks = gridDim.x;\n\tint start = bid * numThreads + tid;\n\tint end = (bid + 1) * numThreads;\n\tint count = 0;\n\tfor (int i = start; i < end; i++) {\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tcount++;\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\tif (tid == 0) {\n\t\tatomicAdd(numEdges, count);\n\t}\n}",
    "translation_function_name": "edgeCount"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "17_graph_highest_degree",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {",
    "translation_prompt": "// CUDA implementation of maxDegree\n// /* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph. \n//    Store the result in maxDegree.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n// \t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n//    output: 3\n// */\n// __global__ void maxDegree(const int *A, size_t N, int *maxDegree) {\n// int tid = threadIdx.x;\n// \tint max = 0;\n// \tfor (int i = tid; i < N; i += blockDim.x) {\n// \t\tint degree = 0;\n// \t\tfor (int j = 0; j < N; j++) {\n// \t\t\tif (A[i * N + j] == 1) {\n// \t\t\t\tdegree++;\n// \t\t\t}\n// \t\t}\n// \t\tif (degree > max) {\n// \t\t\tmax = degree;\n// \t\t}\n// \t}\n// \t__syncthreads();\n// \tatomicMax(maxDegree, max);\n// }\n\n// Kokkos implementation of maxDegree\n#include <Kokkos_Core.hpp>\n\n/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\nint maxDegree(Kokkos::View<const int**> &A, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the highest node degree in the undirected graph. The graph is defined in the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph. \n   Store the result in maxDegree.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n\t input: [[0, 0, 0, 1], [0, 0, 0, 1], [0, 0, 0, 1], [1, 1, 1, 0]]\n   output: 3\n*/\n__global__ void maxDegree(const int *A, size_t N, int *maxDegree) {\nint tid = threadIdx.x;\n\tint max = 0;\n\tfor (int i = tid; i < N; i += blockDim.x) {\n\t\tint degree = 0;\n\t\tfor (int j = 0; j < N; j++) {\n\t\t\tif (A[i * N + j] == 1) {\n\t\t\t\tdegree++;\n\t\t\t}\n\t\t}\n\t\tif (degree > max) {\n\t\t\tmax = degree;\n\t\t}\n\t}\n\t__syncthreads();\n\tatomicMax(maxDegree, max);\n}",
    "translation_function_name": "maxDegree"
  },
  {
    "problem_type": "graph",
    "language": "cpp",
    "name": "19_graph_shortest_path",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {",
    "translation_prompt": "// CUDA implementation of shortestPathLength\n// /* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n//    A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n//    Store the result in pathLength.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n// \t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n//    output: 2\n// */\n// __global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {\n// int row = blockIdx.y * blockDim.y + threadIdx.y;\n//     int col = blockIdx.x * blockDim.x + threadIdx.x;\n// \n//     if (row < N && col < N) {\n//         if (row == col) {\n//             pathLength[row] = 0;\n//         } else if (A[row * N + col] == 1) {\n//             pathLength[row] = 1;\n//         } else {\n//             pathLength[row] = INT_MAX;\n//         }\n// \n//         __syncthreads();\n// \n//         for (int k = 0; k < N; ++k) {\n//             if (pathLength[row] + pathLength[k] < pathLength[col]) {\n//                 pathLength[col] = pathLength[row] + pathLength[k];\n//             }\n//             __syncthreads();\n//         }\n//     }\n// \n//     if (row == dest) {\n//         *pathLength = pathLength[row];\n//     }\n// }\n\n// Kokkos implementation of shortestPathLength\n#include <Kokkos_Core.hpp>\n\n/* Return the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix. A is an undirected graph.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\nint shortestPathLength(Kokkos::View<const int**> &A, size_t N, int source, int dest) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the length of the shortest path from source to dest in the undirected graph defined by the adjacency matrix A.\n   A is an NxN adjacency matrix stored in row-major. A is an undirected graph.\n   Store the result in pathLength.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n\t input: [[0, 1, 1, 0], [1, 0, 0, 1], [1, 0, 0, 1], [0, 1, 1, 0]], source=0, dest=3\n   output: 2\n*/\n__global__ void shortestPathLength(const int *A, size_t N, int source, int dest, int *pathLength) {\nint row = blockIdx.y * blockDim.y + threadIdx.y;\n    int col = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (row < N && col < N) {\n        if (row == col) {\n            pathLength[row] = 0;\n        } else if (A[row * N + col] == 1) {\n            pathLength[row] = 1;\n        } else {\n            pathLength[row] = INT_MAX;\n        }\n\n        __syncthreads();\n\n        for (int k = 0; k < N; ++k) {\n            if (pathLength[row] + pathLength[k] < pathLength[col]) {\n                pathLength[col] = pathLength[row] + pathLength[k];\n            }\n            __syncthreads();\n        }\n    }\n\n    if (row == dest) {\n        *pathLength = pathLength[row];\n    }\n}",
    "translation_function_name": "shortestPathLength"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "28_reduce_smallest_odd_number",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {",
    "translation_prompt": "// CUDA implementation of smallestOdd\n// /* Find the value of the smallest odd number in the vector x. Store it in smallest.\n//    Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n//    Examples:\n// \n//    input: [7, 9, 5, 2, 8, 16, 4, 1]\n//    output: 1\n// \n//    input: [8, 36, 7, 2, 11]\n//    output: 7\n// */\n// __global__ void smallestOdd(const int *x, size_t N, int *smallest) {\n// int thread_id = threadIdx.x;\n//     int block_id = blockIdx.x;\n//     int stride = blockDim.x;\n// \n//     int local_smallest = x[block_id * stride + thread_id];\n//     for (int i = 1; i < stride; i++) {\n//         int value = x[block_id * stride + thread_id + i];\n//         if (value % 2 == 1 && value < local_smallest) {\n//             local_smallest = value;\n//         }\n//     }\n//     if (local_smallest % 2 == 1) {\n//         atomicMin(smallest, local_smallest);\n//     }\n// }\n\n// Kokkos implementation of smallestOdd\n#include <Kokkos_Core.hpp>\n\n/* Return the value of the smallest odd number in the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\nint smallestOdd(Kokkos::View<const int*> const& x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Find the value of the smallest odd number in the vector x. Store it in smallest.\n   Use CUDA to compute in parallel. The kernel is launched with the same number of threads as elements in x.\n   Examples:\n\n   input: [7, 9, 5, 2, 8, 16, 4, 1]\n   output: 1\n\n   input: [8, 36, 7, 2, 11]\n   output: 7\n*/\n__global__ void smallestOdd(const int *x, size_t N, int *smallest) {\nint thread_id = threadIdx.x;\n    int block_id = blockIdx.x;\n    int stride = blockDim.x;\n\n    int local_smallest = x[block_id * stride + thread_id];\n    for (int i = 1; i < stride; i++) {\n        int value = x[block_id * stride + thread_id + i];\n        if (value % 2 == 1 && value < local_smallest) {\n            local_smallest = value;\n        }\n    }\n    if (local_smallest % 2 == 1) {\n        atomicMin(smallest, local_smallest);\n    }\n}",
    "translation_function_name": "smallestOdd"
  },
  {
    "problem_type": "reduce",
    "language": "cpp",
    "name": "27_reduce_average",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {",
    "translation_prompt": "// CUDA implementation of average\n// /* Compute the average of the vector x. Store the result in average.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Examples:\n//    \n// \t input: [1, 8, 4, 5, 1]\n//    output: 3.8\n// \n//    input: [2, 2, 2, 3]\n//    output: 2.25\n// */\n// __global__ void average(const double *x, size_t N, double *average) {\n// \n//     int tid = blockIdx.x * blockDim.x + threadIdx.x;\n//     double sum = 0.0;\n//     if (tid < N) {\n//         sum = x[tid];\n//     }\n//     sum = atomicAdd(average, sum);\n//     __syncthreads();\n//     if (tid == 0) {\n//         *average /= N;\n//     }\n// }\n\n// Kokkos implementation of average\n#include <Kokkos_Core.hpp>\n\n/* Return the average of the vector x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\t\t\n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\ndouble average(Kokkos::View<const double*> const& x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the average of the vector x. Store the result in average.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n   \n\t input: [1, 8, 4, 5, 1]\n   output: 3.8\n\n   input: [2, 2, 2, 3]\n   output: 2.25\n*/\n__global__ void average(const double *x, size_t N, double *average) {\n\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    double sum = 0.0;\n    if (tid < N) {\n        sum = x[tid];\n    }\n    sum = atomicAdd(average, sum);\n    __syncthreads();\n    if (tid == 0) {\n        *average /= N;\n    }\n}\n",
    "translation_function_name": "average"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "31_scan_scan_with_min_function",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {",
    "translation_prompt": "// CUDA implementation of partialMinimums\n// /* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Examples:\n// \n//    input: [8, 6, -1, 7, 3, 4, 4]\n//    output: [8, 6, -1, -1, -1, -1, -1]\n// \n//    input: [5, 4, 6, 4, 3, 6, 1, 1]\n//    output: [5, 4, 4, 4, 3, 3, 1, 1]\n// */\n// __global__ void partialMinimums(float *x, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         float min = x[i];\n//         for (int j = 0; j < i; j++) {\n//             if (x[j] < min) {\n//                 min = x[j];\n//             }\n//         }\n//         x[i] = min;\n//     }\n// }\n\n// Kokkos implementation of partialMinimums\n#include <Kokkos_Core.hpp>\n\n/* Replace the i-th element of the array x with the minimum value from indices 0 through i.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\nvoid partialMinimums(Kokkos::View<float*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Replace the i-th element of the vector x with the minimum value from indices 0 through i.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Examples:\n\n   input: [8, 6, -1, 7, 3, 4, 4]\n   output: [8, 6, -1, -1, -1, -1, -1]\n\n   input: [5, 4, 6, 4, 3, 6, 1, 1]\n   output: [5, 4, 4, 4, 3, 3, 1, 1]\n*/\n__global__ void partialMinimums(float *x, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        float min = x[i];\n        for (int j = 0; j < i; j++) {\n            if (x[j] < min) {\n                min = x[j];\n            }\n        }\n        x[i] = min;\n    }\n}",
    "translation_function_name": "partialMinimums"
  },
  {
    "problem_type": "scan",
    "language": "cpp",
    "name": "30_scan_prefix_sum",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {",
    "translation_prompt": "// CUDA implementation of prefixSum\n// /* Compute the prefix sum of the vector x into output.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n//    Example:\n//    \n//    input: [1, 7, 4, 6, 6, 2]\n//    output: [1, 8, 12, 18, 24, 26]\n// */\n// __global__ void prefixSum(const double *x, double *output, size_t N) {\n// int tid = threadIdx.x + blockIdx.x * blockDim.x;\n//     int stride = blockDim.x * gridDim.x;\n// \n//     for (size_t i = tid; i < N; i += stride) {\n//         double sum = 0;\n//         for (size_t j = 0; j <= i; ++j) {\n//             sum += x[j];\n//         }\n//         output[i] = sum;\n//     }\n// }\n\n// Kokkos implementation of prefixSum\n#include <Kokkos_Core.hpp>\n\n/* Compute the prefix sum of the vector x into output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\nvoid prefixSum(Kokkos::View<const double*> const& x, Kokkos::View<double*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the prefix sum of the vector x into output.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n   \n   input: [1, 7, 4, 6, 6, 2]\n   output: [1, 8, 12, 18, 24, 26]\n*/\n__global__ void prefixSum(const double *x, double *output, size_t N) {\nint tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        double sum = 0;\n        for (size_t j = 0; j <= i; ++j) {\n            sum += x[j];\n        }\n        output[i] = sum;\n    }\n}",
    "translation_function_name": "prefixSum"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "56_transform_negate_odds",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {",
    "translation_prompt": "// CUDA implementation of negateOddsAndHalveEvens\n// /* In the vector x negate the odd values and divide the even values by 2.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n// \n//    input: [16, 11, 12, 14, 1, 0, 5]\n//    output: [8, -11, 6, 7, -1, 0, -5]\n// */\n// __global__ void negateOddsAndHalveEvens(int *x, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         if (x[idx] % 2 == 0) {\n//             x[idx] /= 2;\n//         } else {\n//             x[idx] = -x[idx];\n//         }\n//     }\n// }\n\n// Kokkos implementation of negateOddsAndHalveEvens\n#include <Kokkos_Core.hpp>\n\n/* In the vector x negate the odd values and divide the even values by 2.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\nvoid negateOddsAndHalveEvens(Kokkos::View<int*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* In the vector x negate the odd values and divide the even values by 2.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [16, 11, 12, 14, 1, 0, 5]\n   output: [8, -11, 6, 7, -1, 0, -5]\n*/\n__global__ void negateOddsAndHalveEvens(int *x, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] % 2 == 0) {\n            x[idx] /= 2;\n        } else {\n            x[idx] = -x[idx];\n        }\n    }\n}",
    "translation_function_name": "negateOddsAndHalveEvens"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "59_transform_map_function",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {",
    "translation_prompt": "// CUDA implementation of mapPowersOfTwo\n// __device__ bool isPowerOfTwo(int x) {\n// \treturn (x > 0) && !(x & (x - 1));\n// }\n// \n// /* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n//    Example:\n// \n//    input: [8, 0, 9, 7, 15, 64, 3]\n//    output: [true, false, false, false, false, true, false]\n// */\n// __global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {\n// int i = threadIdx.x + blockIdx.x * blockDim.x;\n// \tif (i < N) {\n// \t\tmask[i] = isPowerOfTwo(x[i]);\n// \t}\n// }\n\n// Kokkos implementation of mapPowersOfTwo\n#include <Kokkos_Core.hpp>\n\nbool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\nvoid mapPowersOfTwo(Kokkos::View<const int*> const& x, Kokkos::View<bool*> &mask) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "__device__ bool isPowerOfTwo(int x) {\n\treturn (x > 0) && !(x & (x - 1));\n}\n\n/* Apply the isPowerOfTwo function to every value in x and store the results in mask.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [8, 0, 9, 7, 15, 64, 3]\n   output: [true, false, false, false, false, true, false]\n*/\n__global__ void mapPowersOfTwo(const int *x, bool *mask, size_t N) {\nint i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < N) {\n\t\tmask[i] = isPowerOfTwo(x[i]);\n\t}\n}",
    "translation_function_name": "mapPowersOfTwo"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "57_transform_inverse_offset",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {",
    "translation_prompt": "// CUDA implementation of oneMinusInverse\n// /* Replace every element of the vector x with 1-1/x.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n//    Example:\n// \n//    input: [2, 4, 1, 12, -2]\n//    output: [0.5, 0.75, 0, 0.91666666, 1.5]\n// */\n// __global__ void oneMinusInverse(double *x, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         x[idx] = 1.0 - 1.0 / x[idx];\n//     }\n// }\n\n// Kokkos implementation of oneMinusInverse\n#include <Kokkos_Core.hpp>\n\n/* Replace every element of the vector x with 1-1/x.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\nvoid oneMinusInverse(Kokkos::View<double*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Replace every element of the vector x with 1-1/x.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as elements in x.\n   Example:\n\n   input: [2, 4, 1, 12, -2]\n   output: [0.5, 0.75, 0, 0.91666666, 1.5]\n*/\n__global__ void oneMinusInverse(double *x, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = 1.0 - 1.0 / x[idx];\n    }\n}",
    "translation_function_name": "oneMinusInverse"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "55_transform_relu",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {",
    "translation_prompt": "// CUDA implementation of relu\n// /* Compute the ReLU function on every element of x. Elements less than zero become zero,\n//    while elements greater than zero stay the same.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n// \n//    input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n//    output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n// */\n// __global__ void relu(double *x, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         x[idx] = max(0.0, x[idx]);\n//     }\n// }\n\n// Kokkos implementation of relu\n#include <Kokkos_Core.hpp>\n\n/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\nvoid relu(Kokkos::View<double*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the ReLU function on every element of x. Elements less than zero become zero,\n   while elements greater than zero stay the same.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [-1.8, 24.0, 1.2, 0.0, -5.1, -0.2, 4.5]\n   output: [0, 24.0, 1.2, 0, 0, 0, 4.5]\n*/\n__global__ void relu(double *x, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        x[idx] = max(0.0, x[idx]);\n    }\n}",
    "translation_function_name": "relu"
  },
  {
    "problem_type": "transform",
    "language": "cpp",
    "name": "58_transform_squaring",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {",
    "translation_prompt": "// CUDA implementation of squareEach\n// /* Replace every element of x with the square of its value.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n// \n//    input: [5, 1, 2, -4, 8]\n//    output: [25, 1, 4, 16, 64]\n// */\n// __global__ void squareEach(int *x, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         x[i] = x[i] * x[i];\n//     }\n// }\n\n// Kokkos implementation of squareEach\n#include <Kokkos_Core.hpp>\n\n/* Replace every element of x with the square of its value.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\nvoid squareEach(Kokkos::View<int*> &x) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Replace every element of x with the square of its value.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n\n   input: [5, 1, 2, -4, 8]\n   output: [25, 1, 4, 16, 64]\n*/\n__global__ void squareEach(int *x, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        x[i] = x[i] * x[i];\n    }\n}",
    "translation_function_name": "squareEach"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "47_sparse_la_spmv",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {",
    "translation_prompt": "// CUDA implementation of spmv\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n//    and A is a sparse matrix stored in COO format with sizeA elements.\n//    A has dimensions MxN, x has N values, and y has M values.\n//    Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n//    Example:\n// \n//    input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n//    output: y=[2, 3]\n// */\n// __global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {\n// size_t row = blockIdx.x * blockDim.x + threadIdx.x;\n//    if (row < M) {\n//       double sum = 0.0;\n//       for (size_t i = 0; i < sizeA; i++) {\n//          if (A[i].row == row) {\n//             sum += A[i].value * x[A[i].column];\n//          }\n//       }\n//       y[row] = alpha * sum + beta * y[row];\n//    }\n// }\n\n// Kokkos implementation of spmv\n#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use Kokkos to parallelize. Assume Kokkos has already been initialized.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\nvoid spmv(double alpha, Kokkos::View<const COOElement*> const& A, Kokkos::View<const double*> const& x, double beta, Kokkos::View<double *> &y, size_t M, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute y = alpha*A*x + beta*y where alpha and beta are scalars, x and y are vectors,\n   and A is a sparse matrix stored in COO format with sizeA elements.\n   A has dimensions MxN, x has N values, and y has M values.\n   Use CUDA to parallelize. The kernel will be launched with at least sizeA threads.\n   Example:\n\n   input: alpha=0.5 beta=1.0 A=[{0,1,3}, {1,0,-1}] x=[-4, 2] y=[-1,1]\n   output: y=[2, 3]\n*/\n__global__ void spmv(double alpha, const COOElement *A, size_t sizeA, const double *x, double beta, double *y, size_t M, size_t N) {\nsize_t row = blockIdx.x * blockDim.x + threadIdx.x;\n   if (row < M) {\n      double sum = 0.0;\n      for (size_t i = 0; i < sizeA; i++) {\n         if (A[i].row == row) {\n            sum += A[i].value * x[A[i].column];\n         }\n      }\n      y[row] = alpha * sum + beta * y[row];\n   }\n}",
    "translation_function_name": "spmv"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "46_sparse_la_spmm",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// CUDA implementation of spmm\n// struct COOElement {\n//    size_t row, column;\n//    double value;\n// };\n// \n// /* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n//    X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n//    Example:\n// \n//    input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n//    output: Y=[{-1,-4}, {1,0}]\n// */\n// __global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {\n// \n//     int row = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (row < M) {\n//         for (int i = 0; i < sizeA; i++) {\n//             if (A[i].row == row) {\n//                 int col = A[i].column;\n//                 double valA = A[i].value;\n//                 for (int j = 0; j < sizeX; j++) {\n//                     if (X[j].row == col) {\n//                         int colX = X[j].column;\n//                         double valX = X[j].value;\n//                         atomicAdd(&Y[row * N + colX], valA * valX);\n//                     }\n//                 }\n//             }\n//         }\n//     }\n// }\n\n// Kokkos implementation of spmm\n#include <Kokkos_Core.hpp>\n\nstruct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format.\n   X is a sparse KxN matrix in COO format. Y is a dense MxN matrix.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\nvoid spmm(Kokkos::View<const COOElement*> &A, Kokkos::View<const COOElement*> &X, Kokkos::View<double**> &Y, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct COOElement {\n   size_t row, column;\n   double value;\n};\n\n/* Compute the matrix multiplication Y=AX. A is a sparse MxK matrix in COO format with sizeA elements.\n   X is a sparse KxN matrix in COO format with sizeX elements. Y is a dense MxN matrix in row-major.\n   Use CUDA to compute in parallel. The kernel is launched with at least sizeA threads.\n   Example:\n\n   input: A=[{0,0,-2}, {0,1,1}, {1,1,-1}] X=[{0,1,2}, {1,0,-1}]\n   output: Y=[{-1,-4}, {1,0}]\n*/\n__global__ void spmm(const COOElement *A, size_t sizeA, const COOElement *X, size_t sizeX, double *Y, size_t M, size_t K, size_t N) {\n\n    int row = blockIdx.x * blockDim.x + threadIdx.x;\n    if (row < M) {\n        for (int i = 0; i < sizeA; i++) {\n            if (A[i].row == row) {\n                int col = A[i].column;\n                double valA = A[i].value;\n                for (int j = 0; j < sizeX; j++) {\n                    if (X[j].row == col) {\n                        int colX = X[j].column;\n                        double valX = X[j].value;\n                        atomicAdd(&Y[row * N + colX], valA * valX);\n                    }\n                }\n            }\n        }\n    }\n}\n",
    "translation_function_name": "spmm"
  },
  {
    "problem_type": "sparse_la",
    "language": "cpp",
    "name": "48_sparse_la_sparse_axpy",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<double*> &z) {",
    "translation_prompt": "// CUDA implementation of sparseAxpy\n// struct Element {\n// \tsize_t index;\n//   double value;\n// };\n// \n// /* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n//    Example:\n//    \n//    input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n//    output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n// */\n// __global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {\n// size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n// \tif (i < N) {\n// \t\tdouble sum = 0;\n// \t\tfor (size_t j = 0; j < Nx; j++) {\n// \t\t\tif (x[j].index == i) {\n// \t\t\t\tsum += x[j].value;\n// \t\t\t}\n// \t\t}\n// \t\tfor (size_t j = 0; j < Ny; j++) {\n// \t\t\tif (y[j].index == i) {\n// \t\t\t\tsum += y[j].value;\n// \t\t\t}\n// \t\t}\n// \t\tz[i] = alpha * sum;\n// \t}\n// }\n\n// Kokkos implementation of sparseAxpy\n#include <Kokkos_Core.hpp>\n\nstruct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\nvoid sparseAxpy(double alpha, Kokkos::View<const Element*> &x, Kokkos::View<const Element*> &y, Kokkos::View<double*> &z) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct Element {\n\tsize_t index;\n  double value;\n};\n\n/* Compute z = alpha*x+y where x and y are sparse vectors of size Nx and Ny. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x or y.\n   Example:\n   \n   input: x=[{5, 12}, {8, 3}, {12, -1}], y=[{3, 1}, {5, -2}, {7, 1}, {8, -3}], alpha=1\n   output: z=[{3, 1}, {5, 10}, {7, 1}, {12, -1}]\n*/\n__global__ void sparseAxpy(double alpha, const Element *x, const Element *y, double *z, size_t Nx, size_t Ny, size_t N) {\nsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tdouble sum = 0;\n\t\tfor (size_t j = 0; j < Nx; j++) {\n\t\t\tif (x[j].index == i) {\n\t\t\t\tsum += x[j].value;\n\t\t\t}\n\t\t}\n\t\tfor (size_t j = 0; j < Ny; j++) {\n\t\t\tif (y[j].index == i) {\n\t\t\t\tsum += y[j].value;\n\t\t\t}\n\t\t}\n\t\tz[i] = alpha * sum;\n\t}\n}",
    "translation_function_name": "sparseAxpy"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "23_histogram_first_letter_counts",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
    "translation_prompt": "// CUDA implementation of firstLetterCounts\n// /* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n//    Assume all strings are in lower case. Store the output in `bins` array.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n//    output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n// */\n// __global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {\n// int tid = threadIdx.x;\n//     int bid = blockIdx.x;\n//     int stride = blockDim.x;\n//     int i = bid * stride + tid;\n//     if (i < N) {\n//         char c = s[i][0];\n//         if (c >= 'a' && c <= 'z') {\n//             atomicAdd(&bins[c - 'a'], 1);\n//         }\n//     }\n// }\n\n// Kokkos implementation of firstLetterCounts\n#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t bins[26]) {\nint tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int stride = blockDim.x;\n    int i = bid * stride + tid;\n    if (i < N) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
    "translation_function_name": "firstLetterCounts"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "21_histogram_bin_0-100",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
    "translation_prompt": "// CUDA implementation of binsBy10Count\n// /* Vector x contains values between 0 and 100, inclusive. Count the number of\n//    values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n//    Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n//    Example:\n// \n//    input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n//    output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n// */\n// __global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {\n// size_t tid = threadIdx.x;\n//     size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n//     size_t stride = blockDim.x * gridDim.x;\n// \n//     for (size_t i = gid; i < N; i += stride) {\n//         size_t bin = (size_t) (x[i] / 10);\n//         atomicAdd(&bins[bin], 1);\n//     }\n// }\n\n// Kokkos implementation of binsBy10Count\n#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t bins[10]) {\nsize_t tid = threadIdx.x;\n    size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = gid; i < N; i += stride) {\n        size_t bin = (size_t) (x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
    "translation_function_name": "binsBy10Count"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "22_histogram_count_quadrants",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
    "translation_prompt": "// CUDA implementation of countQuadrants\n// struct Point {\n//    double x, y;\n// };\n// \n// /* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n//    Store the counts in `bins`.\n//    Use CUDA to count in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n//    output: [3, 1, 0, 2]\n// */\n// __global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {\n// int i = threadIdx.x;\n//     if (i < N) {\n//         double x = points[i].x;\n//         double y = points[i].y;\n//         if (x >= 0 && y >= 0) {\n//             atomicAdd(&bins[0], 1);\n//         } else if (x < 0 && y >= 0) {\n//             atomicAdd(&bins[1], 1);\n//         } else if (x < 0 && y < 0) {\n//             atomicAdd(&bins[2], 1);\n//         } else if (x >= 0 && y < 0) {\n//             atomicAdd(&bins[3], 1);\n//         }\n//     }\n// }\n\n// Kokkos implementation of countQuadrants\n#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t bins[4]) {\nint i = threadIdx.x;\n    if (i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x >= 0 && y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (x < 0 && y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (x < 0 && y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (x >= 0 && y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
    "translation_function_name": "countQuadrants"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "24_histogram_count_quartile",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
    "translation_prompt": "// CUDA implementation of countQuartiles\n// /* Count the number of doubles in the vector x that have a fractional part \n//    in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Examples:\n// \n//    input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n//    output: [2, 1, 2, 2]\n// \n//    input: [1.9, 0.2, 0.6, 10.1, 7.4]\n//    output: [2, 1, 1, 1]\n// */\n// __global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {\n// size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         double frac = fmod(x[i], 1.0);\n//         if (frac < 0.25)\n//             atomicAdd(&bins[0], 1);\n//         else if (frac < 0.5)\n//             atomicAdd(&bins[1], 1);\n//         else if (frac < 0.75)\n//             atomicAdd(&bins[2], 1);\n//         else\n//             atomicAdd(&bins[3], 1);\n//     }\n// }\n\n// Kokkos implementation of countQuartiles\n#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t bins[4]) {\nsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = fmod(x[i], 1.0);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
    "translation_function_name": "countQuartiles"
  },
  {
    "problem_type": "histogram",
    "language": "cpp",
    "name": "20_histogram_pixel_histogram",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
    "translation_prompt": "// CUDA implementation of pixelCounts\n// /* Count the number of pixels in image with each grayscale intensity.\n//    The vector `image` is a grayscale image with values 0-255.\n//    Store the results in `bins`.\n//    Use CUDA to count in parallel. The kernel is launched with at least N threads.\n//    Example:\n//    \n//    input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n//    output: [0, 0, 2, 0, 1, ...]\n// */\n// __global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {\n// int tid = threadIdx.x;\n//     int gid = blockIdx.x * blockDim.x + threadIdx.x;\n//     int bin = 0;\n//     for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n//         bin = image[i];\n//         atomicAdd(&bins[bin], 1);\n//     }\n// }\n\n// Kokkos implementation of pixelCounts\n#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t bins[256]) {\nint tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int bin = 0;\n    for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n        bin = image[i];\n        atomicAdd(&bins[bin], 1);\n    }\n}",
    "translation_function_name": "pixelCounts"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "36_search_check_if_array_contains_value",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {",
    "translation_prompt": "// CUDA implementation of contains\n// /* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n//    Use CUDA to search in parallel. The kernel is launched with at least N threads.\n//    Examples:\n// \n//    input: x=[1, 8, 2, 6, 4, 6], target=3\n//    output: false\n//    \n//    input: x=[1, 8, 2, 6, 4, 6], target=8\n//    output: true\n// */\n// __global__ void contains(const int *x, size_t N, int target, bool *found) {\n// // TODO: implement the kernel\n// }\n\n// Kokkos implementation of contains\n#include <Kokkos_Core.hpp>\n\n/* Return true if the vector x contains the value `target`. Return false otherwise.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\nbool contains(Kokkos::View<const int*> const& x, int target) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Set `found` to true if the vector x contains the value `target`. Set it to false otherwise.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1, 8, 2, 6, 4, 6], target=3\n   output: false\n   \n   input: x=[1, 8, 2, 6, 4, 6], target=8\n   output: true\n*/\n__global__ void contains(const int *x, size_t N, int target, bool *found) {\n// TODO: implement the kernel\n}",
    "translation_function_name": "contains"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "39_search_xor_contains",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, Kokkos::View<const int*> const& y, int val) {",
    "translation_prompt": "// CUDA implementation of xorContains\n// /* Set `found` to true if `val` is only in one of vectors x or y.\n//    Set it to false if it is in both or neither.\n//    Use CUDA to search in parallel. The kernel is launched with at least N threads.\n//    Examples:\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n//    output: true\n// \n//    input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n//    output: false\n// */\n// __global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {\n// int tid = threadIdx.x;\n//   int i = blockIdx.x * blockDim.x + tid;\n//   int found_local = 0;\n//   while (i < N) {\n//     found_local ^= (x[i] == val) | (y[i] == val);\n//     i += blockDim.x * gridDim.x;\n//   }\n//   __syncthreads();\n//   if (tid == 0) {\n//     *found = found_local;\n//   }\n// }\n\n// Kokkos implementation of xorContains\n#include <Kokkos_Core.hpp>\n\n/* Return true if `val` is only in one of vectors x or y.\n   Return false if it is in both or neither.\n   Use Kokkos to search in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\nbool xorContains(Kokkos::View<const int*> const& x, Kokkos::View<const int*> const& y, int val) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Set `found` to true if `val` is only in one of vectors x or y.\n   Set it to false if it is in both or neither.\n   Use CUDA to search in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=7\n   output: true\n\n   input: x=[1,8,4,3,2], y=[3,4,4,1,1,7], val=1\n   output: false\n*/\n__global__ void xorContains(const int *x, const int *y, size_t N, int val, bool *found) {\nint tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + tid;\n  int found_local = 0;\n  while (i < N) {\n    found_local ^= (x[i] == val) | (y[i] == val);\n    i += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  if (tid == 0) {\n    *found = found_local;\n  }\n}",
    "translation_function_name": "xorContains"
  },
  {
    "problem_type": "search",
    "language": "cpp",
    "name": "35_search_search_for_last_struct_by_key",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {",
    "translation_prompt": "// CUDA implementation of findLastShortBook\n// struct Book {\n//    const char* title;\n//    int pages;\n// };\n// \n// /* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n//    Store the result in lastShortBookIndex.\n//    Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n// \t Example:\n// \n//    input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n//    output: 2\n// */\n// __global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {\n// \n// // Thread index\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n// \n// // Check if the thread index is within the bounds of the books array\n// if (i < N) {\n//     // Check if the book has less than 100 pages\n//     if (books[i].pages < 100) {\n//         // Use atomicMax to safely find the largest index among all threads\n//         atomicMax((int*)lastShortBookIndex, i);\n//     }\n// }\n// }\n\n// Kokkos implementation of findLastShortBook\n#include <Kokkos_Core.hpp>\n\nstruct Book {\n   const char *title;\n   int pages;\n};\n\n/* Return the index of the last Book item in the vector books where Book.pages is less than 100.\n   Use Kokkos to search in parallel. Assume Kokkos is already initialized.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\nsize_t findLastShortBook(Kokkos::View<const Book*> const& books) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "struct Book {\n   const char* title;\n   int pages;\n};\n\n/* Find the index of the last Book item in the vector books where Book.pages is less than 100.\n   Store the result in lastShortBookIndex.\n   Use CUDA to search in parallel. The kernel is launched with one thread for every book element.\n\t Example:\n\n   input: [{title=\"Green Eggs and Ham\", pages=72}, {title=\"gulliver's travels\", pages=362}, {title=\"Stories of Your Life\", pages=54}, {title=\"Hamilton\", pages=818}]\n   output: 2\n*/\n__global__ void findLastShortBook(const Book *books, size_t N, size_t *lastShortBookIndex) {\n\n// Thread index\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n\n// Check if the thread index is within the bounds of the books array\nif (i < N) {\n    // Check if the book has less than 100 pages\n    if (books[i].pages < 100) {\n        // Use atomicMax to safely find the largest index among all threads\n        atomicMax((int*)lastShortBookIndex, i);\n    }\n}\n}",
    "translation_function_name": "findLastShortBook"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "06_fft_dft",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(Kokkos::View<const double*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
    "translation_prompt": "// CUDA implementation of dft\n// /* Compute the discrete fourier transform of x. Store the result in output.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [1, 4, 9, 16]\n//    output: [30+0i, -8-12i, -10-0i, -8+12i]\n// */\n// __global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {\n// int tid = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (tid >= N) return;\n// \n//     cuDoubleComplex sum = {0, 0};\n//     for (size_t k = 0; k < N; ++k) {\n//         double angle = -2 * M_PI * tid * k / N;\n//         cuDoubleComplex exp_part = {cos(angle), sin(angle)};\n//         cuDoubleComplex term = cuCmul(exp_part, make_cuDoubleComplex(x[k], 0));\n//         sum = cuCadd(sum, term);\n//     }\n//     output[tid] = sum;\n// }\n\n// Kokkos implementation of dft\n#include <Kokkos_Core.hpp>\n\n/* Compute the discrete fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\nvoid dft(Kokkos::View<const double*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the discrete fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1, 4, 9, 16]\n   output: [30+0i, -8-12i, -10-0i, -8+12i]\n*/\n__global__ void dft(const double *x, cuDoubleComplex *output, size_t N) {\nint tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    cuDoubleComplex sum = {0, 0};\n    for (size_t k = 0; k < N; ++k) {\n        double angle = -2 * M_PI * tid * k / N;\n        cuDoubleComplex exp_part = {cos(angle), sin(angle)};\n        cuDoubleComplex term = cuCmul(exp_part, make_cuDoubleComplex(x[k], 0));\n        sum = cuCadd(sum, term);\n    }\n    output[tid] = sum;\n}",
    "translation_function_name": "dft"
  },
  {
    "problem_type": "fft",
    "language": "cpp",
    "name": "09_fft_fft_out_of_place",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<const Kokkos::complex<double>*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {",
    "translation_prompt": "// CUDA implementation of fft\n// /* Compute the fourier transform of x. Store the result in output.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n//    output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n// */\n// __global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *output, size_t N) {\n// int idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         cuDoubleComplex sum = {0, 0};\n//         for (int k = 0; k < N; k++) {\n//             cuDoubleComplex a = x[k];\n//             cuDoubleComplex b = cuCmul(make_cuDoubleComplex(cos(2 * M_PI * idx * k / N), -sin(2 * M_PI * idx * k / N)), a);\n//             sum = cuCadd(sum, b);\n//         }\n//         output[idx] = sum;\n//     }\n// }\n\n// Kokkos implementation of fft\n#include <Kokkos_Core.hpp>\n\n/* Compute the fourier transform of x. Store the result in output.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\nvoid fft(Kokkos::View<const Kokkos::complex<double>*> &x, Kokkos::View<Kokkos::complex<double>*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute the fourier transform of x. Store the result in output.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0]\n   output: [{4,0}, {1,-2.42421}, {0,0}, {1,-0.414214}, {0,0}, {1,0.414214}, {0,0}, {1,2.41421}]\n*/\n__global__ void fft(const cuDoubleComplex *x, cuDoubleComplex *output, size_t N) {\nint idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        cuDoubleComplex sum = {0, 0};\n        for (int k = 0; k < N; k++) {\n            cuDoubleComplex a = x[k];\n            cuDoubleComplex b = cuCmul(make_cuDoubleComplex(cos(2 * M_PI * idx * k / N), -sin(2 * M_PI * idx * k / N)), a);\n            sum = cuCadd(sum, b);\n        }\n        output[idx] = sum;\n    }\n}",
    "translation_function_name": "fft"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "03_dense_la_axpy",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {",
    "translation_prompt": "// CUDA implementation of axpy\n// /* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n//    Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n//    Example:\n//    \n//    input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n//    output: z=[2, -6, 5, 17]\n// */\n// __global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         z[i] = alpha * x[i] + y[i];\n//     }\n// }\n\n// Kokkos implementation of axpy\n#include <Kokkos_Core.hpp>\n\n/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\nvoid axpy(double alpha, Kokkos::View<const double*> &x, Kokkos::View<const double*> &y, Kokkos::View<double*> &z) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute z = alpha*x+y where x and y are vectors. Store the result in z.\n   Use CUDA to compute in parallel. The kernel is launched with at least as many threads as values in x.\n   Example:\n   \n   input: x=[1, -5, 2, 9] y=[0, 4, 1, -1] alpha=2\n   output: z=[2, -6, 5, 17]\n*/\n__global__ void axpy(double alpha, const double *x, const double *y, double *z, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        z[i] = alpha * x[i] + y[i];\n    }\n}",
    "translation_function_name": "axpy"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "02_dense_la_gemm",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {",
    "translation_prompt": "// CUDA implementation of gemm\n// /* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n//    A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n//    output: C=[[9, 5], [4, 2]]\n// */\n// __global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {\n// int row = blockIdx.x * blockDim.x + threadIdx.x;\n//     int col = blockIdx.y * blockDim.y + threadIdx.y;\n//     double sum = 0;\n//     for (int k = 0; k < K; k++) {\n//         sum += A[row * K + k] * B[k * N + col];\n//     }\n//     C[row * N + col] = sum;\n// }\n\n// Kokkos implementation of gemm\n#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\nvoid gemm(Kokkos::View<const double**> &A, Kokkos::View<const double**> &B, Kokkos::View<double**> &C, size_t M, size_t K, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Multiply the matrix A by the matrix B. Store the results in the matrix C.\n   A is an MxK matrix, B is a KxN matrix, and C is a MxN matrix. The matrices are stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an MxN grid of threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -2, 1]] B=[[4, 1], [-1, 0], [2, 2]]\n   output: C=[[9, 5], [4, 2]]\n*/\n__global__ void gemm(const double *A, const double *B, double *C, size_t M, size_t K, size_t N) {\nint row = blockIdx.x * blockDim.x + threadIdx.x;\n    int col = blockIdx.y * blockDim.y + threadIdx.y;\n    double sum = 0;\n    for (int k = 0; k < K; k++) {\n        sum += A[row * K + k] * B[k * N + col];\n    }\n    C[row * N + col] = sum;\n}",
    "translation_function_name": "gemm"
  },
  {
    "problem_type": "dense_la",
    "language": "cpp",
    "name": "04_dense_la_gemv",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {",
    "translation_prompt": "// CUDA implementation of gemv\n// /* Multiply the matrix A by the vector x. Store the results in the vector y.\n//    A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n//    Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n//    Example:\n// \n//    input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n//    output: y=[1, -3]\n// */\n// __global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {\n// int row = blockIdx.x * blockDim.x + threadIdx.x;\n//   if (row < M) {\n//     double sum = 0.0;\n//     for (int col = 0; col < N; ++col) {\n//       sum += A[row * N + col] * x[col];\n//     }\n//     y[row] = sum;\n//   }\n// }\n\n// Kokkos implementation of gemv\n#include <Kokkos_Core.hpp>\n\n/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix, x has N elements, and y has M elements.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\nvoid gemv(Kokkos::View<const double**> &A, Kokkos::View<const double*> &x, Kokkos::View<double*> &y, size_t M, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Multiply the matrix A by the vector x. Store the results in the vector y.\n   A is an MxN matrix stored in row-major, x has N elements, and y has M elements.\n   Use CUDA to compute in parallel. The kernel is launched with at least M threads.\n   Example:\n\n   input: A=[[1, -1, 2], [0, -3, 1]] x=[2, 1, 0]\n   output: y=[1, -3]\n*/\n__global__ void gemv(const double *A, const double *x, double *y, size_t M, size_t N) {\nint row = blockIdx.x * blockDim.x + threadIdx.x;\n  if (row < M) {\n    double sum = 0.0;\n    for (int col = 0; col < N; ++col) {\n      sum += A[row * N + col] * x[col];\n    }\n    y[row] = sum;\n  }\n}",
    "translation_function_name": "gemv"
  },
  {
    "problem_type": "sort",
    "language": "cpp",
    "name": "42_sort_sorted_ranks",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {",
    "translation_prompt": "// CUDA implementation of ranks\n// /* For each value in the vector x compute its index in the sorted vector.\n//    Store the results in `ranks`.\n//    Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n//    Examples:\n// \n//    input: [3.1, 2.8, 9.1, 0.4, 3.14]\n//    output: [2, 1, 4, 0, 3]\n//  \n//    input: [100, 7.6, 16.1, 18, 7.6]\n//    output: [4, 0, 1, 2, 3]\n// */\n// __global__ void ranks(const float *x, size_t *ranks, size_t N) {\n// size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (idx < N) {\n//         float val = x[idx];\n//         size_t rank = 0;\n//         for (size_t i = 0; i < N; ++i) {\n//             if (x[i] <= val) {\n//                 ++rank;\n//             }\n//         }\n//         ranks[idx] = rank - 1;\n//     }\n// }\n\n// Kokkos implementation of ranks\n#include <Kokkos_Core.hpp>\n\n/* For each value in the array x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\nvoid ranks(Kokkos::View<const float*> const& x, Kokkos::View<size_t*> &ranks) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* For each value in the vector x compute its index in the sorted vector.\n   Store the results in `ranks`.\n   Use CUDA to compute in parallel. The kernel will be launched with at least as many threads as elements in x.\n   Examples:\n\n   input: [3.1, 2.8, 9.1, 0.4, 3.14]\n   output: [2, 1, 4, 0, 3]\n \n   input: [100, 7.6, 16.1, 18, 7.6]\n   output: [4, 0, 1, 2, 3]\n*/\n__global__ void ranks(const float *x, size_t *ranks, size_t N) {\nsize_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        float val = x[idx];\n        size_t rank = 0;\n        for (size_t i = 0; i < N; ++i) {\n            if (x[i] <= val) {\n                ++rank;\n            }\n        }\n        ranks[idx] = rank - 1;\n    }\n}",
    "translation_function_name": "ranks"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "50_stencil_xor_kernel",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
    "translation_prompt": "// CUDA implementation of cellsXOR\n// /* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n//    Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n//    input and output are NxN grids of ints in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input: [[0, 1, 1, 0],\n//            [1, 0, 0, 0],\n//            [0, 0, 0, 0],\n//            [0, 1, 0, 0]\n//    output: [[0, 0, 1, 1],\n//             [1, 0, 0, 1],\n//             [0, 0, 1, 0],\n//             [1, 0, 1, 0]]\n// */\n// __global__ void cellsXOR(const int *input, int *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     int j = blockIdx.y * blockDim.y + threadIdx.y;\n// \n//     if (i < N && j < N) {\n//         int num_neighbors = 0;\n//         if (i > 0 && input[i - 1 + N * j] == 1) {\n//             num_neighbors++;\n//         }\n//         if (i < N - 1 && input[i + 1 + N * j] == 1) {\n//             num_neighbors++;\n//         }\n//         if (j > 0 && input[i + N * (j - 1)] == 1) {\n//             num_neighbors++;\n//         }\n//         if (j < N - 1 && input[i + N * (j + 1)] == 1) {\n//             num_neighbors++;\n//         }\n//         if (num_neighbors == 1) {\n//             output[i + N * j] = 1;\n//         } else {\n//             output[i + N * j] = 0;\n//         }\n//     }\n// }\n\n// Kokkos implementation of cellsXOR\n#include <Kokkos_Core.hpp>\n\n/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\nvoid cellsXOR(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Set every cell's value to 1 if it has exactly one neighbor that's a 1. Otherwise set it to 0.\n   Note that we only consider neighbors and not input_{i,j} when computing output_{i,j}.\n   input and output are NxN grids of ints in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[0, 1, 1, 0],\n           [1, 0, 0, 0],\n           [0, 0, 0, 0],\n           [0, 1, 0, 0]\n   output: [[0, 0, 1, 1],\n            [1, 0, 0, 1],\n            [0, 0, 1, 0],\n            [1, 0, 1, 0]]\n*/\n__global__ void cellsXOR(const int *input, int *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i < N && j < N) {\n        int num_neighbors = 0;\n        if (i > 0 && input[i - 1 + N * j] == 1) {\n            num_neighbors++;\n        }\n        if (i < N - 1 && input[i + 1 + N * j] == 1) {\n            num_neighbors++;\n        }\n        if (j > 0 && input[i + N * (j - 1)] == 1) {\n            num_neighbors++;\n        }\n        if (j < N - 1 && input[i + N * (j + 1)] == 1) {\n            num_neighbors++;\n        }\n        if (num_neighbors == 1) {\n            output[i + N * j] = 1;\n        } else {\n            output[i + N * j] = 0;\n        }\n    }\n}",
    "translation_function_name": "cellsXOR"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "54_stencil_game_of_life",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {",
    "translation_prompt": "// CUDA implementation of gameOfLife\n// /* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n//    A cell is 1 if it is alive and 0 if it is dead.\n//    If a live cell has fewer than 2 live neighbors then it dies.\n//    If a live cell has 2 or 3 live neighbors then it lives on.\n//    If a live cell has more than 3 live neighbords then it dies.\n//    If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input:  [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 0, 0, 0],\n//             [0, 1, 1, 0, 0],\n//             [0, 0, 1, 1, 0],\n//             [0, 1, 0, 0, 0]]\n//    output: [[0, 0, 0, 0, 0],\n// \t\t\t\t\t  [0, 1, 1, 0, 0],\n//             [0, 1, 0, 1, 0],\n//             [0, 0, 0, 1, 0],\n//             [0, 0, 1, 0, 0]]\n// */\n// __global__ void gameOfLife(const int *input, int *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//   int j = blockIdx.y * blockDim.y + threadIdx.y;\n// \n//   if (i < N && j < N) {\n//     int neighbors = 0;\n//     for (int x = -1; x <= 1; x++) {\n//       for (int y = -1; y <= 1; y++) {\n//         if (x == 0 && y == 0) {\n//           continue;\n//         }\n//         int neighbor_i = i + x;\n//         int neighbor_j = j + y;\n//         if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n//           continue;\n//         }\n//         neighbors += input[neighbor_i * N + neighbor_j];\n//       }\n//     }\n//     output[i * N + j] = input[i * N + j];\n//     if (input[i * N + j] == 1) {\n//       if (neighbors < 2 || neighbors > 3) {\n//         output[i * N + j] = 0;\n//       }\n//     } else {\n//       if (neighbors == 3) {\n//         output[i * N + j] = 1;\n//       }\n//     }\n//   }\n// }\n\n// Kokkos implementation of gameOfLife\n#include <Kokkos_Core.hpp>\n\n/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos is already initialized.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\nvoid gameOfLife(Kokkos::View<const int**> &input, Kokkos::View<int**> &output, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Simulate one generation of Game of Life on `input`. Store the results in `output`.\n   A cell is 1 if it is alive and 0 if it is dead.\n   If a live cell has fewer than 2 live neighbors then it dies.\n   If a live cell has 2 or 3 live neighbors then it lives on.\n   If a live cell has more than 3 live neighbords then it dies.\n   If a cell is dead and has exactly 3 live neighbors then it becomes alive.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input:  [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 0, 0, 0],\n            [0, 1, 1, 0, 0],\n            [0, 0, 1, 1, 0],\n            [0, 1, 0, 0, 0]]\n   output: [[0, 0, 0, 0, 0],\n\t\t\t\t\t  [0, 1, 1, 0, 0],\n            [0, 1, 0, 1, 0],\n            [0, 0, 0, 1, 0],\n            [0, 0, 1, 0, 0]]\n*/\n__global__ void gameOfLife(const int *input, int *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n  int j = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (i < N && j < N) {\n    int neighbors = 0;\n    for (int x = -1; x <= 1; x++) {\n      for (int y = -1; y <= 1; y++) {\n        if (x == 0 && y == 0) {\n          continue;\n        }\n        int neighbor_i = i + x;\n        int neighbor_j = j + y;\n        if (neighbor_i < 0 || neighbor_i >= N || neighbor_j < 0 || neighbor_j >= N) {\n          continue;\n        }\n        neighbors += input[neighbor_i * N + neighbor_j];\n      }\n    }\n    output[i * N + j] = input[i * N + j];\n    if (input[i * N + j] == 1) {\n      if (neighbors < 2 || neighbors > 3) {\n        output[i * N + j] = 0;\n      }\n    } else {\n      if (neighbors == 3) {\n        output[i * N + j] = 1;\n      }\n    }\n  }\n}",
    "translation_function_name": "gameOfLife"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "52_stencil_1d_jacobi_3-point_stencil",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {",
    "translation_prompt": "// CUDA implementation of jacobi1D\n// /* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n//    i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n//    Replace with 0 when reading past the boundaries of `input`.\n//    Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n//    Example:\n// \n//    input: [9, -6, -1, 2, 3]\n//    output: [1, 2/3, -5/3, 4/3, 5/3]\n// */\n// __global__ void jacobi1D(const double *input, double *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     if (i < N) {\n//         if (i == 0) {\n//             output[i] = (input[i] + input[i + 1]) / 2;\n//         } else if (i == N - 1) {\n//             output[i] = (input[i - 1] + input[i]) / 2;\n//         } else {\n//             output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n//         }\n//     }\n// }\n\n// Kokkos implementation of jacobi1D\n#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\nvoid jacobi1D(Kokkos::View<const double*> &input, Kokkos::View<double*> &output) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute one iteration of a 3-point 1D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its two neighbors and stored in the corresponding element of `output`.\n   i.e. output[i] = (input[i-1]+input[i]+input[i+1])/3\n   Replace with 0 when reading past the boundaries of `input`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [9, -6, -1, 2, 3]\n   output: [1, 2/3, -5/3, 4/3, 5/3]\n*/\n__global__ void jacobi1D(const double *input, double *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (i == 0) {\n            output[i] = (input[i] + input[i + 1]) / 2;\n        } else if (i == N - 1) {\n            output[i] = (input[i - 1] + input[i]) / 2;\n        } else {\n            output[i] = (input[i - 1] + input[i] + input[i + 1]) / 3;\n        }\n    }\n}",
    "translation_function_name": "jacobi1D"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "51_stencil_edge_kernel",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {",
    "translation_prompt": "// CUDA implementation of convolveKernel\n// __constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n// \n// /* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n//    the dot product of itself and its neighbors with the edge kernel.\n//    Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n//    imageIn and imageOut are NxN grayscale images stored in row-major.\n//    Store the output of the computation in imageOut.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input: [[112, 118, 141, 152],\n//            [93, 101, 119, 203],\n//            [45, 17, 16, 232],\n//            [82, 31, 49, 101]]\n//    output: [[255, 255, 255, 255],\n//             [255, 147, 0, 255],\n//             [36, 0, 0, 255],\n//             [255, 39, 0, 255]]\n// */\n// __global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {\n// // TODO: implement the kernel\n// }\n\n// Kokkos implementation of convolveKernel\n#include <Kokkos_Core.hpp>\n\nconst int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images.\n   Store the output of the computation in imageOut.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\nvoid convolveKernel(Kokkos::View<const int**> &imageIn, Kokkos::View<int**> &imageOut, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "__constant__ int edgeKernel[3][3] = {{-1, -1, -1}, {-1, 8, -1}, {-1, -1, -1}};\n\n/* Convolve the edge kernel with a grayscale image. Each pixel will be replaced with\n   the dot product of itself and its neighbors with the edge kernel.\n   Use a value of 0 for pixels outside the image's boundaries and clip outputs between 0 and 255.\n   imageIn and imageOut are NxN grayscale images stored in row-major.\n   Store the output of the computation in imageOut.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[112, 118, 141, 152],\n           [93, 101, 119, 203],\n           [45, 17, 16, 232],\n           [82, 31, 49, 101]]\n   output: [[255, 255, 255, 255],\n            [255, 147, 0, 255],\n            [36, 0, 0, 255],\n            [255, 39, 0, 255]]\n*/\n__global__ void convolveKernel(const int *imageIn, int *imageOut, size_t N) {\n// TODO: implement the kernel\n}",
    "translation_function_name": "convolveKernel"
  },
  {
    "problem_type": "stencil",
    "language": "cpp",
    "name": "53_stencil_2d_jacobi_5-point_stencil",
    "parallelism_model": "kokkos",
    "prompt": "#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {",
    "translation_prompt": "// CUDA implementation of jacobi2D\n// /* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n//    Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n//    i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n//    Replace with 0 when reading past the boundaries of `input`.\n//    `input` and `output` are NxN grids stored in row-major.\n//    Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n//    Example:\n// \n//    input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n//    output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n// */\n// __global__ void jacobi2D(const double *input, double *output, size_t N) {\n// int i = blockIdx.x * blockDim.x + threadIdx.x;\n//     int j = blockIdx.y * blockDim.y + threadIdx.y;\n//     if (i >= N || j >= N) {\n//         return;\n//     }\n//     int idx = i + j * N;\n//     double sum = 0;\n//     if (i > 0) {\n//         sum += input[idx - 1];\n//     }\n//     if (i < N - 1) {\n//         sum += input[idx + 1];\n//     }\n//     if (j > 0) {\n//         sum += input[idx - N];\n//     }\n//     if (j < N - 1) {\n//         sum += input[idx + N];\n//     }\n//     sum += input[idx];\n//     sum /= 5;\n//     output[idx] = sum;\n// }\n\n// Kokkos implementation of jacobi2D\n#include <Kokkos_Core.hpp>\n\n/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\nvoid jacobi2D(Kokkos::View<const double**> &input, Kokkos::View<double**> &output, size_t N) {\n",
    "translation_src_model": "cuda",
    "translation_dst_model": "kokkos",
    "translation_src_example": "/* Compute one iteration of a 5-point 2D jacobi stencil on `input`. Store the results in `output`.\n   Each element of `input` will be averaged with its four neighbors and stored in the corresponding element of `output`.\n   i.e. output_{i,j} = (input_{i,j-1} + input_{i,j+1} + input_{i-1,j} + input_{i+1,j} + input_{i,j})/5\n   Replace with 0 when reading past the boundaries of `input`.\n   `input` and `output` are NxN grids stored in row-major.\n   Use CUDA to compute in parallel. The kernel is launched on an NxN grid of threads.\n   Example:\n\n   input: [[3, 4, 1], [0, 1, 7], [5, 3, 2]]\n   output: [[1.4, 1.8, 2.4],[1.8, 3, 2.2], [1.6, 2.2, 2.4]]\n*/\n__global__ void jacobi2D(const double *input, double *output, size_t N) {\nint i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = blockIdx.y * blockDim.y + threadIdx.y;\n    if (i >= N || j >= N) {\n        return;\n    }\n    int idx = i + j * N;\n    double sum = 0;\n    if (i > 0) {\n        sum += input[idx - 1];\n    }\n    if (i < N - 1) {\n        sum += input[idx + 1];\n    }\n    if (j > 0) {\n        sum += input[idx - N];\n    }\n    if (j < N - 1) {\n        sum += input[idx + N];\n    }\n    sum += input[idx];\n    sum /= 5;\n    output[idx] = sum;\n}",
    "translation_function_name": "jacobi2D"
  }
]