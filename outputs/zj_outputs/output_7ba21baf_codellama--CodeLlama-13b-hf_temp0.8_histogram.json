[
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: implement this\n  //       use parallel_for, \n  //       atomic_fetch_add, and\n  //       std::placeholders\n\n  // 1. For each pixel in the image, add 1 to the bin at the corresponding index.\n  //    Use std::bind to create a lambda function with the correct bin index\n\n  // 2. Launch a parallel_for over the image. Use a lambda function that adds 1 to the bin at\n  //    the correct index\n\n}",
            "// Fill in code here\n }",
            "// Your code goes here\n\n  const size_t num_pixels = image.extent(0);\n  // Use parallel_for to loop over pixels and update bins\n  Kokkos::parallel_for(\"pixelCounts\", \n      Kokkos::RangePolicy<Kokkos::Rank<2>>(0, num_pixels, 1),\n      KOKKOS_LAMBDA(const int idx) {\n    // Get the pixel value and increment the corresponding bin\n    int value = image(idx);\n    bins(value) += 1;\n  });\n  // Call Kokkos::fence to ensure the loop completes before proceeding\n  Kokkos::fence();\n}",
            "// TODO: Implement this\n}",
            "// TODO\n}",
            "// Implement me\n}",
            "Kokkos::parallel_for(256, KOKKOS_LAMBDA(const int i) {\n    bins[i] = 0;\n  });\n  Kokkos::parallel_for(image.size(), [&] (const int i) {\n    const auto &value = image(i);\n    Kokkos::atomic_increment(&bins(value));\n  });\n}",
            "// TODO: write code\n}",
            "// Insert your code here.\n}",
            "// TODO: Implement this function\n  // Hint: Use the following Kokkos functions:\n  //   Kokkos::parallel_for\n  //   Kokkos::parallel_reduce\n\n}",
            "// Get the size of the image.\n   const int n = image.extent(0);\n   \n   // Declare a Kokkos parallel_for loop that uses a lambda function as the body.\n   Kokkos::parallel_for(\n     Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0, n),\n     KOKKOS_LAMBDA(int i) {\n       // Increment the number of times the intensity at index i appears in the image.\n       atomic_inc(bins(image(i)));\n     }\n   );\n }",
            "// Create a parallel_for lambda function:\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n        // Get the value of the pixel in image\n        int value = image(i);\n        // Increment the value in bins\n        Kokkos::atomic_fetch_add(&bins[value], 1);\n    });\n    // Wait for parallel_for to complete\n    Kokkos::fence();\n}",
            "// TODO: Implement this function\n }",
            "// Fill bins with zeros using Kokkos\n  Kokkos::parallel_for(\"zero_bins\", 256, KOKKOS_LAMBDA(const int i) {\n    bins[i] = 0;\n  });\n\n  // Count the number of times each value appears in the image.\n  // Use a parallel_reduce to do this.  Note that parallel_reduce requires\n  // an initial value for the reduction variable.\n  int num_pixels = Kokkos::parallel_reduce(\"count_pixels\", image.size(), 0,\n    [&](int i, int& num_pixels) {\n      bins[image[i]]++;\n      num_pixels++;\n  });\n  \n  // Wait for the parallel_reduce to finish\n  Kokkos::fence();\n \n  printf(\"Num pixels: %d\\n\", num_pixels);\n  \n  // Check to make sure that num_pixels matches the expected value\n  assert(num_pixels == 9);\n}",
            "// TODO: your code here\n}",
            "using mdrange_policy = Kokkos::MDRangePolicy<Kokkos::Rank<1>>;\n  using loop_policy = Kokkos::RangePolicy<Kokkos::Rank<1>>;\n  Kokkos::parallel_for(\"pixel_counts\", mdrange_policy(0, image.extent(0)), [&](const int i) {\n    Kokkos::atomic_add(&bins[image(i)], 1);\n  });\n  Kokkos::fence();\n}",
            "// Kokkos parallel for loop\n     Kokkos::parallel_for(image.size(), [&](const int i) {\n        const int grayScaleValue = image(i);\n        // atomic increment the element at the given index\n        Kokkos::atomic_increment(&bins(grayScaleValue));\n    });\n     // Kokkos must be explicitly told to synchronize before accessing the results\n     Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"pixelCounts\",\n    Kokkos::RangePolicy<Kokkos::Threads>(0, image.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      const int pixel = image(i);\n      Kokkos::atomic_increment(&bins(pixel));\n    }\n  );\n  Kokkos::fence();\n}",
            "/* Your code here */\n}",
            "/* YOUR CODE GOES HERE */\n }",
            "const size_t numPixels = image.extent(0);\n   \n   // TODO: Create a parallel_for with the range [0, numPixels)\n\n   // TODO: Use the parallel_for to fill the bins\n   //       bins[image[i]]++;\n   \n   Kokkos::fence(); // Force the parallel_for to complete before exiting\n}",
            "// TODO\n}",
            "// Implement this function\n}",
            "// Your code here.\n    Kokkos::parallel_for(\"pixelCounts\", 256, KOKKOS_LAMBDA(const int i) {\n        int count = 0;\n        for (int j = 0; j < image.size(); ++j) {\n            if (image[j] == i) {\n                count++;\n            }\n        }\n        bins(i) = count;\n    });\n    Kokkos::fence();\n}",
            "const size_t N = image.size();\n\n  // parallel code:\n  //  for(size_t i=0; i<N; ++i)\n  //    bins[image[i]] += 1;\n\n  // sequential code:\n  for (size_t i = 0; i < 256; i++)\n    bins[i] = 0;\n  for (size_t i = 0; i < N; i++)\n    bins[image[i]]++;\n}",
            "// Your code goes here\n\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum<int> > >(0, image.size()), [=] (int i) {\n    const size_t pixelValue = image[i];\n    Kokkos::atomic_increment(&bins[pixelValue]);\n  });\n}",
            "// Call your parallel kernel here\n   Kokkos::parallel_for(\n    \"pixelCounts\", \n    image.extent(0),\n    KOKKOS_LAMBDA(int i) {\n      bins[image[i]]++;\n    }\n   );\n   // Don't forget to flush the results\n   Kokkos::fence();\n }",
            "Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n     int val = image(i);\n     atomic_increment(bins[val]);\n   });\n}",
            "// Create a parallel_for lambda and call it\n  // Use a functor to do the work\n  // If the kernel does not need to read data from memory, use Kokkos::parallel_for\n  // If the kernel needs to read data from memory, use Kokkos::parallel_reduce\n  // If the kernel needs to write data to memory, use Kokkos::parallel_scan\n  // If the kernel needs to do both reads and writes, use Kokkos::parallel_scan\n  // If the kernel needs to read and write different locations in memory, use Kokkos::parallel_for with a lambda\n  Kokkos::parallel_for(image.extent(0), [&](int i) {\n    ++bins(image(i));\n  });\n\n  // Make sure Kokkos has finished all work before returning\n  Kokkos::fence();\n}",
            "Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum<size_t> > > range(0, image.extent(0));\n\n  Kokkos::parallel_reduce(\n    range,\n    KOKKOS_LAMBDA(const int i, size_t &update) {\n      const int pixel = image(i);\n      bins(pixel)++;\n    },\n    bins\n  );\n}",
            "// your implementation goes here\n}",
            "int n = image.extent(0);\n  Kokkos::View<int*, Kokkos::LayoutRight, Kokkos::Cuda> d_image(\"image\", n);\n  Kokkos::View<int*, Kokkos::LayoutRight, Kokkos::Cuda> d_bins(\"bins\", 256);\n\n  // initialize bins to 0\n  auto d_bins_host = Kokkos::create_mirror_view(d_bins);\n  Kokkos::deep_copy(d_bins_host, 0);\n  Kokkos::deep_copy(d_bins, d_bins_host);\n\n  // copy input image to device\n  auto image_host = Kokkos::create_mirror_view(image);\n  Kokkos::deep_copy(image_host, image);\n  Kokkos::deep_copy(d_image, image_host);\n\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, n),\n    [=] __device__ (int i) {\n      atomicAdd(&d_bins[d_image[i]], 1);\n    }\n  );\n\n  Kokkos::deep_copy(bins, d_bins);\n}",
            "// TODO: Implement the function.\n  int pixelCount = image.extent(0);\n\n  Kokkos::parallel_for(pixelCount, KOKKOS_LAMBDA(const int& i) {\n      // TODO: Use Kokkos::atomic_add to increment the count for each pixel\n      // in the image.\n  });\n\n  Kokkos::fence();\n}",
            "// TODO: FILL THIS IN\n}",
            "// Your code here!\n }",
            "// TODO\n}",
            "// use Kokkos parallel_for to count\n  Kokkos::parallel_for(\n    image.extent(0),\n    KOKKOS_LAMBDA(const int &i) {\n      // use Kokkos atomic_fetch_add to increment\n      Kokkos::atomic_fetch_add(&(bins[image(i)]), 1);\n    });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"PixelCounts\",\n    image.size(),\n    KOKKOS_LAMBDA(size_t i) {\n      bins[image[i]]++;\n    });\n  Kokkos::fence();\n}",
            "// Kokkos::parallel_for with lambda\n     // Kokkos::Experimental::Reduce::sum with lambda\n     Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int& i) {\n         Kokkos::Experimental::Reduce::sum(Kokkos::Experimental::Reduce::sum(bins(image(i))++, 1);\n     });\n }",
            "const int image_size = image.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image_size),\n    KOKKOS_LAMBDA(const int i) {\n    const int x = image(i);\n    Kokkos::atomic_increment(&(bins(x)));\n  });\n}",
            "using std::cout;\n  using std::endl;\n\n  Kokkos::RangePolicy<Kokkos::Rank<2>> policy(0, image.extent(0), 0, image.extent(1));\n\n  // Add code here to use Kokkos parallel_for to count the number of\n  // pixels in the image with each grayscale intensity.\n  // The resulting bins are stored in the view `bins`, which is a 1-d view\n  // with 256 elements.\n\n  Kokkos::parallel_for(policy, [&] (int x, int y) {\n    int value = image(x, y);\n    Kokkos::atomic_add(&bins(value), 1);\n  });\n\n  // Wait for the kernel to finish, if needed.\n  // This is needed if you are using a CUDA device and want to access the data\n  // after the kernel completes.\n  Kokkos::fence();\n}",
            "// TODO: fill in the implementation.\n\n  // Kokkos::parallel_for(\"\", 0, image.extent(0), KOKKOS_LAMBDA(const int i) {\n  //   // TODO: fill in the body of the parallel_for.\n  //   //\n  //   // Hint:\n  //   // - Use `bins.data()` to get a pointer to the raw data.\n  //   // - `bins` is a pointer to the first element.\n  //   // - `bins[x]` is the same as `*(bins+x)`.\n  //   // - `image[i]` is the ith pixel value.\n  // });\n}",
            "Kokkos::parallel_for(\n      \"pixelCounts\", image.extent(0),\n      KOKKOS_LAMBDA(size_t i) {\n        atomic_increment(&bins[image[i]]);\n      });\n\n    // Copy results from device to host\n    Kokkos::deep_copy(bins, bins);\n  }",
            "// TODO: Fill in the body of the function\n\n    Kokkos::parallel_for(\"pixelCounts\",\n        Kokkos::RangePolicy<Kokkos::Cuda>(0, image.extent(0)), [=] (const int& i) {\n            bins[image[i]]++;\n    });\n}",
            "// TODO\n}",
            "int N=image.extent(0);\n\n  Kokkos::parallel_for(\"pixelCounts\", N, KOKKOS_LAMBDA(const int i) {\n    // bins[image(i)] += 1;\n    Kokkos::atomic_fetch_add(&bins[image(i)], 1);\n  });\n  Kokkos::fence();\n}",
            "/* CODE GOES HERE */\n   Kokkos::parallel_for(\"Pixel Count\", image.extent(0), KOKKOS_LAMBDA (int i) {\n   bins[image(i)]++;\n   });\n}",
            "}",
            "Kokkos::parallel_for(\n     \"pixelCounts\",\n     1,\n     [&](int) {\n       for (size_t i = 0; i < image.extent(0); ++i) {\n         int value = image(i);\n         bins(value) += 1;\n       }\n     }\n   );\n   Kokkos::fence();\n }",
            "}",
            "int numPixels = image.extent(0);\n    auto c = Kokkos::View<size_t[256]>(\"bins\", 256);\n\n    Kokkos::parallel_for(\n        \"pixelCounts\",\n        Kokkos::RangePolicy<Kokkos::Rank<2>>(0, numPixels, 1),\n        KOKKOS_LAMBDA(const int i) {\n            int current_value = image(i);\n            Kokkos::atomic_increment<Kokkos::MemoryOrder::relaxed>(&c(current_value));\n        });\n    Kokkos::fence();\n    Kokkos::deep_copy(bins, c);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>((int64_t)0, (int64_t)image.extent(0)), KOKKOS_LAMBDA(const int64_t i) {\n         const int pixel_value = image(i);\n         bins(pixel_value) += 1;\n     });\n}",
            "// Use a Kokkos parallel for loop over the input array.\n  // This loop is run on the default Kokkos execution space\n  // (which is usually CUDA or OpenMP depending on how Kokkos has been\n  // configured)\n  Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int idx) {\n    // The index is available in the lambda as a const int idx.\n    // The value of the pixel in the image is image(idx).\n    // The bins to increment are passed as a const reference.\n    bins[image(idx)]++;\n  });\n\n  // The bins are not visible outside the Kokkos lambda.\n  // We need to explicitly copy them back to the host (CPU) memory,\n  // because the next section is not running inside Kokkos.\n  // This copy is fast because the memory layouts are compatible.\n  Kokkos::deep_copy(bins, bins);\n}",
            "// TODO: fill in your code here\n\n  int image_size = image.extent(0);\n  //std::cout << \"image size\" << image_size << std::endl;\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, image_size), KOKKOS_LAMBDA(const int i) {\n      //std::cout << \"i: \" << i << std::endl;\n      int idx = image(i);\n      //std::cout << \"idx: \" << idx << std::endl;\n      bins[idx]++;\n  });\n  Kokkos::fence();\n\n}",
            "// Your code here\n }",
            "// TODO: Add code here\n    \n}",
            "Kokkos::RangePolicy<Kokkos::Reduce, int> policy(0, image.extent(0));\n    Kokkos::parallel_reduce(policy, [&] (const int i, Kokkos::Sum<int> &counter) {\n      const int val = image(i);\n      Kokkos::atomic_increment(&(bins[val]));\n    });\n    Kokkos::fence();\n}",
            "// Fill in your solution here\n  Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA (int i) {\n    bins[image[i]]++;\n  });\n}",
            "int n=image.extent(0);\n   Kokkos::parallel_for(\"Histogram_loop\",n,KOKKOS_LAMBDA(const int i) {\n     bins[image[i]]++;\n   });\n   Kokkos::fence();\n }",
            "// TODO: Implement this function\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n   Kokkos::RangePolicy<execution_space> rp(0, image.extent(0));\n   Kokkos::parallel_for(\"pixelCounts\", rp, KOKKOS_LAMBDA(int const i) {\n     int pixel = image(i);\n     Kokkos::atomic_increment(&bins(pixel));\n   });\n }",
            "// YOUR CODE HERE\n    //\n    // Your code should:\n    //\n    // 1. Loop over `image` with a Kokkos parallel for loop.\n    // 2. For each pixel, increment the corresponding bin in `bins`.\n    //    For example, if the pixel is 127, the corresponding bin should\n    //    be incremented by 1.\n    // 3. Use Kokkos::atomic_fetch_add to perform an atomic increment of\n    //    the bin.\n}",
            "int num_pixels = image.size();\n\n   Kokkos::View<int*> bins_k(\"bins_k\", 256);\n   Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int) {\n     for (int i = 0; i < num_pixels; i++) {\n       bins_k(image[i])++;\n     }\n   });\n   Kokkos::fence();\n\n   Kokkos::deep_copy(bins, bins_k);\n }",
            "Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::Serial>, int> policy(0, image.extent(0));\n  Kokkos::parallel_reduce(\"pixelCounts\", policy, [&](int i, size_t& r) {\n    r[image(i)]++;\n  }, bins);\n  Kokkos::fence();\n}",
            "size_t N = image.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, N), [&](int i) {\n    const int value = image(i);\n    Kokkos::atomic_add(&bins(value), 1);\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, image.size()),\n    KOKKOS_LAMBDA (const int i) {\n     int value = image[i];\n     if (value >= 0 && value < 256)\n       bins[value]++;\n  });\n}",
            "Kokkos::View<size_t[256]> counts(\"counts\",256);\n  Kokkos::parallel_for(image.extent(0), [&](int i) {\n      counts[image[i]]++;\n  });\n  Kokkos::View<size_t[256]> counts_host(counts);\n  Kokkos::deep_copy(counts_host, counts);\n  for(size_t i=0; i<256; i++) {\n    bins[i] = counts_host[i];\n  }\n}",
            "const int N = image.extent(0);\n   Kokkos::parallel_for(N, KOKKOS_LAMBDA (int i) {\n    auto val = image(i);\n    auto idx = Kokkos::atomic_fetch_add(&bins(val), 1);\n   });\n}",
            "using range = Kokkos::pair<int, int>;\n  using execution_space = Kokkos::OpenMP;\n  Kokkos::parallel_for(range(0, image.size()), [&](const int i) {\n    const int val = image(i);\n    Kokkos::atomic_increment<execution_space>(&bins(val));\n  });\n}",
            "// TODO: implement this\n    \n    Kokkos::parallel_for( \"PixelCounts\", image.extent(0),\n      [=]( int i ) {\n        bins[image[i]]++;\n      }\n    );\n  }",
            "//...\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "/* YOUR CODE HERE */\n  /* Fill in the lambda expressions that define the Kokkos kernel. */\n  /* You can assume that bins.extent(0) == 256 */\n\n}",
            "Kokkos::parallel_for(\n    \"pixelCounts\",\n    Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::Cuda>>(0, image.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      // TODO\n    }\n  );\n  Kokkos::fence();\n}",
            "using Kokkos::All;\n   using Kokkos::RangePolicy;\n   using Kokkos::parallel_for;\n   using Kokkos::Experimental::UniqueToken;\n   using Kokkos::Experimental::UniqueTokenScope;\n\n   auto count = [=] (size_t const i, UniqueToken const& token) {\n      // TODO: fill this in\n   };\n   parallel_for(RangePolicy<UniqueToken>(0, image.extent(0)), count);\n   Kokkos::fence();\n }",
            "// Get Kokkos range policy, use all threads and all elements\n  Kokkos::RangePolicy<Kokkos::R",
            "// Your code here!\n\n}",
            "// Set the first bin to 0\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace>(0,1), KOKKOS_LAMBDA(const int& i) {\n       bins[0]=0;\n   });\n\n   // Add the number of pixels for each pixel value\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace>(1,256), KOKKOS_LAMBDA(const int& i) {\n       bins[i]=0;\n       for (int j=0; j<image.extent(0); j++) {\n           if (image(j)==i) {\n               bins[i]=bins[i]+1;\n           }\n       }\n   });\n}",
            "// Set bins to 0.\n    Kokkos::parallel_for(\n        \"set_to_zero\",\n        Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0,256),\n        KOKKOS_LAMBDA(const int i) {\n            bins(i) = 0;\n        });\n    // TODO: implement\n    // Hint: Use `Kokkos::atomic_fetch_add` to update bins.\n  Kokkos::parallel_for(\n        \"set_to_zero\",\n        Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0,image.extent(0)),\n        KOKKOS_LAMBDA(const int i) {\n          bins(image(i))++;\n      });\n}",
            "// Implement me!\n   int N=image.extent(0);\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0,N), KOKKOS_LAMBDA(const int i) {\n       bins(image(i)) += 1;\n   });\n}",
            "Kokkos::parallel_for(\n     Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceDataComm<int, int> > > (0, image.extent(0)),\n     [&](const int &i, int &tot) {\n       const int x = image(i);\n       Kokkos::atomic_increment(&bins[x]);\n       Kokkos::atomic_increment(&tot);\n     });\n }",
            "// Your code goes here\n }",
            "// Your code here\n  }",
            "size_t num_elements = image.extent(0);\n\n  Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> > policy(0, num_elements);\n  Kokkos::parallel_for(\"pixelCounts\", policy, KOKKOS_LAMBDA(int i) {\n    bins[image(i)] += 1;\n  });\n\n  // Wait for the kernel to finish\n  Kokkos::fence();\n}",
            "// Count the pixels in each grayscale intensity\n    // Use Kokkos parallel_for to do it in parallel\n\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int idx) {\n        bins[image(idx)] += 1;\n    });\n    Kokkos::fence();\n}",
            "}",
            "// This is an incomplete Kokkos program.\n  // You need to use the Kokkos::parallel_for or \n  // Kokkos::parallel_reduce function to implement the program.\n\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(const int& i) {\n         int value = image[i];\n         Kokkos::atomic_add( &bins[value], 1 );\n     });\n }",
            "/* YOUR CODE GOES HERE */\n  int count[256]={0};\n  for (auto i = 0; i < image.extent(0); i++) {\n    int pixel = image(i);\n    count[pixel]++;\n  }\n  Kokkos::deep_copy(bins, count);\n}",
            "using Kokkos::RangePolicy;\n  using Kokkos::parallel_for;\n\n  const size_t n = image.size();\n\n  auto count_pixels = [&] (const size_t& i) {\n    int index = image[i];\n    Kokkos::atomic_increment(&bins[index]);\n  };\n\n  parallel_for(RangePolicy<>(0, n), count_pixels);\n  Kokkos::fence();\n}",
            "Kokkos::RangePolicy<Kokkos::Rank<1>> policy(0, image.size());\n  Kokkos::parallel_for(\"pixelCounts\", policy, KOKKOS_LAMBDA(size_t i) {\n    bins[image[i]]++;\n  });\n  Kokkos::fence();\n}",
            "// TODO: insert code here\n   // For each pixel value in the image, increment the corresponding bin in bins.\n   // You will need to use a Kokkos parallel_for.\n   Kokkos::parallel_for(\"pixel_counts\", image.extent(0), KOKKOS_LAMBDA(const int i) {\n    // TODO: insert code here\n    });\n   // TODO: insert code here\n }",
            "// Write your code here\n\n     // Implement the following Kokkos parallel_for to count pixels of each\n     // grayscale intensity in bins.\n     Kokkos::parallel_for(\n         \"pixelCounts\",\n         Kokkos::RangePolicy<>(0, image.extent(0)),\n         KOKKOS_LAMBDA(const int i) {\n             int pixel = image(i);\n             Kokkos::atomic_increment(&bins[pixel]);\n         }\n     );\n     Kokkos::fence();\n}",
            "// Initialize bins to zero\n   Kokkos::deep_copy(bins, 0);\n\n   // Create the parallel loop\n   Kokkos::parallel_for(\n       \"pixelCounts\",\n       image.extent(0),\n       KOKKOS_LAMBDA(size_t i) {\n         auto& img = image(i);\n         bins[img]++;\n       });\n}",
            "// Define the parallel_for loop over the data.\n    // Use the RAJA loop syntax.\n\n    Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const size_t i) {\n      // Compute the index of the pixel grayscale intensity.\n      // The following is equivalent to: \n      //     const int& pixel_value = image(i);\n      //     const int idx = pixel_value;\n      const int idx = Kokkos::subview(image, i);\n\n      // Increment the count for the index using atomic_fetch_add.\n      // The following is equivalent to:\n      //     bins(idx) = bins(idx) + 1;\n      Kokkos::atomic_fetch_add(&bins(idx), 1);\n    });\n    // Force the parallel_for loop to complete before the next statement.\n    Kokkos::fence();\n }",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n\n   // TODO: Use parallel_for to write the pixel counts.\n\n   // TODO: Sync to make sure all threads have completed their work.\n}",
            "// Your code goes here\n\n }",
            "// TODO: fill this in\n }",
            "using std::placeholders::_1;\n  Kokkos::parallel_for(\n    \"pixelCounts\",\n    Kokkos::RangePolicy<>(0, image.size()),\n    KOKKOS_LAMBDA(const int& index) {\n      bins[image[index]] += 1;\n    }\n  );\n  Kokkos::fence();\n}",
            "// TODO: your code here\n    // use Kokkos::parallel_for to write a parallel kernel\n    // use Kokkos::atomic to increment the bin counts\n    // use Kokkos::deep_copy to copy bins back to host\n\n  Kokkos::parallel_for(image.extent(0), [&](int i) {\n    Kokkos::atomic_increment(&bins[image[i]]);\n  });\n\n  Kokkos::deep_copy(bins, bins);\n}",
            "// Replace this with your code.\n  Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA (int i) {\n    atomicAdd(&bins[image[i]], 1);\n  });\n  // End replace\n}",
            "// YOUR CODE HERE\n}",
            "// Your code goes here\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, image.size()),\n    [&](int i) {\n      int value = image(i);\n      if (value < 0 || value >= 256) {\n        printf(\"Image value %d is out of range [0, 255]\\n\", value);\n      }\n      Kokkos::atomic_increment(&bins[value]);\n    }\n  );\n\n  Kokkos::fence(); // block until parallel_for completes\n}",
            "// Your code goes here\n}",
            "// TODO: implement a Kokkos parallel_for loop to count the number of pixels for each grayscale intensity\n}",
            "// TODO: add Kokkos parallelism here\n}",
            "//...\n}",
            "const size_t image_size = image.extent(0);\n\n    /* TODO:\n      - Create a `Kokkos::View<size_t[256]>` `result` with Kokkos\n      - Using parallel for, fill result with the number of times each pixel value occurs in image\n      - Write results to bins using `Kokkos::deep_copy`\n    */\n\n  }",
            "// You can use a parallel_for with a lambda to loop over all pixels\n    Kokkos::parallel_for(image.size(), [&](int i) {\n      bins(image(i))++;\n    });\n    Kokkos::fence();\n }",
            "Kokkos::parallel_for(\"pixelCounts\", 256, KOKKOS_LAMBDA(size_t i) {\n     bins(i) = 0;\n   });\n   Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA(size_t i) {\n     bins(image(i))++;\n   });\n }",
            "// your implementation goes here\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(), KOKKOS_LAMBDA (const int i) {\n    size_t pixel = image(i);\n    bins(pixel) = bins(pixel) + 1;\n  });\n\n  // Force kernel to complete before return\n  Kokkos::fence();\n}",
            "const int n = image.extent(0);\n  const int m = image.extent(1);\n\n  // TODO: your code here\n\n}",
            "using ViewInt = Kokkos::View<int*>;\n  using ViewSize = Kokkos::View<size_t[256]>;\n  // TODO (optional): replace `ViewSize` with `ViewInt`, since `size_t` is\n  //                  equivalent to `int` in Kokkos.\n\n  // TODO: fill in the code to count the number of pixels with each intensity\n  //       in image.\n  // Hint: use Kokkos::parallel_for\n  // Hint: use Kokkos::atomic_add\n\n}",
            "// Implement this!\n}",
            "// TODO: Write your parallel kernel here\n\n }",
            "// TODO\n}",
            "const int N = image.extent(0);\n  // Your code here.\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, N),\n                       KOKKOS_LAMBDA(const int i) {\n                         bins[image(i)]++;\n                       });\n\n  Kokkos::fence();\n}",
            "// Your code here\n }",
            "// The actual computation\n  Kokkos::parallel_for(\"pixel_count\", image.extent(0), KOKKOS_LAMBDA(const int i) {\n    //...\n  });\n\n  // A Kokkos parallel_reduce\n  Kokkos::parallel_reduce(\"pixel_count\", image.extent(0),\n                          Kokkos::Sum<size_t>(0),\n                          KOKKOS_LAMBDA(const int i, size_t& sum) {\n    //...\n  });\n}",
            "// TODO: your code here\n     Kokkos::parallel_for(image.extent(0),\n     KOKKOS_LAMBDA(const int i) {\n         bins[image(i)] += 1;\n     });\n\n}",
            "}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,image.size()),\n        [&](int i) {\n           bins[image[i]]++;\n        }\n    );\n    Kokkos::DefaultExecutionSpace().fence();\n }",
            "// Fill in the code here.\n\n\n\n  }",
            "// your code here\n\n}",
            "// Your code here\n}",
            "/* Implementation */\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.size(),\n                       KOKKOS_LAMBDA(int i) {\n                         int pixel = image(i);\n                         size_t bin = bins(pixel);\n                         bins(pixel) = bin + 1;\n                       });\n  Kokkos::fence();\n}",
            "// YOUR CODE GOES HERE\n  \n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::R",
            "Kokkos::parallel_for(\n     \"PixelCounts\",\n     Kokkos::RangePolicy<Kokkos::Cuda>(0, image.extent(0)),\n     KOKKOS_LAMBDA(const int& idx) {\n       bins[image[idx]]++;\n     }\n   );\n}",
            "Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA (const int i) {\n    atomic_add(&bins[image(i)], 1);\n  });\n}",
            "// TODO\n}",
            "// Implement me!\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(const int& i) {\n        const auto pixel = image[i];\n        Kokkos::atomic_fetch_add(&bins[pixel], 1);\n    });\n    Kokkos::fence();\n}",
            "// TODO: Put your solution here\n\n   Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(const int idx) {\n     bins[image(idx)]++;\n   });\n}",
            "Kokkos::parallel_for(\n      \"pixelCounts\", image.extent(0),\n      KOKKOS_LAMBDA(const int i) {\n        const size_t& pixel = image(i);\n        Kokkos::atomic_increment(&(bins[pixel]));\n      });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(image.extent(0),\n    KOKKOS_LAMBDA(int i) {\n      bins(image(i)) += 1;\n    });\n}",
            "Kokkos::View<int*> bins_host(Kokkos::view_alloc(Kokkos::WithoutInitializing, bins));\n  Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(int i) {\n    Kokkos::atomic_increment(&bins_host[image[i]]);\n  });\n  Kokkos::fence();\n}",
            "// Implement me!\n }",
            "// TODO\n}",
            "Kokkos::parallel_for(\n     \"pixelCounts\",\n     image.extent(0),\n     KOKKOS_LAMBDA(size_t i) {\n         bins[image(i)]++;\n     });\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, image.size()),\n                       KOKKOS_LAMBDA(const size_t i) {\n    bins[image[i]]++;\n  });\n}",
            "// YOUR CODE GOES HERE\n }",
            "Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n     auto pixel = image(i);\n     Kokkos::atomic_increment(bins.data() + pixel);\n   });\n   Kokkos::fence();\n }",
            "// TODO: Implement me\n  // Hint: Use a parallel_for, possibly with a reduction\n  // Hint: You may need to use a lambda\n  // Hint: Kokkos::atomic_fetch_add can be useful here\n  Kokkos::parallel_for(\n   \"Count\", image.extent(0),\n   KOKKOS_LAMBDA(const int i) {\n    Kokkos::atomic_fetch_add(&bins[image[i]], 1);\n   });\n}",
            "// Create an array of atomic counter objects\n  Kokkos::View<Kokkos::atomic<size_t>[256]> bins_atomic(\"bins_atomic\");\n\n  // Create a parallel_for lambda that will count the pixels\n  Kokkos::parallel_for(image.extent(0), [=](int i) {\n    // Get the pixel value\n    int pixel_value = image(i);\n    // Increment the corresponding counter\n    Kokkos::atomic_increment(&bins_atomic(pixel_value));\n  });\n  // Copy the results to the return vector\n  Kokkos::parallel_for(256, [=](int i) {\n    bins(i) = bins_atomic(i);\n  });\n}",
            "Kokkos::View<size_t[256]> histogram(\"histogram\", 256);\n     Kokkos::parallel_for(\n         256, KOKKOS_LAMBDA(int i) {\n         histogram(i) = 0;\n     });\n\n     Kokkos::parallel_for(\n         image.extent(0), KOKKOS_LAMBDA(int i) {\n         int pixelValue = image(i);\n         histogram(pixelValue)++;\n     });\n\n     Kokkos::deep_copy(bins, histogram);\n }",
            "// Your code goes here.\n}",
            "// You'll need to use a Kokkos parallel_for here\n  Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::HostSpace>(0, image.extent(0)),\n      [&](const int i) {\n        bins(image(i)) += 1;\n      }\n    );\n\n  // And here\n  Kokkos::deep_copy(bins, bins);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0,image.extent(0)), KOKKOS_LAMBDA (const int i) {\n    int value = image(i);\n    Kokkos::atomic_increment(&bins(value));\n  });\n}",
            "// TODO: Fill this in\n\n  // Set the bins to zero\n  // TODO: Is there a better way to set all the values of the vector to zero?\n  for (size_t i = 0; i < bins.size(); i++)\n    bins(i) = 0;\n\n  // TODO: Create a Kokkos parallel for loop to count the pixels in the image\n\n\n  // TODO: Sync the results back to the CPU\n}",
            "// TODO: parallel code goes here.\n\n    // Initialize bins to all zeros.\n    auto bin_host = Kokkos::create_mirror_view(bins);\n    for (int i = 0; i < 256; ++i) {\n        bin_host(i) = 0;\n    }\n    Kokkos::deep_copy(bins, bin_host);\n\n    // TODO: write a Kokkos parallel_for that\n    // loops over every element of image and\n    // increments the corresponding bin in bins.\n\n    // Copy the result back to the host.\n    auto bins_host = Kokkos::create_mirror_view(bins);\n    Kokkos::deep_copy(bins_host, bins);\n}",
            "/* YOUR CODE HERE */\n  Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA (const int& i) {\n    bins[image(i)] += 1;\n  });\n\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\n      \"count\",\n      image.extent(0),\n      KOKKOS_LAMBDA(const int index) {\n        // Fill in this lambda to count the number of pixels with a given\n        // intensity in image. Store the result in bins[image[index]].\n      }\n  );\n  Kokkos::fence();\n}",
            "const int num_pixels = image.extent(0);\n\n    Kokkos::parallel_for(\"pixelCounts\", num_pixels, KOKKOS_LAMBDA(const int& i) {\n        const int pixel = image[i];\n        Kokkos::atomic_increment(&(bins[pixel]));\n    });\n\n    Kokkos::fence();\n}",
            "// TODO: fill in this function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n}",
            "// Fill this in\n }",
            "// Your code goes here\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n  Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(const int &i) {\n    auto value = image(i);\n    Kokkos::atomic_increment(&bins[value]);\n  });\n}",
            "/* YOUR CODE HERE */\n}",
            "// Count the number of pixels with grayscale value k in image[i]\n  // and store the result in bins[k].\n  // Do this in parallel.\n  // Hint: Use a parallel for loop.\n}",
            "// YOUR CODE HERE\n }",
            "Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>> policy(0,image.extent(0));\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA (const int idx) {\n    bins[image[idx]]++;\n  });\n}",
            "size_t image_size = image.extent(0);\n\n  Kokkos::View<size_t[256]> counts(\"counts\", 256);\n\n  // Initialize counts to zero\n  Kokkos::parallel_for(\n    \"initialize_counts\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin<256>>(0,256),\n    KOKKOS_LAMBDA(const int i) { counts[i] = 0; }\n  );\n\n  // Count the number of times each grayscale value occurs in the image\n  Kokkos::parallel_for(\n    \"count_pixels\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin<256>>(0,image_size),\n    KOKKOS_LAMBDA(const int i) { counts[image(i)]++; }\n  );\n\n  // Copy counts to bins\n  Kokkos::parallel_for(\n    \"copy_counts_to_bins\",\n    Kokkos::RangePolicy<Kokkos::RoundRobin<256>>(0,256),\n    KOKKOS_LAMBDA(const int i) { bins[i] = counts[i]; }\n  );\n\n  Kokkos::fence();\n}",
            "// Your code goes here!\n }",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(size_t i) {\n     int pixel_val = image(i);\n     Kokkos::atomic_increment(&bins(pixel_val));\n   });\n   Kokkos::fence();\n }",
            "Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const size_t i) {\n    bins(image(i))++;\n  });\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  const size_t n = image.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<ExecutionSpace>(0, n), KOKKOS_LAMBDA(const int i) {\n    const int x = image(i);\n    Kokkos::atomic_increment(&bins(x));\n  });\n  Kokkos::fence();\n}",
            "}",
            "// Your code here\n\n}",
            "for (int i = 0; i < image.extent(0); ++i) {\n    Kokkos::atomic_increment<Kokkos::View<size_t[256]> >(&bins[image[i]], 1);\n  }\n}",
            "const int N = image.extent(0);\n    Kokkos::parallel_for(\"Parallel for\", N, KOKKOS_LAMBDA(const int i) {\n        bins[image(i)] += 1;\n    });\n}",
            "// Set up parallel range in Kokkos and launch parallel lambda\n   Kokkos::parallel_for(\n     Kokkos::RangePolicy<Kokkos::HostSpace>(0, image.extent(0)),\n     [image, bins] (int i) {\n       const int pixel = image(i);\n       Kokkos::atomic_increment<Kokkos::HostSpace>(&bins(pixel));\n     }\n   );\n   Kokkos::fence();\n }",
            "Kokkos::parallel_for(256, KOKKOS_LAMBDA (const int& i) {\n        size_t count = 0;\n        for (int j = 0; j < image.extent(0); j++) {\n            if (image[j] == i)\n                count++;\n        }\n        bins[i] = count;\n    });\n\n    Kokkos::fence();\n}",
            "size_t n = image.extent(0);\n  size_t m = 256;\n  Kokkos::parallel_for(\n    \"PixelCounts\",\n    Kokkos::RangePolicy<Kokkos::Rank",
            "// TODO\n }",
            "// TODO: Implement this function\n     // For each pixel, increment the appropriate bin\n\n     Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::ThreadVectorRange, Kokkos::IndexType<unsigned>>(0, image.extent(0)), [&](const int pixel) {\n        const auto pixel_val = image(pixel);\n        const auto pixel_bin = bins(pixel_val);\n        bins(pixel_val) = pixel_bin + 1;\n     });\n     Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(int i) {\n     bins[image(i)] += 1;\n   });\n   Kokkos::fence();\n}",
            "// use `ParallelReduce` to count the number of pixels in each bin\n}",
            "}",
            "// Fill in code\n}",
            "// TODO: Implement pixelCounts using Kokkos\n \n }",
            "Kokkos::View<size_t[256]> counts(\"counts\", 256);\n\n    // your code here\n\n    Kokkos::deep_copy(bins, counts);\n}",
            "Kokkos::parallel_for(image.extent(0),\n                         [image, bins] (const int &index) {\n                            const int value = image(index);\n                            Kokkos::atomic_increment(&bins(value));\n                         });\n    Kokkos::fence();\n}",
            "// Your code here!\n     // Use parallel_for and a lambda to do the counting.\n }",
            "// TODO: Implement\n}",
            "// Use the Kokkos parallel for loop to initialize bins to 0\n    Kokkos::parallel_for(\"PixelCountsInit\", image.extent(0), KOKKOS_LAMBDA(int i) {\n        bins[image[i]]++;\n    });\n}",
            "// TODO: Implement this\n}",
            "// TODO\n\n     Kokkos::View<size_t*> bins_kokkos(bins.data(), bins.size());\n     // initialize the bins to zero\n     Kokkos::parallel_for(\"init_bins\", image.extent(0), KOKKOS_LAMBDA(const int& i) {\n         bins_kokkos[i] = 0;\n     });\n\n     Kokkos::parallel_for(\"count_pixels\", image.extent(0), KOKKOS_LAMBDA(const int& i) {\n         // TODO\n         bins_kokkos[image[i]] += 1;\n     });\n     Kokkos::fence();\n }",
            "// Write your code here.\n}",
            "const size_t n = image.extent(0);\n   Kokkos::RangePolicy<Kokkos::OpenMP> policy(0, n);\n   Kokkos::parallel_for(\"PixelCounts\", policy, KOKKOS_LAMBDA(const int i) {\n     bins[image(i)]++;\n   });\n   Kokkos::fence();\n }",
            "// Loop through the image\n  // Use a parallel_for over image to count the number of pixels with each grayscale intensity.\n  // Use a Kokkos reduction to compute the total count.\n\n}",
            "Kokkos::parallel_for(\"pixel_counts\", image.extent(0), KOKKOS_LAMBDA(const int idx) {\n    int color = image(idx);\n    Kokkos::atomic_increment(&(bins(color)));\n  });\n}",
            "using atomic_type = Kokkos::atomic<size_t>;\n  Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(const int i) {\n    atomic_type::add(bins.data(), image(i), 1);\n  });\n\n  Kokkos::fence();\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t n = image.size();\n  #pragma omp parallel\n  {\n    #pragma omp for schedule(dynamic)\n    for(size_t i = 0; i < n; ++i)\n    {\n      ++bins[image[i]];\n    }\n  }\n}",
            "for (int i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); ++i)\n        ++bins[image[i]];\n }",
            "#pragma omp parallel for\n     for (int i = 0; i < 256; i++){\n         bins[i] = 0;\n         for(size_t j = 0; j < image.size(); j++){\n             if (image[j] == i)\n                 bins[i]++;\n         }\n     }\n }",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++){\n    bins[image[i]] += 1;\n  }\n  return;\n}",
            "// TODO\n}",
            "// TODO: Use omp to count the number of occurrences in image of each value in\n  // the range [0, 255] into bins.\n  // See the example in the README and the OpenMP documentation for guidance.\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO\n   // - #pragma omp parallel for num_threads(omp_get_max_threads())\n   // - bins[intensity] += 1;\n   #pragma omp parallel for num_threads(omp_get_max_threads())\n   for (size_t i=0; i<image.size(); ++i)\n   {\n\t   bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for schedule(dynamic,1)\n   for (int i=0; i<256; i++){\n       bins[i] = 0;\n       for (size_t j=0; j<image.size(); j++){\n           if (image[j]==i){\n               bins[i]++;\n           }\n       }\n   }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; ++i) {\n        bins[i] = 0;\n        for (size_t j = 0; j < image.size(); ++j) {\n            if (image[j] == i) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "// TODO: Implement this function\n  bins.fill(0);\n\n  // Use the OpenMP for loop to count the number of pixels\n  // with each grayscale intensity.\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++)\n    bins.at(image.at(i))++;\n}",
            "#pragma omp parallel for schedule(dynamic, 256)\n    for(size_t i=0; i<image.size(); i++){\n        ++bins[image[i]];\n    }\n}",
            "int n = image.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    bins[image[i]]++;\n  }\n}",
            "// ****** You must replace this with your implementation ******\n\t#pragma omp parallel for\n\tfor(int i = 0; i < image.size(); i++)\n\t\tbins[image[i]]++;\n    // ************************************************************\n}",
            "for(size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO\n }",
            "#pragma omp parallel for schedule(static)\n   for (int i = 0; i < image.size(); i++){\n     bins[image[i]]++;\n   }\n }",
            "size_t N = image.size();\n   for(int i = 0; i < 256; ++i)\n   {\n     bins[i] = 0;\n   }\n\n   for (int i = 0; i < N; ++i)\n   {\n     #pragma omp atomic\n     bins[image[i]]++;\n   }\n}",
            "#pragma omp parallel\n  {\n  std::array<size_t, 256> local_bins;\n  #pragma omp for\n  for (int i = 0; i < image.size(); i++) {\n    local_bins[image[i]]++;\n  }\n\n  #pragma omp critical\n  for (int i = 0; i < 256; i++) {\n    bins[i] += local_bins[i];\n  }\n  }\n}",
            "int num_threads = omp_get_max_threads();\n   int *local_bins = new int[256];\n\n   #pragma omp parallel for num_threads(num_threads)\n   for (size_t i = 0; i < image.size(); i++) {\n     local_bins[image[i]] += 1;\n   }\n\n   for (size_t i = 0; i < bins.size(); i++) {\n     for (int j = 0; j < num_threads; j++) {\n       bins[i] += local_bins[i];\n     }\n   }\n\n   delete[] local_bins;\n }",
            "// Add your code here\n    #pragma omp parallel for\n    for(size_t i=0;i<image.size();i++){\n        int temp=image[i];\n        bins[temp]++;\n    }\n }",
            "// TODO: count pixels in parallel\n\n    // for (int i = 0; i < image.size(); ++i) {\n    //     bins[image[i]]++;\n    // }\n\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n\n}",
            "#pragma omp parallel for\n  for (auto& val: image) {\n    // std::cout << \"val: \" << val << std::endl;\n    bins[val]++;\n  }\n }",
            "// You can assume that image is a 1-dimensional vector of integers\n    // and that the pixel values are in the range 0-255\n\n    // For simplicity, only count the pixels in the first row\n    // TODO: you can ignore the rest of the image for now\n\n    // TODO: fill in this function\n }",
            "#pragma omp parallel\n\t{\n\t\tstd::array<size_t, 256> local_bins = { 0 };\n\t\t#pragma omp for\n\t\tfor (size_t i = 0; i < image.size(); ++i)\n\t\t\tlocal_bins[image[i]]++;\n\n\t\t#pragma omp critical\n\t\tfor (size_t i = 0; i < 256; ++i)\n\t\t\tbins[i] += local_bins[i];\n\t}\n}",
            "#pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// Fill in the code here\n}",
            "// TODO: Your code goes here\n    //#pragma omp parallel for\n    //for(int i=0;i<256;i++){\n    //    bins[i]=0;\n    //}\n    //#pragma omp parallel for\n    //for(int i=0;i<image.size();i++){\n    //    bins[image[i]]++;\n    //}\n\n    \n    #pragma omp parallel for\n    for (int i=0;i<bins.size();i++){\n        bins[i]=0;\n    }\n    #pragma omp parallel for\n    for (int i=0;i<image.size();i++){\n        bins[image[i]]++;\n    }\n}",
            "omp_set_num_threads(4);\n    int n = 256;\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        bins[i] = std::count(image.begin(), image.end(), i);\n    }\n}",
            "int n = image.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    bins[image[i]]++;\n  }\n}",
            "size_t const n = image.size();\n     // TODO: Your code here\n\n     omp_set_num_threads(8);\n     #pragma omp parallel for\n     for (size_t i = 0; i < n; i++) {\n         bins[image[i]]++;\n     }\n}",
            "// Use #pragma omp parallel for to parallelize for loop\n     #pragma omp parallel for\n     for(int i = 0; i < image.size(); i++){\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n   for(int i=0;i<image.size();i++)\n   {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i)\n    {\n        bins.at(image[i])++;\n    }\n\n    for (size_t i = 0; i < 256; ++i)\n    {\n        std::cout << i << \" \" << bins.at(i) << std::endl;\n    }\n\n    return;\n}",
            "//TODO: your code here\n   size_t len = image.size();\n\n   //#pragma omp parallel for\n   //for (int i = 0; i < len; ++i) {\n     //bins[image[i]]++;\n   //}\n   #pragma omp parallel for schedule(dynamic, 10)\n   for (int i = 0; i < len; ++i) {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); ++i) {\n         bins[image[i]]++;\n     }\n }",
            "}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i)\n    {\n        bins[image[i]]++;\n    }\n }",
            "#pragma omp parallel for schedule(dynamic, 10)\n    for (int i = 0; i < image.size(); i++) {\n        #pragma omp atomic update\n        bins.at(image.at(i))++;\n    }\n}",
            "for (auto &x : bins) {\n     x = 0;\n   }\n   #pragma omp parallel\n   {\n     #pragma omp for\n     for (int i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n     }\n   }\n }",
            "int size = image.size();\n     #pragma omp parallel for\n     for (int i = 0; i < size; i++) {\n         bins[image[i]]++;\n     }\n}",
            "for(int i = 0; i < bins.size(); i++) {\n     bins[i] = 0;\n   }\n   #pragma omp parallel for\n   for(int i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++)\n  {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]] += 1;\n  }\n}",
            "size_t const image_size = image.size();\n     #pragma omp parallel for\n     for (size_t pixel_index = 0; pixel_index < image_size; pixel_index++) {\n         auto const pixel_value = image[pixel_index];\n         #pragma omp atomic update\n         bins[pixel_value] += 1;\n     }\n }",
            "#pragma omp parallel for \n   for (int i=0; i<image.size(); i++) {\n     bins[image[i]] = bins[image[i]] + 1;\n   }\n}",
            "#pragma omp parallel for\n   for(int i=0; i<image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "// Add your code here\n    #pragma omp parallel for\n    for (int i=0; i<256; i++)\n    {\n        for (int j=0; j<image.size(); j++)\n        {\n            if (image[j] == i)\n            {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for schedule(static)\n    for (int i = 0; i < image.size(); ++i) {\n      bins[image[i]] += 1;\n    }\n  }\n}",
            "/* YOUR CODE GOES HERE */\n    // 1. Create a 2D array of 256 x 256\n    // 2. Store image data in the 2D array\n    // 3. Iterate over the 2D array and count the number of pixels\n    // 4. Store the number of pixels in bins\n    //\n    // 01\n    // 11\n    // 11\n    // 10\n    // 00\n\n    std::array<std::array<int, 256>, 256> arr;\n    for(auto i = 0; i < image.size(); i++)\n    {\n        arr[image[i]][i]++;\n    }\n\n    for(auto i = 0; i < 256; i++)\n    {\n        for(auto j = 0; j < 256; j++)\n        {\n            bins[i] = bins[i] + arr[i][j];\n        }\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        #pragma omp atomic update\n        bins[image[i]]++;\n    }\n}",
            "int i=0;\n\t\n\t#pragma omp parallel for default(none) shared(bins,image)\n\tfor (i = 0; i < image.size(); i++){\n\t\tbins[image[i]]++;\n\t}\n}",
            "// You code here\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++){\n        bins[image[i]] = bins[image[i]] + 1;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i=0; i<image.size(); i++){\n    bins[image[i]] += 1;\n  }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins.at(image.at(i))++;\n   }\n }",
            "}",
            "size_t const width = image.size();\n   for(size_t i=0; i<width; i++) {\n     bins[image[i]] += 1;\n   }\n   // TODO: Use OpenMP to count the values in parallel\n#pragma omp parallel for\n   for(int i=0; i < 256; i++){\n     bins[i] = 0;\n   }\n   //#pragma omp parallel for\n   //for(size_t i=0; i<width; i++) {\n   //  bins[image[i]] += 1;\n   //}\n }",
            "// YOUR CODE HERE\n     // Replace the following dummy line with your code\n     std::fill(bins.begin(), bins.end(), 0);\n }",
            "#pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int nt  = omp_get_num_threads();\n        int n_per_thread = image.size()/nt;\n        int start = tid*n_per_thread;\n        int end = (tid+1)*n_per_thread;\n\n        for (int i = start; i < end; i++){\n            bins[image[i]]++;\n        }\n    }\n}",
            "// TODO: implement this\n    int n = image.size();\n    omp_set_num_threads(omp_get_max_threads());\n    //#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]] += 1;\n    }\n }",
            "#pragma omp parallel for\n    for (int i=0; i<256; i++) {\n        size_t count = 0;\n        for (int j=0; j<image.size(); j++) {\n            if (image[j] == i)\n                count++;\n        }\n        bins[i] = count;\n    }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    bins[image[i]] += 1;\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for nowait\n        for (int i = 0; i < image.size(); ++i)\n        {\n            #pragma omp atomic\n            ++bins[image[i]];\n        }\n    }\n}",
            "// YOUR CODE HERE\n\n    // YOUR CODE HERE\n    // 1. Initialize the bins array to all zeros.\n    // 2. Use the omp_get_num_threads() function to determine how many threads are being used.\n    // 3. Loop over the image with the correct number of threads.\n    // 4. Use the omp_get_thread_num() function to determine which thread number you are in.\n    // 5. Update the bins array with the pixel value at index image[index].\n}",
            "// Replace this line with your code\n\tint nthreads = 1;\n\t#pragma omp parallel shared(nthreads)\n\t{\n\t\t#pragma omp single\n\t\t{\n\t\t\tnthreads = omp_get_num_threads();\n\t\t}\n\t}\n\tfor(int i=0;i<nthreads;i++)\n\t{\n\t\t#pragma omp parallel for\n\t\tfor(int j=0;j<image.size();j++)\n\t\t{\n\t\t\tbins[image[j]]++;\n\t\t}\n\t}\n }",
            "omp_set_num_threads(4);\n   int size = image.size();\n   int num_threads = 4;\n   int chunk = size / num_threads;\n\n   #pragma omp parallel\n   {\n     #pragma omp for\n     for (int i = 0; i < size; i++) {\n       bins[image[i]]++;\n     }\n   }\n\n}",
            "// TODO: Implement this function\n  // Do not modify the for loop, but you need to replace the code inside.\n  // You can also declare new variables and functions.\n  // To use OpenMP, write #pragma omp for\n\n  #pragma omp for\n  for(size_t i = 0; i < image.size(); i++)\n  {\n    bins[image[i]] += 1;\n  }\n}",
            "// Your code here.\n     #pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (int i=0; i<image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n }",
            "// TODO: Fill in your implementation here.\n     // Do not modify the loop index i or j.\n\n     // #pragma omp parallel for num_threads(4)\n     for(size_t i=0; i<image.size(); i++)\n     {\n        #pragma omp atomic\n        bins[image[i]]++;\n     }\n\n     // // omp_set_num_threads(4);\n     // #pragma omp parallel for num_threads(4)\n     // for(size_t i=0; i<image.size(); i++)\n     // {\n     //     #pragma omp atomic\n     //     bins[image[i]]++;\n     // }\n\n     // #pragma omp parallel for num_threads(4)\n     // for(size_t i=0; i<image.size(); i++)\n     // {\n     //     bins[image[i]]++;\n     // }\n\n     // int number_of_threads=omp_get_num_threads();\n     // std::cout << number_of_threads << std::endl;\n     // int number_of_procs=omp_get_num_procs();\n     // std::cout << number_of_procs << std::endl;\n\n     // int tid=omp_get_thread_num();\n     // std::cout << tid << std::endl;\n     // #pragma omp parallel num_threads(4)\n     // {\n     //     #pragma omp for\n     //     for(size_t i=0; i<image.size(); i++)\n     //     {\n     //         bins[image[i]]++;\n     //     }\n     // }\n\n     // #pragma omp parallel for num_threads(4)\n     // for(size_t i=0; i<image.size(); i++)\n     // {\n     //     bins[image[i]]++;\n     // }\n }",
            "// YOUR CODE HERE\n    \n\n    // Count the number of pixels in the image\n    // using a loop from 0 to 256\n    \n    // Fill the bins with the counts\n\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < image.size(); ++i) {\n            ++bins[image[i]];\n        }\n    }\n}",
            "#pragma omp parallel for\n   for(auto & i : bins) {\n     i = 0;\n   }\n   #pragma omp parallel for\n   for(auto x : image) {\n     #pragma omp atomic update\n     bins[x]++;\n   }\n }",
            "#pragma omp parallel for\n   for(int i=0; i<image.size();i++){\n      #pragma omp atomic\n      bins[image[i]]++;\n   }\n }",
            "size_t size = image.size();\n  std::vector<int> temp(size);\n\n#pragma omp parallel for shared(image, temp)\n  for (size_t i = 0; i < size; ++i) {\n    temp[i] = image[i];\n  }\n\n  for (size_t i = 0; i < size; ++i) {\n    bins[temp[i]]++;\n  }\n}",
            "// TODO: Add your code here",
            "size_t n = image.size();\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n   for(int i=0; i<image.size(); i++)\n   {\n      //omp_set_num_threads(8);\n      int pixel = image[i];\n      bins[pixel]++;\n   }\n}",
            "// TODO: YOUR CODE HERE.\n  #pragma omp parallel for schedule(static)\n   for(int i=0;i<image.size();i++){\n       bins[image[i]]++;\n   }\n }",
            "// You need to write the code for this function\n   // You can use the helper function isGrayscale\n   // If you get this wrong, you might not get many points.\n  \n   \n   // First set all the bins to 0.\n   //for (int i = 0; i < 256; ++i) {\n   //    bins[i] = 0;\n   //}\n   //\n   //\n   //\n   //// Now go through the image, incrementing the bin for each pixel\n   //for (int i = 0; i < image.size(); i++) {\n   //\n   //    // bins is the vector we are writing to, image is the vector we're reading from.\n   //    int value = image[i];\n   //    bins[value]++;\n   //}\n   //\n   //// This is just a sample solution, but not the right answer!\n   //// Don't copy this code.\n   //\n   //#pragma omp parallel for\n   //for (int i = 0; i < 256; ++i) {\n   //    bins[i] = 0;\n   //}\n   //\n   //#pragma omp parallel for\n   //for (int i = 0; i < image.size(); i++) {\n   //\n   //    // bins is the vector we are writing to, image is the vector we're reading from.\n   //    int value = image[i];\n   //    bins[value]++;\n   //}\n   \n   for (int i = 0; i < 256; ++i) {\n    bins[i] = 0;\n   }\n   \n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n\n    // bins is the vector we are writing to, image is the vector we're reading from.\n    int value = image[i];\n    bins[value]++;\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n     #pragma omp parallel\n     {\n         std::array<size_t, 256> private_bins;\n         private_bins.fill(0);\n         #pragma omp for\n         for (int i = 0; i < image.size(); ++i) {\n             private_bins[image[i]]++;\n         }\n         #pragma omp critical\n         for (int i = 0; i < 256; ++i) {\n             bins[i] += private_bins[i];\n         }\n     }\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < image.size(); ++i) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "// Your code here.\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); ++i)\n    bins[image[i]]++;\n}",
            "#pragma omp parallel\n   {\n     #pragma omp for\n     for (int i = 0; i < 256; ++i)\n     {\n       bins[i] = 0;\n     }\n     for (int j = 0; j < image.size(); ++j)\n     {\n       #pragma omp atomic\n       bins[image[j]]++;\n     }\n   }\n}",
            "bins.fill(0);\n\n   #pragma omp parallel for\n   for (int p=0; p<image.size(); p++) {\n     bins[image[p]]++;\n   }\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel\n     {\n\n         #pragma omp for\n         for (size_t i = 0; i < image.size(); i++) {\n             bins[image[i]]++;\n         }\n     }\n}",
            "/* YOUR CODE GOES HERE */\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); i++) {\n        for (auto const& val : image) {\n            if (val == i) {\n                #pragma omp atomic\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// Your code here\n    //#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        int pixel = image[i];\n        bins[pixel]++;\n    }\n}",
            "#pragma omp parallel\n\t{\n\t\t#pragma omp for\n\t\tfor (size_t i = 0; i < 256; ++i) {\n\t\t\tbins[i] = 0;\n\t\t\tfor (size_t j = 0; j < image.size(); ++j) {\n\t\t\t\tif (image[j] == i) {\n\t\t\t\t\tbins[i]++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n  for (auto i : image)\n    bins[i]++;\n}",
            "// Count bins sequentially\n   #pragma omp parallel for\n   for (auto const& pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "#pragma omp parallel for schedule(dynamic)\n  for (int i = 0; i < image.size(); ++i) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\nfor (int i = 0; i < image.size(); i++) {\n  bins[image[i]] += 1;\n}\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = std::count(image.begin(), image.end(), i);\n    }\n}",
            "#pragma omp parallel for\n  for (auto i = 0; i < image.size(); i++)\n    bins[image[i]] += 1;\n}",
            "// Add code here\n }",
            "}",
            "#pragma omp parallel\n   {\n     #pragma omp for\n     for (int i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n     }\n   }\n }",
            "#pragma omp parallel for schedule(static)\n  for (int i = 0; i < image.size(); i++) {\n    auto pixel = image[i];\n    bins[pixel]++;\n  }\n}",
            "// Implement this function\n }",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i=0; i<image.size(); ++i) {\n        bins[image[i]]++;\n    }\n\n}",
            "// your code goes here\n    bins = {};\n\n    omp_set_num_threads(10);\n    #pragma omp parallel for schedule(static, 1)\n    for (int i=0; i < image.size(); i++){\n        bins[image[i]] += 1;\n    }\n}",
            "// TODO: count the number of pixels with each intensity in parallel\n\n   // YOUR CODE HERE\n\n   // count\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    bins[image[i]] += 1;\n  }\n}",
            "size_t N = image.size();\n\n#pragma omp parallel for shared(bins)\n    for (int i = 0; i < N; i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n\t for (auto i = 0; i < image.size(); i++) {\n\t\t bins[image[i]] += 1;\n\t }\n }",
            "// You can access individual elements of bins using \n\t// bins[x], where x is the index of the element.\n\t// Example:\n\t//    bins[0] contains the number of pixels with grayscale value 0\n\n\t// Fill in this function...\n\n}",
            "// TODO\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n\n    return;\n}",
            "#pragma omp parallel\n{\n#pragma omp for\nfor (int i=0; i<image.size(); i++) {\n  bins[image[i]]++;\n}\n}\n\n\n}",
            "#pragma omp parallel for\n    for (int i=0; i<256; i++) {\n        for (int j=0; j<image.size(); j++) {\n            if (image[j] == i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n   for(size_t i = 0; i < image.size(); ++i){\n     ++bins[image[i]];\n   }\n }",
            "#pragma omp parallel for\n   for(int i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "size_t max_threads = omp_get_max_threads();\n    size_t chunk_size = (image.size() + max_threads - 1) / max_threads;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// YOUR CODE HERE\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n\n }",
            "#pragma omp parallel\n    {\n        #pragma omp for schedule(dynamic, 50)\n        for(int x = 0; x < image.size(); x++)\n        {\n            bins[image[x]] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < image.size(); ++i){\n        bins[image[i]] += 1;\n    }\n}",
            "//...\n}",
            "#pragma omp parallel for\n     for (int i=0; i<image.size(); i++) {\n     bins[image[i]]++;\n     }\n     \n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n  for (auto i=0; i < image.size(); i++) {\n   bins[image[i]]++;\n  }\n }",
            "int nthreads = omp_get_max_threads();\n  std::vector<std::array<size_t, 256>> localBins(nthreads);\n#pragma omp parallel for\n  for (int i = 0; i < image.size(); ++i) {\n    auto bin = localBins[omp_get_thread_num()];\n    bin[image[i]] += 1;\n    localBins[omp_get_thread_num()] = bin;\n  }\n  for (auto& bin : localBins) {\n    for (int i = 0; i < 256; ++i) {\n      bins[i] += bin[i];\n    }\n  }\n}",
            "// Fill bins with zeros\n   for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n   }\n   #pragma omp parallel for schedule(guided)\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n}",
            "// TODO: Fill in code to count the number of pixels in `image`\n  #pragma omp parallel for\n  for (size_t i=0; i<image.size(); ++i) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Implement me\n  #pragma omp parallel for\n  for (int i=0; i < image.size(); ++i)\n   {\n    bins[image[i]] += 1;\n   }\n}",
            "// TODO: count the number of pixels in image with each grayscale intensity\n\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Implement this function!\n   // Loop over all pixels in the image\n   //    * In parallel for all pixels\n   //    * Update the appropriate bin\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    int value = image[i];\n    bins[value]++;\n  }\n}",
            "#pragma omp parallel for schedule(dynamic,1)\n    for(size_t i = 0; i < image.size(); i++)\n        bins[image[i]]++;\n}",
            "// TODO: Fill this in\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 256> private_bins;\n\n#pragma omp for\n        for (size_t i = 0; i < image.size(); ++i) {\n            ++private_bins[image[i]];\n        }\n\n#pragma omp critical\n        for (size_t i = 0; i < 256; ++i) {\n            bins[i] += private_bins[i];\n        }\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n     for (size_t i = 0; i < image.size(); ++i) {\n         bins[image[i]]++;\n     }\n }",
            "int *data_ptr = image.data();\n     int data_size = image.size();\n\n     #pragma omp parallel for\n     for (int i=0; i < data_size; i++) {\n         bins[data_ptr[i]]++;\n     }\n }",
            "int num_threads = omp_get_num_threads();\n     int thread_num = omp_get_thread_num();\n     int size = image.size();\n     //std::cout << \"Number of threads = \" << num_threads << std::endl;\n     //std::cout << \"Number of thread = \" << thread_num << std::endl;\n     for (int i = 0; i < size; i++) {\n         bins[image[i]] += 1;\n     }\n }",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n}",
            "// TODO: Replace this with your code\n   int imageSize = image.size();\n   int nThreads = 0;\n   #pragma omp parallel\n   {\n      nThreads = omp_get_num_threads();\n      if(omp_get_thread_num() == 0) printf(\"Number of threads: %d\\n\", nThreads);\n      printf(\"Thread number: %d\\n\", omp_get_thread_num());\n   }\n   printf(\"Number of threads: %d\\n\", nThreads);\n\n   for(int i = 0; i < imageSize; i++)\n   {\n      int pixel = image[i];\n      #pragma omp atomic\n      bins[pixel]++;\n   }\n\n\n}",
            "#pragma omp parallel for\n  for (auto val : image) {\n    bins[val] += 1;\n  }\n}",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); ++i) {\n         bins[image[i]]++;\n     }\n }",
            "int N = image.size();\n  #pragma omp parallel for\n  for (int i=0; i<N; i++){\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n    }\n }",
            "int num_threads = omp_get_num_threads();\n    std::cout<<\"num_threads=\"<<num_threads<<std::endl;\n    \n    std::cout<<\"Before parallel region\"<<std::endl;\n    //std::array<size_t, 256> bins;\n    \n    #pragma omp parallel shared(bins)\n    {\n        std::cout<<\"In parallel region\"<<std::endl;\n        \n        #pragma omp for schedule(static,10)\n        for (int i = 0; i < image.size(); i++) {\n            int pixel = image[i];\n            bins[pixel]++;\n        }\n        std::cout<<\"After parallel region\"<<std::endl;\n    }\n    std::cout<<\"After parallel region\"<<std::endl;\n    \n}",
            "// TODO: replace with your code\n    std::fill(bins.begin(), bins.end(), 0);\n\n    //std::cout << \"image size \" << image.size() << std::endl;\n    //std::cout << \"bins size \" << bins.size() << std::endl;\n\n    //#pragma omp parallel for // use OpenMP to parallelise this for loop\n\n    //std::cout << \"omp_get_num_threads() \" << omp_get_num_threads() << std::endl;\n\n    #pragma omp parallel for\n    for (int i=0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (int i=0; i<image.size(); i++) {\n      bins[image[i]]++;\n    }\n  }\n}",
            "// TODO: count each grayscale intensity.\n\n#pragma omp parallel for\n     for(int i = 0; i < 256; i++) {\n         int count = 0;\n         for(auto pixel : image) {\n             if(pixel == i)\n                 count++;\n         }\n\n         bins[i] = count;\n     }\n}",
            "#pragma omp parallel for\n\tfor(size_t i=0; i<image.size(); ++i){\n\t\t++bins[image[i]];\n\t}\n}",
            "// YOUR CODE HERE\n     #pragma omp parallel for\n     for (int i = 0; i < image.size(); i++){\n         bins[image[i]]++;\n     }\n }",
            "for (size_t i = 0; i < 256; ++i) {\n     bins[i] = 0;\n   }\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); ++i) {\n     int const pixel = image[i];\n     bins[pixel]++;\n   }\n }",
            "//#pragma omp parallel for\n   for(auto i = 0; i < image.size(); i++){\n    bins[image[i]]++;\n   }\n}",
            "for (int i : image) {\n     #pragma omp atomic\n     bins[i]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++)\n  {\n\n    bins[image[i]]++;\n  }\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n\n}",
            "#pragma omp parallel for\n  for (int i=0; i<256; i++) {\n    bins[i]=0;\n  }\n  for (int i=0; i<image.size(); i++) {\n    #pragma omp atomic\n    bins[image[i]]++;\n  }\n}",
            "bins.fill(0);\n  \n   #pragma omp parallel for\n   for (size_t pixelIndex = 0; pixelIndex < image.size(); pixelIndex++)\n    {\n      bins[image[pixelIndex]]++;\n    }\n\n }",
            "// std::cout << \"Entered function\" << std::endl;\n   for (int i = 0; i < image.size(); ++i) {\n     // std::cout << \"i is: \" << i << \" and image is: \" << image[i] << std::endl;\n     ++bins[image[i]];\n   }\n }",
            "// Fill in this function\n\n}",
            "// TODO: implement this function\n}",
            "for (int i = 0; i < image.size(); ++i) {\n    #pragma omp parallel for\n        for (int j = 0; j < image.size(); ++j) {\n            bins[image[j]] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n }",
            "for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for shared(image, bins)\n    for (int i = 0; i < image.size(); i++) {\n        #pragma omp atomic\n        bins[image[i]] += 1;\n    }\n}",
            "size_t n = image.size();\n   #pragma omp parallel for\n   for (size_t i = 0; i < n; i++) {\n     int v = image[i];\n     bins[v] += 1;\n   }\n }",
            "#pragma omp parallel for\n   for (size_t i = 0; i < image.size(); ++i) {\n    // TODO: Count each image[i] and add to bins.\n  }\n}",
            "// Fill in code here\n    bins.fill(0);\n\n    int number_of_threads = 0;\n    #pragma omp parallel\n    {\n        #pragma omp single\n        {\n            number_of_threads = omp_get_num_threads();\n            std::cout << \"Using \" << number_of_threads << \" threads\\n\";\n        }\n        #pragma omp for\n        for (int i = 0; i < image.size(); i++) {\n            bins[image[i]] += 1;\n        }\n    }\n\n    // Print out the number of times each grayscale intensity appears in the image\n    for (int i = 0; i < 256; i++) {\n        std::cout << i << \": \" << bins[i] << std::endl;\n    }\n\n}",
            "// Your code goes here!\n  \n }",
            "for (int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n    \n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]] += 1;\n    }\n    \n}",
            "for(int i = 0; i < 256; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < image.size(); i++){\n        bins[image[i]] += 1;\n    }\n\n}",
            "for (size_t i = 0; i < 256; i++) {\n         bins[i] = 0;\n     }\n\n     #pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n\n\n\n }",
            "// CODE\n}",
            "// TODO: implement\n#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++){\n    bins[image[i]] = bins[image[i]] + 1;\n  }\n\n }",
            "int i = 0;\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: replace this code by parallel code that counts the pixels.\n   for (int i : image) {\n     bins[i]++;\n   }\n }",
            "#pragma omp parallel for\n     for (int i=0; i < image.size(); ++i) {\n         bins[image[i]] += 1;\n     }\n }",
            "int size = image.size();\n     int i;\n     int j;\n\n     #pragma omp parallel shared(bins) private(i, j)\n     {\n        #pragma omp for\n        for (i = 0; i < size; i++) {\n            j = image[i];\n            bins[j]++;\n        }\n     }\n }",
            "//...\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "bins.fill(0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: Use omp to count in parallel.\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n  // TODO: End parallel region.\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n     #pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         #pragma omp atomic\n         bins[image[i]]++;\n     }\n }",
            "// Initialize bins array\n    // Use OpenMP to initialize bins to zeros\n#pragma omp parallel for\n    for (size_t i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    \n    // Count the number of pixels with each grayscale intensity value.\n    // Store the results in bins[].\n    // Use OpenMP to count in parallel.\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "size_t n = image.size();\n  #pragma omp parallel for\n  for(size_t i = 0; i < n; i++) {\n    int gray = image[i];\n    bins[gray]++;\n  }\n }",
            "#pragma omp parallel for num_threads(2)\n\tfor (size_t i = 0; i < image.size(); i++) {\n\t\tbins.at(image.at(i)) += 1;\n\t}\n }",
            "// TODO: Fix this bug\n    #pragma omp parallel for\n    for(int pixel: image){\n        bins[pixel]++;\n    }\n}",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         bins[image[i]] += 1;\n     }\n}",
            "#pragma omp parallel for\n    for (int i=0; i<image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// YOUR CODE HERE\n   int size = image.size();\n   #pragma omp parallel for num_threads(4)\n   for (int i=0; i<size; i++){\n     int grayscale = image[i];\n     bins[grayscale]++;\n   }\n }",
            "int num_threads = omp_get_num_threads();\n  std::cout << \"There are \" << num_threads << \" threads.\" << std::endl;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++) {\n    int v = image[i];\n    bins[v]++;\n  }\n}",
            "int nr_of_threads = 4;\n    omp_set_num_threads(nr_of_threads);\n\n    // your code here\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for(size_t i=0; i<image.size(); i++){\n        bins[image[i]]++;\n    }\n }",
            "omp_set_num_threads(8);\n  #pragma omp parallel for\n  for(int x=0;x<image.size();x++)\n  {\n    bins[image[x]]++;\n  }\n }",
            "for (int i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n    \n    // TODO: add parallel for loop\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (auto pixel : image)\n    bins.at(pixel)++;\n}",
            "for (const auto& i : image) {\n        bins[i]++;\n    }\n}",
            "for (auto grayscalePixel : image) {\n    bins[grayscalePixel]++;\n  }\n}",
            "for (auto &pixel : image) {\n   bins[pixel]++;\n  }\n }",
            "for (int px : image)\n     ++bins[px];\n }",
            "// TODO: Replace this code with your solution.\n    // Loop over the input data and update the counts in the output vector\n    // bins.at(pixel_value) should contain the count of pixel values less than\n    // or equal to pixel_value.  In other words, if you were to plot the data\n    // in a histogram, the y-axis would be pixel_value and the x-axis would be\n    // the number of pixels with that value or lower.\n    //\n    // For example, if bins[i] contains the number of pixels with values 0-i,\n    // then bins[255] should contain the total number of pixels.\n    //\n    // bins[255] should equal image.size().\n    for(size_t i=0; i<image.size(); ++i)\n    {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: implement\n   for(int i = 0; i < image.size(); i++){\n   \tbins[image[i]]++;\n   }\n }",
            "for(auto i: image) {\n        bins[i]++;\n    }\n}",
            "for(size_t i = 0; i < image.size(); i++)\n    bins[image[i]]++;\n }",
            "for (int x : image) {\n        bins[x]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "std::array<size_t, 256> tmp;\n    for(size_t i = 0; i < image.size(); i++) {\n        tmp[image[i]]++;\n    }\n    for(size_t i = 0; i < 256; i++) {\n        bins[i] += tmp[i];\n    }\n}",
            "// TODO: Complete this function\n    for (auto i : image) {\n      bins[i]++;\n    }\n}",
            "for(int i=0; i<image.size(); i++)\n  {\n    bins.at(image.at(i))++;\n  }\n}",
            "// your code here\n\n    for (int num : image)\n    {\n        bins[num]++;\n    }\n\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins.at(image.at(i))++;\n    }\n}",
            "int size = image.size();\n\t int sum = 0;\n\t for (int i = 0; i < size; i++) {\n\t\t bins[image[i]]++;\n\t}\n}",
            "for (int i: image) {\n         bins[i]++;\n     }\n }",
            "for (int i : image) {\n        bins[i]++;\n    }\n}",
            "bins = {0}; // initialise to 0\n    for (auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "for (auto i : image) {\n        bins[i]++;\n    }\n}",
            "for(int x : image){\n        bins[x]++;\n    }\n}",
            "// TODO: implement\n  \n  for (int i=0; i<image.size(); i++) {\n    bins[image[i]] = bins[image[i]] + 1;\n  }\n}",
            "// TODO: Implement this function\n    size_t numPixels = image.size();\n    for(int i = 0; i < numPixels; i++){\n        int pixelValue = image.at(i);\n        bins.at(pixelValue)++;\n    }\n}",
            "for (int i : image)\n     ++bins[i];\n }",
            "for(int& i: image)\n        ++bins[i];\n}",
            "for(int i:image){\n     bins[i]++;\n   }\n   return;\n}",
            "for (auto pixel:image) {\n         bins[pixel]++;\n     }\n }",
            "size_t n = image.size();\n    for (int i = 0; i < n; i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: COMPLETE THIS FUNCTION\n\n\tbins.fill(0);\n\n\tfor (auto& gray : image) {\n\t\tbins[gray]++;\n\t}\n}",
            "for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    std::for_each(image.begin(), image.end(), [&bins](int value) {\n        ++bins[value];\n    });\n}",
            "// TODO: your code goes here\n    for(int i = 0; i < image.size(); i++){\n        bins[image[i]] += 1;\n    }\n}",
            "for (auto i : image) {\n        bins[i]++;\n    }\n}",
            "// TODO\n}",
            "for(auto const& i : image) {\n     ++bins[i];\n   }\n }",
            "for (auto &pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "for(auto &pixel: image) {\n        ++bins[pixel];\n    }\n}",
            "for(int &value: image){\n        bins.at(value)++;\n    }\n}",
            "for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for(auto const &x : image)\n        bins[x]++;\n}",
            "for (auto pixel: image) {\n     bins[pixel] += 1;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "// TODO\n\n  }",
            "bins.fill(0);\n    for (auto v : image) {\n        ++bins[v];\n    }\n}",
            "// TODO: write the implementation here\n   return;\n}",
            "// TODO: Your code here\n  // 1st pass to get the count of each intensity\n  for (size_t i = 0; i < image.size(); i++){\n    bins[image[i]] += 1;\n  }\n  return;\n}",
            "for (int i=0; i<image.size(); i++) {\n    int grayscale = image[i];\n    bins.at(grayscale) = bins.at(grayscale) + 1;\n  }\n}",
            "/* ======= STUDENT PART START ======= */\n    bins.fill(0);\n    for(auto i : image)\n    {\n        bins[i]++;\n    }\n    /* ======= STUDENT PART START ======= */\n}",
            "// write your code here\n   for (int x : image) {\n     bins[x]++;\n   }\n }",
            "bins.fill(0);\n     for (size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "for (const int & value : image) {\n        bins[value]++;\n    }\n}",
            "for (int x : image) {\n     bins[x]++;\n   }\n }",
            "// TODO: Replace this with your code\n\n    bins.fill(0);\n    for (const auto& val : image)\n    {\n        ++bins[val];\n    }\n    \n}",
            "// TODO: implement\n    //...\n}",
            "for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++){\n        bins[image[i]] += 1;\n    }\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "for (size_t i{0}; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < 256; i++) {\n       bins.at(i) = 0;\n    }\n\n    for (int i = 0; i < image.size(); i++) {\n       bins.at(image.at(i))++;\n    }\n }",
            "for (const auto& i: image) {\n     bins[i]++;\n   }\n }",
            "// Count the number of pixels in image with each grayscale intensity.\n    // The vector image is a grayscale image with values 0-255.\n    // Store the results in bins.\n    // Example:\n    // \n    // input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n    // output: [0, 0, 2, 0, 1,...]\n    \n    for(int i = 0; i < image.size(); i++){\n        \n        bins[image[i]]++;\n        \n    }\n    \n}",
            "// Your code here\n\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "for (auto i : image) {\n\t\t bins[i] += 1;\n\t }\n}",
            "// TODO: Replace the following with your code\n    for (int i = 0; i < image.size(); ++i){\n        bins[image[i]]++;\n    }\n\n    //std::array<size_t, 256> bins;\n    //for (int i = 0; i < 256; i++){\n    //    bins[i] = 0;\n    //}\n    //for (int i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n}",
            "// TODO: Implement\n}",
            "for(int pixel : image)\n        bins[pixel]++;\n}",
            "for (auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "for(int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n   }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Implement this function\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: replace with your code here.\n\n   for (int val : image)\n     bins[val]++;\n\n}",
            "size_t const n = image.size();\n     for (size_t i = 0; i < n; ++i) {\n         bins[image[i]]++;\n     }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n}",
            "bins.fill(0);\n    for (auto it = image.begin(); it!= image.end(); it++) {\n        bins[*it]++;\n    }\n}",
            "// TODO: your code here\n    for(int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for(int i = 0; i < image.size(); i++) {\n         bins.at(image.at(i))++;\n     }\n }",
            "for (auto i : image)\n   bins[i]++;\n}",
            "// TODO: implement me\n  for (auto it = image.begin(); it!= image.end(); it++) {\n    bins[*it]++;\n  }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++){\n      if (image[i] >= 0 && image[i] <= 255){\n        bins[image[i]] = bins[image[i]] + 1;\n      }\n    }\n\n    /*\n     * Your code here\n     */\n\n  }",
            "for (const int& pixel: image) {\n        bins.at(pixel)++;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n\t\t bins[image[i]] += 1;\n\t}\n }",
            "// TODO:\n   for(auto value: image){\n   // std::cout<<value;\n    bins[value]++;\n   }\n  }",
            "for (size_t i = 0; i < bins.size(); i++) {\n     bins[i] = 0;\n   }\n\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n    for (auto const& pixel: image) {\n        bins[pixel] += 1;\n    }\n}",
            "// Count the number of pixels with each grayscale intensity\n  for (auto const& pixel : image) {\n    bins[pixel] += 1;\n  }\n}",
            "// TODO\n  for (int& val: image) {\n    bins.at(val)++;\n  }\n}",
            "for(int i: image)\n   {\n     bins[i]++;\n   }\n}",
            "// COMPLETED (C1)\n\n    // Count number of pixels for each gray level.\n    for (int pixel: image) {\n        bins[pixel]++;\n    }\n\n    return;\n}",
            "for (int px : image)\n     ++bins[px];\n }",
            "for (int i: image)\n     bins[i]++;\n }",
            "for (auto const& pixel : image) {\n    // Insert count of this pixel in the appropriate bin\n    bins[pixel]++;\n  }\n}",
            "//std::array<size_t, 256> bins = {0};\n    size_t b = 0;\n\n    for (int x : image) {\n        if (x >= 0 && x < 256) {\n            bins[x]++;\n        }\n    }\n    return bins;\n\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n\n        bins[image[i]]++;\n    }\n}",
            "// TODO: count the number of pixels with each grayscale value in `image`\n  //   and store the results in the vector `bins`.\n  //   Note: don't forget to initialize `bins` to 0.\n  //   Hint: see the std::fill function.\n  // TODO: this function should take O(1) time.\n  std::fill(bins.begin(), bins.end(), 0);\n  for(int i = 0; i < image.size(); i++)\n  {\n    bins[image[i]] += 1;\n  }\n  return;\n}",
            "// Your code here.\n}",
            "for (auto& i: image)\n    bins[i]++;\n }",
            "bins.fill(0);\n\t for (size_t i = 0; i < image.size(); i++) {\n\t\t bins[image[i]]++;\n\t }\n }",
            "// Fill me in\n    // You can use the `++` operator to increment an array element\n    // e.g. `bins[42]++` would increase the element at index 42 by 1\n    // You can use `{}` to initialize an array, e.g. `std::array<size_t, 256> bins = { 0, 0, 0,... }`\n\n\n    for(int i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n }",
            "for (int i : image){\n         bins[i]++;\n     }\n }",
            "for(int pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "// YOUR CODE HERE\n  for (auto pixel : image) {\n    bins[pixel]++;\n  }\n}",
            "// std::array<size_t, 256> bins; // declare array of length 256\n   for (int i = 0; i < image.size(); i++)\n    bins[image[i]]++; // increment bin corresponding to image value\n}",
            "for (auto i = 0u; i < bins.size(); ++i)\n        bins[i] = 0u;\n\n    for (auto j = 0u; j < image.size(); j++) {\n        bins[image[j]]++;\n    }\n}",
            "for (int i : image) {\n        bins[i]++;\n    }\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "for (int gray_value : image) {\n        bins[gray_value]++;\n    }\n}",
            "// TODO: Implement this function\n}",
            "// IMPLEMENT THIS!\n    \n    \n    for (auto pixel : image) {\n        bins[pixel] += 1;\n    }\n\n\n    \n}",
            "// TODO\n\n }",
            "for (int pixel: image) {\n   bins.at(pixel)++;\n  }\n}",
            "for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n    }\n  }",
            "// TODO: YOUR CODE HERE\n \n}",
            "// write your code here\n    for (auto &pixel : image)\n    {\n        bins[pixel]++;\n    }\n}",
            "for(int i=0; i < image.size(); i++)\n  {\n    bins[image[i]]++;\n  }\n}",
            "for (int i : image) {\n\t\t++bins[i];\n\t}\n}",
            "for (auto &i : image)\n   bins[i]++;\n}",
            "for (size_t i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "for(size_t i = 0; i < image.size(); i++)\n   {\n     bins[image[i]]++;\n   }\n}",
            "for (auto element : image) {\n        bins.at(element)++;\n    }\n}",
            "//TODO:\n}",
            "for (auto i = 0; i < image.size(); i++) {\n     // use the value at index i in image as the index into bins\n     // increment the value at that index in bins\n   }\n }",
            "size_t i;\n   for (i=0; i<image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n    for (auto& pixel : image) {\n        bins.at(pixel) += 1;\n    }\n}",
            "for (int px : image) {\n        bins[px] += 1;\n    }\n}",
            "// TODO: fill this in\n }",
            "// TODO\n\t bins = { 0 };\n\t for (auto x : image)\n\t {\n\t\t bins[x] += 1;\n\t }\n }",
            "}",
            "// TODO: Count pixels\n   \n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n     // Implement your solution here\n\n     // Initialize all bins to zero\n     for (int i = 0; i < 256; ++i) {\n         bins[i] = 0;\n     }\n     // count the frequency of each pixel intensity\n     for (int i : image) {\n         bins[i]++;\n     }\n}",
            "for (int &v : image) {\n         ++bins[v];\n     }\n }",
            "for (int i=0;i<image.size();i++){\n\t\tbins[image[i]]++;\n\t}\n}",
            "bins.fill(0);\n    for (auto elem : image)\n        ++bins[elem];\n}",
            "for (int i = 0; i < image.size(); i++)\n\t{\n\t\tbins[image[i]] += 1;\n\t}\n}",
            "for (int value : image) {\n        ++bins[value];\n    }\n}",
            "size_t n = image.size();\n     for(size_t i = 0; i < n; ++i) {\n         bins[image[i]]++;\n     }\n }",
            "bins = std::array<size_t, 256>{0};\n    for (auto const &item : image) {\n    bins[item] += 1;\n  }\n}",
            "for (int i=0; i< image.size(); i++){\n      bins[image[i]]++;\n    }\n  }",
            "for (int pixel : image) {\n    bins[pixel]++;\n  }\n}",
            "for (size_t i = 0; i < image.size(); ++i)\n\t{\n\t\tbins[image[i]]++;\n\t}\n}",
            "size_t nPixels = image.size();\n   std::fill(bins.begin(), bins.end(), 0);\n   for (size_t i = 0; i < nPixels; ++i) {\n       bins[image[i]]++;\n   }\n}",
            "for (auto i: image) {\n     bins[i]++;\n   }\n}",
            "for (auto pixel : image) {\n         bins[pixel]++;\n     }\n}",
            "for (auto pix : image) {\n    bins[pix]++;\n  }\n  // TODO: Your code here\n  return;\n}",
            "for(int i = 0; i < image.size(); i++)\n  {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: write your implementation here\n   \n}",
            "/* YOUR CODE GOES HERE */\n}",
            "// Fill in code\n}",
            "bins = std::array<size_t, 256>{};\n    for (auto const& grayValue : image) {\n        ++bins[grayValue];\n    }\n}",
            "bins.fill(0);\n     for (auto i : image) {\n         bins[i] += 1;\n     }\n }",
            "for (auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "//...\n }",
            "for (int i = 0; i < image.size(); i++)\n    {\n      bins[image[i]] += 1;\n    }\n}",
            "for (int &i : image) {\n        bins[i]++;\n    }\n}",
            "for (int i : image) {\n        bins[i]++;\n    }\n}",
            "for (const auto& el : image) {\n        bins[el]++;\n    }\n}",
            "for (int &val : image) {\n         bins[val] += 1;\n     }\n }",
            "for (auto const& px : image) {\n        ++bins[px];\n    }\n}",
            "// YOUR CODE HERE\n}",
            "for (int pixel : image) {\n\t\t bins[pixel]++;\n\t }\n }",
            "for (int i : image) {\n    bins.at(i)++;\n  }\n}",
            "for(auto i : image) {\n        bins[i]++;\n    }\n}",
            "for (int i : image) {\n        bins[i]++;\n    }\n}",
            "// COMPLETAR\n    for (auto pixel: image) {\n        bins[pixel]++;\n    }\n}",
            "}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// Replace this with your code\n    std::array<size_t, 256> result;\n    result.fill(0);\n    \n    for (int i : image) {\n        result[i]++;\n    }\n    \n    bins = result;\n}",
            "for(size_t i = 0; i < image.size(); ++i){\n    ++bins[image[i]];\n  }\n}",
            "for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "}",
            "for (auto &i:image)\n        bins[i]++;\n}",
            "int pixel;\n    for (int i = 0; i < image.size(); i++) {\n        pixel = image[i];\n        bins[pixel] += 1;\n    }\n\n}",
            "// TODO: Implement this function\n    for(int grayScale: image)\n    {\n        bins[grayScale]++;\n    }\n }",
            "// Implement this function...\n   int pixel = 0;\n   for (int i=0; i < image.size(); i++)\n   {\n     pixel = image[i];\n     bins[pixel] = bins[pixel] + 1;\n   }\n   \n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (int i = 0; i < image.size(); ++i) {\n    ++bins.at(image.at(i));\n  }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: complete this function\n}",
            "}",
            "// YOUR CODE HERE\n     // Hint: Use std::array<size_t, 256> for the array of bins, instead of the std::array of size 256.\n }",
            "// TODO: Add your code here\n\n}",
            "// YOUR CODE HERE\n}",
            "for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "std::array<size_t, 256> bins = {0};\n     \n     for (auto const pixel : image){\n         bins[pixel]++;\n     }\n }",
            "bins.fill(0);\n    for (auto const& pixel: image) {\n        bins[pixel]++;\n    }\n}",
            "for (int i : image) {\n\t\t\tbins[i]++;\n\t\t}\n  }",
            "// TODO: Implement this function\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO:\n \n \tfor(int i=0; i<image.size(); i++){\n \t\tbins[image[i]] += 1;\n \t}\n \n }",
            "for (const auto& pixel : image) {\n        bins.at(pixel) += 1;\n    }\n}",
            "for(int i = 0; i < image.size(); i++)\n  {\n    bins[image[i]] = bins[image[i]] + 1;\n  }\n }",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Fill the bins\n     for (int pix : image)\n         bins[pix]++;\n }",
            "// TODO\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n         ++bins[image[i]];\n     }\n }",
            "// TODO: Fill bins with the number of pixels in image with each grayscale intensity.\n    \n    for (size_t i = 0; i < image.size(); i++)\n    {\n        bins.at(image.at(i))++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t tid = blockIdx.x*blockDim.x+threadIdx.x;\n    if(tid < N)\n        atomicAdd(&bins[image[tid]], 1);\n}",
            "int bin = image[threadIdx.x + blockIdx.x * blockDim.x];\n  atomicAdd(&bins[bin], 1);\n}",
            "//\n  // Your code here:\n  //\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) bins[image[tid]]++;\n}",
            "// TODO\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (idx >= N) return;\n   atomicAdd(&bins[image[idx]], 1);\n}",
            "// TODO: Your code here\n}",
            "int idx = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (idx < N) {\n      atomicAdd(&bins[image[idx]], 1);\n   }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "__shared__ int tempBins[256];\n\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (idx < N) {\n    atomicAdd(&(tempBins[image[idx]]), 1);\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 256; ++i) {\n      atomicAdd(&(bins[i]), tempBins[i]);\n    }\n  }\n}",
            "// TODO\n  //__shared__ int *sharedMem;\n  int my_id = blockIdx.x*blockDim.x + threadIdx.x;\n  if(my_id < N)\n    atomicAdd(&bins[image[my_id]], 1);\n}",
            "const size_t index = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n    if (index < N) {\n        bins[image[index]]++;\n    }\n}",
            "int index = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  if (index < N)\n    atomicAdd(&bins[image[index]], 1);\n}",
            "int i = threadIdx.x + blockIdx.x*blockDim.x;\n  if (i < N) atomicAdd(&bins[image[i]], 1);\n}",
            "// Get the index of the current thread\n   size_t tid = threadIdx.x + blockDim.x*blockIdx.x;\n   if (tid < N)\n      atomicAdd(&(bins[image[tid]]), 1);\n}",
            "// Each thread increments a global histogram of the pixel intensities\n}",
            "// TODO\n\n}",
            "const int bin = image[threadIdx.x];\n    atomicAdd(&bins[bin], 1);\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO\n}",
            "const int x = threadIdx.x + blockDim.x * blockIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "// Fill code here...\n}",
            "// TODO: implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) bins[image[i]]++;\n}",
            "int idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "const size_t thread_id = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n    if (thread_id < N) {\n        // Perform atomic increment on bin with index image[thread_id]\n    }\n}",
            "// TODO\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N)\n      atomicAdd(&bins[image[tid]], 1);\n}",
            "// This function has 256 * 32 threads.\n    // Threads of the same warp will execute this code together.\n    // Each thread will have its own counter.\n\n    // Allocate the counter in the register file.\n    // The compiler will use the registers s0 - s31.\n    int counter = 0;\n\n    // Find the index of the first pixel that is part of this thread.\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // If the thread is outside of the image, return.\n    if (i >= N)\n        return;\n\n    // Count the pixel value in the register.\n    counter++;\n\n    // Store the result in global memory.\n    atomicAdd(&bins[image[i]], 1);\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    // atomicInc(&bins[image[i]], 1);\n    // atomicAdd(&bins[image[i]], 1);\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "const int tid = threadIdx.x;\n  // 256*threadIdx.x = threadIdx.x*256\n  __shared__ size_t sBins[256];\n  for (int i=0; i<256; i++) sBins[i]=0;\n  __syncthreads();\n\n  if (tid < N) {\n    atomicAdd(&sBins[image[tid]], 1);\n  }\n  __syncthreads();\n\n  // Copy the contents of the shared memory to global memory\n  for (int i=0; i<256; i++) {\n    atomicAdd(&bins[i], sBins[i]);\n  }\n}",
            "// Use dynamic parallelism to launch the inner loop.\n    // The first thread will launch the next N threads.\n    // The rest of the threads will do nothing\n    // (because of the condition \"threadIdx.x == 0\").\n    if (threadIdx.x == 0) {\n        pixelCounts_inner<<<1, N>>>(image, N, bins);\n    }\n}",
            "// Set the number of threads in a block.\n    const unsigned int blockSize = 256;\n    \n    // Get the blockId and threadId\n    const unsigned int blockId = blockIdx.x;\n    const unsigned int threadId = threadIdx.x;\n\n    // Get the current index for the thread.\n    size_t i = blockId * blockSize + threadId;\n\n    // Loop over the pixels in the current thread.\n    while (i < N) {\n        // Count the current pixel.\n        atomicAdd(&bins[image[i]], 1);\n\n        // Get the next index for the thread.\n        i += blockSize * gridDim.x;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) return;\n  atomicAdd(&bins[image[tid]], 1);\n}",
            "// TODO: Implement in parallel\n}",
            "int pixel = image[blockIdx.x * blockDim.x + threadIdx.x];\n    atomicAdd(&bins[pixel], 1);\n}",
            "int i = threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = blockIdx.y * blockDim.y + threadIdx.y;\n  if (i < N && j < 256)\n    atomicAdd(&bins[j], image[i] == j);\n}",
            "unsigned int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "// TODO\n}",
            "int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n  if (thread_id < N) {\n    atomicAdd(&bins[image[thread_id]], 1);\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    if(gid >= N) return;\n\n    atomicAdd(&bins[image[gid]], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N)\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "const int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx < N)\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N)\n        atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO\n}",
            "// TODO: count the number of pixels in image with each grayscale intensity.\n    //       Store the results in `bins`.\n    //       N is the number of pixels in the image.\n    //       Hint: use N/256 as the block size.\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// Your code here\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    __atomic_fetch_add(&(bins[image[i]]), 1, __ATOMIC_SEQ_CST);\n  }\n}",
            "// Counts the number of pixels with a given grayscale value.\n  // Input: grayscale value in the range 0-255\n  // Output: count\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + tid;\n  if (gid < N) {\n    atomicAdd(&bins[image[gid]], 1);\n  }\n}",
            "const unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    const int value = image[index];\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "size_t tid = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "const size_t i = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    __atomic_add(&bins[image[i]], 1);\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x*blockDim.x;\n  if (idx < N) {\n    atomicAdd(&(bins[image[idx]]), 1);\n  }\n}",
            "// TODO: Add your code here\n}",
            "// Use the hipIndex() function to get the current index in the parallel loop\n   const size_t i = hipIndex_X();\n   // Skip all indices that are out-of-bounds for the image\n   if(i >= N) return;\n   // Increment the bin at index `image[i]` by 1\n   atomicAdd(bins + image[i], 1);\n}",
            "// Find the global index of this thread in the array of image elements\n  size_t global_id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  // Check to make sure this thread's index is less than the size of the image\n  if (global_id >= N)\n    return;\n\n  // Increment the bin value associated with the current element\n  atomicAdd(&bins[image[global_id]], 1);\n}",
            "// TODO: implement\n    // get global thread index\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    //printf(\"idx %d\\n\", idx);\n    // check if global index within range\n    if (idx < N) {\n        //printf(\"index %d\\n\", idx);\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  size_t i;\n  for (i = idx; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "__shared__ size_t cache[256];\n\n  // Initialize the cache\n  const size_t cacheIndex = threadIdx.x;\n  cache[cacheIndex] = 0;\n  __syncthreads();\n\n  // Iterate through the pixels\n  for (size_t pixel = blockIdx.x * blockDim.x + threadIdx.x; pixel < N;\n       pixel += blockDim.x * gridDim.x) {\n    // Use atomic add to increment the correct bin\n    atomicAdd(&cache[image[pixel]], 1);\n  }\n\n  __syncthreads();\n\n  // Write the cache back to the global memory\n  if (cacheIndex < 256) {\n    bins[cacheIndex] = cache[cacheIndex];\n  }\n}",
            "const int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    const int value = image[index];\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  // TODO 1: Replace this line\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "//...\n\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  atomicAdd(&bins[image[tid]], 1);\n}",
            "}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (idx >= N) return;\n   int val = image[idx];\n   atomicAdd(&bins[val], 1);\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "// This is a dummy code, you have to write your code here.\n\n}",
            "// TODO\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&(bins[image[i]]), 1);\n    }\n}",
            "const size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < N) {\n    __atomic_fetch_add(bins+image[gid], 1, __ATOMIC_SEQ_CST);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        bins[image[i]]++;\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t idx = threadIdx.x;\n\n    // TODO: Implement this function.\n\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO\n}",
            "// Launch the kernel with at least N threads.\n  int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n  if (thread_id >= N) return;\n\n  // Count the number of pixels in image with each grayscale intensity.\n  // Use atomicAdd to handle concurrent accesses.\n  atomicAdd(&bins[image[thread_id]], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) bins[image[i]]++;\n}",
            "int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  if (ix < N) {\n    atomicAdd(&bins[image[ix]], 1);\n  }\n}",
            "size_t index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  for (size_t i = index; i < N; i += stride) {\n    unsigned char val = image[i];\n    atomicAdd(&bins[val], 1);\n  }\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N)\n      atomicAdd(&bins[image[tid]], 1);\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N)\n    atomicAdd(bins + image[i], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int grayValue;\n  if (idx < N) {\n    grayValue = image[idx];\n    atomicAdd(&bins[grayValue], 1);\n  }\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    __atomic_add(&bins[image[i]], 1);\n  }\n}",
            "const int thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n  if (thread_id < N) {\n    atomicAdd(&bins[image[thread_id]], 1);\n  }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid < N)\n        atomicAdd(&bins[image[tid]], 1);\n}",
            "// TODO: implement me!\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    bins[image[tid]] += 1;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int idx = threadIdx.x;\n  while (idx < N) {\n    atomicAdd(bins + image[idx], 1);\n    idx += blockDim.x;\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   while(tid < N) {\n      // Use atomicInc to increment the bin corresponding to the\n      // value of the pixel in `image`\n      atomicInc(&bins[image[tid]]);\n      tid += gridDim.x * blockDim.x;\n   }\n}",
            "int pixel = image[blockIdx.x * blockDim.x + threadIdx.x];\n    atomicAdd(&bins[pixel], 1);\n}",
            "for (size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n         idx < N;\n         idx += blockDim.x * gridDim.x) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x*blockDim.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "const size_t i = blockDim.x*blockIdx.x + threadIdx.x;\n   if (i < N) atomicAdd(&bins[image[i]], 1);\n}",
            "const size_t id = threadIdx.x + blockDim.x * blockIdx.x;\n    if(id >= N) return;\n    atomicAdd(&bins[image[id]], 1);\n}",
            "// TODO\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n    if(id >= N) return;\n    atomicAdd(&bins[image[id]], 1);\n}",
            "// Add your code here\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        __atomic_fetch_add(&bins[image[i]], 1, __ATOMIC_SEQ_CST);\n    }\n}",
            "}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N)\n        atomicAdd(&bins[image[i]], 1);\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N)\n    atomicAdd(&bins[image[tid]], 1);\n}",
            "// TODO\n\n}",
            "unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&(bins[image[i]]), 1);\n    }\n}",
            "int tid = hipThreadIdx_x;\n    if (tid >= N) return;\n    atomicAdd(&bins[image[tid]], 1);\n}",
            "size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n   if (id < N)\n      atomicAdd(bins + image[id], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  // Count the number of pixels in image with intensity i in bins[i]\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// each thread works on a pixel\n    int i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (i >= N) return;\n    // TODO: store the pixel grayscale intensity in `intensity`\n    int intensity = image[i];\n    // TODO: use atomicAdd to add 1 to the bin with this intensity\n    atomicAdd(bins + intensity, 1);\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "size_t id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (id < N) {\n        atomicAdd(&bins[image[id]], 1);\n    }\n}",
            "int x = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    int y = hipThreadIdx_y + hipBlockIdx_y * hipBlockDim_y;\n    if (x < N && y < N) {\n        int gray = image[y * N + x];\n        atomicAdd(&bins[gray], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t threadIdx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (threadIdx < N)\n      atomicAdd(&bins[image[threadIdx]], 1);\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// Launch at least N threads, so N threads will count the pixels.\n   // This is a naive implementation.\n   // You should use shared memory to avoid race conditions and reduce thread conflicts.\n   // For simplicity, we assume that all pixel values are within [0, 255].\n   // The thread will count the number of occurrences of its own grayscale value in the image.\n   const int index = blockDim.x * blockIdx.x + threadIdx.x;\n   if (index < N) {\n      bins[image[index]]++;\n   }\n}",
            "unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N)\n    atomicAdd(&bins[image[tid]], 1);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n    const int n = gridDim.x * blockDim.x;\n\n    for (int idx = i; idx < N; idx += n) {\n        atomicAdd(bins + image[idx], 1);\n    }\n}",
            "int index = (blockDim.x*blockIdx.x + threadIdx.x);\n  if (index >= N) return;\n  atomicAdd(&(bins[image[index]]), 1);\n}",
            "__shared__ size_t bins_shared[256];\n\n    int tid = threadIdx.x;\n    int cnt = blockDim.x;\n\n    // Initialize local memory to zero.\n    for (int i = 0; i < 256; i++) {\n        bins_shared[i] = 0;\n    }\n\n    __syncthreads();\n\n    // Iterate over the image, count pixels with each grayscale intensity.\n    for (int i = tid; i < N; i += cnt) {\n        bins_shared[image[i]]++;\n    }\n\n    __syncthreads();\n\n    // Use atomic operations to update global memory.\n    for (int i = 0; i < 256; i++) {\n        atomicAdd(&bins[i], bins_shared[i]);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        bins[image[tid]]++;\n    }\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if(i >= N) return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "__shared__ size_t counts[256];\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&counts[image[i]], 1);\n  }\n  __syncthreads();\n  atomicAdd(&bins[image[i]], counts[image[i]]);\n}",
            "int index = threadIdx.x + blockIdx.x*blockDim.x;\n    if (index >= N) return;\n    atomicAdd(bins+image[index], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N)\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "__shared__ size_t counts[256];\n  for (int i = 0; i < 256; i++) counts[i] = 0;\n  __syncthreads();\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    // TODO: fill in code here.\n  }\n  __syncthreads();\n  for (int i = 0; i < 256; i++) {\n    atomicAdd(&bins[i], counts[i]);\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N)\n    return;\n  bins[image[i]]++;\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "size_t gid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (gid < N) {\n        atomicAdd(&bins[image[gid]], 1);\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    atomicAdd(&(bins[image[tid]]), 1);\n  }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if(index < N)\n        atomicAdd(&bins[image[index]], 1);\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    atomicAdd(bins + image[tid], 1);\n  }\n}",
            "const size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  const int value = image[i];\n  if (i < N) {\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "// each thread computes one pixel\n   unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i < N) bins[image[i]]++;\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = index; i < N; i += stride)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO 1: implement me!\n}",
            "// TODO: write your solution here\n}",
            "// Use `blockIdx.x` and `threadIdx.x` to determine the index of the current\n  // thread.\n\n  // Use atomicAdd() to count.\n}",
            "//...\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  while (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n    i += gridDim.x * blockDim.x;\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t gid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (gid < N) {\n    __atomic_add(&bins[image[gid]], 1);\n  }\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx >= N) return;\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) {\n    return;\n  }\n\n  atomicAdd(&bins[image[i]], 1);\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if(index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n}",
            "__shared__ size_t sdata[BLOCK_SIZE];\n  unsigned int tid = threadIdx.x;\n  unsigned int bid = blockIdx.x;\n  unsigned int idx = bid * blockDim.x + tid;\n  unsigned int sidx = tid;\n  sdata[sidx] = 0;\n  __syncthreads();\n\n  // Copy the global data to the shared memory\n  if (idx < N) {\n    sdata[sidx] = image[idx];\n  }\n  __syncthreads();\n\n  // Aggregate the data in the shared memory\n  for (size_t i = 1; i < blockDim.x; i <<= 1) {\n    __syncthreads();\n    if (sidx >= i) {\n      sdata[sidx] += sdata[sidx - i];\n    }\n  }\n\n  // Copy the results back to the global memory\n  __syncthreads();\n  if (idx < N && tid == 0) {\n    bins[image[idx]] = sdata[sidx];\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i >= N) {\n        return;\n    }\n\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// Write code here\n\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    size_t stride = gridDim.x * blockDim.x;\n    for (size_t i = tid; i < N; i += stride) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        bins[image[i]]++;\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      atomicAdd(&(bins[image[i]]), 1);\n   }\n}",
            "// TODO\n    unsigned int t_idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (t_idx >= N) return;\n    atomicAdd(&bins[image[t_idx]], 1);\n}",
            "// TODO\n}",
            "// TODO: Implement the kernel\n\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Get a unique id for the thread\n  size_t id = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n\n  // Count in parallel\n  if (id < N)\n    atomicAdd(&bins[image[id]], 1);\n}",
            "// TODO\n  // Find the index of the current thread (0 to N-1)\n  int tid = /* TODO: Fill this in */;\n\n  // If we are in bounds, update the histogram\n  if(tid < N) {\n    // Find the color of the current pixel\n    int pixel = image[tid];\n\n    // Update the bin for the color\n    atomicAdd(&bins[pixel], 1);\n  }\n}",
            "// get the id of the current thread\n    size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // loop over the bins in parallel\n    for (int i = 0; i < 256; i++) {\n        // use `atomicAdd` to prevent race conditions\n        atomicAdd(&bins[i], (image[tid] == i)? 1 : 0);\n    }\n}",
            "// Write code here\n    int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "// TODO\n}",
            "int global_id = threadIdx.x + blockIdx.x * blockDim.x;\n  int local_id = threadIdx.x;\n\n  __shared__ size_t local_bins[256];\n\n  if (global_id < N) {\n    atomicAdd(&local_bins[image[global_id]], 1);\n  }\n\n  __syncthreads();\n\n  if (local_id == 0) {\n    for (int i = 0; i < 256; ++i) {\n      atomicAdd(&bins[i], local_bins[i]);\n    }\n  }\n\n}",
            "const int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "// The thread ID in the block.\n  // See \"CUDA C Programming Guide\" Section 3.2.2.\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  // Check if the thread ID is in bounds.\n  // If not, then return from the kernel.\n  if (tid >= N) {\n    return;\n  }\n  // Get the pixel value.\n  int pixel = image[tid];\n  // Increment the bin corresponding to the pixel value.\n  atomicAdd(&bins[pixel], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Fill this in.\n}",
            "// Get the index of the current thread in the kernel\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    // Check if the index is valid\n    if(idx < N) {\n        atomicAdd(&(bins[image[idx]]), 1);\n    }\n}",
            "size_t thread = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (thread >= N) return;\n    atomicAdd(&bins[image[thread]], 1);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int const rank = MPI_RANK;\n   int const size = MPI_SIZE;\n   int const root = 0;\n\n   // YOUR CODE HERE\n   // Store the result on root.\n   // All other ranks should store an empty array\n\n   // All processors will receive the result from root.\n   MPI_Gather(&bins[0], 256, MPI_UNSIGNED_LONG, &bins[0], 256, MPI_UNSIGNED_LONG, root, MPI_COMM_WORLD);\n }",
            "int rank, size, image_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_size(MPI_COMM_WORLD, &image_size);\n\n    // YOUR CODE HERE\n    int pixel;\n    int local_image_size;\n\n    local_image_size = image.size()/size;\n    std::vector<int> local_image(local_image_size);\n    for (int i = 0; i < local_image_size; i++) {\n        local_image[i] = image[rank*local_image_size + i];\n    }\n\n    std::vector<int> local_bins(256, 0);\n\n    for (int i = 0; i < local_image_size; i++) {\n        pixel = local_image[i];\n        local_bins[pixel]++;\n    }\n\n    std::array<size_t, 256> global_bins(256, 0);\n\n    MPI_Reduce(&local_bins[0], &global_bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "}",
            "// Your code here.\n  int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  auto nPixels = image.size();\n  auto nPerRank = nPixels / size;\n  auto remainder = nPixels % size;\n\n  // each rank stores its local pixels in a copy of the image\n  std::vector<int> imageCopy(nPerRank);\n  if (rank == 0) {\n    std::copy(image.begin(), image.begin() + nPerRank, imageCopy.begin());\n  } else {\n    std::copy(image.begin() + rank * nPerRank, image.begin() + (rank * nPerRank) + nPerRank, imageCopy.begin());\n  }\n  // if the rank doesn't get an equal number of pixels, add the remainder to the last rank\n  if (rank == size - 1) {\n    std::copy(image.begin() + (size - 1) * nPerRank, image.end(), imageCopy.begin());\n  }\n\n  // initialize the local bin counts\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  // fill the local bin counts\n  for (int i = 0; i < imageCopy.size(); i++) {\n    bins[imageCopy[i]]++;\n  }\n\n  // collect the counts from each rank\n  std::vector<size_t> allCounts(256 * size);\n  MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, allCounts.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // combine the counts on rank 0\n  if (rank == 0) {\n    for (int i = 0; i < allCounts.size(); i++) {\n      bins[i / size] += allCounts[i];\n    }\n  }\n}",
            "for (int i = 0; i < 256; i++)\n   bins[i] = 0;\n  \n  // TODO\n  \n  // MPI_Reduce(bins,bins,256,MPI_UNSIGNED_LONG,MPI_SUM,0,MPI_COMM_WORLD);\n }",
            "// TODO: Implement this function.\n}",
            "// TODO: your code here\n}",
            "size_t image_size = image.size();\n    size_t rank = 0;\n    size_t num_ranks = 1;\n\n    // use MPI to get the rank and total number of ranks\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // create a copy of the image data for each rank\n    std::vector<int> image_copy(image.begin(), image.end());\n\n    // send the image_copy to each rank\n    MPI_Scatter(image_copy.data(), image_size, MPI_INT, &image_copy[0], image_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // count the number of pixels with each grayscale intensity for this rank\n    for (auto it = image_copy.begin(); it!= image_copy.end(); it++) {\n        bins[*it]++;\n    }\n\n    // combine the counts for each rank into one array of bins\n    std::vector<int> bins_for_each_rank(256, 0);\n    std::vector<int> bins_for_all_ranks(256, 0);\n\n    // send the local bins to rank 0\n    MPI_Gather(&bins_for_each_rank[0], 256, MPI_INT, &bins_for_all_ranks[0], 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // rank 0 combines the counts for all ranks\n    if (rank == 0) {\n        for (size_t i = 0; i < 256; i++) {\n            for (size_t j = 0; j < num_ranks; j++) {\n                bins[i] += bins_for_all_ranks[j * 256 + i];\n            }\n        }\n    }\n}",
            "//TODO: Implement pixelCounts.\n }",
            "}",
            "// TODO\n}",
            "//\n    // YOUR CODE HERE\n    //\n    // The first and last element should be 0\n    bins = {0};\n    for (auto pixel : image) {\n        bins[pixel] += 1;\n    }\n\n    std::vector<std::array<size_t, 256>> localBins;\n    localBins.push_back(bins);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int i;\n    int left;\n    int right;\n    int sendto;\n    int recfrom;\n    int count = 1;\n    if (size > 1) {\n        left = rank - 1;\n        right = rank + 1;\n        if (rank == 0)\n            left = MPI_PROC_NULL;\n        if (rank == size - 1)\n            right = MPI_PROC_NULL;\n\n        MPI_Sendrecv(&bins, 256, MPI_INT, right, 100, &localBins[1], 256, MPI_INT, left, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        if (rank == 0) {\n            for (i = 1; i < size; i++) {\n                recfrom = i;\n                MPI_Recv(&localBins[i + 1], 256, MPI_INT, recfrom, 101, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n        else {\n            sendto = 0;\n            MPI_Send(&localBins[1], 256, MPI_INT, sendto, 101, MPI_COMM_WORLD);\n        }\n\n        for (i = 1; i < size; i++) {\n            localBins[0] = std::array<size_t, 256>(localBins[i]);\n            for (auto& it : localBins[0])\n                it += it;\n        }\n        bins = localBins[0];\n    }\n}",
            "int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     // YOUR CODE HERE\n }",
            "const int N = image.size();\n   for (int i = 0; i < N; i++) {\n     bins[image[i]]++;\n   }\n }",
            "// Fill this in!\n    const size_t imageSize = image.size();\n    for(int i=0; i<imageSize; i++){\n        bins[image[i]]++;\n    }\n}",
            "for(int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    // your code here\n\n    int size;\n    int rank;\n    int source;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<int> bin_data(256, 0);\n\n    if(rank == 0) {\n        for(int i = 0; i < image.size(); i++) {\n            bin_data[image[i]]++;\n        }\n\n        for(int i = 1; i < size; i++) {\n            source = i;\n            MPI_Recv(&bin_data, 256, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for(int i = 0; i < 256; i++) {\n                bin_data[i] += bin_data[i];\n            }\n        }\n\n        bins = bin_data;\n    }\n\n    else {\n        for(int i = 0; i < image.size(); i++) {\n            bin_data[image[i]]++;\n        }\n\n        source = 0;\n        MPI_Send(&bin_data, 256, MPI_INT, source, 0, MPI_COMM_WORLD);\n    }\n}",
            "// This function uses OpenMP and should be run with `mpiexec -np 4./count_pixels`\n  // If you run with `mpiexec -np 1./count_pixels` then it should be identical\n  // to pixelCounts_serial\n  //\n  // Hint:\n  //  1. Break up the loop into N chunks, with N being the number of MPI processes.\n  //  2. Each process should handle chunk N\n  //  3. Combine the results on rank 0\n\n  // This is a naive serial implementation\n  pixelCounts_serial(image, bins);\n}",
            "// TODO: Implement this function\n\n }",
            "// TODO: Implement this function\n }",
            "MPI_Datatype mpi_type;\n   MPI_Type_vector(10, 1, 10, MPI_INT, &mpi_type);\n   MPI_Type_commit(&mpi_type);\n\n   // TODO: add code here\n   std::vector<int> mpi_vector(10, 0);\n   for (int i = 0; i < 10; i++) {\n     mpi_vector[i] = image[i];\n   }\n   // TODO: add code here\n\n   std::vector<int> global_vector(10, 0);\n\n   MPI_Allreduce(mpi_vector.data(), global_vector.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   for (int i = 0; i < 256; i++) {\n     bins[i] = global_vector[i];\n   }\n\n   MPI_Type_free(&mpi_type);\n }",
            "// TODO\n }",
            "// TODO: Replace this with your code\n   int rank, size, n;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   n = image.size();\n   std::array<size_t, 256> counts;\n   if(rank == 0) {\n     for(int i=0; i<256; i++)\n       counts[i] = 0;\n   }\n   std::array<int, 256> local_counts;\n   for(int i=0; i<256; i++)\n     local_counts[i] = 0;\n   int i, part = n/size;\n   for(i=rank*part; i<(rank+1)*part; i++) {\n     local_counts[image[i]]++;\n   }\n   MPI_Gather(local_counts.data(), 256, MPI_UNSIGNED_LONG, counts.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   if(rank == 0) {\n     for(int i=0; i<256; i++)\n       bins[i] = counts[i];\n   }\n }",
            "// TODO: count grayscale intensities in a distributed fashion\n }",
            "// Your code goes here\n\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int length = image.size();\n\n   int start = rank * length / size;\n   int end = (rank + 1) * length / size;\n\n   std::array<size_t, 256> local_bins = {0};\n\n   for(int i = start; i < end; ++i){\n     local_bins[image[i]]++;\n   }\n\n   std::array<size_t, 256> all_bins = {0};\n\n   MPI_Reduce(local_bins.data(), all_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if(rank == 0) {\n     bins = all_bins;\n   }\n}",
            "// TODO\n}",
            "// TODO\n    // \n}",
            "// TODO: Replace with your implementation\n  \n}",
            "// Your code goes here\n    int myrank;\n    int size;\n    int image_size;\n    int my_image_size;\n    int my_first_pixel;\n    int my_last_pixel;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    image_size = image.size();\n    my_image_size = image_size / size;\n    my_first_pixel = my_image_size * myrank;\n    my_last_pixel = my_first_pixel + my_image_size;\n    for (int i = my_first_pixel; i < my_last_pixel; i++) {\n        bins[image[i]] += 1;\n    }\n    if (myrank == 0) {\n        for (int j = 1; j < size; j++) {\n            int count = 0;\n            MPI_Recv(&count, my_image_size, MPI_INT, j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int k = 0; k < my_image_size; k++) {\n                bins[count] += 1;\n            }\n        }\n    } else {\n        MPI_Send(&image[my_first_pixel], my_image_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n\n}",
            "int N = image.size();\n\t int i, sum, rank, nproc;\n\t MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\t int* my_sums = new int[256];\n\t MPI_Request req;\n\t MPI_Status status;\n\t MPI_Irecv(&my_sums[0], 256, MPI_INT, rank-1, 0, MPI_COMM_WORLD, &req);\n\t for(i = 0; i < 256; i++){\n\t\t my_sums[i] = 0;\n\t }\n\t for(i = 0; i < N; i++){\n\t\t my_sums[image[i]]++;\n\t }\n\t if(rank == nproc-1){\n\t\t MPI_Send(&my_sums[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t }else{\n\t\t MPI_Send(&my_sums[0], 256, MPI_INT, rank+1, 0, MPI_COMM_WORLD);\n\t }\n\t MPI_Wait(&req, &status);\n\t sum = 0;\n\t for(i = 0; i < 256; i++){\n\t\t bins[i] = my_sums[i] + my_sums[i+256];\n\t }\n\t delete [] my_sums;\n }",
            "// Implement me!\n }",
            "// TODO: YOUR CODE HERE\n\n    int world_size, world_rank, i, j;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int start_idx, end_idx, idx, value;\n    start_idx = world_rank * (image.size() / world_size);\n    end_idx = (world_rank + 1) * (image.size() / world_size) - 1;\n    for(i = start_idx; i <= end_idx; i++) {\n        value = image[i];\n        idx = value / 10;\n        bins[idx] = bins[idx] + 1;\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), 256, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "bins.fill(0);\n    for (int pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "// TODO: Use MPI to count bins\n   if (image.empty()) {\n     return;\n   }\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<std::array<size_t, 256>> bins_vec(size);\n   auto bin_size = image.size() / size;\n   auto remainder = image.size() % size;\n   auto start_at = bin_size * rank;\n   auto end_at = start_at + bin_size;\n   if (rank < remainder) {\n     end_at++;\n   }\n   if (rank == 0) {\n     for (size_t i = 0; i < 256; i++) {\n       for (size_t j = 0; j < image.size(); j++) {\n         if (image[j] == i) {\n           bins[i]++;\n         }\n       }\n     }\n   } else {\n     for (size_t i = start_at; i < end_at; i++) {\n       if (image[i] == rank) {\n         bins_vec[rank - 1][rank]++;\n       }\n     }\n   }\n\n   MPI_Gather(&bins_vec[rank - 1], 256, MPI_UNSIGNED_LONG, &bins, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // TODO: end\n }",
            "// Your code here\n  \n  size_t size=image.size();\n  size_t chunk=size/mpi::size();\n  size_t rem=size%mpi::size();\n  size_t start;\n  size_t stop;\n  int r;\n  int local_bins[256];\n  \n  if(mpi::rank()==0){\n      bins.fill(0);\n  }\n  \n  if(mpi::rank()<rem){\n      start=mpi::rank()*(chunk+1);\n      stop=start+(chunk+1);\n  }\n  else{\n      start=rem*chunk+mpi::rank()*chunk;\n      stop=start+chunk;\n  }\n  \n  for(int i=0;i<256;i++){\n      local_bins[i]=0;\n  }\n  \n  for(size_t i=start;i<stop;i++){\n      local_bins[image[i]]++;\n  }\n  \n  MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  \n  \n  }",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<int> local_image(image.size() / size);\n  MPI_Scatter(image.data(), local_image.size(), MPI_INT, local_image.data(), local_image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 256> local_bins;\n  for (auto i = 0; i < local_bins.size(); i++) {\n    local_bins[i] = std::count(local_image.begin(), local_image.end(), i);\n  }\n\n  std::array<size_t, 256> global_bins;\n  MPI_Reduce(local_bins.data(), global_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "// TODO: Implement me!\n\t// Add the code to count the pixels with intensity i, and put the result in bin[i]\n}",
            "}",
            "// TODO: Your code goes here.\n}",
            "//TODO: insert your code here\n\tint image_size = image.size();\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint send_size = image_size / size;\n\tint remainder = image_size % size;\n\tint send_size_with_remainder = (rank + 1) * send_size + (rank + 1 < remainder? rank + 1 : remainder);\n\n\tint recv_size = image_size / size;\n\tint recv_size_with_remainder = (rank + 1) * recv_size + (rank + 1 < remainder? rank + 1 : remainder);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < recv_size_with_remainder; i++) {\n\t\t\tbins[image[i]]++;\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < send_size_with_remainder; i++) {\n\t\t\tbins[image[i]]++;\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size - 1; i++) {\n\t\t\tint source = i + 1;\n\t\t\tstd::vector<int> receive_image;\n\t\t\tint receive_image_size = 0;\n\t\t\tMPI_Recv(&receive_image_size, 1, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\treceive_image.resize(receive_image_size);\n\t\t\tMPI_Recv(&receive_image[0], receive_image_size, MPI_INT, source, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n\t\t\tfor (int j = 0; j < receive_image_size; j++) {\n\t\t\t\tbins[receive_image[j]]++;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < size - 1; i++) {\n\t\t\tint source = i + 1;\n\t\t\tMPI_Send(&send_size_with_remainder, 1, MPI_INT, source, 0, MPI_COMM_WORLD);\n\t\t\tMPI_Send(&image[0], send_size_with_remainder, MPI_INT, source, 0, MPI_COMM_WORLD);\n\t\t}\n\t}\n}",
            "int num_of_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_of_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n \n  int size=image.size();\n  int mysize=size/num_of_ranks;\n  int mystart=mysize*rank;\n  int myend=mysize*(rank+1);\n\n  if(mysize*num_of_ranks>size)\n    myend=size;\n  else if(rank==(num_of_ranks-1))\n    myend=size;\n  std::vector<int> myimage(myend-mystart);\n  std::copy(image.begin()+mystart, image.begin()+myend, myimage.begin());\n\n  std::array<size_t, 256> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n  for (int i=0; i<mysize; i++)\n    local_bins[myimage[i]]++;\n\n  std::array<size_t, 256> global_bins;\n  std::fill(global_bins.begin(), global_bins.end(), 0);\n  MPI_Reduce(&local_bins, &global_bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  \n  if(rank==0)\n    bins=global_bins;\n}",
            "// YOUR CODE HERE\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 256> binArray;\n    binArray.fill(0);\n\n    int start = (image.size() / size) * rank;\n    int end = (image.size() / size) * (rank + 1);\n\n    if (rank == size - 1)\n    {\n        end = image.size();\n    }\n\n    for (int i = start; i < end; i++)\n    {\n        binArray[image[i]]++;\n    }\n\n    std::vector<size_t> vec(binArray.begin(), binArray.end());\n\n    if (rank == 0)\n    {\n        MPI_Reduce(MPI_IN_PLACE, &vec[0], 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else\n    {\n        MPI_Reduce(&vec[0], &vec[0], 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    for (int i = 0; i < 256; i++)\n    {\n        bins[i] = vec[i];\n    }\n}",
            "int numProcs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const size_t imageSize = image.size();\n    std::array<size_t, 256> localCounts = {};\n    size_t sizePerProc = imageSize/numProcs;\n\n    if(rank == 0){\n        // rank 0 calculates the leftover\n        size_t localSize = sizePerProc;\n        for(size_t i = 0; i < localSize; i++){\n            localCounts[image[i]]++;\n        }\n\n        // the rest of the processors will send their counts to rank 0\n        for(int i = 1; i < numProcs; i++){\n            size_t bufferSize;\n            MPI_Status status;\n            MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n            MPI_Get_count(&status, MPI_UNSIGNED_LONG, &bufferSize);\n            std::vector<size_t> buffer(bufferSize);\n            MPI_Recv(buffer.data(), bufferSize, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(size_t i = 0; i < buffer.size(); i++){\n                localCounts[i] += buffer[i];\n            }\n        }\n    }\n    else{\n        // all other processors send their counts to rank 0\n        MPI_Send(localCounts.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if(rank == 0){\n        // rank 0 writes to bins\n        for(size_t i = 0; i < 256; i++){\n            bins[i] = localCounts[i];\n        }\n    }\n}",
            "//TODO: YOUR CODE HERE\n}",
            "int size = image.size();\n\n \tint rank, num;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num);\n\n \tint counts = size/num;\n \tint remainder = size%num;\n \t\n \tstd::array<size_t, 256> bins_local;\n \tstd::fill(bins_local.begin(), bins_local.end(), 0);\n\n \t// count pixel counts\n \tfor (int i=0; i < counts; i++) {\n \t\tbins_local[image[i*num+rank]]++;\n \t}\n\n \t// count the extra elements\n \tif (rank < remainder) {\n \t\tbins_local[image[counts*num+rank]]++;\n \t}\n\n \t// merge data\n \tif (rank == 0) {\n \t\tfor (int i=1; i < num; i++) {\n \t\t\tfor (int j=0; j < 256; j++) {\n \t\t\t\tbins[j] += bins_local[j];\n \t\t\t}\n \t\t}\n \t}\n }",
            "// Initialize the bins to 0\n  for (auto& x : bins) x = 0;\n\n  // Count the number of pixels with each intensity in parallel.\n  // Use std::array<size_t, 256> for bins\n}",
            "int my_rank;\n    int numprocs;\n    int my_tag = 99;\n    int dest;\n    int source;\n    int count = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    int part_size = image.size() / numprocs;\n    int part_start = part_size * my_rank;\n    int part_end = part_start + part_size;\n\n    if(my_rank == 0){\n        dest = MPI_ANY_SOURCE;\n        source = MPI_ANY_SOURCE;\n\n        for(int i = 1; i < numprocs; i++){\n            MPI_Recv(bins.data(), 256, MPI_INT, dest, my_tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < 256; j++){\n                bins[j] += bins[j];\n            }\n        }\n\n        for(int i = 0; i < part_size; i++){\n            count++;\n            bins[image[part_start + i]]++;\n        }\n\n        MPI_Send(bins.data(), 256, MPI_INT, source, my_tag, MPI_COMM_WORLD);\n\n    } else{\n        dest = 0;\n        source = my_rank;\n        MPI_Send(bins.data(), 256, MPI_INT, dest, my_tag, MPI_COMM_WORLD);\n        MPI_Recv(bins.data(), 256, MPI_INT, source, my_tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if(my_rank == 0){\n        for(int i = 0; i < 256; i++){\n            bins[i] += bins[i];\n        }\n    }\n}",
            "// TODO: Implement this\n}",
            "for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "int n_proc, id_proc;\n     MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n     MPI_Comm_rank(MPI_COMM_WORLD, &id_proc);\n     if (n_proc == 1) {\n         for (int i = 0; i < image.size(); i++) {\n             bins[image[i]] += 1;\n         }\n     } else {\n         int length = image.size();\n         int size_block = (length + n_proc - 1) / n_proc;\n         int start = id_proc * size_block;\n         int end = std::min((id_proc + 1) * size_block, length);\n         int diff = end - start;\n         if (diff > 0) {\n             std::vector<int> block(diff);\n             for (int i = 0; i < diff; i++) {\n                 block[i] = image[start + i];\n             }\n             MPI_Reduce(&block[0], &bins[0], 256, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n         }\n     }\n }",
            "}",
            "int sz, rank, next, prev;\n     int const n = image.size();\n\n     MPI_Comm_size(MPI_COMM_WORLD, &sz);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     MPI_Status status;\n\n     std::array<size_t, 256> counts_local = {};\n     std::array<size_t, 256> counts_global = {};\n     for (int i = 0; i < n; ++i) {\n         counts_local[image[i]] += 1;\n     }\n\n     MPI_Reduce(&counts_local, &counts_global, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n     MPI_Bcast(&counts_global, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n     bins = counts_global;\n}",
            "size_t image_size = image.size();\n  \n   // Calculate the number of bins on each rank.\n   size_t bins_per_rank = image_size / num_ranks;\n   \n   // Calculate the index of the first pixel on each rank.\n   size_t rank_start = num_ranks * bins_per_rank * rank_id;\n   \n   // Count the number of pixels in the image\n   for (int i = 0; i < bins_per_rank; i++) {\n     bins[image[rank_start + i]]++;\n   }\n   \n   if (rank_id == 0) {\n     // We're the root rank, so add the bins from each rank.\n     for (int i = 1; i < num_ranks; i++) {\n       MPI_Status status;\n       MPI_Recv(&bins, bins.size(), MPI_SIZE_T, i, 0, MPI_COMM_WORLD, &status);\n     }\n   } else {\n     // We're not the root rank, so send our bins to the root rank.\n     MPI_Send(&bins, bins.size(), MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n   }\n }",
            "for(int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n\n    // TODO: count in parallel\n\n}",
            "// TODO\n }",
            "// TODO: Your code goes here.\n }",
            "// TODO: implement\n\t\n\t\n}",
            "// YOUR CODE HERE\n}",
            "int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   int no_of_procs;\n   MPI_Comm_size(MPI_COMM_WORLD, &no_of_procs);\n   \n   // your code here\n   //\n   //\n   //\n   //\n   \n   //\n   //\n   //\n   //\n   \n   \n   // Do not modify code below\n   if (my_rank == 0) {\n     // combine the results\n     for (int i = 1; i < no_of_procs; ++i) {\n       MPI_Status status;\n       MPI_Recv(&bins[0], 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n     }\n   }\n   else {\n     MPI_Send(&bins[0], 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n }",
            "// your code here\n  \n  \n  }",
            "for (int i = 0; i < image.size(); i++){\n    bins[image[i]]++;\n  }\n}",
            "size_t size = image.size();\n\n   // Use bins as a temporary storage\n   // Initialize all bin values to zero\n   for (size_t i = 0; i < bins.size(); i++) {\n     bins[i] = 0;\n   }\n\n   // Scatter image values to all processors\n   for (int i = 0; i < size; i++) {\n     // Get the rank index of the processor that will handle this pixel value\n     int rank = image[i] % (int)bins.size();\n     // Increment the bin that corresponds to the rank\n     bins[rank] += 1;\n   }\n\n   // Gather all bin values from all processors\n   int i = 0;\n   for (int j = 0; j < (int)bins.size(); j++) {\n     // Gather all bin values from the processor that has the same rank\n     int rank = j % (int)bins.size();\n     // If it's the master process, gather the bin values and store in the final array\n     if (rank == 0) {\n       // Sum up the bins values for the same rank from all the processors\n       bins[j] += bins[i];\n       // Increment to the next rank index\n       i += 1;\n     }\n   }\n }",
            "int const size = image.size();\n  int my_size = size;\n  int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  // for (int i=0; i<size; i++) {\n  //   printf(\"Image %d on proc %d is %d\\n\", i, my_rank, image[i]);\n  // }\n\n  if (my_rank == 0) {\n    my_size = size / 2;\n    for (int i = 0; i < my_size; i++) {\n      bins[image[i]]++;\n    }\n    MPI_Reduce(&my_size, &size, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(&my_size, &size, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n  // MPI_Reduce(&my_size, &size, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  // if (my_rank == 0) {\n  //   for (int i = 0; i < 256; i++) {\n  //     printf(\"proc %d, bins[%d] = %d\\n\", my_rank, i, bins[i]);\n  //   }\n  //   printf(\"proc %d, my_size = %d, size = %d\\n\", my_rank, my_size, size);\n  //   for (int i = 0; i < 256; i++) {\n  //     bins[i] *= my_size;\n  //   }\n  //   printf(\"proc %d, bins[0] = %d, bins[255] = %d, size = %d\\n\", my_rank, bins[0], bins[255], size);\n  // }\n}",
            "// YOUR CODE HERE\n }",
            "// TODO: Your code here\n}",
            "int np = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n   int rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Your code here.\n   int size = image.size();\n   int delta = size/np;\n   std::vector<int> partial_hist(256);\n   int j = 0;\n   for (int i = 0; i < size; i++)\n   {\n     partial_hist[image[i]]++;\n     if (i % delta == 0 && i!= 0) {\n       MPI_Send(&partial_hist[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n       std::fill(partial_hist.begin(), partial_hist.end(), 0);\n       j = 0;\n     }\n   }\n\n   if (rank == 0) {\n     std::vector<int> tmp(256);\n     MPI_Status status;\n     for (int i = 1; i < np; i++) {\n       MPI_Recv(&tmp[0], 256, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n       int source = status.MPI_SOURCE;\n       int tag = status.MPI_TAG;\n       for (int j = 0; j < 256; j++) {\n         bins[j] += tmp[j];\n       }\n     }\n     for (int i = 0; i < size; i++) {\n       bins[image[i]]++;\n     }\n   }\n }",
            "}",
            "// Your code here!\n   for (auto val : image) {\n     bins[val] += 1;\n   }\n }",
            "int image_size = image.size();\n\t int p = image_size / 256;\n\t int r = image_size % 256;\n\t \n\t for (int i = 0; i < 256; i++)\n\t {\n\t\t bins[i] = 0;\n\t }\n\t \n\t MPI_Status status;\n\t \n\t int i = 0;\n\t int q = 0;\n\t \n\t while (i < image_size) {\n\t\t \n\t\t if (i % 256 == 0) {\n\t\t\t if (q!= 0) {\n\t\t\t\t MPI_Send(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t\t\t }\n\t\t\t else {\n\t\t\t\t MPI_Recv(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n\t\t\t }\n\t\t\t q++;\n\t\t }\n\t\t bins[image[i]]++;\n\t\t i++;\n\t }\n\t \n\t if (q!= 0) {\n\t\t MPI_Send(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t }\n\t else {\n\t\t MPI_Recv(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n\t }\n\t \n\t MPI_Barrier(MPI_COMM_WORLD);\n\t \n }",
            "MPI_Status status;\n  int rank;\n  int size;\n  int root = 0;\n  int source;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // int source = (rank - 1 + size) % size;\n  // int destination = (rank + 1) % size;\n\n  for (int i = 0; i < 256; i++){\n    bins[i] = 0;\n  }\n\n  for (int i = 0; i < image.size(); i++){\n    int pixel = image[i];\n    bins[pixel] += 1;\n  }\n\n  if (rank == 0){\n    for (int i = 0; i < size; i++){\n      if (i == 0){\n        source = size - 1;\n      } else {\n        source = i - 1;\n      }\n      MPI_Recv(&bins, 256, MPI_UNSIGNED_LONG_LONG, source, 0, MPI_COMM_WORLD, &status);\n    }\n  } else if (rank > 0){\n    MPI_Send(&bins, 256, MPI_UNSIGNED_LONG_LONG, root, 0, MPI_COMM_WORLD);\n  }\n\n\n  // MPI_Scatter(&bins, 256, MPI_UNSIGNED_LONG_LONG, &bins, 256, MPI_UNSIGNED_LONG_LONG, root, MPI_COMM_WORLD);\n  // MPI_Barrier(MPI_COMM_WORLD);\n\n  // if (rank > 0){\n  //   MPI_Recv(&bins, 256, MPI_UNSIGNED_LONG_LONG, source, 0, MPI_COMM_WORLD, &status);\n  // }\n\n  // MPI_Gather(&bins, 256, MPI_UNSIGNED_LONG_LONG, &bins, 256, MPI_UNSIGNED_LONG_LONG, root, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int mpi_size = 0;\n    int mpi_rank = 0;\n    MPI_Comm_size(comm, &mpi_size);\n    MPI_Comm_rank(comm, &mpi_rank);\n\n    // Partition the image\n    int image_size = image.size();\n    int image_per_rank = image_size / mpi_size;\n    int remainder = image_size % mpi_size;\n    int image_start_index = image_per_rank * mpi_rank + std::min(mpi_rank, remainder);\n    int image_end_index = image_start_index + image_per_rank + (mpi_rank < remainder? 1 : 0);\n    std::vector<int> my_image(image.begin() + image_start_index, image.begin() + image_end_index);\n\n    // Count pixels\n    std::array<size_t, 256> my_bins;\n    my_bins.fill(0);\n    for (auto i : my_image)\n      my_bins[i]++;\n\n    // Combine the bins\n    MPI_Reduce(my_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n}",
            "int p = 0;\n  if (p == 0){\n    int n = image.size();\n    int r = (n + 9)/10;\n    int rr = n/r;\n    int nn = n-r;\n    int nn2 = 2*nn;\n    int jj = n/2;\n    std::vector<int> buf0(r);\n    std::vector<int> buf1(rr);\n    std::vector<int> buf2(nn2);\n    for (int i=0; i<n; i+=r){\n      for (int j=0; j<r; j++){\n        if (i+j < n){\n          buf0[j] = image[i+j];\n        }\n      }\n      MPI_Gather(&buf0[0], r, MPI_INT, &buf1[0], r, MPI_INT, 0, MPI_COMM_WORLD);\n      if (p == 0){\n        for (int j=0; j<r; j++){\n          buf2[j] = buf1[j];\n        }\n        for (int j=r; j<r*2; j++){\n          buf2[j] = buf1[j+r];\n        }\n      }\n      MPI_Scatter(&buf2[0], rr, MPI_INT, &buf1[0], rr, MPI_INT, 0, MPI_COMM_WORLD);\n      for (int j=0; j<rr; j++){\n        bins[buf1[j]]++;\n      }\n    }\n  }\n}",
            "std::array<size_t, 256> local_bins;\n   local_bins.fill(0);\n   int my_rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   int num_pixels = image.size();\n   int pixels_per_rank = num_pixels / num_ranks;\n   int start_pos = my_rank * pixels_per_rank;\n   int end_pos = start_pos + pixels_per_rank;\n   if (my_rank == num_ranks - 1) {\n     end_pos = num_pixels;\n   }\n   for (int i = start_pos; i < end_pos; i++) {\n     local_bins[image[i]] += 1;\n   }\n   std::array<size_t, 256> global_bins;\n   global_bins.fill(0);\n   MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (my_rank == 0) {\n     bins = global_bins;\n   }\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int chunk = image.size() / size;\n   int offset = chunk * rank;\n   // Complete this function\n\n   if(rank == 0)\n   {\n       for(int i = 1; i < size; i++)\n       {\n           std::vector<int> recv(chunk, 0);\n           MPI_Recv(recv.data(), chunk, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n           for(int j = 0; j < chunk; j++)\n           {\n               bins[recv[j]]++;\n           }\n       }\n   }\n   else if(rank!= 0)\n   {\n       std::vector<int> send(chunk, 0);\n       for(int i = 0; i < chunk; i++)\n       {\n           send[i] = image[i + offset];\n       }\n       MPI_Send(send.data(), chunk, MPI_INT, 0, 1, MPI_COMM_WORLD);\n   }\n }",
            "// TODO\n\n }",
            "}",
            "// YOUR CODE HERE\n  const int imageSize = image.size();\n  int rank = 0;\n  int procs = 0;\n  int chunkSize = 0;\n  int start = 0;\n  int end = 0;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  chunkSize = imageSize / procs;\n  start = rank * chunkSize;\n  end = (rank + 1) * chunkSize;\n\n  if(rank == 0) {\n    for(int i = 0; i < chunkSize; i++) {\n      bins[image[i]]++;\n    }\n  } else if(rank > 0 && rank < procs - 1) {\n    for(int i = 0; i < chunkSize; i++) {\n      bins[image[i]]++;\n    }\n  } else if(rank == procs - 1) {\n    for(int i = 0; i < imageSize % procs; i++) {\n      bins[image[i]]++;\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if(rank > 0 && rank < procs - 1) {\n    MPI_Send(&bins[0], chunkSize, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  } else if(rank == procs - 1) {\n    MPI_Send(&bins[0], imageSize % procs, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if(rank == 0) {\n    std::vector<int> temp(chunkSize);\n    for(int i = 1; i < procs; i++) {\n      MPI_Recv(&temp[0], chunkSize, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int j = 0; j < chunkSize; j++) {\n        bins[j] += temp[j];\n      }\n    }\n\n    MPI_Recv(&temp[0], imageSize % procs, MPI_INT, procs - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for(int i = 0; i < imageSize % procs; i++) {\n      bins[i] += temp[i];\n    }\n  }\n\n  if(rank == 0) {\n    std::cout << \"Counts: \";\n    for(int i = 0; i < 256; i++) {\n      std::cout << bins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "}",
            "int i;\n   MPI_Status status;\n   std::vector<int> send;\n   std::vector<int> recv;\n   int size, rank;\n   int rsize, rrank;\n   int count;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<int> sum;\n   for(i=0; i < 256; i++) {\n     sum[i] = 0;\n   }\n   int temp[256];\n   for(i=0; i < 256; i++) {\n     temp[i] = 0;\n   }\n   MPI_Comm_size(MPI_COMM_WORLD, &rsize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rrank);\n   int chunks = image.size() / rsize;\n   int remainder = image.size() % rsize;\n   if(rank == 0) {\n     int k;\n     for(k = 1; k < rsize; k++) {\n       MPI_Send(&image[k*chunks], chunks, MPI_INT, k, 1, MPI_COMM_WORLD);\n     }\n     if(remainder!= 0) {\n       MPI_Send(&image[k*chunks], remainder, MPI_INT, k, 1, MPI_COMM_WORLD);\n     }\n     for(k = 1; k < rsize; k++) {\n       MPI_Recv(&recv, chunks, MPI_INT, k, 1, MPI_COMM_WORLD, &status);\n     }\n     if(remainder!= 0) {\n       MPI_Recv(&recv, remainder, MPI_INT, k, 1, MPI_COMM_WORLD, &status);\n     }\n   } else {\n     int k;\n     for(k = 0; k < chunks; k++) {\n       temp[image[k]]++;\n     }\n     if(remainder!= 0) {\n       for(k = chunks; k < image.size(); k++) {\n         temp[image[k]]++;\n       }\n     }\n     MPI_Send(&temp, 256, MPI_INT, 0, 1, MPI_COMM_WORLD);\n   }\n   if(rank == 0) {\n     int k;\n     for(k = 0; k < 256; k++) {\n       sum[k] += temp[k];\n     }\n   }\n   if(rank == 0) {\n     for(i = 0; i < 256; i++) {\n       bins[i] = sum[i];\n     }\n   }\n }",
            "/* Your code here */\n}",
            "int num_procs, my_id;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_id);\n\n  int local_image[image.size()];\n  for (int i = 0; i < image.size(); i++) {\n    local_image[i] = image[i];\n  }\n\n  int sum = 0;\n  int count = 0;\n  for (int i = 0; i < image.size(); i++) {\n    sum += local_image[i];\n    count++;\n  }\n\n  MPI_Gather(&sum, 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "// Create a vector to store image.\n  std::vector<int> im(image.size());\n  // Copy data to the vector im.\n  for(int i=0; i < image.size(); i++){\n    im[i] = image[i];\n  }\n  // Initialize MPI\n  int nranks;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // Broadcast the size of the image from the root rank to all other ranks.\n  int img_size;\n  MPI_Bcast(&img_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  // Broadcast the image from the root rank to all other ranks.\n  int* im_array;\n  im_array = new int[img_size];\n  MPI_Bcast(&im_array, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  // Declare a variable to keep track of which part of the image each rank should process.\n  int start = rank * img_size/nranks;\n  int end = (rank+1) * img_size/nranks;\n  if(rank == nranks-1)\n    end = img_size;\n  // Count the number of pixels in the image.\n  int bin_value = 0;\n  for(int i=start; i < end; i++){\n    bin_value = im_array[i];\n    bins[bin_value]++;\n  }\n  // Reduce the bins array on each rank to bins on rank 0.\n  std::array<size_t, 256> bins_reduce;\n  MPI_Reduce(bins.data(), bins_reduce.data(), 256, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank == 0)\n    bins = bins_reduce;\n}",
            "// TODO: use MPI to count the number of pixels with each intensity\n\n}",
            "// Your code here\n  // Use MPI_Reduce to sum values of bins on all procecesses\n  // You can use MPI_Reduce with `MPI_SUM` operation, which is defined in `mpi.h`\n\n}",
            "int numprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: count the number of pixels per intensity\n  // Hint: You can use array[value]++ to increment an element in an array\n\n  MPI_Gather(&bins, 256, MPI_INT, &bins, 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: implement this function\n\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 256> localBins = {0};\n  for (int i = rank * (image.size() / size); i < (rank + 1) * (image.size() / size); i++) {\n    localBins[image[i]]++;\n  }\n\n  std::array<size_t, 256> globalBins;\n  MPI_Reduce(&localBins[0], &globalBins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = globalBins;\n  }\n}",
            "for (int i : image) {\n      bins[i]++;\n   }\n }",
            "// Fill this in\n\n     std::fill(bins.begin(), bins.end(), 0);\n     size_t rank = 0;\n     size_t size = 0;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     std::vector<int> partImage;\n     int partSize = image.size() / size;\n     int remainder = image.size() % size;\n\n     if (rank == 0)\n     {\n         for (int i = 0; i < size - 1; i++)\n         {\n             MPI_Send(image.data() + i * partSize, partSize, MPI_INT, i, 0, MPI_COMM_WORLD);\n         }\n         MPI_Send(image.data() + (size - 1) * partSize, partSize + remainder, MPI_INT, size - 1, 0, MPI_COMM_WORLD);\n\n         for (int i = 1; i < size; i++)\n         {\n             MPI_Recv(bins.data(), 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         }\n     }\n     else\n     {\n         if (rank == size - 1)\n         {\n             partSize += remainder;\n         }\n         MPI_Recv(partImage.data(), partSize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n         for (int i = 0; i < partSize; i++)\n         {\n             bins[partImage[i]]++;\n         }\n\n         MPI_Send(bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n     }\n}",
            "// Your code here\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int length = image.size();\n   int sublength = length / size;\n   std::vector<int> myvector(sublength);\n   if (rank == 0) {\n     for (int i = 1; i < size; i++) {\n       MPI_Send(&image[i * sublength], sublength, MPI_INT, i, 0, MPI_COMM_WORLD);\n     }\n   }\n   if (rank == 0) {\n     for (int i = 0; i < sublength; i++) {\n       myvector[i] = image[i];\n     }\n   } else {\n     MPI_Recv(&myvector[0], sublength, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n   int sum = 0;\n   for (int i = 0; i < sublength; i++) {\n     sum += myvector[i];\n     bins[myvector[i]] += 1;\n   }\n   if (rank!= 0) {\n     MPI_Send(&sum, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   } else {\n     for (int i = 1; i < size; i++) {\n       int recvcount;\n       MPI_Recv(&recvcount, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       bins[0] += recvcount;\n     }\n   }\n }",
            "// YOUR CODE HERE\n\n}",
            "const int N = image.size();\n   int rank, size;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int nums_per_rank = N / size;\n   int nums_to_add = N % size;\n\n   std::vector<int> my_image(nums_per_rank + (rank < nums_to_add? 1 : 0));\n\n   MPI_Scatter(&image[0], nums_per_rank + (rank < nums_to_add? 1 : 0),\n               MPI_INT, &my_image[0], nums_per_rank + (rank < nums_to_add? 1 : 0),\n               MPI_INT, 0, MPI_COMM_WORLD);\n\n   std::array<size_t, 256> bins_local;\n   std::fill(bins_local.begin(), bins_local.end(), 0);\n\n   for (int i = 0; i < my_image.size(); i++) {\n     bins_local[my_image[i]]++;\n   }\n\n   if (rank == 0)\n     std::fill(bins.begin(), bins.end(), 0);\n\n   MPI_Reduce(&bins_local, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n }",
            "// TODO\n  }",
            "// TODO: Your code here!\n    MPI_Status status;\n    int rank;\n    int size;\n    int n = image.size();\n    MPI_Comm_size(MPI_COMM_WORLD,&size);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n    int n_per_process = (n/size);\n    int remain = n%size;\n    std::vector<int> my_image;\n    if(rank == 0){\n        for(int i=0;i<n;i++){\n            my_image.push_back(image[i]);\n        }\n    }\n    else{\n        my_image.resize(n_per_process);\n        for(int i=0;i<n_per_process;i++){\n            my_image[i] = image[n_per_process*rank+i];\n        }\n        if(rank == remain)\n            my_image.resize(n_per_process+1);\n    }\n    int *my_bins = new int[256];\n    for(int i = 0;i<256;i++){\n        my_bins[i] = 0;\n    }\n    for(int i = 0;i<my_image.size();i++){\n        my_bins[my_image[i]]++;\n    }\n    int *global_bins = new int[256];\n    MPI_Reduce(my_bins,global_bins,256,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n    if(rank == 0){\n        for(int i=0;i<256;i++){\n            bins[i] = global_bins[i];\n        }\n    }\n    delete my_bins;\n    delete global_bins;\n}",
            "// 1. Initialize counts to zero\n  // 2. Iterate over the image and increment the counts for each value\n  // 3. Reduce counts with MPI\n  \n\n  // 1. Initialize counts to zero\n  // 2. Iterate over the image and increment the counts for each value\n  \n  // 1. Initialize counts to zero\n  bins.fill(0);\n  for (int &pixel : image)\n    bins[pixel]++;\n  \n  // 2. Iterate over the image and increment the counts for each value\n  \n  \n  // 3. Reduce counts with MPI\n  int n_procs;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n  int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  if (my_rank == 0) {\n    int tmp[256];\n    MPI_Reduce(bins.data(), tmp, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < bins.size(); i++)\n      bins[i] = tmp[i];\n  } else {\n    MPI_Reduce(bins.data(), NULL, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n  \n}",
            "// TODO: Replace this with your code\n}",
            "int numProc;\n     MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n     int rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     int size = image.size();\n     int eachSize = size/numProc;\n     int rem = size%numProc;\n     int count = 0;\n     for(int i = rank*eachSize; i < rank*eachSize + eachSize + (rank < rem); i++) {\n         bins[image[i]] += 1;\n     }\n\n     MPI_Reduce(&bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n }",
            "// Your code goes here!\n }",
            "// TODO\n    const int image_size = image.size();\n    const int root = 0;\n    const int rank = 0;\n    int count = 0;\n\n    if (rank == root){\n      // Count each value in the image and add to bins array\n      for (int i = 0; i < image.size(); i++){\n        count = 0;\n        for (int j = 0; j < image.size(); j++){\n          if (image[i] == image[j]){\n            count++;\n          }\n        }\n        bins[image[i]] = count;\n      }\n    }\n}",
            "// Your code here\n }",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    if (world_size > 256) {\n        if (world_rank == 0) {\n            std::cout << \"Too many MPI processes.\\n\";\n        }\n        MPI_Finalize();\n        exit(0);\n    }\n\n    if (world_rank == 0) {\n        for (auto &el : bins) {\n            el = 0;\n        }\n    }\n\n    for (int i = 0; i < image.size(); i++) {\n        MPI_Send(&image[i], 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n    }\n\n    if (world_rank == 1) {\n        for (int i = 0; i < image.size(); i++) {\n            MPI_Status status;\n            MPI_Recv(&image[i], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n            bins[image[i]]++;\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (world_rank == 1) {\n        for (int i = 0; i < 256; i++) {\n            MPI_Send(&bins[i], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (world_rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            MPI_Status status;\n            MPI_Recv(&bins[i], 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: Implement\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Create a sub-array of the input image to be handled by each rank.\n  int subArraySize = image.size()/size;\n  int subArrayStart = rank*subArraySize;\n  int subArrayEnd = (rank+1)*subArraySize;\n  std::vector<int> subArray(image.begin()+subArrayStart, image.begin()+subArrayEnd);\n  \n  // Initialize bins to 0 on all ranks\n  std::fill(bins.begin(), bins.end(), 0);\n  for (auto &pixel : subArray) {\n    ++bins[pixel];\n  }\n\n  std::vector<int> binsAll(256, 0);\n  MPI_Gather(&bins[0], 256, MPI_UNSIGNED, &binsAll[0], 256, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i=1; i < size; ++i) {\n      for (int j=0; j < 256; ++j) {\n        bins[j] += binsAll[i*256 + j];\n      }\n    }\n  }\n}",
            "//TODO\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD,&my_rank);\n    std::vector<int> subimage;\n    if (my_rank==0) {\n        for (int i=0; i<image.size();i++) {\n            subimage.push_back(image[i]);\n        }\n    } else {\n        for (int i=my_rank; i<image.size();i+=4) {\n            subimage.push_back(image[i]);\n        }\n    }\n    std::array<int, 256> locBin;\n    locBin.fill(0);\n    for (int i=0; i<subimage.size();i++) {\n        locBin[subimage[i]] += 1;\n    }\n    std::array<int, 256> glbBin;\n    glbBin.fill(0);\n    MPI_Reduce(&locBin, &glbBin, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (my_rank==0) {\n        for (int i=0; i<256;i++) {\n            bins[i]=glbBin[i];\n        }\n    }\n}",
            "MPI_Bcast(&image[0], image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Reduce(MPI_IN_PLACE, &bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int myRank, numProcs;\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n   std::vector<int> subImage;\n   int totalSize = image.size();\n   int subSize = totalSize / numProcs;\n   int remainder = totalSize % numProcs;\n\n   if (myRank < remainder) {\n     subSize += 1;\n     subImage.resize(subSize);\n     MPI_Scatter(image.data(), subSize, MPI_INT, subImage.data(), subSize, MPI_INT, 0, MPI_COMM_WORLD);\n   } else {\n     subImage.resize(subSize);\n     MPI_Scatter(image.data(), subSize, MPI_INT, subImage.data(), subSize, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n\n   std::array<size_t, 256> counts;\n   counts.fill(0);\n\n   for (size_t i = 0; i < subImage.size(); ++i) {\n     counts[subImage[i]]++;\n   }\n\n   std::array<size_t, 256> counts_tmp;\n   counts_tmp.fill(0);\n\n   int root = 0;\n   MPI_Reduce(counts.data(), counts_tmp.data(), 256, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n\n   if (myRank == root) {\n     bins = counts_tmp;\n   }\n }",
            "}",
            "// TODO: count the number of pixels with each grayscale intensity\n     // 0-255 and store the results in bins.\n     // Assume image has already been copied to each rank.\n\n     // TODO: use MPI to add up all the counts.\n\n     // TODO: add the result to bins.\n\n }",
            "bins.fill(0);\n    for (int i = 0; i < image.size(); i++){\n        bins[image[i]] += 1;\n    }\n}",
            "int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     std::vector<int> my_image;\n     if (rank == 0) {\n         my_image = image;\n     } else {\n         int count = image.size() / size;\n         int rem = image.size() % size;\n         if (rank <= rem) {\n             count++;\n         }\n         std::vector<int> buf(count);\n         MPI_Scatter(image.data(), count, MPI_INT, buf.data(), count, MPI_INT, 0, MPI_COMM_WORLD);\n         my_image = buf;\n     }\n\n     std::fill(bins.begin(), bins.end(), 0);\n\n     for (auto i : my_image) {\n         ++bins[i];\n     }\n\n     if (rank!= 0) {\n         MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n     }\n }",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int myImageSize = image.size();\n  std::vector<int> myImage(myImageSize);\n\n  MPI_Scatter(image.data(), myImageSize, MPI_INT, myImage.data(), myImageSize, MPI_INT, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 256> myBins;\n  std::fill(myBins.begin(), myBins.end(), 0);\n  for (int i = 0; i < myImageSize; i++) {\n    myBins[myImage[i]]++;\n  }\n\n  MPI_Gather(myBins.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// \n\n}",
            "std::array<size_t, 256> temp;\n   for(int i = 0; i < 256; i++)\n     temp[i] = 0;\n\n   int size = image.size();\n   int rank = 0;\n   int nprocs = 1;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if(rank == 0){\n     for(int i = 0; i < size; i++)\n       bins[image[i]]++;\n   }\n\n   else{\n     for(int i = rank; i < size; i+=nprocs)\n       temp[image[i]]++;\n   }\n\n   if(rank!= 0){\n     MPI_Send(&temp[0], 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n\n   else{\n     std::array<size_t, 256> temp1;\n     for(int i = 0; i < 256; i++)\n       temp1[i] = 0;\n     for(int i = 1; i < nprocs; i++)\n       MPI_Recv(&temp1[0], 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     for(int i = 0; i < 256; i++)\n       bins[i] += temp1[i];\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n }",
            "int size, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<int> myImage(image.size());\n    MPI_Scatter(image.data(), myImage.size(), MPI_INT, myImage.data(), myImage.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    for(auto &x : myImage) {\n        MPI_Reduce(&x, nullptr, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    MPI_Gather(bins.data(), bins.size(), MPI_INT, bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    // TODO: Your code here\n}",
            "// TODO\n }",
            "for (size_t i=0; i < image.size(); ++i) {\n      ++bins[image[i]];\n    }\n  }",
            "// TODO: YOUR CODE HERE\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int *pixel_counts = new int[256];\n\n   int num_pixels = image.size();\n   int chunk_size = num_pixels / size;\n\n   if(rank == 0) {\n      for (int i = 1; i < size; i++) {\n         MPI_Send(&image[i * chunk_size], chunk_size, MPI_INT, i, 0, MPI_COMM_WORLD);\n      }\n   }\n\n   if(rank!= 0) {\n      MPI_Recv(&image[0], num_pixels, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   for (int i = 0; i < 256; i++) {\n      pixel_counts[i] = std::count(image.begin(), image.end(), i);\n   }\n\n   if(rank == 0) {\n      for (int i = 1; i < size; i++) {\n         MPI_Recv(&pixel_counts[0], 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j = 0; j < 256; j++) {\n            pixel_counts[j] += pixel_counts[j];\n         }\n      }\n      bins = std::array<size_t, 256>();\n      for (int i = 0; i < 256; i++) {\n         bins[i] = pixel_counts[i];\n      }\n   } else {\n      MPI_Send(&pixel_counts[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n   delete[] pixel_counts;\n}",
            "int my_rank;\n     int comm_size;\n     MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n     if (my_rank == 0) {\n         // fill with zeros\n         bins.fill(0);\n     }\n     // TODO: use MPI to count in parallel\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]] = bins[image[i]] + 1;\n     }\n\n     int count;\n     // TODO: reduce the local vector bins to a single vector\n     MPI_Reduce(&bins, &count, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     if (my_rank == 0) {\n         std::cout << \"Result:\" << std::endl;\n         for (int i = 0; i < 256; i++) {\n             std::cout << bins[i] << \" \";\n         }\n         std::cout << std::endl;\n     }\n }",
            "}",
            "//std::cout << \"in pixelCounts\\n\";\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     if (rank == 0) {\n         for (int i = 0; i < image.size(); i++) {\n             bins[image[i]]++;\n         }\n     }\n     for (int i = 1; i < size; i++) {\n         std::vector<int> image_copy(image);\n         MPI_Send(image_copy.data(), image_copy.size(), MPI_INT, i, 0, MPI_COMM_WORLD);\n     }\n     int image_size;\n     std::vector<int> image_copy;\n     for (int i = 1; i < size; i++) {\n         MPI_Status status;\n         MPI_Probe(MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n         MPI_Get_count(&status, MPI_INT, &image_size);\n         image_copy.resize(image_size);\n         MPI_Recv(image_copy.data(), image_size, MPI_INT, status.MPI_SOURCE, 0, MPI_COMM_WORLD, &status);\n         for (int j = 0; j < image_copy.size(); j++) {\n             bins[image_copy[j]]++;\n         }\n     }\n }",
            "// TODO: replace this code with your solution\n}",
            "int my_rank, num_proc;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   int image_size = image.size();\n   int image_start = my_rank * image_size / num_proc;\n   int image_end = (my_rank + 1) * image_size / num_proc;\n   for(int i = image_start; i < image_end; i++) {\n     int value = image[i];\n     bins[value] += 1;\n   }\n\n   if(my_rank == 0) {\n     for(int i = 1; i < num_proc; i++) {\n       MPI_Status status;\n       MPI_Recv(bins.data(), 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n     }\n   } else {\n     MPI_Send(bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n }",
            "int world_size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // 1. Count in parallel.\n\n   // 2. Combine the results.\n   \n}",
            "int n=image.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> localImage(n/size,0);\n  std::array<size_t, 256> localBins(256,0);\n  MPI_Scatter(&image[0],n/size,MPI_INT,&localImage[0],n/size,MPI_INT,0,MPI_COMM_WORLD);\n\n  for(int i=0;i<n/size;i++){\n    int value = localImage[i];\n    localBins[value]++;\n  }\n\n  MPI_Reduce(&localBins[0],&bins[0],256,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n}",
            "int mpiSize;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpiSize);\n\n  int mpiRank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpiRank);\n\n  if(mpiSize == 1) {\n    // No need for MPI\n    for(auto pixel : image) {\n      bins.at(pixel)++;\n    }\n  } else {\n    // Use MPI\n    std::vector<int> imagePart(image.size() / mpiSize);\n    MPI_Scatter(image.data(), imagePart.size(), MPI_INT, imagePart.data(), imagePart.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Counts each image part separately\n    for(auto pixel : imagePart) {\n      bins.at(pixel)++;\n    }\n\n    // MPI_Reduce(sendbuffer, recvbuffer, count, datatype, op, root, comm)\n    // Sums the image counts on rank 0.\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "for (int i=0;i<image.size();i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO\n }",
            "size_t num_pixels = image.size();\n  std::vector<int> rank_counts(256, 0); //counts on rank\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  //create local copy of image\n  std::vector<int> local_image(num_pixels/num_ranks, 0);\n  MPI_Scatter(image.data(), num_pixels/num_ranks, MPI_INT, local_image.data(), num_pixels/num_ranks, MPI_INT, 0, MPI_COMM_WORLD);\n\n  //count local image\n  for(size_t i=0; i<local_image.size(); i++)\n    rank_counts[local_image[i]]++;\n\n  //gather counts on rank 0\n  std::vector<int> global_counts(256, 0);\n  MPI_Gather(rank_counts.data(), 256, MPI_INT, global_counts.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if(rank_id == 0)\n    bins = global_counts;\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int sendSize, receiveSize;\n   if (size == 1)\n   {\n     sendSize = image.size();\n     for (int i = 0; i < image.size(); ++i)\n       bins[image[i]] += 1;\n   }\n   else\n   {\n     sendSize = image.size() / size;\n     if (rank < image.size() % size)\n     {\n       sendSize += 1;\n     }\n     std::vector<int> send(sendSize);\n     std::vector<int> receive(sendSize);\n     int startIndex = 0;\n     MPI_Scatter(&image[0], sendSize, MPI_INT, &send[0], sendSize, MPI_INT, 0, MPI_COMM_WORLD);\n     for (int i = 0; i < sendSize; ++i)\n     {\n       bins[send[i]] += 1;\n     }\n     MPI_Gather(&bins[0], sendSize, MPI_INT, &receive[0], sendSize, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n }",
            "// Implement here\n  \n  int my_rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  std::vector<int> my_image(image.size() / num_procs);\n  std::array<size_t, 256> my_bins;\n  my_bins.fill(0);\n  std::vector<int>::iterator it = my_image.begin();\n\n  for (int i = 0; i < image.size(); i++) {\n    if (i % num_procs == my_rank)\n      *(it++) = image[i];\n  }\n\n  for (int i = 0; i < my_image.size(); i++) {\n    my_bins[my_image[i]]++;\n  }\n\n  MPI_Reduce(&my_bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "for(size_t i = 0; i < bins.size(); i++)\n  {\n    bins[i] = 0;\n  }\n\n  int world_size, rank, i;\n  int imageSize = image.size();\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int chunks = world_size;\n  int chunkSize = imageSize/chunks;\n  int remainder = imageSize % chunks;\n\n  int start = rank * chunkSize;\n  int end = start + chunkSize;\n  if(rank == 0)\n  {\n    end = start + remainder;\n  }\n  else if(rank == world_size - 1)\n  {\n    end = imageSize;\n  }\n  for(i = start; i < end; i++)\n  {\n    bins[image[i]]++;\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  if(rank == 0)\n  {\n    for(i = 1; i < chunks; i++)\n    {\n      MPI_Recv(bins.data(), 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n  else\n  {\n    MPI_Send(bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// 1. Find out how many pixels the local process has\n  int n_local = image.size();\n  // 2. Find out how many processes are involved in the computation\n  int n_total = MPI::COMM_WORLD.Get_size();\n  // 3. Find out which rank the local process is\n  int rank = MPI::COMM_WORLD.Get_rank();\n\n  // 4. Initialize the vector of bins\n  // For the purpose of this exercise, assume that MPI_Init has been called\n  // and MPI_Finalize has not.\n  for (int i = 0; i < 256; ++i)\n    bins[i] = 0;\n  \n  // 5. Broadcast the number of local pixels to all processes\n  int n_local_bcast;\n  if (rank == 0)\n    n_local_bcast = n_local;\n  MPI::COMM_WORLD.Bcast(&n_local_bcast, 1, MPI::INT, 0);\n  // 6. Broadcast the local pixels to all processes\n  int *local_image = new int[n_local];\n  for (int i = 0; i < n_local; ++i)\n    local_image[i] = image[i];\n  MPI::COMM_WORLD.Bcast(local_image, n_local, MPI::INT, 0);\n  \n  // 7. Count the number of pixels for each intensity\n  std::array<size_t, 256> local_bins;\n  for (int i = 0; i < 256; ++i)\n    local_bins[i] = 0;\n  \n  for (int i = 0; i < n_local; ++i)\n    local_bins[local_image[i]] += 1;\n\n  // 8. Sum the local bins on the root process\n  if (rank == 0) {\n    std::array<int, 256> local_bins_sum;\n    for (int i = 0; i < 256; ++i)\n      local_bins_sum[i] = 0;\n    \n    for (int i = 0; i < n_total; ++i) {\n      MPI::COMM_WORLD.Recv(local_bins_sum.data(), 256, MPI::INT, i, 0);\n      for (int j = 0; j < 256; ++j)\n        local_bins[j] += local_bins_sum[j];\n    }\n  }\n  else {\n    MPI::COMM_WORLD.Send(local_bins.data(), 256, MPI::INT, 0, 0);\n  }\n}",
            "// TODO 3: your code here\n    MPI_Datatype mytype;\n    MPI_Datatype_contiguous(bins.size(), MPI_UNSIGNED_LONG, &mytype);\n    MPI_Type_commit(&mytype);\n    MPI_Reduce(&bins, NULL, bins.size(), mytype, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Type_free(&mytype);\n    if (MPI_COMM_WORLD.rank == 0) {\n        for (auto i = 0; i < bins.size(); i++) {\n            bins[i] /= MPI_COMM_WORLD.size;\n        }\n    }\n}",
            "int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  /* Your solution goes here */\n  int *local_bins = new int[256];\n\n  for(int i = 0; i < 256; i++)\n    local_bins[i] = 0;\n\n  for(auto i : image) {\n    local_bins[i]++;\n  }\n\n  if(rank == 0) {\n    for(int i = 0; i < 256; i++)\n      bins[i] = local_bins[i];\n  }\n}",
            "// YOUR CODE HERE\n\n    //...\n}",
            "// TODO: implement this function\n}",
            "int mpi_rank, mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    std::vector<int> my_bins;\n    if(mpi_rank==0){\n        my_bins.resize(256);\n    }\n    for(int i=0;i<image.size();i++){\n        my_bins[image[i]]++;\n    }\n    MPI_Gather(my_bins.data(),256,MPI_INT,bins.data(),256,MPI_INT,0,MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int image_size = image.size();\n  int images_per_process = image_size / size;\n  int remainder = image_size % size;\n  int start = rank * images_per_process + std::min(rank, remainder);\n  int end = start + images_per_process + (rank < remainder? 1 : 0);\n  std::vector<int> local_image(image.begin() + start, image.begin() + end);\n\n  std::array<size_t, 256> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  for (size_t i = 0; i < local_image.size(); ++i) {\n    local_bins[local_image[i]]++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    int size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Get the image size (assume square)\n    int N = sqrt(image.size());\n\n    // Compute how many rows of pixels each rank will have\n    // e.g. 4 ranks, 9 rows of pixels -> 2 rows per rank\n    int rowsPerRank = N/size;\n    int remainder = N%size;\n\n    // Compute the number of rows this rank will have\n    int rows = rowsPerRank;\n    if(rank < remainder) {\n        rows += 1;\n    }\n\n    // Compute the starting row for this rank\n    int startRow = rank * rowsPerRank;\n    if(rank < remainder) {\n        startRow += rank;\n    }\n\n    // Compute the end row for this rank\n    int endRow = startRow + rows - 1;\n\n    // Initialise the bins to zero\n    for(int i=0; i<256; i++) {\n        bins[i] = 0;\n    }\n\n    // Loop over the rows that this rank will have\n    for(int row=startRow; row<=endRow; row++) {\n        // Loop over the columns in this row\n        for(int col=0; col<N; col++) {\n            int val = image[row*N+col];\n            bins[val] += 1;\n        }\n    }\n\n    // Reduce all the results together\n    // Use the MPI_IN_PLACE option to avoid needing a temporary array\n    MPI_Reduce(MPI_IN_PLACE, &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// Get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // Get the rank of the process\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Get the name of the processor\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    // Print off a hello world message\n    printf(\"Hello world from processor %s, rank %d out of %d processors\\n\",\n           processor_name, rank, world_size);\n\n    // Your code here!\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Fill `bins` with 0s.\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n\n  // Copy `image` to `local_image`\n  std::vector<int> local_image = image;\n\n  // Count the pixels on rank 0.\n  if (rank == 0) {\n    for (int pixel : image) {\n      bins[pixel]++;\n    }\n  }\n\n  // Count the pixels on all other ranks.\n  if (rank!= 0) {\n    for (int pixel : local_image) {\n      bins[pixel]++;\n    }\n  }\n\n  // Gather all the pixel counts to rank 0.\n  MPI_Gather(MPI_IN_PLACE, 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "}",
            "}",
            "int np, myrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &np);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    int size = image.size();\n    int start, end;\n    start = size / np * myrank;\n    end = size / np * (myrank + 1);\n    std::fill(bins.begin(), bins.end(), 0);\n    std::vector<int> local(end - start);\n    std::copy(image.begin() + start, image.begin() + end, local.begin());\n\n    for(int i = 0; i < local.size(); i++) {\n      bins[local[i]]++;\n    }\n\n    std::vector<int> tmp(bins.size());\n    MPI_Reduce(&bins[0], &tmp[0], bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(myrank == 0) {\n      std::copy(tmp.begin(), tmp.end(), bins.begin());\n    }\n  }",
            "// Your code here\n\n  std::array<size_t, 256> local_bins = {};\n  size_t local_image_size = image.size();\n\n  for (size_t i = 0; i < local_image_size; i++) {\n    local_bins[image[i]]++;\n  }\n\n  int n = 256;\n  MPI_Datatype datatype = MPI_UNSIGNED_LONG;\n  MPI_Op op = MPI_SUM;\n\n  MPI_Reduce(local_bins.data(), bins.data(), n, datatype, op, 0, MPI_COMM_WORLD);\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = image.size();\n    int chunkSize = n / size;\n    int rest = n % size;\n\n    std::vector<int> localImage(chunkSize + (rank < rest? 1 : 0));\n    MPI_Scatter(&image[0], chunkSize + (rank < rest? 1 : 0), MPI_INT, &localImage[0], chunkSize + (rank < rest? 1 : 0), MPI_INT, 0, MPI_COMM_WORLD);\n    std::array<size_t, 256> localBins;\n    localBins.fill(0);\n    for(int i = 0; i < localImage.size(); i++){\n        localBins[localImage[i]]++;\n    }\n    std::array<size_t, 256> globalBins;\n    globalBins.fill(0);\n    MPI_Reduce(&localBins[0], &globalBins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0){\n        bins = globalBins;\n    }\n}",
            "// Use MPI to count the number of pixels with each grayscale intensity in parallel\n   // Put the result in bins\n   \n   // Do not modify the following code\n   if (image.size() == 0) {\n     return;\n   }\n\n   int rank = 0;\n   int nranks = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n   int total_image_size = image.size();\n   int each_block_size = total_image_size / nranks;\n   int block_start = rank * each_block_size;\n\n   if (rank == 0) {\n     for (int i = 1; i < nranks; i++) {\n       int block_start_temp = i * each_block_size;\n       MPI_Send(&(image[block_start_temp]), each_block_size, MPI_INT, i, 0, MPI_COMM_WORLD);\n     }\n   }\n\n   std::vector<int> block_image(each_block_size);\n   MPI_Status status;\n   MPI_Recv(&(block_image[0]), each_block_size, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n\n   for (int i = 0; i < each_block_size; i++) {\n     bins[block_image[i]]++;\n   }\n\n   if (rank!= 0) {\n     MPI_Send(&(bins[0]), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   } else {\n     for (int i = 1; i < nranks; i++) {\n       MPI_Recv(&(bins[0]), 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n     }\n   }\n }",
            "// count number of pixels with value 0, 1,..., 255\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0){\n        for (int i = 0; i < 256; i++){\n            int sum = 0;\n            for (int i = 0; i < image.size(); i++){\n                if (image[i] == i){\n                    sum++;\n                }\n            }\n            bins[i] = sum;\n        }\n    } else {\n        for (int i = 0; i < 256; i++){\n            if (image[i] == i){\n                MPI_Send(&i, 1, MPI_INT, 0, i, MPI_COMM_WORLD);\n            }\n        }\n    }\n}",
            "// TODO\n     int pixel;\n     int i;\n     int rank;\n     int size;\n     int number;\n     MPI_Status status;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     std::vector<int> local_image(image.size() / size);\n     MPI_Scatter(image.data(), 1, MPI_INT, local_image.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n     for (auto& pixel: local_image) {\n         bins[pixel]++;\n     }\n     for (i = 1; i < size; i++) {\n         MPI_Recv(&number, 1, MPI_INT, MPI_ANY_SOURCE, 1, MPI_COMM_WORLD, &status);\n         bins[number]++;\n     }\n     if (rank!= 0) {\n         MPI_Send(&number, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n     }\n     MPI_Barrier(MPI_COMM_WORLD);\n }",
            "// Fill this in!\n}",
            "// YOUR CODE HERE\n}",
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int start;\n    int end;\n    int image_size = image.size();\n    int chunk_size = image_size / size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            start = chunk_size * i;\n            end = start + chunk_size - 1;\n            if (end >= image_size) {\n                end = image_size - 1;\n            }\n            MPI_Send(image.data() + start, chunk_size, MPI_INT, i, 1, MPI_COMM_WORLD);\n        }\n        start = 0;\n        end = chunk_size - 1;\n        if (end >= image_size) {\n            end = image_size - 1;\n        }\n    } else {\n        MPI_Recv(image.data(), chunk_size, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = start; i <= end; i++) {\n            bins[image[i]]++;\n        }\n\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 256> recv_bins;\n            MPI_Recv(recv_bins.data(), 256, MPI_SIZE_T, i, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += recv_bins[j];\n            }\n        }\n\n    } else {\n        for (int i = start; i <= end; i++) {\n            bins[image[i]]++;\n        }\n        MPI_Send(bins.data(), 256, MPI_SIZE_T, 0, 2, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: replace this with your implementation\n  bins.fill(0);\n\n  // For each pixel, increment the corresponding bin\n  for (const auto &pixel : image) {\n    bins.at(pixel) += 1;\n  }\n}",
            "int world_size, world_rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n     std::vector<int> local_image;\n     if(world_rank == 0) {\n         local_image.resize(image.size());\n         for(int i = 1; i < world_size; ++i) {\n             int count = image.size() / world_size;\n             MPI_Scatter(&image[count*i], count, MPI_INT, &local_image[0], count, MPI_INT, 0, MPI_COMM_WORLD);\n         }\n         for(int i = 0; i < image.size() % world_size; ++i) {\n             local_image[i] = image[i];\n         }\n     } else {\n         MPI_Scatter(&image[0], image.size() / world_size + image.size() % world_size, MPI_INT, &local_image[0], image.size() / world_size + image.size() % world_size, MPI_INT, 0, MPI_COMM_WORLD);\n     }\n     for(int i = 0; i < local_image.size(); ++i) {\n         ++bins[local_image[i]];\n     }\n     if(world_rank == 0) {\n         for(int i = 1; i < world_size; ++i) {\n             int count = image.size() / world_size;\n             MPI_Gather(&bins[0], count, MPI_INT, &bins[0], count, MPI_INT, i, MPI_COMM_WORLD);\n         }\n         for(int i = 0; i < image.size() % world_size; ++i) {\n             bins[i] += bins[i + world_size*count];\n         }\n     } else {\n         MPI_Gather(&bins[0], image.size() / world_size + image.size() % world_size, MPI_INT, &bins[0], image.size() / world_size + image.size() % world_size, MPI_INT, 0, MPI_COMM_WORLD);\n     }\n }",
            "// Count the number of pixels in the image, storing the counts in bins.\n  // Hint: you'll need to use std::accumulate.\n  // Example:\n  //     input:  bins = [0, 0, 0,...]\n  //     output: bins = [0, 0, 2, 0, 1,...]\n\n  // TODO 2: complete this function\n   int my_size, my_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &my_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   \n   std::vector<int> my_image;\n   std::vector<int> my_bins(256, 0);\n\n   int chunk_size = image.size() / my_size;\n   int start = my_rank * chunk_size;\n   int end = (my_rank+1) * chunk_size;\n\n   if (my_rank == (my_size - 1))\n    end = image.size();\n\n   my_image.resize(end - start);\n   std::copy(image.begin() + start, image.begin() + end, my_image.begin());\n\n   for (int i = 0; i < my_image.size(); i++) {\n     my_bins[my_image[i]] += 1;\n   }\n\n   if (my_rank == 0)\n    for (int i = 1; i < my_size; i++) {\n      MPI_Recv(my_bins.data(), 256, MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 256; j++)\n        bins[j] += my_bins[j];\n    }\n   else\n    MPI_Send(my_bins.data(), 256, MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n }",
            "int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     const int chunkSize = image.size() / size;\n\n     if (rank == 0) {\n         std::fill(std::begin(bins), std::end(bins), 0);\n     }\n     MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n     int localStart = rank * chunkSize;\n     int localEnd = localStart + chunkSize;\n     if (localEnd > image.size()) {\n         localEnd = image.size();\n     }\n\n     for (int i = localStart; i < localEnd; i++) {\n         bins[image[i]]++;\n     }\n     MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int i = 0, n = image.size();\n  int start = i * n / size;\n  int end = (i + 1) * n / size;\n  int count = 0;\n  for (i = start; i < end; i++)\n    count += image[i];\n\n  int recvcounts[size];\n  int displs[size];\n\n  MPI_Gather(&count, 1, MPI_INT, recvcounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    int sum = 0;\n    displs[0] = 0;\n    for (i = 0; i < size; i++) {\n      bins[i] += recvcounts[i];\n      sum += recvcounts[i];\n      displs[i + 1] = sum;\n    }\n  }\n}",
            "}",
            "// TODO: write this\n}",
            "size_t n = image.size();\n   \n   MPI_Datatype pixelType;\n   MPI_Type_contiguous(1, MPI_INT, &pixelType);\n   MPI_Type_commit(&pixelType);\n   \n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   \n   int nPerRank = n/size;\n   int nRemainder = n % size;\n   \n   std::vector<int> myImage(nPerRank);\n   if (rank!= 0) {\n     MPI_Scatter(&image[0], nPerRank, pixelType, &myImage[0], nPerRank, pixelType, 0, MPI_COMM_WORLD);\n   }\n   \n   if (rank == 0) {\n     for (size_t i=0; i<256; ++i) {\n       bins[i] = 0;\n     }\n     for (int i=1; i<size; ++i) {\n       MPI_Recv(&bins[0], 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     }\n   }\n   \n   if (rank == 0) {\n     for (size_t i=0; i<nPerRank; ++i) {\n       ++bins[myImage[i]];\n     }\n   } else {\n     for (size_t i=0; i<nPerRank; ++i) {\n       ++bins[myImage[i]];\n     }\n     MPI_Send(&bins[0], 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n   \n   MPI_Type_free(&pixelType);\n   \n }",
            "// your code goes here\n }",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    //int size=1;\n    //int rank=0;\n    //int localSize = image.size()/size;\n    //std::vector<int> local(localSize);\n\n    std::array<size_t, 256> local;\n    std::fill(local.begin(), local.end(), 0);\n    size_t count=0;\n    for(int i=0; i < image.size(); i++){\n        if (i % size == rank){\n            local[image[i]]+=1;\n            //count++;\n        }\n    }\n\n    //std::fill(local.begin(), local.end(), 0);\n    //for(int i=0; i < localSize; i++){\n        //local[image[i]]+=1;\n    //}\n\n    MPI_Reduce(local.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    \n    //MPI_Reduce(local.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Use MPI to fill bins\n\n}",
            "// Your code here\n  MPI_Status status;\n  int rank, size;\n  int local_size, local_rank;\n  std::vector<int> local_image;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // create a vector with the same size as the image\n  local_size = image.size();\n  local_rank = rank;\n\n  // allocate a local image with the same size\n  local_image.resize(local_size);\n\n  // distribute the image equally between the processes\n  int offset = local_size / size;\n  int remainder = local_size % size;\n  int begin = rank * offset + std::min(rank, remainder);\n  int end = begin + offset + (rank < remainder? 1 : 0);\n  local_image = std::vector<int>(image.begin() + begin, image.begin() + end);\n\n  // count the number of pixels for each value\n  std::array<size_t, 256> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n  for (auto pixel : local_image)\n    local_bins[pixel]++;\n\n  // accumulate the number of pixels for each value\n  if (rank!= 0) {\n    MPI_Send(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n      for (size_t j = 0; j < local_bins.size(); j++) {\n        bins[j] += local_bins[j];\n      }\n    }\n  }\n}",
            "// TODO\n     MPI_Reduce(&bins,&bins,256,MPI_INT,MPI_SUM,0,MPI_COMM_WORLD);\n\n }",
            "// TODO\n }",
            "// TODO: Implement me!\n    MPI_Reduce(&bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size = 0, rank = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  //...\n}",
            "for(int i = 0; i < image.size(); i++)\n \t{\n \t\tbins[image[i]]++;\n \t}\n }",
            "int p[256];\n\tfor(int i = 0; i < 256; i++){\n\t\tp[i] = 0;\n\t}\n\tfor(int i = 0; i < image.size(); i++){\n\t\tint temp = image[i];\n\t\tp[temp]++;\n\t}\n\n\tint comm_size, my_rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\tint n = image.size();\n\tint n_proc = comm_size;\n\tint n_per_proc = (n + n_proc - 1)/n_proc;\n\tint extra = n % n_proc;\n\tint n_total = 0;\n\n\tint start = my_rank*n_per_proc;\n\tif (my_rank < extra){\n\t\tstart = start + my_rank;\n\t\tn_total = n_per_proc + 1;\n\t}\n\telse{\n\t\tstart = start + extra;\n\t\tn_total = n_per_proc;\n\t}\n\tint end = start + n_total;\n\tend = end > n? n : end;\n\n\tint local_count[256];\n\tfor(int i = 0; i < 256; i++){\n\t\tlocal_count[i] = 0;\n\t}\n\tfor(int i = start; i < end; i++){\n\t\tint temp = image[i];\n\t\tlocal_count[temp]++;\n\t}\n\n\tint n_recv = 0;\n\tint local_recv_count = 0;\n\tint recv_count[n_proc];\n\tint recv_displ[n_proc];\n\n\tif (my_rank == 0){\n\t\tfor(int i = 1; i < n_proc; i++){\n\t\t\tMPI_Recv(&n_recv, 1, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\trecv_count[i] = n_recv;\n\t\t\trecv_displ[i] = i * 256;\n\t\t}\n\t}\n\telse{\n\t\tMPI_Send(&n_total, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n\t}\n\n\tif (my_rank == 0){\n\t\tfor(int i = 1; i < n_proc; i++){\n\t\t\tMPI_Recv(&local_count[0], 256, MPI_INT, i, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t\tfor(int i = 1; i < n_proc; i++){\n\t\t\tfor(int j = 0; j < 256; j++){\n\t\t\t\tbins[j + recv_displ[i]] += local_count[j];\n\t\t\t}\n\t\t}\n\t}\n\telse{\n\t\tMPI_Send(&local_count[0], 256, MPI_INT, 0, 2, MPI_COMM_WORLD);\n\t}\n}",
            "// TODO\n     int size;\n     int rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     const size_t npixels = image.size();\n     const size_t chunk_size = npixels / size;\n     const size_t left_over = npixels % size;\n\n     size_t begin = rank * chunk_size;\n     if(left_over > rank)\n     {\n        begin += left_over;\n        ++begin;\n     }\n     else if(rank >= left_over)\n     {\n        begin += left_over;\n     }\n\n     size_t end = (rank + 1) * chunk_size;\n     if(rank + 1 >= left_over)\n     {\n        end += left_over;\n     }\n\n     size_t counter = 0;\n     std::array<size_t, 256> local_counter{0};\n     for (int i = begin; i < end; ++i)\n     {\n         ++local_counter[image[i]];\n     }\n\n     if(rank == 0)\n     {\n        for (int i = 1; i < size; ++i)\n        {\n            std::array<size_t, 256> temp_counter{0};\n            MPI_Recv(temp_counter.data(), 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; ++j)\n            {\n                bins[j] += temp_counter[j];\n            }\n        }\n     }\n     else\n     {\n        MPI_Send(local_counter.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n     }\n }",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n\n  std::vector<int> local_image = image;\n\n  std::array<size_t,256> local_bins = bins;\n  local_bins.fill(0);\n\n  for (int i = rank; i < local_image.size(); i += size) {\n    local_bins[local_image[i]]++;\n  }\n\n  MPI_Reduce(&local_bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int size, rank, i, j;\n   int my_size, my_rank;\n   std::array<size_t, 256> my_bins;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   my_size = image.size();\n   my_rank = size / my_size;\n   std::vector<int> my_image(my_size);\n   MPI_Scatter(&image[0], my_size, MPI_INT, &my_image[0], my_size, MPI_INT, 0, MPI_COMM_WORLD);\n   for (i = 0; i < 256; i++) {\n     my_bins[i] = 0;\n   }\n\n   for (i = 0; i < my_size; i++) {\n     my_bins[my_image[i]]++;\n   }\n\n   std::array<size_t, 256> results;\n   MPI_Gather(&my_bins, 256, MPI_UNSIGNED_LONG_LONG, &results, 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n     for (i = 0; i < 256; i++) {\n       bins[i] = results[i];\n     }\n   }\n}",
            "// Your code here\n\n}",
            "// YOUR CODE HERE\n\n}",
            "// TODO: Your code here\n   for(auto i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n\n }",
            "}",
            "// Get size of the communicator\n   int nRanks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank!= 0) {\n     // Create new array of the same size as bins to count local pixels\n     std::array<size_t, 256> localBins;\n\n     // Iterate over each pixel in local image\n     for (auto const& pixel : image) {\n       localBins[pixel]++;\n     }\n\n     // Gather the local results in the root node\n     MPI_Gather(&localBins, 256, MPI_SIZE_T, &bins, 256, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n   } else {\n     // Iterate over each pixel in image\n     for (auto const& pixel : image) {\n       bins[pixel]++;\n     }\n\n     // Gather the local results in the root node\n     MPI_Gather(&bins, 256, MPI_SIZE_T, &bins, 256, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n   }\n}",
            "int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   int num_procs;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   // TODO: fill in your code here\n\n }",
            "// TODO: Add your solution here\n    for(int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "// Use MPI_Comm_size to get the number of ranks\n     // Use MPI_Comm_rank to get the rank of the current process\n     // Use MPI_Reduce to sum the local counts from every process\n     int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     std::array<size_t, 256> localBins;\n     localBins.fill(0);\n     for (int i = 0; i < image.size(); i++) {\n         localBins[image[i]]++;\n     }\n     MPI_Reduce(&localBins[0], &bins[0], 256, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n }",
            "// TODO: replace this code with your own implementation\n    for (int rank = 0; rank < 256; rank++) {\n        bins[rank] = 0;\n    }\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "//TODO\n }",
            "}",
            "int numProcs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // allocate local bins\n  std::array<size_t, 256> localBins{};\n\n  // partition work between processes\n  size_t perProc = image.size() / numProcs;\n  size_t leftover = image.size() % numProcs;\n\n  // process 0-leftover-1 have one extra element to process\n  size_t start = rank * perProc;\n  size_t end = (rank == numProcs - 1)? image.size() : start + perProc + (rank < leftover);\n\n  // count in local bins\n  for (size_t i = start; i < end; i++) {\n    localBins[image[i]]++;\n  }\n\n  // combine local results into global results\n  if (rank == 0) {\n    for (int p = 1; p < numProcs; p++) {\n      MPI_Recv(bins.data() + perProc * p + leftover, perProc + (p < leftover),\n               MPI_UNSIGNED_LONG, p, p, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(localBins.data(), localBins.size(), MPI_UNSIGNED_LONG, 0, rank, MPI_COMM_WORLD);\n  }\n}",
            "const int root = 0;\n    const int n_ranks = 3;\n    const int image_size = image.size();\n\n    std::vector<int> sub_image(image_size / n_ranks);\n    std::array<size_t, 256> sub_bins(256);\n\n    int local_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &local_rank);\n\n    if (local_rank == root) {\n        MPI_Gather(&sub_bins, 256, MPI_INT, &bins, 256, MPI_INT, root, MPI_COMM_WORLD);\n    } else {\n        MPI_Gather(&sub_image, sub_image.size(), MPI_INT, &sub_bins, 256, MPI_INT, root, MPI_COMM_WORLD);\n        for (size_t i = 0; i < sub_image.size(); i++) {\n            sub_bins[sub_image[i]]++;\n        }\n        MPI_Gather(&sub_bins, 256, MPI_INT, &bins, 256, MPI_INT, root, MPI_COMM_WORLD);\n    }\n}",
            "int numprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0) {\n        //std::cout << \"rank = \" << rank << std::endl;\n        //std::cout << \"image size = \" << image.size() << std::endl;\n        //std::cout << \"numprocs = \" << numprocs << std::endl;\n        //std::cout << \"bins size = \" << bins.size() << std::endl;\n        //std::cout << \"image = \" << image[0] << \", \" << image[1] << std::endl;\n        //std::cout << \"bins = \" << bins[0] << \", \" << bins[1] << std::endl;\n    }\n    std::vector<size_t> local_bins(256, 0);\n    for(size_t i = 0; i < image.size(); ++i) {\n        local_bins[image[i]] += 1;\n    }\n    std::vector<size_t> global_bins(256, 0);\n    MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0) {\n        bins = std::array<size_t, 256>{global_bins.begin(), global_bins.end()};\n    }\n}",
            "int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   // Compute the number of elements in each rank\n   size_t total_elements = image.size();\n   size_t elements_per_rank = total_elements / world_size;\n   size_t remainder = total_elements % world_size;\n   if (my_rank < remainder) {\n     elements_per_rank++;\n   }\n\n   // Compute the range of elements for this rank\n   size_t elements_start = my_rank * elements_per_rank;\n   size_t elements_end = (my_rank + 1) * elements_per_rank;\n   if (my_rank == world_size - 1) {\n     elements_end = total_elements;\n   }\n\n   // Count in parallel\n   int count;\n   MPI_Reduce(&image[elements_start], &count, elements_per_rank, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Gather the counts from all ranks into the array\n   int counts[256] = {0};\n   MPI_Gather(counts, 256, MPI_INT, counts, 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // Copy the counts into the bins array\n   if (my_rank == 0) {\n     for (size_t i = 0; i < 256; i++) {\n       bins[i] = counts[i];\n     }\n   }\n}",
            "int numprocs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n     for (int x = 0; x < image.size(); x++) {\n       bins.at(image[x])++;\n     }\n   }\n   else {\n     std::vector<int> local_image;\n     for (int x = rank; x < image.size(); x += numprocs) {\n       local_image.push_back(image[x]);\n     }\n     std::array<size_t, 256> local_bins;\n     for (int x = 0; x < local_bins.size(); x++) {\n       local_bins.at(x) = 0;\n     }\n     for (int x = 0; x < local_image.size(); x++) {\n       local_bins.at(local_image[x])++;\n     }\n     MPI_Send(local_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n\n   if (rank == 0) {\n     for (int i = 1; i < numprocs; i++) {\n       std::array<size_t, 256> temp_bins;\n       MPI_Status status;\n       MPI_Recv(temp_bins.data(), temp_bins.size(), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n       for (int x = 0; x < 256; x++) {\n         bins.at(x) += temp_bins.at(x);\n       }\n     }\n   }\n}",
            "// TODO: implement\n  \n  MPI_Barrier(MPI_COMM_WORLD);\n  int size,rank;\n  MPI_Comm_size(MPI_COMM_WORLD,&size);\n  MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\n  int n=image.size();\n  int n_per_proc=n/size;\n  int remainder=n%size;\n  int x;\n\n  std::vector<int> p_count;\n\n  if(rank==0) {\n      p_count.resize(256);\n      for(int i=0; i<256; i++)\n        p_count[i]=0;\n  }\n\n  if(rank==0) {\n    p_count[image[0]]++;\n    for(int i=1; i<n_per_proc; i++) {\n      p_count[image[i]]++;\n    }\n    x=n_per_proc;\n  }\n  else {\n    p_count.resize(256);\n    for(int i=0; i<256; i++)\n      p_count[i]=0;\n    for(int i=0; i<n_per_proc; i++) {\n      p_count[image[i]]++;\n    }\n    x=n_per_proc;\n  }\n  \n\n  int* temp=new int[x];\n\n  for(int i=0; i<x; i++)\n    temp[i]=p_count[image[i]];\n\n  MPI_Gather(temp,x,MPI_INT,bins.data(),x,MPI_INT,0,MPI_COMM_WORLD);\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if(rank==0) {\n    for(int i=n_per_proc; i<n_per_proc+remainder; i++) {\n      p_count[image[i]]++;\n    }\n    for(int i=0; i<remainder; i++)\n      bins[image[i+n_per_proc]]=p_count[image[i+n_per_proc]];\n  }\n\n}",
            "// MPI Variables\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute the number of bins in each process\n  int nbins_per_proc = 256 / size;\n  int remaining_bins = 256 % size;\n\n  std::array<size_t, 256> local_bins;\n  // Initialize local_bins to zero\n  for (size_t i = 0; i < 256; i++) {\n    local_bins[i] = 0;\n  }\n\n  // Compute the bin counts for each process\n  for (auto pix: image) {\n    local_bins[pix]++;\n  }\n\n  // Collect the local_bins\n  std::vector<int> send_data(256);\n  std::vector<int> recv_data(256);\n  for (int i = 0; i < 256; i++) {\n    send_data[i] = local_bins[i];\n  }\n\n  // Collect the data\n  MPI_Gather(send_data.data(), 256, MPI_INT, recv_data.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 256; i++) {\n      bins[i] = recv_data[i];\n    }\n  }\n}",
            "std::vector<int> local(image.size());\n    int n_size = image.size();\n    MPI_Scatter(image.data(), n_size, MPI_INT, local.data(), n_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::array<int, 256> local_bin;\n    std::fill(local_bin.begin(), local_bin.end(), 0);\n\n    for (int i : local) {\n        local_bin[i]++;\n    }\n\n    MPI_Gather(local_bin.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  std::vector<int> bins_local;\n  std::vector<int> image_local;\n  \n  image_local.resize(image.size());\n  if (rank == 0) {\n    image_local = image;\n  }\n  bins_local.resize(256);\n  int count = image.size()/size;\n  MPI_Scatter(image_local.data(), count, MPI_INT, image_local.data(), count, MPI_INT, 0, MPI_COMM_WORLD);\n  \n  for (int i = 0; i < image_local.size(); i++) {\n    bins_local[image_local[i]] += 1;\n  }\n  \n  MPI_Gather(bins_local.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// YOUR CODE HERE\n    if (image.size() == 0) {\n        return;\n    }\n    int mySize = image.size();\n    int numProc;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n    int myRank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    int myStart, mySize;\n    MPI_Scatter(&mySize, 1, MPI_INT, &mySize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (myRank == 0) {\n        myStart = 0;\n        mySize = mySize / numProc;\n        MPI_Scatter(image.data(), mySize, MPI_INT, image.data(), mySize, MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Scatter(&mySize, 1, MPI_INT, &mySize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        myStart = myRank * mySize;\n    }\n    std::array<size_t, 256> myBins;\n    for (int i = 0; i < 256; i++) {\n        myBins[i] = 0;\n    }\n    for (int i = 0; i < mySize; i++) {\n        myBins[image[i]] += 1;\n    }\n    std::array<size_t, 256> allBins;\n    MPI_Gather(myBins.data(), 256, MPI_INT, allBins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n    if (myRank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = allBins[i];\n        }\n    }\n}",
            "int rank = 0, size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // your code here\n  int n = image.size();\n  std::array<size_t, 256> res = {0};\n  if (rank == 0) {\n    for (int i = 0; i < n; i++) res[image[i]]++;\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 256> temp;\n      MPI_Recv(temp.data(), 256, MPI_SIZE_T, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 256; j++) {\n        res[j] += temp[j];\n      }\n    }\n  } else {\n    for (int i = 0; i < n; i++) res[image[i]]++;\n    MPI_Send(res.data(), 256, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n  }\n  if (rank == 0) bins = res;\n}",
            "// TODO: Add your code here\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int root = 0;\n  int count;\n  std::array<size_t, 256> localBins;\n  if (rank == root) {\n    count = image.size();\n    MPI_Bcast(&count, 1, MPI_INT, root, MPI_COMM_WORLD);\n  } else {\n    MPI_Bcast(&count, 1, MPI_INT, root, MPI_COMM_WORLD);\n  }\n\n  for (int i = 0; i < count; i++) {\n    localBins[image[i]]++;\n  }\n\n  if (rank == root) {\n    for (int i = 0; i < 256; i++) {\n      MPI_Reduce(MPI_IN_PLACE, &localBins[i], 1, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Reduce(&localBins[0], &localBins[0], 256, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n  }\n  if (rank == root) {\n    for (int i = 0; i < 256; i++) {\n      bins[i] = localBins[i];\n    }\n  }\n}",
            "const int rank = 0; // FIXME: Replace 0 with my rank\n    const int size = 0; // FIXME: Replace 0 with the number of processes\n    const int chunk = 0; // FIXME: Replace 0 with the chunk size\n\n    // FIXME: Replace this array with a vector of size 256\n    // (i.e., one element per intensity level)\n    std::array<int, 256> counts;\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            counts[i] = 0;\n        }\n    }\n\n    // FIXME: Replace 0s with my rank and size\n    MPI_Scatter(&counts, chunk, MPI_INT,\n                &counts, chunk, MPI_INT,\n                0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < chunk; i++) {\n        counts[image[i]] += 1;\n    }\n\n    // FIXME: Replace 0s with my rank and size\n    MPI_Gather(&counts, chunk, MPI_INT,\n               &counts, chunk, MPI_INT,\n               0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = counts[i];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    // add code here\n    if (x < N)\n        atomicAdd(&bins[image[x]], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "const int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// The index of this thread (0..N-1)\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid < N) {\n      atomicAdd(&bins[image[tid]], 1);\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "// Your code here\n\n    int x = threadIdx.x;\n    int y = threadIdx.y;\n    int i = x*gridDim.y + y;\n\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int x = blockIdx.x*blockDim.x + threadIdx.x;\n\tif (x < N) {\n\t\tatomicAdd(&bins[image[x]], 1);\n\t}\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) bins[image[id]]++;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N)\n        atomicAdd(&bins[image[i]], 1);\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "const size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n  if(i < N)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    bins[image[i]] += 1;\n  }\n}",
            "// TODO: fill this in\n}",
            "}",
            "// The current thread index in the array\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    __atomic_fetch_add(&bins[image[index]], 1, __ATOMIC_RELAXED);\n  }\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n   const int idx = image[i];\n   atomicAdd(&bins[idx], 1);\n}",
            "// Insert your code here\n\n}",
            "int tid = threadIdx.x;\n    int blockId = blockIdx.x;\n    int blockSize = blockDim.x;\n    __shared__ int counts[256];\n\n    if (tid < 256) {\n        counts[tid] = 0;\n    }\n    __syncthreads();\n\n    for (int i = blockId * blockSize + tid; i < N; i += gridDim.x * blockSize) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "}",
            "}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    size_t pixel = image[idx];\n    atomicAdd(&(bins[pixel]), 1);\n}",
            "size_t index = threadIdx.x;\n  if (index < N) {\n    bins[image[index]] += 1;\n  }\n}",
            "// Your code here\n\n}",
            "// TODO: implement kernel\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "// TODO: Implement this function\n    unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n    int pix = image[i];\n    atomicAdd(&bins[pix], 1);\n}",
            "// TODO\n}",
            "/* Add your code here */\n\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    // printf(\"pixelCounts: tid=%d, image[tid]=%d\\n\", tid, image[tid]);\n    atomicAdd(&(bins[image[tid]]), 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// Compute the index of the current thread.\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  // Only threads with indices in the range [0, N) will contribute to the histogram\n  // as they will read from a valid position in the image.\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t thread = blockIdx.x * blockDim.x + threadIdx.x;\n    if (thread < N) {\n        atomicAdd(&bins[image[thread]], 1);\n    }\n}",
            "__shared__ int block_bins[256];\n\n   // Initialize the shared memory\n   for (int i = threadIdx.x; i < 256; i += blockDim.x)\n      block_bins[i] = 0;\n\n   __syncthreads();\n\n   // Increment the value for each pixel\n   for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x)\n      atomicAdd(&block_bins[image[i]], 1);\n\n   __syncthreads();\n\n   // Merge the values from each thread into global memory\n   for (int i = threadIdx.x; i < 256; i += blockDim.x)\n      atomicAdd(&bins[i], block_bins[i]);\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // If our thread is not assigned a pixel, then it should exit the kernel\n    if (idx >= N)\n        return;\n\n    // Increment the bin with the value of the pixel by 1\n    atomicAdd(&(bins[image[idx]]), 1);\n}",
            "__shared__ size_t threadBins[256];\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(&threadBins[image[tid]], 1);\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    for (int i=0; i<256; i++) {\n      atomicAdd(&bins[i], threadBins[i]);\n    }\n  }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) return;\n\n  const int pixel = image[tid];\n  atomicAdd(&bins[pixel], 1);\n}",
            "int index = blockDim.x*blockIdx.x+threadIdx.x;\n   if (index < N) {\n       bins[image[index]]++;\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t j;\n    if (i < N) {\n        j = image[i];\n        atomicAdd(&bins[j], 1);\n    }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\n   if (idx < N) {\n      atomicAdd(&bins[image[idx]], 1);\n   }\n}",
            "}",
            "__shared__ size_t bin_counts[256]; // global memory\n  int tId = threadIdx.x; // thread ID\n  int bId = blockIdx.x; // block ID\n\n  if (bId < 256 && tId == 0) {\n    bin_counts[bId] = 0;\n  }\n  __syncthreads(); // wait for initialization\n  for (size_t i = bId*blockDim.x + tId; i < N; i += blockDim.x*gridDim.x) {\n    atomicAdd(&bin_counts[image[i]], 1);\n  }\n  __syncthreads(); // wait for counts\n  if (tId == 0) {\n    atomicAdd(&bins[bId], bin_counts[bId]);\n  }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n   int bin = image[index];\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   // TODO: Replace the 0 with the correct CUDA code to count pixels\n   // TODO: Use `atomicAdd` to safely increment bins[image[i]]\n   if (i < N)\n   atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "unsigned int threadId = threadIdx.x + blockIdx.x*blockDim.x;\n    if (threadId < N) {\n        atomicAdd(&bins[image[threadId]], 1);\n    }\n}",
            "const size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N)\n    {\n        atomicAdd(&bins[image[id]], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i >= N) return;\n\n  atomicAdd(&bins[image[i]], 1);\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadId < N) {\n        atomicAdd(&bins[image[threadId]], 1);\n    }\n}",
            "// get the thread index\n    unsigned int i = threadIdx.x;\n    // set the value to 0\n    bins[i] = 0;\n\n    while (i < N) {\n        // check if it's the same grayscale intensity\n        if (image[i] == i) {\n            // increment by one\n            atomicAdd(&bins[i], 1);\n        }\n        // move to the next element\n        i += blockDim.x;\n    }\n}",
            "const int tid = threadIdx.x;\n  __shared__ int s_bins[256];\n\n  // Initialize shared memory\n  if (tid < 256) {\n    s_bins[tid] = 0;\n  }\n\n  // Wait for all threads to be ready\n  __syncthreads();\n\n  for (int i = tid; i < N; i += blockDim.x) {\n    atomicAdd(&s_bins[image[i]], 1);\n  }\n\n  __syncthreads();\n\n  // Write the bins back to global memory\n  if (tid < 256) {\n    atomicAdd(&bins[tid], s_bins[tid]);\n  }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "__shared__ size_t counts[256];\n  // TODO: Your code here\n  __syncthreads();\n  for (size_t i=0; i<256; i++)\n    atomicAdd(&bins[i], counts[i]);\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N) {\n    return;\n  }\n\n  int grayscale = image[index];\n  atomicAdd(&bins[grayscale], 1);\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// Implement this.\n}",
            "const int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if(i < N) {\n      atomicAdd(&bins[image[i]], 1);\n   }\n}",
            "}",
            "__shared__ int hist[256];\n  int id = threadIdx.x;\n  if (id < 256) {\n    hist[id] = 0;\n  }\n  __syncthreads();\n\n  // Count the number of pixels for each grayscale intensity.\n  // Hint: use an if statement to check the pixel value.\n  if (id < N) {\n    int pixel = image[id];\n    if (pixel >= 0 && pixel < 256) {\n      atomicAdd(&hist[pixel], 1);\n    }\n  }\n\n  // Sum the histograms to a single value.\n  for (int s = 16; s > 0; s /= 2) {\n    if (id < s) {\n      hist[id] += hist[id + s];\n    }\n    __syncthreads();\n  }\n\n  // Store the result into `bins`\n  if (id == 0) {\n    for (int i = 0; i < 256; i++) {\n      bins[i] = hist[i];\n    }\n  }\n}",
            "unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// Compute the index into the bins array.\n  int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N)\n        atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO:\n    int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    if(i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n\n  for (int i = index; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "/* YOUR CODE HERE */\n   int index = threadIdx.x + blockIdx.x * blockDim.x;\n   while (index < N)\n   {\n     atomicAdd(&bins[image[index]], 1);\n     index += blockDim.x * gridDim.x;\n   }\n   /* END OF YOUR CODE */\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO:\n  int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// The thread ID\n  const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N)\n    return;\n  \n  // Get the pixel value and count the corresponding bin\n  const int value = image[tid];\n  atomicAdd(bins+value, 1);\n}",
            "}",
            "// TODO\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int pixel = 0;\n  if(tid < N) {\n    pixel = image[tid];\n  }\n  atomicAdd(&bins[pixel], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if(idx < N) {\n      atomicAdd(&(bins[image[idx]]), 1);\n   }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N)\n        atomicAdd(&bins[image[idx]], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "// TODO\n    //...\n}",
            "int idx = threadIdx.x;\n  int val = image[idx];\n  atomicAdd(&bins[val], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// Set the thread index and image index.\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int i = tid + blockIdx.y * blockDim.x * gridDim.x;\n\n  // Check that the thread index is less than the image length.\n  if (i < N) {\n    // Get the grayscale value from the image.\n    // The value will be between 0 and 255.\n    int pixel = image[i];\n\n    // Increment the count in `bins` for the grayscale value in `pixel`.\n    atomicAdd(&bins[pixel], 1);\n  }\n}",
            "const size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N) { return; }\n  atomicAdd(&bins[image[index]], 1);\n}",
            "size_t offset = threadIdx.x;\n    if (offset < N) {\n        int value = image[offset];\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "// Your code here\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    // Each thread takes care of one pixel\n    if (i < N) {\n        // Increment the count for the given grayscale intensity\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// YOUR CODE HERE\n\n  // Fill in the kernel to count the number of occurrences of each grayscale\n  // value in the image.\n  // The number of bins should be 256.\n  // Use atomicAdd to prevent race conditions.\n  //\n  // Remember to use a for-loop to iterate over the entire image.\n  // Note that the image is a one-dimensional array with no strides.\n  // You can index into the array in the same way you'd index into a\n  // matrix, e.g. image[x + y * width]\n\n  // TODO\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if(idx < N)\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "// Fill in your code here\n}",
            "int bin[256];\n  memset(bin, 0, sizeof(bin));\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bin[image[idx]], 1);\n  }\n  __syncthreads();\n  size_t i = threadIdx.x;\n  while (i < 256) {\n    atomicAdd(&bins[i], bin[i]);\n    i += blockDim.x;\n  }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = gridDim.x * blockDim.x;\n\n  for (size_t i = index; i < N; i += stride) {\n    size_t pixelValue = image[i];\n    atomicAdd(&bins[pixelValue], 1);\n  }\n}",
            "// TODO\n  int i;\n  i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N)\n    atomicAdd(&bins[image[i]], 1);\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // TODO: Implement\n  __syncthreads();\n}",
            "//...\n}",
            "}",
            "__shared__ int bin[256];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  // Initialize local memory to 0\n  if (tid < 256) {\n    bin[tid] = 0;\n  }\n  __syncthreads();\n\n  // Increment local memory\n  if (tid < 256 && bid < N) {\n    int val = image[bid];\n    atomicAdd(&bin[val], 1);\n  }\n  __syncthreads();\n\n  // Copy back to global memory\n  if (tid < 256) {\n    atomicAdd(&bins[tid], bin[tid]);\n  }\n  __syncthreads();\n}",
            "int i = threadIdx.x;\n  if (i < N) bins[image[i]]++;\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) bins[image[i]]++;\n}",
            "// TODO\n}",
            "// TODO: Count the number of pixels in image with each grayscale intensity.\n  //       Store the results in bins\n  //       Use a shared memory array to keep track of the counts,\n  //       and then update the global bins array once per thread.\n  // Hint: You can use atomicAdd() to safely add values to the global array.\n  __shared__ int shared[32];\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int id = threadIdx.x + blockIdx.x*blockDim.x;\n\n  if (id < N) {\n    atomicAdd(&(bins[image[id]]), 1);\n  }\n\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t j;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: fill this in\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n    if (index < N)\n        atomicAdd(&bins[image[index]], 1);\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int gid = tid + blockDim.x * bid;\n\n    // Check if threadIdx.x is less than N\n    if (gid < N) {\n        atomicAdd(&bins[image[gid]], 1);\n    }\n}",
            "}",
            "const int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if(tid < N) {\n        atomicAdd(bins+image[tid], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    // bins[image[idx]]++;\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// TODO\n  // Hint:\n  // - You have to declare a shared memory variable in the kernel\n  // - You have to use __syncthreads() in the kernel\n  // - Avoid collisions of threads in shared memory\n}",
            "// your code here\n    //__syncthreads();\n    atomicAdd(bins + image[threadIdx.x], 1);\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (threadId < N) {\n        atomicAdd(&(bins[image[threadId]]), 1);\n    }\n}",
            "//TODO: Write CUDA kernel to compute histogram.\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index >= N) return;\n    atomicAdd(&bins[image[index]], 1);\n}",
            "/* Each thread handles one pixel. */\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n\n  /* Update the histogram. */\n  atomicAdd(&bins[image[i]], 1);\n}",
            "// Use an atomically incrementing integer to store the number of pixels\n   // that each thread updates in its corresponding bin.\n   unsigned int numPixels = 0;\n   // Find the index of this thread in the array.\n   size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      // Each thread atomically increments the number of pixels with its value.\n      atomicAdd(&bins[image[i]], 1);\n      numPixels += 1;\n   }\n   // Use a shared memory array to store how many pixels each thread updates.\n   __shared__ unsigned int numPixelsShared[32];\n   // Get the index of the first thread in the block.\n   unsigned int threadID = threadIdx.x;\n   // Store the number of pixels updated by this thread.\n   numPixelsShared[threadID] = numPixels;\n   // Wait for all threads in the block to finish.\n   __syncthreads();\n   // Find the total number of pixels updated by all the threads in this block.\n   unsigned int totalPixels = 0;\n   // Get the number of threads in the block.\n   unsigned int numThreads = blockDim.x;\n   for (unsigned int i = 0; i < numThreads; i++) {\n      totalPixels += numPixelsShared[i];\n   }\n   // Find the index of the thread in the array.\n   size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n   // Update the number of pixels in the array `bins`.\n   atomicAdd(&bins[image[index]], totalPixels);\n}",
            "// TODO\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "size_t idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx >= N) { return; }\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "int x = threadIdx.x + blockDim.x * blockIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "// get this thread's index\n   int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // get this thread's value\n   if (i < N)\n      atomicAdd(&bins[image[i]], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "__shared__ int cache[THREADS_PER_BLOCK];\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  int x = 0;\n  if (i < N)\n    x = image[i];\n  atomicAdd(&bins[x], 1);\n}",
            "// Get the thread id\n    size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) {\n        atomicAdd(&bins[image[id]], 1);\n    }\n}",
            "// Implement this\n}",
            "// Get a reference to the global memory block \"bins\"\n    extern __shared__ size_t binsShared[];\n\n    // The index of the current thread\n    size_t idx = threadIdx.x;\n\n    // Initialize a shared memory copy of bins\n    for (size_t i = idx; i < 256; i += blockDim.x)\n        binsShared[i] = 0;\n\n    // Synchronize all threads to make sure that they have copied bins\n    __syncthreads();\n\n    // Count the number of pixels with each grayscale intensity\n    // For each pixel\n    for (size_t i = idx; i < N; i += blockDim.x) {\n        // Increment the number of pixels with intensity `pixel`\n        atomicAdd(&binsShared[image[i]], 1);\n    }\n\n    // Synchronize all threads to make sure that they have finished counting\n    __syncthreads();\n\n    // Write shared memory back to global memory\n    for (size_t i = idx; i < 256; i += blockDim.x)\n        bins[i] = binsShared[i];\n}",
            "const int index = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (index < N)\n\t\tatomicAdd(&bins[image[index]], 1);\n}",
            "__shared__ int sharedImage[256];\n\n  int index = threadIdx.x;\n  if (index < 256) {\n    sharedImage[index] = 0;\n  }\n  __syncthreads();\n\n  if (index < N) {\n    atomicAdd(&(sharedImage[image[index]]), 1);\n  }\n  __syncthreads();\n\n  if (index < 256) {\n    atomicAdd(&(bins[index]), sharedImage[index]);\n  }\n}",
            "// TODO: count pixels with each grayscale intensity in parallel\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N)\n    return;\n  atomicAdd(&bins[image[idx]], 1);\n}",
            "/* TODO */\n}",
            "/* Your code here */\n}",
            "// TODO: Implement this\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) bins[image[i]]++;\n}",
            "}",
            "// Fill in the kernel code here\n}",
            "int tid = threadIdx.x;\n  if (tid < 256) {\n    bins[tid] = 0;\n  }\n  __syncthreads();\n\n  if (tid < N) {\n    int value = image[tid];\n    atomicAdd(&bins[value], 1);\n  }\n  __syncthreads();\n}",
            "// TODO: Replace this with your own code\n  // int index = blockIdx.x*blockDim.x + threadIdx.x;\n  // if (index < N) {\n  //   atomicAdd(&bins[image[index]], 1);\n  // }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // TODO: Complete this kernel\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "int my_index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (my_index < N) {\n        atomicAdd(&bins[image[my_index]], 1);\n    }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x >= N)\n    return;\n  // Add the number of pixels that are equal to x\n  atomicAdd(&bins[image[x]], 1);\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  //...\n}",
            "}",
            "// Add code here\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadId < N) {\n        atomicAdd(&bins[image[threadId]], 1);\n    }\n}",
            "/* TODO */\n}",
            "__shared__ size_t my_bins[256];\n  // initialize shared memory\n  for (int i = 0; i < 256; i++) my_bins[i] = 0;\n  __syncthreads();\n\n  // Count pixels in block\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    atomicAdd(&my_bins[image[i]], 1);\n  }\n\n  // reduce the block bins into thread 0's bins\n  __syncthreads();\n  for (int i = 0; i < 256; i++) {\n    atomicAdd(&bins[i], my_bins[i]);\n  }\n}",
            "// TO DO: Add CUDA kernel code here\n  \n  \n}",
            "const size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N)\n    atomicAdd(&bins[image[index]], 1);\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "__shared__ size_t local_bins[256];\n    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    int num_threads = gridDim.x * blockDim.x;\n    for (int i = 0; i < 256; i++) {\n        local_bins[i] = 0;\n    }\n    for (int i = thread_id; i < N; i += num_threads) {\n        int pixel = image[i];\n        local_bins[pixel] += 1;\n    }\n    __syncthreads();\n    for (int i = 0; i < 256; i++) {\n        atomicAdd(&bins[i], local_bins[i]);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if(tid >= N)\n    return;\n  atomicAdd(&bins[image[tid]], 1);\n}",
            "__shared__ int counts[256];\n  int idx = threadIdx.x;\n  counts[idx] = 0;\n  __syncthreads();\n  while (idx < N) {\n    atomicAdd(&counts[image[idx]], 1);\n    idx += blockDim.x;\n  }\n  __syncthreads();\n  if (threadIdx.x < 256)\n    atomicAdd(&bins[threadIdx.x], counts[threadIdx.x]);\n}",
            "__shared__ int sh_image[1024];\n  int idx = threadIdx.x;\n  int bin;\n  int total = 0;\n\n  // Copy the chunk of the image data to the shared memory\n  sh_image[idx] = image[idx];\n  __syncthreads();\n\n  // Count the number of pixels with a value of idx\n  // Use 1024 threads per block\n  bin = sh_image[idx];\n  total += bin;\n  //__syncthreads();\n\n  // Copy the results to the global memory\n  atomicAdd(&(bins[bin]), total);\n}",
            "/*\n  // TODO: replace this with your code\n  size_t thread = threadIdx.x;\n  size_t idx = blockIdx.x * blockDim.x + thread;\n  if (idx < N) {\n    bins[image[idx]] += 1;\n  }\n  */\n  \n  // TODO: replace this with your code\n  __shared__ int shared[32];\n  int thread = threadIdx.x;\n  int idx = thread + blockIdx.x*blockDim.x;\n  int temp = 0;\n  if (idx < N)\n  {\n    atomicAdd(&shared[image[idx]],1);\n  }\n\n  __syncthreads();\n\n  if (thread < 32)\n  {\n    temp = 0;\n    for (int i = thread; i < 256; i+=32)\n    {\n      temp += shared[i];\n    }\n    atomicAdd(&bins[thread],temp);\n  }\n}",
            "// The first thread of every block is responsible for initializing the bins.\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n    }\n  }\n\n  // Now count the pixels.\n  // Each thread handles one pixel.\n  int value = image[blockIdx.x * blockDim.x + threadIdx.x];\n  atomicAdd(&(bins[value]), 1);\n}",
            "// TODO\n}",
            "int *globalBins = bins;\n\n   int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      int pixel = image[i];\n      atomicAdd(globalBins+pixel, 1);\n   }\n}",
            "size_t start = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t end = start + blockDim.x;\n\n  for (size_t i = start; i < end && i < N; ++i) {\n    bins[image[i]]++;\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N)\n        atomicAdd(&bins[image[tid]], 1);\n}",
            "int index = threadIdx.x + blockIdx.x*blockDim.x;\n  if(index < N){\n    atomicAdd(&bins[image[index]], 1);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "int x = threadIdx.x;\n  //__syncthreads();\n  int value = image[x];\n  atomicAdd(&bins[value], 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N)\n        atomicAdd(&(bins[image[i]]), 1);\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (index < N) {\n        atomicAdd(&bins[image[index]], 1);\n    }\n\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "__shared__ int histogram[256];\n\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // TODO\n  if (i < N) {\n    atomicAdd(&(bins[image[i]]), 1);\n  }\n}",
            "// Write your code here.\n    for (int i=blockIdx.x*blockDim.x+threadIdx.x; i<N; i+=blockDim.x*gridDim.x)\n    {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO\n}",
            "const int thread_id = blockIdx.x*blockDim.x+threadIdx.x;\n   if (thread_id < N) {\n      atomicAdd(&bins[image[thread_id]], 1);\n   }\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// Write your code here\n\n  __shared__ int sharedBins[256];\n  int idx = threadIdx.x;\n\n  // Initialize the shared bins\n  for (int i = idx; i < 256; i+=blockDim.x) {\n    sharedBins[i] = 0;\n  }\n  __syncthreads();\n\n  int pixel = image[idx];\n  atomicAdd(&sharedBins[pixel], 1);\n  __syncthreads();\n\n  for (int i = idx; i < 256; i+=blockDim.x) {\n    atomicAdd(&bins[i], sharedBins[i]);\n  }\n}",
            "}",
            "int tid = blockIdx.x*blockDim.x + threadIdx.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "// TODO: Implement\n\n}",
            "// Implement this function.\n\n}",
            "const int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n  atomicAdd(&(bins[image[i]]), 1);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Fill in your code here.\n }",
            "// TODO\n}",
            "int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   const size_t imageSize = image.size();\n   const size_t chunkSize = imageSize / size;\n   std::vector<int> image_per_rank(chunkSize);\n   if(rank == 0){\n    for(int i = 1; i < size; i++){\n      MPI_Send(&image.at(i * chunkSize), chunkSize, MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n    std::fill(bins.begin(), bins.end(), 0);\n    std::vector<std::vector<int>> bins_per_rank(size-1);\n    for(int i = 0; i < chunkSize; i++){\n      bins[image.at(i)]++;\n    }\n    for(int i = 1; i < size; i++){\n      MPI_Recv(&image_per_rank.at(0), chunkSize, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      std::vector<int> temp(chunkSize);\n      std::fill(temp.begin(), temp.end(), 0);\n      for(int j = 0; j < chunkSize; j++){\n        temp[image_per_rank.at(j)]++;\n      }\n      bins_per_rank[i - 1] = temp;\n    }\n    for(int i = 0; i < size - 1; i++){\n      for(int j = 0; j < 256; j++){\n        bins[j] += bins_per_rank[i][j];\n      }\n    }\n  } else {\n    MPI_Recv(&image_per_rank.at(0), chunkSize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    std::fill(bins.begin(), bins.end(), 0);\n    for(int i = 0; i < chunkSize; i++){\n      bins[image.at(i)]++;\n    }\n    MPI_Send(&bins.at(0), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// YOUR CODE HERE\n\n    // You may assume that the length of the input image is divisible by the\n    // number of ranks\n\n}",
            "int num_ranks, rank_id;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n   \n   // YOUR CODE HERE\n   const int kLocalSize = image.size() / num_ranks;\n   std::vector<int> local_image;\n   for (int i = 0; i < kLocalSize; ++i) {\n     local_image.push_back(image[i + rank_id * kLocalSize]);\n   }\n   const int kThreads = omp_get_max_threads();\n   const int kLocalSizePerThread = kLocalSize / kThreads;\n   const int kLastThreadLocalSize = kLocalSize - (kThreads - 1) * kLocalSizePerThread;\n   std::array<size_t, 256> local_bins;\n   local_bins.fill(0);\n   if (rank_id == 0) {\n     for (int i = 0; i < kLocalSize; ++i) {\n       ++local_bins[local_image[i]];\n     }\n   } else {\n     for (int i = 0; i < kThreads; ++i) {\n       int thread_local_size = 0;\n       int thread_local_index = i * kLocalSizePerThread;\n       if (i == kThreads - 1) {\n         thread_local_size = kLastThreadLocalSize;\n         thread_local_index = i * kLastThreadLocalSize;\n       } else {\n         thread_local_size = kLocalSizePerThread;\n         thread_local_index = i * kLocalSizePerThread;\n       }\n#pragma omp parallel for num_threads(kThreads)\n       for (int j = 0; j < thread_local_size; ++j) {\n         ++local_bins[local_image[thread_local_index + j]];\n       }\n     }\n   }\n   \n   std::array<size_t, 256> bins_tmp;\n   MPI_Reduce(local_bins.data(), bins_tmp.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank_id == 0) {\n     for (int i = 0; i < 256; ++i) {\n       bins[i] = bins_tmp[i];\n     }\n   }\n   // END YOUR CODE HERE\n}",
            "int const image_size = image.size();\n    int const chunk_size = image_size / omp_get_num_threads();\n\n    #pragma omp parallel\n    {\n        int const rank = omp_get_thread_num();\n        int const num_ranks = omp_get_num_threads();\n        int const chunk_start = rank * chunk_size;\n        int const chunk_end = (rank == num_ranks - 1)? image_size : chunk_start + chunk_size;\n\n        int local_bins[256] = {0};\n\n        #pragma omp for\n        for (int i = chunk_start; i < chunk_end; i++) {\n            local_bins[image[i]]++;\n        }\n\n        int local_bins_count[256] = {0};\n        #pragma omp for\n        for (int i = 0; i < 256; i++) {\n            local_bins_count[i] = local_bins[i];\n        }\n\n        MPI_Reduce(local_bins_count, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "//TODO\n\n  int n = image.size();\n  int n_part = n / omp_get_max_threads();\n\n  //TODO\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (n_part > 0) {\n    for (int i = 0; i < omp_get_max_threads(); i++) {\n      std::vector<int> image_part(n_part);\n      if (i == 0) {\n        for (int j = 0; j < n_part; j++) {\n          image_part[j] = image[j];\n        }\n      } else if (i == omp_get_max_threads() - 1) {\n        for (int j = 0; j < n - n_part * (omp_get_max_threads() - 1); j++) {\n          image_part[j] = image[j + n_part * (omp_get_max_threads() - 1)];\n        }\n      } else {\n        for (int j = 0; j < n_part; j++) {\n          image_part[j] = image[j + n_part * i];\n        }\n      }\n\n      std::array<size_t, 256> bins_part;\n      std::fill(bins_part.begin(), bins_part.end(), 0);\n#pragma omp parallel for\n      for (int j = 0; j < image_part.size(); j++) {\n        bins_part[image_part[j]]++;\n      }\n\n      MPI_Send(bins_part.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    std::array<size_t, 256> bins_part;\n    std::fill(bins_part.begin(), bins_part.end(), 0);\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n      bins_part[image[i]]++;\n    }\n\n    MPI_Send(bins_part.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (omp_get_max_threads() > 1) {\n    for (int i = 1; i < omp_get_max_threads(); i++) {\n      std::array<size_t, 256> bins_part;\n      MPI_Status status;\n      MPI_Recv(bins_part.data(), 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n\n      for (int j = 0; j < 256; j++) {\n        bins[j] += bins_part[j];\n      }\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "/* Your code goes here */\n\n}",
            "// Your code goes here!\n    //\n    // Make sure to use a parallel reduction on the bins array!\n    // Example:\n    // #pragma omp parallel for\n    // for (int i = 0; i < 100; i++) {\n    //   bins[i] = 0;\n    // }\n    //\n    // #pragma omp parallel for reduction(+:bins[:])\n    // for (int i = 0; i < 100; i++) {\n    //   bins[i] += i;\n    // }\n    //\n  }",
            "// TODO: YOUR CODE HERE\n \n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   size_t image_size = image.size();\n   // TODO\n   if (rank==0) {\n      #pragma omp parallel for\n      for (size_t i=0; i<image_size; i++) {\n         bins[image[i]] += 1;\n      }\n   }\n   else {\n      #pragma omp parallel for\n      for (size_t i=0; i<image_size; i++) {\n         int num = image[i];\n         MPI_Send(&num, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      }\n   }\n   if (rank!= 0) {\n      int local;\n      MPI_Status stat;\n      MPI_Probe(0, 0, MPI_COMM_WORLD, &stat);\n      MPI_Get_count(&stat, MPI_INT, &local);\n      for (int i=0; i<local; i++) {\n         MPI_Recv(&num, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &stat);\n         bins[num]++;\n      }\n   }\n}",
            "// This is your solution.\n }",
            "// Fill me in\n }",
            "int image_size = image.size();\n    int num_threads = omp_get_max_threads();\n    int num_rank = 1;\n    int rank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_rank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int block_size = image_size / num_rank;\n    int start = rank*block_size;\n    int end = start + block_size;\n    if (rank == num_rank-1) {\n        end = image_size;\n    }\n    std::vector<int> tmp(256, 0);\n\n    if (rank == 0) {\n        #pragma omp parallel num_threads(num_threads) shared(bins, image, tmp)\n        {\n            #pragma omp for\n            for (int i=0; i<image_size; ++i) {\n                bins[image[i]]++;\n            }\n\n            for (int i=1; i<num_rank; ++i) {\n                MPI_Recv(&tmp[0], 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                #pragma omp for\n                for (int j=0; j<256; ++j) {\n                    bins[j] += tmp[j];\n                }\n            }\n        }\n    } else {\n        #pragma omp parallel num_threads(num_threads) shared(tmp, image)\n        {\n            #pragma omp for\n            for (int i=start; i<end; ++i) {\n                tmp[image[i]]++;\n            }\n        }\n        MPI_Send(&tmp[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size;\n     int rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     // Count the bins on the process with rank 0\n     std::array<size_t, 256> counts = {0};\n     if (rank == 0) {\n         for (auto i : image) {\n             ++counts[i];\n         }\n     }\n\n     // Reduce the counts into bins on the process with rank 0\n     MPI_Reduce(&counts, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: Add OpenMP parallel for loop\n  // TODO: Add MPI parallel for loop\n}",
            "int nranks;\n     MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n     int myrank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n     int n = image.size();\n     int local_n = n / nranks;\n     int extra = n % nranks;\n     int local_start = myrank * local_n;\n\n     std::vector<int> local_image(local_n);\n     if (extra > myrank) {\n         local_n += 1;\n         local_start += 1;\n     }\n     local_start = std::min(local_start, n);\n     local_n = std::min(local_start + local_n, n) - local_start;\n\n     if (local_n > 0) {\n         local_image = std::vector<int>(image.begin() + local_start,\n                                        image.begin() + local_start + local_n);\n     }\n\n     // std::cout << \"Local image for rank \" << myrank << \": \";\n     // for (int i = 0; i < local_image.size(); i++) {\n     //     std::cout << local_image[i] << \" \";\n     // }\n     // std::cout << \"\\n\";\n\n     std::fill(bins.begin(), bins.end(), 0);\n     std::vector<size_t> local_bins(256);\n     if (myrank == 0) {\n         local_bins = bins;\n     }\n\n     std::fill(local_bins.begin(), local_bins.end(), 0);\n\n     #pragma omp parallel for\n     for (int i = 0; i < local_n; i++) {\n         local_bins[local_image[i]] += 1;\n     }\n\n     MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n     if (myrank == 0) {\n         std::cout << \"Pixel counts on rank 0: \";\n         for (int i = 0; i < 10; i++) {\n             std::cout << bins[i] << \" \";\n         }\n         std::cout << \"\\n\";\n     }\n }",
            "bins.fill(0);\n    size_t imageSize = image.size();\n    int imageChunk = imageSize / omp_get_num_threads();\n    int imageStart = imageChunk * omp_get_thread_num();\n    int imageEnd = imageStart + imageChunk;\n    if (omp_get_thread_num() == omp_get_num_threads() - 1) {\n        imageEnd = imageSize;\n    }\n\n    #pragma omp parallel for\n    for (int i = imageStart; i < imageEnd; i++) {\n        bins[image[i]]++;\n    }\n }",
            "int N = image.size();\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n     for (int i = 1; i < size; i++) {\n       std::vector<int> tmp(N);\n       MPI_Recv(&tmp[0], N, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       for (int j = 0; j < N; j++) {\n         bins[tmp[j]]++;\n       }\n     }\n\n   } else {\n     std::vector<int> tmp(N);\n     for (int i = 0; i < N; i++) {\n       tmp[i] = image[i];\n     }\n\n     MPI_Send(&tmp[0], N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n }",
            "for (int i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n   #pragma omp parallel\n   {\n     // your code here\n   }\n }",
            "// The number of MPI processes\n   int worldSize;\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n   // The rank of this process in MPI_COMM_WORLD\n   int worldRank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n   // The number of OpenMP threads\n   int nThreads = omp_get_num_threads();\n\n   // Number of rows per process\n   int rowsPerProcess = image.size() / worldSize;\n   int rem = image.size() % worldSize;\n   if (worldRank < rem) {\n     rowsPerProcess += 1;\n   }\n\n   // Starting pixel for this rank\n   int start = rowsPerProcess * worldRank;\n   if (worldRank < rem) {\n     start += worldRank;\n   }\n\n   // Ending pixel for this rank\n   int end = start + rowsPerProcess;\n   if (worldRank < rem) {\n     end += 1;\n   }\n   if (worldRank == worldSize - 1) {\n     end = image.size();\n   }\n\n   std::array<size_t, 256> localBins;\n   localBins.fill(0);\n\n   // Count the number of pixels in this rank's part of the image\n   for (int i = start; i < end; i++) {\n     localBins[image[i]]++;\n   }\n\n   // Merge the bins on all ranks into rank 0\n   MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: fill this in\n    // For example, one might use nested omp and mpi loops, as illustrated in the\n    // lecture slides.\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "int nranks;\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  const size_t nvalues = image.size();\n  const size_t nvalues_per_proc = nvalues / nranks;\n  const size_t nvalues_first_proc = nvalues_per_proc + nvalues % nranks;\n  const size_t nvalues_last_proc = nvalues_per_proc;\n\n  std::vector<int> values;\n  if (rank == 0) {\n    values = image;\n  } else {\n    // 1. \u8ba9\u9664\u4e86 rank 0 \u7684 rank \u4ee5 256 \u4e2a\u7a7a\u5143\u7d20\u586b\u5145\n    for (size_t i = 0; i < 256; i++) {\n      values.push_back(0);\n    }\n  }\n\n  // 2. \u628a\u6570\u636e\u5206\u53d1\u7ed9\u6bcf\u4e2a rank\n  int data[10];\n  MPI_Scatter(image.data(), 10, MPI_INT, data, 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // 3. \u7ec6\u8282\n  // 3.1 \u6bcf\u4e2a rank \u6309\u7167 10 \u4e2a\u4e00\u7ec4\uff0c\u8ba1\u7b97\u6bcf\u4e2a group \u4e2d\u6bcf\u4e2a\u5143\u7d20\u7684 count\n  int chunk_size = 10;\n  for (size_t i = 0; i < nvalues / chunk_size; i++) {\n    int local_counter[256] = {0};\n    #pragma omp parallel for\n    for (int j = 0; j < chunk_size; j++) {\n      int id = data[j];\n      local_counter[id]++;\n    }\n    // 3.2 \u628a\u8ba1\u7b97\u7ed3\u679c\u6dfb\u52a0\u5230\u5168\u5c40\u8ba1\u6570\u5668\u4e2d\n    for (int i = 0; i < 256; i++) {\n      values[i] += local_counter[i];\n    }\n  }\n\n  // 4. \u628a\u8ba1\u7b97\u7ed3\u679c\u6536\u96c6\u5230 rank 0 \u4e2d\n  if (rank!= 0) {\n    MPI_Gather(&values[0], 256, MPI_INT, NULL, 256, MPI_INT, 0, MPI_COMM_WORLD);\n  } else {\n    std::vector<int> recv_buf(256 * (nranks - 1));\n    for (int r = 1; r < nranks; r++) {\n      MPI_Recv(&recv_buf[0], 256, MPI_INT, r, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (size_t i = 0; i < 256; i++) {\n      values[i] += recv_buf[i];\n    }\n  }\n\n  if (rank == 0) {\n    bins = std::array<size_t, 256>{};\n    for (int i = 0; i < 256; i++) {\n      bins[i] = values[i];\n    }\n  }\n}",
            "// TODO: Fill in your code\n    int const rank = 0;\n    int const world_size = 0;\n    int const max_size = 100000;\n    int const max_threads = 8;\n\n    if (image.size() > max_size) {\n        std::cout << \"Image too large\" << std::endl;\n        return;\n    }\n\n    if (rank == 0) {\n        int image_size = image.size();\n        int image_length = image_size/world_size;\n        int image_offset = rank * image_length;\n\n        int local_bins[256];\n        for (int i = 0; i < 256; i++) {\n            local_bins[i] = 0;\n        }\n\n        #pragma omp parallel num_threads(max_threads)\n        {\n            int local_count[256];\n            int image_thread_length = image_length/max_threads;\n            int image_thread_offset = omp_get_thread_num() * image_thread_length;\n            image_thread_offset += image_offset;\n            int count_thread_length = 256/max_threads;\n            int count_thread_offset = omp_get_thread_num() * count_thread_length;\n            for (int i = 0; i < count_thread_length; i++) {\n                local_count[count_thread_offset + i] = 0;\n            }\n\n            #pragma omp for schedule(static) nowait\n            for (int i = 0; i < image_length; i++) {\n                int local_count_offset = (image[image_thread_offset + i] / max_threads) + count_thread_offset;\n                local_count[local_count_offset] += 1;\n            }\n\n            #pragma omp critical\n            {\n                for (int i = 0; i < count_thread_length; i++) {\n                    local_bins[count_thread_offset + i] += local_count[count_thread_offset + i];\n                }\n            }\n        }\n\n        int offset = 0;\n        for (int i = 0; i < world_size; i++) {\n            int counts[256];\n            MPI_Status status;\n            MPI_Recv(counts, 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 256; j++) {\n                bins[offset + j] = counts[j];\n            }\n            offset += 256;\n        }\n\n        for (int i = 0; i < 256; i++) {\n            bins[i] += local_bins[i];\n        }\n    } else {\n        int local_bins[256];\n        for (int i = 0; i < 256; i++) {\n            local_bins[i] = 0;\n        }\n\n        #pragma omp parallel num_threads(max_threads)\n        {\n            int local_count[256];\n            int image_thread_length = image.size()/max_threads;\n            int image_thread_offset = omp_get_thread_num() * image_thread_length;\n            int count_thread_length = 256/max_threads;\n            int count_thread_offset = omp_get_thread_num() * count_thread_length;\n            for (int i = 0; i < count_thread_length; i++) {\n                local_count[count_thread_offset + i] = 0;\n            }\n\n            #pragma omp for schedule(static) nowait\n            for (int i = 0; i < image_thread_length; i++) {\n                int local_count_offset = (image[image_thread_offset + i] / max_threads) + count_thread_offset;\n                local_count[local_count_offset] += 1;\n            }\n\n            #pragma omp critical\n            {\n                for (int i = 0; i < count_thread_length; i++) {\n                    local_bins",
            "int size;\n  int rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int mySize = image.size() / size;\n  int myStart = rank * mySize;\n  int myEnd = myStart + mySize;\n\n  std::vector<int> myImage(mySize);\n  for (int i = myStart; i < myEnd; ++i) {\n    myImage[i - myStart] = image[i];\n  }\n\n  std::vector<std::array<size_t, 256>> localBins(mySize);\n\n  // TODO: Fill the localBins vector with the number of pixels with each grayscale intensity.\n  //   Hint: You should use OpenMP here.\n#pragma omp parallel\n  {\n#pragma omp for\n    for (int i = 0; i < mySize; ++i) {\n      for (int j = 0; j < 256; ++j) {\n        localBins[i][j] = 0;\n        for (int k = 0; k < mySize; ++k) {\n          if (myImage[i] == j) {\n            localBins[i][j] += 1;\n          }\n        }\n      }\n    }\n  }\n  // TODO: Add up all of the localBins into the final result.\n  //   Hint: You should use OpenMP here.\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(localBins.data(), mySize, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n#pragma omp parallel\n      {\n#pragma omp for\n        for (int j = 0; j < mySize; ++j) {\n          for (int k = 0; k < 256; ++k) {\n            bins[k] += localBins[j][k];\n          }\n        }\n      }\n    }\n  } else {\n    MPI_Send(localBins.data(), mySize, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "int const n = image.size();\n  int const num_threads = omp_get_max_threads();\n  int const num_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n\n  // Your code goes here!\n\n\n  // Make sure the bins are zeroed out beforehand\n  for(auto i = 0; i < 256; i++) bins[i] = 0;\n}",
            "// TODO: Fix this\n     //#pragma omp parallel for\n     for (int i = 0; i < image.size(); i++)\n     {\n         #pragma omp atomic\n         bins[image[i]]++;\n     }\n }",
            "bins.fill(0);\n  int num_threads = 4;\n  int rank = 0;\n  int num_procs = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  \n  //int image_size = image.size();\n  if(rank == 0){\n    //int n_block = image_size/num_procs;\n    std::vector<int> sub_image = image;\n    for(int i = 1; i < num_procs; i++){\n      MPI_Send(image.data() + (image.size() / num_procs)*i, image.size() / num_procs, MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n    for(int i = 1; i < num_procs; i++){\n      MPI_Recv(sub_image.data(), image.size() / num_procs, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int j = 0; j < image.size() / num_procs; j++){\n        bins[sub_image[j]]++;\n      }\n    }\n  } else {\n    int n_block = image.size() / num_procs;\n    std::vector<int> sub_image = image;\n    sub_image.resize(n_block);\n    MPI_Recv(sub_image.data(), image.size() / num_procs, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for(int j = 0; j < n_block; j++){\n      bins[sub_image[j]]++;\n    }\n  }\n\n}",
            "int num_procs, my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    int num_threads, my_thread;\n    #pragma omp parallel private(my_thread)\n    {\n      my_thread = omp_get_thread_num();\n    }\n    int size = image.size();\n    int local_size = (size+num_procs-1)/num_procs;\n    int my_start = local_size*my_rank;\n    int my_end = (my_rank==num_procs-1)? size:my_start+local_size;\n    std::array<size_t, 256> local_bins;\n    for(int i=0; i<256; i++)\n      local_bins[i]=0;\n    for(int i=my_start; i<my_end; i++){\n      local_bins[image[i]]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "int numberOfProcesses, currentRank;\n     MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcesses);\n     MPI_Comm_rank(MPI_COMM_WORLD, &currentRank);\n     int numberOfBins = 256;\n     int numberOfBinsPerProcess = numberOfBins / numberOfProcesses;\n     int binBegin = currentRank * numberOfBinsPerProcess;\n     int binEnd = binBegin + numberOfBinsPerProcess;\n     int binStep = 1;\n     std::vector<int> subImage;\n     subImage.reserve(image.size());\n     for (int i = binBegin; i < binEnd; i += binStep) {\n         subImage.push_back(image[i]);\n     }\n     std::fill(bins.begin(), bins.end(), 0);\n     omp_set_num_threads(4);\n     if (currentRank == 0) {\n         for (int i = 0; i < image.size(); i++) {\n             bins[image[i]]++;\n         }\n     } else {\n         for (int i = 0; i < subImage.size(); i++) {\n             bins[subImage[i]]++;\n         }\n     }\n     int *binsArray = &bins[0];\n     MPI_Reduce(subImage.data(), binsArray, subImage.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     if (currentRank == 0) {\n         for (int i = 0; i < image.size(); i++) {\n             printf(\"bin %d: %d\\n\", i, bins[i]);\n         }\n     }\n }",
            "// TODO\n }",
            "int worldSize, worldRank;\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n  std::vector<int> local_image;\n\n  int size = image.size();\n  int chunk_size = size / worldSize;\n\n  int start = worldRank * chunk_size;\n  int end = (worldRank == worldSize - 1)? size : (worldRank + 1) * chunk_size;\n  for (int i = start; i < end; i++)\n    local_image.push_back(image[i]);\n\n  #pragma omp parallel\n  {\n    int num_threads = omp_get_num_threads();\n    int thread_rank = omp_get_thread_num();\n\n    if (worldRank == 0) {\n      #pragma omp for\n      for (int i = 0; i < num_threads; i++)\n        for (int j = 0; j < 256; j++)\n          bins[j] += 1;\n    } else {\n      #pragma omp for\n      for (int i = 0; i < 256; i++)\n        bins[i] += 1;\n    }\n  }\n\n  if (worldRank == 0)\n    MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, NULL, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  else\n    MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, NULL, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  return;\n}",
            "int myid;\n    int numprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    \n    int num_per_proc = image.size() / numprocs;\n    int remainder = image.size() % numprocs;\n    \n    std::vector<int> local_bins(256, 0);\n    std::vector<int> local_image;\n    if (myid == 0) {\n        local_image.resize(num_per_proc + remainder);\n        for (int i = 0; i < num_per_proc + remainder; i++) {\n            local_image[i] = image[i];\n        }\n    } else {\n        local_image.resize(num_per_proc);\n        for (int i = 0; i < num_per_proc; i++) {\n            local_image[i] = image[num_per_proc*myid + i];\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_image.size(); i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    if (myid == 0) {\n        for (int i = 1; i < numprocs; i++) {\n            MPI_Recv(&local_bins[0], 256, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(&local_bins[0], 256, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n    \n    for (int i = 0; i < 256; i++) {\n        bins[i] = local_bins[i];\n    }\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate chunk size\n    int chunk_size = image.size() / size;\n    int chunk_remainder = image.size() % size;\n\n    // Allocate space for local image\n    std::vector<int> local_image;\n    local_image.resize(chunk_size + chunk_remainder);\n\n    // Get local image\n    MPI_Scatter(\n      image.data(),\n      chunk_size + chunk_remainder,\n      MPI_INT,\n      local_image.data(),\n      chunk_size + chunk_remainder,\n      MPI_INT,\n      0,\n      MPI_COMM_WORLD\n    );\n\n    // Initialize bins\n    std::array<size_t, 256> local_bins;\n    local_bins.fill(0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < (chunk_size + chunk_remainder); i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    // Combine the results\n    MPI_Reduce(\n      local_bins.data(),\n      bins.data(),\n      bins.size(),\n      MPI_UNSIGNED_LONG_LONG,\n      MPI_SUM,\n      0,\n      MPI_COMM_WORLD\n    );\n\n}",
            "// Your code here.\n\n\n\n }",
            "const int rank = omp_get_thread_num();\n\tconst int size = omp_get_num_threads();\n\tconst int N = image.size();\n\tconst int chunk = N / size;\n\tconst int rem = N % size;\n\n\tstd::vector<int> chunk_image(chunk);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < size; i++) {\n\t\tint start = i * chunk;\n\t\tint end = start + chunk;\n\t\tif (i < rem) end++;\n\n\t\tfor (int j = 0; j < chunk; j++) {\n\t\t\tchunk_image[j] = image[start + j];\n\t\t}\n\n\t\tstd::array<size_t, 256> bins_local;\n\t\tstd::fill(bins_local.begin(), bins_local.end(), 0);\n\t\tfor (int k = 0; k < chunk; k++) {\n\t\t\tbins_local[chunk_image[k]]++;\n\t\t}\n\n\t\tif (rank == 0) {\n\t\t\tfor (int k = 0; k < 256; k++) {\n\t\t\t\tbins[k] += bins_local[k];\n\t\t\t}\n\t\t}\n\t}\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  auto localSize = image.size() / size;\n  auto start = rank * localSize;\n  auto end = (rank + 1) * localSize;\n  if (rank == size - 1) {\n    end = image.size();\n  }\n  std::array<size_t, 256> localBins;\n  localBins.fill(0);\n  if (rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n    }\n  }\n  MPI_Bcast(bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  std::for_each(std::execution::par, image.begin() + start, image.begin() + end, [&](int i) { localBins[i]++; });\n  MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     #pragma omp parallel\n     {\n         int nth = omp_get_num_threads();\n         int tid = omp_get_thread_num();\n     }\n\n     // TODO: Your code goes here\n\n     // TODO: add your code here\n     // TODO: use the `nth` and `tid` variables to make sure only the right number of threads are used\n     // TODO: use MPI_Reduce to combine the counters\n }",
            "// Make an array of counts to store partial results\n   constexpr int num_procs = 4;\n   std::array<size_t, 256> counts = {};\n\n   // Find the number of elements in each chunk\n   int num_elements = image.size()/num_procs;\n   int extra_elements = image.size() - num_procs*num_elements;\n\n   // Divide up the array into chunks for each MPI process\n   std::vector<int> my_image;\n   int start_idx;\n   int end_idx;\n\n   // If the first process, process the first extra_elements elements\n   if (num_procs==0) {\n     start_idx = 0;\n     end_idx = extra_elements;\n   } else {\n     // For all other processes, add the extra elements onto the number of elements\n     start_idx = extra_elements + num_procs*num_elements;\n     end_idx = start_idx + num_elements;\n   }\n   // For the first extra_elements elements, add them to the my_image\n   for (int i=0; i < extra_elements; i++) {\n     my_image.push_back(image.at(i));\n   }\n   // For all remaining elements, add them to the my_image\n   for (int i=start_idx; i < end_idx; i++) {\n     my_image.push_back(image.at(i));\n   }\n   // Find the number of elements in my_image\n   int my_image_size = my_image.size();\n\n   // Add the counts from each chunk to the counts array\n   for (int i=0; i < my_image_size; i++) {\n     counts.at(my_image.at(i))++;\n   }\n\n   // Combine the counts from all processes into one array\n   MPI_Reduce(counts.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "for(auto i=0; i < bins.size(); i++)\n        bins[i] = 0;\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int start = rank*image.size()/size;\n    int end = (rank+1)*image.size()/size;\n    int local_size = end - start;\n    std::vector<int> local_image(local_size);\n    for(int i=start; i < end; i++)\n        local_image[i] = image[i];\n\n    #pragma omp parallel for\n    for(int i=0; i < local_size; i++)\n        bins[local_image[i]]++;\n\n    if(rank == 0) {\n        for(int i=1; i < size; i++) {\n            MPI_Status status;\n            int recv_size;\n            MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n            MPI_Get_count(&status, MPI_INT, &recv_size);\n            std::vector<int> recv_image(recv_size);\n            MPI_Recv(recv_image.data(), recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            #pragma omp parallel for\n            for(int j=0; j < recv_size; j++)\n                bins[recv_image[j]]++;\n        }\n    } else {\n        MPI_Send(local_image.data(), local_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t const bins_per_rank = 256 / omp_get_num_threads();\n\tsize_t const n_local_bins = bins_per_rank * omp_get_num_threads();\n\t\n\tif(n_local_bins!= 256) {\n\t\tthrow std::runtime_error(\"not enough threads\");\n\t}\n\t\n\t// Initialize to zero\n    for (size_t i = 0; i < bins_per_rank; i++) {\n\t\tbins[i] = 0;\n\t}\n\t\n\t// Count on each thread\n    for (size_t thread = 0; thread < omp_get_num_threads(); thread++) {\n        size_t const i_start = thread * bins_per_rank;\n        size_t const i_stop  = (thread + 1) * bins_per_rank;\n        for (size_t i = i_start; i < i_stop; i++) {\n            size_t const j_start = thread * n_local_bins;\n            size_t const j_stop  = (thread + 1) * n_local_bins;\n            for (size_t j = j_start; j < j_stop; j++) {\n                bins[i] += image[j];\n            }\n        }\n    }\n\t\n\t// Reduce on rank 0\n    if(MPI_Rank == 0) {\n        for (size_t i = 0; i < bins_per_rank; i++) {\n            size_t const j_start = 1 * bins_per_rank;\n            size_t const j_stop  = 1 * bins_per_rank;\n            for (size_t j = j_start; j < j_stop; j++) {\n                bins[i] += image[j];\n            }\n        }\n    }\n\t\n\t// Broadcast from rank 0\n    if(MPI_Rank == 0) {\n        for (size_t i = 0; i < bins_per_rank; i++) {\n            size_t const j_start = 0 * bins_per_rank;\n            size_t const j_stop  = 0 * bins_per_rank;\n            for (size_t j = j_start; j < j_stop; j++) {\n                bins[i] += image[j];\n            }\n        }\n    }\n}",
            "// TODO: insert code here\n   // YOUR CODE HERE\n   int nproc;\n   int rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int local_size = image.size() / nproc;\n   int local_start = local_size * rank;\n   int local_end = local_start + local_size;\n   int i;\n\n   #pragma omp parallel for\n   for (i = local_start; i < local_end; i++) {\n      int value = image[i];\n      #pragma omp atomic\n      bins[value]++;\n   }\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      std::cout << \"Histogram\\n\";\n      for (int i = 0; i < bins.size(); i++) {\n         std::cout << i << \": \" << bins[i] << '\\n';\n      }\n      std::cout << '\\n';\n   }\n}",
            "/* Your code goes here */\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int imageSize = image.size();\n    int chunkSize = imageSize / size;\n\n    if (chunkSize == 0) {\n        if (rank == 0)\n            bins.fill(0);\n        return;\n    }\n\n    int remainder = imageSize % size;\n\n    int imageStart = rank * chunkSize;\n    int imageEnd = imageStart + chunkSize - 1;\n    if (rank == size - 1) {\n        imageEnd += remainder;\n    }\n\n    std::array<size_t, 256> localBins;\n    localBins.fill(0);\n\n    for (int i = imageStart; i <= imageEnd; i++) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int myrank;\n   int nprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n   int size_of_image = image.size();\n   std::vector<int> local_image;\n   int num_part = size_of_image / nprocs;\n   int extra = size_of_image % nprocs;\n   int start, end;\n   int i;\n   int local_count[256] = {0};\n\n   if(myrank == 0) {\n     for(i=1;i<nprocs;i++) {\n       start = num_part*i+extra*(i-1);\n       end = start+num_part+extra;\n       if(i == (nprocs-1)) {\n         end = size_of_image;\n       }\n       MPI_Send(image.data()+start, end-start, MPI_INT, i, 1, MPI_COMM_WORLD);\n     }\n   }\n\n   start = myrank * num_part;\n   end = start + num_part;\n   if(myrank == (nprocs-1)) {\n     end = size_of_image;\n   }\n   if(myrank!= 0) {\n     MPI_Recv(local_image.data(), num_part+extra, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n   local_image.assign(image.begin() + start, image.begin() + end);\n   int local_size = local_image.size();\n   #pragma omp parallel for schedule(static)\n   for(i=0;i<local_size;i++) {\n     local_count[local_image[i]]++;\n   }\n\n   if(myrank == 0) {\n     for(i=1;i<nprocs;i++) {\n       MPI_Recv(local_count, 256, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     }\n\n     int count = 0;\n     for(i=0;i<256;i++) {\n       for(int j=0;j<nprocs;j++) {\n         count += local_count[i];\n       }\n       bins[i] = count;\n     }\n   } else {\n     MPI_Send(local_count, 256, MPI_INT, 0, 1, MPI_COMM_WORLD);\n   }\n }",
            "// Your code here!\n  \n}",
            "// TODO: Implement this\n }",
            "for (size_t i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n   int nb_proc;\n   int rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nb_proc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int nb_pixel_per_proc = (int) image.size() / nb_proc;\n   std::vector<int> local_image(nb_pixel_per_proc, 0);\n   std::vector<size_t> local_bins(256, 0);\n   if (rank == 0) {\n    for (size_t i = 0; i < nb_pixel_per_proc; i++) {\n      local_image[i] = image[i];\n    }\n   }\n   else {\n    MPI_Recv(&local_image[0], nb_pixel_per_proc, MPI_INT, rank - 1, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n   for (size_t i = 0; i < local_image.size(); i++) {\n     local_bins[local_image[i]] += 1;\n   }\n   MPI_Reduce(&local_bins[0], &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank!= 0) {\n    MPI_Send(&local_bins[0], 256, MPI_UNSIGNED_LONG, rank - 1, 1, MPI_COMM_WORLD);\n   }\n }",
            "int n = image.size();\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Compute the number of rows assigned to each process.\n    // The first two lines handle the case where the number of rows\n    // is not divisible by the number of processes.\n    int n_per_proc = n / (rank + 1);\n    int n_left = n - n_per_proc * (rank + 1);\n    int my_rows = n_per_proc + (rank < n_left);\n\n    // Compute the range of rows assigned to each process.\n    int i_start = n_per_proc * rank + (rank < n_left);\n    int i_end = i_start + my_rows;\n\n    // Count the pixels in the assigned range.\n    // We count over the whole array for simplicity.\n#pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n      int gray = image[i];\n      bins[gray]++;\n    }\n }",
            "int nprocs, myrank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n   std::vector<int> local(image.size());\n   std::array<size_t, 256> counts;\n   counts.fill(0);\n   int size = image.size();\n   int perprocs = size / nprocs;\n   int begin = myrank * perprocs;\n   int end = begin + perprocs;\n   if (myrank == nprocs - 1) {\n     end = size;\n   }\n   for (int i = 0; i < end - begin; i++) {\n     local[i] = image[begin + i];\n   }\n\n#pragma omp parallel for schedule(static)\n   for (int i = 0; i < end - begin; i++) {\n     counts[local[i]]++;\n   }\n\n   int binsize = 256 / nprocs;\n   int binstart = myrank * binsize;\n   int binend = binstart + binsize;\n   if (myrank == nprocs - 1) {\n     binend = 256;\n   }\n   for (int i = binstart; i < binend; i++) {\n     MPI_Reduce(&counts[i], &bins[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n\n }",
            "int const n = image.size();\n   int const rank = omp_get_thread_num();\n   // int const nthreads = omp_get_num_threads();\n   int const nranks = omp_get_num_threads();\n\n#pragma omp parallel num_threads(nranks)\n   {\n      if (rank == 0) {\n         for (int i = 1; i < nranks; ++i)\n         {\n            MPI_Recv(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, i, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         }\n\n         std::array<int, 256> local_bins;\n         local_bins.fill(0);\n\n#pragma omp parallel for schedule(static, 1)\n         for (int i = 0; i < n; ++i) {\n            int const pixel = image[i];\n            ++local_bins[pixel];\n         }\n\n         // std::cout << \"rank \" << rank << \" local_bins \" << local_bins << std::endl;\n\n         for (int i = 1; i < nranks; ++i)\n         {\n            for (int j = 0; j < 256; ++j) {\n               bins[j] += local_bins[j];\n            }\n         }\n      }\n      else if (rank == 1) {\n         std::array<int, 256> local_bins;\n         local_bins.fill(0);\n\n#pragma omp parallel for schedule(static, 1)\n         for (int i = 0; i < n; ++i) {\n            int const pixel = image[i];\n            ++local_bins[pixel];\n         }\n\n         MPI_Send(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n      }\n      else {\n         std::array<int, 256> local_bins;\n         local_bins.fill(0);\n\n#pragma omp parallel for schedule(static, 1)\n         for (int i = 0; i < n; ++i) {\n            int const pixel = image[i];\n            ++local_bins[pixel];\n         }\n\n         MPI_Send(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n      }\n   }\n\n   // std::cout << \"rank \" << rank << \" bins \" << bins << std::endl;\n}",
            "// Your code here\n    int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<int> loc_image;\n    int loc_size = image.size() / nprocs;\n    loc_image.reserve(loc_size);\n\n    if (rank == 0) {\n        std::copy(image.begin(), image.end(), loc_image.begin());\n    } else {\n        std::copy(image.begin() + rank * loc_size, image.begin() + (rank + 1) * loc_size, loc_image.begin());\n    }\n\n    int chunk_size = loc_image.size() / nprocs;\n    int rem = loc_image.size() % nprocs;\n\n    std::array<std::array<size_t, 256>, 256> temp_bins;\n    std::fill(temp_bins.begin(), temp_bins.end(), std::array<size_t, 256>({}));\n    #pragma omp parallel num_threads(8)\n    {\n        #pragma omp for\n        for (int i = 0; i < chunk_size; i++) {\n            temp_bins[loc_image[i]][loc_image[i]]++;\n        }\n\n        if (rem > 0) {\n            for (int i = 0; i < rem; i++) {\n                temp_bins[loc_image[chunk_size * nprocs + i]][loc_image[chunk_size * nprocs + i]]++;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < nprocs; i++) {\n            for (int j = 0; j < 256; j++) {\n                bins[j] += temp_bins[i][j];\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n      // Add up values in bins\n  }\n  else {\n      // Create your own local copy of bins\n      // Count values\n      // Send to rank 0\n  }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    if (world_rank == 0) {\n        for (size_t i = 0; i < bins.size(); ++i) bins[i] = 0;\n    }\n    if (image.size() == 0) return;\n    int n = image.size();\n    int blocksize = n / world_size;\n    int remainder = n % world_size;\n    int local_n;\n    int local_start;\n    if (world_rank < remainder) {\n        local_n = blocksize + 1;\n        local_start = world_rank * local_n;\n    }\n    else {\n        local_n = blocksize;\n        local_start = remainder * (blocksize + 1) + (world_rank - remainder) * blocksize;\n    }\n    std::vector<int> local(local_n);\n    std::copy(image.begin() + local_start, image.begin() + local_start + local_n, local.begin());\n    std::array<size_t, 256> local_bins;\n    for (size_t i = 0; i < 256; ++i) local_bins[i] = 0;\n    if (local_start + local_n <= n) {\n        #pragma omp parallel for\n        for (int i = 0; i < local_n; ++i) {\n            ++local_bins[local[i]];\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  //int image_size = image.size();\n  int image_size = image.size();\n  int num_per_rank = image_size/size;\n  int num_remain = image_size%size;\n  int num_local = num_per_rank;\n  if (rank == size - 1) {\n    num_local += num_remain;\n  }\n  \n  //int *image_local = (int*) malloc(num_local*sizeof(int));\n  std::vector<int> image_local(num_local);\n  MPI_Scatter(image.data(), num_local, MPI_INT, image_local.data(), num_local, MPI_INT, 0, MPI_COMM_WORLD);\n  \n  //int *local_bins = (int*) malloc(256*sizeof(int));\n  std::array<size_t, 256> local_bins;\n  local_bins.fill(0);\n  \n  omp_set_num_threads(8);\n  #pragma omp parallel for\n  for (int i=0; i<num_local; i++) {\n    int val = image_local[i];\n    //printf(\"val %d\\n\", val);\n    local_bins[val]++;\n  }\n  \n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int mpi_rank;\n\tint mpi_size;\n\n\tint total_size = image.size();\n\tint my_size;\n\tint first, last;\n\t\n\tMPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\t\n\tif(mpi_size == 1)\n\t{\n\t\t#pragma omp parallel for\n\t\tfor(int i = 0; i < image.size(); i++)\n\t\t{\n\t\t\tbins[image[i]]++;\n\t\t}\n\t}\n\telse\n\t{\n\t\tmy_size = total_size / mpi_size;\n\t\t\n\t\tif(mpi_rank == 0)\n\t\t{\n\t\t\tfirst = 0;\n\t\t\tlast = my_size;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tfirst = mpi_rank * my_size;\n\t\t\tlast = (mpi_rank + 1) * my_size;\n\t\t}\n\t\t\n\t\tif(mpi_rank == mpi_size - 1)\n\t\t{\n\t\t\tlast = total_size;\n\t\t}\n\t\t\n\t\tfor(int i = first; i < last; i++)\n\t\t{\n\t\t\tbins[image[i]]++;\n\t\t}\n\t\t\n\t\tstd::vector<size_t> local_bins(256, 0);\n\t\tMPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, local_bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\t\t\n\t\tif(mpi_rank == 0)\n\t\t{\n\t\t\tfor(int i = 1; i < mpi_size; i++)\n\t\t\t{\n\t\t\t\tfor(int j = 0; j < 256; j++)\n\t\t\t\t{\n\t\t\t\t\tbins[j] += local_bins[j];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}",
            "}",
            "// Do this:\n     // 1. Decide how many bins you want to use\n     // 2. Divide the image into `bins` parts using MPI\n     // 3. Count the number of pixels in each bin using OpenMP\n     // 4. Combine the bins using MPI\n\n}",
            "/* YOUR CODE HERE */\n    int numProcs, myRank, numThreads;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numThreads);\n    if(myRank == 0){\n    size_t i, len = image.size();\n    #pragma omp parallel for\n    for(i = 0; i < len; i++){\n        bins[image[i]]++;\n    }\n    }\n    else {\n    size_t i, len = image.size();\n    #pragma omp parallel for\n    for(i = 0; i < len; i++){\n        MPI_Send(&image[i], 1, MPI_INT, 0, myRank, MPI_COMM_WORLD);\n    }\n    }\n    return;\n }",
            "int myrank,nproc;\n  int n,i;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  n=image.size();\n  std::vector<int> myimage;\n  if(myrank==0)\n  {\n    for(int i=0;i<n;i++)\n      bins[image[i]]++;\n    for(int i=1;i<nproc;i++)\n    {\n      MPI_Recv(&myimage,n,MPI_INT,i,0,MPI_COMM_WORLD,MPI_STATUS_IGNORE);\n      for(int j=0;j<n;j++)\n        bins[myimage[j]]++;\n    }\n  }\n  else\n  {\n    MPI_Send(&image[0],n,MPI_INT,0,0,MPI_COMM_WORLD);\n  }\n }",
            "// TODO: use OpenMP to count histogram\n     // TODO: use MPI to distribute computation across ranks\n }",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   \n   int chunksize = image.size() / size;\n   int start = rank*chunksize;\n   int end = (rank+1) * chunksize;\n   if (rank == size - 1)\n      end = image.size();\n   \n   if (rank == 0){\n      for (int i = 0; i < 256; i++){\n         bins[i] = 0;\n      }\n   }\n\n   #pragma omp parallel for schedule(static)\n   for (int i = start; i < end; i++){\n      #pragma omp atomic update\n      bins[image[i]]++;\n   }\n   \n   //Reduce the counts\n   MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0){\n      for (int i = 0; i < 256; i++){\n         std::cout << i << \" : \" << bins[i] << std::endl;\n      }\n   }\n}",
            "// YOUR CODE HERE\n   \n}",
            "bins.fill(0);\n    int numprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int *local = new int[image.size()/numprocs];\n    int *local_bins = new int[256];\n    local_bins[0] = 0;\n    local_bins[1] = 0;\n\n    //MPI_Scatter(image.data(), image.size()/numprocs, MPI_INT, local, image.size()/numprocs, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (size_t i = rank; i < image.size(); i += numprocs)\n    {\n        if (local_bins[image[i]] == 0)\n            local_bins[image[i]] = 1;\n        else\n            local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    delete[] local;\n    delete[] local_bins;\n}",
            "// TODO: Implement me\n}",
            "// Your code here\n\n }",
            "// Replace the call to pixelCountsSingleThread below with calls to pixelCountsMultiThread and pixelCountsParallel\n    // You will need to use MPI to distribute work to other ranks.\n\n    // pixelCountsSingleThread(image, bins);\n    // pixelCountsMultiThread(image, bins);\n    pixelCountsParallel(image, bins);\n\n}",
            "/* YOUR CODE HERE */\n     int numprocs, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     int proc_size = image.size()/numprocs;\n     int proc_start = proc_size*rank;\n     int proc_end = proc_start + proc_size;\n     if (proc_end > image.size())\n     {\n         proc_end = image.size();\n     }\n     for (int i = proc_start; i < proc_end; i++)\n     {\n         bins[image[i]]++;\n     }\n     if (rank == 0)\n     {\n         MPI_Status status;\n         for (int i = 1; i < numprocs; i++)\n         {\n             MPI_Recv(&bins, 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n         }\n     }\n     else\n     {\n         MPI_Send(&bins, 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n     }\n }",
            "int num_processors, rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &num_processors);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // YOUR CODE GOES HERE\n  std::array<size_t, 256> counts;\n  if(rank == 0){\n    for(int i=0; i<256; i++){\n      counts[i] = 0;\n    }\n  }\n  for(int i=0; i<image.size(); i++){\n    counts[image[i]]++;\n  }\n  std::array<size_t, 256> counts_sum;\n  for(int i=0; i<256; i++){\n    counts_sum[i] = 0;\n  }\n  MPI_Reduce(&counts[0], &counts_sum[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank == 0){\n    for(int i=0; i<256; i++){\n      bins[i] = counts_sum[i];\n    }\n  }\n \n  MPI_Barrier(MPI_COMM_WORLD);\n  \n}",
            "// TODO\n    // Set bins to zeros (i.e. bins[i] = 0 for each i in [0, 255])\n    #pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    // TODO\n    // Count the number of pixels in the image with each intensity\n    // Store the results in bins\n\n\n    // TODO\n    // Collect the partial results from the individual ranks into bins\n    // Use a MPI_Reduce operation\n    int numProc;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n\n    int myRank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    if (myRank == 0) {\n        int bins_perRank[256];\n        MPI_Reduce(bins.data(), bins_perRank, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        for (int i = 0; i < 256; i++) {\n            bins[i] = bins_perRank[i];\n        }\n    } else {\n        int bins_perRank[256];\n        for (int i = 0; i < 256; i++) {\n            bins_perRank[i] = bins[i];\n        }\n        MPI_Reduce(bins_perRank, NULL, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n\n}",
            "// Implement this function\n }",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = image.size();\n  int num_per_rank = n / size;\n  int num_left = n % size;\n\n  std::vector<int> image_rank(num_per_rank);\n  if (rank == 0) {\n    for (int i = 0; i < num_left; ++i) {\n      image_rank.push_back(image[i]);\n    }\n  }\n\n  if (rank < num_left) {\n    for (int i = num_left; i < n; i += num_left) {\n      image_rank.push_back(image[i]);\n    }\n  }\n\n  int bins_per_rank = 256 / size;\n  int bins_left = 256 % size;\n  std::vector<size_t> bins_rank(bins_per_rank);\n  if (rank == 0) {\n    for (int i = 0; i < bins_left; ++i) {\n      bins_rank.push_back(0);\n    }\n  }\n\n  if (rank < bins_left) {\n    for (int i = bins_left; i < 256; i += bins_left) {\n      bins_rank.push_back(0);\n    }\n  }\n\n#pragma omp parallel for\n  for (int i = 0; i < image_rank.size(); ++i) {\n    bins_rank[image_rank[i] / (256 / size)]++;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < bins_rank.size(); ++i) {\n      bins[i * size + rank] = bins_rank[i];\n    }\n  }\n\n  if (rank!= 0) {\n    MPI_Send(bins_rank.data(), bins_rank.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Status status;\n      MPI_Recv(bins.data() + bins_per_rank * i, bins_per_rank, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n    }\n  }\n}",
            "// TODO: Your code here\n  }",
            "}",
            "// TODO: add your code here\n\n\n  int rank, size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\n  int pixelCount = image.size();\n  int chunkSize = pixelCount/size;\n  int remainder = pixelCount % size;\n\n  std::vector<int> myImage(chunkSize,0);\n  if(rank == 0) {\n    myImage = std::vector<int>(chunkSize + remainder,0);\n  }\n\n  MPI_Scatter(&image[0], chunkSize + remainder, MPI_INT, &myImage[0], chunkSize + remainder, MPI_INT, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for(int i = 0; i < myImage.size(); i++) {\n    bins[myImage[i]]++;\n  }\n\n  MPI_Reduce(&bins[0], &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\n\n\n\n\n\n\n\n\n\n}",
            "// Your code here\n\n}",
            "int numProc;\n     int rank;\n\n     MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n     std::vector<int> local_image;\n     int len = image.size();\n     int offset = len / numProc;\n     int remainder = len % numProc;\n     int size_local_image = 0;\n     if (rank < remainder) {\n         size_local_image = offset + 1;\n     } else {\n         size_local_image = offset;\n     }\n     local_image.resize(size_local_image);\n\n     MPI_Scatter(image.data(), size_local_image, MPI_INT, local_image.data(),\n     size_local_image, MPI_INT, 0, MPI_COMM_WORLD);\n\n     std::array<size_t, 256> local_bins;\n     local_bins.fill(0);\n     #pragma omp parallel for\n     for (int i = 0; i < size_local_image; i++) {\n         local_bins[local_image[i]]++;\n     }\n\n     MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0,\n     MPI_COMM_WORLD);\n }",
            "// TODO: Replace this with a call to the function `pixelCountsHelper`.\n  //...\n  return;\n}",
            "/* YOUR CODE HERE */\n    int numRanks, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    int lowerBound = image.size() / numRanks * rank;\n    int upperBound = (rank == numRanks - 1)? image.size() : image.size() / numRanks * (rank + 1);\n\n    int numLocal = upperBound - lowerBound;\n\n    // Counts\n    std::array<size_t, 256> localBins;\n    for (int i = 0; i < 256; i++)\n        localBins[i] = 0;\n\n    #pragma omp parallel for schedule(dynamic)\n    for (int i = lowerBound; i < upperBound; i++)\n    {\n        localBins[image[i]]++;\n    }\n\n    if (rank == 0)\n    {\n        for (int i = 0; i < 256; i++)\n        {\n            bins[i] = localBins[i];\n            for (int j = 1; j < numRanks; j++)\n            {\n                MPI_Recv(&bins[i], 1, MPI_UNSIGNED_LONG, j, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n    }\n    else\n    {\n        for (int i = 0; i < 256; i++)\n            MPI_Send(&localBins[i], 1, MPI_UNSIGNED_LONG, 0, i, MPI_COMM_WORLD);\n    }\n    // printf(\"%d\\n\", lowerBound);\n}",
            "if (image.size() == 0) {\n        return;\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<int> image_rank(image.size() / size);\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(image.data() + i * (image.size() / size), image.size() / size, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(image_rank.data(), image.size() / size, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        image_rank = image;\n    }\n\n    // Parallel counts with OpenMP\n    #pragma omp parallel\n    {\n        std::array<size_t, 256> bins_local;\n        bins_local.fill(0);\n\n        for (int i = 0; i < image_rank.size(); i++) {\n            #pragma omp atomic\n            bins_local[image_rank[i]] += 1;\n        }\n\n        if (rank!= 0) {\n            MPI_Send(bins_local.data(), bins_local.size(), MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n        } else {\n            for (int i = 1; i < size; i++) {\n                MPI_Recv(bins.data(), bins.size(), MPI_SIZE_T, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                for (int j = 0; j < 256; j++) {\n                    bins[j] += bins_local[j];\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n   #pragma omp single\n   {\n    int rank = omp_get_num_threads();\n    int np   = omp_get_num_threads();\n   }\n  }\n  #pragma omp parallel for\n  for (auto j = 0; j < image.size(); j++)\n  {\n    bins[image[j]]++;\n  }\n }",
            "for (int i = 0; i < 256; i++)\n   {\n     bins[i] = 0;\n   }\n\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::array<int, 256> myBins;\n   for (int i = 0; i < 256; i++)\n   {\n     myBins[i] = 0;\n   }\n\n\n\n   #pragma omp parallel\n   {\n     int numThreads = omp_get_num_threads();\n     int threadID = omp_get_thread_num();\n     //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n\n     int i, j;\n     int chunkSize = image.size() / numThreads;\n\n     int startIndex = threadID * chunkSize;\n     int endIndex = (threadID + 1) * chunkSize;\n\n     for (int i = startIndex; i < endIndex; i++) {\n        myBins[image[i]]++;\n     }\n\n     MPI_Reduce(myBins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   }\n\n   //int numThreads = omp_get_num_threads();\n   //int threadID = omp_get_thread_num();\n   //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n   //for (int i = 0; i < 256; i++) {\n   //  printf(\"myBins[%d] = %d\\n\", i, myBins[i]);\n   //}\n   //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n   //MPI_Reduce(myBins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n   //for (int i = 0; i < 256; i++) {\n   //  printf(\"bins[%d] = %d\\n\", i, bins[i]);\n   //}\n\n\n\n\n\n\n\n\n   //if (rank == 0)\n   //{\n   //  for (int i = 0; i < 256; i++)\n   //  {\n   //    bins[i] = 0;\n   //  }\n   //}\n\n   //for (int i = 0; i < 256; i++)\n   //{\n   //  myBins[i] = 0;\n   //}\n\n   //int size, rank;\n   //MPI_Comm_size(MPI_COMM_WORLD, &size);\n   //MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   //int numThreads = omp_get_num_threads();\n   //int threadID = omp_get_thread_num();\n   //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n\n   //for (int i = 0; i < 256; i++) {\n   //  printf(\"myBins[%d] = %d\\n\", i, myBins[i]);\n   //}\n   //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n   //MPI_Reduce(myBins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   //printf(\"Hello from thread %d out of %d\\n\", threadID, numThreads);\n   //for (int i = 0; i < 256; i++) {\n   //  printf(\"bins[%d] = %d\\n\", i, bins[i]);",
            "// TODO\n }",
            "// YOUR CODE HERE\n   int rank, size;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int N = image.size();\n   int n_per_rank = N/size;\n   int n_last = n_per_rank + N%size;\n   std::vector<int> local_image(n_per_rank);\n   if(rank == 0){\n   \t  local_image = std::vector<int>(image.begin(), image.begin() + n_per_rank);\n   }\n   else if(rank < size - 1){\n   \t  local_image = std::vector<int>(image.begin() + rank*n_per_rank, image.begin() + rank*n_per_rank + n_per_rank);\n   }\n   else{\n   \t  local_image = std::vector<int>(image.begin() + (rank-1)*n_per_rank, image.begin() + (rank-1)*n_per_rank + n_last);\n   }\n\n   std::array<size_t, 256> local_bins;\n   local_bins.fill(0);\n   #pragma omp parallel for num_threads(4)\n   for (int i = 0; i < local_image.size(); i++){\n   \tlocal_bins[local_image[i]]++;\n   }\n\n   if(rank == 0){\n   \tstd::vector<size_t> global_bins(256);\n   \tMPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n   \tbins = std::array<size_t, 256>(global_bins.begin(), global_bins.end());\n   }\n   else{\n   \tMPI_Reduce(local_bins.data(), NULL, 256, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO: YOUR CODE HERE\n\n }",
            "// TODO\n}",
            "// TODO: YOUR CODE HERE\n  int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> my_image;\n  int index, i;\n  int n = image.size();\n  int m = n / size;\n  int remainder = n % size;\n\n  if (rank == 0) {\n    my_image = image;\n  }\n  else {\n    for (index = 0; index < m + remainder; index++) {\n      if (index < remainder) {\n        my_image.push_back(image[rank * m + index]);\n      }\n      else {\n        my_image.push_back(image[(rank - 1) * m + index - remainder]);\n      }\n    }\n  }\n\n  std::array<size_t, 256> local_bins;\n  for (i = 0; i < 256; i++) {\n    local_bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (i = 0; i < my_image.size(); i++) {\n    local_bins[my_image[i]] += 1;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n   if (image.size() < 2)\n    return;\n\n    int count;\n    size_t n_threads;\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int subarray_length = image.size() / size;\n    int remainder = image.size() % size;\n\n    int lower = rank * subarray_length;\n    int upper = lower + subarray_length;\n\n    // Rank 0 also processes the extra values that don't fit in the division\n    if (rank == 0) {\n        lower = 0;\n        upper += remainder;\n    }\n    else {\n        lower += remainder * (rank - 1);\n    }\n\n    std::vector<int> local_image(image.begin() + lower, image.begin() + upper);\n    //std::cout << \"Rank \" << rank << \" has \" << local_image.size() << \" elements\" << std::endl;\n    bins.fill(0);\n\n    // Count the number of pixels with each grayscale intensity in this image.\n    // Store the results in bins.\n    // You can use OpenMP to process this array in parallel.\n    // Example:\n    //\n    // local_image = [2, 116, 201, 11, 92, 92, 201, 4, 2]\n    // bins = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    //\n    //  for i in range(len(local_image)):\n    //    bins[local_image[i]] += 1\n    //\n    // bins = [0, 0, 2, 0, 1,...]\n\n    #pragma omp parallel\n    {\n      #pragma omp for\n      for (int i = 0; i < local_image.size(); i++)\n        bins.at(local_image.at(i)) += 1;\n    }\n\n    // Reduce the counts across all ranks using MPI.\n    // Assume the vector bins has the counts for this rank only.\n    // Combine the results from all ranks to form the final histogram.\n    // Use the `reduce` function provided in utils.hpp.\n    // Example:\n    //\n    // bins = [0, 0, 2, 0, 1,...]\n    // count = 3\n    // MPI_Reduce(&count, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    // bins = [0, 0, 5, 0, 4,...]\n    //\n    // Note that bins only has the correct values on rank 0\n\n    if (rank == 0)\n      bins.fill(0);\n\n    MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int world_size = omp_get_num_threads();\n    const int world_rank = omp_get_thread_num();\n    std::vector<size_t> local_bins(256, 0);\n    // your code here\n#pragma omp parallel for\n    for (int i=world_rank; i < image.size(); i += world_size) {\n        local_bins[image[i]] += 1;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "for (int i=0;i<image.size();i++)\n        bins[image[i]]++;\n   \n}",
            "// Get the total number of ranks and the rank id\n  int num_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute the size of a chunk for each rank\n  auto chunk_size = image.size() / num_ranks;\n  auto last_chunk_size = image.size() - chunk_size * (num_ranks - 1);\n\n  // Every rank will use the MPI type MPI_LONG to communicate the size of the chunk\n  MPI_Datatype MPI_LONG = sizeof(long) == 8? MPI_LONG : MPI_LONG_LONG;\n\n  // Send the size of the chunk to other ranks\n  long chunk_size_to_send;\n  if (rank == 0) {\n    // If the rank is the root, the chunk size is determined by the last rank\n    chunk_size_to_send = last_chunk_size;\n  } else {\n    // If the rank is not the root, the chunk size is determined by the chunk_size variable\n    chunk_size_to_send = chunk_size;\n  }\n\n  MPI_Bcast(&chunk_size_to_send, 1, MPI_LONG, 0, MPI_COMM_WORLD);\n\n  // Set the number of threads to use\n  #pragma omp parallel\n  {\n    // Set the number of threads to use\n    omp_set_num_threads(omp_get_num_threads());\n  }\n\n  // Determine the chunk size for the current rank\n  long chunk_size_to_compute;\n  long offset;\n  if (rank == 0) {\n    // If the rank is the root, it will compute all chunks except the last one\n    chunk_size_to_compute = chunk_size;\n    offset = 0;\n  } else {\n    // If the rank is not the root, it will compute the last chunk only\n    chunk_size_to_compute = last_chunk_size;\n    offset = chunk_size * (rank - 1);\n  }\n\n  // Allocate a buffer to store the pixel counts from each rank\n  std::array<size_t, 256> bins_per_rank;\n  bins_per_rank.fill(0);\n\n  // Compute the pixel counts for the current rank\n  for (long i = 0; i < chunk_size_to_compute; i++) {\n    #pragma omp parallel for\n    for (long j = 0; j < 256; j++) {\n      bins_per_rank[j] += (image[i + offset] == j);\n    }\n  }\n\n  // Gather all the pixel counts to the rank 0\n  std::array<size_t, 256> bins_to_send;\n  bins_to_send.fill(0);\n\n  MPI_Gather(bins_per_rank.data(), 256, MPI_LONG, bins_to_send.data(), 256, MPI_LONG, 0, MPI_COMM_WORLD);\n\n  // If the rank is the root, it will collect all the pixel counts and save the results in the `bins` variable\n  if (rank == 0) {\n    bins = bins_to_send;\n  }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  const int threads = omp_get_max_threads();\n\n  std::vector<size_t> count(256, 0);\n  int n = image.size();\n  int n_per_thread = n / threads;\n  int i_start = rank * n_per_thread;\n  int i_end = (rank + 1) * n_per_thread;\n\n  if (rank == 0) {\n    for (int i = 0; i < threads; i++) {\n      MPI_Recv(&count, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 256; j++) {\n        bins[j] += count[j];\n      }\n    }\n  }\n  else {\n    for (int i = i_start; i < i_end; i++) {\n      count[image[i]]++;\n    }\n    MPI_Send(&count, 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int np = size; // number of processes\n    int p; // process id\n    int nl; // number of local lines per process\n    int nl_prev; // number of local lines per process\n    int g; // global pixel count\n\n    if (rank == 0) {\n        // count the number of pixels globally\n        g = std::accumulate(image.begin(), image.end(), 0);\n    }\n\n    // calculate number of lines per process\n    MPI_Bcast(&g, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    nl = std::floor(g / np);\n    nl_prev = std::floor((g - 1) / np);\n    p = g - (nl_prev * np);\n\n    if (p == 0 && rank!= 0) {\n        nl++;\n    }\n    else if (p > 0 && rank == np - 1) {\n        nl++;\n    }\n\n    // count the number of pixels locally\n    std::array<size_t, 256> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    int local_pixels = nl * image.size();\n    int local_pixels_prev = nl_prev * image.size();\n\n    int i = 0;\n    int j = 0;\n\n    for (i = 0; i < nl; i++) {\n        for (j = 0; j < image.size(); j++) {\n            local_bins[image[j]]++;\n        }\n    }\n\n    // reduce\n    if (rank == 0) {\n        for (i = 1; i < size; i++) {\n            std::vector<int> temp_image;\n            MPI_Recv(temp_image.data(), image.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            std::array<size_t, 256> temp_bins;\n            MPI_Recv(temp_bins.data(), 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for (j = 0; j < 256; j++) {\n                bins[j] += temp_bins[j];\n            }\n        }\n    }\n    else {\n        MPI_Send(image.data(), image.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(local_bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// YOUR CODE HERE\n    int nb_threads;\n    int nb_ranks;\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    nb_ranks = size;\n\n    std::array<size_t, 256> bins_rank;\n    bins_rank.fill(0);\n\n    if (rank == 0) {\n        nb_threads = 1;\n    } else {\n        nb_threads = omp_get_max_threads();\n    }\n\n    int start, end;\n    start = rank * nb_threads;\n    end = (rank + 1) * nb_threads;\n\n    if (start < image.size()) {\n        std::vector<int> chunk;\n        chunk.resize(end - start);\n        std::copy(image.begin() + start, image.begin() + end, chunk.begin());\n\n        #pragma omp parallel\n        {\n            std::array<size_t, 256> bins_chunk;\n            bins_chunk.fill(0);\n\n            #pragma omp for schedule(static)\n            for (int i = 0; i < chunk.size(); i++) {\n                bins_chunk[chunk[i]]++;\n            }\n\n            #pragma omp critical\n            {\n                for (int j = 0; j < 256; j++) {\n                    bins_rank[j] += bins_chunk[j];\n                }\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = bins_rank[i];\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static)\n    for(int i = 0; i < image.size(); ++i)\n        ++bins[image[i]];\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement this function\n}",
            "}",
            "// TODO\n }",
            "// Your code goes here.\n  const int size = image.size();\n\n  // #pragma omp parallel\n  // {\n  //   #pragma omp for\n  //   for (int i = 0; i < size; i++) {\n  //     // ++bins[image[i]];\n  //     #pragma omp atomic\n  //     bins[image[i]]++;\n  //   }\n  // }\n\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    #pragma omp atomic\n    bins[image[i]]++;\n  }\n }",
            "}",
            "int numprocs, my_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   std::array<int, 256> local;\n   local.fill(0);\n\n   if(my_rank == 0) {\n     for(int i=0; i < image.size(); i++)\n       local[image[i]]++;\n\n     for(int i=1; i < numprocs; i++) {\n       std::array<int, 256> temp;\n       MPI_Recv(temp.data(), 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       for(int j = 0; j < 256; j++) {\n         local[j] += temp[j];\n       }\n     }\n\n     bins = local;\n   } else {\n     MPI_Send(local.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO: Add your code here\n   int rank;\n   int size;\n   int source = 0;\n   MPI_Status status;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int block_size = image.size()/size;\n   int block_size_left = image.size() - block_size * size;\n   //printf(\"image_size %d, block_size %d, block_size_left %d\\n\", image.size(), block_size, block_size_left);\n   if (rank == 0) {\n     //printf(\"Rank 0: block_size %d\\n\", block_size);\n     MPI_Send(&block_size_left, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n     MPI_Recv(&block_size, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &status);\n   }\n   else if (rank == 1) {\n     //printf(\"Rank 1: block_size %d\\n\", block_size);\n     MPI_Recv(&block_size_left, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n     MPI_Send(&block_size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n   #pragma omp parallel\n   {\n     //printf(\"thread_count %d\\n\", omp_get_num_threads());\n     std::array<size_t, 256> local_bins = {};\n     int i;\n     int start = block_size * rank;\n     int end = block_size * rank + block_size;\n     if (rank == 0) {\n       start -= block_size_left;\n       end -= block_size_left;\n     }\n     else if (rank == size-1) {\n       end += block_size_left;\n     }\n     //printf(\"Rank %d, start %d, end %d\\n\", rank, start, end);\n     for (i = start; i < end; i++) {\n       //printf(\"Rank %d, index %d, value %d\\n\", rank, i, image[i]);\n       local_bins[image[i]]++;\n     }\n     MPI_Reduce(&local_bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n }",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    int i;\n    int j;\n    int k;\n    int l;\n    int m;\n    int n;\n    int p;\n    int q;\n    int r;\n    int s;\n    int t;\n    int u;\n    int v;\n    int w;\n    int x;\n    int y;\n    int z;\n    int a;\n    int b;\n    int c;\n    int d;\n    int e;\n    int f;\n    int g;\n    int h;\n    int image_size;\n    int image_size1;\n    int rank_size;\n    int rank_size1;\n    int a1;\n    int b1;\n    int c1;\n    int d1;\n    int e1;\n    int f1;\n    int g1;\n    int h1;\n    int rank_size2;\n    int rank_size3;\n    int rank_size4;\n    int rank_size5;\n    int rank_size6;\n    int rank_size7;\n    int rank_size8;\n    int rank_size9;\n    int rank_size10;\n    int rank_size11;\n    int rank_size12;\n    int rank_size13;\n    int rank_size14;\n    int rank_size15;\n    int rank_size16;\n    int rank_size17;\n    int rank_size18;\n    int rank_size19;\n    int rank_size20;\n    int rank_size21;\n    int rank_size22;\n    int rank_size23;\n    int rank_size24;\n    int rank_size25;\n    int rank_size26;\n    int rank_size27;\n    int rank_size28;\n    int rank_size29;\n    int rank_size30;\n    int rank_size31;\n    int rank_size32;\n    int rank_size33;\n    int rank_size34;\n    int rank_size35;\n    int rank_size36;\n    int rank_size37;\n    int rank_size38;\n    int rank_size39;\n    int rank_size40;\n    int rank_size41;\n    int rank_size42;\n    int rank_size43;\n    int rank_size44;\n    int rank_size45;\n    int rank_size46;\n    int rank_size47;\n    int rank_size48;\n    int rank_size49;\n    int rank_size50;\n    int rank_size51;\n    int rank_size52;\n    int rank_size53;\n    int rank_size54;\n    int rank_size55;\n    int rank_size56;\n    int rank_size57;\n    int rank_size58;\n    int rank_size59;\n    int rank_size60;\n    int rank_size61;\n    int rank_size62;\n    int rank_size63;\n    int rank_size64;\n    int rank_size65;\n    int rank_size66;\n    int rank_size67;\n    int rank_size68;\n    int rank_size69;\n    int rank_size70;\n    int rank_size71;\n    int rank_size72;\n    int rank_size73;\n    int rank_size74;\n    int rank_size75;\n    int rank_size76;\n    int rank_size77;\n    int rank_size78;\n    int rank_size79;\n    int rank_size80;\n    int rank_size81;\n    int rank_size82;\n    int rank_size83;\n    int rank_size84;\n    int rank_size85;\n    int rank_size86;\n    int",
            "// Use OpenMP to parallelize over the number of grayscale intensity levels\n    #pragma omp parallel for\n    for (int i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n    \n    // Use MPI to partition the array of pixel intensities between ranks\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n    int chunkSize = image.size() / numprocs;\n    int remainder = image.size() % numprocs;\n\n    int start = myrank * chunkSize;\n    int end = myrank * chunkSize + chunkSize - 1;\n    if (myrank == 0) {\n        start = 0;\n    }\n    if (myrank == numprocs - 1) {\n        end = image.size() - 1;\n    }\n\n    for (int i = start; i <= end; ++i) {\n        bins[image[i]] += 1;\n    }\n\n    // Use MPI to combine the results in bins\n    std::vector<size_t> sendBuffer(256);\n    for (int i = 0; i < 256; ++i) {\n        sendBuffer[i] = bins[i];\n    }\n\n    std::vector<size_t> recvBuffer(256);\n\n    int tag = 0;\n    MPI_Status status;\n    MPI_Sendrecv(&sendBuffer[0], 256, MPI_SIZE_T, 0, tag,\n                 &recvBuffer[0], 256, MPI_SIZE_T, 0, tag, MPI_COMM_WORLD, &status);\n\n    if (myrank == 0) {\n        for (int i = 0; i < 256; ++i) {\n            bins[i] = recvBuffer[i];\n        }\n        for (int i = 1; i < numprocs; ++i) {\n            MPI_Status status;\n            MPI_Recv(&recvBuffer[0], 256, MPI_SIZE_T, i, tag, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 256; ++j) {\n                bins[j] += recvBuffer[j];\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n  int size, rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int img_size = image.size();\n  int part = img_size/size;\n  int remain = img_size%size;\n  int start = rank * part;\n  int end = start + part;\n  if (rank == 0) {\n    end += remain;\n  } else if (rank < remain) {\n    start += rank;\n    end += rank + 1;\n  } else if (rank == remain) {\n    start += remain;\n    end += remain;\n  } else {\n    end += remain;\n  }\n  #pragma omp parallel for num_threads(8)\n  for (int i = start; i < end; i++) {\n    #pragma omp atomic\n    bins[image[i]]++;\n  }\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(bins.data()+256*i, 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(bins.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size;\n\tint rank;\n\tint num_threads;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tomp_set_num_threads(4);\n\t#pragma omp parallel\n\t{\n\t\tnum_threads = omp_get_num_threads();\n\t}\n\tstd::array<int, 256> local_bins;\n\tint local_size = image.size() / size;\n\tint local_rank = rank * local_size;\n\tint local_image_size = local_rank + local_size;\n\n\t#pragma omp parallel for\n\tfor (int i = local_rank; i < local_image_size; i++) {\n\t\tint color = image.at(i);\n\t\tlocal_bins.at(color)++;\n\t}\n\n\tMPI_Reduce(&local_bins, &bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* YOUR CODE HERE */\n    int num_ranks;\n    int rank;\n    int size;\n    int chunk;\n    int start;\n    int end;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    chunk = image.size() / size;\n    start = chunk * rank;\n    end = chunk * (rank + 1);\n\n    // count in parallel\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "int size, rank, num_threads, sum = 0;\n  int* recv;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Status status;\n\n  int start_index = image.size() / size * rank;\n  int end_index = image.size() / size * (rank + 1);\n\n  std::vector<int> my_bins(256);\n  if (rank == 0) {\n    recv = new int[256 * size];\n  }\n\n  int sum = 0;\n  for (int i = start_index; i < end_index; i++) {\n    my_bins[image[i]]++;\n  }\n\n  MPI_Gather(&my_bins[0], 256, MPI_INT, recv, 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      for (int j = 0; j < size; j++) {\n        bins[i] += recv[j * 256 + i];\n      }\n    }\n    delete[] recv;\n  }\n}",
            "// Your code here!\n     int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\n\n    int num_of_pixels = image.size();\n    int per_rank = num_of_pixels / size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&image[i * per_rank], per_rank, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    int local_counts[256];\n\n    if (rank!= 0) {\n        MPI_Status status;\n        MPI_Recv(&image[rank * per_rank], per_rank, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    for (int i = 0; i < 256; i++) {\n        local_counts[i] = 0;\n    }\n\n    int num_of_threads = omp_get_max_threads();\n    int per_thread = num_of_pixels / num_of_threads;\n\n    for (int i = 0; i < num_of_threads; i++) {\n        #pragma omp parallel for num_threads(num_of_threads)\n        for (int j = 0; j < per_thread; j++) {\n            local_counts[image[i * per_thread + j]] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&image[i * per_rank], per_rank, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        for (int i = 0; i < 256; i++) {\n            bins[i] = 0;\n        }\n        for (int i = 0; i < num_of_pixels; i++) {\n            bins[image[i]] += 1;\n        }\n    }\n}",
            "int const rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int const size = MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<int> locBin;\n  locBin.assign(bins.size(), 0);\n\n  if (rank == 0) {\n    omp_set_num_threads(size);\n    #pragma omp parallel\n    {\n      int my_id = omp_get_thread_num();\n      size_t my_start = my_id * image.size() / size;\n      size_t my_end = (my_id + 1) * image.size() / size;\n\n      for (size_t i = my_start; i < my_end; i++) {\n        locBin[image[i]]++;\n      }\n\n      #pragma omp critical\n      {\n        for (int i = 0; i < 256; i++) {\n          bins[i] += locBin[i];\n        }\n      }\n    }\n  }\n  else {\n    omp_set_num_threads(size);\n    #pragma omp parallel\n    {\n      int my_id = omp_get_thread_num();\n      size_t my_start = my_id * image.size() / size;\n      size_t my_end = (my_id + 1) * image.size() / size;\n\n      for (size_t i = my_start; i < my_end; i++) {\n        locBin[image[i]]++;\n      }\n    }\n    MPI_Gather(locBin.data(), locBin.size(), MPI_INT, bins.data(), locBin.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n   \t\t#pragma omp atomic\n   \t\tbins[image[i]] += 1;\n   }\n }",
            "}",
            "// You will need to modify the code in this function.\n\t#pragma omp parallel for\n\tfor (int i = 0; i < 256; ++i) {\n\t\t#pragma omp atomic\n\t\tbins[i] += std::count(image.begin(), image.end(), i);\n\t}\n}",
            "// ******* your code here *******\n\n\n  }",
            "// YOUR CODE HERE\n }",
            "// TODO: replace this line with your code\n\n\n }",
            "// YOUR CODE GOES HERE\n\n   // Use MPI and OpenMP to do the counting\n   // You should parallelize over both the MPI ranks and the OpenMP threads\n\n   // You may assume that image.size() % (MPI_SIZE * NUM_THREADS) == 0\n\n   // HINT: You may want to use a parallel reduction algorithm for this\n   // HINT: You may want to use std::for_each\n   // HINT: You may want to use a lambda expression\n}",
            "// TODO\n   bins = std::array<size_t, 256>();\n}",
            "int p;\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int npixels = image.size();\n  int nlocal = npixels/p;\n  std::vector<int> local(nlocal, 0);\n  std::vector<int> local_send(nlocal, 0);\n\n  for (int i = 0; i < nlocal; i++) {\n    local[i] = image[rank*nlocal + i];\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < nlocal; i++) {\n    local_send[i] = local[i];\n  }\n  MPI_Reduce(local_send.data(), local.data(), nlocal, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      bins[i] = local[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 256; i++) {\n    int tmp = 0;\n    for (auto j : image) {\n      if (i == j) {\n        tmp++;\n      }\n    }\n    bins[i] = tmp;\n  }\n}",
            "int N = image.size();\n\n  // Set number of MPI processes.\n  int world_size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Set number of OpenMP threads.\n  int nthreads = omp_get_max_threads();\n\n  // Set how many MPI processes to use for every OpenMP thread.\n  int mpi_per_thread = world_size / nthreads;\n\n  // Define the first and last index of the image.\n  int first_index = mpi_per_thread * (rank % nthreads);\n  int last_index = (rank == (nthreads - 1)? N - 1 : mpi_per_thread * (rank % nthreads) + mpi_per_thread - 1);\n  int size = last_index - first_index + 1;\n\n  // Declare an empty vector to store the counts.\n  std::vector<int> local_bins(256, 0);\n\n  #pragma omp parallel\n  {\n    int local_rank = omp_get_thread_num();\n\n    #pragma omp for schedule(static)\n    for (int i = 0; i < size; i++) {\n      int index = i + first_index;\n      int value = image[index];\n      local_bins[value] += 1;\n    }\n\n    // Reduce the local_bins on rank 0.\n    if (rank == 0) {\n      MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n      // Reduce the local_bins on rank > 0.\n      MPI_Reduce(local_bins.data(), local_bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n  }\n}",
            "// Implement me!\n\n}",
            "/* YOUR CODE HERE */\n }",
            "//...\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int local_size = image.size();\n   int local_rank = omp_get_thread_num();\n   int num_threads = omp_get_num_threads();\n\n   for (int i = 0; i < local_size; i++) {\n     bins[image[i]]++;\n   }\n\n   int* bins_send = new int[256]();\n   for (int i = 0; i < 256; i++) {\n     bins_send[i] = bins[i];\n   }\n\n   if (local_rank == 0) {\n     for (int i = 1; i < num_threads; i++) {\n       MPI_Recv(bins_send, 256, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       for (int j = 0; j < 256; j++) {\n         bins[j] += bins_send[j];\n       }\n     }\n   } else {\n     MPI_Send(bins_send, 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n   delete[] bins_send;\n }",
            "// TODO: Your code here\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        int count = image.size() / size;\n        int remainder = image.size() % size;\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&image[count * i], count, MPI_INT, i, i, MPI_COMM_WORLD);\n            if (remainder!= 0) {\n                MPI_Send(&image[count * i + count], 1, MPI_INT, i, i, MPI_COMM_WORLD);\n                remainder--;\n            }\n        }\n    } else {\n        int count;\n        MPI_Recv(&count, 1, MPI_INT, 0, rank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        std::vector<int> buffer(count);\n        MPI_Recv(&buffer[0], count, MPI_INT, 0, rank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        std::vector<int> localBins;\n        localBins.resize(256, 0);\n        #pragma omp parallel for\n        for (int i = 0; i < count; i++) {\n            localBins[buffer[i]] += 1;\n        }\n        MPI_Send(&localBins[0], 256, MPI_INT, 0, rank, MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        int count = image.size() / size;\n        int remainder = image.size() % size;\n        for (int i = 1; i < size; i++) {\n            std::vector<int> buffer(256, 0);\n            MPI_Recv(&buffer[0], 256, MPI_INT, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += buffer[j];\n            }\n            if (remainder!= 0) {\n                MPI_Recv(&bins[j], 1, MPI_INT, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                remainder--;\n            }\n        }\n    }\n}",
            "int num_procs, my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    std::array<size_t, 256> bins_local;\n    int img_size = image.size();\n    int img_size_per_proc = img_size/num_procs;\n    int rem = img_size%num_procs;\n    int num_of_pixel = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < img_size; i++){\n        bins_local[image[i]] += 1;\n        num_of_pixel += 1;\n    }\n    MPI_Reduce(&bins_local, &bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\n}",
            "// TODO: replace code below with your code\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    bins.fill(0);\n  }\n  std::array<size_t, 256> threadBins;\n  threadBins.fill(0);\n  int threadNum = omp_get_max_threads();\n  int chunkSize = image.size() / threadNum;\n  int start = rank * chunkSize;\n  int end = (rank + 1) * chunkSize;\n  if (rank == size - 1) {\n    end = image.size();\n  }\n  for (int i = start; i < end; i++) {\n    threadBins[image[i]] += 1;\n  }\n  MPI_Reduce(threadBins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n   const size_t image_size = image.size();\n   int num_processes, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (image_size == 0) return;\n   int elements_per_thread = image_size / num_processes;\n   int leftover = image_size % num_processes;\n   int first = rank * elements_per_thread;\n   int last = first + elements_per_thread;\n   if (rank == num_processes - 1) {\n    last += leftover;\n   }\n   int num_threads = 4;\n   std::vector<int> *my_bins = new std::vector<int> (256);\n   std::fill(my_bins->begin(), my_bins->end(), 0);\n   int thread_counts[num_threads];\n   int image_thread_start = first;\n   int image_thread_end = first + elements_per_thread;\n   if (rank == num_processes - 1) {\n    image_thread_end = last;\n   }\n   for (int i = 0; i < num_threads; i++) {\n    int *thread_counts_ptr = &thread_counts[i];\n    *thread_counts_ptr = 0;\n    if (image_thread_start >= image_thread_end) {\n      break;\n    }\n    omp_set_num_threads(num_threads);\n    #pragma omp parallel default(none) shared(my_bins, image_thread_start, image_thread_end)\n    {\n      int thread_id = omp_get_thread_num();\n      int image_thread_start_thread = image_thread_start + thread_id * (elements_per_thread / num_threads);\n      int image_thread_end_thread = image_thread_start_thread + (elements_per_thread / num_threads);\n      if (thread_id == num_threads - 1) {\n        image_thread_end_thread = image_thread_end;\n      }\n      for (int k = image_thread_start_thread; k < image_thread_end_thread; k++) {\n        (*my_bins)[image[k]] += 1;\n        #pragma omp atomic\n        (*thread_counts_ptr)++;\n      }\n    }\n   }\n   std::vector<int> *thread_results = new std::vector<int> (num_threads);\n   for (int i = 0; i < num_threads; i++) {\n    thread_results->at(i) = thread_counts[i];\n   }\n   int *num_elements_ptr = &(thread_counts[num_threads - 1]);\n   *num_elements_ptr = 0;\n   for (int i = 0; i < num_threads; i++) {\n    *num_elements_ptr += thread_results->at(i);\n   }\n   if (rank == 0) {\n    for (int i = 0; i < num_threads; i++) {\n      for (int j = 0; j < 256; j++) {\n        bins[j] += (*my_bins)[j];\n      }\n    }\n   }\n   delete my_bins;\n   delete thread_results;\n}",
            "// TODO: your code here\n   for (int i = 0; i < image.size(); i++) {\n    int x = image[i];\n    bins[x]++;\n   }\n   \n }",
            "// TODO: Use MPI to distribute the image to all processors.\n\n    // TODO: Use OpenMP to count the number of pixels with each intensity.\n\n    // TODO: Gather all the results from all ranks in bins on rank 0.\n\n}",
            "int myid;\n   int numprocs;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n   const int chunksize = image.size() / numprocs;\n   std::vector<int> mybins;\n   mybins.resize(256, 0);\n\n   #pragma omp parallel\n   {\n    for (int i = 0; i < chunksize; i++) {\n      #pragma omp atomic\n      mybins[image[i]] += 1;\n    }\n   }\n\n   int root = 0;\n   MPI_Gather(&mybins, 256, MPI_INT, bins.data(), 256, MPI_INT, root, MPI_COMM_WORLD);\n }",
            "}",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk_size = image.size() / size;\n  int start = rank * chunk_size;\n  int end = start + chunk_size;\n  if (rank == size - 1) {\n    end = image.size();\n  }\n  std::fill(bins.begin(), bins.end(), 0);\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n  for (int i = start; i < end; i++) {\n    bins[image[i]]++;\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n  int size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_threads = omp_get_num_threads();\n  int num_values_per_thread = image.size() / num_threads;\n\n  int start, end;\n  if (rank == 0) {\n    start = 0;\n    end = num_values_per_thread;\n  } else {\n    start = num_values_per_thread * rank;\n    end = num_values_per_thread * (rank + 1);\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 256; i++) {\n      bins[i] = 0;\n    }\n  }\n\n  #pragma omp parallel for\n  for (int i = start; i < end; i++) {\n    bins[image[i]]++;\n  }\n\n  for (int r = 1; r < size; r++) {\n    int start, end;\n    start = num_values_per_thread * r;\n    end = num_values_per_thread * (r + 1);\n\n    std::array<size_t, 256> partial_bins;\n    for (int i = 0; i < 256; i++) {\n      partial_bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n      partial_bins[image[i]]++;\n    }\n\n    MPI_Send(&partial_bins, 256, MPI_UNSIGNED_LONG, r, r, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int r = 1; r < size; r++) {\n      MPI_Status status;\n      std::array<size_t, 256> partial_bins;\n      MPI_Recv(&partial_bins, 256, MPI_UNSIGNED_LONG, r, r, MPI_COMM_WORLD, &status);\n      for (int i = 0; i < 256; i++) {\n        bins[i] += partial_bins[i];\n      }\n    }\n  }\n}",
            "// Create a vector of rank-local histogram.\n     // Rank 0 will collect results from other ranks.\n     std::array<size_t, 256> local_histogram;\n     local_histogram.fill(0);\n\n     int comm_size;\n     int comm_rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n     // Divide the image into equally sized chunks\n     // and process each chunk in parallel on each core of each rank.\n     // Use OpenMP to run in parallel on each core.\n     // Note:\n     // The number of threads (for OpenMP) should be equal to the number\n     // of cores per rank.\n     #pragma omp parallel for\n     for (size_t i = 0; i < image.size(); i++) {\n         local_histogram[image[i]]++;\n     }\n\n     // Now we need to sum up histogram from all ranks and store it in bins.\n     // We can use MPI_Reduce to achieve this goal.\n     // We will use the function MPI_Allreduce in the future.\n     MPI_Reduce(local_histogram.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n     // If you wish to have the results in bins on all ranks, use MPI_Allreduce.\n     // Note: This step is not needed in this homework.\n     //MPI_Allreduce(local_histogram.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// Your code here!\n     MPI_Status status;\n     int count;\n     MPI_Comm_size(MPI_COMM_WORLD, &count);\n     int rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     std::vector<int> image_part;\n     int count_part;\n     if (rank == 0) {\n         count_part = image.size() / count;\n         for (int i = 1; i < count; i++) {\n             MPI_Send(&image.at(i * count_part), count_part, MPI_INT, i, 1, MPI_COMM_WORLD);\n         }\n     }\n     else {\n         MPI_Recv(&image_part, count_part, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n     }\n     if (rank == 0) {\n         std::vector<int> image_full(image);\n         image_full.insert(image_full.end(), image_part.begin(), image_part.end());\n         for (int i = 0; i < image_full.size(); i++) {\n             bins[image_full[i]] += 1;\n         }\n     }\n     else {\n         for (int i = 0; i < image_part.size(); i++) {\n             bins[image_part[i]] += 1;\n         }\n     }\n }",
            "size_t m = 256;\n    std::vector<std::array<size_t, 256> > localBins(m);\n    int s = image.size();\n    int chunk = s / 4;\n    int chunk1 = chunk;\n    int chunk2 = chunk;\n    int chunk3 = chunk;\n    int chunk4 = chunk;\n    int size;\n    int i;\n    int rank;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        localBins[0][image[0]] = 1;\n    }\n    if (rank == 1) {\n        localBins[0][image[chunk1]] = 1;\n    }\n    if (rank == 2) {\n        localBins[0][image[chunk2]] = 1;\n    }\n    if (rank == 3) {\n        localBins[0][image[chunk3]] = 1;\n    }\n    if (rank == 4) {\n        localBins[0][image[chunk4]] = 1;\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int k = 1; k < size; ++k) {\n            std::vector<int> temp;\n            MPI_Status status;\n            MPI_Recv(&temp, 1, MPI_INT, k, 0, MPI_COMM_WORLD, &status);\n            if (temp.size() == 1) {\n                localBins[0][temp[0]] = 1;\n            }\n        }\n    }\n\n    else {\n        if (rank == 1) {\n            MPI_Send(&image[chunk1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n        if (rank == 2) {\n            MPI_Send(&image[chunk2], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n        if (rank == 3) {\n            MPI_Send(&image[chunk3], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n        if (rank == 4) {\n            MPI_Send(&image[chunk4], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            for (int j = 0; j < m; j++) {\n                bins[j] += localBins[i][j];\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        if (rank == 0) {\n            #pragma omp for schedule(static)\n            for (int i = 0; i < 256; i++) {\n                bins[i] = 0;\n            }\n\n            #pragma omp for schedule(static)\n            for (int i = 0; i < image.size(); i++) {\n                bins[image[i]]++;\n            }\n        }\n        #pragma omp barrier\n    }\n\n    int sum;\n    MPI_Reduce(&bins, &sum, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int size = 8;\n   const int rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   const int nprocs = 8;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n   if (nprocs > size) {\n     if (rank == 0) {\n       std::cout << \"This program needs at most \" << size << \" processes\" << std::endl;\n     }\n     MPI_Finalize();\n     exit(0);\n   }\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] = 0;\n     }\n   }\n\n   std::vector<int> myImage(image.begin() + rank * size / nprocs,\n                            image.begin() + (rank + 1) * size / nprocs);\n\n   std::vector<int> myBins(256, 0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < myImage.size(); i++) {\n     myBins[myImage[i]]++;\n   }\n\n   int count = 0;\n   MPI_Reduce(&myBins, &bins, 256, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       std::cout << i << \" \" << bins[i] << std::endl;\n     }\n   }\n }",
            "// Implement this function\n}",
            "}",
            "int rank, num_proc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n   // Your code goes here.\n }",
            "// your code here\n  int nproc, rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int mpi_bins[256] = {0};\n  size_t omp_bins[256] = {0};\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i){\n      omp_bins[image[i]]++;\n  }\n\n  MPI_Reduce(&omp_bins, &mpi_bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0){\n      for (size_t i = 0; i < 256; ++i){\n          bins[i] = mpi_bins[i];\n      }\n  }\n}",
            "// TODO: Implement this.\n  int nRank, nTask;\n  int rSize, tSize;\n  int rOffset, tOffset;\n  int imgSize = image.size();\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &nRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nTask);\n \n  rSize = imgSize / nTask;\n  tSize = imgSize % nTask;\n  rOffset = nRank * rSize;\n  tOffset = rSize * nRank + tSize * (nRank > tSize? (nRank - tSize) : nRank);\n\n  // printf(\"rank: %d, rSize: %d, tSize: %d, rOffset: %d, tOffset: %d\\n\", nRank, rSize, tSize, rOffset, tOffset);\n\n  std::array<size_t, 256> myBins = {};\n  #pragma omp parallel for\n  for (int i = tOffset; i < tOffset + rSize + (nRank < tSize? 1 : 0); i++) {\n    myBins[image[i]]++;\n  }\n\n  if (nRank == 0) {\n    for (int i = 0; i < nTask; i++) {\n      MPI_Status status;\n      std::array<size_t, 256> tempBins;\n      MPI_Recv(tempBins.data(), 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n      for (int j = 0; j < 256; j++) {\n        bins[j] += tempBins[j];\n      }\n    }\n  } else {\n    MPI_Send(myBins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "for (int i = 0; i < 256; i++) {\n\t\tbins[i] = 0;\n\t}\n\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint num_elements = image.size();\n\tint elements_per_rank = num_elements / size;\n\n\tfor (int i = 0; i < elements_per_rank; i++) {\n\t\tint index = rank * elements_per_rank + i;\n\t\tbins[image[index]]++;\n\t}\n\n\tMPI_Reduce(&bins, &bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: add your code here\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (rank==0) {\n     for (int i=0; i<image.size(); i++)\n       bins[image[i]]++;\n   }\n   else {\n     for (int i=0; i<image.size(); i++) {\n       if (image[i]==rank)\n         bins[image[i]]++;\n     }\n   }\n   MPI_Reduce(&bins, &bins, 256, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// initialize vector of bins to 0\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n  int nranks, rank, mystart, myend, mycount;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  // get range of pixels that each rank should count\n  int perrank = image.size() / nranks;\n  mystart = perrank * rank;\n  myend = perrank * (rank + 1);\n  \n  // count the pixels in the range for the rank\n  for (int i = mystart; i < myend; i++) {\n    mycount = image[i];\n    bins[mycount] += 1;\n  }\n  \n  // gather all counts on rank 0\n  std::array<size_t, 256> recv_bins;\n  MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, recv_bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  \n  // rank 0 should now have all counts in recv_bins\n  if (rank == 0) {\n    bins = recv_bins;\n  }\n  return;\n}",
            "int i, j;\n  int npixels, npixels_local;\n  int mpi_rank, mpi_size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  int npixels_per_proc = image.size() / mpi_size;\n\n  npixels = image.size();\n  npixels_local = npixels_per_proc;\n\n  // Count pixels on rank 0.\n  if(mpi_rank == 0) {\n    for(i = 0; i < npixels; i++) {\n      bins[image[i]] += 1;\n    }\n  }\n  else {\n    for(i = 0; i < npixels_local; i++) {\n      bins[image[i]] += 1;\n    }\n  }\n\n  // Merge counts from other ranks.\n  if(mpi_rank == 0) {\n    for(j = 1; j < mpi_size; j++) {\n      MPI_Status status;\n      MPI_Recv(&bins, 256, MPI_UNSIGNED_LONG_LONG, j, j, MPI_COMM_WORLD, &status);\n    }\n  }\n  else {\n    MPI_Send(&bins, 256, MPI_UNSIGNED_LONG_LONG, 0, mpi_rank, MPI_COMM_WORLD);\n  }\n}",
            "int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    //std::cout << \"I am rank: \" << my_rank << std::endl;\n\n    int comm_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    //std::cout << \"Communicator size: \" << comm_size << std::endl;\n\n    // Partition image\n    int elements_per_rank = image.size() / comm_size;\n    int rest = image.size() % comm_size;\n\n    int begin = my_rank * elements_per_rank;\n    int end = begin + elements_per_rank;\n    if (my_rank == comm_size - 1) {\n        end += rest;\n    }\n\n    // Count in parallel\n    std::array<size_t, 256> counts;\n    #pragma omp parallel for\n    for (int i = begin; i < end; ++i) {\n        ++counts[image[i]];\n    }\n\n    // Combine results\n    std::array<size_t, 256> counts_total;\n    MPI_Reduce(counts.data(), counts_total.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Write result to bins\n    if (my_rank == 0) {\n        std::copy(counts_total.begin(), counts_total.end(), bins.begin());\n    }\n\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   auto local_bins = std::array<size_t, 256>{};\n   auto local_size = image.size();\n   int size_per_thread, remain;\n\n#pragma omp parallel num_threads(4)\n   {\n     size_per_thread = image.size() / 4;\n     remain = image.size() % 4;\n#pragma omp critical\n     {\n       local_size += remain;\n     }\n\n     std::vector<int> image_thread(size_per_thread);\n     int thread_id = omp_get_thread_num();\n     int thread_size = size_per_thread + (thread_id < remain? 1 : 0);\n     int start_index = thread_id * size_per_thread + (thread_id < remain? thread_id : remain);\n     int end_index = start_index + thread_size;\n     for (int i = start_index; i < end_index; i++)\n     {\n       image_thread[i - start_index] = image[i];\n     }\n\n     for (int i = 0; i < 256; i++)\n     {\n       for (int j = 0; j < thread_size; j++)\n       {\n         if (image_thread[j] == i)\n         {\n           local_bins[i]++;\n         }\n       }\n     }\n   }\n\n   if (rank == 0)\n   {\n     MPI_Gather(&local_bins, 256, MPI_UNSIGNED_LONG, &bins, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   }\n   else\n   {\n     MPI_Gather(&local_bins, 256, MPI_UNSIGNED_LONG, nullptr, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO: insert your implementation here.\n\tstd::fill(bins.begin(), bins.end(), 0);\n\tif (image.size() == 0) return;\n\t\n\tint m = image.size();\n\tint n = 256;\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint p_rows = m / size;\n\tint p_cols = n;\n\t\n\tstd::vector<int> image_rank;\n\tint rank_start;\n\tint rank_end;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size - 1; i++) {\n\t\t\trank_start = p_rows * i;\n\t\t\trank_end = p_rows * (i + 1) - 1;\n\t\t\timage_rank.insert(image_rank.end(), image.begin() + rank_start, image.begin() + rank_end);\n\t\t\tMPI_Send(image_rank.data(), p_rows, MPI_INT, i + 1, 0, MPI_COMM_WORLD);\n\t\t}\n\t\t\n\t\trank_start = p_rows * (size - 1);\n\t\trank_end = m;\n\t\timage_rank.insert(image_rank.end(), image.begin() + rank_start, image.begin() + rank_end);\n\t\t\n\t\tMPI_Send(image_rank.data(), p_rows, MPI_INT, size - 1, 0, MPI_COMM_WORLD);\n\t}\n\telse {\n\t\tMPI_Status status;\n\t\tMPI_Recv(image_rank.data(), p_rows, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n\t}\n\t\n\tif (rank!= 0) {\n\t\tstd::array<size_t, 256> bins_rank;\n\t\tstd::fill(bins_rank.begin(), bins_rank.end(), 0);\n\t\t\n\t\t#pragma omp parallel for\n\t\tfor (int i = 0; i < p_rows; i++) {\n\t\t\tbins_rank[image_rank[i]]++;\n\t\t}\n\t\t\n\t\tMPI_Send(bins_rank.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n\t}\n\t\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size - 1; i++) {\n\t\t\tMPI_Status status;\n\t\t\tMPI_Recv(bins.data() + i * p_cols, 256, MPI_UNSIGNED_LONG, i + 1, 0, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\telse {\n\t\tMPI_Send(bins.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "// TODO: add parallel code here.\n\n\n }",
            "// TODO\n }",
            "// TODO: add parallel code here\n   // Hint: \n   // 1. use OpenMP to count pixels in parallel on each rank\n   // 2. use MPI to sum up all bins on rank 0\n   // 3. check https://stackoverflow.com/questions/14498235/how-to-sum-up-arrays-of-different-sizes-in-c for reference\n   // 4. note that MPI_Reduce cannot be called in parallel. So need to use MPI_Allreduce\n   // 5. note that MPI_Reduce cannot be called in parallel. So need to use MPI_Allreduce\n   // 6. use MPI_Reduce to sum up bins from all ranks on rank 0\n   \n   // TODO: write your code here\n   const int world_size = omp_get_num_threads();\n   const int rank = omp_get_thread_num();\n   int temp[256]={0};\n   int temp_sum[256]={0};\n\n   #pragma omp parallel for\n   for(int i=0; i<256; ++i){\n\t   for(int j=0; j<image.size(); j++){\n\t\t   if(image[j]==i){\n\t\t\t   temp[i] +=1;\n\t\t   }\n\t   }\n   }\n\n   MPI_Reduce(temp, temp_sum, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n\t\tfor(int i=0; i<256; ++i){\n\t\t\tbins[i] = temp_sum[i];\n\t\t}\n   }\n   \n   // printf(\"temp[255]= %d\\n\", temp[255]);\n   \n   // // MPI_Reduce(temp, temp_sum, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   // // if (rank == 0) {\n   // // \tfor(int i=0; i<256; ++i){\n   // // \t\tbins[i] = temp_sum[i];\n   // // \t}\n   // // }\n   // printf(\"temp[255]= %d\\n\", temp[255]);\n\n}",
            "// TODO\n }",
            "int size, rank;\n\n  // Get number of processes\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Get rank of process\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Check if size is a multiple of 2\n  if (size % 2 == 0){\n    // Check if rank 0\n    if (rank == 0){\n      // Split the image array into 2 arrays\n      std::vector<int> array1(image.size() / 2);\n      std::vector<int> array2(image.size() / 2);\n      // Store the half of image in array1 and array2\n      array1 = std::vector<int>(image.begin(), image.begin() + image.size() / 2);\n      array2 = std::vector<int>(image.begin() + image.size() / 2, image.end());\n      // Send array1 to rank 1\n      MPI_Send(&array1[0], image.size() / 2, MPI_INT, 1, 0, MPI_COMM_WORLD);\n      // Send array2 to rank 2\n      MPI_Send(&array2[0], image.size() / 2, MPI_INT, 2, 0, MPI_COMM_WORLD);\n      // Receive array1 from rank 2\n      MPI_Recv(&array1[0], image.size() / 2, MPI_INT, 2, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      // Receive array2 from rank 1\n      MPI_Recv(&array2[0], image.size() / 2, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      // Concatenate array1 and array2 to get image back\n      image = std::vector<int>(array1.begin(), array1.end());\n      image.insert(image.end(), array2.begin(), array2.end());\n    }\n    else if (rank == 1){\n      // Split the image array into 2 arrays\n      std::vector<int> array1(image.size() / 2);\n      std::vector<int> array2(image.size() / 2);\n      // Store the half of image in array1 and array2\n      array1 = std::vector<int>(image.begin(), image.begin() + image.size() / 2);\n      array2 = std::vector<int>(image.begin() + image.size() / 2, image.end());\n      // Send array1 to rank 2\n      MPI_Send(&array1[0], image.size() / 2, MPI_INT, 2, 0, MPI_COMM_WORLD);\n      // Receive array2 from rank 0\n      MPI_Recv(&array2[0], image.size() / 2, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      // Concatenate array1 and array2 to get image back\n      image = std::vector<int>(array1.begin(), array1.end());\n      image.insert(image.end(), array2.begin(), array2.end());\n    }\n    else if (rank == 2){\n      // Split the image array into 2 arrays\n      std::vector<int> array1(image.size() / 2);\n      std::vector<int> array2(image.size() / 2);\n      // Store the half of image in array1 and array2\n      array1 = std::vector<int>(image.begin(), image.begin() + image.size() / 2);\n      array2 = std::vector<int>(image.begin() + image.size() / 2, image.end());\n      // Receive array1 from rank 1\n      MPI_Recv(&array1[0], image.size() / 2, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      // Send array2 to rank 0\n      MPI_Send(&array2[0], image.size() / 2,",
            "MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int imageSize = image.size();\n   int imagePartSize = imageSize/world_size;\n   int imageLastPartSize = imageSize - (imageSize - imagePartSize*world_size);\n   int beginPos = rank * imagePartSize;\n   int endPos = (rank + 1) * imagePartSize;\n   if(rank == world_size - 1) {\n    endPos += imageLastPartSize;\n   }\n\n   #pragma omp parallel for\n   for (int i=beginPos; i<endPos; ++i) {\n    bins[image[i]]++;\n   }\n\n   MPI_Reduce(bins, bins, 256, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n}",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    \n    //...\n}",
            "// replace this with your code\n    int size;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int* local = new int[256];\n    int* total = new int[256];\n    for (int i = 0; i < 256; i++) local[i] = 0;\n\n    int count = image.size() / size;\n    int rem = image.size() % size;\n\n    #pragma omp parallel\n    {\n        int local_count = count + (rem > 0);\n        int local_rem = (rem > 0);\n        int start = local_count * rank;\n        int end = start + local_count;\n\n        for (int i = start; i < end; i++) {\n            int value = image[i];\n            #pragma omp atomic\n            local[value]++;\n        }\n\n        int recv_buf[256];\n        int send_buf[256];\n        for (int i = 0; i < 256; i++) send_buf[i] = local[i];\n\n        MPI_Reduce(send_buf, recv_buf, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            for (int i = 0; i < 256; i++) bins[i] = recv_buf[i];\n        }\n    }\n}",
            "bins.fill(0);\n    int rank;\n    int size;\n\n    // Get rank and number of processes\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Number of pixels in the image\n    int n = image.size();\n\n    // Number of pixels per rank\n    int per_rank = n / size;\n\n    // Last rank gets any leftovers\n    int leftovers = n % size;\n\n    // Last rank gets one more pixel than the rest\n    if(rank == (size - 1)) {\n        per_rank += leftovers;\n    }\n\n    // The first pixel index for this rank\n    int start = rank * per_rank;\n\n    // The last pixel index for this rank\n    int end = (rank + 1) * per_rank - 1;\n\n    // If this is the last rank, it needs to go from start to n - 1\n    if(rank == (size - 1)) {\n        end = n - 1;\n    }\n\n    // Use OpenMP to parallelize the counting\n    #pragma omp parallel for\n    for(int i = start; i <= end; i++) {\n        bins[image[i]]++;\n    }\n\n    // Sum across ranks\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: count the pixels with each grayscale intensity\n   // Store the results in `bins` on rank 0\n   // Example:\n   //\n   // image = [2, 116, 201, 11, 92, 92, 201, 4, 2]\n   // bins = [0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0,...]\n   //\n   // Note: You should use OpenMP to parallelize the loop over the pixels.\n   // TODO: use MPI to distribute the work of counting to multiple ranks\n   //\n   // Example:\n   //\n   // image = [2, 116, 201, 11, 92, 92, 201, 4, 2]\n   // MPI_Size = 2\n   // rank 0: counts pixels with value 0-127\n   // rank 1: counts pixels with value 128-255\n   //\n   // rank 0 bins = [2, 0, 0, 1, 0,...]\n   // rank 1 bins = [0, 0, 0, 0, 1, 1,...]\n   //\n   // rank 0 bins = [2, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0,...]\n   //\n   // Note: bins is only updated on rank 0\n   //\n   // You can use a chunk size that is smaller than the image size, but the chunk\n   // size should be a multiple of the number of ranks. For example, if the image\n   // size is 64, then we should use a chunk size of 64, 128, 192, 256, or 320.\n   //\n   // You can assume that the image size is a multiple of the chunk size.\n   //\n   // Note: The bins array should be initialized to 0 before calling this function\n   //\n   // Example:\n   //\n   // bins = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...]\n   //\n   // You can assume that image has at least one element, that the size of the image\n   // is a multiple of the number of ranks, and that the size of the image is a\n   // multiple of the chunk size.\n   //\n   // Note: You should use OpenMP to parallelize the loop over the pixels.\n   // TODO: use MPI to distribute the work of counting to multiple ranks\n   //\n   // Example:\n   //\n   // image = [2, 116, 201, 11, 92, 92, 201, 4, 2]\n   // MPI_Size = 2\n   // rank 0: counts pixels with value 0-127\n   // rank 1: counts pixels with value 128-255\n   //\n   // rank 0 bins = [2, 0, 0, 1, 0,...]\n   // rank 1 bins = [0, 0, 0, 0, 1, 1,...]\n   //\n   // rank 0 bins = [2, 0, 0, 1, 0, 0, 2, 0, 0, 1, 0, 0,...]\n   //\n   // Note: bins is only updated on rank 0\n   //\n   // You can use a chunk size that is smaller than the image size, but the chunk\n   // size should be a multiple of the number of ranks. For example, if the image\n   // size is 64, then we should use a chunk size of 64, 128, 192, 256, or 320.\n   //\n   // You can assume that the image size is a multiple of the chunk size.\n   //\n   // Note: The bins array should be initialized to 0 before calling this function\n   //\n   // Example:\n   //",
            "int numprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int blockSize = image.size() / numprocs;\n  int blockStart = rank * blockSize;\n  int blockEnd = (rank + 1) * blockSize;\n\n  int sum = 0;\n\n  #pragma omp parallel for reduction(+:sum)\n  for (int i = blockStart; i < blockEnd; ++i) {\n    sum += bins[image[i]];\n    bins[image[i]]++;\n  }\n  MPI_Reduce(&sum, &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Your code goes here\n  if (rank == 0){\n      #pragma omp parallel for\n      for (size_t i = 0; i < image.size(); ++i){\n          ++bins[image[i]];\n      }\n  }\n  else {\n      #pragma omp parallel for\n      for (size_t i = 0; i < image.size(); ++i){\n          bins[image[i]] += 0;\n      }\n  }\n\n}",
            "int p, rank, num_procs;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<int> local_image(image.size()/num_procs, 0);\n  std::array<size_t, 256> local_bins;\n\n  if(rank == 0) {\n    std::copy(image.begin(), image.end(), local_image.begin());\n  }\n  MPI_Scatter(local_image.data(), \n              local_image.size(), \n              MPI_INT, \n              image.data(), \n              local_image.size(), \n              MPI_INT, \n              0, \n              MPI_COMM_WORLD);\n\n  local_bins.fill(0);\n  #pragma omp parallel for\n  for(int i = 0; i < local_image.size(); i++) {\n    local_bins[local_image[i]]++;\n  }\n\n  MPI_Gather(local_bins.data(), \n             local_bins.size(), \n             MPI_UNSIGNED_LONG, \n             bins.data(), \n             local_bins.size(), \n             MPI_UNSIGNED_LONG, \n             0, \n             MPI_COMM_WORLD);\n\n}",
            "// You should fill this in\n\n }",
            "MPI_Status status;\n  // int world_rank;\n  // int world_size;\n  int num_rows, num_cols;\n  // int my_rank;\n  int my_row, my_col;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Dims_create(world_size, 2, coords);\n\n  my_row = coords[0];\n  my_col = coords[1];\n\n  MPI_Cart_rank(MPI_COMM_WORLD, coords, &my_rank);\n  MPI_Cart_coords(MPI_COMM_WORLD, my_rank, 2, coords);\n  my_row = coords[0];\n  my_col = coords[1];\n\n  int num_rows_per_proc = int(image.size()) / world_size;\n\n  int row_start = num_rows_per_proc * my_rank;\n  int row_end = row_start + num_rows_per_proc;\n\n  // For each row:\n  // Find local max and local min, then find the global min and max\n  // Send row data to the proc with rank rank_for_max\n  // Send row data to the proc with rank rank_for_min\n  // Receive min_val and max_val from the proc with rank rank_for_max\n  // Receive min_val and max_val from the proc with rank rank_for_min\n  // Compute global_max_val and global_min_val\n\n  for (int i = 0; i < world_size; i++) {\n    for (int j = 0; j < world_size; j++) {\n      MPI_Cart_coords(MPI_COMM_WORLD, i, 2, coords);\n    }\n  }\n\n  MPI_Allreduce(&row_start, &num_rows, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&row_end, &num_rows, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n\n\n  int local_max = 0, local_min = 255;\n  for (int i = 0; i < num_rows; i++) {\n    local_max = max(local_max, image[i]);\n    local_min = min(local_min, image[i]);\n  }\n\n\n\n\n\n\n\n\n  MPI_Bcast(&local_max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&local_min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\n  for (int i = 0; i < world_size; i++) {\n    for (int j = 0; j < world_size; j++) {\n      MPI_Cart_coords(MPI_COMM_WORLD, i, 2, coords);\n    }\n  }\n\n  MPI_Allreduce(&local_max, &global_max, 1, MPI_INT, MPI_MAX, MPI_COMM_WORLD);\n  MPI_Allreduce(&local_min, &global_min, 1, MPI_INT, MPI_MIN, MPI_COMM_WORLD);\n\n  int bins_per_proc = int(bins.size()) / world_size;\n  for (int i = 0; i < world_size; i++) {\n    MPI_Cart_coords(MPI_COMM_WORLD, i, 2, coords);\n    int row_start = coords[0] * bins_per_proc;\n    int row_end = coords[0] * bins_per_proc + bins_per_proc;\n    if (i == 0) {\n      for (int i = row_start; i < row_end; i++) {\n        bins[i] = 0;\n      }\n    }\n  }\n\n  // The following",
            "}",
            "}",
            "// TODO\n}",
            "/* Your code here */\n\n}",
            "int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: Your code here\n }",
            "int np; // Number of processors\n   int myrank; // My rank\n   int num_threads; // Number of threads\n\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   omp_get_max_threads(&num_threads);\n\n   // TODO: Your code here\n\n}",
            "}",
            "size_t size = image.size();\n   size_t num_procs = 0;\n   int my_rank = 0;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   std::array<size_t, 256> my_bins;\n   for (int i = 0; i < 256; i++) {\n     my_bins[i] = 0;\n   }\n\n   // int num_threads = 0;\n   // #pragma omp parallel\n   // {\n   //   #pragma omp single\n   //   num_threads = omp_get_num_threads();\n   // }\n   int num_threads = 2;\n   size_t chunk_size = size / num_threads;\n   size_t remainder = size % num_threads;\n\n   // int my_thread_id = 0;\n   // #pragma omp parallel\n   // {\n   //   #pragma omp single\n   //   my_thread_id = omp_get_thread_num();\n   // }\n   int my_thread_id = 0;\n\n   if (my_rank == 0) {\n     for (int i = 1; i < num_procs; i++) {\n       MPI_Recv(&my_bins, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n     }\n     for (int i = 0; i < 256; i++) {\n       for (int j = 1; j < num_procs; j++) {\n         my_bins[i] += bins[i];\n       }\n     }\n   } else {\n     for (int i = 0; i < 256; i++) {\n       for (int j = 0; j < chunk_size; j++) {\n         if (image[j + my_thread_id * chunk_size] == i) {\n           my_bins[i]++;\n         }\n       }\n     }\n     for (int i = 0; i < remainder; i++) {\n       if (image[i + chunk_size * num_threads] == my_thread_id) {\n         my_bins[i]++;\n       }\n     }\n     MPI_Send(&my_bins, 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n   if (my_rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] += my_bins[i];\n     }\n   }\n}",
            "// Use MPI and OpenMP to parallelize across the whole image.\n  // Every rank has a complete copy of image.\n  // The results are stored in bins on rank 0.\n  MPI_Status status;\n  std::array<size_t, 256> localBins;\n  std::fill(bins.begin(), bins.end(), 0);\n  if (image.size() == 0) return;\n  auto chunkSize = image.size() / omp_get_num_threads();\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); ++i) {\n    #pragma omp critical\n    ++bins[image[i]];\n  }\n  for (int i = 1; i < omp_get_num_threads(); ++i) {\n    MPI_Recv(&localBins, 256, MPI_SIZE_T, i, 0, MPI_COMM_WORLD, &status);\n    for (int j = 0; j < localBins.size(); ++j) {\n      bins[j] += localBins[j];\n    }\n  }\n}",
            "// TODO\n }",
            "// TODO: your code here\n\n  }",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]] += 1;\n     }\n }",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Your code here\n}",
            "// Get number of MPI ranks\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\t\n\t// Calculate number of elements per rank\n\tint numberPerRank = image.size() / size;\n\t\n\t// Calculate the first and last elements for each rank\n\tint first = rank * numberPerRank;\n\tint last = first + numberPerRank;\n\t\n\t// Create local bins\n\tstd::array<size_t, 256> localBins = {};\n\t\n\t// Calculate bins on each rank in parallel\n\t#pragma omp parallel for\n\tfor (int i = first; i < last; ++i) {\n\t\tint color = image[i];\n\t\t++localBins[color];\n\t}\n\t\n\t// Send all bins to rank 0\n\tMPI_Gather(localBins.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement\n}",
            "MPI_Status status;\n \tint rank, size;\n \tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n \tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n \tint count = image.size();\n \tint count_per_rank = count/size;\n \tint remainder = count%size;\n \tint start_rank = rank*count_per_rank;\n\n \tstd::vector<int> local_image;\n \tif (rank == 0) {\n \t\tlocal_image.resize(count_per_rank+remainder, 0);\n \t\tstd::copy_n(image.begin() + start_rank, count_per_rank+remainder, local_image.begin());\n \t}\n \telse {\n \t\tlocal_image.resize(count_per_rank, 0);\n \t\tstd::copy_n(image.begin() + start_rank, count_per_rank, local_image.begin());\n \t}\n\n \tif (rank == 0) {\n \t\tfor (int i = 0; i < size; ++i) {\n \t\t\tMPI_Recv(&bins[0], 256, MPI_UNSIGNED_LONG, i, i, MPI_COMM_WORLD, &status);\n \t\t}\n \t}\n \telse {\n \t\tstd::array<size_t, 256> temp;\n \t\tstd::fill(temp.begin(), temp.end(), 0);\n \t\tfor (auto &x : local_image) {\n \t\t\ttemp[x]++;\n \t\t}\n \t\tMPI_Send(&temp[0], 256, MPI_UNSIGNED_LONG, 0, rank, MPI_COMM_WORLD);\n \t}\n\n \tMPI_Barrier(MPI_COMM_WORLD);\n }",
            "// The number of pixels in the image\n  int N = image.size();\n  std::vector<int> localImage(image.size());\n  std::array<size_t, 256> localBins;\n  localBins.fill(0);\n  localImage = image;\n\n  int myRank;\n  int numProc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n\n  // The number of pixels to be counted by each MPI process\n  int mySize = N/numProc;\n  int leftover = N%numProc;\n\n  // The first rank is assigned extra pixels to complete the division of pixels\n  int myStart = myRank * mySize + (myRank == 0? leftover : 0);\n  int myEnd = myRank == numProc-1? N : (myRank+1) * mySize + (myRank+1 == numProc-1? leftover : 0);\n  localSize = myEnd - myStart;\n  //std::cout << myStart << \" \" << myEnd << std::endl;\n\n  #pragma omp parallel for\n  for (int i = myStart; i < myEnd; i++) {\n    localBins[image[i]]++;\n  }\n\n  MPI_Reduce(&localBins, &bins, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    \n    std::vector<int> local_image;\n    \n    if(rank == 0) {\n        for(int i=0; i<(int)image.size(); ++i)\n            local_image.push_back(image[i]);\n    } else {\n        MPI_Recv(&local_image, image.size(), MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    \n    for(int i=0; i<(int)local_image.size(); ++i)\n        bins[local_image[i]]++;\n    \n    if(rank == 0) {\n        MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(bins.data(), NULL, bins.size(), MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// 256 bins\n  for(int i=0;i<256;i++){\n\tbins[i]=0;\n  }\n\n  const int sz = image.size();\n  const int num_threads = omp_get_max_threads();\n  const int chunk = (sz + num_threads - 1) / num_threads;\n  //printf(\"%d\\n\",num_threads);\n  #pragma omp parallel\n  {\n  const int tid = omp_get_thread_num();\n  const int start = tid*chunk;\n  const int end = (tid + 1) * chunk;\n  //printf(\"start: %d, end: %d\\n\",start,end);\n  //printf(\"size: %d\\n\",sz);\n  //printf(\"%d, %d\\n\",start,end);\n  for(int i = start; i < end; i++){\n\tbins[image[i]] += 1;\n  }\n\n  }\n  MPI_Reduce(bins.data(),bins.data(),256,MPI_UNSIGNED_LONG_LONG,MPI_SUM,0,MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n\t// Hint: you may find it useful to use the std::reduce method to sum over the local counts\n\n }",
            "// Your code here\n    MPI_Status status;\n    MPI_Request request;\n    int rank, size;\n    int i, j, k;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::array<size_t, 256>> image_part(size);\n    std::array<size_t, 256> image_part_local;\n    int image_part_size = image.size()/size;\n    int image_part_size_remain = image.size()%size;\n    if(rank==0){\n        for(i=1; i<size; i++){\n            MPI_Send(&image[0]+(i-1)*image_part_size, image_part_size, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n        for(i=0; i<image_part_size; i++){\n            for(j=0; j<256; j++){\n                image_part_local[j] = 0;\n            }\n            for(j=0; j<image_part_size; j++){\n                image_part_local[image[j]]++;\n            }\n            image_part[0][j] = image_part_local[j];\n        }\n    }\n    else{\n        MPI_Recv(&image_part[rank][0], image_part_size, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        MPI_Get_count(&status, MPI_INT, &image_part_size);\n        for(j=0; j<image_part_size; j++){\n            for(k=0; k<256; k++){\n                image_part[rank][k] = 0;\n            }\n            for(k=0; k<image_part_size; k++){\n                image_part[rank][image[j]]++;\n            }\n            MPI_Send(&image_part[rank][k], 256, MPI_INT, 0, rank, MPI_COMM_WORLD);\n        }\n    }\n    if(rank==0){\n        for(i=0; i<256; i++){\n            bins[i] = 0;\n        }\n        for(i=0; i<size; i++){\n            MPI_Recv(&image_part[i][k], 256, MPI_INT, i, i, MPI_COMM_WORLD, &status);\n            MPI_Get_count(&status, MPI_INT, &image_part_size);\n            for(j=0; j<image_part_size; j++){\n                bins[j] += image_part[i][j];\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: implement this!\n    int num_threads = omp_get_max_threads();\n    int num_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n    int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n\n    std::array<size_t, 256> bin;\n    int image_size = image.size();\n    int block_size = image_size / num_threads;\n    int thread_id = omp_get_thread_num();\n    int start = block_size * thread_id;\n    int end = start + block_size;\n    if(thread_id == num_threads - 1) {\n        end = image_size;\n    }\n\n    for(int i = start; i < end; i++) {\n        bin[image[i]]++;\n    }\n\n    MPI_Reduce(bin.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// YOUR CODE GOES HERE.\n  // You can use OpenMP parallelization and MPI collective communications.\n}",
            "// Make sure the results are initialized to zero.\n   for (size_t i = 0; i < 256; ++i) {\n     bins[i] = 0;\n   }\n\n   int myid, numprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n   const int n = image.size();\n   // Each rank counts its own portion.\n   std::array<size_t, 256> counts;\n   for (int i = 0; i < 256; ++i) {\n     counts[i] = 0;\n   }\n   if (n > 0) {\n     int start = myid*n/numprocs;\n     int end = (myid+1)*n/numprocs;\n     for (int i = start; i < end; i++) {\n       counts[image[i]]++;\n     }\n   }\n\n   // All ranks need to contribute their counts.\n   // First, get the counts of each rank.\n   std::array<size_t, 256> counts_recv;\n   MPI_Gather(counts.data(), 256, MPI_UNSIGNED_LONG,\n             counts_recv.data(), 256, MPI_UNSIGNED_LONG,\n             0, MPI_COMM_WORLD);\n   if (myid == 0) {\n     for (int i = 0; i < numprocs; ++i) {\n       for (int j = 0; j < 256; ++j) {\n         bins[j] += counts_recv[j];\n       }\n     }\n   }\n }",
            "// Count the number of pixels with each intensity\n    // and store the results in `bins`\n\n    // 1. Compute the size of the image vector.\n    // 2. Loop over the pixels and update the bins vector with pixel intensities.\n    // 3. Update the bins vector with the correct results.\n}",
            "// YOUR CODE HERE\n }",
            "// TODO\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Partition image\n    size_t chunk_size = image.size()/size;\n    size_t start_index = rank*chunk_size;\n    size_t end_index = (rank == size - 1)? image.size() : start_index + chunk_size;\n    std::vector<int> my_image(image.begin() + start_index, image.begin() + end_index);\n\n    // Parallel count\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < my_image.size(); i++) {\n        bins[my_image[i]]++;\n    }\n\n    // Reduce\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "using ExecutionSpace = typename Kokkos::DefaultExecutionSpace;\n\n    Kokkos::parallel_for(\"counts\", 26, KOKKOS_LAMBDA(size_t i) {\n        for (int j = 0; j < s.extent(0); j++) {\n            if (s(j)[0] == 'a' + i) {\n                Kokkos::atomic_increment(&bins(i));\n            }\n        }\n    });\n}",
            "Kokkos::parallel_for(s.extent(0),\n    KOKKOS_LAMBDA(size_t i) {\n      char first = s(i)[0];\n      if (first >= 'a' && first <= 'z') {\n        ++bins[first - 'a'];\n      }\n    });\n}",
            "const size_t n = s.extent(0);\n    // use Kokkos parallel_for and Kokkos parallel_reduce to compute the counts\n    //\n    // you can access the elements in s using the [] operator\n    // you can access the elements in bins using the () operator\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //\n    //",
            "constexpr size_t num_letters = 26;\n\n  // Use this to count the first letter of each string.\n  Kokkos::View<size_t[num_letters]> counts(\"counts\");\n\n  // TODO: Count the number of strings in the input vector that start with each letter of the alphabet\n  // Hint: you can use the C++ std::string library to get the first letter of each string.\n  // Hint: you can use Kokkos parallel_for with a lambda function.\n  // Hint: you can use the `counts` array to store the results\n  // Hint: use Kokkos::single to update the `bins` array\n\n  // Copy the results into the `bins` array\n  Kokkos::parallel_for(\n    \"copy_counts\",\n    Kokkos::RangePolicy<Kokkos::",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> (0, s.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      char first = tolower(s(i)[0]);\n      if (first >= 'a' && first <= 'z') {\n        bins[first - 'a']++;\n      }\n    });\n  Kokkos::fence();\n}",
            "// Implement me\n}",
            "// Create a parallel_for lambda function\n    Kokkos::parallel_for(\n        \"firstLetterCounts\",\n        Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceFull, Kokkos::DefaultExecutionSpace>>(0, s.extent(0)),\n        KOKKOS_LAMBDA(int i, Kokkos::Reduce<Kokkos::ReduceFull, Kokkos::DefaultExecutionSpace> &reducer) {\n            auto &value = reducer.reference();\n            auto &bin = bins[s(i,0) - 'a'];\n            Kokkos::atomic_fetch_add(&bin, 1);\n    });\n\n    // The lambda function has now been executed in parallel.\n    // The final value of `bin` has been computed in a parallel-safe way.\n    Kokkos::Reduce<Kokkos::ReduceFull, Kokkos::DefaultExecutionSpace> reducer(Kokkos::DefaultExecutionSpace());\n    Kokkos::parallel_reduce(\n        \"firstLetterCounts\",\n        Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceFull, Kokkos::DefaultExecutionSpace>>(0, s.extent(0)),\n        KOKKOS_LAMBDA(int i, Kokkos::Reduce<Kokkos::ReduceFull, Kokkos::DefaultExecutionSpace> &reducer) {\n            auto &value = reducer.reference();\n            auto &bin = bins[s(i,0) - 'a'];\n            Kokkos::atomic_fetch_add(&bin, 1);\n    }, reducer);\n}",
            "//TODO: Fill in the implementation\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n                         KOKKOS_LAMBDA(const int i) {\n                             char c = s(i)[0];\n                             if (c >= 'a' && c <= 'z') {\n                                 Kokkos::atomic_increment(&bins[c - 'a']);\n                             }\n                         });\n    Kokkos::fence();\n}",
            "//...\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 26), KOKKOS_LAMBDA(const int i) {\n        bins(i) = 0;\n    });\n\n    const int n = s.extent_int(0);\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, n), KOKKOS_LAMBDA(const int i) {\n        if(s(i)!= NULL) {\n            bins(s(i)[0] - 'a')++;\n        }\n    });\n}",
            "using functor_t = Kokkos::RangePolicy<Kokkos::OpenMP, size_t>;\n  functor_t::run([=] (size_t i) {\n      if (s(i) == nullptr) return;\n      auto const letter = s(i)[0];\n      if (letter >= 'a' && letter <= 'z') {\n        auto const bin_index = size_t(letter - 'a');\n        Kokkos::atomic_add(&bins[bin_index], 1);\n      }\n  });\n  Kokkos::fence();\n}",
            "// (1) Use Kokkos parallel_for to loop over all strings\n  //     Use a lambda to get the first letter of each string\n  // (2) Use Kokkos parallel_reduce to loop over all strings\n  //     Use a lambda to get the first letter of each string\n\n  // TODO: complete the missing code.\n\n  // The following code is for testing.\n  // Do not modify it.\n  std::vector<std::string> s_host(s.extent(0));\n  Kokkos::deep_copy(s_host, s);\n  std::vector<size_t> bins_host(26, 0);\n  for (const auto& str : s_host) {\n    if (str.size() == 0)\n      continue;\n    const size_t firstLetter = str[0] - 'a';\n    bins_host[firstLetter]++;\n  }\n  Kokkos::deep_copy(bins, bins_host);\n  Kokkos::fence();\n}",
            "// Your code here\n}",
            "// Insert your code here.\n}",
            "// Create a kernel, which will be executed in parallel for each string.\n    // The kernel should:\n    // - Compute the first character of the string, which is the index in the alphabet.\n    // - Increment the corresponding bin in the bins array.\n    // Hint: You can use the `toupper` function from `cctype`.\n    // Hint: You can use `size_t` as the index type for bins.\n\n    Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      char f = tolower(s(i)[0]);\n      if (f >= 'a' && f <= 'z') {\n        bins(f - 'a') += 1;\n      }\n    });\n    Kokkos::fence();\n}",
            "/* Use a parallel_for loop to count the number of strings in the vector s that start with each letter of the alphabet.\n     Assume all strings are in lower case.\n     Assume Kokkos::RangePolicy and Kokkos::Schedule<Kokkos::Static> are valid scheduling policies for the parallel_for loop.\n\n     The letter \"a\" has index 0 in the array bins.\n     The letter \"b\" has index 1 in the array bins.\n     The letter \"c\" has index 2 in the array bins.\n    ...\n     The letter \"z\" has index 25 in the array bins.\n\n     Hint: You can use the std::toupper function to transform a character into upper case.\n     Hint: You can use the std::tolower function to transform a character into lower case.\n     Hint: You can use the std::isalpha function to test if a character is alphabetic.\n     Hint: You can use the std::isupper function to test if a character is upper case.\n     Hint: You can use the std::islower function to test if a character is lower case.\n     Hint: You can use the Kokkos::atomic_fetch_add function to increment an atomic variable.\n     Hint: Kokkos::atomic_fetch_add takes three arguments: the atomic variable to increment,\n           the value to add to it, and the memory space to use.\n           The first and second arguments are references while the third argument is the memory space.\n           For example, Kokkos::atomic_fetch_add(var, 1, Kokkos::HostSpace()) increments the atomic variable var by 1.\n     Hint: You can use the Kokkos::atomic_fetch_add function to increment an atomic variable.\n     Hint: Kokkos::atomic_fetch_add takes three arguments: the atomic variable to increment,\n           the value to add to it, and the memory space to use.\n           The first and second arguments are references while the third argument is the memory space.\n           For example, Kokkos::atomic_fetch_add(var, 1, Kokkos::HostSpace()) increments the atomic variable var by 1.\n  */\n  Kokkos::parallel_for(\n  /* Your code goes here */\n  );\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n  using device_type = Kokkos::Device<execution_space, Kokkos::HostSpace>;\n  using view_type = Kokkos::View<const char**, device_type>;\n  using counts_type = Kokkos::View<size_t[26], device_type>;\n\n  const size_t n = s.extent_int(0);\n  const size_t n_bins = bins.extent_int(0);\n\n  //TODO: Implement\n\n  // Copy the counts to host:\n  Kokkos::deep_copy(bins, bins_on_device);\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::ExecPolicy>(0, s.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      char firstLetter = s(i)[0];\n      if (firstLetter >= 'a' && firstLetter <= 'z') {\n        Kokkos::atomic_fetch_add(&bins[firstLetter - 'a'], 1);\n      }\n    }\n  );\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"First Letter Count\", Kokkos::RangePolicy<Kokkos::Rank<1>>(0, s.size()), KOKKOS_LAMBDA(const int i) {\n        const char c = *s(i);\n        if (c >= 'a' && c <= 'z') {\n            // Accessing Kokkos Views must happen within a parallel for\n            // loop, unless it is a write-only view.\n            ++bins(c - 'a');\n        }\n    });\n}",
            "using namespace Kokkos;\n\n  int N = s.extent(0);\n  parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    char first = tolower(s(i)[0]);\n    if (first >= 'a' && first <= 'z')\n      atomic_fetch_add(&bins(first - 'a'), 1);\n  });\n\n  // TODO\n}",
            "// Your code goes here\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(size_t i) {\n    bins[s[i][0] - 'a']++;\n  });\n}",
            "using ExecutionPolicy = Kokkos::RangePolicy<Kokkos::OpenMP>;\n  Kokkos::parallel_for(ExecutionPolicy(0, s.extent(0)), KOKKOS_LAMBDA(int i) {\n    for (size_t j = 0; j < 26; ++j)\n      if (s(i)[0] == 'a' + j)\n        ++bins(j);\n  });\n  Kokkos::fence();\n}",
            "// Fill in this function\n\n}",
            "// TODO: Fill in your code here\n}",
            "// TODO\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::Serial>(0, s.extent(0));\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(size_t i) {\n    char first = s(i)[0];\n    if (first >= 'a' && first <= 'z') {\n      auto pos = first - 'a';\n      Kokkos::atomic_increment(&bins(pos));\n    }\n  });\n}",
            "// Replace this code with your solution.\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", s.extent(0), KOKKOS_LAMBDA(const int i) {\n        int letter = s(i, 0) - 'a';\n        assert(letter >= 0 && letter < 26);\n        Kokkos::atomic_increment(&bins(letter));\n    });\n    Kokkos::fence();\n}",
            "Kokkos::View<char*> alphabet(\"alphabet\", 26);\n    // Create alphabet array\n    Kokkos::parallel_for(\n        \"Initialize alphabet\",\n        Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 26),\n        KOKKOS_LAMBDA(const int i) { alphabet(i) = 'a' + i; });\n    // Create parallel_reduce lambda function that will be executed on the GPU\n    Kokkos::parallel_reduce(\n        \"Count first letters\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n        KOKKOS_LAMBDA(const int i, size_t& lsum) {\n            char letter = s(i)[0];\n            if(letter >= 'a' && letter <= 'z')\n                Kokkos::atomic_fetch_add(&bins(letter - 'a'), 1);\n        },\n        Kokkos::Sum<size_t>(bins.data()));\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n    char c = s(i)[0];\n    if (c!= 0) {\n      bins(c - 'a')++;\n    }\n  });\n\n  // NOTE: no need to add a Kokkos::fence() here since the View bins is not const\n}",
            "// Put your code here\n}",
            "}",
            "//...\n}",
            "//...\n}",
            "// TODO: Replace this with the code to perform parallel first letter counting using Kokkos.\n}",
            "const size_t n = s.size();\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int& i) {\n    size_t bin = s(i)[0] - 'a';\n    Kokkos::atomic_add(&bins[bin], 1);\n  });\n}",
            "Kokkos::parallel_for(s.size(), KOKKOS_LAMBDA(size_t i) {\n    char c = s(i)[0];\n    if (c >= 'a' && c <= 'z') {\n      ++bins(c - 'a');\n    }\n  });\n}",
            "//\n    // your code here\n    //\n}",
            "// TODO\n}",
            "// TODO\n}",
            "using device_t = Kokkos::DefaultExecutionSpace;\n    using size_type = Kokkos::View<size_t[26]>::size_type;\n\n    const size_type n = s.extent(0);\n    Kokkos::View<size_t[26], device_t> counts(Kokkos::ViewAllocateWithoutInitializing(\"counts\"), 26);\n\n    const int count_team_size = 128;\n    const int count_vector_size = 128;\n    const int count_league_size = 100;\n\n    // compute the first letter counts in parallel\n    Kokkos::parallel_for(\"count_first_letters\",\n                         Kokkos::TeamPolicy<>(count_league_size,\n                                              count_team_size,\n                                              count_vector_size),\n                         KOKKOS_LAMBDA(const Kokkos::TeamPolicy<>::member_type &teamMember) {\n        const size_type i = teamMember.league_rank();\n\n        // count the first letters in s[i]\n        if (i < n) {\n            auto letter = s(i, 0);\n            if (letter >= 'a' && letter <= 'z') {\n                Kokkos::atomic_increment<Kokkos::MemoryOrder::relaxed, Kokkos::MemoryTraits<Kokkos::Unmanaged>>(&(counts[letter - 'a']));\n            }\n        }\n    });\n\n    Kokkos::fence();\n\n    // copy back to the host the counts array\n    Kokkos::deep_copy(bins, counts);\n}",
            "// your code here\n}",
            "// TODO: Your code here\n  // Make sure to use bins[0] and bins[1] for the first two letters\n\n  // Use parallel for to iterate over each string in s\n  // You can do this with a Kokkos::parallel_for\n  // Example:\n  // Kokkos::parallel_for( \"string_counts_functor\", s.extent(0), KOKKOS_LAMBDA( int i ) {\n  //     if (s(i)[0] == 'a')\n  //       bins[0] += 1;\n  //   });\n\n}",
            "// TODO\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    bins[s(i)[0] - 'a']++;\n  });\n  Kokkos::fence();\n}",
            "using Kokkos::View;\n  using Kokkos::parallel_for;\n\n  // TODO: Implement this function\n  //\n  // See http://stackoverflow.com/a/7666621/1391753 for the general algorithm\n  // See http://stackoverflow.com/a/17605516/1391753 for how to implement parallel_for\n  //\n  // There is a parallel version of std::fill available in Kokkos::parallel_for.\n  //\n  // Kokkos::parallel_for<TeamPolicy<> >(... );\n  //\n  // See http://kokkos.github.io/release/1.0/kokkos_api_docs/md_kokkos__parallel_for.html for API documentation.\n  //\n  // Kokkos::fill(... );\n  //\n  // See http://kokkos.github.io/release/1.0/kokkos_api_docs/md_kokkos__algorithm.html#fill\n  //\n  //\n  // Here's the data needed to implement the algorithm:\n  //\n  // Kokkos::View<char *> const &s;\n  // Kokkos::View<size_t[26]> &bins;\n  //\n  // s.extent(0) is the number of strings\n  // s.extent(1) is the size of each string\n  //\n  // You'll have to do the following:\n  //\n  // 1. Create a Kokkos::View<int[26]> to store the temporary count array\n  // 2. Create a Kokkos::View<size_t[26]> to store the temporary histogram\n  // 3. Launch a parallel_for with a policy of \"RangePolicy\", where the range is the number of strings in s\n  // 4. For each string s(i),\n  //    4.1. Convert the first letter to its character position in the alphabet (a - 'a')\n  //    4.2. Increment bins(a) by one\n  // 5. Launch another parallel_for to add bins into the original bins array\n  //\n  //\n  // You can access the first letter of a string s(i) by using the subscript operator, s(i,0)\n  // You can access the last letter of a string s(i) by using the subscript operator, s(i, s.extent(1) - 1)\n  //\n  // You can access the element at position i in a Kokkos::View<int> a by using the subscript operator, a(i)\n  // You can access the element at position i in a Kokkos::View<int[N]> a by using the subscript operator, a(i)\n\n  // This line of code will not compile if the TODO has not been completed!\n  static_assert(true, \"You must implement the firstLetterCounts function!\");\n}",
            "using MDRangePolicy = Kokkos::MDRangePolicy<Kokkos::Rank<1>, Kokkos::Schedule<Kokkos::Dynamic>, Kokkos::IndexType<size_t>>;\n  Kokkos::parallel_for(MDRangePolicy({0}, s.size()), KOKKOS_LAMBDA(const size_t i) {\n    const char c = s[i][0];\n    if (c < 'a' || c > 'z') {\n      return;\n    }\n    Kokkos::atomic_increment(&bins[c - 'a']);\n  });\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    size_t bin = s[i][0] - 'a';\n    Kokkos::atomic_fetch_add(&bins[bin], 1);\n  });\n}",
            "using namespace Kokkos;\n\n  // TODO: Create parallel_for lambda that fills bins based on the first letter of each string in s\n  parallel_for(s.extent(0), [&](int i) {\n    bins[tolower(s(i)[0]) - 'a'] += 1;\n  });\n\n  // TODO: Wait for parallel_for to finish\n  Kokkos::fence();\n}",
            "// Your code here\n}",
            "// TODO: Implement\n}",
            "// your code here\n}",
            "// TODO: Replace this with your code\n    std::cout << \"WARNING: firstLetterCounts not implemented\" << std::endl;\n}",
            "// TODO: fill this in\n}",
            "// TODO:\n\n    // you can use the following to get the number of strings in the input\n    size_t n = s.extent(0);\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char first = std::tolower(s(i)[0]);\n    int bin = first - 'a';\n    Kokkos::atomic_increment(&bins(bin));\n  });\n\n  Kokkos::fence();\n}",
            "// Your code goes here.\n\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    if (s(i)[0] >= 'a' && s(i)[0] <= 'z')\n      Kokkos::atomic_increment(&bins[s(i)[0] - 'a']);\n  });\n}",
            "const size_t num_strings = s.extent(0);\n  Kokkos::parallel_for(num_strings, KOKKOS_LAMBDA (const size_t i) {\n    const char letter = s(i, 0);\n    bins(letter - 'a')++;\n  });\n}",
            "// your code here\n}",
            "// TODO: Fill this in\n  //\n  // You can use lambda functions. For example:\n  //\n  // Kokkos::parallel_for(\n  //   Kokkos::RangePolicy<Kokkos::OpenMP>(0, bins.size()),\n  //   KOKKOS_LAMBDA(size_t i) {\n  //     bins(i) = 0;\n  //   });\n  //\n  // Kokkos::parallel_for(\n  //   Kokkos::RangePolicy<Kokkos::OpenMP>(0, bins.size()),\n  //   KOKKOS_LAMBDA(size_t i) {\n  //     char c = 'a' + i;\n  //     bins(i) = 0;\n  //     for (size_t j = 0; j < s.extent(0); ++j) {\n  //       if (s(j)[0] == c) {\n  //         ++bins(i);\n  //       }\n  //     }\n  //   });\n}",
            "/* YOUR CODE GOES HERE */\n}",
            "using ExecutionSpace = typename Kokkos::DefaultHostExecutionSpace;\n  Kokkos::parallel_for(\"firstLetterCounts\",\n      Kokkos::RangePolicy<ExecutionSpace>(0, s.size()), KOKKOS_LAMBDA(size_t i) {\n    int letter = s[i][0] - 'a';\n    if(letter >= 0 && letter < 26) {\n      // We can assume that bins was pre-initialized to zero\n      Kokkos::atomic_increment(&bins(letter));\n    }\n  });\n  Kokkos::fence();\n}",
            "// TODO: Your code here\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, s.extent(0)),\n                         KOKKOS_LAMBDA(const size_t i) {\n                             char firstLetter = tolower(s(i)[0]);\n                             if (firstLetter >= 'a' && firstLetter <= 'z')\n                             {\n                                 bins[firstLetter - 'a'] += 1;\n                             }\n                         });\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int& i) {\n        char letter = s(i, 0);\n        if (letter!= '\\0') {\n            bins(letter - 'a') += 1;\n        }\n    });\n}",
            "using namespace Kokkos;\n\n  // Create a parallel for loop with range of 26 and\n  // execute the following lambda function for each\n  // i.\n  Kokkos::parallel_for(26, KOKKOS_LAMBDA(int i) {\n    // Get the corresponding letter\n    char c = 'a' + i;\n\n    // Count the number of strings that start with `c`\n    // in the input vector s and store in `bins` array.\n    size_t count = 0;\n    for (int j = 0; j < s.extent(0); j++) {\n      count += s(j)[0] == c;\n    }\n    bins(i) = count;\n  });\n}",
            "// TODO: implement the Kokkos kernel\n}",
            "using value_type = Kokkos::View<size_t[26]>::value_type;\n\n    // TODO: use parallel_for to count the number of strings in the input `s` that start with each letter\n    Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(const int i) {\n        // TODO: write your parallel code here\n        // Hint:\n        // 1. s[i] is a pointer to a C-style string\n        // 2. use the following to convert a letter to an index:\n        //      int index = letter - 'a';\n        // 3. use the following to update the bin at index:\n        //      bins(i, index) += 1;\n    });\n    Kokkos::fence();\n}",
            "//\n  // TODO: Compute the bins using Kokkos parallel for.\n  //\n  Kokkos::parallel_for(s.extent(0), [=](size_t i) {\n    size_t idx = s(i)[0] - 'a';\n    Kokkos::atomic_add(&(bins(idx)), 1);\n  });\n  Kokkos::fence();\n  //\n  // TODO: End of your code\n  //\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n                       [=](int i) {\n    // TODO: fill in the body of this loop\n  });\n  // TODO: fill in the code to",
            "/* Your code goes here */\n}",
            "const int n_threads = 256; // Number of threads per block\n    const int n_blocks = 1; // Number of blocks in the grid\n    Kokkos::View<int*> hist(\"Histogram\", 26); // Array to store thread-local counts\n    // Write your parallel code here\n    //...\n}",
            "// TODO: Use parallel_for with a lambda to compute the first letter counts\n  // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0, s.extent(0)), [=] (int idx) {\n  //   char c = s(idx)[0];\n  //   //...\n  // });\n\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n\n  constexpr size_t a = 'a';\n  Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(const int i) {\n    const char c = *s(i);\n    if (c >= 'a' && c <= 'z')\n      Kokkos::atomic_fetch_add(&bins[c-a], 1);\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"first_letter_counts\",\n                       Kokkos::RangePolicy<Kokkos::Serial>(0, s.extent(0)),\n                       KOKKOS_LAMBDA(int i) {\n                         auto first_letter = tolower(s(i, 0));\n                         if (first_letter >= 'a' && first_letter <= 'z')\n                           Kokkos::atomic_increment(&bins(first_letter - 'a'));\n                       });\n}",
            "using ExecutionPolicy = Kokkos::TeamPolicy<Kokkos::Cuda>;\n\n  // TODO: fill in the lambda to compute first letter counts\n  Kokkos::parallel_for(ExecutionPolicy(s.extent(0), Kokkos::AUTO),\n    KOKKOS_LAMBDA(const int i) {\n\n    });\n\n  Kokkos::deep_copy(bins, bins_d);\n}",
            "// TODO: Implement me\n}",
            "const size_t num_strings = s.extent(0);\n\n    // TODO: replace this with a parallel Kokkos kernel that counts the first letter of each string\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0,num_strings),\n        KOKKOS_LAMBDA(const int& i) {\n            size_t letter = tolower(s(i)[0]);\n            bins(letter)++;\n        });\n\n    // TODO: add a Kokkos::fence() statement to make sure that the results are written out\n\n    // TODO: replace this with a parallel Kokkos kernel that counts the first letter of each string\n\n    Kokkos::fence();\n\n    return;\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for(s.extent(0), [=] (size_t i) {\n        char ch = s(i)[0];\n        if (ch >= 'a' && ch <= 'z') {\n            bins(ch - 'a')++;\n        }\n    });\n    Kokkos::fence();\n}",
            "// TODO: your code goes here\n\n}",
            "const size_t n = s.extent(0);\n  Kokkos::parallel_for(\"firstLetterCounts\", n, KOKKOS_LAMBDA(const size_t i) {\n    const char* c = s(i);\n    if (*c!= '\\0') {\n      size_t index = static_cast<size_t>(*c - 'a');\n      Kokkos::atomic_increment(&bins(index));\n    }\n  });\n  Kokkos::fence();\n}",
            "using Policy = Kokkos::RangePolicy<Kokkos::RoundRobin<Kokkos::Rank<2>>>;\n  Kokkos::parallel_for(\n    \"firstLetterCounts\",\n    Policy(0, s.extent(0), 1),\n    KOKKOS_LAMBDA(const int i) {\n      const char letter = s(i)[0];\n      if (letter >= 'a' && letter <= 'z') {\n        Kokkos::atomic_increment(&bins(letter - 'a'));\n      }\n    });\n}",
            "// TODO: implement the Kokkos version of firstLetterCounts\n\n}",
            "}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), [&] (int i) {\n    int index = s[i][0] - 'a';\n    Kokkos::atomic_add(&bins[index], 1);\n  });\n\n  Kokkos::fence();\n\n  return;\n}",
            "/* TODO: Fill in this function */\n\n}",
            "// your code here\n}",
            "// TODO:\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, s.size()),\n      KOKKOS_LAMBDA(const size_t i) {\n        bins[s[i][0] - 'a']++;\n      });\n}",
            "// TODO\n  Kokkos::parallel_for(\"FirstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(const int i) {\n    char c = s(i)[0];\n    if (c >= 'a' && c <= 'z') {\n      Kokkos::atomic_increment(&bins[c - 'a']);\n    }\n  });\n}",
            "Kokkos::parallel_for(\n    \"firstLetterCounts\",\n    Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Serial>>(0, s.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      const char c = s(i)[0];\n      if (c >= 'a' && c <= 'z')\n        Kokkos::atomic_increment<Kokkos::View<size_t*>>(&bins(c - 'a'));\n    }\n  );\n  Kokkos::fence();\n}",
            "// TODO: fill in the function body\n}",
            "// Implement me\n}",
            "// The lambda function is executed on the device (GPU).\n  // This is a C++11 feature.  This is the main Kokkos programming model.\n  // The \"histogram\" function is a parallel Kokkos for_loop.\n  // The bins are accessed in parallel.\n  Kokkos::parallel_for(s.extent(0), [=](int i) {\n    char firstLetter = s(i)[0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n\n  Kokkos::fence();\n\n  // Check the results.\n  // If you remove this line, the compiler will not check that the code is correct.\n  for (int i = 0; i < 26; ++i) {\n    printf(\"[%d] = %d\\n\", i, bins(i));\n  }\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, s.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n                         char c = tolower(s(i)[0]);\n                         if (c >= 'a' && c <= 'z') {\n                           Kokkos::atomic_increment(&bins(c - 'a'));\n                         }\n                       });\n}",
            "using Kokkos::parallel_for;\n  using Kokkos::Experimental::ROI;\n  using Kokkos::RangePolicy;\n\n  // Use a parallel_for to write to each bin.\n  parallel_for(\"Letter Counts\", s.extent(0), ROI([&](size_t i) {\n    size_t letter = s(i)[0];\n    if (letter >= 'a' && letter <= 'z') {\n      bins(letter - 'a')++;\n    }\n  }));\n}",
            "const size_t N = s.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace>(0, N), KOKKOS_LAMBDA(const size_t& i) {\n    auto c = std::tolower(s(i)[0]);\n    if (c >= 'a' && c <= 'z') {\n      bins(c - 'a')++;\n    }\n  });\n  Kokkos::fence();\n}",
            "using namespace Kokkos;\n  using Kokkos::parallel_for;\n  using Kokkos::RangePolicy;\n\n  auto n = s.extent(0);\n  const char* alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n  // Create a Kokkos view for the alphabet.\n  View<const char*> alpha_view(alphabet, 26);\n\n  // Create a Kokkos view for the output.\n  View<size_t[26]> bins_view(bins);\n\n  // Compute the number of strings in s that start with a letter from the alphabet.\n  parallel_for(RangePolicy<Reduce<size_t>>(0, n), KOKKOS_LAMBDA(const int i, size_t& sum) {\n    const char* s_i = s(i);\n    const int a_i = (int) *s_i;\n    if (a_i >= 97 && a_i <= 122) {\n      const int ind = a_i - 97;\n      bins_view[ind] += 1;\n    }\n  });\n\n  // Copy the results back to the host.\n  Kokkos::deep_copy(bins, bins_view);\n}",
            "using namespace Kokkos;\n\n  // Fill the bins array\n  parallel_for(1, [&](int i) {\n    int ibin = 0;\n    for (size_t j = 0; j < s.extent(0); j++) {\n      const char *ptr = s(j);\n      if (*ptr == '\\0')\n        continue;\n      if (*ptr >= 'a' && *ptr <= 'z')\n        ibin = *ptr - 'a';\n      else if (*ptr >= 'A' && *ptr <= 'Z')\n        ibin = *ptr - 'A';\n      Kokkos::atomic_increment(&bins(ibin));\n      ptr++;\n    }\n  });\n}",
            "// Implement the algorithm described in the header comment\n  // You may assume Kokkos has been initialized, and\n  // the input arguments are valid.\n}",
            "// TODO\n\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n  using device_type = Kokkos::device_t<execution_space>;\n\n  using view_type_a = Kokkos::View<const char**, Kokkos::LayoutStride, Kokkos::Device<execution_space, device_type>>;\n  using view_type_b = Kokkos::View<size_t[26], Kokkos::LayoutStride, Kokkos::Device<execution_space, device_type>>;\n\n  // Copy data from host to device\n  auto s_d = view_type_a(s.data(), s.extent(0), s.stride_0());\n  auto bins_d = view_type_b(bins.data(), bins.extent(0), bins.stride_0());\n\n  // Clear the output\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) { bins_d(i) = 0; });\n\n  Kokkos::parallel_for(\n      s.extent(0),\n      KOKKOS_LAMBDA(const int i) {\n        if (s_d(i)[0] >= 'a' && s_d(i)[0] <= 'z') {\n          bins_d(s_d(i)[0] - 'a')++;\n        }\n      });\n\n  // Copy data back to the host\n  Kokkos::deep_copy(bins, bins_d);\n}",
            "// TODO: replace this code with parallel Kokkos code\n    int i;\n    for (i = 0; i < 26; i++) bins(i) = 0;\n    for (i = 0; i < s.extent(0); i++) bins(s(i)[0] - 97)++;\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    char first = s(i)[0];\n    if (first >= 'a' && first <= 'z') {\n      Kokkos::atomic_add(&bins[first - 'a'], 1);\n    }\n  });\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n\n  // TODO: Implement.\n  //\n  // Use parallel_for to iterate over the elements in the input vector.\n  // Use the first letter in the string to index into the output array.\n  // (Hint: s[i] - 'a' will convert a letter into its index.)\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\"Histogram\", s.extent(0), KOKKOS_LAMBDA(int const& i) {\n    char const first = s(i, 0);\n    bins(first - 'a')++;\n  });\n}",
            "// Your code here\n}",
            "// your code goes here\n}",
            "// Fill in this function.\n}",
            "// insert your code here\n\n}",
            "// TODO: Fill in the code here\n}",
            "// TODO: write your parallel kernel here\n\n}",
            "size_t const n = s.extent_int(0);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(size_t i) {\n        bins[tolower(s(i)[0]) - 'a']++;\n    });\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"FirstLetterCounts\", 26, KOKKOS_LAMBDA(const int& i) {\n        // 0 <= i < 26\n        // You may assume that all strings have at least one character, and that characters are in the range 'a' to 'z'.\n        // bins(i) will hold the total number of words that start with the ith letter of the alphabet.\n        bins(i) = 0;\n        // Your code here\n        for (int j = 0; j < s.extent(0); j++)\n        {\n            if (s(j)[0] == 'a'+i)\n            {\n                bins(i)++;\n            }\n        }\n    });\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA (int i) {\n    bins(s[i][0] - 'a') += 1;\n  });\n}",
            "// Create a kernel that is executed once for each element in s\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(const int i) {\n    // Assume the input string is in lower case\n    const char c = tolower(s(i)[0]);\n    // Update the bins array\n    Kokkos::atomic_add(&bins[c-'a'], 1);\n  });\n\n  // Create a kernel that is executed once\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int i) {\n    // Print out the bins array\n    for(int i = 0; i < 26; i++) {\n      printf(\"%c: %zu\\n\", i+'a', bins(i));\n    }\n  });\n}",
            "using namespace Kokkos;\n\n  // your code here\n\n  // Do not modify this. This is the right answer.\n  HostSpace::execution_space::fence();\n  for (size_t i = 0; i < 26; ++i) {\n    printf(\"%zu \", bins[i]);\n  }\n  printf(\"\\n\");\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0, s.extent(0)), KOKKOS_LAMBDA(const int i) {\n    auto letter = tolower(s(i)[0]);\n    // Kokkos guarantees that all threads are in the same warp, so no need to use atomicAdd.\n    bins(letter - 'a') += 1;\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), KOKKOS_LAMBDA(int i) {\n    const char firstLetter = s(i, 0);\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      Kokkos::atomic_add(&bins(firstLetter - 'a'), 1);\n    }\n  });\n}",
            "using view_type = decltype(bins);\n  using value_type = typename view_type::value_type;\n  using array_layout = typename view_type::array_layout;\n  using device_type = typename view_type::device_type;\n  using execution_space = typename view_type::execution_space;\n\n  // Set up a lambda function that will increment a bin value\n  auto increment_bin = KOKKOS_LAMBDA(value_type i) {\n    if(i >= 0 && i < 26) {\n      bins(i) += 1;\n    }\n  };\n\n  // Set up a lambda function that will increment a bin value\n  // based on the first letter of a string\n  auto count_letter = KOKKOS_LAMBDA(const char* str) {\n    if(str[0] >= 'a' && str[0] <= 'z') {\n      increment_bin((size_t)(str[0] - 'a'));\n    } else {\n      increment_bin(25);\n    }\n  };\n\n  // Apply the lambda function to all elements of the input vector\n  Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, s.extent(0)), count_letter);\n\n  // Make sure that the kernel has completed before exiting this function\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "// Your code goes here.\n}",
            "using execution_space = Kokkos::DefaultHostExecutionSpace;\n  using policy_type     = Kokkos::RangePolicy<execution_space>;\n\n  Kokkos::parallel_for(\n      \"FirstLetterCounts\",\n      policy_type(0, s.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        bins[s(i)[0] - 'a']++;\n      });\n\n  // This is just a workaround to get a single device sync point\n  Kokkos::View<int, Kokkos::HostSpace> temp(\"temp\", 1);\n  temp(0) = 0;\n  Kokkos::parallel_for(\n      \"FirstLetterCounts\",\n      policy_type(0, 1),\n      KOKKOS_LAMBDA(const int i) {\n        temp(i) = 0;\n      });\n}",
            "size_t count_size = bins.size();\n    Kokkos::parallel_for(count_size, KOKKOS_LAMBDA(size_t i) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (tolower(s(j)[0]) == 'a' + i) {\n                bins(i) += 1;\n            }\n        }\n    });\n}",
            "Kokkos::parallel_for(s.extent(0), [&] (size_t i) {\n    for (size_t j=0; j<s.extent(1); j++) {\n      // TODO: Fill in this inner loop with the correct code\n    }\n  });\n\n  Kokkos::fence();\n\n  // TODO: Add the following line to the end of the function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, s.extent(0)), KOKKOS_LAMBDA(size_t i) {\n    char ch = s(i)[0];\n    Kokkos::atomic_increment<Kokkos::Cuda>(&bins[ch - 'a']);\n  });\n}",
            "Kokkos::parallel_for(s.extent(0), [&](int i) {\n    bins[s(i)[0] - 'a']++;\n  });\n}",
            "Kokkos::parallel_for(\n        \"First letter count\",\n        Kokkos::RangePolicy<Kokkos::Cuda>(0, s.extent(0)),\n        KOKKOS_LAMBDA(const size_t i) {\n            char first = s(i, 0);\n            if (first >= 'a' && first <= 'z') {\n                bins(first - 'a') += 1;\n            }\n        }\n    );\n}",
            "// Fill in the body of the function\n}",
            "// Define and allocate a Kokkos view to store results.\n  Kokkos::View<size_t[26]> result(\"result\");\n\n  // Fill the view with zeros.\n  Kokkos::deep_copy(result, 0);\n\n  // Define a functor to be called for each vector element in parallel.\n  // The functor has two arguments: the index of the vector element and a\n  // reference to the first letter of the string. The lambda captures the\n  // `result` view.\n  auto functor = KOKKOS_LAMBDA(const int i, const char firstLetter) {\n    if ('a' <= firstLetter && firstLetter <= 'z') {\n      // Update the bin count for the letter.\n      result(firstLetter - 'a') += 1;\n    }\n  };\n\n  // Call functor for each string in parallel.\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n    functor);\n\n  // Copy the results back to host memory.\n  Kokkos::deep_copy(bins, result);\n}",
            "// Insert your code here\n\n}",
            "using namespace Kokkos;\n\n  // TODO: Fill in the body of the functor\n\n  // You can use a Kokkos::parallel_for with a range policy to loop over the input\n  // vector of strings.\n\n  // You can use a Kokkos::parallel_for with a range policy to loop over the\n  // letters in the alphabet.\n\n  // You can use a Kokkos::parallel_for with a team policy to loop over the\n  // letters in the alphabet.\n\n}",
            "// Put your code here\n}",
            "// Your code here\n\n}",
            "using T = Kokkos::DefaultHostExecutionSpace;\n    using FunctorType = Kokkos::RangePolicy<T, size_t>;\n\n    FunctorType f(0, s.size());\n    Kokkos::parallel_for(f, KOKKOS_LAMBDA(const size_t &i) {\n        auto c = static_cast<size_t>(tolower(s(i)[0]));\n        Kokkos::atomic_increment(&bins(c));\n    });\n}",
            "// TODO: Your code here\n\n  // TODO: Copy back to host\n\n}",
            "const size_t n = s.size();\n\n  Kokkos::View<size_t*> counts(\"counts\", 26);\n\n  // Kokkos::parallel_for(n, [=](int i) {\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,n), [=](int i) {\n    // for (int i = 0; i < n; i++) {\n      bins(s(i)[0] - 'a') += 1;\n    // }\n  });\n\n  Kokkos::deep_copy(bins, counts);\n}\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<int, 26> &bins) {\n  Kokkos::View<std::string*> v_s(\"v_s\", s.size());\n  Kokkos::View<size_t*> v_bins(\"v_bins\", 26);\n\n  // Kokkos::parallel_for(s.size(), [=](int i) {\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,s.size()), [=](int i) {\n    // for (int i = 0; i < n; i++) {\n      v_s(i) = s[i];\n    // }\n  });\n\n  firstLetterCounts(v_s, v_bins);\n\n  // Kokkos::parallel_for(26, [=](int i) {\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,26), [=](int i) {\n    // for (int i = 0; i < 26; i++) {\n      bins[i] = v_bins(i);\n    // }\n  });\n}\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<int, 26> &bins) {\n  std::vector<std::string> vs;\n  vs.resize(s.size());\n\n  Kokkos::View<std::string*> v_s(\"v_s\", vs.size());\n  Kokkos::View<size_t*> v_bins(\"v_bins\", 26);\n\n  // Kokkos::parallel_for(s.size(), [=](int i) {\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,s.size()), [=](int i) {\n    // for",
            "// TODO: Your code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0, 26),\n                         KOKKOS_LAMBDA(int i) {\n                             for (int j = 0; j < s.extent(0); j++) {\n                                 if (s(j)[0] == 'a' + i) bins[i]++;\n                             }\n                         });\n}",
            "// TODO: Implement\n  // Hint: you can use a Kokkos::parallel_for to do the counting.\n  // The \"parallel_for\" should cover all the strings in the vector.\n}",
            "// TODO\n}",
            "using ExecutionPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n\n    const size_t numStrings = s.extent_int(0);\n    Kokkos::parallel_for(ExecutionPolicy(0, numStrings),\n        KOKKOS_LAMBDA(const size_t &i) {\n            const char &c = s(i)[0];\n            const size_t bin = c - 'a';\n            Kokkos::atomic_fetch_add(&bins(bin), 1);\n        });\n\n    // This is equivalent to using `auto h_bins = Kokkos::create_mirror_view(bins);`\n    // and `Kokkos::deep_copy(h_bins, bins);` but avoids an extra copy.\n    // Instead, we copy the values from device to host while we are still on device.\n    Kokkos::deep_copy(bins.data(), bins.data());\n}",
            "using std::string_view;\n  using Kokkos::parallel_for;\n  using Kokkos::RangePolicy;\n\n  auto firstLetter = [](string_view const& str) {\n    return str[0];\n  };\n  auto isLower = [](char const c) {\n    return 'a' <= c && c <= 'z';\n  };\n  auto toIndex = [](char const c) {\n    return c - 'a';\n  };\n\n  // Parallel implementation\n  parallel_for(\n      \"first_letter_counts_parallel\",\n      RangePolicy(0, s.extent(0)),\n      KOKKOS_LAMBDA(size_t i) {\n        auto const c = firstLetter(s(i));\n        if (isLower(c)) {\n          Kokkos::atomic_increment<size_t>(&bins(toIndex(c)));\n        }\n      });\n\n  // Sequential implementation\n  for (size_t i = 0; i < s.extent(0); ++i) {\n    auto const c = firstLetter(s(i));\n    if (isLower(c)) {\n      bins(toIndex(c))++;\n    }\n  }\n}",
            "// Your code here\n}",
            "// Your code goes here\n  size_t n = s.extent(0);\n  size_t m = 26;\n  auto first_letter = Kokkos::View<int*>(\"first_letter\", n);\n  Kokkos::parallel_for(\"first_letter\", n, KOKKOS_LAMBDA(int i) {\n    char first_letter_char = s(i)[0];\n    if(first_letter_char >= 'a' && first_letter_char <= 'z')\n      first_letter(i) = first_letter_char - 'a';\n    else\n      first_letter(i) = 25;\n  });\n  Kokkos::parallel_for(\"count\", n, KOKKOS_LAMBDA(int i) {\n    int first_letter_index = first_letter(i);\n    if(first_letter_index < 26)\n      Kokkos::atomic_add(&bins(first_letter_index), 1);\n  });\n\n  // Kokkos::parallel_for(\"count\", n, KOKKOS_LAMBDA(int i) {\n  //   int first_letter_index = first_letter(i);\n  //   if(first_letter_index < 26)\n  //     Kokkos::atomic_add(&bins(first_letter_index), 1);\n  // });\n\n  // Kokkos::parallel_for(\"count\", n, KOKKOS_LAMBDA(int i) {\n  //   int first_letter_index = first_letter(i);\n  //   if(first_letter_index < 26)\n  //     bins(first_letter_index) += 1;\n  // });\n}",
            "constexpr int nchars = 26;\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::parallel_for(nchars, KOKKOS_LAMBDA(const int &i) {\n    char c = 'a' + i;\n    bins_host[i] = 0;\n    for (auto j = 0; j < s.extent(0); j++) {\n      if (s(j, 0) == c) bins_host[i]++;\n    }\n  });\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO: replace this with the actual code to count the first letter of the strings\n}",
            "Kokkos::View<char*> letter(\"letter\");\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA (int i) {\n    letter(0) = s(i)[0];\n    Kokkos::atomic_add(&bins(letter(0) - 'a'), 1);\n  });\n  Kokkos::fence();\n}",
            "using functor_type = KOKKOS_LAMBDA(int i, Kokkos::View<size_t[26]> bins) {\n    const char first = s(i)[0];\n    if (first >= 'a' && first <= 'z') {\n      bins[first - 'a']++;\n    }\n  };\n  Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), functor_type{bins});\n}",
            "using Policy = Kokkos::RangePolicy<Kokkos::Cuda>;\n    Kokkos::parallel_for(\n        \"first_letter_counts\",\n        Policy(0, s.size()),\n        KOKKOS_LAMBDA(int i) {\n            auto firstLetter = s(i)[0];\n            if (firstLetter >= 'a' && firstLetter <= 'z') {\n                Kokkos::atomic_increment(&bins(firstLetter - 'a'));\n            }\n        }\n    );\n    Kokkos::fence();\n}",
            "// Create a parallel_for loop over the size of the input.\n  Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i) {\n    // The string we are working on.\n    char const* str = s(i);\n    // The first letter of the string.\n    char const first = str[0];\n    // Update the correct bin in the bins array.\n    bins(first - 'a') += 1;\n  });\n}",
            "// TODO: Replace this with your code\n  // Make sure that you do not modify the input parameter `s`\n  Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(size_t i) {\n    const char letter = s(i, 0);\n    if(letter >= 'a' && letter <= 'z')\n      Kokkos::atomic_increment(&bins[(size_t)(letter - 'a')]);\n  });\n  Kokkos::fence();\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n  using size_type = Kokkos::View<size_t[26]>::size_type;\n  using range_type = Kokkos::Range<size_type>;\n\n  Kokkos::parallel_for(range_type(0, s.extent(0)), KOKKOS_LAMBDA(const size_type i) {\n    bins[s(i, 0) - 'a']++;\n  });\n}",
            "const auto n = s.extent(0);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(const size_t i) {\n    const auto c = s(i)[0];\n    if (c >= 'a' && c <= 'z') {\n      Kokkos::atomic_increment(&bins[c - 'a']);\n    }\n  });\n}",
            "// TODO: Implement\n}",
            "size_t n = s.extent(0);\n  Kokkos::parallel_for(\"FirstLetterCounts\", n, KOKKOS_LAMBDA(size_t i) {\n    char firstLetter = std::tolower(s(i)[0]);\n    if (firstLetter < 'a' || firstLetter > 'z') {\n      firstLetter = 'z' + 1;\n    }\n    auto &bin = bins(firstLetter - 'a');\n    Kokkos::atomic_increment(&bin);\n  });\n}",
            "const size_t n = s.extent(0);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(size_t i) {\n        // Your code here!\n    });\n}",
            "// Your code here\n\n}",
            "using std::isalpha;\n  using std::tolower;\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n  [=] (const int& i) {\n    char c = tolower(s(i)[0]);\n    if (isalpha(c)) {\n      Kokkos::atomic_increment(&bins(c - 'a'));\n    }\n  });\n\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "// Replace this statement with your implementation\n  Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(int i) {\n    const char c = s(i)[0];\n    if (c >= 'a' && c <= 'z') {\n      Kokkos::atomic_increment<Kokkos::View<size_t[26]> >(&bins[c - 'a']);\n    }\n  });\n  Kokkos::fence();\n}",
            "// TODO: Implement\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n  Kokkos::parallel_for(\"firstLetterCounts\", ExecPolicy(0, s.extent(0)), KOKKOS_LAMBDA(int i) {\n    // First get the first letter.\n    char firstLetter = s(i)[0];\n    if (firstLetter == '\\0') {\n      // String is empty.\n      return;\n    }\n\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      // Use the letter to index into the output array.\n      bins(firstLetter - 'a') += 1;\n    }\n  });\n}",
            "using T = Kokkos::DefaultExecutionSpace;\n  Kokkos::parallel_for(Kokkos::RangePolicy<T>(0, s.extent(0)), [s, &bins] (size_t i) {\n    char c = s(i)[0];\n    if ('a' <= c && c <= 'z') {\n      Kokkos::atomic_increment(&bins(c - 'a'));\n    }\n  });\n}",
            "using lam_t = Kokkos::parallel_for_t<class fLC>;\n  const lam_t lam{\n    [&](const int i) {\n      if (s(i)[0]!= '\\0') {\n        ++bins[s(i)[0] - 'a'];\n      }\n    }\n  };\n  Kokkos::RangePolicy<lam_t> policy(0, s.extent(0));\n  Kokkos::parallel_for(\"firstLetterCounts\", policy, lam);\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA (const int i) {\n    // Your code here\n  });\n  Kokkos::fence();\n}",
            "/*\n   * This is your task: implement the for loop below.\n   * We've given you the following hints:\n   *\n   * - Use a parallel_for with a schedule of static.\n   * - For each string, use Kokkos::parallel_for to compute each char's offset from 'a'.\n   * - Use Kokkos::parallel_reduce to aggregate the counts for each letter.\n   * - Each string's offset from 'a' should be independent of all other strings.\n   * - To convert a char to an offset, you can use the formula:\n   *   (letter - 'a')\n   *\n   * The rest of this comment block has been filled in to help you get started.\n   * Delete these comments once you've written your solution.\n   */\n\n  // Set all bins to zero\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, bins.size()),\n                       KOKKOS_LAMBDA(const size_t i) {\n                         bins(i) = 0;\n                       });\n\n  // Loop through all strings, compute offsets and add to bins.\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, s.size()),\n                       KOKKOS_LAMBDA(const size_t i) {\n                         auto str = s(i);\n                         // Convert string to an array of chars\n                         Kokkos::View<char[26]> chars;\n                         for (size_t j = 0; j < strlen(str); ++j) {\n                           chars[j] = str[j];\n                         }\n                         // Compute offset for each letter in the array and add to bins\n                         Kokkos::parallel_reduce(\n                             Kokkos::RangePolicy<>(0, 26),\n                             KOKKOS_LAMBDA(const size_t j, size_t &total) {\n                               total += (chars[j] - 'a');\n                             },\n                             bins[0]);\n                       });\n}",
            "/*\n       YOUR CODE HERE\n    */\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, s.extent(0)),\n                       KOKKOS_LAMBDA(int i) {\n                         switch (s(i)[0]) {\n                           case 'a': ++bins[0]; break;\n                           case 'b': ++bins[1]; break;\n                           case 'c': ++bins[2]; break;\n                           case 'd': ++bins[3]; break;\n                           case 'e': ++bins[4]; break;\n                           case 'f': ++bins[5]; break;\n                           case 'g': ++bins[6]; break;\n                           case 'h': ++bins[7]; break;\n                           case 'i': ++bins[8]; break;\n                           case 'j': ++bins[9]; break;\n                           case 'k': ++bins[10]; break;\n                           case 'l': ++bins[11]; break;\n                           case'm': ++bins[12]; break;\n                           case 'n': ++bins[13]; break;\n                           case 'o': ++bins[14]; break;\n                           case 'p': ++bins[15]; break;\n                           case 'q': ++bins[16]; break;\n                           case 'r': ++bins[17]; break;\n                           case's': ++bins[18]; break;\n                           case 't': ++bins[19]; break;\n                           case 'u': ++bins[20]; break;\n                           case 'v': ++bins[21]; break;\n                           case 'w': ++bins[22]; break;\n                           case 'x': ++bins[23]; break;\n                           case 'y': ++bins[24]; break;\n                           case 'z': ++bins[25]; break;\n                         }\n                       });\n}",
            "// Get the number of items in the input vector s\n  const auto n = s.extent(0);\n\n  // Loop over all the strings in s to compute the first letter counts\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int& i) {\n    // Use std::toupper to transform the first letter to upper case\n    char ch = std::toupper(s(i)[0]);\n    if (ch >= 'A' && ch <= 'Z') {\n      // The first letter of s(i) is a letter from A to Z. Increment the\n      // counter in bins for this letter.\n      Kokkos::atomic_increment(&bins[ch-'A']);\n    }\n  });\n\n  // Make sure that the above parallel code is finished before we read from\n  // bins\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "// TODO\n}",
            "//...\n}",
            "Kokkos::parallel_for( \"First letter counts\", s.extent(0), KOKKOS_LAMBDA(int i) {\n    char c = tolower(s(i)[0]);\n    bins(c - 'a')++;\n  });\n  Kokkos::fence();\n}",
            "// TODO: use Kokkos parallel for loop to count the number of strings in the input that begin with each\n  // letter of the alphabet.\n\n}",
            "// your code here\n\n}",
            "Kokkos::parallel_for( \"First letter counts\", s.extent(0), KOKKOS_LAMBDA( size_t const i ) {\n    if (s[i] == nullptr) return;\n    Kokkos::atomic_increment(&bins[s[i][0] - 'a']);\n  });\n  Kokkos::fence();\n}",
            "// Your code goes here\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, s.extent(0)),\n    KOKKOS_LAMBDA(size_t i) {\n      // Get the first character of the string in position `i`.\n      const char first = s(i)[0];\n      // Convert to lower case using `tolower` from `ctype.h`.\n      const char lower = std::tolower(first);\n      // Add 1 to the value in the `bins` array corresponding to `lower`.\n      // Use atomic add since multiple strings may have the same first letter.\n      Kokkos::atomic_add(&(bins(lower)), 1);\n  });\n}",
            "// TODO: Replace this with a parallel Kokkos loop\n    for (int i = 0; i < s.extent(0); i++) {\n        int bin_idx = s(i)[0] - 'a';\n        Kokkos::atomic_increment<size_t>(&(bins(bin_idx)));\n    }\n\n}",
            "//...\n\n}",
            "size_t n = s.extent(0);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(const size_t& i) {\n    size_t letter = s(i)[0] - 'a';\n    Kokkos::atomic_increment<decltype(bins)>(&bins(letter));\n  });\n}",
            "// TODO\n\n}",
            "/* Compute the first letter counts in parallel. */\n}",
            "Kokkos::parallel_for(\n    \"first_letter_counts\",\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, s.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      char c = s(i)[0];\n      if (c >= 'a' && c <= 'z') {\n        Kokkos::atomic_increment(&bins[c - 'a']);\n      }\n    });\n\n  Kokkos::fence();\n}",
            "//\n  // Insert code here that does the work.\n  //\n}",
            "// TODO: implement\n}",
            "/*\n     Use Kokkos parallel_for to count the number of strings that start with each letter.\n     In the parallel_for, the lambda should compute the letter of the current index and\n     then increment the bin corresponding to that letter.\n     Assume s is not empty.\n  */\n}",
            "Kokkos::parallel_for(\"FirstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(size_t idx) {\n    // Use the ASCII table to find the index of the first character of the string s[idx].\n    // Use that index to find the first element of bins that corresponds to that letter.\n    // Increment that element by 1.\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"firstLetterCounts\",\n    Kokkos::RangePolicy<>(0, s.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      // Your code here\n    }\n  );\n}",
            "auto device = Kokkos::DefaultExecutionSpace::execution_space;\n\n    const int N = s.extent(0);\n    const int NALPHA = 26;\n\n    // Create a parallel for over N, iterate over all the strings in the vector s\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n        // Each thread gets a string, find the first letter and update bins\n        // Note: if s[i] is not null-terminated, this will read past the end of the string!\n        char c = s(i)[0];\n        if (c >= 'a' && c <= 'z')\n            Kokkos::atomic_increment<Kokkos::MemoryOrder::relaxed>(&bins(c - 'a'));\n    });\n\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", s.extent(0), KOKKOS_LAMBDA(const int i) {\n        bins[s[i][0] - 'a']++;\n    });\n}",
            "// Your code here.\n}",
            "Kokkos::parallel_for(\n    s.extent(0),\n    KOKKOS_LAMBDA(const int i) {\n      size_t hash = s(i)[0];\n      if(hash >= 'a' && hash <= 'z') bins(hash - 'a') += 1;\n      else if(hash >= 'A' && hash <= 'Z') bins(hash - 'A') += 1;\n    }\n  );\n}",
            "size_t n = s.extent(0);\n  Kokkos::parallel_for(\"first letter count\", 26, KOKKOS_LAMBDA(const size_t i) {\n    bins(i) = 0;\n  });\n  Kokkos::parallel_for(\"first letter count\", n, KOKKOS_LAMBDA(const size_t i) {\n    const char c = s(i)[0];\n    if (c >= 'a' && c <= 'z') {\n      Kokkos::atomic_fetch_add(&bins(c - 'a'), 1);\n    }\n  });\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "}",
            "// Your code here.\n}",
            "for(auto& s_elem : s){\n        //TODO: Implement the body of the loop\n    }\n}",
            "// TODO\n}",
            "// TODO\n    for (int i=0; i<omp_get_max_threads(); i++)\n        for (int j=0; j<26; j++)\n            bins[j] = 0;\n\n    #pragma omp parallel for\n    for (int i=0; i<s.size(); i++) {\n        int char_index = (int) s[i][0] - 97;\n        #pragma omp atomic\n        bins[char_index]++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++)\n    {\n        if (s[i].length() > 0)\n        {\n            char first = s[i].at(0);\n            if (first > 96 && first < 123)\n            {\n                int index = first - 97;\n                bins[index]++;\n            }\n        }\n    }\n}",
            "// #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++){\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// your code goes here\n}",
            "// TODO: Fill bins array with 0s\n  // Hint: you can use bins.fill()\n\n  // TODO: Use OpenMP parallel for loop to fill in the bins array\n}",
            "#pragma omp parallel for\n    for(auto& word : s) {\n        // TODO\n    }\n}",
            "// Use a parallel for loop to iterate over the vector `s`\n    #pragma omp parallel for\n    for(int i = 0; i < s.size(); i++) {\n\n        // Compute the index into the output array where the current element is stored.\n        // Assume that all characters are in lower case.\n        int bin_num = (int)(s[i][0] - 'a');\n\n        #pragma omp atomic\n        bins[bin_num]++;\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    for (int j = 0; j < s.size(); j++) {\n      if (s[j][0] == 'a' + i) bins[i]++;\n    }\n  }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        int firstChar = s[i][0];\n        firstChar -= 'a';\n        bins[firstChar]++;\n    }\n}",
            "}",
            "// Your code here\n}",
            "int size = s.size();\n    #pragma omp parallel for\n    for (int i = 0; i < size; i++) {\n        int c = s[i][0] - 'a';\n        #pragma omp atomic\n        bins[c]++;\n    }\n}",
            "for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: Implement\n}",
            "for (size_t i = 0; i < 26; i++)\n        bins[i] = 0;\n\n    size_t n = s.size();\n\n    std::vector<char> alphabet {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n                               'm', 'n', 'o', 'p', 'q', 'r','s', 't', 'u', 'v', 'w', 'x',\n                                'y', 'z'};\n\n    // TODO: Fill in the code\n\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i)\n    ++bins[s[i][0] - 'a'];\n}",
            "// #pragma omp parallel for\n    for(int i = 0; i < s.size(); ++i)\n    {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    for (auto const &ss : s) {\n        char first = std::tolower(ss.at(0));\n        if (first >= 'a' && first <= 'z') {\n            ++bins[first - 'a'];\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    #pragma omp atomic\n    bins[s[i][0] - 'a']++;\n  }\n\n}",
            "// TODO\n\n}",
            "// Replace this comment with your code\n}",
            "// Fill this in\n}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < s.size(); ++i){\n    // Fill the bins array here\n  }\n}",
            "for (auto const& str : s) {\n        // TODO\n    }\n}",
            "size_t i, j, num_threads, size;\n    char c;\n\n    size = s.size();\n    num_threads = omp_get_max_threads();\n\n    #pragma omp parallel for private(i, c) shared(s)\n    for (i = 0; i < size; i++) {\n        c = s[i][0];\n        if (c >= 'a' && c <= 'z')\n            bins[c - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++)\n    {\n        char c = s[i][0];\n        int x = c - 'a';\n\n        #pragma omp critical\n        {\n            bins[x]++;\n        }\n    }\n\n}",
            "// write your solution here\n}",
            "}",
            "#pragma omp parallel for\n    for(size_t i=0; i<s.size(); i++){\n        char c = s[i].front();\n        if(c >= 'a' && c <= 'z'){\n            int bin = c - 'a';\n            bins[bin]++;\n        }\n    }\n\n    for(auto bin : bins){\n        std::cout << bin << \" \";\n    }\n    std::cout << std::endl;\n}",
            "// *****************************\n    // YOUR CODE GOES HERE!\n    // *****************************\n    #pragma omp parallel for\n    for(size_t i = 0; i < 26; ++i)\n    {\n        bins[i] = 0;\n        for(size_t j = 0; j < s.size(); ++j)\n        {\n            if(s[j][0] == 'a' + i)\n            {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        auto word = s[i];\n        bins[word[0] - 'a']++;\n    }\n}",
            "// TODO\n}",
            "// TODO\n    const char start = 'a';\n    const int bins_size = bins.size();\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> local_bins{};\n\n        #pragma omp for\n        for(int i = 0; i < s.size(); i++) {\n            char first = s[i][0];\n            local_bins[first - start]++;\n        }\n\n        #pragma omp critical\n        {\n            for (int i = 0; i < bins_size; i++) {\n                bins[i] += local_bins[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tint count = 0;\n\t\tfor (size_t j = 0; j < s.at(i).size(); j++) {\n\t\t\tif (s.at(i).at(j) == 'a')\n\t\t\t\tcount++;\n\t\t}\n\t\tbins.at(count)++;\n\t}\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        auto const& str = s[i];\n        if (str.size() == 0) {\n            continue;\n        }\n        int index = str[0] - 'a';\n        if (index < 0 || index > 25) {\n            // ignore\n        } else {\n            bins[index]++;\n        }\n    }\n}",
            "std::array<size_t, 26> counts;\n\tcounts.fill(0);\n#pragma omp parallel for\n\tfor(size_t i = 0; i < s.size(); i++) {\n\t\t// TODO: update counts array with letter count from s[i]\n\t}\n\n#pragma omp parallel for\n\tfor(size_t i = 0; i < counts.size(); i++) {\n\t\t// TODO: update bins array with the letter count from counts\n\t}\n}",
            "//std::array<size_t, 26> bins;\n    // fill the bins array with 0's\n    //bins = std::array<size_t, 26>();\n\n    #pragma omp parallel for\n    for (size_t i=0; i<s.size(); i++) {\n\n        char a = s[i][0];\n        a = std::tolower(a);\n\n        size_t index = a - 'a';\n        bins[index]++;\n\n    }\n\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < s.size(); ++i){\n      for(size_t j = 0; j < s[i].length(); ++j){\n        if(s[i][j] >= 'a' && s[i][j] <= 'z') {\n          #pragma omp atomic\n          bins[s[i][j] - 'a']++;\n        }\n      }\n    }\n}",
            "//#pragma omp parallel for // Uncomment this line to solve the problem\n  for (int i = 0; i < s.size(); i++) {\n    int letter = s[i][0] - 97;\n    bins[letter] += 1;\n  }\n}",
            "// TODO: Implement this function\n  // Start by making sure all bins are initialized to 0\n  for (auto& bin : bins)\n    bin = 0;\n  \n  // Your code here\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// YOUR CODE HERE\n\n}",
            "// TODO: Add code here\n\n\n}",
            "bins.fill(0);\n  // TODO\n}",
            "}",
            "// TODO: Fill in this method\n\n  // parallel for loop\n  #pragma omp parallel for\n  for (int i=0; i<s.size(); i++) {\n    // if-else statement\n    if (s[i][0] == 'a') {\n      bins[0] = bins[0] + 1;\n    } else if (s[i][0] == 'b') {\n      bins[1] = bins[1] + 1;\n    } else if (s[i][0] == 'c') {\n      bins[2] = bins[2] + 1;\n    } else if (s[i][0] == 'd') {\n      bins[3] = bins[3] + 1;\n    } else if (s[i][0] == 'e') {\n      bins[4] = bins[4] + 1;\n    } else if (s[i][0] == 'f') {\n      bins[5] = bins[5] + 1;\n    } else if (s[i][0] == 'g') {\n      bins[6] = bins[6] + 1;\n    } else if (s[i][0] == 'h') {\n      bins[7] = bins[7] + 1;\n    } else if (s[i][0] == 'i') {\n      bins[8] = bins[8] + 1;\n    } else if (s[i][0] == 'j') {\n      bins[9] = bins[9] + 1;\n    } else if (s[i][0] == 'k') {\n      bins[10] = bins[10] + 1;\n    } else if (s[i][0] == 'l') {\n      bins[11] = bins[11] + 1;\n    } else if (s[i][0] =='m') {\n      bins[12] = bins[12] + 1;\n    } else if (s[i][0] == 'n') {\n      bins[13] = bins[13] + 1;\n    } else if (s[i][0] == 'o') {\n      bins[14] = bins[14] + 1;\n    } else if (s[i][0] == 'p') {\n      bins[15] = bins[15] + 1;\n    } else if (s[i][0] == 'q') {\n      bins[16] = bins[16] + 1;\n    } else if (s[i][0] == 'r') {\n      bins[17] = bins[17] + 1;\n    } else if (s[i][0] =='s') {\n      bins[18] = bins[18] + 1;\n    } else if (s[i][0] == 't') {\n      bins[19] = bins[19] + 1;\n    } else if (s[i][0] == 'u') {\n      bins[20] = bins[20] + 1;\n    } else if (s[i][0] == 'v') {\n      bins[21] = bins[21] + 1;\n    } else if (s[i][0] == 'w') {\n      bins[22] = bins[22] + 1;\n    } else if (s[i][0] == 'x') {\n      bins[23] = bins[23] + 1;\n    } else if (s[i][0] == 'y') {\n      bins[24] = bins[24] + 1;\n    } else if (s[i][0] == 'z') {\n      bins[25] = bins[25] + 1;\n    }\n  }\n}",
            "// TODO: replace me\n}",
            "// Your code here\n  #pragma omp parallel for\n  for (int i = 0; i < 26; i++)\n  {\n      for (int j = 0; j < s.size(); j++)\n      {\n        if (s[j].length()!= 0)\n        {\n          if (s[j].at(0) == i + 97)\n          {\n            bins[i]++;\n          }\n        }\n      }\n  }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        char letter = s[i].at(0);\n        bins.at(letter - 'a')++;\n    }\n}",
            "// TODO: Your code here\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "/* Your solution goes here */\n\n}",
            "for (size_t i=0; i<s.size(); ++i) {\n    #pragma omp parallel\n    {\n      if (s[i][0] >= 'a' && s[i][0] <= 'z')\n        bins[s[i][0] - 'a'] += 1;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < s.size(); i++) {\n    auto it = s[i].begin();\n    bins[std::tolower(*it) - 'a']++;\n  }\n}",
            "#pragma omp parallel for default(none) shared(s, bins)\n    for (size_t i = 0; i < s.size(); i++) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            int ind = c - 'a';\n            #pragma omp atomic update\n            bins[ind]++;\n        }\n    }\n}",
            "bins = { 0 };\n    size_t n = s.size();\n    for (size_t i = 0; i < n; i++) {\n        char c = s[i].at(0);\n        bins[c - 'a']++;\n    }\n}",
            "// TODO: replace this with your code\n\n}",
            "// Add your code here\n}",
            "for (size_t i=0; i<s.size(); ++i) {\n    std::string firstLetter = s[i].substr(0, 1);\n    std::cout << firstLetter << std::endl;\n    size_t firstLetterIndex = firstLetter[0] - 97;\n    bins[firstLetterIndex]++;\n  }\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    char letter = s[i][0];\n    if (letter >= 'a' && letter <= 'z') {\n      bins[letter - 'a'] += 1;\n    }\n  }\n\n}",
            "// Your code here\n\n  // You can use this as a starting point\n  // bins.fill(0);\n}",
            "// #pragma omp parallel for\n  for (auto i = 0; i < s.size(); ++i) {\n      ++bins[s[i][0] - 'a'];\n  }\n}",
            "#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); i++) {\n        auto c = s[i][0] - 'a';\n        bins[c]++;\n    }\n}",
            "}",
            "// TODO: implement\n\n  for (auto i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < s.size(); ++i) {\n    if (s[i].length() == 0)\n      continue;\n    char x = s[i][0];\n    if (x >= 'a' && x <= 'z')\n      bins[x-'a']++;\n  }\n\n}",
            "// Use omp_get_thread_num() function to compute the number of threads available\n    int num_threads = omp_get_num_threads();\n    // Compute the number of strings for each thread\n    int num_per_thread = s.size() / num_threads;\n    int offset = 0;\n    if (omp_get_thread_num() == 0) {\n        // First thread gets the extra strings\n        num_per_thread += s.size() % num_threads;\n    }\n    else {\n        // Other threads skip ahead\n        offset = omp_get_thread_num() * num_per_thread;\n    }\n\n    // Compute histogram of the first letter\n    for (int i = 0; i < num_per_thread; i++) {\n        // Get first letter of current string\n        std::string current_string = s[i + offset];\n        char first_letter = current_string[0];\n        // Compute the position of the first letter in the alphabet\n        int pos = (int) first_letter - 97;\n        // Increment the histogram\n        bins[pos]++;\n    }\n}",
            "#pragma omp parallel for\n  for (auto& bin : bins) {\n    bin = 0;\n  }\n  \n  #pragma omp parallel for\n  for (auto const& str : s) {\n    ++bins[str.front() - 'a'];\n  }\n}",
            "// YOUR CODE HERE\n\n}",
            "// TODO\n\n}",
            "// TODO: Your code here!\n}",
            "#pragma omp parallel for\n  for (int i=0; i < s.size(); i++) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z')\n      bins[c - 'a']++;\n  }\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n\n        char c = s[i].at(0);\n        int index = c - 'a';\n\n        #pragma omp critical\n        {\n            bins[index]++;\n        }\n    }\n\n}",
            "}",
            "bins.fill(0);\n\n  #pragma omp parallel\n  {\n    #pragma omp for nowait\n    for(size_t i=0; i<s.size(); i++) {\n      if(s[i][0] >= 'a' && s[i][0] <= 'z') {\n        bins[s[i][0]-'a']++;\n      }\n    }\n  }\n}",
            "// your code here\n}",
            "// YOUR CODE HERE\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < s.size(); ++i) {\n        bins[s[i].at(0) - 97]++;\n    }\n\n    return;\n}",
            "omp_set_num_threads(10);\n    #pragma omp parallel for\n    for (size_t i=0; i<s.size(); ++i) {\n        char letter = s[i][0];\n        if (letter >= 'a' && letter <= 'z') {\n            bins[letter - 'a'] += 1;\n        }\n    }\n}",
            "// TODO: Implement me!\n    #pragma omp parallel for\n    for (int i=0; i < s.size(); i++) {\n        char c = s[i][0];\n        bins[c - 'a']++;\n    }\n}",
            "// your code here\n}",
            "// TODO: Implement this function.\n}",
            "const size_t N = s.size();\n#pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n        printf(\"Hello, I'm thread %d of %d\\n\", id, nthreads);\n#pragma omp for\n        for (int i = 0; i < N; ++i) {\n            std::string const& st = s[i];\n            bins[st[0] - 'a'] += 1;\n        }\n    }\n}",
            "// TODO\n}",
            "const size_t nthreads = omp_get_max_threads();\n    const size_t nstrings = s.size();\n    const size_t N = nthreads * nstrings;\n\n    std::vector<std::string> temp(N);\n    std::array<size_t, 26> counts(0);\n\n    // TODO\n\n    bins = counts;\n}",
            "int n_threads = omp_get_num_threads();\n  int thread_id = omp_get_thread_num();\n\n  std::cout << \"Thread \" << thread_id << \" of \" << n_threads << std::endl;\n\n  for (size_t i = 0; i < s.size(); i++) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      bins[s[i][0]-'a'] += 1;\n    }\n  }\n}",
            "size_t n = s.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        size_t idx = s[i].at(0) - 'a';\n        bins[idx] += 1;\n    }\n}",
            "}",
            "int i, tid;\n    // TODO: Fill in this function\n\n    #pragma omp parallel shared(s, bins) private(i, tid)\n    {\n        tid = omp_get_thread_num();\n        for (i = 0; i < 26; i++)\n        {\n            #pragma omp for schedule(dynamic)\n            for (auto it = s.begin(); it < s.end(); it++)\n            {\n                if (it->at(0) == i + 'a')\n                    bins[i]++;\n            }\n        }\n    }\n\n}",
            "int nthreads = omp_get_max_threads();\n    std::cout << \"Threads: \" << nthreads << std::endl;\n\n    // TODO: implement me!\n    // Hint:\n    // - Use OpenMP for parallelization\n    // - Consider using std::map<char, size_t> instead of std::array\n    // - Use std::transform to apply a function to each element in a container\n    // - Use std::accumulate to sum the elements in a container\n    // - You can use the function std::toupper to convert to upper case\n\n    for (auto it = s.begin(); it!= s.end(); it++) {\n        bins[*it]++;\n    }\n\n}",
            "// TODO: complete this function\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < s.size(); i++) {\n      char c = s[i][0];\n      if (islower(c)) {\n         bins[c - 'a']++;\n      }\n   }\n}",
            "//...\n\n}",
            "// your code here!\n}",
            "size_t n = s.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        if (s[i].length()!= 0) {\n            char letter = s[i][0];\n            if (letter >= 'a' && letter <= 'z') {\n                bins[letter - 'a']++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "// ******** Your code here ********\n  for(int i = 0; i < s.size(); i++){\n    char x = s[i][0];\n    int bin = x - 'a';\n    bins[bin]++;\n  }\n  \n}",
            "// Your code here\n}",
            "#pragma omp parallel\n    {\n        int i = omp_get_thread_num();\n        size_t size = s.size();\n        int num_threads = omp_get_num_threads();\n        int j = 0;\n        #pragma omp for nowait\n        for(j = 0; j < size; j++) {\n            int index = s[j][0] - 'a';\n            #pragma omp atomic update\n            bins[index] += 1;\n        }\n    }\n}",
            "// Fill your code here\n\n}",
            "// TODO: Fill in the missing code\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        if (s[i].size() == 0)\n            continue;\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "// Initialize all values to 0\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    // Count the number of strings that start with the letter a...z\n    //...\n\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < s.size(); i++) {\n        if (s[i][0] == 'a') {\n            bins[0] += 1;\n        }\n        if (s[i][0] == 'b') {\n            bins[1] += 1;\n        }\n        if (s[i][0] == 'c') {\n            bins[2] += 1;\n        }\n        if (s[i][0] == 'd') {\n            bins[3] += 1;\n        }\n        if (s[i][0] == 'e') {\n            bins[4] += 1;\n        }\n        if (s[i][0] == 'f') {\n            bins[5] += 1;\n        }\n        if (s[i][0] == 'g') {\n            bins[6] += 1;\n        }\n        if (s[i][0] == 'h') {\n            bins[7] += 1;\n        }\n        if (s[i][0] == 'i') {\n            bins[8] += 1;\n        }\n        if (s[i][0] == 'j') {\n            bins[9] += 1;\n        }\n        if (s[i][0] == 'k') {\n            bins[10] += 1;\n        }\n        if (s[i][0] == 'l') {\n            bins[11] += 1;\n        }\n        if (s[i][0] =='m') {\n            bins[12] += 1;\n        }\n        if (s[i][0] == 'n') {\n            bins[13] += 1;\n        }\n        if (s[i][0] == 'o') {\n            bins[14] += 1;\n        }\n        if (s[i][0] == 'p') {\n            bins[15] += 1;\n        }\n        if (s[i][0] == 'q') {\n            bins[16] += 1;\n        }\n        if (s[i][0] == 'r') {\n            bins[17] += 1;\n        }\n        if (s[i][0] =='s') {\n            bins[18] += 1;\n        }\n        if (s[i][0] == 't') {\n            bins[19] += 1;\n        }\n        if (s[i][0] == 'u') {\n            bins[20] += 1;\n        }\n        if (s[i][0] == 'v') {\n            bins[21] += 1;\n        }\n        if (s[i][0] == 'w') {\n            bins[22] += 1;\n        }\n        if (s[i][0] == 'x') {\n            bins[23] += 1;\n        }\n        if (s[i][0] == 'y') {\n            bins[24] += 1;\n        }\n        if (s[i][0] == 'z') {\n            bins[25] += 1;\n        }\n\n    }\n}",
            "// You need to use OpenMP here.\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char first_letter = s[i].front();\n        bins[first_letter - 'a']++;\n    }\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; ++i) {\n        // fill in this line\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// Your code here\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n    for (auto const& str : s) {\n      if (str[0] == static_cast<char>('a' + i)) {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "for (int i=0; i<omp_get_max_threads(); i++){\n        omp_set_num_threads(26);\n        bins[i] = 0;\n        for (auto &str:s){\n            #pragma omp parallel for\n            for (int j=0; j<26; j++){\n                if (str[0] == ('a'+j)){\n                    bins[i]++;\n                }\n            }\n        }\n    }\n}",
            "// TODO: Implement me!\n}",
            "//#pragma omp parallel\n  //{\n    //#pragma omp for\n    for (int i = 0; i < 26; i++) {\n      bins[i] = 0;\n    }\n    for (int i = 0; i < 26; i++) {\n      for (int j = 0; j < s.size(); j++) {\n        if (s[j][0] == i + 97) {\n          bins[i]++;\n        }\n      }\n    }\n  //}\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        int first = s[i][0] - 'a';\n        #pragma omp atomic\n        bins[first]++;\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < s.size(); i++){\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "// Fill in the code here\n  \n  // Don't forget to uncomment this line\n  // omp_set_num_threads(4);\n}",
            "// TODO: insert your code here\n  #pragma omp parallel for\n  for(int i=0; i < s.size(); i++){\n    char c = s.at(i).at(0);\n    if(c >= 'a' && c <= 'z')\n    {\n      bins.at(c - 'a')++;\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (auto & word: s) {\n    bins[word[0] - 'a'] += 1;\n  }\n}",
            "//TODO: Your code here\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "}",
            "// The code to fill the bins array\n    // Fill the bins array with 0\n    // Iterate over all the strings in s\n    // Use the first letter of the string to index the bins array\n    // Increment the array at the index corresponding to the first letter\n    //  - Use std::tolower to convert the character to lowercase\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        size_t index = s[i][0] - 'a';\n        bins[index] += 1;\n    }\n}",
            "}",
            "// Your code here\n}",
            "// TODO: replace this with your solution\n    for (auto x : s) {\n        if (x.length() >= 1) {\n            int index = tolower(x[0]) - 'a';\n            bins[index]++;\n        }\n    }\n}",
            "size_t n = s.size();\n    // TODO: implement\n}",
            "#pragma omp parallel for num_threads(1)\n    for (size_t i = 0; i < s.size(); i++) {\n        const auto letter = s[i][0];\n        if (letter > 'z' || letter < 'a') {\n            bins[0] += 1;\n        }\n        else {\n            bins[letter - 'a' + 1] += 1;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for(size_t i=0; i<s.size(); ++i) {\n        if(s[i].empty()) continue;\n\n        // Find the first character\n        auto first_char = s[i][0];\n\n        // Convert to lower case\n        if('A' <= first_char && first_char <= 'Z') first_char += 'a' - 'A';\n\n        // Count the number of strings starting with `first_char`\n        bins[first_char - 'a']++;\n    }\n}",
            "// YOUR CODE HERE\n    int len = s.size();\n    int thread_num = omp_get_max_threads();\n    int thread_id = omp_get_thread_num();\n    int chunk = (len-1)/thread_num+1;\n\n    #pragma omp parallel for\n    for (int i = 0; i < len; i++) {\n        int idx = s[i][0] - 'a';\n        // printf(\"%d %d\\n\", i, idx);\n        #pragma omp atomic\n        bins[idx]++;\n    }\n\n}",
            "}",
            "for (auto& b : bins) {\n        b = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        if (s[i].empty()) continue;\n        ++bins[std::tolower(s[i].front()) - 'a'];\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++)\n        bins[s[i][0] - 'a'] += 1;\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s[j][0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// Your code goes here\n\n}",
            "for(int i = 0; i < 26; i++){\n    #pragma omp parallel for\n    for(auto word : s){\n      if (word[0] == (char)(97+i)){\n        bins[i]++;\n      }\n    }\n  }\n}",
            "}",
            "#pragma omp parallel for\n   for (size_t i=0; i<s.size(); i++) {\n      char c = s[i][0];\n      if ((c >= 'a') && (c <= 'z')) {\n         bins[c - 'a'] += 1;\n      }\n   }\n}",
            "#pragma omp parallel for num_threads(3)\n  for (size_t i = 0; i < s.size(); ++i) {\n    std::string str = s[i];\n    if (str.size() > 0) {\n      //std::cout << str[0] << std::endl;\n      bins[str[0] - 'a'] += 1;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        if (s[i].size() > 0) {\n            char c = s[i][0];\n            if (c >= 'a' && c <= 'z') {\n                bins[c-'a']++;\n            }\n        }\n    }\n}",
            "// TODO: implement\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); i++)\n  {\n    int j = 0;\n    while (s[i][j]!= '\\0')\n    {\n      bins[s[i][j] - 'a']++;\n      j++;\n    }\n  }\n}",
            "// TODO: fill in your code here.\n    // Use omp_get_num_threads() and omp_get_thread_num() to implement parallelization.\n    for (size_t i = 0; i < bins.size(); ++i) bins[i] = 0;\n    #pragma omp parallel\n    {\n        // TODO: use OpenMP to initialize bins with 0.\n        // You don't need to use lock for this, just use atomic writes to bins[i]\n        // hint: use omp_get_num_threads() and omp_get_thread_num()\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char c = s[i].front();\n        // TODO: use OpenMP to count the number of strings in s that start with each letter\n        // and update the corresponding bin.\n        // You don't need to use lock for this, just use atomic writes to bins[i]\n        // hint: use omp_get_num_threads() and omp_get_thread_num()\n    }\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "#pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < s.size(); i++) {\n        char currentChar = s.at(i).at(0);\n        if (currentChar >= 'a' && currentChar <= 'z') {\n            bins[currentChar - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s[j][0] == 'a' + i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        if (s[i].empty())\n            continue;\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "}",
            "// use openmp\n  #pragma omp parallel for\n  for (auto i = 0; i < 26; ++i) {\n\n    // your code here\n  }\n}",
            "bins.fill(0);\n\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n\n    // Add your code here\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    const auto& str = s[i];\n    const auto& first = str[0];\n    const auto& pos = std::tolower(first) - 'a';\n    if (pos < bins.size()) {\n      bins[pos]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        for (size_t j = 0; j < 26; j++) {\n            if (s[i][0] == char('a' + j))\n                bins[j]++;\n        }\n    }\n}",
            "for(auto &bin : bins) {\n    bin = 0;\n  }\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); i++) {\n    auto c = s[i][0];\n    #pragma omp atomic update\n    bins[c - 'a']++;\n  }\n}",
            "// TODO: Complete this function\n\n    // hint: you can use `omp_get_thread_num()` to get the id of the current thread\n\n    #pragma omp parallel for\n    for(int i = 0; i < s.size(); i++) {\n        char c = s[i][0];\n        int bin = c - 'a';\n        bins[bin]++;\n    }\n}",
            "// TODO: Implement me!\n\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < 26; i++) {\n    bins[i] = 0;\n    for(int j = 0; j < s.size(); j++) {\n      if(s[j][0] == char(i + 97))\n        bins[i]++;\n    }\n  }\n}",
            "// Fill in your code here\n}",
            "// TODO: Fill in the code to implement this function.\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < s.size(); ++i) {\n    auto const& word = s[i];\n    auto const& firstLetter = word.at(0);\n    ++bins[firstLetter - 'a'];\n  }\n}",
            "// Add your code here\n}",
            "// TODO: Fill this in.\n}",
            "// TODO: Implement this function\n}",
            "// Implement your solution here.\n\n    // TODO:\n    // 1. initialize the output to 0\n    // 2. go over all strings in the vector and count the first letters\n    // 3. store the result to the corresponding array index\n    // 4. use the atomic increment to increment the count for a particular letter\n\n}",
            "size_t size = s.size();\n    //omp_set_num_threads(16);\n    for (size_t i=0; i < size; i++) {\n        int count = s[i][0] - 'a';\n        #pragma omp atomic\n        bins[count]++;\n    }\n}",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i)\n    {\n        // Count the number of strings that start with this letter\n        size_t index = s[i][0] - 'a';\n        bins[index]++;\n    }\n}",
            "int nthreads = omp_get_max_threads();\n    std::vector<size_t> thread_bins(nthreads * 26);\n\n    #pragma omp parallel\n    {\n        int thread_num = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n\n        for (auto i = thread_num; i < s.size(); i+=nthreads)\n        {\n            int letter = s[i][0] - 97;\n            thread_bins[26*thread_num + letter] += 1;\n        }\n\n    }\n\n    for (int i = 0; i < 26; i++) {\n        for (int j = 1; j < nthreads; j++)\n        {\n            thread_bins[i] += thread_bins[j * 26 + i];\n        }\n        bins[i] = thread_bins[i];\n    }\n\n}",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); ++i){\n        char firstLetter = s[i][0];\n        bins[firstLetter-'a']++;\n    }\n}",
            "// TODO\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); ++i) {\n        char c = s[i].at(0);\n        bins[c-'a']++;\n    }\n}",
            "// Fill in your solution here.\n    int count = 0;\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        int letter = s[i][0] - 'a';\n        #pragma omp atomic\n        bins[letter] += 1;\n    }\n}",
            "// #pragma omp for\n    for (int i = 0; i < s.size(); i++) {\n        // std::cout << s[i][0] << std::endl;\n        if (std::isalpha(s[i][0]))\n            bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "#pragma omp parallel for schedule(static)\n    for (int i = 0; i < s.size(); i++)\n    {\n        if (s[i].length() > 0)\n        {\n            std::string a = s[i].substr(0, 1);\n            std::transform(a.begin(), a.end(), a.begin(), ::tolower);\n            char letter = a[0];\n            if (letter >= 'a' && letter <= 'z')\n            {\n                #pragma omp atomic\n                bins[letter - 'a']++;\n            }\n        }\n    }\n}",
            "// Fill this in\n    bins.fill(0);\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++){\n        // Fill this in\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "}",
            "// TODO: Fill in the code here\n  // The code is already written for you\n  // You can change the number of threads\n  // and the size of the chunksize\n\n  // Make it work for only one thread first\n  // Then change the number of threads to 2\n  // Then change the chunksize to 1\n\n  // Use omp_get_num_threads() to get the number of threads\n  // Use omp_get_thread_num() to get the thread id\n\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_get_max_threads() to get the maximum number of threads\n\n  // Use omp_set_num_threads(4) to set the number of threads\n  // Use omp_set_nested(true) to enable nested parallelism\n  // Use omp_get_num_procs() to get the number of cores\n  // Use omp_",
            "#pragma omp parallel for\n    for(auto i = 0; i < s.size(); i++){\n        //bins[s[i][0] - 'a'] += 1;\n        // or better\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: Your code here\n#pragma omp parallel\n  {\n  // each thread should start from 0\n#pragma omp for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i) {\n      // TODO: Your code here\n      char letter = s[i][0];\n      if (letter >= 'a' && letter <= 'z') {\n        bins[letter - 'a']++;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char ch = s[i].at(0);\n        ++bins[ch - 'a'];\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        std::string letter = s[i][0];\n        bins[letter[0] - 'a']++;\n    }\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "// TODO: use OpenMP to parallelize\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n\tfor(size_t i = 0; i < s.size(); ++i) {\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "// Reset counts to zero\n    bins.fill(0);\n\n    // Add code here\n#pragma omp parallel for\n    for (auto &str: s) {\n        if (!str.empty()) {\n            char c = str[0];\n            if (c >= 'a' && c <= 'z') {\n                bins[c - 'a']++;\n            }\n        }\n    }\n}",
            "}",
            "size_t size = s.size();\n    int const alphabetSize = 26;\n\n    #pragma omp parallel\n    {\n        size_t nthreads = omp_get_num_threads();\n        size_t threadNum = omp_get_thread_num();\n        size_t chunk = size / nthreads;\n        size_t startIndex = threadNum * chunk;\n\n        if (threadNum == nthreads - 1) {\n            chunk = size - chunk * nthreads;\n        }\n\n        for (size_t i = 0; i < chunk; i++) {\n            std::string letter = s[i + startIndex][0];\n            int index = letter[0] - 'a';\n\n            #pragma omp atomic\n            bins[index]++;\n        }\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char firstLetter = s[i][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
            "// Replace this with your code\n    int n = s.size();\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static, 1)\n        for (int i = 0; i < n; i++) {\n            bins[s[i][0]-'a']++;\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "serial",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "for (auto const &str : s) {\n    if (str.length() > 0) {\n      size_t index = str.at(0) - 'a';\n      bins[index]++;\n    }\n  }\n}",
            "size_t count = 0;\n  char c;\n\n  for (auto& str : s) {\n    c = str[0];\n\n    if (c >= 'a' && c <= 'z') {\n      bins[c - 'a']++;\n    }\n  }\n}",
            "std::array<size_t, 26> counts{};\n    for (auto const& e : s)\n        ++counts[e[0] - 'a'];\n    return counts;\n}",
            "for (auto const& str: s) {\n        for (size_t i = 0; i < 26; i++) {\n            if (str[0] == 'a' + i) {\n                bins[i] += 1;\n                break;\n            }\n        }\n    }\n}",
            "char ch;\n    int index;\n    std::for_each(std::begin(s), std::end(s),\n        [&](std::string const& str) {\n            ch = tolower(str[0]);\n            index = ch - 'a';\n            bins[index]++;\n        }\n    );\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (std::string const& str: s) {\n        if (!str.empty())\n            bins[std::tolower(str[0]) - 'a'] += 1;\n    }\n}",
            "for(auto &elem : s) {\n    bins[elem[0] - 'a'] += 1;\n  }\n}",
            "for (std::string const& x: s) {\n        if (x.size() > 0) {\n            char first_letter = std::tolower(x[0]);\n            if (first_letter >= 'a' && first_letter <= 'z') {\n                bins[first_letter - 'a'] += 1;\n            }\n        }\n    }\n}",
            "for (size_t i{0}; i < s.size(); ++i) {\n        int n = s[i][0] - 'a';\n        ++bins[n];\n    }\n}",
            "}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& x : s) {\n        ++bins[x[0] - 'a'];\n    }\n}",
            "bins.fill(0);\n    for(std::string const& word : s) {\n        if(word.size() > 0) {\n            bins[word[0]-'a']++;\n        }\n    }\n}",
            "for (int i = 0; i < 26; ++i)\n\t\tbins[i] = 0;\n\tfor (std::string x : s) {\n\t\tstd::string t = x;\n\t\tstd::transform(t.begin(), t.end(), t.begin(), ::tolower);\n\t\tif (t.length() > 0)\n\t\t\tbins[t[0] - 'a']++;\n\t}\n}",
            "for (const auto &word : s)\n        ++bins[toupper(word[0]) - 'A'];\n}",
            "}",
            "for (const auto& x : s) {\n    auto letter = x[0] - 'a';\n    ++bins[letter];\n  }\n}",
            "for(const std::string& str:s){\n\t\tchar ch = std::tolower(str[0]);\n\t\tbins[ch-'a']++;\n\t}\n}",
            "for (auto const &i: s) {\n        int num = i[0] - 'a';\n        ++bins[num];\n    }\n}",
            "for (auto const& w : s) {\n        if (!w.empty()) {\n            size_t const index{w[0] - 'a'};\n            ++bins[index];\n        }\n    }\n}",
            "for (std::string const& w : s) {\n\t\tbins[w[0] - 'a']++;\n\t}\n}",
            "for (std::string word: s) {\n        ++bins[word[0] - 'a'];\n    }\n}",
            "//...\n}",
            "// Your code here\n}",
            "for (std::string const& str : s) {\n        if (str.length() == 0) {\n            bins[0]++;\n        } else {\n            // get first letter\n            char ch = str[0];\n            // if it is a letter then convert to lower case\n            if (std::isalpha(ch)) {\n                ch = std::tolower(ch);\n            }\n            bins[ch - 'a']++;\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (std::string const& s : s) {\n        char ch = s.front();\n        if (ch >= 'a' && ch <= 'z') {\n            bins[ch - 'a']++;\n        }\n    }\n}",
            "for (size_t i = 0; i < bins.size(); i++) {\n      bins[i] = 0;\n   }\n\n   for (auto &str : s) {\n      char first = str[0];\n      if (first >= 'a' && first <= 'z') {\n         bins[first - 'a'] += 1;\n      }\n   }\n}",
            "//std::array<size_t, 26> bins; // TODO\n\n    bins.fill(0);\n\n    for (auto const& str : s) {\n        // TODO\n        if (str.empty()) {\n            continue;\n        }\n        bins[str[0] - 'a']++;\n    }\n\n}",
            "// TODO\n}",
            "bins.fill(0);\n   size_t pos=0;\n   for (auto & word : s){\n        if (isalpha(word[0])){\n            pos = tolower(word[0]) - 97;\n            bins[pos]++;\n        }\n   }\n}",
            "for (const std::string& str : s) {\n      if (str.length() > 0) {\n         char c = str[0];\n         if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n         }\n      }\n   }\n}",
            "//...\n}",
            "// TODO\n\n}",
            "for(const auto& str : s) {\n\t\tif(!str.empty())\n\t\t\tbins[str[0] - 'a']++;\n\t}\n}",
            "for (const auto& str : s) {\n        auto letter = std::tolower(str[0]);\n        bins[letter - 'a'] += 1;\n    }\n\n}",
            "// Fill in this function\n}",
            "}",
            "for (auto const& word : s) {\n        // Convert to lowercase\n        std::string lowerWord = word;\n        std::transform(lowerWord.begin(), lowerWord.end(), lowerWord.begin(), [](unsigned char c){ return std::tolower(c); });\n        // Count the letter in the first position\n        if (word.size() >= 1 && lowerWord[0] >= 'a' && lowerWord[0] <= 'z') {\n            bins[lowerWord[0] - 'a']++;\n        }\n    }\n}",
            "for (size_t i = 0; i < bins.size(); i++)\n\t{\n\t\tbins[i] = 0;\n\t}\n\tfor (size_t i = 0; i < s.size(); i++)\n\t{\n\t\tchar start = s[i][0];\n\t\tbins[start - 'a'] += 1;\n\t}\n\n\treturn;\n}",
            "// TODO: Your code goes here\n    bins = {};\n    std::string alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n    for (auto str : s)\n    {\n        for (int i = 0; i < alphabet.length(); i++)\n        {\n            if (str[0] == alphabet[i])\n            {\n                bins[i] += 1;\n                break;\n            }\n        }\n    }\n}",
            "//write your code here\n    int idx;\n\n    for(auto const& word : s)\n    {\n        idx = word[0] - 'a';\n        bins[idx]++;\n    }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a'] += 1;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& item : s) {\n    if (!item.empty()) {\n      ++bins[item[0] - 'a'];\n    }\n  }\n}",
            "for(auto word: s) {\n      auto c = tolower(word[0]);\n      if(c >= 'a' && c <= 'z')\n         ++bins[c - 'a'];\n   }\n}",
            "std::array<size_t, 26> counts{0};\n\n  for (auto const &i : s) {\n    for (auto const &c : i) {\n      if (std::isalpha(c)) {\n        counts[(c - 'a')] += 1;\n      }\n    }\n  }\n\n  for (auto const &i : s) {\n    for (auto const &c : i) {\n      if (std::isalpha(c)) {\n        bins[(c - 'a')] += 1;\n      }\n    }\n  }\n}",
            "// TODO: Implement\n\n}",
            "for (const auto &i : s) {\n\t\tint firstLetter = (int)i[0] - 97;\n\t\tbins[firstLetter]++;\n\t}\n}",
            "// write your code here\n    for(size_t i = 0; i < 26; i++)\n    {\n        bins[i] = 0;\n    }\n    for(auto str: s)\n    {\n        if(str.size() > 0)\n        {\n            int index = (int)(str[0] - 'a');\n            bins[index]++;\n        }\n    }\n}",
            "}",
            "for (auto const& it : s) {\n        if (it.empty()) {\n            continue;\n        }\n        char first_letter = it.at(0);\n        size_t index = first_letter - 'a';\n        if (index < bins.size()) {\n            ++bins[index];\n        }\n    }\n}",
            "// 1. Write your code here\n  //...\n}",
            "// TODO\n}",
            "for (int i = 0; i < bins.size(); i++)\n        bins.at(i) = 0;\n\n    for (auto const& it : s) {\n        char letter = it.at(0);\n        bins.at(letter - 'a')++;\n    }\n}",
            "bins.fill(0);\n    // for each string in the vector\n    for(auto const& str : s) {\n        // Get the first letter of the string and store its index in the array of bins\n        bins[std::tolower(str.front()) - 'a']++;\n    }\n}",
            "}",
            "for (auto const& i : s) {\n        if (i.length() >= 1) {\n            auto letter = i[0];\n            if (letter >= 'a' && letter <= 'z')\n                bins[letter - 'a']++;\n        }\n    }\n}",
            "// TODO: implement this\n\n}",
            "bins.fill(0);\n  for(auto const& str : s) {\n    bins[str[0] - 'a'] += 1;\n  }\n}",
            "for (const std::string &str : s) {\n    bins[str.front() - 'a']++;\n  }\n}",
            "}",
            "for (auto const& word : s) {\n        if (word.size()!= 0) {\n            bins[word[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (str.length() > 0) {\n            bins[std::tolower(str[0]) - 'a']++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto& s1:s) {\n        bins[s1[0] - 'a']++;\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tint val = (int)tolower(s[i][0]) - 97;\n\t\tbins[val]++;\n\t}\n}",
            "for (auto & i : s)\n        bins[i[0] - 'a']++;\n}",
            "for (auto i = 0; i < s.size(); i++)\n    bins[s[i][0] - 'a']++;\n\n}",
            "for (auto it = std::begin(s); it!= std::end(s); ++it) {\n        if (it->length() > 0) {\n            int index = (int) it->at(0) - 97; // index of first character\n            ++bins[index];\n        }\n    }\n}",
            "for (auto &i : bins) {\n        i = 0;\n    }\n\n    for (auto &j : s) {\n        std::size_t pos = j[0] - 'a';\n        bins[pos] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const &word : s) {\n    auto letterIndex = static_cast<unsigned int>(tolower(word.front())) - 'a';\n    bins[letterIndex] += 1;\n  }\n}",
            "for (auto & i : bins) i = 0;\n    for (auto & i : s) {\n        if (i.length() > 0)\n            bins[i[0] - 'a']++;\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n  for (auto str : s) {\n    if (str.size() > 0) {\n      int bin = str[0] - 'a';\n      bins[bin]++;\n    }\n  }\n}",
            "for (size_t i{0}; i < 26; ++i) {\n\t\tfor (auto const& element : s) {\n\t\t\tif (element[0] == i + 'a') {\n\t\t\t\tbins[i]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: write code here.\n    for (auto i = 0; i < bins.size(); i++){\n        for (auto j = 0; j < s.size(); j++){\n            if (s[j][0] == 'a' + i) bins[i]++;\n        }\n    }\n}",
            "//std::array<size_t, 26> bins = { 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 };\n\n  for(auto word : s) {\n    bins[word.front()-'a']++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for(const auto& i : s) {\n        ++bins[i[0] - 'a'];\n    }\n}",
            "}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (int j = 0; j < s.size(); j++) {\n        int ind = s[j][0] - 'a';\n        bins[ind]++;\n    }\n}",
            "for (auto const& str: s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (std::vector<std::string>::size_type i = 0; i!= s.size(); i++) {\n        // TODO\n    }\n}",
            "for(auto str : s) {\n    if(str.size() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "// TODO\n}",
            "for (const auto& ss : s) {\n        auto i = ss[0] - 'a';\n        bins[i] += 1;\n    }\n}",
            "// Fill in the body of the function\n    //...\n}",
            "for (auto const &x:s) {\n        if (x.size()!= 0) {\n            bins[x[0] - 'a']++;\n        }\n    }\n}",
            "// TODO: write this function\n}",
            "for (auto letter : s) {\n        ++bins[letter[0] - 'a'];\n    }\n}",
            "// Implement this function\n}",
            "for(auto& word : s) {\n        int letter = tolower(word[0]) - 'a';\n        ++bins[letter];\n    }\n}",
            "for (auto& word : s) {\n\t\tbins[word[0] - 'a']++;\n\t}\n}",
            "for (auto str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto const& word : s) {\n    char letter = word.front();\n    ++bins[letter - 'a'];\n  }\n}",
            "for (int i = 0; i < bins.size(); ++i) {\n      bins[i] = 0;\n   }\n   for (auto it = s.begin(); it!= s.end(); ++it) {\n      std::string str = *it;\n      char first = str.front();\n      size_t pos = first - 'a';\n      bins[pos] += 1;\n   }\n}",
            "// TODO: Implement this function\n  // For simplicity, assume no punctuation or spaces\n  // For simplicity, assume all strings are in lower case\n  // Ignore any errors\n\n  // TODO: Implement this function\n  // For simplicity, assume no punctuation or spaces\n  // For simplicity, assume all strings are in lower case\n  // Ignore any errors\n  for (size_t i = 0; i < s.size(); ++i)\n  {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "for (const auto& word: s) {\n    auto l = tolower(word[0]);\n    bins[l - 'a'] += 1;\n  }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n    size_t binIndex = s[i][0] - 'a';\n    ++bins[binIndex];\n  }\n}",
            "for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO: Count how many strings start with each letter\n    std::string alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n    for (int i = 0; i < s.size(); ++i)\n    {\n        for (int j = 0; j < alphabet.size(); ++j)\n        {\n            if (s[i][0] == alphabet[j])\n            {\n                bins[j] += 1;\n            }\n        }\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a'] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (auto const& element : s) {\n      bins[element[0] - 'a']++;\n   }\n}",
            "// TODO: Implement this function\n}",
            "for(size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    for(const std::string& str : s) {\n        if (str.size() > 0) {\n            const char& first = str.at(0);\n            if(first >= 'a' && first <= 'z') {\n                bins[first - 'a']++;\n            }\n        }\n    }\n}",
            "// TODO:\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tint index = s[i][0] - 'a';\n\t\tbins[index]++;\n\t}\n}",
            "for (auto x:s) {\n        bins[x[0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n      if (str.size() > 0) {\n         bins[str.front() - 'a'] += 1;\n      }\n   }\n}",
            "for (auto word : s)\n    {\n        for (auto ch : word)\n        {\n            // Subtract the ascii value of 'a' to get an index from 0-25\n            int index = ch - 'a';\n            bins[index]++;\n        }\n    }\n}",
            "bins.fill(0);\n   for (auto i : s) {\n      if (i.size() > 0) {\n         bins[i[0] - 'a']++;\n      }\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& str : s) {\n    int idx = str[0] - 'a';\n    ++bins[idx];\n  }\n}",
            "bins.fill(0);\n    for (std::string str : s) {\n        ++bins[str.at(0) - 'a'];\n    }\n}",
            "for (std::string const& str : s) {\n\t\tfor (char c : str) {\n\t\t\tbins[c - 'a']++;\n\t\t}\n\t}\n}",
            "// Write your code here\n    bins.fill(0);\n    std::array<char, 26> char_array = {'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'};\n    for (std::string const& word : s) {\n        for (auto const& c : word) {\n            int index = c - char_array.at(0);\n            bins.at(index)++;\n        }\n    }\n}",
            "// TODO\n}",
            "}",
            "for (auto const& el: s)\n\t\tbins[tolower(el.at(0)) - 'a']++;\n}",
            "// Your code goes here\n    int bin;\n    std::for_each(s.begin(), s.end(), [&](auto& it) {\n        bin = it[0] - 'a';\n        ++bins[bin];\n    });\n}",
            "for (const auto& str: s) {\n        bins[str.at(0) - 'a']++;\n    }\n}",
            "for (const std::string& i : s) {\n\t\tbins[static_cast<unsigned char>(i[0]) - 97] += 1;\n\t}\n}",
            "// TODO:\n  bins.fill(0);\n  for (const auto& str: s) {\n    ++bins[str[0] - 'a'];\n  }\n}",
            "// TODO: Fill the `bins` array with the number of strings in `s` that start with a letter.\n    // bins should be an array of size 26 with 0 in each index.\n    // The index should be the integer representation of the character that starts a string.\n    // The value in each index should be the number of strings in `s` that start with that character.\n\n    for(int i = 0; i < s.size(); i++){\n        if(s[i][0] == 'a'){\n            bins[0] += 1;\n        }\n        else if(s[i][0] == 'b'){\n            bins[1] += 1;\n        }\n        else if(s[i][0] == 'c'){\n            bins[2] += 1;\n        }\n        else if(s[i][0] == 'd'){\n            bins[3] += 1;\n        }\n        else if(s[i][0] == 'e'){\n            bins[4] += 1;\n        }\n        else if(s[i][0] == 'f'){\n            bins[5] += 1;\n        }\n        else if(s[i][0] == 'g'){\n            bins[6] += 1;\n        }\n        else if(s[i][0] == 'h'){\n            bins[7] += 1;\n        }\n        else if(s[i][0] == 'i'){\n            bins[8] += 1;\n        }\n        else if(s[i][0] == 'j'){\n            bins[9] += 1;\n        }\n        else if(s[i][0] == 'k'){\n            bins[10] += 1;\n        }\n        else if(s[i][0] == 'l'){\n            bins[11] += 1;\n        }\n        else if(s[i][0] =='m'){\n            bins[12] += 1;\n        }\n        else if(s[i][0] == 'n'){\n            bins[13] += 1;\n        }\n        else if(s[i][0] == 'o'){\n            bins[14] += 1;\n        }\n        else if(s[i][0] == 'p'){\n            bins[15] += 1;\n        }\n        else if(s[i][0] == 'q'){\n            bins[16] += 1;\n        }\n        else if(s[i][0] == 'r'){\n            bins[17] += 1;\n        }\n        else if(s[i][0] =='s'){\n            bins[18] += 1;\n        }\n        else if(s[i][0] == 't'){\n            bins[19] += 1;\n        }\n        else if(s[i][0] == 'u'){\n            bins[20] += 1;\n        }\n        else if(s[i][0] == 'v'){\n            bins[21] += 1;\n        }\n        else if(s[i][0] == 'w'){\n            bins[22] += 1;\n        }\n        else if(s[i][0] == 'x'){\n            bins[23] += 1;\n        }\n        else if(s[i][0] == 'y'){\n            bins[24] += 1;\n        }\n        else if(s[i][0] == 'z'){\n            bins[25] += 1;\n        }\n    }\n\n}",
            "// TODO: replace this comment with your code\n    std::array<size_t, 26> counts;\n    for(size_t i = 0; i < 26; ++i)\n        counts[i] = 0;\n\n    for(auto& it : s)\n    {\n        counts[it[0] - 97] += 1;\n    }\n\n    for(size_t i = 0; i < 26; ++i)\n    {\n        bins[i] = counts[i];\n    }\n}",
            "// TODO\n}",
            "for(auto const& ss : s) {\n        bins[ss[0] - 'a']++;\n    }\n}",
            "for (auto i = 0; i < bins.size(); ++i)\n    bins[i] = 0;\n\n  for (auto const& str : s) {\n    auto letter = str.at(0);\n    if (letter >= 'a' && letter <= 'z')\n      bins[letter - 'a'] += 1;\n  }\n}",
            "std::array<size_t, 26> empty = {};\n    bins = empty;\n    std::sort(s.begin(), s.end());\n    for(const auto& e : s) {\n        ++bins[e[0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto& word: s) {\n        if (word.size() > 0) {\n            bins[word[0] - 'a']++;\n        }\n    }\n}",
            "for (std::string word : s)\n    bins[word[0] - 'a']++;\n}",
            "for (const auto& word : s) {\n        ++bins[word.front() - 'a'];\n    }\n}",
            "bins.fill(0);\n  for (auto const &str : s) {\n    auto idx = str[0] - 'a';\n    ++bins[idx];\n  }\n}",
            "// TODO\n}",
            "// TODO: Your code goes here\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a'] += 1;\n  }\n}",
            "for (auto i = 0; i < s.size(); ++i) {\n    int j = s[i].at(0);\n    bins.at(j)++;\n  }\n}",
            "// TODO: complete the function body\n    for(auto const& str : s) {\n        char f = str.front();\n        if(f >= 'a' && f <= 'z') {\n            bins[f - 'a']++;\n        }\n    }\n}",
            "for (auto letter:s) {\n        auto firstLetter = letter[0];\n\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            ++bins[firstLetter - 'a'];\n        }\n    }\n}",
            "bins.fill(0); // Set all elements of array to zero.\n\n    for (const std::string& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "std::array<size_t, 26> letterCount{0};\n\n  for (auto& word : s) {\n    if (!word.empty()) {\n      letterCount[word[0] - 'a']++;\n    }\n  }\n  bins = letterCount;\n}",
            "for (auto const &str : s) {\n        // Find out which bucket does the str belongs to\n        unsigned int const bucket = str[0] - 'a';\n        // Add 1 to the bucket\n        bins[bucket] += 1;\n    }\n}",
            "// TODO: implement this function\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n\t\tchar letter = tolower(s[i][0]);\n\t\t++bins[letter - 'a'];\n\t}\n}",
            "// Complete this function\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        char letter = s[i].at(0);\n        if (letter >= 'a' && letter <= 'z') {\n            bins[letter - 'a'] += 1;\n        }\n    }\n}",
            "for (auto &c : bins) {\n        c = 0;\n    }\n\n    for (auto const &str : s) {\n        auto letter = tolower(str[0]);\n        if (letter < 'a' || letter > 'z') {\n            continue;\n        }\n\n        bins[letter - 'a'] += 1;\n    }\n}",
            "// Write your code here\n  for (int i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n  for (std::vector<std::string>::const_iterator it = s.begin(); it!= s.end(); it++) {\n    if (it->length() > 0) {\n      bins[it->at(0) - 'a']++;\n    }\n  }\n}",
            "for (auto word : s) {\n        bins[word[0] - 'a']++;\n    }\n}",
            "std::transform(s.cbegin(), s.cend(), bins.begin(), [](std::string const& word) {\n        return word[0] - 'a';\n    });\n}",
            "// Your code here\n  std::fill(bins.begin(), bins.end(), 0);\n  for (auto i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO\n}",
            "for (std::string const& str : s) {\n        // write the code here\n    }\n}",
            "for (auto& word: s) {\n    auto first_char = tolower(word[0]);\n    ++bins[first_char - 'a'];\n  }\n}",
            "for(auto const& i : s) {\n        char c = i[0];\n        if(c >= 'a' && c <= 'z') bins[c - 'a'] += 1;\n    }\n}",
            "// std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto &str : s) {\n        // if (str[0] >= 'a' && str[0] <= 'z') {\n        if (str[0] >= 'a') {\n            bins[str[0] - 'a'] += 1;\n        }\n    }\n}\n\n\n/* Write a function that takes a vector of strings and an integer N as parameters.\n   Return the list of strings that start with exactly N letters.\n   Assume all strings are in lower case.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"], N = 2\n   output: [\"cow\", \"code\", \"type\"]\n*/\nstd::vector<std::string> startsWith(std::vector<std::string> const& s, size_t N) {\n    std::vector<std::string> result;\n\n    for (auto &str : s) {\n        if (str.length() == N) {\n            result.push_back(str);\n        }\n    }\n\n    return result;\n}\n\n\n/* For each letter in the alphabet, count the number of strings in the vector s that contain that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1]\n*/\nvoid containsLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n    // std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto &str : s) {\n        for (auto &letter : str) {\n            if (letter >= 'a') {\n                bins[letter - 'a'] += 1;\n            }\n        }\n    }\n}\n\n\n/* Write a function that takes a vector of strings and an integer N as parameters.\n   Return the list of strings that contain exactly N letters.\n   Assume all strings are in lower case.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"], N = 3\n   output: [\"type\", \"flower\"]\n*/\nstd::vector<std::string> contains(std::vector<std::string> const& s, size_t N) {\n    std::vector<std::string> result;\n\n    for (auto &str : s) {\n        if (str.length() == N) {\n            result.push_back(str);\n        }\n    }\n\n    return result;\n}\n\n\n/* For each letter in the alphabet, count the number of strings in the vector s that end with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1]\n*/\nvoid endsWithLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {\n    // std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto &str : s) {\n        // if (str[str.length()-1] >= 'a' && str[str.length()-1] <= 'z') {\n        if (str[str.length()-1] >= 'a') {\n            bins[str[str.length()-1] -",
            "for (auto &str: s) {\n      auto letter = str[0] - 'a'; // convert to index 0 - 25\n      ++bins[letter];\n   }\n}",
            "// bins is an array of 26 bins\n    // each bin is the number of strings starting with the character.\n    // initialize all the bins to zero\n    for (auto &bin : bins)\n        bin = 0;\n\n    // Go through each string\n    for (auto const& str : s) {\n        // Each bin contains the number of strings that start with the corresponding letter\n        bins[str[0] - 'a']++;\n    }\n}",
            "// TODO\n\tfor (size_t i = 0; i < bins.size(); ++i)\n\t{\n\t\tbins[i] = 0;\n\t}\n\n\tfor (std::string word : s) {\n\t\tfor (char letter : word) {\n\t\t\tif (letter >= 'a' && letter <= 'z')\n\t\t\t{\n\t\t\t\tbins[letter - 'a'] += 1;\n\t\t\t}\n\t\t}\n\t}\n}",
            "bins.fill(0);\n    for (auto &str : s) {\n        if (not str.empty()) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (const std::string& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "// Implement me!\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n      for (size_t j = 0; j < s[i].length(); ++j) {\n         if (s[i][j] >= 'a' && s[i][j] <= 'z') {\n            bins[s[i][j] - 'a'] += 1;\n         }\n      }\n   }\n}",
            "// Write your code here\n    bins.fill(0);\n    for (const auto& word : s) {\n        char c = word[0];\n        ++bins[c - 'a'];\n    }\n}",
            "for(auto const& word : s)\n        bins[word[0] - 'a']++;\n}",
            "for (std::string const& str : s) {\n    size_t first_letter_index = str[0] - 'a';\n    bins[first_letter_index]++;\n  }\n}",
            "for(int i = 0; i < bins.size(); i++){\n        bins[i] = 0;\n    }\n\n    for(auto const& str : s){\n        if(str[0] >= 'a' && str[0] <= 'z'){\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n      auto letter = str[0] - 'a';\n      bins[letter] += 1;\n   }\n}",
            "for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "// TODO: write your code here\n    std::for_each(s.cbegin(), s.cend(), [&](auto &x) { ++bins[x[0] - 'a']; });\n}",
            "for (auto const& word : s) {\n    if (word.size() > 0) {\n      char letter = word[0];\n      if (letter >= 'a' && letter <= 'z')\n        bins[letter - 'a']++;\n    }\n  }\n}",
            "for (std::string s1: s){\n        size_t bin = s1[0] - 'a';\n        bins[bin]++;\n    }\n}",
            "for (auto const& str : s) {\n        char first_letter = str[0];\n        size_t index = static_cast<size_t>(first_letter - 'a');\n        bins[index]++;\n    }\n}",
            "for (auto const& str : s) {\n\t\tauto firstLetter = tolower(str[0]);\n\t\tif (firstLetter >= 'a' && firstLetter <= 'z') {\n\t\t\t++bins[firstLetter - 'a'];\n\t\t}\n\t}\n}",
            "for (auto const& e : s)\n        bins[e.at(0) - 'a']++;\n\n}",
            "for (auto& word: s) {\n        char firstLetter = word.front();\n        if ('a' <= firstLetter && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
            "char letter = '\\0';\n    for(int i = 0; i < s.size(); i++) {\n        letter = s[i][0];\n        bins[letter-'a']++;\n    }\n}",
            "for (auto const &i : s) {\n        ++bins[i[0] - 'a'];\n    }\n}",
            "// TODO: implement the body\n\n    for (auto &i:bins) {\n        i = 0;\n    }\n\n    for (auto const& j:s) {\n        char ch = j[0];\n        bins[ch - 'a']++;\n    }\n}",
            "for(std::string i : s) {\n      if (std::isalpha(i[0])) {\n         bins[i[0] - 'a']++;\n      }\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto& elem : s) {\n        if (!elem.empty()) {\n            ++bins[std::toupper(elem[0]) - 'A'];\n        }\n    }\n}",
            "for (auto str: s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto i : s) {\n        ++bins[i[0] - 'a'];\n    }\n}",
            "std::array<size_t, 26> binCounts = {};\n    for(auto const& element : s){\n        ++binCounts[element[0] - 'a'];\n    }\n}",
            "// TODO\n}",
            "// Your code here!\n}",
            "for(int i = 0; i < s.size(); i++){\n\t\tif(s[i][0] == 'a') bins[0]++;\n\t\tif(s[i][0] == 'b') bins[1]++;\n\t\tif(s[i][0] == 'c') bins[2]++;\n\t\tif(s[i][0] == 'd') bins[3]++;\n\t\tif(s[i][0] == 'e') bins[4]++;\n\t\tif(s[i][0] == 'f') bins[5]++;\n\t\tif(s[i][0] == 'g') bins[6]++;\n\t\tif(s[i][0] == 'h') bins[7]++;\n\t\tif(s[i][0] == 'i') bins[8]++;\n\t\tif(s[i][0] == 'j') bins[9]++;\n\t\tif(s[i][0] == 'k') bins[10]++;\n\t\tif(s[i][0] == 'l') bins[11]++;\n\t\tif(s[i][0] =='m') bins[12]++;\n\t\tif(s[i][0] == 'n') bins[13]++;\n\t\tif(s[i][0] == 'o') bins[14]++;\n\t\tif(s[i][0] == 'p') bins[15]++;\n\t\tif(s[i][0] == 'q') bins[16]++;\n\t\tif(s[i][0] == 'r') bins[17]++;\n\t\tif(s[i][0] =='s') bins[18]++;\n\t\tif(s[i][0] == 't') bins[19]++;\n\t\tif(s[i][0] == 'u') bins[20]++;\n\t\tif(s[i][0] == 'v') bins[21]++;\n\t\tif(s[i][0] == 'w') bins[22]++;\n\t\tif(s[i][0] == 'x') bins[23]++;\n\t\tif(s[i][0] == 'y') bins[24]++;\n\t\tif(s[i][0] == 'z') bins[25]++;\n\t}\n}",
            "//...\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        char firstLetter = s[i][0];\n        bins[firstLetter - 'a']++;\n    }\n}",
            "size_t index;\n    for (auto& word : s) {\n        index = word[0] - 'a';\n        ++bins[index];\n    }\n}",
            "for (std::string const& str : s) {\n        char firstLetter = tolower(str[0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            bins[firstLetter - 'a']++;\n        }\n    }\n}",
            "for (size_t i{0}; i!= s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO:\n}",
            "for (const auto &item : s) {\n        if (!item.empty()) {\n            bins[item[0] - 'a']++;\n        }\n    }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    for (size_t i = 0; i < s.size(); ++i) {\n        size_t firstLetter = s[i][0] - 'a';\n        bins[firstLetter]++;\n    }\n}",
            "// Your code here\n\n}",
            "for (const auto& str : s) {\n        if (str.size() == 0) {\n            continue;\n        }\n        bins[str[0] - 'a']++;\n    }\n}",
            "std::fill(std::begin(bins), std::end(bins), 0);\n    for (auto& s_str : s) {\n        if (!s_str.empty()) {\n            size_t char_ascii_value = static_cast<size_t>(std::tolower(s_str[0]) - 'a');\n            ++bins[char_ascii_value];\n        }\n    }\n}",
            "for (size_t i{}; i < s.size(); ++i)\n        bins[s[i][0] - 'a']++;\n}",
            "for (std::string const& a: s) {\n        for (char c: a) {\n            bins[c - 'a']++;\n        }\n    }\n}",
            "// TODO: Your code here\n   //for(auto &item:bins) {\n   //item = 0;\n   //}\n   std::fill(bins.begin(), bins.end(), 0);\n\n   for(auto &item:s) {\n    bins[item[0] - 'a']++;\n   }\n\n}",
            "for(auto &w : s){\n        int bin = w[0] - 'a';\n        bins[bin]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "hip",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int letter = 'a' + threadIdx.x;\n    int count = 0;\n    for (size_t i = 0; i < N; ++i) {\n        if (s[i][0] == letter) {\n            count++;\n        }\n    }\n    atomicAdd(&bins[letter - 'a'], count);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') bins[c - 'a'] += 1;\n  }\n}",
            "// TODO: your code here\n}",
            "const char *str = s[blockIdx.x];\n  if (*str!= '\\0') {\n    atomicAdd(&bins[*str - 'a'], 1);\n  }\n}",
            "int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  // your code here\n}",
            "// Fill this function in!\n}",
            "// TODO: Implement me\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        char c = s[tid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "int tid = hipThreadIdx_x;\n  if (tid < 26) {\n    bins[tid] = 0;\n  }\n  __syncthreads();\n  if (tid < N) {\n    size_t letter = s[tid][0];\n    if (letter >= 'a' && letter <= 'z') {\n      atomicAdd(&bins[letter - 'a'], 1);\n    }\n  }\n  __syncthreads();\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    char first = s[i][0];\n    if (first >= 'a' && first <= 'z')\n        atomicAdd(&bins[first - 'a'], 1);\n}",
            "size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if(id < N) {\n        bins[tolower(s[id][0]) - 'a']++;\n    }\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    bins[s[i][0]-'a']++;\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    char firstLetter = s[i][0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  int tid = threadIdx.x;\n  int bidx = 0;\n  if (idx < N) {\n    int letter = s[idx][0];\n    bidx = letter - 'a';\n  }\n  atomicAdd(&bins[bidx], 1);\n  __syncthreads();\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        char firstLetter = s[tid][0];\n        if (firstLetter < 'a' || firstLetter > 'z') {\n            firstLetter = '?';\n        }\n        firstLetter -= 'a';\n        atomicAdd(&bins[firstLetter], 1);\n    }\n}",
            "int i = hipThreadIdx_x;\n  size_t letter = tolower(s[i][0]);\n  if (letter < 'a' || letter > 'z')\n    letter = 26;\n  atomicAdd(&bins[letter - 'a'], 1);\n}",
            "// TODO\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        const char c = *s[tid];\n\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// TODO (12.1)\n    size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        if (s[idx][0] >= 'a' && s[idx][0] <= 'z') {\n            atomicAdd(&bins[s[idx][0] - 'a'], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x*blockDim.x;\n    if (tid >= N) return;\n    char c = s[tid][0];\n    if (c >= 'a' && c <= 'z') {\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    char first = s[i][0];\n    if (first >= 'a' && first <= 'z') {\n        atomicAdd(&bins[first-'a'], 1);\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    int first = tolower(s[tid][0]) - 'a';\n    atomicAdd(&(bins[first]), 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    bins[tolower(s[i][0]) - 'a']++;\n  }\n}",
            "const int thread = blockIdx.x * blockDim.x + threadIdx.x;\n  if (thread >= N) return;\n  char c = tolower(s[thread][0]);\n  if (c >= 'a' && c <= 'z')\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "// your code here\n  size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) {\n    return;\n  }\n\n  if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n    atomicAdd(&bins[s[i][0] - 'a'], 1);\n  }\n}",
            "unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= N) {\n    return;\n  }\n  char firstLetter = tolower(s[index][0]);\n  atomicAdd(&(bins[firstLetter - 'a']), 1);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "/*\n     If the alphabet size is 26, you can declare an array to store the output of the kernel\n     as such:\n\n     size_t bins[26] = {0};\n\n     However, it is not allowed to declare array in HIP.\n     Instead, you should declare an array of pointers that point to an element in `bins`.\n\n     size_t *bins[26] = {&bins[0], &bins[1], &bins[2], &bins[3], &bins[4],\n                         &bins[5], &bins[6], &bins[7], &bins[8], &bins[9],\n                         &bins[10], &bins[11], &bins[12], &bins[13], &bins[14],\n                         &bins[15], &bins[16], &bins[17], &bins[18], &bins[19],\n                         &bins[20], &bins[21], &bins[22], &bins[23], &bins[24],\n                         &bins[25]};\n\n     Each thread will be assigned a letter to count.\n   */\n\n   /* TODO: your code here */\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    const char *t = s[i];\n    if (t[0] >= 'a' && t[0] <= 'z') {\n      atomicAdd(&bins[t[0] - 'a'], 1);\n    }\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x+threadIdx.x;\n   if (i >= N) return;\n\n   // The following line of code is an example of how to access the i'th string in the array of strings `s`\n   char first_letter = tolower(s[i][0]);\n\n   if (first_letter >= 'a' && first_letter <= 'z') {\n      atomicAdd(&bins[first_letter - 'a'], 1);\n   }\n}",
            "// TODO: Replace this kernel with something better.\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    // TODO:\n  }\n}",
            "const int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid >= N) return;\n  char c = s[tid][0];\n  if (c >= 'a' && c <= 'z')\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i >= N) {\n        return;\n    }\n\n    char letter = *s[i] - 'a';\n    atomicAdd(&bins[letter], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x*blockDim.x;\n  if (tid < N) {\n    char firstLetter = tolower(s[tid][0]);\n    if (firstLetter < 26)\n      atomicAdd(&bins[firstLetter], 1);\n  }\n}",
            "// Your code here\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n    if (index < N) {\n        int letter = s[index][0] - 'a';\n        atomicAdd(&bins[letter], 1);\n    }\n}",
            "const int i = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if (i < N)\n        bins[s[i][0] - 'a']++;\n}",
            "int idx = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a']++;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        const char *p = s[i];\n        while (*p && *p!='') {\n            bins[tolower(*p) - 'a']++;\n            p++;\n        }\n    }\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int letter = tolower(s[idx][0]) - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "int first = s[blockIdx.x][0];\n    atomicAdd(&bins[first - 'a'], 1);\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    char c = tolower(s[tid][0]);\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "const int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    const char ch = s[tid][0];\n    atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        char c = s[tid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = idx; i < N; i += stride) {\n    const char ch = s[i][0];\n    if (isalpha(ch)) {\n      bins[ch - 'a']++;\n    }\n  }\n}",
            "size_t tIdx = hipThreadIdx_x;\n    if (tIdx < N) {\n        bins[s[tIdx][0] - 'a']++;\n    }\n}",
            "// TODO: implement this function\n}",
            "unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      int idx = s[i][0] - 'a';\n      atomicAdd(&bins[idx], 1);\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    char letter = tolower(s[idx][0]);\n    if (letter >= 'a' && letter <= 'z') {\n        atomicAdd(&bins[letter - 'a'], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Fill in the code\n}",
            "// Your code here\n}",
            "size_t tid = blockIdx.x*blockDim.x+threadIdx.x;\n    if (tid < N) {\n        char c = tolower(s[tid][0]);\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "// Your code here\n  int my_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (my_id < N)\n    bins[s[my_id][0] - 'a']++;\n\n}",
            "const size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    char c = tolower(s[i][0]);\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "unsigned int gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   unsigned int tid = hipThreadIdx_x;\n   if(gid >= N) return;\n\n   // bins is not a pointer to the bins array.\n   // It is a \"reference\" to the bins array.\n   // bins is a 26-element array, so it can be indexed as bins[index].\n   // Here we use the first 26 letters of the alphabet.\n   char firstLetter = s[gid][0];\n   if(firstLetter >= 'a' && firstLetter <= 'z') {\n      atomicAdd(&bins[firstLetter - 'a'], 1);\n   }\n}",
            "int tID = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (tID < N) {\n        int x = s[tID][0] - 'a';\n        atomicAdd(&bins[x], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z')\n      atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "// TODO\n}",
            "/*\n    YOUR CODE HERE\n    */\n\n}",
            "const char *in = s[blockIdx.x];\n  __shared__ char firstLetter[1];\n  if (threadIdx.x == 0)\n    firstLetter[0] = tolower(in[0]);\n  __syncthreads();\n  unsigned int c = (unsigned int)firstLetter[0];\n  atomicAdd(&bins[c - 'a'], 1);\n}",
            "// For each input string, count the number of strings that start with\n  // the same letter as the first letter of that string\n  size_t gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (gid < N) {\n    int l = (int)(tolower(s[gid][0]) - 'a');\n    atomicAdd(&(bins[l]), 1);\n  }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index >= N) return;\n    char c = s[index][0];\n    bins[(c - 'a')]++;\n}",
            "const int IDX = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n    if (IDX < N) {\n        int letter = s[IDX][0] - 'a';\n        atomicAdd(&bins[letter], 1);\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        int firstLetter = tolower(s[tid][0]);\n        assert(firstLetter >= 'a' && firstLetter <= 'z');\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char first = s[tid][0];\n    if ('a' <= first && first <= 'z') {\n      atomicAdd(&bins[first - 'a'], 1);\n    }\n  }\n}",
            "// your code here\n}",
            "// TODO: implement this function.\n  size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    char c = tolower(s[tid][0]);\n    atomicAdd(&(bins[c - 'a']), 1);\n  }\n}",
            "// TODO\n}",
            "// TODO: implement me\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i < N) {\n    const char *str = s[i];\n    size_t firstLetter = tolower(str[0]) - 'a';\n    atomicAdd(&bins[firstLetter], 1);\n  }\n}",
            "// TODO\n}",
            "int tid = blockDim.x*blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    bins[s[tid][0]-'a']++;\n  }\n}",
            "int index = threadIdx.x;\n    int threadNum = blockDim.x;\n    int letter = 'a';\n    for (int i = 0; i < N; i++) {\n        if (s[index]!= 0) {\n            if (s[index][0] == letter) {\n                atomicAdd(&bins[letter - 'a'], 1);\n            }\n            index = (index + 1) % threadNum;\n        }\n    }\n}",
            "const char c = s[hipBlockIdx_x][0];\n  // TODO:\n}",
            "// Calculate the index of this thread in the array\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // If this thread falls within the array bounds, process its value\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i < N) {\n    // TODO\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char firstLetter = tolower(s[tid][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}",
            "unsigned int i = threadIdx.x;\n  unsigned int letterIndex = (unsigned int)(s[i][0] - 'a');\n  atomicAdd(&(bins[letterIndex]), 1);\n}",
            "for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z')\n      atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "// TODO\n\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gid < N) {\n        if ('a' <= s[gid][0] && s[gid][0] <= 'z') {\n            atomicAdd(&bins[s[gid][0] - 'a'], 1);\n        }\n    }\n}",
            "// get the index of the letter\n   // convert the alphabet from ASCII to integer\n   // the offset of 'a' is 97\n   const char *letter = s[blockIdx.x] + threadIdx.x;\n   const size_t letterIndex = tolower(*letter) - 97;\n\n   // atomically increment the corresponding bin of the letter\n   atomicAdd(bins + letterIndex, 1);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) {\n    return;\n  }\n  char letter = s[tid][0];\n  if (letter >= 'a' && letter <= 'z') {\n    atomicAdd(&bins[letter - 'a'], 1);\n  }\n}",
            "int thid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (thid >= N) return;\n\n  if (s[thid][0] >= 'a' && s[thid][0] <= 'z') bins[s[thid][0] - 'a']++;\n}",
            "size_t index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (index < N) {\n      char c = s[index][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "size_t idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  char c = tolower(s[idx][0]);\n  atomicAdd(&(bins[c-'a']), 1);\n}",
            "// TO DO\n\n}",
            "//...\n}",
            "int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (gid < N) {\n        char first = tolower(s[gid][0]);\n        if (first < 'a' || first > 'z') first = 'a';\n        atomicAdd(bins + first - 'a', 1);\n    }\n}",
            "// TODO:\n    // Replace this statement with your own code\n    int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if(idx<N){\n        char first_letter = *(s[idx]);\n        if(first_letter >= 97 && first_letter <= 122){\n            atomicAdd(&bins[first_letter - 97], 1);\n        }\n    }\n}",
            "// TODO: Fill this in\n\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    int letter = tolower(s[id][0]);\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "const int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    int letter = s[tid][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= N) {\n        return;\n    }\n\n    char f = tolower(s[i][0]);\n    atomicAdd(&bins[f - 'a'], 1);\n}",
            "// TODO: Implement this function\n}",
            "// your code goes here\n\n}",
            "const char *letter = s[blockIdx.x];\n  if (letter[0] >= 'a' && letter[0] <= 'z') {\n    atomicAdd(&bins[letter[0] - 'a'], 1);\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        char c = tolower(s[i][0]);\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n    const char *str = s[tid];\n    int letter = str[0];\n    atomicAdd(&bins[letter - 'a'], 1);\n}",
            "size_t id = threadIdx.x;\n    size_t pos = 0;\n    size_t letter = tolower((unsigned char)s[id][pos]);\n    while (letter < 'a' || letter > 'z') {\n        if (letter == '\\0') return;\n        pos++;\n        letter = tolower((unsigned char)s[id][pos]);\n    }\n    atomicAdd(bins + letter - 'a', 1);\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (gid >= N) {\n    return;\n  }\n  char c = tolower(s[gid][0]);\n  assert(c >= 'a' && c <= 'z');\n  atomicAdd(&bins[c - 'a'], 1);\n}",
            "}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        size_t k = tolower(s[i][0]) - 'a';\n        atomicAdd(&bins[k], 1);\n    }\n}",
            "// TODO: Replace this with your code.\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        char c = tolower(s[tid][0]);\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "int i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    char c = tolower(s[tid][0]);\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t c = s[i][0] - 'a';\n        atomicAdd(&bins[c], 1);\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char ch = tolower(s[tid][0]);\n    atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "// TODO: Implement\n    const char **local_s = s + blockIdx.x * blockDim.x + threadIdx.x;\n\n    size_t letter = tolower(**local_s) - 'a';\n    atomicAdd(&bins[letter], 1);\n}",
            "// Define the shared memory to be used by the threads in the block\n  // (1.1) The shared memory is defined in the kernel\n  // (1.2) The shared memory is used to hold the elements from the input array to be processed by each block\n  // (1.3) The shared memory can be accessed with the same syntax as arrays\n  // (1.4) The shared memory can be initialized with the same syntax as arrays\n  extern __shared__ char smem[];\n  // Define the index of the current thread in the block\n  // (2.1) Index of the thread in the block is computed as `threadIdx.x + blockIdx.x * blockDim.x`\n  size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  // Define the index of the current thread in the shared memory\n  // (3.1) Index of the thread in the shared memory is computed as `threadIdx.x`\n  size_t si = threadIdx.x;\n  // Load the element of the input array to the shared memory\n  // (4.1) The shared memory is accessed with the same syntax as arrays\n  // (4.2) The index of the element in the shared memory is computed as `si`\n  smem[si] = s[i][0];\n  __syncthreads();\n  // Increment the bin corresponding to the first letter of the string\n  // (5.1) The bin corresponding to the first letter is accessed as `smem[0]`\n  atomicAdd(&bins[smem[0]-'a'], 1);\n}",
            "unsigned int letter = s[hipBlockIdx_x][0] - 'a';\n  atomicAdd(&bins[letter], 1);\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      size_t c = s[i][0] - 'a';\n      atomicAdd(&bins[c], 1);\n   }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) return;\n\n    bins[s[tid][0] - 'a']++;\n}",
            "}",
            "size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (gid < N) {\n    char c = tolower(s[gid][0]);\n    if (c >= 'a' && c <= 'z')\n      atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "/* Your code here */\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    if (s[idx]!= nullptr) {\n      if ((*s[idx])!= '\\0') {\n        bins[tolower((*s[idx])) - 'a'] += 1;\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i >= N) return;\n  char c = *s[i];\n  if (c >= 'a' && c <= 'z') {\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        char letter = s[idx][0];\n        if (letter >= 'a' && letter <= 'z') {\n            atomicAdd(&bins[letter - 'a'], 1);\n        }\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        bins[s[i][0]-'a']++;\n    }\n}",
            "unsigned int tid = threadIdx.x + blockDim.x*blockIdx.x;\n  if (tid < N) {\n    unsigned char first = s[tid][0];\n    if (first >= 'a' && first <= 'z') {\n      atomicAdd(&(bins[first - 'a']), 1);\n    }\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    size_t c = (size_t)(s[i][0]) - 97;\n    atomicAdd(&bins[c], 1);\n  }\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x*blockDim.x;\n  if (tid < N) {\n    bins[s[tid][0]-'a']++;\n  }\n}",
            "//TODO: implement\n   printf(\"This is the function that I need to change\\n\");\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    bins[(unsigned char)s[i][0] - 'a']++;\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        int bin = s[index][0] - 'a';\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "const int bin = 'a' + threadIdx.x;\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        bins[bin] += (s[i][0] == bin);\n    }\n}",
            "int tid = threadIdx.x;\n   if (tid < N) {\n     size_t firstLetter = (size_t) tolower(s[tid][0]) - (size_t) 'a';\n     atomicAdd(&bins[firstLetter], 1);\n   }\n}",
            "int bin_id = threadIdx.x;\n\n    // This is where the kernel code should be written\n    // Note that only the bin for the first letter of each string is incremented\n    // This is because each thread is responsible for only one string\n    // Therefore, a loop is unnecessary\n    if(s[bin_id][0]!= '\\0'){\n      atomicAdd(&bins[s[bin_id][0] - 'a'], 1);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (idx < N) {\n    char firstLetter = tolower(s[idx][0]);\n    atomicAdd(&bins[firstLetter - 'a'], 1);\n  }\n}",
            "// Create a local copy of bins array and fill it with 0.\n  __shared__ size_t localBins[26];\n  if (threadIdx.x < 26) {\n    localBins[threadIdx.x] = 0;\n  }\n\n  __syncthreads();\n\n  // If this thread is inside the vector bounds, then compute the first letter of the current element,\n  // convert the character to lowercase and use it as an index to update the bins array.\n  if (blockIdx.x * blockDim.x + threadIdx.x < N) {\n    const char *current_str = s[blockIdx.x * blockDim.x + threadIdx.x];\n    char first_letter = current_str[0];\n    if (first_letter >= 'a' && first_letter <= 'z') {\n      first_letter = first_letter - 'a';\n      atomicAdd(&localBins[first_letter], 1);\n    }\n  }\n\n  __syncthreads();\n\n  // Copy the local bins array to global bins array\n  if (threadIdx.x < 26) {\n    bins[threadIdx.x] = localBins[threadIdx.x];\n  }\n}",
            "int globalThreadIndex = blockIdx.x * blockDim.x + threadIdx.x;\n  if (globalThreadIndex >= N) return;\n\n  // Your code goes here\n  //...\n\n}",
            "/*\n    Your code goes here.\n    */\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    const char* p = s[idx];\n    if (p) {\n      bins[toLower(p[0])]++;\n    }\n  }\n}",
            "//\n    //\n    //\n\n    // TODO: Replace the line below by an expression that computes the index in the `bins` array for each thread.\n    // The index should correspond to the first letter of the string whose pointer is in the `s` array at index `index`\n\n    int index = 0;\n\n    // TODO: Replace the line below by an expression that increments the element of the `bins` array at index `index` by 1.\n\n    atomicAdd(&bins[index], 1);\n}",
            "/*\n     TODO\n   */\n}",
            "// Your code here\n  int threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  if (threadId < N) {\n    int count = 0;\n    if (s[threadId][0]!= '\\0') {\n      if (s[threadId][0] >= 'a' && s[threadId][0] <= 'z') {\n        int idx = s[threadId][0] - 'a';\n        bins[idx]++;\n      }\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  char c = tolower(s[tid][0]);\n  assert(c >= 'a' && c <= 'z');\n  atomicAdd(&bins[c - 'a'], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char first = tolower(s[i][0]);\n    if (first >= 'a' && first <= 'z') {\n      atomicAdd(&bins[first - 'a'], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "size_t id = threadIdx.x;\n  size_t numThreads = blockDim.x;\n  size_t offset = numThreads * blockIdx.x;\n  if (id < N) {\n    if (s[id][0] >= 'a' && s[id][0] <= 'z') {\n      atomicAdd(&bins[s[id][0] - 'a'], 1);\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t nLetters = 26;\n\n    if (tid < nLetters) {\n        for (int i = 0; i < N; i++) {\n            if (s[i][0] == 'a' + tid) {\n                atomicAdd(&bins[tid], 1);\n            }\n        }\n    }\n}",
            "int my_id = threadIdx.x + blockIdx.x * blockDim.x;\n    if (my_id < N) {\n        char ch = *s[my_id];\n        if (ch >= 'a' && ch <= 'z') {\n            atomicAdd(&bins[ch - 'a'], 1);\n        }\n    }\n}",
            "// TODO\n    size_t index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (index < N) {\n        int firstLetter = tolower(s[index][0]);\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "// Implement the kernel here\n}",
            "int id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (id < N) {\n    if (s[id][0] >= 'a' && s[id][0] <= 'z') {\n      atomicAdd(&bins[s[id][0] - 'a'], 1);\n    }\n  }\n}",
            "// TODO\n  // Get a thread-unique index\n  // Index of the first letter in the string\n  // Atomic add to bins[letter] for that thread-unique index\n\n  // Get a thread-unique index\n  size_t index = threadIdx.x + blockDim.x*blockIdx.x;\n\n  // If index is less than the number of strings\n  if (index < N) {\n    // Get the first letter in the string\n    char letter = s[index][0];\n    // Add 1 to bins[letter]\n    atomicAdd(&bins[letter-'a'], 1);\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    int bin = s[tid][0] - 'a';\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    bins[s[index][0] - 'a'] += 1;\n  }\n}",
            "// Complete the kernel code to implement the letter count.\n    // You can use an array of 26 counters or a single counter.\n    // You can also use the __syncthreads function.\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int counter = 0;\n    if(index < N)\n    {\n        if((int)s[index][0] >= 0 && (int)s[index][0] <= 25)\n        {\n            atomicAdd(bins+s[index][0], 1);\n        }\n    }\n}",
            "int tID = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n\n  if (tID < N) {\n    char ch = tolower(s[tID][0]);\n    atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    char c = s[tid][0];\n    if (c >= 'a' && c <= 'z') {\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    for (int i = 0; i < 26; i++) {\n      if (s[idx][0] == 'a' + i) {\n        atomicAdd(&bins[i], 1);\n      }\n    }\n  }\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n\n   if (i < N) {\n      bins[s[i][0] - 'a']++;\n   }\n}",
            "/* Add you kernel code here */\n}",
            "// TODO: Implement this function.\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        char c = tolower(s[i][0]);\n        atomicAdd(bins + c, 1);\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  if (tid < N) {\n    char letter = tolower(s[tid][0]);\n    if (letter >= 'a' && letter <= 'z') {\n      atomicAdd(&bins[letter - 'a'], 1);\n    }\n  }\n}",
            "size_t threadID = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadID >= N) return;\n  bins[s[threadID][0] - 'a']++;\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        int x = s[i][0];\n        if (x >= 'a' && x <= 'z') {\n            atomicAdd(&bins[x - 'a'], 1);\n        }\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  char first = s[tid][0];\n  atomicAdd(&(bins[first - 'a']), 1);\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        char c = s[tid][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO: compute the index of the current thread (the letter) and the count of elements that start with that letter\n  size_t i = threadIdx.x;\n  size_t count = 0;\n  for (size_t j = 0; j < N; j++) {\n    if (s[j][0] == i+'a') {\n      count++;\n    }\n  }\n  bins[i] = count;\n}",
            "int letterIndex = threadIdx.x;\n  for (int i = 0; i < N; ++i) {\n    if (s[i][0] == 'a' + letterIndex) {\n      atomicAdd(&bins[letterIndex], 1);\n    }\n  }\n}",
            "int idx = blockDim.x*blockIdx.x + threadIdx.x;\n  if (idx >= N) return;\n\n  if (s[idx][0] == 'a')\n    atomicAdd(&(bins[0]), 1);\n  else if (s[idx][0] == 'b')\n    atomicAdd(&(bins[1]), 1);\n  else if (s[idx][0] == 'c')\n    atomicAdd(&(bins[2]), 1);\n  else if (s[idx][0] == 'd')\n    atomicAdd(&(bins[3]), 1);\n  else if (s[idx][0] == 'e')\n    atomicAdd(&(bins[4]), 1);\n  else if (s[idx][0] == 'f')\n    atomicAdd(&(bins[5]), 1);\n  else if (s[idx][0] == 'g')\n    atomicAdd(&(bins[6]), 1);\n  else if (s[idx][0] == 'h')\n    atomicAdd(&(bins[7]), 1);\n  else if (s[idx][0] == 'i')\n    atomicAdd(&(bins[8]), 1);\n  else if (s[idx][0] == 'j')\n    atomicAdd(&(bins[9]), 1);\n  else if (s[idx][0] == 'k')\n    atomicAdd(&(bins[10]), 1);\n  else if (s[idx][0] == 'l')\n    atomicAdd(&(bins[11]), 1);\n  else if (s[idx][0] =='m')\n    atomicAdd(&(bins[12]), 1);\n  else if (s[idx][0] == 'n')\n    atomicAdd(&(bins[13]), 1);\n  else if (s[idx][0] == 'o')\n    atomicAdd(&(bins[14]), 1);\n  else if (s[idx][0] == 'p')\n    atomicAdd(&(bins[15]), 1);\n  else if (s[idx][0] == 'q')\n    atomicAdd(&(bins[16]), 1);\n  else if (s[idx][0] == 'r')\n    atomicAdd(&(bins[17]), 1);\n  else if (s[idx][0] =='s')\n    atomicAdd(&(bins[18]), 1);\n  else if (s[idx][0] == 't')\n    atomicAdd(&(bins[19]), 1);\n  else if (s[idx][0] == 'u')\n    atomicAdd(&(bins[20]), 1);\n  else if (s[idx][0] == 'v')\n    atomicAdd(&(bins[21]), 1);\n  else if (s[idx][0] == 'w')\n    atomicAdd(&(bins[22]), 1);\n  else if (s[idx][0] == 'x')\n    atomicAdd(&(bins[23]), 1);\n  else if (s[idx][0] == 'y')\n    atomicAdd(&(bins[24]), 1);\n  else if (s[idx][0] == 'z')\n    atomicAdd(&(bins[25]), 1);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        int index = s[tid][0] - 'a';\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "// use the block ID to index into the array of bins\n  int idx = blockIdx.x;\n  // use the thread ID to index into the array of strings\n  int tid = threadIdx.x;\n\n  // check if this thread is still valid\n  if (tid < N) {\n    // get the first letter of the string\n    char c = tolower(s[tid][0]);\n\n    // convert the letter into an index\n    int letterIndex = c - 'a';\n\n    // increment the bin if it matches the letter at the given index\n    atomicAdd(&bins[letterIndex], 1);\n  }\n}",
            "// Each thread takes care of one string\n    int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        // Compute the first letter and count it\n        char ch = tolower(*s[tid]);\n        atomicAdd(&bins[ch - 'a'], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    bins[(int)s[idx][0] - (int)'a']++;\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a'] += 1;\n    }\n}",
            "int index = hipThreadIdx_x;\n    char firstChar = s[index][0];\n    bins[(int) firstChar - 'a'] = 0;\n    // TODO: implement this function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = s[i][0] - 'a';\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int id = blockIdx.x*blockDim.x + threadIdx.x;\n  char c;\n  if (id < N) {\n    c = s[id][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c-'a'], 1);\n    }\n  }\n}",
            "// TODO: use AMD HIP to compute the histogram of the first letters of the words in vector s.\n  // You can use bins[i] to store the number of words in s that begin with the letter in position i\n  // in the alphabet.\n\n  // TODO: use AMD HIP to compute the histogram of the first letters of the words in vector s.\n  // You can use bins[i] to store the number of words in s that begin with the letter in position i\n  // in the alphabet.\n  // Fill in the code to use AMD HIP to launch a parallel reduction kernel, the kernel will compute\n  // the histogram of the first letters of the words in vector s. You can use bins[i] to store the\n  // number of words in s that begin with the letter in position i in the alphabet.\n  //\n  // HINT:\n  // - Use hipDeviceSynchronize() to synchronize all work in the device.\n  // - Use hipLaunchKernelGGL() to launch the kernel.\n  // - Use __syncthreads() to synchronize the block.\n  // - Use blockIdx.x and blockDim.x to compute a prefix sum.\n  // - Use threadIdx.x and threadDim.x to loop over the words in the vector s.\n  // - Use a single thread to compute the total number of words in the vector s.\n\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n  }\n}",
            "int index = blockDim.x*blockIdx.x + threadIdx.x;\n    int letterIndex = s[index][0] - 'a';\n    atomicAdd(&bins[letterIndex], 1);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    char letter = tolower(s[tid][0]);\n    assert('a' <= letter && letter <= 'z');\n    atomicAdd(bins + letter - 'a', 1);\n  }\n}",
            "size_t i = threadIdx.x;\n  if (i < 26) {\n    size_t count = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (s[j][0] == 'a' + i) count++;\n    }\n    bins[i] = count;\n  }\n}",
            "for (size_t i = 0; i < 26; i++) bins[i] = 0;\n  char* ch = *s;\n  char* ch1 = \"abcdefghijklmnopqrstuvwxyz\";\n  for (int i = 0; i < N; i++) {\n    if (ch[0] == ch1[i])\n      atomicAdd(&bins[i], 1);\n  }\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    int firstLetter = s[tid][0] - 'a';\n    atomicAdd(bins + firstLetter, 1);\n  }\n}",
            "// Get the index of the current thread\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check to make sure we do not go out of bounds\n  if (idx < N) {\n\n    // Get the first character of the current string\n    char c = s[idx][0];\n\n    // Convert the character to an index in the array\n    int index = (int)(c - 'a');\n\n    // Add one to the correct bin\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "const char *str = s[blockIdx.x];\n    int firstLetter = (int)str[0];\n    if (firstLetter >= 0 && firstLetter < 26)\n        atomicAdd(&bins[firstLetter], 1);\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  //...\n}",
            "for(size_t i = 0; i < N; ++i) {\n        size_t bin = s[i][0] - 'a';\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = start; i < N; i += stride) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    bins[s[tid][0] - 'a']++;\n  }\n}",
            "// TODO\n    // ***********************************************************************\n    size_t index = blockIdx.x*blockDim.x + threadIdx.x;\n    if(index<N){\n        int first = s[index][0];\n        if(first>=97&&first<=122){\n            bins[first-97]++;\n        }\n    }\n}",
            "unsigned int id = blockDim.x * blockIdx.x + threadIdx.x;\n  if(id < N) {\n    bins[s[id][0] - 'a']++;\n  }\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (index < N) {\n    bins[s[index][0] - 'a']++;\n  }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    bins[s[idx][0] - 'a']++;\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N)\n        return;\n\n    // TODO: insert code here\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        bins[to_bin(s[idx][0])] += 1;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: Your code here\n\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0){\n      int num_strings = s.size();\n      int num_per_proc = num_strings/size;\n      int remainder = num_strings % size;\n\n      int displacement = 0;\n      int total_displacement = 0;\n      for(int i = 0; i < size; i++){\n          if(i == 0)\n          {\n              MPI_Send(&num_strings, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n          }\n          else\n          {\n              MPI_Send(&num_per_proc, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n          }\n          MPI_Send(&displacement, 1, MPI_INT, i, 1, MPI_COMM_WORLD);\n          displacement += num_per_proc + (i < remainder? 1 : 0);\n      }\n  }\n  else{\n      int num_strings;\n      int my_displacement;\n      MPI_Recv(&num_strings, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&my_displacement, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      std::vector<std::string> my_s(num_strings);\n      MPI_Scatterv(&s[0], &num_strings, &my_displacement, MPI_CHAR, &my_s[0], num_strings, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n      for(int i = 0; i < num_strings; i++){\n          bins[my_s[i][0] - 'a'] += 1;\n      }\n\n      int num_my_bins;\n      for(int i = 0; i < 26; i++){\n          if(bins[i]!= 0){\n              num_my_bins++;\n          }\n      }\n\n      MPI_Send(&num_my_bins, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(&bins, 26, MPI_INT, 0, 1, MPI_COMM_WORLD);\n  }\n  if (rank == 0){\n      int total_num_strings = s.size();\n      int num_per_proc = total_num_strings/size;\n      int remainder = total_num_strings % size;\n\n      int displacement = 0;\n      int total_displacement = 0;\n      for(int i = 0; i < size; i++){\n          int num_my_bins;\n          MPI_Recv(&num_my_bins, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n          std::array<size_t, 26> my_bins;\n          MPI_Recv(&my_bins, 26, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n          for(int j = 0; j < 26; j++){\n              if(my_bins[j]!= 0){\n                  bins[j] += my_bins[j];\n              }\n          }\n      }\n  }\n}",
            "// TODO\n}",
            "int size;\n    int rank;\n\n    // Get the number of processes\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Get the rank of the process\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (std::string string : s) {\n            // Increment the count for the first letter in the string\n            bins[toupper(string.at(0)) - 'A']++;\n        }\n\n        for (int i = 1; i < size; i++) {\n            // Receive the counts from the rank i\n            MPI_Recv(&bins, 26, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            // Add the counts to the total\n            for (int j = 0; j < 26; j++) {\n                bins[j] += bins[j];\n            }\n        }\n    } else {\n        std::array<size_t, 26> counts;\n\n        for (std::string string : s) {\n            // Increment the count for the first letter in the string\n            counts[toupper(string.at(0)) - 'A']++;\n        }\n\n        // Send the counts to the rank 0\n        MPI_Send(&counts, 26, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "//TODO: Implement this\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  for (std::string word : s) {\n    char c = word[0];\n    if (c >= 'a' && c <= 'z') {\n      int bin = c - 'a';\n      bins[bin]++;\n    }\n  }\n\n  if (rank == 0) {\n    // Print the result in rank 0\n    std::cout << \"firstLetterCounts: \" << std::endl;\n    for (int i = 0; i < 26; i++) {\n      std::cout << bins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "int rank;\n    int num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // TODO: Fill this in.\n}",
            "//...\n}",
            "// Your code here.\n}",
            "const int rank = 0;\n  const int root = 0;\n  const int numProcs = 1;\n\n  for(int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n  for(auto str : s) {\n    bins[str[0] - 'a']++;\n  }\n  //std::cout << rank << \" \" << bins << std::endl;\n  MPI_Reduce(&bins, 0, 26, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n  //std::cout << rank << \" \" << bins << std::endl;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  std::vector<std::string> subvector;\n\n  // First, each rank takes a subvector of `s`\n  // Note: it is possible for some of the subvectors to be empty\n  // Note: the last rank will have an extra element\n  if (rank == 0) {\n    for (size_t i = 0; i < s.size() - 1; i += size) {\n      subvector.push_back(s[i]);\n    }\n  } else {\n    for (size_t i = rank; i < s.size() - 1; i += size) {\n      subvector.push_back(s[i]);\n    }\n  }\n\n  // Second, each rank computes the first letter counts for its subvector\n  std::array<size_t, 26> counts;\n  for (size_t i = 0; i < 26; i++) {\n    counts[i] = 0;\n  }\n\n  for (auto const& str : subvector) {\n    counts[str[0] - 'a']++;\n  }\n\n  // Third, rank 0 collects the results from all the ranks\n  std::array<size_t, 26> results;\n  if (rank == 0) {\n    for (size_t i = 0; i < size; i++) {\n      // Note: the MPI_Reduce function is a bit hard to use here\n      // It's probably easier to use MPI_Allgather\n      MPI_Gather(&counts, 26, MPI_UNSIGNED_LONG, &results, 26, MPI_UNSIGNED_LONG, 0, comm);\n    }\n  } else {\n    MPI_Gather(&counts, 26, MPI_UNSIGNED_LONG, &results, 26, MPI_UNSIGNED_LONG, 0, comm);\n  }\n\n  // Finally, rank 0 fills the output array with the results\n  if (rank == 0) {\n    for (size_t i = 0; i < 26; i++) {\n      bins[i] = results[i];\n    }\n  }\n}",
            "// TODO\n\n    // Find the count for each letter in rank 0, and sum the counts for each letter\n    // The counts from all ranks are summed up and put in bins on rank 0\n    //\n    // Hint: Use MPI_Reduce to sum up the counts from all ranks\n    //\n    // Hint: Use MPI_Scatter to scatter the input vector to each rank\n    //\n    // Hint: Use MPI_Allgather to collect the counts from each rank to rank 0\n    //\n    // Hint: For each letter i, the counts are:\n    //         bins[i] = count( s, [0, i + 'a') )\n\n}",
            "int size = s.size();\n  int my_rank = 0;\n  int procs = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  int send_count = size / procs;\n  int send_rem = size % procs;\n\n  int rcount = send_count;\n  int rrem = send_rem;\n\n  int rank;\n  MPI_Status status;\n  if (my_rank!= 0) {\n    std::vector<std::string> send_vec;\n    for (int i = my_rank * send_count; i < my_rank * send_count + send_count; i++) {\n      send_vec.push_back(s.at(i));\n    }\n    if (my_rank < send_rem) {\n      rcount++;\n    }\n    MPI_Send(send_vec.data(), send_vec.size(), MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n  } else {\n    std::vector<std::string> recv_vec(size);\n    for (int i = 0; i < send_count; i++) {\n      MPI_Recv(recv_vec.data(), size, MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n    }\n    if (send_rem > 0) {\n      int rrank;\n      MPI_Recv(recv_vec.data(), size, MPI_CHAR, send_rem, 0, MPI_COMM_WORLD, &status);\n      rrank = status.MPI_SOURCE;\n      for (int i = 0; i < recv_vec.size(); i++) {\n        std::string s_temp = recv_vec.at(i);\n        if (s_temp.at(0) == 'a') {\n          bins.at(0) += 1;\n        } else if (s_temp.at(0) == 'b') {\n          bins.at(1) += 1;\n        } else if (s_temp.at(0) == 'c') {\n          bins.at(2) += 1;\n        } else if (s_temp.at(0) == 'd') {\n          bins.at(3) += 1;\n        } else if (s_temp.at(0) == 'e') {\n          bins.at(4) += 1;\n        } else if (s_temp.at(0) == 'f') {\n          bins.at(5) += 1;\n        } else if (s_temp.at(0) == 'g') {\n          bins.at(6) += 1;\n        } else if (s_temp.at(0) == 'h') {\n          bins.at(7) += 1;\n        } else if (s_temp.at(0) == 'i') {\n          bins.at(8) += 1;\n        } else if (s_temp.at(0) == 'j') {\n          bins.at(9) += 1;\n        } else if (s_temp.at(0) == 'k') {\n          bins.at(10) += 1;\n        } else if (s_temp.at(0) == 'l') {\n          bins.at(11) += 1;\n        } else if (s_temp.at(0) =='m') {\n          bins.at(12) += 1;\n        } else if (s_temp.at(0) == 'n') {\n          bins.at(13) += 1;\n        } else if (s_temp.at(0) == 'o') {\n          bins.at(14) += 1;\n        } else if (s_temp.at(0) == 'p') {\n          bins.at(15) += 1;\n        } else if (s_temp.at(0) == 'q') {\n          bins.at(16) += 1;\n        } else if (s_temp",
            "//...\n}",
            "int rank = 0;\n  int size = 1;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int number_of_strings_per_process = s.size()/size;\n  int rank_of_last_process = size - 1;\n\n  int start_index = rank*number_of_strings_per_process;\n  int end_index = (rank + 1)*number_of_strings_per_process;\n  if (rank == rank_of_last_process) {\n    end_index = s.size();\n  }\n\n  for (int i = start_index; i < end_index; ++i) {\n    bins[s[i].front() - 'a'] += 1;\n  }\n\n  MPI_Reduce(&bins[0], NULL, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Status status;\n      MPI_Recv(&bins[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n    }\n  }\n}",
            "}",
            "int my_rank;\n  int comm_sz;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n  if (my_rank == 0) {\n    for (size_t i=0; i<s.size(); i++) {\n      bins[s[i][0] - 'a'] += 1;\n    }\n  } else {\n    for (size_t i=0; i<s.size(); i++) {\n      MPI_Send(&(s[i][0]), 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  if (my_rank == 0) {\n    int count;\n    MPI_Status status;\n    for (int i=1; i<comm_sz; i++) {\n      MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n      MPI_Get_count(&status, MPI_CHAR, &count);\n      char *c = new char[count];\n      MPI_Recv(c, count, MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n      bins[c[0] - 'a'] += 1;\n    }\n  }\n\n}",
            "}",
            "// Count the number of strings that start with each letter\n    for (size_t i = 0; i < s.size(); ++i) {\n        auto letter = s[i].at(0);\n        if (letter >= 'a' && letter <= 'z') {\n            bins[letter - 'a']++;\n        }\n    }\n}",
            "for (int i = 0; i < s.size(); i++) {\n        int letterIndex = s[i][0] - 'a';\n        bins[letterIndex]++;\n    }\n}",
            "// Your code here\n}",
            "// TODO: Replace this line with your code\n    for (int i = 0; i < 26; i++)\n        bins[i] = 0;\n\n}",
            "// TODO\n}",
            "// You will need to modify this function.\n  // You can only use C++ STL containers that are declared in header <array>\n  // You cannot use iostream or string classes from the STL\n\n  // TODO:\n  // You will need to modify this function.\n  // You can only use C++ STL containers that are declared in header <array>\n  // You cannot use iostream or string classes from the STL\n\n  // TODO:\n  // You will need to modify this function.\n  // You can only use C++ STL containers that are declared in header <array>\n  // You cannot use iostream or string classes from the STL\n}",
            "// YOUR CODE HERE\n}",
            "for(int i=0; i < s.size(); i++){\n    int pos = s[i][0];\n    bins[pos] = bins[pos] + 1;\n  }\n}",
            "/* Your solution goes here */\n}",
            "// You code here!\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t chunkSize = s.size() / size;\n  size_t chunkOffset = rank * chunkSize;\n  if(rank == size - 1)\n    chunkSize = s.size() - (size - 1) * chunkSize;\n\n  for(size_t i = 0; i < chunkSize; i++)\n    bins[s[i+chunkOffset][0] - 'a']++;\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if(rank == 0){\n    for(int i = 1; i < size; i++)\n    {\n      MPI_Status status;\n      MPI_Recv(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, i, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n    }\n  }\n  else{\n    MPI_Send(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code here\n}",
            "}",
            "// TODO\n}",
            "auto rank = 0;\n    auto size = 1;\n    // This is just a dummy call to ensure that MPI has been initialized.\n    // If you remove it, MPI will be initialized when first used.\n    // This is useful for MPI_Init() and MPI_Finalize(), but not for MPI_Comm_rank() and MPI_Comm_size().\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    auto num_strings = s.size();\n    // Compute the range of strings for each rank.\n    // Each rank should have approximately the same amount of strings.\n    auto start = 0;\n    auto end = 0;\n    if (size > 1) {\n        start = num_strings * rank / size;\n        end = num_strings * (rank + 1) / size;\n    } else {\n        start = 0;\n        end = num_strings;\n    }\n\n    // Count the first letters of all strings that belong to this rank.\n    std::array<size_t, 26> localBins{0};\n    for (auto i = start; i < end; ++i) {\n        ++localBins[s[i][0] - 'a'];\n    }\n\n    // Reduce the result on rank 0.\n    // Every rank sends its localBins to rank 0.\n    // Rank 0 will then sum up the results.\n    if (rank == 0) {\n        for (auto i = 1; i < size; ++i) {\n            auto status = MPI_Status();\n            auto counts = std::array<size_t, 26>{};\n            // This is a blocking call.\n            // It will wait until the message from rank i is received.\n            MPI_Recv(counts.data(), 26, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (auto j = 0; j < 26; ++j) {\n                bins[j] += counts[j];\n            }\n        }\n    } else {\n        // Send localBins to rank 0.\n        // This is a non-blocking call.\n        // Rank 0 will receive it as soon as it is ready.\n        auto status = MPI_Status();\n        MPI_Isend(localBins.data(), 26, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    }\n}",
            "// TODO\n}",
            "int n; //number of elements\n    int rank; //rank\n    int p; //number of processes\n    MPI_Comm_size(MPI_COMM_WORLD, &p); //number of processes\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank); //rank\n    int local_n = s.size()/p; //local number of elements\n    int local_start = rank*local_n; //local start element\n    int local_end = local_start + local_n; //local end element\n    std::array<size_t, 26> local_bins;\n    if (rank == 0)\n        n = s.size();\n    if (rank == p-1)\n        local_end = s.size();\n    for (int i = local_start; i < local_end; i++) {\n        char first = s[i][0];\n        if (islower(first))\n            local_bins[first-'a']++;\n    }\n    MPI_Gather(&local_bins[0], 26, MPI_INT, &bins[0], 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "if (s.size() == 0) return;\n\n    // TODO: Your code here\n    MPI_Status status;\n    std::vector<std::string> s_left(0);\n    std::vector<std::string> s_right(0);\n    std::vector<std::string> s_local(0);\n\n    int n, my_rank, n_proc;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_proc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int split_pos;\n    if (my_rank == 0) {\n        n = s.size() / n_proc;\n        split_pos = n;\n    } else {\n        n = s.size() / n_proc;\n        split_pos = (n + 1) * my_rank;\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    s_local = std::vector<std::string>(s.begin() + split_pos, s.begin() + split_pos + n);\n\n    int size_left = split_pos;\n    int size_right = s.size() - split_pos - n;\n    MPI_Sendrecv(&size_left, 1, MPI_INT, 0, 1, &size_right, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    s_left = std::vector<std::string>(s.begin(), s.begin() + split_pos);\n    s_right = std::vector<std::string>(s.begin() + split_pos + n, s.end());\n\n    // TODO: Your code here\n    if (my_rank == 0) {\n        std::vector<std::string> s_merged;\n        s_merged.reserve(s_left.size() + s_local.size() + s_right.size());\n        s_merged.insert(s_merged.end(), s_left.begin(), s_left.end());\n        s_merged.insert(s_merged.end(), s_local.begin(), s_local.end());\n        s_merged.insert(s_merged.end(), s_right.begin(), s_right.end());\n\n        for (int rank = 1; rank < n_proc; ++rank) {\n            std::vector<std::string> temp_s_local(0);\n            int temp_size_left = 0;\n            int temp_size_right = 0;\n            MPI_Recv(&temp_size_left, 1, MPI_INT, rank, 1, MPI_COMM_WORLD, &status);\n            temp_s_local.resize(temp_size_left);\n            MPI_Recv(temp_s_local.data(), temp_size_left, MPI_CHAR, rank, 2, MPI_COMM_WORLD, &status);\n            s_merged.insert(s_merged.end(), temp_s_local.begin(), temp_s_local.end());\n\n            MPI_Send(&temp_size_right, 1, MPI_INT, rank, 1, MPI_COMM_WORLD);\n            MPI_Send(s_right.data(), temp_size_right, MPI_CHAR, rank, 2, MPI_COMM_WORLD);\n        }\n\n        for (const auto &i : s_merged)\n            ++bins[i.front() - 'a'];\n    } else {\n        MPI_Send(&size_left, 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n        MPI_Send(s_local.data(), size_left, MPI_CHAR, 0, 2, MPI_COMM_WORLD);\n        std::vector<std::string> temp_s_right(0);\n        int temp_size_right = 0;\n        MPI_Recv(&temp_size_right, 1, MPI_INT, 0, 1, MPI_COMM_WOR",
            "std::vector<char> firstLetter;\n    int myrank, size;\n    char mychar;\n    MPI_Status status;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    MPI_Request req;\n    MPI_Status status;\n\n    for (int i = 0; i < s.size(); i++) {\n        firstLetter.push_back(s[i][0]);\n    }\n\n    if (myrank == 0) {\n        // Rank 0 initializes bins\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n        // Rank 0 loops through the elements of vector s and increments the corresponding bins\n        for (int i = 0; i < s.size(); i++) {\n            char mychar = firstLetter[i];\n            int index = mychar - 'a';\n            bins[index]++;\n        }\n    }\n\n    // All ranks loop through the elements of firstLetter and send to rank 0\n    for (int i = 0; i < firstLetter.size(); i++) {\n        char mychar = firstLetter[i];\n        MPI_Send(&mychar, 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Rank 0 receives from each rank and adds to the corresponding bins\n    if (myrank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&mychar, 1, MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n            int index = mychar - 'a';\n            bins[index]++;\n        }\n    }\n\n}",
            "}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Your code here\n\n}",
            "// your code here\n\n}",
            "const int rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 26> my_bins;\n  for (int i=0; i < 26; ++i) {\n    my_bins[i] = 0;\n  }\n\n  for (auto const& str: s) {\n    if (str.size() == 0) continue;\n    my_bins[str.at(0) - 'a']++;\n  }\n\n  MPI_Reduce(my_bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Rank 0 has the complete list.\n  size_t rank = 0;\n  int numProcs;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Divide the data evenly over the ranks.\n  size_t numItemsPerProc = s.size() / numProcs;\n  size_t numItemsLastProc = s.size() % numProcs;\n  if (rank == 0) {\n    for (size_t i = 0; i < numItemsPerProc; i++) {\n      bins[s[i][0]-'a']++;\n    }\n  }\n  else {\n    for (size_t i = rank * numItemsPerProc; i < (rank + 1) * numItemsPerProc + numItemsLastProc; i++) {\n      bins[s[i][0]-'a']++;\n    }\n  }\n\n  // Combine the data from all ranks.\n  std::array<size_t, 26> total;\n  std::array<size_t, 26> countsPerProc;\n  countsPerProc.fill(0);\n  for (size_t i = 0; i < numItemsPerProc; i++) {\n    countsPerProc[s[i][0]-'a']++;\n  }\n\n  MPI_Reduce(&countsPerProc, &total, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = total;\n  }\n}",
            "for(auto c : s){\n    bins[c[0]-'a']++;\n  }\n}",
            "for (int i = 0; i < s.size(); i++) {\n    char firstLetter = s[i][0];\n\n    if (firstLetter >= 'a' && firstLetter <= 'z')\n      bins[firstLetter - 'a']++;\n  }\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "//TODO: YOUR CODE HERE\n\n}",
            "// **** Begin your code ****\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  assert(size >= 1);\n\n  /* Compute the number of strings in each chunk */\n  size_t per_rank = s.size() / size;\n  size_t leftover = s.size() % size;\n  size_t my_offset = per_rank * rank;\n  size_t my_size = per_rank + (rank < leftover);\n\n  for (auto i = 0u; i < my_size; ++i) {\n    bins[s[my_offset + i][0] - 'a'] += 1;\n  }\n\n  /* Reduce the counts in bins to rank 0 */\n  if (rank == 0) {\n    std::array<size_t, 26> counts = {};\n    for (auto i = 1u; i < size; ++i) {\n      MPI_Recv(&counts, 26, MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (auto j = 0u; j < 26; ++j) {\n        bins[j] += counts[j];\n      }\n    }\n  }\n  else {\n    MPI_Send(&bins, 26, MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Fill in the code\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int mpi_size, mpi_rank;\n    MPI_Comm_size(comm, &mpi_size);\n    MPI_Comm_rank(comm, &mpi_rank);\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    size_t string_per_process = s.size() / mpi_size;\n    size_t rest = s.size() % mpi_size;\n    size_t begin = mpi_rank * string_per_process;\n    size_t end = (mpi_rank == mpi_size - 1)? begin + string_per_process + rest : begin + string_per_process;\n\n    for (size_t i = begin; i < end; i++) {\n        local_bins[s[i][0] - 'a']++;\n    }\n\n    if (mpi_rank == 0) {\n        bins.fill(0);\n        for (int i = 0; i < mpi_size; i++) {\n            MPI_Status status;\n            MPI_Recv(&local_bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 26; j++)\n                bins[j] += local_bins[j];\n        }\n    } else {\n        MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int size, rank;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n\n    // MPI_Allgatherv\n\n    int globalLength = s.size();\n    int localLength = globalLength / size;\n\n    std::vector<int> sendCounts(size, 0);\n    std::vector<int> displs(size, 0);\n\n    if (rank == 0) {\n        // std::vector<int> sendCounts(size, localLength);\n        // std::vector<int> displs(size, 0);\n        for (int i = 0; i < size - 1; ++i) {\n            displs[i + 1] = displs[i] + sendCounts[i];\n        }\n    }\n\n    MPI_Allgather(&localLength, 1, MPI_INT, &sendCounts[0], 1, MPI_INT, comm);\n    MPI_Allgatherv(s.data(), localLength, MPI_INT, &bins, &sendCounts[0], &displs[0], MPI_INT, comm);\n\n}",
            "// TODO: Your code here!\n}",
            "}",
            "}",
            "// Your code here\n\n  int n = s.size();\n  //int m = s[0].length();\n  int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 26> local_bins = {};\n  //std::array<size_t, 26> local_bins_count = {};\n  int local_count = 0;\n  std::vector<std::string> local_s = {};\n  for (int i = 0; i < n; i++) {\n    if (s[i][0] < 'a' || s[i][0] > 'z') {\n      std::cout << \"Incorrect alphabet, please use only lower case letters!\" << std::endl;\n      return;\n    }\n    if (rank == 0) {\n      local_s.push_back(s[i]);\n    } else if (rank == 1) {\n      if (s[i][0] == 'a') {\n        local_count++;\n      }\n    } else if (rank == 2) {\n      if (s[i][0] == 'b') {\n        local_count++;\n      }\n    } else if (rank == 3) {\n      if (s[i][0] == 'c') {\n        local_count++;\n      }\n    } else if (rank == 4) {\n      if (s[i][0] == 'd') {\n        local_count++;\n      }\n    } else if (rank == 5) {\n      if (s[i][0] == 'e') {\n        local_count++;\n      }\n    } else if (rank == 6) {\n      if (s[i][0] == 'f') {\n        local_count++;\n      }\n    } else if (rank == 7) {\n      if (s[i][0] == 'g') {\n        local_count++;\n      }\n    } else if (rank == 8) {\n      if (s[i][0] == 'h') {\n        local_count++;\n      }\n    } else if (rank == 9) {\n      if (s[i][0] == 'i') {\n        local_count++;\n      }\n    } else if (rank == 10) {\n      if (s[i][0] == 'j') {\n        local_count++;\n      }\n    } else if (rank == 11) {\n      if (s[i][0] == 'k') {\n        local_count++;\n      }\n    } else if (rank == 12) {\n      if (s[i][0] == 'l') {\n        local_count++;\n      }\n    } else if (rank == 13) {\n      if (s[i][0] =='m') {\n        local_count++;\n      }\n    } else if (rank == 14) {\n      if (s[i][0] == 'n') {\n        local_count++;\n      }\n    } else if (rank == 15) {\n      if (s[i][0] == 'o') {\n        local_count++;\n      }\n    } else if (rank == 16) {\n      if (s[i][0] == 'p') {\n        local_count++;\n      }\n    } else if (rank == 17) {\n      if (s[i][0] == 'q') {\n        local_count++;\n      }\n    } else if (rank == 18) {\n      if (s[i][0] == 'r') {\n        local_count++;\n      }\n    } else if (rank == 19) {\n      if (s[i][0] =='s') {\n        local_count++;\n      }\n    } else if (rank == 20) {\n      if (s[i][0] == 't') {\n        local_count++;\n      }\n    } else if (rank == 21) {\n      if (s[i][0] == 'u') {\n        local_count++;\n      }\n    } else if (rank == 2",
            "if (s.empty()) return;\n\n    // TODO\n\n    // Compute bins using s.\n    // Use std::get<0>(*it) to get the first letter of string s.\n    // Use std::get<1>(*it) to get the count.\n    // std::get<0>(*it) should be a char.\n    // Use std::toupper() to convert the character to upper case.\n}",
            "/*\n   * This is the number of characters in the alphabet\n   * All of your code must work correctly for any positive integer alphabetSize\n   * including numbers larger than 26.\n   */\n  const int alphabetSize = 26;\n\n  /*\n   * This is the number of items that will be assigned to\n   * each processor\n   */\n  const int itemsPerProcessor = 4;\n\n  /*\n   * For each processor, store the number of items that\n   * it will be responsible for\n   */\n  int localItems = itemsPerProcessor;\n\n  /*\n   * The rank of this processor\n   * Rank 0 is responsible for storing the final result\n   */\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  /*\n   * The total number of processors\n   */\n  int p;\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  /*\n   * Ensure that the last processor gets any leftover items\n   */\n  if (rank == p - 1) {\n    localItems = s.size() % p;\n  }\n\n  /*\n   * Initialize the array that will hold the counts\n   */\n  std::array<size_t, alphabetSize> localBins;\n  localBins.fill(0);\n\n  /*\n   * If the vector is empty, there is nothing to do\n   */\n  if (s.size() == 0) {\n    return;\n  }\n\n  // TODO: Fill in your code here\n  /*\n  * Iterate through the array to compute local first letter counts\n  * localBins[first letter] = number of times first letter appears\n  */\n  int localStart = rank * localItems;\n  int localEnd = localStart + localItems;\n  if (rank == p - 1) {\n    localEnd = s.size();\n  }\n\n  for (int i = localStart; i < localEnd; i++) {\n    char letter = s[i].at(0);\n    if (isalpha(letter)) {\n      letter = tolower(letter);\n      localBins[letter - 'a']++;\n    }\n  }\n\n  // TODO: Fill in your code here\n  /*\n  * Reduce the local first letter counts into a global array\n  */\n  int binsSize = alphabetSize;\n  std::array<size_t, alphabetSize> globalBins;\n  globalBins.fill(0);\n\n  MPI_Reduce(&localBins, &globalBins, binsSize, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // TODO: Fill in your code here\n  /*\n  * If this is rank 0, copy the counts into the provided array\n  */\n  if (rank == 0) {\n    bins = globalBins;\n  }\n\n}",
            "// TODO\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n    const size_t size = s.size();\n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++) {\n            char c = s[i][0];\n            int bin_id = c - 'a';\n            bins[bin_id]++;\n        }\n    } else {\n        int rank_id = 1;\n        while (rank_id < num_procs) {\n            for (size_t i = rank_id * (size / num_procs); i < (rank_id + 1) * (size / num_procs); i++) {\n                char c = s[i][0];\n                int bin_id = c - 'a';\n                bins[bin_id]++;\n            }\n            rank_id++;\n        }\n    }\n}",
            "// Your code goes here\n  for (int i = 0; i < s.size(); i++) {\n    for (int j = 0; j < 26; j++) {\n      if (s[i][0] == 'a' + j) {\n        bins[j] = bins[j] + 1;\n      }\n    }\n  }\n\n}",
            "// Your code goes here\n  // Rank 0 will have the correct answer in bins\n\n}",
            "int worldSize, myRank;\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n  int elementsPerRank = (s.size() + worldSize - 1) / worldSize;\n  int startIdx = myRank * elementsPerRank;\n  int endIdx = (myRank + 1) * elementsPerRank;\n  if (endIdx > s.size()) {\n    endIdx = s.size();\n  }\n  std::vector<std::string> myData(s.begin() + startIdx, s.begin() + endIdx);\n\n  // Your code here\n  // This is a stub\n\n  MPI_Reduce(myData, bins, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int size, rank;\n    int const tag = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (auto &word: s) {\n            ++bins[word[0] - 'a'];\n        }\n    } else {\n        std::vector<std::string> chunk;\n        for (auto it = s.begin() + rank * (s.size() / size); it < s.begin() + (rank + 1) * (s.size() / size); ++it) {\n            chunk.push_back(*it);\n        }\n\n        for (auto &word: chunk) {\n            ++bins[word[0] - 'a'];\n        }\n\n        for (int i = 0; i < 26; i++) {\n            MPI_Send(&bins[i], 1, MPI_INT, 0, tag, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            for (int j = 0; j < 26; j++) {\n                MPI_Recv(&bins[j], 1, MPI_INT, i, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n        }\n    }\n\n    return;\n}",
            "// Your code here\n\n    bins.fill(0);\n    for(auto str: s)\n    {\n        bins[str[0] - 'a']++;\n    }\n}",
            "// your code here\n\n}",
            "auto const n_bins = s.size();\n    int const rank = 0;\n    int const n_ranks = 1;\n    MPI_Bcast(&n_bins, 1, MPI_UNSIGNED_LONG_LONG, rank, MPI_COMM_WORLD);\n\n    auto const per_rank = n_bins / n_ranks;\n    auto const remainder = n_bins % n_ranks;\n    auto const begin_at = rank * per_rank + std::min(rank, remainder);\n    auto const end_at = (rank + 1) * per_rank + std::min(rank + 1, remainder);\n    auto const n_local = end_at - begin_at;\n    std::array<size_t, 26> local_bins{};\n    for (size_t i = 0; i < n_local; ++i) {\n        auto const local_index = i + begin_at;\n        auto const first = s[local_index][0];\n        if ('a' <= first && first <= 'z') {\n            ++local_bins[first - 'a'];\n        }\n    }\n    MPI_Reduce(&local_bins, &bins, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n}",
            "int N = s.size();\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<std::string> s_local;\n    std::vector<int> bins_local(26,0);\n\n    // Split the vector in blocks and assign to each rank\n    int N_per_process = N / size;\n    int remainder = N % size;\n\n    int start = rank*N_per_process;\n    int end = start + N_per_process;\n    if(rank==0)\n        end += remainder;\n    else if(rank == size - 1)\n        end += (N-end);\n\n    for(int i=start;i<end;i++){\n        s_local.push_back(s[i]);\n    }\n\n    // Fill the bins of each process\n    for(auto e : s_local){\n        char first = e[0];\n        bins_local[first-'a']++;\n    }\n\n    // Merge the bins\n    int recvcounts[size];\n    int displs[size];\n\n    for(int i=0; i<size;i++){\n        recvcounts[i] = N_per_process;\n        if(i==0)\n            displs[i] = 0;\n        else\n            displs[i] = displs[i-1] + recvcounts[i-1];\n        if(rank == 0){\n            recvcounts[i] += (remainder > 0)? 1 : 0;\n            displs[i] -= (remainder > 0)? 1 : 0;\n            remainder--;\n        }\n    }\n\n    int sum;\n    MPI_Reduce_scatter(bins_local.data(), bins.data(), recvcounts, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t num_strings = s.size();\n\n    std::vector<size_t> bin_sizes(26, 0);\n\n    // compute local counts\n    for(size_t i = 0; i < num_strings; i++) {\n        char first_letter = s[i].at(0);\n        bin_sizes.at(first_letter - 'a')++;\n    }\n\n    // find total size of bins array\n    int bin_size;\n    MPI_Allreduce(&bin_sizes.at(0), &bin_size, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // fill out bins array\n    int bin_count[26] = {0};\n    for(int i = 0; i < 26; i++) {\n        bins[i] = bin_count[i];\n        bin_count[i] += bin_sizes.at(i);\n    }\n\n    // count bins\n    for(size_t i = 0; i < num_strings; i++) {\n        char first_letter = s[i].at(0);\n        bins.at(first_letter - 'a')++;\n    }\n\n    // find total size of bins array\n    int bin_size_total;\n    MPI_Allreduce(&bin_size, &bin_size_total, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n}",
            "// TODO\n\n}",
            "for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  size_t my_start = rank*s.size()/size;\n  size_t my_end = (rank + 1)*s.size()/size;\n  for (size_t i = my_start; i < my_end; i++) {\n    size_t l = s[i].size() > 0? s[i][0] - 'a' : 26;\n    bins[l]++;\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < bins.size(); i++) {\n      printf(\"%c: %lu\\n\", 'a' + i, bins[i]);\n    }\n  }\n}",
            "const size_t size = s.size();\n    size_t count = 0;\n\n    // TODO: implement\n    MPI_Reduce(s.data(),bins.data(),size,MPI_STRING,MPI_SUM,0,MPI_COMM_WORLD);\n}",
            "// TODO: Implement me\n\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<std::string> s_local(s.size() / size);\n  if (rank == 0)\n    std::copy(s.begin(), s.begin() + s.size() / size, s_local.begin());\n  else\n    std::copy(s.begin() + s.size() / size * rank,\n              s.begin() + s.size() / size * (rank + 1),\n              s_local.begin());\n  for (auto const& str : s_local)\n    bins[str[0] - 'a']++;\n  if (rank == 0) {\n    std::vector<size_t> bins_all(26 * size);\n    MPI_Gather(bins.data(), 26, MPI_INT, bins_all.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < size; i++)\n      std::copy(bins_all.begin() + 26 * i, bins_all.begin() + 26 * (i + 1), bins.begin());\n  } else {\n    MPI_Gather(bins.data(), 26, MPI_INT, NULL, 26, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n\n}",
            "int num_proc;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int n = s.size();\n    const int block_size = n/num_proc;\n    const int remain = n%num_proc;\n    const int begin = rank*block_size + std::min(rank, remain);\n    const int end = begin + block_size + (rank < remain? 1 : 0);\n\n    const std::array<char, 26> alphabet = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l','m', 'n', 'o', 'p', 'q', 'r','s', 't', 'u', 'v', 'w', 'x', 'y', 'z'};\n    for (char c : alphabet) {\n        size_t count = 0;\n        for (int i = begin; i < end; ++i) {\n            count += (s[i][0] == c);\n        }\n        bins[c - 'a'] = count;\n    }\n\n    // Reduce the partial counts\n    std::array<size_t, 26> local_bins;\n    if (rank == 0) {\n        MPI_Reduce(bins.data(), local_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        std::copy(local_bins.begin(), local_bins.end(), bins.begin());\n    } else {\n        MPI_Reduce(bins.data(), nullptr, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "for (int i = 0; i < s.size(); i++) {\n    char ch = s[i][0];\n    if (ch >= 'a' && ch <= 'z') {\n      bins[ch - 'a']++;\n    }\n  }\n}",
            "// Fill this in\n}",
            "}",
            "// Fill this in\n}",
            "// TODO: YOUR CODE HERE!\n  MPI_Reduce(MPI_IN_PLACE, &bins[0], bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Reduce(\n        MPI_IN_PLACE,\n        bins.data(),\n        26,\n        MPI_SIZE_T,\n        MPI_SUM,\n        0,\n        MPI_COMM_WORLD\n    );\n}",
            "int numprocs, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    //...\n}",
            "// Replace with your code\n}",
            "// your code here\n    int size = bins.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        for (int i = 0; i < size; i++)\n            bins[i] = 0;\n        for (std::string x : s) {\n            int letter = x[0] - 'a';\n            bins[letter] += 1;\n        }\n    }\n    else {\n        int rank_letter = 0;\n        for (std::string x : s) {\n            int letter = x[0] - 'a';\n            if (letter == rank)\n                rank_letter++;\n        }\n        MPI_Send(&rank_letter, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    int recv_letter;\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&recv_letter, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[i] += recv_letter;\n        }\n    }\n    else {\n        MPI_Send(&rank_letter, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "for (size_t i = 0; i < bins.size(); i++)\n        bins[i] = 0;\n    for (std::string str : s) {\n        bins[str[0] - 'a'] += 1;\n    }\n}",
            "// Your code goes here!\n\n  // Use MPI_Reduce to reduce the count of each letter to rank 0\n\n}",
            "// your code here\n\n\n}",
            "// TODO\n}",
            "/* Your solution goes here */\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// Your code here\n  // For every letter in the alphabet, count the number of strings in s that start with that letter.\n  // Assume all strings are in lower case.\n  // Store the output in bins array.\n\n\n  // Example:\n  // input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n  // output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int len = s.size();\n   int locallen = len/size;\n   int leftover = len%size;\n\n   // Get the section of the vector for this rank\n   std::vector<std::string> mys;\n   for (int i=0; i<size; ++i) {\n      if (rank==i) {\n         for (int j=0; j<locallen; ++j) {\n            mys.push_back(s[i*locallen+j]);\n         }\n         for (int j=0; j<leftover; ++j) {\n            mys.push_back(s[i*locallen+j+locallen]);\n         }\n      }\n      MPI_Barrier(MPI_COMM_WORLD);\n   }\n\n   // Count how many strings start with each letter\n   std::array<size_t, 26> mybins;\n   std::fill(mybins.begin(), mybins.end(), 0);\n   for (std::string const& str : mys) {\n      char c = str.front();\n      if ('a' <= c && c <= 'z') {\n         ++mybins[c-'a'];\n      }\n   }\n\n   // Add all local bins into the global bins\n   std::vector<size_t> sendbuf(mybins.begin(), mybins.end());\n   std::vector<size_t> recvbuf(26);\n   if (rank==0) {\n      for (int i=1; i<size; ++i) {\n         MPI_Recv(recvbuf.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j=0; j<26; ++j) {\n            bins[j] += recvbuf[j];\n         }\n      }\n   }\n   else {\n      MPI_Send(sendbuf.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "}",
            "int rank = 0;\n  int size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO\n  if (rank == 0) {\n      for (auto i = 0; i < s.size(); ++i) {\n          if (isalpha(s[i][0])) {\n              bins[s[i][0] - 'a']++;\n          }\n      }\n  } else {\n      for (auto i = rank; i < s.size(); i += size) {\n          if (isalpha(s[i][0])) {\n              bins[s[i][0] - 'a']++;\n          }\n      }\n  }\n\n  if (rank == 0) {\n      for (auto i = 1; i < size; i++) {\n          MPI_Recv(&bins[0], bins.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n          for (auto j = 0; j < bins.size(); j++) {\n              bins[j] += bins[j];\n          }\n      }\n  } else {\n      MPI_Send(&bins[0], bins.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "const int mpi_size = MPI::COMM_WORLD.Get_size();\n  const int mpi_rank = MPI::COMM_WORLD.Get_rank();\n\n  // your code here\n\n}",
            "// your code here\n}",
            "// TODO: implement\n}",
            "//...\n}",
            "// Your code here\n  size_t size = s.size();\n  int rank, numtasks;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n  std::vector<int> bin_per_rank(26, 0);\n\n  // For the first letter of each string:\n  for (int i = 0; i < size; i++)\n  {\n    bin_per_rank[s[i][0] - 'a']++;\n  }\n\n  // Calculate the sum of each bin:\n  std::vector<int> bin_per_rank_sum(26, 0);\n  for (int i = 0; i < 26; i++)\n  {\n    MPI_Reduce(&bin_per_rank[i], &bin_per_rank_sum[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  // Assign the result to the bins array:\n  if (rank == 0)\n  {\n    for (int i = 0; i < 26; i++)\n    {\n      bins[i] = bin_per_rank_sum[i];\n    }\n  }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int i, j, k;\n    std::vector<std::string> temp;\n    std::array<size_t, 26> bins_temp;\n    std::array<size_t, 26> bins_all;\n\n    if (world_rank == 0) {\n        for (i = 0; i < s.size(); ++i) {\n            bins_temp[s[i][0]-97] += 1;\n        }\n\n        for (i = 1; i < world_size; ++i) {\n            MPI_Recv(&bins_temp, 26, MPI_SIZE_T, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (j = 0; j < 26; ++j) {\n                bins[j] += bins_temp[j];\n            }\n        }\n    } else {\n        MPI_Send(&bins_temp, 26, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "const int rank = 0;\n    const int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::array<size_t, 26> counts;\n    counts.fill(0);\n    for (auto &e: s)\n        ++counts[e[0] - 'a'];\n    MPI_Reduce(counts.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Implement this function\n  bins.fill(0);\n  for(int i = 0; i < s.size(); i++)\n  {\n    for(int j = 0; j < s[i].length(); j++)\n    {\n      bins[s[i].at(j)-'a']++;\n    }\n  }\n}",
            "// Your code here\n}",
            "//TODO: Your code here\n}",
            "//TODO\n}",
            "// your code here\n  auto rank = 0;\n  auto size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<size_t> localBins(26, 0);\n\n  auto numStrings = s.size();\n\n  if(rank == 0) {\n    for(auto i = 0; i < numStrings; i++) {\n      localBins[s[i][0] - 'a'] += 1;\n    }\n  } else {\n    auto localNumStrings = numStrings/size;\n\n    auto startIndex = rank*localNumStrings;\n\n    if(rank == (size - 1)) {\n      localNumStrings = numStrings - (size - 1)*localNumStrings;\n    }\n\n    for(auto i = 0; i < localNumStrings; i++) {\n      localBins[s[startIndex + i][0] - 'a'] += 1;\n    }\n\n  }\n\n  MPI_Reduce(&localBins[0], &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank = 0;\n    int size = 0;\n    int const n = s.size();\n    int const num_per_proc = n / size;\n    int const rem = n % size;\n    int const start_idx = num_per_proc * rank;\n    int const end_idx = num_per_proc * (rank + 1);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        for (auto i = 0; i < n; i++) {\n            for (auto j = 0; j < 26; j++) {\n                if (s[i][0] == j + 97) {\n                    bins[j] += 1;\n                }\n            }\n        }\n    }\n\n    for (auto i = start_idx; i < end_idx; i++) {\n        for (auto j = 0; j < 26; j++) {\n            if (s[i][0] == j + 97) {\n                bins[j] += 1;\n            }\n        }\n    }\n\n    MPI_Gather(&bins, bins.size(), MPI_INT, &bins, bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "MPI_Status status;\n  const int root = 0;\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Calculate how many string for each rank\n  int chunkSize = s.size() / size;\n\n  // Calculate how many string for the last rank\n  int lastChunkSize = s.size() % size;\n\n  // Start index\n  int start = rank * chunkSize;\n\n  // End index\n  int end;\n\n  if (rank < size - 1)\n    end = start + chunkSize;\n  else\n    end = start + lastChunkSize;\n\n  // Create an array to store the counts\n  std::array<size_t, 26> count;\n  count.fill(0);\n\n  // Count the number of string for each rank\n  for (int i = start; i < end; i++)\n    count[s[i][0] - 'a']++;\n\n  // Send the counts to the root rank\n  if (rank == 0) {\n    for (int i = 1; i < size; i++)\n      MPI_Send(&count, 26, MPI_INT, i, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Send(&count, 26, MPI_INT, root, 0, MPI_COMM_WORLD);\n  }\n\n  // Receive the count from the root rank\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&bins, 26, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    MPI_Recv(&bins, 26, MPI_INT, root, 0, MPI_COMM_WORLD, &status);\n  }\n\n  // Add the counts to the counts array\n  if (rank!= 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] += count[i];\n    }\n  }\n}",
            "// Your code goes here.\n}",
            "// Add code here\n\n}",
            "// TODO\n}",
            "// TODO: Your code here\n\n    bins.fill(0);\n    std::array<std::string, 26> array;\n    int num_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int range = 26 / num_ranks;\n    int rest = 26 % num_ranks;\n\n    std::vector<int> start_index(num_ranks, 0);\n    std::vector<int> end_index(num_ranks, 0);\n\n    for (int i = 0; i < num_ranks; i++) {\n        if (i!= 0)\n            start_index[i] = end_index[i - 1] + 1;\n        if (i!= num_ranks - 1)\n            end_index[i] = start_index[i] + range;\n        if (i == num_ranks - 1)\n            end_index[i] = start_index[i] + range + rest;\n    }\n\n    int count = 0;\n    for (int i = 0; i < num_ranks; i++) {\n        for (int j = start_index[i]; j < end_index[i]; j++) {\n            array[j] = s[count];\n            count++;\n        }\n    }\n    MPI_Reduce(&array[start_index[rank]], &bins[start_index[rank]],\n            end_index[rank] - start_index[rank], MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "for (int i=0; i<bins.size(); i++)\n    {\n        bins[i]=0;\n    }\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int count = s.size();\n    int localCount = count / size;\n    int localStart = rank * localCount;\n    int localEnd = (rank + 1) * localCount;\n\n    if (rank == size - 1)\n    {\n        localEnd = count;\n    }\n\n    for (int i = localStart; i < localEnd; i++)\n    {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z')\n        {\n            bins[c - 'a']++;\n        }\n    }\n\n    if (rank == 0)\n    {\n        for (int i = 1; i < size; i++)\n        {\n            std::array<size_t, 26> bins_temp;\n            MPI_Recv(&bins_temp, 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for (int j = 0; j < bins.size(); j++)\n            {\n                bins[j] += bins_temp[j];\n            }\n        }\n    }\n    else\n    {\n        MPI_Send(&bins, 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Put your code here\n}",
            "if (bins.size()!= 26) {\n        throw std::invalid_argument(\"invalid size of the bins array\");\n    }\n    if (s.size() == 0) {\n        throw std::invalid_argument(\"input string is empty\");\n    }\n    int myrank, numranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numranks);\n    if (numranks == 1) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (const auto &str : s) {\n            bins[str[0] - 'a']++;\n        }\n        return;\n    }\n\n    // TODO\n\n    // 1. Divide the array into segments of size (s.size() / numranks) + (s.size() % numranks)\n    // 2. Send the corresponding segment to the appropriate rank\n    // 3. Receive the counts from all other ranks\n    // 4. Add the counts and store in bins\n\n    std::vector<std::string> s_copy;\n    MPI_Request request;\n    if (myrank == 0) {\n        int i;\n        for (i = 1; i < numranks; i++) {\n            MPI_Send(&s[i * (s.size() / numranks)], (s.size() / numranks) + (s.size() % numranks),\n                     MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n        s_copy = s;\n    } else {\n        int i;\n        s_copy.resize((s.size() / numranks) + (s.size() % numranks));\n        MPI_Recv(&s_copy[0], (s.size() / numranks) + (s.size() % numranks),\n                 MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    std::fill(bins.begin(), bins.end(), 0);\n    for (const auto &str : s_copy) {\n        bins[str[0] - 'a']++;\n    }\n\n    std::vector<int> bins_copy(bins.size());\n    if (myrank == 0) {\n        for (int i = 1; i < numranks; i++) {\n            MPI_Irecv(&bins_copy[0], bins.size(), MPI_INT, i, 0, MPI_COMM_WORLD, &request);\n            MPI_Wait(&request, MPI_STATUS_IGNORE);\n            for (int j = 0; j < bins.size(); j++) {\n                bins[j] += bins_copy[j];\n            }\n        }\n    } else {\n        MPI_Send(&bins[0], bins.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (size_t i = 0; i < s.size(); i++) {\n        for (int j = 0; j < 26; j++) {\n            if (s[i][0] == 'a' + j) {\n                bins[j]++;\n            }\n        }\n    }\n\n    MPI_Reduce(&bins, &bins, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// You need to implement this function\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = s.size();\n    MPI_Scatter(&n, 1, MPI_INT, &n_local, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    int start = 0;\n    if (rank > 0) {\n        start = n / nranks * rank;\n        n_local = n / nranks;\n        if (rank == nranks - 1)\n            n_local += n % nranks;\n    }\n    int* s_local = new int[n_local];\n    MPI_Scatterv(&s[0], &n_local, &start, MPI_INT, s_local, n_local, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n    for (int i = 0; i < n_local; i++) {\n        bins[s_local[i][0] - 'a']++;\n    }\n    MPI_Gather(bins.begin(), 26, MPI_INT, &bins, 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO: Implement this function\n\n    //...\n\n    return;\n}",
            "int n = s.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int delta = n / size;\n    int start = rank * delta;\n    int end = (rank + 1) * delta;\n    if (rank == size - 1) {\n        end = n;\n    }\n\n    std::array<size_t, 26> localbins{};\n    for (int i = start; i < end; i++) {\n        char c = s[i][0];\n        int index = c - 'a';\n        localbins[index] += 1;\n    }\n\n    // Reduce localbins to bins\n    int result[26];\n    MPI_Reduce(localbins.data(), result, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Copy result to bins\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = result[i];\n        }\n    }\n}",
            "size_t size = s.size();\n    std::vector<size_t> counts;\n\n    int numtasks, rank, i;\n    int offset;\n    int chunk;\n    int rem;\n\n    // Get number of tasks\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n    // Get rank\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Divide the vector s into equal-sized chunks and do the counting\n    // Calculate number of elements in each chunk\n    chunk = size / numtasks;\n    rem = size % numtasks;\n\n    // Send/Receive size to/from root\n    MPI_Sendrecv(&size, 1, MPI_INT, 0, 0, &size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    // Calculate the offset\n    offset = rank * chunk;\n\n    // Send/Receive offset to/from root\n    MPI_Sendrecv(&offset, 1, MPI_INT, 0, 0, &offset, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    if (rank!= 0)\n    {\n        std::vector<std::string> strs;\n\n        // Divide the vector s into equal-sized chunks and do the counting\n        // Calculate number of elements in each chunk\n        chunk = size / numtasks;\n        rem = size % numtasks;\n\n        // Send/Receive size to/from root\n        MPI_Sendrecv(&size, 1, MPI_INT, 0, 0, &size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        // Calculate the offset\n        offset = rank * chunk;\n\n        // Send/Receive offset to/from root\n        MPI_Sendrecv(&offset, 1, MPI_INT, 0, 0, &offset, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        // For each chunk, count the number of strings that start with each letter\n        for (i = 0; i < chunk; i++)\n        {\n            std::string str = s[i + offset];\n\n            if (str[0] == 'a')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'b')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'c')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'd')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'e')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'f')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'g')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'h')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'i')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'j')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'k')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'l')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] =='m')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'n')\n            {\n                counts.push_back(1);\n            }\n            if (str[0] == 'o')\n            {\n                counts.push_back(1);\n            }\n            if",
            "if (s.size() < 2) {\n    throw std::invalid_argument(\"Not enough elements in the vector. At least two elements are required.\");\n  }\n  // Add your code here\n\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tsize_t n = s.size();\n\tsize_t k = n / size;\n\tsize_t l = n % size;\n\n\tstd::array<size_t, 26> bins_rank;\n\tfor (int i = 0; i < 26; ++i) bins_rank[i] = 0;\n\tfor (size_t i = 0; i < k; ++i) {\n\t\tbins_rank[s[i][0] - 'a']++;\n\t}\n\n\tMPI_Gather(bins_rank.data(), 26, MPI_UNSIGNED_LONG, bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n}",
            "int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank!= 0) {\n        for (auto &e: bins) {\n            e = 0;\n        }\n    }\n    int local_size = s.size() / num_ranks;\n    int extra = s.size() % num_ranks;\n    std::vector<std::string> local_s;\n    for (int i = 0; i < local_size; i++) {\n        local_s.push_back(s.at(i + rank * local_size + std::min(rank, extra)));\n    }\n    int idx = 0;\n    for (int i = 0; i < local_s.size(); i++) {\n        idx = local_s.at(i)[0] - 'a';\n        bins.at(idx)++;\n    }\n    std::vector<int> recv_bins(26, 0);\n    if (rank!= 0) {\n        MPI_Send(bins.data(), 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < num_ranks; r++) {\n            MPI_Recv(recv_bins.data(), 26, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < 26; i++) {\n                bins.at(i) += recv_bins.at(i);\n            }\n        }\n    }\n}",
            "// TODO\n\n}",
            "// Complete this function\n\n    // Example:\n    // std::vector<std::string> s = {\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"};\n    // std::array<size_t, 26> bins;\n    // firstLetterCounts(s, bins);\n    // for (auto i : bins) {\n    //     std::cout << i << std::endl;\n    // }\n\n    // bins should be: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n}",
            "// Your code goes here\n\n}",
            "const int sz = s.size();\n  const int myid = MPI::COMM_WORLD.Get_rank();\n  const int root = 0;\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:\n\n  // TODO:",
            "}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Use 1 process per letter.\n    // This means 26 processes in total, one for each letter.\n    // We will use rank 0 to accumulate the results later.\n    if (size!= 26)\n        throw std::runtime_error(\"The number of processes must be 26.\");\n\n    // Assign 1 process per letter.\n    // So process 0 is assigned letter 'a', etc.\n    int letter = rank;\n\n    // Count strings that start with letter 'a'\n    size_t count = std::count_if(s.begin(), s.end(),\n        [letter](const std::string& str) { return str[0] == letter+'a'; });\n\n    // Accumulate results from all processes.\n    // Since we only have 1 process per letter, there is no overlap between processes.\n    // Use MPI_Reduce to accumulate results from processes.\n    MPI_Reduce(&count, &bins[letter], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // This is rank 0. So output the result.\n    if (rank == 0) {\n        std::cout << \"First letter counts:\" << std::endl;\n        for (size_t i = 0; i < 26; ++i) {\n            std::cout << (char)(i+'a') << \": \" << bins[i] << std::endl;\n        }\n    }\n}",
            "const int rank = 0;\n\n    // Create a copy of the vector to be sorted on this rank.\n    std::vector<std::string> sCopy(s);\n\n    // Create a copy of the array to store the output in.\n    std::array<size_t, 26> binsCopy(bins);\n\n    // Sort the strings on this rank.\n    std::sort(sCopy.begin(), sCopy.end());\n\n    // For each string in the vector, increment the corresponding element in the bins array.\n    for (auto str : sCopy) {\n        binsCopy[str[0] - 'a']++;\n    }\n\n    // Merge the partial bins arrays.\n    MPI_Reduce(binsCopy.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "auto const& size = s.size();\n    for (int i = 0; i < 26; ++i) {\n        for (int j = 0; j < size; ++j) {\n            if (s[j][0] == char('a' + i)) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "// TODO:\n\n}",
            "// your code here\n}",
            "for (std::string const& str: s) {\n      ++bins[str[0] - 'a'];\n   }\n}",
            "// Add your code here.\n}",
            "// TODO\n}",
            "}",
            "if (MPI::COMM_WORLD.Get_rank() == 0) {\n        /* write your code here */\n    }\n}",
            "// replace this code with a parallel implementation\n}",
            "const size_t N = bins.size();\n    if (s.size() == 0) {\n        for (size_t i = 0; i < N; i++) {\n            bins[i] = 0;\n        }\n    } else {\n        MPI_Status status;\n        int numProcs;\n        MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n        int myRank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n        if (myRank == 0) {\n            int numStrings = s.size();\n            int totalStrings = numStrings;\n            for (int i = 1; i < numProcs; i++) {\n                MPI_Send(&numStrings, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                std::vector<std::string> myStrings(numStrings);\n                for (int j = 0; j < numStrings; j++) {\n                    myStrings[j] = s[j + i * numStrings];\n                }\n                MPI_Send(&myStrings[0], numStrings, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n            }\n\n            std::vector<std::string> myStrings(numStrings);\n            for (int j = 0; j < numStrings; j++) {\n                myStrings[j] = s[j];\n            }\n            std::array<size_t, 26> counts(0);\n            countFirstLetter(myStrings, counts);\n            for (int i = 1; i < numProcs; i++) {\n                std::vector<std::string> myStrings(numStrings);\n                MPI_Recv(&myStrings[0], numStrings, MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n                int numStrings;\n                MPI_Recv(&numStrings, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n                std::array<size_t, 26> counts(0);\n                countFirstLetter(myStrings, counts);\n                for (int j = 0; j < N; j++) {\n                    bins[j] += counts[j];\n                }\n            }\n        } else {\n            int numStrings;\n            MPI_Recv(&numStrings, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n            std::vector<std::string> myStrings(numStrings);\n            MPI_Recv(&myStrings[0], numStrings, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n            std::array<size_t, 26> counts(0);\n            countFirstLetter(myStrings, counts);\n            MPI_Send(&counts[0], N, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "// Implement this function.\n}",
            "if (bins.size()!= 26) {\n    throw std::invalid_argument(\"bins must have size 26\");\n  }\n\n  // TODO\n}",
            "/* YOUR CODE GOES HERE */\n    int rank,size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int length = s.size();\n    int blockSize = length / size;\n    int lastSize = length - (size - 1) * blockSize;\n\n    std::vector<std::string> subS;\n    if(rank == 0)\n    {\n        for(int i = 1; i < size; i++)\n        {\n            MPI_Send(&s[i * blockSize], blockSize, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n\n        for(int i = 1; i < size; i++)\n        {\n            int tempLength;\n            MPI_Recv(&tempLength, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            std::vector<std::string> tempSubS(tempLength);\n            MPI_Recv(&tempSubS[0], tempLength, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < tempLength; j++)\n            {\n                subS.push_back(tempSubS[j]);\n            }\n        }\n        subS.insert(subS.end(), s.begin(), s.begin() + blockSize);\n    }\n    else\n    {\n        int tempLength = s.size();\n        MPI_Send(&tempLength, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&s[0], s.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    for(int i = 0; i < 26; i++)\n    {\n        bins[i] = 0;\n    }\n\n    for(int i = 0; i < subS.size(); i++)\n    {\n        char firstChar = subS[i].at(0);\n        if(firstChar >= 'a' && firstChar <= 'z')\n        {\n            bins[firstChar - 'a']++;\n        }\n    }\n    if(rank == 0)\n    {\n        for(int i = 1; i < size; i++)\n        {\n            int tempBin[26];\n            MPI_Recv(&tempBin, 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < 26; j++)\n            {\n                bins[j] += tempBin[j];\n            }\n        }\n    }\n    else\n    {\n        MPI_Send(&bins[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Fill this in\n\n}",
            "size_t const rank = 0;\n  size_t const size = 0;\n\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  // TODO\n\n}",
            "size_t size = s.size();\n\n    if (size == 0)\n        return;\n\n    size_t bin_size = size / 26;\n    if (bin_size == 0)\n        bin_size = 1;\n\n    size_t n = s.size();\n    size_t rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> local_s;\n    if (rank == 0) {\n        local_s = std::vector<std::string>(s.begin() + rank * bin_size, s.begin() + std::min(rank * bin_size + bin_size, n));\n    } else {\n        local_s = std::vector<std::string>(s.begin() + rank * bin_size, s.begin() + std::min(rank * bin_size + bin_size, n));\n    }\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n    for (auto &word : local_s) {\n        if (word.length() > 0)\n            local_bins[word[0] - 'a']++;\n    }\n\n    std::array<size_t, 26> global_bins;\n    global_bins.fill(0);\n\n    MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> s_local;\n\n    int num_of_elem = s.size();\n    int local_size;\n\n    if (rank == 0) {\n        local_size = num_of_elem / size;\n\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&s[i * local_size], local_size, MPI_C_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&s_local, num_of_elem, MPI_C_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        local_size = s_local.size();\n    }\n\n    if (rank == 0) {\n        MPI_Recv(&s_local, num_of_elem, MPI_C_CHAR, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    } else {\n        local_size = s_local.size();\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&s_local, local_size, MPI_C_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n\n    std::vector<std::string> copy;\n    for (int i = 0; i < local_size; i++) {\n        copy.push_back(s_local[i]);\n    }\n\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < local_size; i++) {\n        char firstLetter = copy[i][0];\n        firstLetter = tolower(firstLetter);\n        if (isalpha(firstLetter)) {\n            bins[firstLetter - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        int sum = 0;\n        for (int i = 0; i < 26; i++) {\n            sum += bins[i];\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        for (int i = 0; i < 26; i++) {\n            bins[i] = sum + bins[i];\n        }\n    } else {\n        MPI_Send(&bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO 1: Implement this function\n\n}",
            "// TODO\n}",
            "int const rank = MPI::COMM_WORLD.Get_rank();\n  int const size = MPI::COMM_WORLD.Get_size();\n\n  // Fill the bins array with 0s\n  for (size_t i = 0; i < 26; ++i)\n    bins[i] = 0;\n\n  // count the number of strings that start with each letter\n  for (auto const& str : s) {\n    if (str.size() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n\n  // Reduce bins on rank 0 to get the result\n  std::array<size_t, 26> local_bins{};\n  MPI::COMM_WORLD.Reduce(&bins[0], &local_bins[0], bins.size(), MPI::INT, MPI::SUM, 0);\n\n  if (rank == 0) {\n    bins = local_bins;\n  }\n}",
            "// Replace this line with your code.\n  // If necessary, use MPI to communicate between processes.\n  // Assume s is the same on every process.\n  // The results are stored in bins on rank 0.\n  if (bins.size()!= 26) {\n    throw std::runtime_error(\"bins array is of wrong size.\");\n  }\n\n  if (s.empty()) {\n    return;\n  }\n\n  MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins[0]);\n\n  int r, p;\n  r = bins.size();\n  p = 0;\n  int n = s.size();\n  int q = n / r;\n  int m = n % r;\n  int i;\n  int j;\n  std::vector<int> temp(r);\n  MPI_Scatter(\n    &bins[0],\n    q + (m > p? 1 : 0),\n    MPI_INT,\n    &temp[0],\n    q + (m > p? 1 : 0),\n    MPI_INT,\n    0,\n    MPI_COMM_WORLD\n  );\n  for (i = 0; i < q + (m > p? 1 : 0); i++) {\n    for (j = 0; j < s.size(); j++) {\n      if (s[j][0] == 'a' + i) {\n        temp[i]++;\n      }\n    }\n  }\n  MPI_Gather(\n    &temp[0],\n    q + (m > p? 1 : 0),\n    MPI_INT,\n    &bins[0],\n    q + (m > p? 1 : 0),\n    MPI_INT,\n    0,\n    MPI_COMM_WORLD\n  );\n\n  // End of your code.\n\n}",
            "// TODO\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, nranks;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &nranks);\n\n  const int nperrank = s.size() / nranks;\n  const int nleft = s.size() % nranks;\n\n  std::array<size_t, 26> bins_per_rank;\n  for (int i = 0; i < 26; ++i) {\n    bins_per_rank[i] = 0;\n  }\n\n  // We need to make sure that each rank knows how many ranks have more than the alloted number of strings to work on.\n  // If this is the case, we need to make sure that we don't go out of bounds.\n  // Since we already know the number of strings per rank, and we know how many ranks have more strings, we can use this to figure out which ranks have extra strings.\n  // This is done by creating a vector that tells us which ranks have extra strings, then getting the number of extra strings from that rank.\n\n  int extra_strings = 0;\n  int extra_ranks = 0;\n  std::vector<int> extra_ranks_vector(nranks);\n  for (int i = 0; i < nranks; ++i) {\n    extra_ranks_vector[i] = 0;\n    if ((nperrank + 1) * i > s.size()) {\n      extra_ranks_vector[i] = 1;\n    }\n  }\n\n  MPI_Allgather(&extra_ranks_vector[rank], 1, MPI_INT, extra_ranks_vector.data(), 1, MPI_INT, comm);\n\n  for (int i = 0; i < nranks; ++i) {\n    if (extra_ranks_vector[i] == 1) {\n      extra_ranks += 1;\n    }\n  }\n\n  // Now that we know the number of extra ranks, we can figure out how many extra strings are in each rank.\n\n  int nperrank_extra;\n  if (rank < extra_ranks) {\n    nperrank_extra = (nperrank + 1);\n  } else {\n    nperrank_extra = nperrank;\n  }\n\n  std::vector<std::string> s_per_rank;\n  s_per_rank.resize(nperrank_extra);\n\n  MPI_Scatter(s.data(), nperrank_extra, MPI_CHAR, s_per_rank.data(), nperrank_extra, MPI_CHAR, 0, comm);\n\n  for (auto const& str : s_per_rank) {\n    char first_letter = str[0];\n    ++bins_per_rank[first_letter - 'a'];\n  }\n\n  std::vector<std::array<size_t, 26>> bins_vector(nranks);\n\n  MPI_Gather(&bins_per_rank, 26, MPI_UNSIGNED_LONG_LONG, bins_vector.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, comm);\n\n  if (rank == 0) {\n    for (int i = 0; i < 26; ++i) {\n      for (int j = 0; j < nranks; ++j) {\n        bins[i] += bins_vector[j][i];\n      }\n    }\n  }\n}",
            "}",
            "// Replace this with your code\n  // Use MPI_Reduce to do the counting\n  // Do not send all the data from all ranks to rank 0\n  // Note: bins array is cleared to 0 by constructor\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t n_proc = static_cast<size_t>(MPI::COMM_WORLD.Get_size());\n    size_t id = static_cast<size_t>(MPI::COMM_WORLD.Get_rank());\n    size_t len = s.size() / n_proc;\n    size_t rem = s.size() % n_proc;\n    size_t start = id*len + std::min(id, rem);\n    size_t end = (id + 1) * len + std::min(id + 1, rem);\n    for (size_t i = start; i < end; ++i) {\n        if (s[i][0] < 'a' || s[i][0] > 'z') {\n            throw std::runtime_error(\"Input string contains an non-alphabetic character.\");\n        }\n        bins[s[i][0] - 'a'] += 1;\n    }\n    std::vector<size_t> bins_all(26);\n    MPI::COMM_WORLD.Reduce(&bins[0], &bins_all[0], bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0);\n    if (id == 0) {\n        bins = bins_all;\n    }\n}",
            "// Your code here\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // count strings on each process\n  std::array<size_t, 26> counts = std::array<size_t, 26>{};\n\n  for(auto &i : s) {\n    if(i.length() > 0) {\n      counts[static_cast<size_t>(i[0] - 'a')]++;\n    }\n  }\n  std::array<size_t, 26> counts_sum;\n\n  // collect counts from each process\n  MPI_Reduce(counts.data(), counts_sum.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // fill the output array on rank 0 only\n  if(rank == 0) {\n    for(size_t i = 0; i < counts_sum.size(); i++) {\n      bins[i] = counts_sum[i];\n    }\n  }\n\n}",
            "// YOUR CODE HERE\n}",
            "auto myBins = bins;\n  auto const n = s.size();\n  auto const myRank = static_cast<size_t>(mpi::comm().rank());\n\n  // TODO: fill in the missing code\n\n  if (myRank == 0) {\n    for (auto const &r : mpi::comm()) {\n      auto const rBins = r.recv<decltype(myBins)>(0);\n      for (size_t i = 0; i < myBins.size(); ++i) {\n        myBins[i] += rBins[i];\n      }\n    }\n  }\n  else {\n    mpi::comm().send(myBins, 0);\n  }\n}",
            "size_t n = s.size();\n\n    // TODO\n    int numprocs;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<size_t> temp;\n\n    if (rank == 0) {\n        for (int i = 0; i < n; ++i) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n\n    if (rank!= 0) {\n        for (int i = 0; i < n; ++i) {\n            temp.push_back(s[i][0] - 'a');\n        }\n    }\n\n    MPI_Gather(temp.data(), n, MPI_UNSIGNED_LONG_LONG, bins.data(), n, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "MPI_Reduce(&bins, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "}",
            "std::fill(bins.begin(), bins.end(), 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = s.size()/size;\n  int remainder = s.size()%size;\n  if (rank == 0)\n    n += remainder;\n  int start = rank*n;\n  int end = start + n;\n  if (rank == size-1)\n    end += remainder;\n\n  std::array<size_t, 26> bins_private;\n  std::fill(bins_private.begin(), bins_private.end(), 0);\n  for (int i = start; i < end; i++) {\n    char c = s[i].at(0);\n    bins_private[c - 'a']++;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = bins_private[i];\n    }\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(bins_private.data(), 26, MPI_LONG, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 26; j++) {\n        bins[j] += bins_private[j];\n      }\n    }\n  }\n  else {\n    MPI_Send(bins_private.data(), 26, MPI_LONG, 0, rank, MPI_COMM_WORLD);\n  }\n}",
            "const int num_procs = MPI::COMM_WORLD.Get_size();\n    const int rank      = MPI::COMM_WORLD.Get_rank();\n\n    if (rank == 0) {\n        /* Counts in rank 0 and copies them to all other ranks */\n        std::fill(bins.begin(), bins.end(), 0);\n        for (auto const& str : s)\n            bins[str[0] - 'a'] += 1;\n\n        /* Send the counts to other ranks */\n        for (int i = 1; i < num_procs; ++i)\n            MPI::COMM_WORLD.Send(&bins[0], bins.size(), MPI::INT, i, 1);\n    }\n    else {\n        /* Receives the counts from rank 0 */\n        MPI::COMM_WORLD.Recv(&bins[0], bins.size(), MPI::INT, 0, 1);\n    }\n}",
            "size_t const rank = MPI::COMM_WORLD.Get_rank();\n    size_t const size = MPI::COMM_WORLD.Get_size();\n\n    if (rank == 0) {\n        //...\n    } else {\n        //...\n    }\n\n    // barrier to synchronize\n    MPI::COMM_WORLD.Barrier();\n\n    if (rank == 0) {\n        //...\n    } else {\n        //...\n    }\n}",
            "// Replace this line\n}",
            "//TODO: Your code goes here\n\n}",
            "// Your code here\n}",
            "// your code here\n\n}",
            "std::array<size_t, 26> local_bins;\n\n    for (auto const& word : s) {\n        local_bins[word.front() - 'a']++;\n    }\n\n    int n = 26;\n    MPI_Reduce(&local_bins[0], &bins[0], n, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "/* Your code here */\n  size_t num_strings = s.size();\n  // int proc_num, proc_rank;\n  int num_procs;\n  int proc_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n\n  // std::array<size_t, 26> proc_bins;\n  std::array<size_t, 26> proc_bins;\n  std::fill(proc_bins.begin(), proc_bins.end(), 0);\n  // std::array<size_t, 26> proc_bins;\n  // proc_bins.fill(0);\n\n  // int num_per_proc = num_strings/num_procs;\n  int num_per_proc = num_strings/num_procs;\n  int extra = num_strings - num_per_proc*num_procs;\n  int start_index = proc_rank*num_per_proc + std::min(proc_rank, extra);\n  int end_index = (proc_rank+1)*num_per_proc + std::min(proc_rank+1, extra) - 1;\n\n  for (int i = start_index; i <= end_index; i++) {\n    char c = s.at(i).at(0);\n    if (c >= 'a' && c <= 'z') {\n      int bin_index = (int)(c - 'a');\n      proc_bins.at(bin_index)++;\n    }\n  }\n\n  if (proc_rank == 0) {\n    for (int i = 1; i < num_procs; i++) {\n      std::vector<int> recvbuf(26);\n      MPI_Recv(&recvbuf[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 26; j++) {\n        bins.at(j) += recvbuf.at(j);\n      }\n    }\n    MPI_Send(&proc_bins[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Send(&proc_bins[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code here\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Status status;\n    std::vector<std::string> temp;\n    if(rank == 0){\n        temp = s;\n    }\n    else{\n        temp.clear();\n    }\n    MPI_Bcast(&temp[0],temp.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n    for(int i = 0; i<26; i++){\n        char x = 'a'+i;\n        for(int j = 0; j<temp.size(); j++){\n            if(temp[j][0] == x){\n                bins[i]++;\n            }\n        }\n    }\n\n\n}",
            "const int num_tasks = omp_get_max_threads();\n  const int rank = omp_get_thread_num();\n\n  // TODO: Compute the size of each task and the start and end of each task\n  //       For example, if there are 16 tasks and 17 strings, then the\n  //       first 9 strings will be handled by task 0, the next 8 strings\n  //       will be handled by task 1, and the last 1 string will be handled\n  //       by task 15\n  int size = s.size();\n  int size_per_task = size / num_tasks;\n  int start = rank * size_per_task;\n  int end = (rank + 1) * size_per_task;\n\n  // TODO: Process each string in the task's assigned range\n  for (auto i = start; i < end; ++i) {\n    // TODO: count the first letter of each string\n    //       and store the count in the corresponding index of `bins`\n  }\n}",
            "// Replace with your code\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // Count number of strings on this rank that start with each letter\n    std::array<size_t, 26> counts;\n    counts.fill(0);\n    for (auto &str : s) {\n        counts[static_cast<int>(str[0] - 'a')] += 1;\n    }\n    // Reduce counts on all ranks to bins on rank 0\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Status status;\n            MPI_Recv(&bins, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 26; ++j) {\n                bins[j] += counts[j];\n            }\n        }\n    } else {\n        MPI_Send(&counts, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO:\n}",
            "// TODO\n}",
            "// TODO: Complete this function\n  MPI_Reduce(bins, bins, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Your code here\n\n}",
            "// TODO\n}",
            "if (MPI::COMM_WORLD.Get_rank() == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n  /* Your code here */\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "cuda",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "// TODO: Implement me!\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    size_t letter = tolower(s[idx][0]) - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "// TODO\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        char first_letter = tolower(s[idx][0]);\n        if (first_letter >= 'a' && first_letter <= 'z')\n            atomicAdd(&bins[first_letter - 'a'], 1);\n    }\n}",
            "int letterIdx = threadIdx.x;\n    size_t binIdx = 0;\n    char letter = letterIdx + 'a';\n\n    // Initialize bins to zero for each thread\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < N; i++) {\n        if (s[i][0] == letter) {\n            bins[letterIdx]++;\n        }\n    }\n}",
            "}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t letter = 0;\n\n  if (index < N) {\n    // If you have trouble calculating the letter index, you can do this:\n    // letter = (int)(s[index][0]) - 97;\n    letter = s[index][0] - 'a';\n    atomicAdd(&(bins[letter]), 1);\n  }\n}",
            "// TODO: implement\n    size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (index < N) {\n        char firstLetter = tolower(s[index][0]);\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "// Your code here\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  char c = tolower(s[i][0]);\n  atomicAdd(&bins[c - 'a'], 1);\n}",
            "// TODO: fill this in!\n\n    const char *word = s[blockIdx.x];\n    __syncthreads();\n    int count = 0;\n\n    if (threadIdx.x == 0) {\n        char firstChar = word[0];\n        count = bins[firstChar - 97];\n        if (isalpha(firstChar)) {\n            bins[firstChar - 97] = count + 1;\n        }\n    }\n}",
            "}",
            "// Get the id of the current thread\n  int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Get the first letter of the string at thread_id position\n  char firstLetter = tolower(s[thread_id][0]);\n\n  // Count the number of strings that start with this letter\n  // You can use atomicAdd or atomicInc\n  atomicAdd(&(bins[firstLetter - 'a']), 1);\n}",
            "}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char first = tolower(s[i][0]);\n    if (first >= 'a' && first <= 'z')\n      atomicAdd(&bins[first - 'a'], 1);\n  }\n}",
            "// TODO\n\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    char ch = tolower(s[idx][0]);\n    if (ch >= 'a' && ch <= 'z') {\n      atomicAdd(&bins[ch - 'a'], 1);\n    }\n  }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a']++;\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int letter = tolower(s[tid][0]) - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    const char first = s[tid][0];\n    __syncthreads();\n    if (first >= 'a' && first <= 'z') {\n        atomicAdd(&bins[first - 'a'], 1);\n    }\n}",
            "const size_t id = threadIdx.x;\n    const size_t stride = blockDim.x;\n\n    for (size_t i = id; i < N; i += stride) {\n        char first = tolower(s[i][0]);\n        if (first >= 'a' && first <= 'z')\n            atomicAdd(&bins[first - 'a'], 1);\n    }\n}",
            "// TODO: your code here\n\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char firstLetter = s[tid][0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}",
            "const size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    char letter = s[id][0];\n    if (letter >= 'a' && letter <= 'z')\n      atomicAdd(&bins[letter - 'a'], 1);\n  }\n}",
            "const char *str = s[blockIdx.x];\n\n    __shared__ size_t bin[26];\n\n    if (threadIdx.x < 26) bin[threadIdx.x] = 0;\n\n    __syncthreads();\n\n    if (str[0] - 'a' < 26) atomicAdd(&bin[str[0] - 'a'], 1);\n\n    __syncthreads();\n\n    if (threadIdx.x < 26) bins[threadIdx.x] += bin[threadIdx.x];\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a']++;\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tint letter = tolower(s[idx][0]);\n\t\tif (letter >= 'a' && letter <= 'z') {\n\t\t\tatomicAdd(&bins[letter - 'a'], 1);\n\t\t}\n\t}\n}",
            "}",
            "int i = threadIdx.x;\n\n  if (i < N) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(bins + (c - 'a'), 1);\n    }\n  }\n}",
            "// Insert code here...\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        bins[s[tid][0]-'a']++;\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid >= N) return;\n  __shared__ size_t tlc[26];\n  memset(tlc,0,26*sizeof(size_t));\n  __syncthreads();\n  char *p=s[tid];\n  int n = p[0];\n  //printf(\"Thread %d: %s %d\\n\",tid,p,n);\n  if (n!= '\\0')\n  {\n    tlc[n-'a']=tlc[n-'a']+1;\n  }\n  //printf(\"Thread %d: %d\\n\",tid,tlc[n-'a']);\n  __syncthreads();\n  atomicAdd(&bins[n-'a'],tlc[n-'a']);\n  //printf(\"Thread %d: %d\\n\",tid,bins[n-'a']);\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "int threadID = threadIdx.x + blockIdx.x * blockDim.x;\n  if (threadID < N) {\n    char first = s[threadID][0];\n    atomicAdd(&bins[first - 'a'], 1);\n  }\n}",
            "const char* str = s[blockIdx.x];\n\tconst char* c = str;\n\n\tif (*c >= 'a' && *c <= 'z')\n\t\tatomicAdd(bins + *c - 'a', 1);\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id >= N) return;\n  if (s[id][0] >= 'a' && s[id][0] <= 'z')\n    atomicAdd(&bins[s[id][0] - 'a'], 1);\n  __syncthreads();\n\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char ch = s[tid][0];\n        bins[ch - 'a']++;\n    }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      bins[s[i][0] - 'a'] += 1;\n    }\n  }\n}",
            "// TODO: Your code here\n\n    // Your code will likely involve calling\n    // 1) cudaMalloc(&bins,...)\n    // 2) cudaMemcpy(bins,...,...)\n\n    // You may need to call cudaDeviceSynchronize() to force the kernel to complete\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if(index >= N) return;\n\n    char letter = tolower(s[index][0]);\n    atomicAdd(&bins[letter - 'a'], 1);\n}",
            "}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        size_t letterIndex = (size_t)s[idx][0] - (size_t)'a';\n        atomicAdd(&bins[letterIndex], 1);\n    }\n}",
            "// TODO\n}",
            "__shared__ size_t[26] bins_shared;\n\n    // Initialize shared memory\n    size_t tid = threadIdx.x;\n    for (int i=0; i<26; i++) {\n        bins_shared[i] = 0;\n    }\n    __syncthreads();\n\n    // Compute histogram\n    if (tid < N) {\n        // Get first letter of string\n        char c = tolower(s[tid][0]);\n\n        // Increment count for that letter\n        atomicAdd(&bins_shared[c-'a'], 1);\n    }\n    __syncthreads();\n\n    // Reduce across threads\n    if (tid < 26) {\n        atomicAdd(&bins[tid], bins_shared[tid]);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n  int x = tolower(s[i][0]);\n  atomicAdd(&bins[x], 1);\n}",
            "}",
            "// TODO:\n}",
            "}",
            "// TODO: implement\n}",
            "size_t i = threadIdx.x;\n\n    char c = tolower(s[i][0]);\n    if (c >= 'a' && c <= 'z') {\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "/* Your code here */\n}",
            "// TODO: fill this in\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if(i < N) {\n        int first_letter = tolower(s[i][0]);\n        int bin_num = first_letter - 97;\n        atomicAdd(&bins[bin_num], 1);\n    }\n}",
            "__shared__ size_t local_bins[26];\n\n    int thread_idx = threadIdx.x;\n    int block_idx = blockIdx.x;\n    size_t idx = block_idx * blockDim.x + thread_idx;\n\n    if (idx < N) {\n        char letter = s[idx][0];\n        letter = tolower(letter);\n        atomicAdd(&local_bins[letter - 'a'], 1);\n    }\n\n    __syncthreads();\n\n    if (thread_idx == 0) {\n        for (int i = 0; i < 26; i++) {\n            atomicAdd(&bins[i], local_bins[i]);\n        }\n    }\n}",
            "int t = threadIdx.x;\n    if (t < N) {\n        int letter = tolower(s[t][0]) - 'a';\n        atomicAdd(&bins[letter], 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        char first = tolower(s[tid][0]);\n        atomicAdd(&bins[first - 'a'], 1);\n    }\n}",
            "int index = blockIdx.x*blockDim.x + threadIdx.x;\n  if (index < N) {\n    int idx = s[index][0] - 'a';\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "// TODO: Implement this function.\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i >= N) return;\n\tchar first = s[i][0];\n\tatomicAdd(&bins[first - 'a'], 1);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  int count = 0;\n  for (size_t i = 0; i < 26; i++) {\n    char letter = 'a' + i;\n    if (s[tid][0] == letter) count++;\n  }\n  atomicAdd(bins + tid, count);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        const char *str = s[i];\n        char firstChar = str[0];\n        if (firstChar >= 'a' && firstChar <= 'z') {\n            bins[firstChar - 'a']++;\n        }\n    }\n}",
            "for (int j = blockIdx.x * blockDim.x + threadIdx.x; j < N; j += blockDim.x * gridDim.x) {\n        bins[s[j][0] - 'a']++;\n    }\n}",
            "size_t gid = threadIdx.x + blockDim.x*blockIdx.x;\n    if (gid < N) {\n        const char *str = s[gid];\n        if (str!= nullptr) {\n            char firstLetter = tolower(str[0]);\n            if (firstLetter >= 'a' && firstLetter <= 'z') {\n                atomicAdd(&bins[firstLetter - 'a'], 1);\n            }\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int i = (s[idx][0] - 'a');\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int firstLetter = tolower(s[idx][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z')\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "// TODO: add your code here\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (idx >= N) {\n\t\treturn;\n\t}\n\tif (s[idx][0] == '\\0') {\n\t\treturn;\n\t}\n\tint k = s[idx][0] - 'a';\n\tatomicAdd(&bins[k], 1);\n}",
            "// TODO: replace this code with CUDA kernel\n    int index = threadIdx.x;\n    if (index < N) {\n        // Get first character of the string\n        char letter = tolower(s[index][0]);\n        // Count how many strings start with this character\n        if (letter > 'a' && letter <= 'z') {\n            atomicAdd(bins + (letter - 'a'), 1);\n        }\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        char letter = s[tid][0];\n        if (letter >= 'a' && letter <= 'z')\n            atomicAdd(bins + letter - 'a', 1);\n    }\n}",
            "// TODO\n  const int idx = threadIdx.x;\n  if(idx < N) {\n    int index = s[idx][0] - 'a';\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // Your code goes here\n\n}",
            "// TODO: Fill this in with a for-loop\n    int threadID = blockDim.x * blockIdx.x + threadIdx.x;\n    if (threadID < N)\n    {\n        char letter = s[threadID][0];\n        if (letter >= 'a' && letter <= 'z')\n        {\n            atomicAdd(&bins[letter - 'a'], 1);\n        }\n    }\n}",
            "}",
            "// Your code goes here.\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    int i = s[index][0] - 'a';\n    atomicAdd(bins + i, 1);\n  }\n}",
            "//...\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = tolower(s[i][0]);\n    assert(c >= 'a' && c <= 'z');\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "const int nthreads = blockDim.x;\n  const int threadid = threadIdx.x;\n  const int nblocks = gridDim.x;\n  const int blockid = blockIdx.x;\n\n  __shared__ size_t bins_s[26];\n  if (threadid == 0) {\n    for (int i = 0; i < 26; ++i)\n      bins_s[i] = 0;\n  }\n  __syncthreads();\n\n  size_t i;\n  char c;\n  for (i = blockid * nthreads + threadid; i < N; i += nblocks * nthreads) {\n    c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins_s[c - 'a'], 1);\n    }\n  }\n\n  for (int i = 0; i < 26; ++i) {\n    atomicAdd(&bins[i], bins_s[i]);\n  }\n}",
            "int idx = threadIdx.x;\n    if (idx >= N) return;\n    char first = tolower(s[idx][0]);\n    if (first >= 'a' && first <= 'z')\n        atomicAdd(&bins[first - 'a'], 1);\n}",
            "// TODO: use the CUDA for-loop syntax to iterate over the array of strings\n    // TODO: use the CUDA if-statement syntax to check the first letter of each string\n    // TODO: use the CUDA atomicAdd function to increment the bin counter\n\n}",
            "// TODO: Insert your code here\n\n  // TODO: Insert your code here\n\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    // TODO: implement me\n}",
            "int idx = threadIdx.x;\n    char letter = 'a';\n\n    if (idx < N) {\n        letter = s[idx][0];\n        bins[letter - 'a']++;\n    }\n}",
            "}",
            "// Add your code here\n    // Note that each thread is in charge of processing a string.\n}",
            "// TODO: Your code here\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "// TO DO: use CUDA to parallelize the first letter counts\n    // You may need to use CUDA shared memory\n\n    const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    const char c = s[idx][0];\n    const int i = tolower(c) - 'a';\n    atomicAdd(&bins[i], 1);\n}",
            "// Compute the thread number\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if(tid >= N) {\n        return;\n    }\n\n    char c = s[tid][0];\n    if(c >= 'a' && c <= 'z') {\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "// TODO: Your code here\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    char first = tolower(s[index][0]);\n    if (first >= 'a' && first <= 'z')\n      atomicAdd(&bins[first - 'a'], 1);\n  }\n}",
            "const char *word = s[blockIdx.x];\n    if (word[0] - 'a' >= 0 && word[0] - 'a' <= 25)\n        atomicAdd(&bins[word[0] - 'a'], 1);\n}",
            "const size_t tid = threadIdx.x;\n  if (tid < N) {\n    bins[s[tid][0] - 'a']++;\n  }\n}",
            "const char *cur = s[threadIdx.x];\n  int pos = cur[0] - 'a';\n  atomicAdd(&bins[pos], 1);\n}",
            "int i = threadIdx.x;\n  if (i >= N) return;\n  if (s[i] == NULL) return;\n  char firstLetter = tolower(s[i][0]);\n  int bin = firstLetter - 'a';\n  bins[bin]++;\n}",
            "// TODO: Your code here\n}",
            "}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tint c = tolower(s[tid][0]);\n\t\tatomicAdd(&bins[c-'a'], 1);\n\t}\n}",
            "// Your code here\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= N) return;\n  bins[tolower(s[index][0]) - 'a']++;\n}",
            "size_t thread = blockDim.x * blockIdx.x + threadIdx.x;\n    if (thread >= N) return;\n    char first = s[thread][0];\n    if (first >= 'a' && first <= 'z') atomicAdd(&bins[first - 'a'], 1);\n}",
            "// 1. Use CUDA to launch a thread for each element of the `s` vector\n    // 2. For each thread, set the corresponding bin in `bins`\n    // 3. Use atomics to avoid race conditions\n    // 4. For efficiency, use threadIdx.x and blockDim.x\n\n    // TODO: Complete the function\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    char firstLetter = tolower(s[idx][0]);\n    if (firstLetter < 'a' || firstLetter > 'z')\n      return;\n    atomicAdd(&bins[firstLetter - 'a'], 1);\n  }\n}",
            "__shared__ char *local_arr;\n    __shared__ size_t *local_bins;\n    const unsigned int tid = threadIdx.x;\n    if(tid == 0)\n    {\n        local_arr = new char[26];\n        local_bins = new size_t[26];\n    }\n    __syncthreads();\n    int idx = (blockIdx.x*blockDim.x) + threadIdx.x;\n    if(idx < N)\n    {\n        local_bins[s[idx][0]-'a']++;\n    }\n    __syncthreads();\n    if(tid == 0)\n    {\n        atomicAdd(bins + s[idx][0]-'a', local_bins[0]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[1]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[2]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[3]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[4]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[5]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[6]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[7]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[8]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[9]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[10]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[11]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[12]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[13]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[14]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[15]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[16]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[17]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[18]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[19]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[20]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[21]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[22]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[23]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[24]);\n        atomicAdd(bins + s[idx][0]-'a', local_bins[25]);\n        delete[] local_arr;\n        delete[] local_bins;\n    }\n}",
            "}",
            "// Fill in\n}",
            "int idx = threadIdx.x;\n    char c = 'a' + idx;\n    if (idx < 26) {\n        // TODO\n    }\n}",
            "// TODO:\n    // int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    // if (tid >= N) return;\n    // bins[s[tid][0] - 97] += 1;\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N) return;\n\n    // if the string at index 'idx' starts with a letter 'a' to 'z'\n    // then increment the corresponding bin.\n\n    char c = s[idx][0];\n    if (c >= 'a' && c <= 'z')\n        atomicAdd(&bins[c - 'a'], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    // TODO\n  }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        if (s[idx][0] >= 'a' && s[idx][0] <= 'z') {\n            atomicAdd(&bins[s[idx][0] - 'a'], 1);\n        }\n    }\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id >= N)\n    return;\n\n  size_t letter = s[id][0];\n\n  if (letter < 'a' || letter > 'z')\n    return;\n\n  //...\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int tid_mod = tid % 26;\n  char c = 'a' + tid_mod;\n  if (tid < N) {\n    if (s[tid][0] == c) {\n      atomicAdd(bins+tid_mod, 1);\n    }\n  }\n}",
            "__shared__ size_t local_bins[26];\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  int tid2 = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    local_bins[s[tid][0] - 'a']++;\n  }\n  __syncthreads();\n  if (tid < 26) {\n    atomicAdd(&bins[tid], local_bins[tid2]);\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) return;\n\n    // Insert your code here\n\n}",
            "// TODO\n}",
            "}",
            "// TODO\n}",
            "//...\n}",
            "}",
            "// TODO: Implement\n}",
            "const char* string = s[blockIdx.x];\n  const char first = tolower(string[0]);\n  if (first >= 'a' && first <= 'z') {\n    atomicAdd(&bins[first - 'a'], 1);\n  }\n}",
            "__shared__ char str[THREADS_PER_BLOCK];\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  char first = s[idx][0];\n  str[threadIdx.x] = first;\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    atomicAdd(bins + (first - 'a'), countWords(str, THREADS_PER_BLOCK));\n  }\n}",
            "int tid = threadIdx.x;\n    //int bid = blockIdx.x;\n\n    int l = s[tid][0] - 'a';\n    atomicAdd(&bins[l], 1);\n    //printf(\"%d %d %c\\n\", tid, blockIdx.x, s[tid][0]);\n}",
            "int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (s[idx][0] >= 'a' && s[idx][0] <= 'z')\n            atomicAdd(&bins[s[idx][0] - 'a'], 1);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    // find the first letter of the current string and increment the corresponding bin\n    bins[s[idx][0] - 'a']++;\n  }\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gid < N) {\n        bins[s[gid][0]-'a']++;\n    }\n}",
            "__shared__ size_t sm[26];\n    int idx = threadIdx.x;\n    sm[idx] = 0;\n    __syncthreads();\n\n    if (blockIdx.x * blockDim.x + threadIdx.x < N) {\n        char first = s[blockIdx.x * blockDim.x + threadIdx.x][0];\n        if (first >= 'a' && first <= 'z') {\n            sm[first - 'a']++;\n        }\n    }\n\n    for (int i = blockDim.x / 2; i > 0; i /= 2) {\n        __syncthreads();\n        if (idx < i) {\n            sm[idx] += sm[i + idx];\n        }\n    }\n\n    if (idx == 0) {\n        bins[blockIdx.x] = sm[0];\n    }\n}",
            "const char* string = s[threadIdx.x];\n    if (string == NULL)\n        return;\n    char first_letter = string[0];\n    if (first_letter >= 'a' && first_letter <= 'z')\n        atomicAdd(&bins[first_letter - 'a'], 1);\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n\t\tif (s[i] == NULL) {\n\t\t\tcontinue;\n\t\t}\n\t\tif (s[i][0] >= 'a' && s[i][0] <= 'z') {\n\t\t\tatomicAdd(&bins[s[i][0] - 'a'], 1);\n\t\t}\n\t}\n}",
            "}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        size_t x = s[idx][0];\n        if (x >= 'a' && x <= 'z')\n            atomicAdd(&bins[x - 'a'], 1);\n    }\n}",
            "// Your code here\n}",
            "}",
            "int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (gid < N) {\n        bins[s[gid][0] - 'a']++;\n    }\n}",
            "// TODO\n}",
            "// TODO: Replace 0x80 with the code for 'a'\n  const int base = 0x80;\n  const int stride = blockDim.x * gridDim.x;\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += stride) {\n    bins[s[i][0] - base]++;\n  }\n}",
            "int idx = threadIdx.x;\n    if(idx < N)\n    {\n        char c = *s[idx];\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "// TODO\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        const char *str = s[i];\n        if (str[0]!= '\\0')\n            atomicAdd(&bins[(str[0] - 'a')], 1);\n    }\n}",
            "unsigned char c = s[blockIdx.x][0];\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "}",
            "// TODO\n}",
            "int i = threadIdx.x;\n    char c = tolower(s[i][0]);\n    int idx = c - 'a';\n    atomicAdd(&bins[idx], 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N)\n    {\n        int a = s[i][0];\n        if (a >= 'a' && a <= 'z')\n            atomicAdd(&bins[a - 'a'], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tchar c = s[idx][0];\n\t\tif (c >= 'a' && c <= 'z') {\n\t\t\tatomicAdd(&(bins[c - 'a']), 1);\n\t\t}\n\t}\n}",
            "}",
            "int start = 97; // ASCII 'a'\n  int end = 122;  // ASCII 'z'\n  for (int k = blockIdx.x * blockDim.x + threadIdx.x; k < N; k += blockDim.x * gridDim.x) {\n    if (s[k][0] >= start && s[k][0] <= end) {\n      atomicAdd(&bins[s[k][0] - start], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "}",
            "// TODO: Write your code here\n\n}",
            "// TODO\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        char first_letter = tolower(s[i][0]);\n        if (first_letter >= 'a' && first_letter <= 'z') {\n            atomicAdd(&bins[first_letter - 'a'], 1);\n        }\n    }\n}",
            "}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    char first = tolower(s[tid][0]);\n    if(first >= 'a' && first <= 'z')\n        atomicAdd(&bins[first - 'a'], 1);\n}",
            "}",
            "const size_t gid = blockIdx.x*blockDim.x + threadIdx.x;\n    if(gid < N) {\n        char first_letter = tolower(s[gid][0]);\n        atomicAdd(&bins[first_letter - 'a'], 1);\n    }\n}",
            "// TODO: compute a single item of bins[], atomically (use atomicAdd)\n    // TODO: replace with your own code\n\n    size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    char c = s[i][0];\n\n    if(i < N && c >= 'a' && c <= 'z'){\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n\n\tif(i >= N) return;\n\n\tif(s[i][0] >= 'a' && s[i][0] <= 'z')\n\t\tatomicAdd(&bins[s[i][0] - 'a'], 1);\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (idx < N) {\n      char c = s[idx][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    bins[tolower(s[idx][0]) - 'a']++;\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) return;\n    char firstLetter = s[i][0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "// TODO: Implement kernel\n\n}",
            "/*\n     * Note: The `threadIdx.x` value is 0-indexed, so a value of 0 corresponds to the\n     * first thread in the block.  Also, the number of threads in a block is specified\n     * as a template parameter to the kernel launch, so it is a constant value\n     * known to your program at compile time.\n     */\n\n    if (threadIdx.x < 26) {\n        // TODO: Count how many strings start with each letter of the alphabet.\n    }\n}",
            "// TODO\n}",
            "}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    int letter = tolower(s[idx][0]);\n    if (letter >= 'a' && letter <= 'z') {\n        atomicAdd(&bins[letter - 'a'], 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        char firstLetter = tolower(s[tid][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        char ch = tolower(s[tid][0]);\n        atomicAdd(&bins[ch - 'a'], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  char letter = s[i][0];\n  if (letter >= 'a' && letter <= 'z') {\n    atomicAdd(&bins[letter - 'a'], 1);\n  }\n}",
            "__shared__ size_t sBins[26];\n\n    size_t i = threadIdx.x;\n    sBins[i] = 0;\n    __syncthreads();\n\n    if (i < N) {\n        char c = tolower(*s[i]);\n        if (c >= 'a' && c <= 'z')\n            atomicAdd(&sBins[c - 'a'], 1);\n    }\n    __syncthreads();\n\n    atomicAdd(&bins[i], sBins[i]);\n}",
            "// TODO\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (idx >= N) return;\n  if (s[idx][0] == '\\0') return;\n\n  __shared__ size_t binCounts[26];\n  if (threadIdx.x < 26) {\n    binCounts[threadIdx.x] = 0;\n  }\n\n  __syncthreads();\n\n  // Check if the first character is in the range A-Z and then increment\n  // the corresponding bin.\n  //\n  // Note: In ASCII, the characters 'a' to 'z' are 32 less than the\n  // characters 'A' to 'Z'.\n  if (s[idx][0] >= 'A' && s[idx][0] <= 'Z') {\n    atomicAdd(&binCounts[s[idx][0] - 'A'], 1);\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x < 26) {\n    atomicAdd(&bins[threadIdx.x], binCounts[threadIdx.x]);\n  }\n}",
            "size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n    if (index < N) {\n        bins[s[index][0] - 'a']++;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      // TODO\n   }\n}",
            "__shared__ size_t counts[26];\n   counts[threadIdx.x] = 0;\n\n   __syncthreads();\n\n   // Counts the number of words that start with the ith letter of the alphabet.\n   // The index of the thread is used to get the ith letter of the alphabet.\n   for (size_t i = 0; i < N; i++) {\n      char c = s[i][0];\n      if (c == 'a' + threadIdx.x) {\n         counts[threadIdx.x] += 1;\n      }\n   }\n\n   __syncthreads();\n\n   // Stores the counts in the array.\n   for (size_t i = 0; i < 26; i++) {\n      atomicAdd(&bins[i], counts[i]);\n   }\n}",
            "// TODO\n\n  __syncthreads();\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n            atomicAdd(bins + s[i][0] - 'a', 1);\n        }\n    }\n}",
            "// TODO\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a']++;\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  // Replace this line with your implementation.\n}",
            "int start = 97; // ASCII code for 'a'\n    int end = 123; // ASCII code for 'z'\n    // TODO: fill in the kernel code.\n}",
            "int tid = threadIdx.x;\n    // TODO\n}",
            "int tid = threadIdx.x;\n    if (tid < 26) {\n        bins[tid] = 0;\n    }\n    __syncthreads();\n    while (tid < N) {\n        if (s[tid][0] == 'a' + tid) {\n            atomicAdd(&bins[tid], 1);\n        }\n        tid += blockDim.x;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t index = s[i][0] - 'a';\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        char c = tolower(s[idx][0]);\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < N){\n        int first_letter = s[i][0];\n        atomicAdd(&bins[first_letter-'a'], 1);\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tchar c = tolower(s[tid][0]);\n\t\tif (c >= 'a' && c <= 'z')\n\t\t\tatomicAdd(&bins[c - 'a'], 1);\n\t}\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  //...\n}",
            "}",
            "size_t idx = blockIdx.x*blockDim.x+threadIdx.x;\n  if (idx < N) {\n    size_t first = s[idx][0] - 'a';\n    atomicAdd(bins+first, 1);\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        int first = (int)tolower((int)s[index][0]);\n        if (first >= 0 && first < 26)\n            atomicAdd(&bins[first], 1);\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int world_size;\n  int my_rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  std::array<size_t, 26> my_bins;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < 26; ++i) {\n    my_bins[i] = 0;\n  }\n\n  for (size_t i = 0; i < s.size(); ++i) {\n    if (s[i].size() > 0) {\n      my_bins[s[i][0] - 'a'] += 1;\n    }\n  }\n\n  std::array<size_t, 26> tmp_bins;\n  if (my_rank == 0) {\n    for (int i = 1; i < world_size; ++i) {\n      MPI_Recv(&tmp_bins, 26, MPI_UNSIGNED_LONG_LONG, i, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < 26; ++j) {\n        my_bins[j] += tmp_bins[j];\n      }\n    }\n  } else {\n    MPI_Send(&my_bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (my_rank == 0) {\n    bins = my_bins;\n  }\n}",
            "if (bins.size()!= 26) {\n        throw std::invalid_argument(\"bins must have length 26\");\n    }\n\n    size_t const n = s.size();\n    if (n < 1) {\n        return;\n    }\n    auto const last = s.back();\n    if (last.empty()) {\n        throw std::invalid_argument(\"input string cannot be empty\");\n    }\n    if (last.size()!= s.front().size()) {\n        throw std::invalid_argument(\"all strings must have same length\");\n    }\n    size_t const m = last.size();\n    size_t const chunk_size = 256; // 256 = 2^8\n    size_t const n_chunks = (n + chunk_size - 1) / chunk_size;\n\n    if (bins.size()!= 26) {\n        throw std::invalid_argument(\"bins must have length 26\");\n    }\n    std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel\n    {\n        std::array<size_t, 26> counts = {0};\n#pragma omp for schedule(static)\n        for (size_t i = 0; i < n_chunks; ++i) {\n            size_t const chunk_begin = i * chunk_size;\n            size_t const chunk_end = std::min(n, chunk_begin + chunk_size);\n            for (size_t j = chunk_begin; j < chunk_end; ++j) {\n                char const letter = s[j][0];\n                ++counts[letter - 'a'];\n            }\n        }\n#pragma omp critical\n        {\n            for (size_t i = 0; i < 26; ++i) {\n                bins[i] += counts[i];\n            }\n        }\n    }\n}",
            "int numProc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Compute the number of items to do per task, and the number of tasks per process\n    const int nPerTask = 1 + s.size() / (numProc * omp_get_num_threads());\n    const int nTasks = omp_get_num_threads();\n\n    if (rank == 0) {\n        std::memset(bins.data(), 0, sizeof(size_t) * 26);\n        for (auto &str : s) {\n            ++bins[str[0] - 'a'];\n        }\n    }\n\n    // Use OpenMP to distribute the work to the threads.\n    #pragma omp parallel num_threads(nTasks)\n    {\n        // Each thread calculates the start and end index for its work\n        int start = omp_get_thread_num() * nPerTask;\n        int end = std::min(s.size(), start + nPerTask);\n\n        std::array<size_t, 26> bins_thread;\n        std::memset(bins_thread.data(), 0, sizeof(size_t) * 26);\n        for (int i = start; i < end; ++i) {\n            ++bins_thread[s[i][0] - 'a'];\n        }\n\n        // The first thread also accumulates all the thread results.\n        if (omp_get_thread_num() == 0) {\n            for (int i = 1; i < nTasks; ++i) {\n                MPI_Recv(bins_thread.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                for (int j = 0; j < 26; ++j) {\n                    bins[j] += bins_thread[j];\n                }\n            }\n        } else {\n            MPI_Send(bins_thread.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "// TODO: Fill this in\n}",
            "}",
            "const int root = 0;\n    int my_rank;\n    int my_size;\n    std::vector<std::string> my_s;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &my_size);\n\n    std::array<size_t, 26> my_bins;\n    my_bins.fill(0);\n\n    if (my_rank == root){\n\n        size_t s_size = s.size();\n        my_s = s;\n\n        for (int i = 1; i < my_size; i++) {\n\n            MPI_Send(&s_size, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD);\n\n            for (size_t j = 0; j < s_size; j++) {\n\n                MPI_Send(my_s[j].c_str(), my_s[j].length() + 1, MPI_CHAR, i, 1, MPI_COMM_WORLD);\n\n            }\n        }\n\n    } else {\n\n        MPI_Status status;\n        size_t s_size;\n\n        MPI_Recv(&s_size, 1, MPI_UNSIGNED_LONG, root, 0, MPI_COMM_WORLD, &status);\n\n        my_s.resize(s_size);\n\n        for (size_t i = 0; i < s_size; i++) {\n\n            char temp[100];\n            int n_char;\n            MPI_Recv(temp, 100, MPI_CHAR, root, 1, MPI_COMM_WORLD, &status);\n\n            my_s[i] = temp;\n        }\n    }\n\n    int s_size = my_s.size();\n\n    // Do work in parallel\n    #pragma omp parallel for num_threads(4)\n    for (int i = 0; i < s_size; i++) {\n\n        // Count only the first letter of the word\n        char c = my_s[i][0];\n\n        // Convert to lowercase\n        if (c >= 'A' && c <= 'Z') {\n            c = c + 32;\n        }\n\n        // Count the first letter\n        my_bins[(int)c - 'a'] += 1;\n    }\n\n    if (my_rank == root){\n\n        // Reduce\n        for (int i = 1; i < my_size; i++) {\n\n            std::array<size_t, 26> tmp_bins;\n            tmp_bins.fill(0);\n\n            MPI_Recv(&tmp_bins, 26, MPI_UNSIGNED_LONG, i, 2, MPI_COMM_WORLD, &status);\n\n            for (size_t j = 0; j < 26; j++){\n                bins[j] += tmp_bins[j];\n            }\n        }\n\n    } else {\n\n        MPI_Send(&my_bins, 26, MPI_UNSIGNED_LONG, root, 2, MPI_COMM_WORLD);\n    }\n}",
            "int my_rank;\n    int p;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> local_bins;\n        #pragma omp for\n        for (int i = 0; i < 26; i++) {\n            local_bins[i] = 0;\n        }\n\n        #pragma omp for\n        for (int i = 0; i < s.size(); i++) {\n            char first_letter = s[i][0];\n            char letter_to_count = first_letter - 'a';\n            local_bins[letter_to_count]++;\n        }\n\n        #pragma omp critical\n        for (int i = 0; i < 26; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        int counts[26] = {0};\n        for (auto const& element : s) {\n            counts[element[0] - 'a']++;\n        }\n\n        for (int i = 0; i < 26; i++) {\n            bins[i] = counts[i];\n        }\n    }\n\n    int sub_counts[26] = {0};\n    int sub_length = s.size() / size;\n    int start = rank * sub_length;\n    int end = rank == size - 1? s.size() : (rank + 1) * sub_length;\n    for (int i = start; i < end; i++) {\n        sub_counts[s[i][0] - 'a']++;\n    }\n\n    MPI_Reduce(sub_counts, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this\n}",
            "MPI_Status status;\n    std::vector<std::string> s_copy = s;\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    std::array<size_t, 26> counts;\n    counts.fill(0);\n    auto letter = [](char c) {\n        return tolower(c) - 'a';\n    };\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s_copy.size(); ++i) {\n        counts[letter(s_copy[i][0])]++;\n    }\n    if (rank == 0) {\n        for (int r = 1; r < num_ranks; ++r) {\n            MPI_Recv(counts.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, r, 1, MPI_COMM_WORLD, &status);\n            for (size_t i = 0; i < counts.size(); ++i) {\n                counts[i] += counts[i];\n            }\n        }\n        bins = counts;\n    } else {\n        MPI_Send(counts.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "#pragma omp parallel for\n    for (auto &str : s) {\n        char c = str[0];\n        if (c >= 'a' && c <= 'z') {\n            #pragma omp atomic\n            bins[c - 'a'] += 1;\n        }\n    }\n\n}",
            "// Your code goes here.\n}",
            "int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  std::array<size_t, 26> bin{};\n\n  // Count the first letter of every word in parallel\n#pragma omp parallel for schedule(dynamic)\n  for (size_t i = 0; i < s.size(); ++i) {\n    ++bin[s[i][0] - 'a'];\n  }\n\n  // Collect all counts\n  std::array<size_t, 26> globalBin;\n  MPI_Reduce(bin.data(), globalBin.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Copy to result on rank 0\n  if (rank == 0) {\n    bins = globalBin;\n  }\n}",
            "int my_rank, nb_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nb_ranks);\n\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n\n    if (my_rank == 0) {\n        // do the computation\n        std::vector<std::string> s_copy = s;\n\n        // count the bins using a single thread\n        for (auto s_it = s_copy.begin(); s_it!= s_copy.end(); ++s_it)\n            ++local_bins[(*s_it)[0] - 'a'];\n\n        // send bins to other ranks\n        for (int r = 1; r < nb_ranks; ++r)\n            MPI_Send(&local_bins[0], local_bins.size(), MPI_UNSIGNED_LONG_LONG, r, 0, MPI_COMM_WORLD);\n    } else {\n        // receive bins from rank 0\n        MPI_Recv(&local_bins[0], local_bins.size(), MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    // gather the results in rank 0\n    if (my_rank == 0) {\n        std::array<size_t, 26> global_bins;\n        global_bins.fill(0);\n\n        for (int r = 0; r < nb_ranks; ++r) {\n            std::array<size_t, 26> bins_from_rank;\n            MPI_Recv(&bins_from_rank[0], bins_from_rank.size(), MPI_UNSIGNED_LONG_LONG, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for (size_t i = 0; i < bins_from_rank.size(); ++i)\n                global_bins[i] += bins_from_rank[i];\n        }\n\n        // copy the global result to the output\n        std::copy(global_bins.begin(), global_bins.end(), bins.begin());\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "}",
            "// *******    Your code here!  *******\n  bins.fill(0);\n\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n\n}",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    int size_per_process = s.size() / mpi_size;\n    int last_rank_size = s.size() % mpi_size;\n    int first_process_size = size_per_process + last_rank_size;\n\n    std::vector<std::string> part_of_s(s.begin() + size_per_process * mpi_rank, s.begin() + (size_per_process * (mpi_rank + 1) + last_rank_size));\n\n    if (mpi_rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    for (int i = 0; i < 26; i++) {\n        #pragma omp parallel for\n        for (int j = 0; j < part_of_s.size(); j++) {\n            if (part_of_s[j][0] == 'a' + i) {\n                #pragma omp atomic\n                bins[i]++;\n            }\n        }\n    }\n\n    MPI_Gather(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// Your code goes here\n}",
            "}",
            "int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO\n}",
            "int world_size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 26> local_bins = {};\n\n  if (rank == 0) {\n    for (auto const& word : s)\n      ++local_bins[word[0] - 'a'];\n  }\n\n  MPI_Gather(local_bins.data(), 26, MPI_UNSIGNED_LONG, bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    if (s[i].length() > 0) {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n}",
            "}",
            "// YOUR CODE HERE\n\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk_size = s.size() / size;\n\n  #pragma omp parallel\n  {\n    std::array<size_t, 26> local_bins{};\n    #pragma omp for\n    for (size_t i = 0; i < s.size(); ++i) {\n      local_bins[s[i][0] - 'a']++;\n    }\n\n    MPI_Reduce(&local_bins, &bins, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "if (s.empty()) {\n        return;\n    }\n    // TODO: Implement\n    std::array<size_t, 26> counts {};\n    int nproc;\n    int rank;\n    int i;\n    int start;\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size = s.size();\n    start = rank*size/nproc;\n    end = (rank+1)*size/nproc;\n    for (int i=start; i<end; ++i) {\n        char c = s[i][0];\n        counts[c-'a']++;\n    }\n    std::array<size_t, 26> buf;\n    MPI_Reduce(&counts, &bins, 26, MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    std::array<size_t, 26> lcounts;\n    std::fill(lcounts.begin(), lcounts.end(), 0);\n\n    size_t n = s.size();\n    size_t q = n / size;\n\n    std::vector<size_t> loc_counts(q, 0);\n\n    for (size_t i = 0; i < n; ++i)\n    {\n      char c = s[i][0];\n      loc_counts[i] = ++lcounts[c - 'a'];\n    }\n\n    MPI_Send(loc_counts.data(), q, MPI_SIZE_T, 1, 0, MPI_COMM_WORLD);\n  }\n  else\n  {\n    std::array<size_t, 26> lcounts;\n    std::fill(lcounts.begin(), lcounts.end(), 0);\n\n    size_t n = s.size();\n    size_t q = n / size;\n\n    std::vector<size_t> loc_counts(q, 0);\n\n    for (size_t i = 0; i < n; ++i)\n    {\n      char c = s[i][0];\n      loc_counts[i] = ++lcounts[c - 'a'];\n    }\n\n    MPI_Send(loc_counts.data(), q, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Recv(bins.data(), 26, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n}",
            "for(auto i = 0u; i < bins.size(); ++i) {\n    bins[i] = 0u;\n  }\n  for(auto const& word : s) {\n    auto firstChar = word.front();\n    if(firstChar == '\\0') {\n      continue;\n    }\n    auto firstCharAsInt = static_cast<unsigned int>(firstChar);\n    auto ascii = static_cast<unsigned int>(static_cast<unsigned char>(firstChar));\n    if(ascii < 97) {\n      firstCharAsInt -= 32;\n    }\n    bins[firstCharAsInt - 97] += 1;\n  }\n}",
            "const size_t n = s.size();\n    const size_t block = n / omp_get_num_threads();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::array<size_t, 26> bins_loc;\n    #pragma omp parallel for schedule(static, block)\n    for (size_t i = 0; i < n; ++i) {\n        char first = s[i][0];\n        if (first >= 'a' && first <= 'z') {\n            ++bins_loc[first - 'a'];\n        }\n    }\n\n    std::array<size_t, 26> bins_sum;\n    MPI_Reduce(&bins_loc[0], &bins_sum[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = bins_sum[i];\n        }\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // TODO\n\n}",
            "auto size = s.size();\n    auto rank = omp_get_thread_num();\n    auto nThreads = omp_get_num_threads();\n    auto nRanks = omp_get_num_threads();\n\n    std::vector<std::array<size_t, 26>> counts(nRanks);\n    auto countsPtr = counts.data();\n    countsPtr[rank] = std::array<size_t, 26>();\n\n    #pragma omp parallel\n    {\n        int rank = omp_get_thread_num();\n        for (auto i = rank; i < size; i += nRanks) {\n            auto c = s[i][0];\n            if (c >= 'a' && c <= 'z') {\n                ++(countsPtr[rank][c - 'a']);\n            }\n        }\n    }\n\n    for (int i = 0; i < nRanks; ++i) {\n        MPI_Send(counts[i].data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n\n        for (int i = 0; i < nRanks; ++i) {\n            MPI_Recv(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const int n = s.size();\n    std::vector<std::string> local_s(n / size);\n    for (int i = 0; i < n; ++i) {\n        if (i % size == rank) {\n            local_s.push_back(s[i]);\n        }\n    }\n\n    // Use OpenMP to count first letters on each rank in parallel\n    std::array<size_t, 26> local_bins;\n    local_bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < local_s.size(); ++i) {\n        size_t letter = local_s[i][0] - 'a';\n        ++local_bins[letter];\n    }\n\n    // Use MPI to gather counts from each rank\n    std::array<size_t, 26> global_bins;\n    MPI_Gather(&local_bins[0], 26, MPI_INT, &global_bins[0], 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Store results on rank 0\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    size_t n = s.size();\n    size_t chunksize = n / world_size;\n    size_t remain = n % world_size;\n    size_t start = chunksize * world_rank + std::min(world_rank, remain);\n    size_t end = start + chunksize;\n    if (remain > world_rank)\n        end += 1;\n    for (size_t i = start; i < end; i++) {\n        std::string tmp = s[i];\n        if (tmp.length() == 0)\n            continue;\n        char first = tmp[0];\n        if ((int)first >= 'a' && (int)first <= 'z')\n            bins[first - 'a']++;\n    }\n    MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = s.size();\n  int chunk = n/size;\n  std::vector<int> bins_per_process(26, 0);\n  std::vector<int> global_bins_per_process(26, 0);\n  for (int i = 0; i < n; i++) {\n    if (i % size == rank) {\n      char c = s[i][0];\n      if (c >= 'a' && c <= 'z') {\n        bins_per_process[c - 'a'] += 1;\n      }\n    }\n  }\n\n  MPI_Reduce(&bins_per_process[0], &global_bins_per_process[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = global_bins_per_process[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < s.size(); i++){\n    bins[(int)s[i][0]-97]++;\n  }\n}",
            "// TODO\n}",
            "// TODO: Fill in the rest of the body of the function\n  int p, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<size_t> local_bins(26, 0);\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < s.size(); ++i) {\n      if (s[i][0] >= 97 && s[i][0] <= 122) {\n        #pragma omp atomic\n        local_bins[s[i][0] - 97] += 1;\n      }\n    }\n  }\n  std::vector<size_t> global_bins(26, 0);\n  MPI_Reduce(&local_bins[0], &global_bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "#pragma omp parallel for schedule(dynamic)\n    for (auto i = 0u; i < s.size(); ++i) {\n        auto index = s[i].front() - 'a';\n        #pragma omp atomic\n        ++bins[index];\n    }\n}",
            "size_t i;\n    int num_rank, my_rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_rank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    int num_threads;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n\n    size_t length = s.size();\n\n    std::vector<size_t> count_of_strings(26);\n    std::vector<size_t> count_of_strings_partial(26);\n\n    size_t my_size = length / num_rank;\n    size_t partial_size = length % num_rank;\n\n    size_t offset = my_rank * my_size;\n    if (my_rank == num_rank - 1) {\n        my_size += partial_size;\n    }\n\n    size_t end = offset + my_size;\n\n    for (i = 0; i < 26; ++i) {\n        count_of_strings[i] = 0;\n    }\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        int i;\n\n        for (i = offset; i < end; ++i) {\n            if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n                #pragma omp atomic\n                ++count_of_strings[s[i][0] - 'a'];\n            }\n        }\n\n        #pragma omp barrier\n\n        if (my_rank == 0) {\n            for (i = 0; i < 26; ++i) {\n                count_of_strings_partial[i] = count_of_strings[i];\n            }\n        }\n\n        #pragma omp barrier\n\n        if (my_rank!= 0) {\n            MPI_Send(count_of_strings_partial.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n        } else {\n            for (int i = 1; i < num_rank; ++i) {\n                MPI_Recv(count_of_strings.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n                for (int j = 0; j < 26; ++j) {\n                    count_of_strings[j] += count_of_strings_partial[j];\n                }\n            }\n        }\n    }\n\n    if (my_rank == 0) {\n        for (i = 0; i < 26; ++i) {\n            bins[i] = count_of_strings[i];\n        }\n    }\n}",
            "}",
            "// TODO: Write the code here!\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 26> local_counts;\n  std::fill(local_counts.begin(), local_counts.end(), 0);\n\n#pragma omp parallel for\n  for (int i = 0; i < s.size(); i++) {\n    int c = s[i][0];\n    int count = local_counts[c - 'a'];\n    local_counts[c - 'a'] = count + 1;\n  }\n\n  std::array<size_t, 26> tmp_counts;\n  std::fill(tmp_counts.begin(), tmp_counts.end(), 0);\n\n  // Gathering\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(tmp_counts.data(), 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 26; j++) {\n        local_counts[j] += tmp_counts[j];\n      }\n    }\n  } else {\n    MPI_Send(local_counts.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    bins = local_counts;\n  }\n\n}",
            "// TODO\n}",
            "int num_threads, rank, num_procs;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  std::vector<int> loc_s;\n  std::vector<int> loc_s_copy;\n  int size, size_loc, size_rem;\n\n  if (rank == 0) {\n    loc_s.assign(s.begin(), s.end());\n    size = s.size();\n  }\n\n  MPI_Bcast(&size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    size_loc = size / num_procs;\n    size_rem = size % num_procs;\n  }\n\n  MPI_Scatter(loc_s.data(), size_loc, MPI_INT, &loc_s_copy, size_loc, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (size_rem!= 0) {\n    if (rank == 0) {\n      loc_s_copy.insert(loc_s_copy.end(), loc_s.begin() + size - size_rem, loc_s.end());\n    }\n  }\n\n  omp_set_num_threads(num_procs);\n\n  #pragma omp parallel private(bins)\n  {\n    std::array<size_t, 26> thread_bins;\n    for (int i = 0; i < 26; i++) {\n      thread_bins[i] = 0;\n    }\n\n    #pragma omp for nowait\n    for (int i = 0; i < loc_s_copy.size(); i++) {\n      thread_bins[loc_s_copy[i] - 97]++;\n    }\n\n    #pragma omp critical\n    {\n      for (int i = 0; i < 26; i++) {\n        bins[i] += thread_bins[i];\n      }\n    }\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // your implementation here\n\n}",
            "}",
            "#pragma omp parallel for\n    for (size_t i=0; i<s.size(); ++i) {\n        #pragma omp atomic update\n        bins[s[i][0]-'a']++;\n    }\n}",
            "int nranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        /* First compute bins on rank 0. */\n        for (auto const& word: s) {\n            if (word.size() > 0)\n                bins[word[0] - 'a']++;\n        }\n        /* Broadcast bins to other ranks. */\n        MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n    } else {\n        /* Receive bins from rank 0. */\n        MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n    }\n\n    /* Now each rank updates its bins */\n    for (auto const& word: s) {\n        if (word.size() > 0)\n            bins[word[0] - 'a']++;\n    }\n}",
            "// TODO: Implement this function\n}",
            "if(MPI::COMM_WORLD.Get_rank() == 0) {\n    size_t size = s.size();\n    size_t N = 26;\n    size_t block_size = size/MPI::COMM_WORLD.Get_size();\n    std::array<size_t, 26> local_bins;\n    for(size_t i = 0; i < 26; i++) {\n      local_bins[i] = 0;\n    }\n\n    MPI::COMM_WORLD.Scatter(local_bins.data(), block_size, MPI_INT, bins.data(), block_size, MPI_INT, 0);\n    std::cout << \"bins: \" << bins << std::endl;\n    for(size_t i = 0; i < 26; i++) {\n      local_bins[i] = 0;\n    }\n  } else {\n    size_t size = s.size();\n    size_t N = 26;\n    size_t block_size = size/MPI::COMM_WORLD.Get_size();\n    std::array<size_t, 26> local_bins;\n    for(size_t i = 0; i < 26; i++) {\n      local_bins[i] = 0;\n    }\n\n    MPI::COMM_WORLD.Scatter(local_bins.data(), block_size, MPI_INT, bins.data(), block_size, MPI_INT, 0);\n  }\n\n  if(MPI::COMM_WORLD.Get_rank() == 0) {\n    for(size_t i = 0; i < s.size(); i++) {\n      char letter = s[i][0];\n      bins[letter-'a']++;\n    }\n  } else {\n    for(size_t i = 0; i < s.size(); i++) {\n      char letter = s[i][0];\n      bins[letter-'a']++;\n    }\n  }\n\n  if(MPI::COMM_WORLD.Get_rank() == 0) {\n    MPI::COMM_WORLD.Gather(bins.data(), block_size, MPI_INT, local_bins.data(), block_size, MPI_INT, 0);\n  } else {\n    MPI::COMM_WORLD.Gather(bins.data(), block_size, MPI_INT, local_bins.data(), block_size, MPI_INT, 0);\n  }\n\n}",
            "// TODO: write your implementation here\n}",
            "bins.fill(0);\n#pragma omp parallel\n  {\n    // each thread has a local copy of bins\n    std::array<size_t, 26> bins_local;\n    bins_local.fill(0);\n\n#pragma omp for\n    for (size_t i = 0; i < s.size(); ++i) {\n      // each thread will update its local copy of bins\n      bins_local[s[i][0] - 'a']++;\n    }\n\n#pragma omp critical\n    {\n      for (size_t i = 0; i < 26; ++i) {\n        bins[i] += bins_local[i];\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < s.size(); i++) {\n    if (s[i][0] == 'a' + i) {\n      bins[i]++;\n    }\n  }\n}",
            "}",
            "size_t rank, nranks, len;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    len = s.size();\n\n    // TODO: Implement in parallel\n    if (rank == 0) {\n        for (size_t i = 0; i < len; ++i) {\n            for (size_t j = 0; j < s[i].size(); ++j) {\n                ++bins[s[i][j] - 'a'];\n            }\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: implement this function\n\n    for (int i = 0; i < 26; i++)\n        if (rank == 0)\n            bins[i] = 0;\n}",
            "// TODO\n\n  if (omp_get_thread_num() == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = 0;\n    }\n  }\n\n  int id, size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 26> chunk;\n\n  for (int i = 0; i < 26; i++) {\n    chunk[i] = 0;\n  }\n\n  size_t chunk_size = s.size() / size;\n  int chunk_start = chunk_size * id;\n  int chunk_end = chunk_size * (id + 1);\n\n  if (id == size - 1) {\n    chunk_end = s.size();\n  }\n\n  for (int i = chunk_start; i < chunk_end; i++) {\n    chunk[s[i][0] - 'a'] += 1;\n  }\n\n  MPI_Reduce(chunk.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n        for (auto const& str : s) {\n            if (str[0] == i+'a') {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = s.size() / size;\n    int remainder = s.size() % size;\n\n    int chunk_size_rank = (rank < remainder)? chunk_size + 1 : chunk_size;\n    int start_rank = (rank < remainder)? rank * (chunk_size + 1) : (rank * chunk_size) + remainder;\n\n    // Declaring variables\n    char letter;\n    int thread_count;\n\n#pragma omp parallel\n    {\n        thread_count = omp_get_num_threads();\n\n#pragma omp for\n        for (int i = start_rank; i < start_rank + chunk_size_rank; i++) {\n            letter = s[i][0];\n            if (letter >= 'a' && letter <= 'z') {\n                bins[letter - 'a']++;\n            }\n        }\n    }\n}",
            "// TODO: Your code goes here\n    // You can use MPI_Allreduce() to aggregate the results\n\n}",
            "// your code here\n    for(int i=0;i<s.size();i++){\n        char firstChar = s[i][0];\n        if(firstChar >= 'a' && firstChar <= 'z'){\n            bins[firstChar - 'a'] += 1;\n        }\n    }\n}",
            "int num_threads, rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  omp_set_num_threads(2);\n  num_threads = omp_get_num_threads();\n  std::cout << \"There are \" << num_threads << \" threads and \" << rank << \" rank.\" << std::endl;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    int k = s[i].at(0) - 'a';\n    bins[k] += 1;\n  }\n}",
            "//...\n}",
            "if (MPI::COMM_WORLD.Get_rank() == 0) {\n    // count number of strings that start with each letter\n    std::array<size_t, 26> bins_temp;\n    bins_temp.fill(0);\n    for (auto const& str : s) {\n      if (str.size() == 0) {\n        continue;\n      }\n      // convert to lower case\n      char c = str.front();\n      if (c >= 'A' && c <= 'Z') {\n        c += 'a' - 'A';\n      }\n      // use char as index in array\n      bins_temp[c - 'a']++;\n    }\n\n    // copy array to bins\n    bins = bins_temp;\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO\n}",
            "int const rank = omp_get_thread_num();\n  int const size = omp_get_num_threads();\n\n  if (rank == 0) {\n    bins = {};\n  }\n\n  std::vector<size_t> thread_bins(26, 0);\n\n  for (size_t i = rank; i < s.size(); i += size) {\n    if (i % size == 0) {\n      thread_bins = {};\n    }\n    char letter = s[i][0];\n    if (letter!= '\\0') {\n      letter = tolower(letter) - 'a';\n      thread_bins[letter]++;\n    }\n  }\n\n  MPI_Reduce(thread_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Add your code here\n}",
            "size_t mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    if (mpi_rank == 0) {\n        for (auto const &str: s) {\n            bins[str[0] - 'a']++;\n        }\n\n        for (size_t i = 1; i < mpi_size; ++i) {\n            size_t const* counts = static_cast<size_t*>(MPI_BOTTOM);\n            MPI_Status status;\n            MPI_Recv(&counts, 26, MPI_SIZE_T, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 26; ++j) {\n                bins[j] += counts[j];\n            }\n        }\n\n    } else {\n        size_t counts[26];\n        std::fill(std::begin(counts), std::end(counts), 0);\n        for (auto const &str: s) {\n            counts[str[0] - 'a']++;\n        }\n        MPI_Send(counts, 26, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 26> temp;\n    std::fill(temp.begin(), temp.end(), 0);\n\n    if (rank == 0) {\n        #pragma omp parallel for\n        for (int i = 0; i < s.size(); i++) {\n            int idx = s[i].front() - 'a';\n            bins[idx] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        std::array<size_t, 26> total;\n        std::fill(total.begin(), total.end(), 0);\n        for (int i = 0; i < size; i++) {\n            if (i!= 0) {\n                MPI_Recv(temp.data(), temp.size(), MPI_UNSIGNED_LONG_LONG, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                for (int j = 0; j < temp.size(); j++) {\n                    total[j] += temp[j];\n                }\n            }\n        }\n        std::copy(total.begin(), total.end(), bins.begin());\n    }\n\n    else {\n        MPI_Send(temp.data(), temp.size(), MPI_UNSIGNED_LONG_LONG, 0, rank, MPI_COMM_WORLD);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// Your code here\n    // 1) Use MPI for communication (you can use any other method to split the work)\n    // 2) Use OpenMP to speed up the inner loop\n}",
            "// TODO: Fill this in\n}",
            "int N = s.size(); // size of array\n    int rank = 0, size = 0;\n    int i = 0, j = 0;\n    int k = 0, l = 0;\n    int k_max = 0, l_max = 0;\n    char c_l = 0, c_k = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // create equal chunks of s to send to other nodes\n    std::vector<std::string> s_send;\n\n    int chunk_size = N / size;\n    int remainder = N % size;\n\n    // if rank is 0, send N-1 chunks of size chunk_size + remainder + 1 to all other nodes\n    if (rank == 0) {\n\n        for (i = 1; i < size; i++) {\n\n            s_send.clear();\n\n            for (j = 0; j < chunk_size + remainder; j++) {\n                s_send.push_back(s[k]);\n                k++;\n            }\n            remainder--;\n            MPI_Send(s_send.data(), s_send.size(), MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n\n        // send remainder to rank 0\n        s_send.clear();\n        s_send.resize(chunk_size);\n\n        for (j = 0; j < remainder; j++) {\n            s_send[j] = s[k];\n            k++;\n        }\n\n        MPI_Send(s_send.data(), remainder, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // if rank is not 0, receive N-1 chunks of size chunk_size + remainder + 1 from rank 0\n    else {\n\n        s_send.resize(chunk_size + remainder + 1);\n\n        MPI_Recv(s_send.data(), s_send.size(), MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        std::cout << s_send[0];\n    }\n\n    // now, count the first letters in the s_send vector of each node\n    // assume each node has a different number of strings to process\n    // use OpenMP to process the strings in parallel\n    // store the counts in the bins array\n\n    if (rank == 0) {\n\n        bins.fill(0);\n\n        #pragma omp parallel for private(i, k, c_k, c_l)\n        for (i = 0; i < s.size(); i++) {\n            c_l = s[i][0];\n            c_k = c_l - 'a';\n            bins[c_k]++;\n        }\n\n        // sum all the counts in bins array from all the nodes\n        for (i = 1; i < size; i++) {\n\n            std::array<size_t, 26> bins_recv;\n\n            MPI_Recv(bins_recv.data(), 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for (j = 0; j < 26; j++) {\n                bins[j] += bins_recv[j];\n            }\n        }\n    }\n\n    // if rank is not 0, send the counts in bins array back to rank 0\n    else {\n\n        MPI_Send(bins.data(), 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "int rank, numRanks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n  std::array<size_t, 26> localBins{};\n  for (int i = 0; i < s.size(); i++) {\n    localBins[s[i][0] - 'a'] += 1;\n  }\n\n  std::array<size_t, 26> tmp{};\n\n  MPI_Reduce(&localBins, &tmp, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = tmp;\n  }\n}",
            "// TODO\n\n}",
            "#pragma omp parallel for\n  for(auto& str : s) {\n    #pragma omp atomic\n    bins[str[0] - 'a']++;\n  }\n}",
            "// TODO: Fill this in\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> s_rank;\n    std::vector<std::string> s_rank_result;\n\n    int n = s.size();\n\n    int chunk = n / size;\n    int rest = n % size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&s.at(0) + i * chunk, chunk + (i < rest? 1 : 0), MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(&s_rank[0], chunk + (rank < rest? 1 : 0), MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        s_rank.resize(chunk + (rank < rest? 1 : 0));\n    } else {\n        s_rank.resize(chunk + (rank < rest? 1 : 0));\n    }\n\n    if (rank == 0) {\n        s_rank_result.resize(chunk + (rank < rest? 1 : 0));\n    } else {\n        s_rank_result.resize(chunk + (rank < rest? 1 : 0));\n    }\n\n    if (rank == 0) {\n\n        for (int i = 0; i < chunk + (rank < rest? 1 : 0); i++) {\n            s_rank_result.at(i) = s.at(i);\n        }\n\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&s_rank_result.at(0) + i * chunk, chunk + (i < rest? 1 : 0), MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < s_rank.size(); i++) {\n        std::string word = s_rank.at(i);\n        if (word.length()!= 0) {\n            char letter = word.at(0);\n            if (letter >= 'a' && letter <= 'z') {\n                bins[letter - 'a']++;\n            }\n        }\n    }\n\n}",
            "int nproc, procId;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &procId);\n\n    if (procId == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (auto const& str : s) {\n            int bin = str.front() - 'a';\n            bins[bin]++;\n        }\n    }\n    else {\n        std::fill(bins.begin(), bins.end(), 0);\n        int len = s.size() / nproc;\n        int rem = s.size() % nproc;\n        int start = procId * len;\n        int end = (procId == nproc - 1)? s.size() : start + len;\n        for (int i = start; i < end; ++i) {\n            int bin = s[i].front() - 'a';\n            bins[bin]++;\n        }\n    }\n\n    // TODO\n}",
            "// You can assume there is at least one element in s.\n  //\n  // Note that bins[0] counts the number of strings that start with an 'a'.\n  // bins[1] counts the number of strings that start with an 'b', etc.\n  //\n  // Make sure you use OpenMP for parallelization and you have the following\n  // number of threads for each rank:\n  //\n  // 1. 8 for rank 0\n  // 2. 2 for ranks 1 to 12 (total 24 threads)\n  // 3. 4 for ranks 13 to 16 (total 48 threads)\n  // 4. 2 for ranks 17 to 20 (total 64 threads)\n  // 5. 4 for ranks 21 to 24 (total 128 threads)\n\n  // YOUR CODE HERE\n  int size, rank, nthreads;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  omp_set_num_threads(8);\n  omp_set_nested(1);\n  if (rank == 0) {\n    nthreads = 8;\n  } else if (rank >= 1 && rank <= 12) {\n    nthreads = 2;\n  } else if (rank >= 13 && rank <= 16) {\n    nthreads = 4;\n  } else if (rank >= 17 && rank <= 20) {\n    nthreads = 2;\n  } else if (rank >= 21 && rank <= 24) {\n    nthreads = 4;\n  } else {\n    nthreads = 1;\n  }\n  const int first_char = 'a';\n  const int last_char = 'z';\n  const int num_char = last_char - first_char;\n\n  std::array<size_t, 26> local_bins;\n  local_bins.fill(0);\n  auto nstrings = s.size();\n  auto chunk_size = nstrings / size;\n  int lower = rank * chunk_size;\n  int upper = (rank + 1) * chunk_size;\n  if (rank == size - 1)\n    upper = nstrings;\n  #pragma omp parallel num_threads(nthreads)\n  {\n    #pragma omp for schedule(static, 1)\n    for (int i = lower; i < upper; i++) {\n      auto first_letter = s[i][0];\n      if (first_letter >= first_char && first_letter <= last_char) {\n        #pragma omp atomic\n        local_bins[first_letter - first_char]++;\n      }\n    }\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<int> local_vector;\n\n    if (world_rank == 0)\n    {\n        local_vector.resize(s.size());\n\n        for (int i = 1; i < world_size; i++)\n        {\n            MPI_Send(&s[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else\n    {\n        MPI_Recv(&local_vector[0], s.size(), MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_vector.size(); i++)\n    {\n        if (local_vector[i] < 97 || local_vector[i] > 122) continue;\n        bins[local_vector[i] - 97]++;\n    }\n\n    if (world_rank == 0)\n    {\n        for (int i = 1; i < world_size; i++)\n        {\n            std::vector<int> recv_vector;\n            MPI_Recv(&recv_vector[0], s.size(), MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < recv_vector.size(); i++)\n            {\n                if (recv_vector[i] < 97 || recv_vector[i] > 122) continue;\n                bins[recv_vector[i] - 97]++;\n            }\n        }\n    }\n}",
            "int num_threads = 0;\n  #pragma omp parallel\n  {\n    num_threads = omp_get_num_threads();\n  }\n\n  int num_ranks = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (num_ranks!= num_threads) {\n    std::cout << \"Error: number of MPI ranks (\" << num_ranks << \") is not equal to number of threads (\" << num_threads << \")\" << std::endl;\n    return;\n  }\n\n  if (rank == 0) {\n    // TODO: use MPI_Scatter to send each rank a portion of s. Use MPI_Send for 0th thread on each rank.\n    // TODO: use OpenMP to count the number of strings in s that start with a letter\n    // TODO: use MPI_Gather to send each rank a portion of bins to rank 0\n  } else {\n    // TODO: use MPI_Recv to receive a portion of s from rank 0\n    // TODO: use OpenMP to count the number of strings in s that start with a letter\n    // TODO: use MPI_Send to send a portion of bins to rank 0\n  }\n}",
            "bins.fill(0);\n\n    // Replace this line with your solution.\n}",
            "// TODO: your code here\n}",
            "// TODO: Complete this\n}",
            "bins.fill(0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    // TODO\n  }\n}",
            "// TODO: Your code goes here\n    int size, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0){\n        omp_set_num_threads(size);\n        for (int i = 0; i < 26; i++){\n            #pragma omp parallel for\n            for (int j = 0; j < s.size(); j++){\n                if(s[j][0] == i + 'a'){\n                    bins[i]++;\n                }\n            }\n        }\n    }\n\n    if(rank!= 0){\n        omp_set_num_threads(size);\n        for (int i = 0; i < 26; i++){\n            #pragma omp parallel for\n            for (int j = 0; j < s.size(); j++){\n                if(s[j][0] == i + 'a'){\n                    bins[i]++;\n                }\n            }\n        }\n    }\n\n    // TODO: End your code here\n}",
            "int size;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Write code here\n\n    // For correctness, we need to broadcast the result to all ranks.\n    MPI_Bcast(&bins, 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "int numRanks, rankId;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rankId);\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n  int binSize = bins.size();\n  int numBinsPerRank = (binSize + numRanks - 1) / numRanks;\n  int binStart = rankId * numBinsPerRank;\n  int binEnd = std::min(binStart + numBinsPerRank, binSize);\n\n  for (int binId = binStart; binId < binEnd; binId++) {\n    bins[binId] = 0;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    int binId = s[i][0] - 'a';\n    if (binId >= 0 && binId < binSize) {\n      bins[binId]++;\n    }\n  }\n\n  // Reduce the data from each rank into the first rank\n  std::vector<int> localBins(bins.begin() + binStart, bins.begin() + binEnd);\n  std::vector<int> globalBins(bins.size());\n  MPI_Reduce(localBins.data(), globalBins.data(), binEnd - binStart, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Copy the reduced data to the original bins array\n  if (rankId == 0) {\n    for (int binId = binStart; binId < binEnd; binId++) {\n      bins[binId] = globalBins[binId - binStart];\n    }\n  }\n}",
            "int numberOfProcess = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcess);\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<std::vector<std::string>, 26> s_parallel;\n    std::array<size_t, 26> counts = {0};\n    for(size_t i = 0; i < s.size(); i++) {\n        size_t letter_index = s[i][0] - 'a';\n        if(rank == 0) {\n            s_parallel[letter_index].push_back(s[i]);\n        } else {\n            std::vector<std::string> local;\n            MPI_Bcast(s[i].data(), s[i].length()+1, MPI_CHAR, 0, MPI_COMM_WORLD);\n            local.push_back(s[i]);\n            MPI_Send(&local, 1, MPI_CXX_STRING, 0, 1, MPI_COMM_WORLD);\n        }\n    }\n    if(rank == 0) {\n        for(int i = 1; i < numberOfProcess; i++) {\n            std::vector<std::string> local;\n            MPI_Status status;\n            MPI_Probe(i, 1, MPI_COMM_WORLD, &status);\n            int count;\n            MPI_Get_count(&status, MPI_CXX_STRING, &count);\n            local.resize(count);\n            MPI_Recv(&local, count, MPI_CXX_STRING, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(size_t j = 0; j < local.size(); j++) {\n                size_t letter_index = local[j][0] - 'a';\n                s_parallel[letter_index].push_back(local[j]);\n            }\n        }\n    }\n    if(rank == 0) {\n        for(size_t i = 0; i < s.size(); i++) {\n            size_t letter_index = s[i][0] - 'a';\n            counts[letter_index] = s_parallel[letter_index].size();\n        }\n    }\n    if(rank == 0) {\n        MPI_Reduce(counts.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(counts.data(), NULL, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Implement this function\n    // Rank 0 bins is empty.\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char c = s[i][0];\n        int r = c - 'a';\n        #pragma omp critical\n        {\n            bins[r]++;\n        }\n    }\n\n}",
            "// TODO: fill this in\n\n}",
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = s.size()/size;\n    std::vector<std::string> chunkedStrings;\n    if(rank == 0){\n        for (int i=0; i<size; i++) {\n            if (i!= size-1) {\n                chunkedStrings.insert(chunkedStrings.end(), s.begin() + i*chunk, s.begin() + (i+1)*chunk);\n            }\n            else {\n                chunkedStrings.insert(chunkedStrings.end(), s.begin() + i*chunk, s.end());\n            }\n        }\n        MPI_Scatter(chunkedStrings.data(), chunk, MPI_C_CHAR, NULL, chunk, MPI_C_CHAR, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Scatter(NULL, chunk, MPI_C_CHAR, NULL, chunk, MPI_C_CHAR, 0, MPI_COMM_WORLD);\n    }\n    if(rank == 0){\n        for(int i=0; i<s.size(); i++){\n            bins[s[i][0]-'a']++;\n        }\n    }\n    else {\n        for(int i=0; i<s.size(); i++){\n            bins[s[i][0]-'a']++;\n        }\n    }\n    MPI_Gather(bins.data(), 26, MPI_INT, NULL, 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int my_rank, comm_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    int num_threads;\n    if (my_rank == 0) {\n        num_threads = omp_get_max_threads();\n    }\n    MPI_Bcast(&num_threads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int count = s.size() / comm_size;\n    int remainder = s.size() % comm_size;\n\n    if (my_rank == 0) {\n        std::vector<std::string> s_part;\n        for (int i = 1; i < comm_size; ++i) {\n            for (int j = 0; j < count; ++j) {\n                s_part.push_back(s[i * count + j]);\n            }\n        }\n        for (int j = 0; j < remainder; ++j) {\n            s_part.push_back(s[count * comm_size + j]);\n        }\n\n        std::array<size_t, 26> bins_part = {0};\n        int number_of_threads = omp_get_max_threads();\n#pragma omp parallel num_threads(number_of_threads)\n        {\n            #pragma omp for\n            for (int i = 0; i < s_part.size(); i++) {\n                bins_part[s_part[i][0] - 'a']++;\n            }\n        }\n\n        std::vector<size_t> bins_part_vect(bins_part.begin(), bins_part.end());\n        MPI_Scatter(bins_part_vect.data(), count, MPI_INT, bins.data(), count, MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<size_t> bins_part_vect(26);\n        MPI_Scatter(0, count, MPI_INT, bins_part_vect.data(), count, MPI_INT, 0, MPI_COMM_WORLD);\n\n        std::vector<std::string> s_part;\n        for (int j = 0; j < count; ++j) {\n            s_part.push_back(s[my_rank * count + j]);\n        }\n\n        for (int j = 0; j < remainder; ++j) {\n            s_part.push_back(s[count * comm_size + j]);\n        }\n\n        int number_of_threads = omp_get_max_threads();\n#pragma omp parallel num_threads(number_of_threads)\n        {\n            #pragma omp for\n            for (int i = 0; i < s_part.size(); i++) {\n                bins_part_vect[s_part[i][0] - 'a']++;\n            }\n        }\n        MPI_Gather(bins_part_vect.data(), count, MPI_INT, bins.data(), count, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "int num_threads, rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0)\n    {\n        num_threads = 4;\n    }\n\n    MPI_Bcast(&num_threads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        std::array<size_t, 26> bins_local = {0};\n        int thread_id = omp_get_thread_num();\n        size_t total_strings = s.size();\n\n        #pragma omp for\n        for (size_t i = thread_id; i < total_strings; i += num_threads)\n        {\n            char first_letter = s[i][0];\n            if (first_letter >= 'a' && first_letter <= 'z')\n            {\n                bins_local[first_letter - 'a']++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 26; i++)\n            {\n                bins[i] += bins_local[i];\n            }\n        }\n    }\n\n    if (rank == 0)\n    {\n        std::cout << \"Number of strings starting with \\\"a\\\": \" << bins[0] << '\\n';\n    }\n}",
            "// TODO: implement\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: Your code here\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    if(rank == 0) {\n        for(int i = 1; i < size; i++) {\n            std::array<size_t, 26> counts;\n            MPI_Recv(&counts, 26, MPI_UNSIGNED_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < 26; j++) {\n                bins[j] += counts[j];\n            }\n        }\n    }\n    else {\n        MPI_Send(&bins, 26, MPI_UNSIGNED_LONG, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: add code\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n\n  // your code here\n}",
            "//...\n\n}",
            "// Your code here\n}",
            "const int root = 0;\n    const int size = 26;\n    int rank;\n    int num_of_procs;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_of_procs);\n\n    // We're going to need to keep track of some number of strings on each process.\n    // Determine how many we're going to handle on this process.\n    // (This is just an arbitrary distribution method - feel free to improve on it.)\n    int my_num_of_strings = (s.size() + num_of_procs - 1) / num_of_procs;\n\n    // Divide the vector of strings up among the processes\n    std::vector<std::string> my_s(my_num_of_strings);\n    for (int i = 0; i < my_num_of_strings; ++i) {\n        my_s[i] = s[i];\n    }\n\n    // Resize the array to match the number of characters in the alphabet.\n    // (We're using a C-style array instead of a C++ array so that we can use MPI.)\n    bins.resize(size);\n\n    // Each process will handle a range of elements in the array.\n    int start = size * rank / num_of_procs;\n    int end = size * (rank + 1) / num_of_procs;\n\n    // Count the occurrences of each letter on this process in parallel.\n    #pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        int sum = 0;\n        for (std::string const& str : my_s) {\n            if (str[0] == i) {\n                sum += 1;\n            }\n        }\n        bins[i] = sum;\n    }\n\n    // Now gather the results from all the processes.\n    // (This assumes that all processes have the same number of elements.)\n    int const elements = size * num_of_procs;\n    int const recvcounts[elements];\n\n    for (int i = 0; i < elements; ++i) {\n        recvcounts[i] = 1;\n    }\n\n    MPI_Gatherv(MPI_IN_PLACE, 1, MPI_LONG, &bins[0], recvcounts, start, MPI_LONG, root, MPI_COMM_WORLD);\n}",
            "const size_t n = s.size();\n\n    // TODO: Use MPI and OpenMP to count first letters\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < 26; ++i)\n        {\n            bins[i] = 0;\n        }\n\n        #pragma omp for\n        for (int i = 0; i < n; ++i)\n        {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n\n}",
            "size_t size = s.size();\n    if (size > 0) {\n\n        // First, count the number of strings in each rank\n        std::vector<size_t> counts(size);\n        for (size_t i = 0; i < size; i++) {\n            counts[i] = s[i].size();\n        }\n\n        // Then, gather the counts\n        std::vector<size_t> counts_gather(size*size);\n        MPI_Allgather(counts.data(), size, MPI_UNSIGNED_LONG, counts_gather.data(), size, MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n        // Initialize bins to zeros\n        bins.fill(0);\n\n        // Count the number of strings in each rank\n        size_t total_count = 0;\n        for (size_t i = 0; i < size; i++) {\n            for (size_t j = 0; j < size; j++) {\n                if (i == 0) {\n                    total_count += counts_gather[j + i*size];\n                }\n\n                // Count the number of strings starting with a certain letter\n                if (i > 0) {\n                    std::string letter = s[j];\n                    if (letter.size() > 0 && letter[0] == (char)('a' + i - 1)) {\n                        bins[i - 1] += counts_gather[j + i*size];\n                    }\n                }\n            }\n        }\n\n        // Use MPI_Reduce to sum up all the bins\n        std::vector<size_t> bins_reduced(bins.size());\n        MPI_Reduce(bins.data(), bins_reduced.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        if (MPI_COMM_WORLD->rank == 0) {\n            bins = bins_reduced;\n        }\n    }\n}",
            "// TODO\n    const int comm_size = 0, my_rank = 0;\n    int num_threads = 0, thread_num = 0;\n    int *local_bins, *global_bins;\n    // allocate the local_bins memory space\n    local_bins = new int[26];\n    // fill the array with zeros\n    for(int i = 0; i < 26; i++){\n        local_bins[i] = 0;\n    }\n    // find the number of threads\n    num_threads = omp_get_num_threads();\n    // determine thread number\n    thread_num = omp_get_thread_num();\n    // calculate the number of strings for each thread\n    int num_strings = s.size() / num_threads;\n    // calculate the starting index for each thread\n    int start = thread_num * num_strings;\n    // calculate the ending index for each thread\n    int end = start + num_strings;\n    // find the number of strings for the remaining threads\n    num_strings = s.size() % num_threads;\n    // if thread_num is greater than or equal to the remaining number of strings, then adjust the end\n    if(thread_num >= num_strings){\n        end = end + num_strings;\n    }\n    // otherwise, adjust the end by 1\n    else{\n        end = end + 1;\n    }\n    // calculate the bins for each thread\n    for(int i = start; i < end; i++){\n        local_bins[s[i][0] - 'a'] += 1;\n    }\n    // allocate global_bins\n    global_bins = new int[26];\n    // initialize global_bins with zeros\n    for(int i = 0; i < 26; i++){\n        global_bins[i] = 0;\n    }\n    // gather the local_bins from each thread to global_bins\n    MPI_Gather(local_bins, 26, MPI_INT, global_bins, 26, MPI_INT, 0, MPI_COMM_WORLD);\n    // if rank is 0, calculate the final results and put them in the bins array\n    if(my_rank == 0){\n        for(int i = 0; i < 26; i++){\n            for(int j = 0; j < comm_size; j++){\n                global_bins[i] = global_bins[i] + global_bins[j * 26 + i];\n            }\n        }\n        for(int i = 0; i < 26; i++){\n            bins[i] = global_bins[i];\n        }\n    }\n    // free memory\n    delete[] local_bins;\n    delete[] global_bins;\n}",
            "size_t const rank = MPI::COMM_WORLD.Get_rank();\n    size_t const size = MPI::COMM_WORLD.Get_size();\n\n    // TODO: implement\n}",
            "int num_threads = 0;\n    int num_procs = 0;\n    int rank = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        num_threads = omp_get_max_threads();\n    }\n\n    MPI_Bcast(&num_threads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank!= 0) {\n        omp_set_num_threads(num_threads);\n    }\n\n    #pragma omp parallel for\n    for (size_t idx = 0; idx < s.size(); ++idx) {\n        char letter = tolower(s[idx][0]);\n        if (letter >= 'a' && letter <= 'z') {\n            #pragma omp atomic\n            bins[letter - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel num_threads(4)\n    {\n        #pragma omp for schedule(static,1)\n        for(size_t i = 0; i < s.size(); i++) {\n\n            if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n                #pragma omp atomic\n                bins[s[i][0] - 'a']++;\n            }\n\n        }\n    }\n\n}",
            "// This is the code I have so far, but I am not sure if it's correct or how to use OpenMP to further it.\n  MPI_Status status;\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if(rank!= 0){\n  for(int i = 0; i < s.size(); i++){\n    if(s[i][0] == 'a' || s[i][0] == 'b' || s[i][0] == 'c' || s[i][0] == 'd' || s[i][0] == 'e' || s[i][0] == 'f' || s[i][0] == 'g' || s[i][0] == 'h' || s[i][0] == 'i' || s[i][0] == 'j' || s[i][0] == 'k' || s[i][0] == 'l' || s[i][0] =='m' || s[i][0] == 'n' || s[i][0] == 'o' || s[i][0] == 'p' || s[i][0] == 'q' || s[i][0] == 'r' || s[i][0] =='s' || s[i][0] == 't' || s[i][0] == 'u' || s[i][0] == 'v' || s[i][0] == 'w' || s[i][0] == 'x' || s[i][0] == 'y' || s[i][0] == 'z'){\n      bins[0] += 1;\n    }\n  }\n  }\n  else{\n    bins[0] += 1;\n  }\n\n}",
            "int n_ranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n_threads = omp_get_max_threads();\n\n    #pragma omp parallel num_threads(n_threads)\n    {\n        int thread_id = omp_get_thread_num();\n        // Use the thread id to partition the input vector s.\n        // The first thread gets the first portion of the input vector, the second thread gets the second portion, etc.\n        // For example, if thread 0 has the first half of the input vector, thread 1 has the second half.\n        //\n        // You may use any method to partition the input vector.\n        // Assume the thread id is in the range [0, n_threads).\n\n        if(rank == 0)\n        {\n            for(size_t i = 0; i < s.size(); ++i)\n            {\n                // Compute the bin for the current string\n                int bin = /* code here */ ;\n\n                // The following code needs to be executed by only one thread.\n                #pragma omp critical\n                {\n                    // Add to the counter\n                    bins[bin] += 1;\n                }\n            }\n        }\n    }\n}",
            "// Your code here\n\n    int n = s.size();\n\n    MPI_Init(NULL, NULL);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        int block_size = n/size;\n        int remain = n%size;\n        std::vector<std::string> s_rank0;\n        std::vector<std::string> s_temp;\n\n        for (int i=1; i<size; i++) {\n            MPI_Send(&block_size, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&s[i*block_size], block_size, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n            s_temp.resize(block_size);\n            MPI_Recv(&s_temp[0], block_size, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            s_rank0.insert(s_rank0.end(), s_temp.begin(), s_temp.end());\n        }\n        s_rank0.insert(s_rank0.end(), s.begin(), s.begin() + block_size);\n        s_rank0.insert(s_rank0.end(), s.begin() + (block_size + remain)*size, s.end());\n\n        for (int i=0; i<s_rank0.size(); i++) {\n            bins[s_rank0[i][0]-97] += 1;\n        }\n    }\n\n    else {\n        int block_size;\n        int remain;\n        int local_rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &local_rank);\n        MPI_Recv(&block_size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        std::vector<std::string> s_temp(block_size);\n        MPI_Recv(&s_temp[0], block_size, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n        for (int i=0; i<s_temp.size(); i++) {\n            bins[s_temp[i][0]-97] += 1;\n        }\n    }\n\n    MPI_Finalize();\n\n}",
            "// TODO: Your code here\n}",
            "// TODO: your code here\n  size_t n = s.size();\n  int rank = 0, size = 1;\n\n  // Get the rank number\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Divide the work\n  int N = n / size;\n  int rem = n % size;\n\n  // Allocate memory for the string array\n  std::string *sub;\n  if (rank == 0) {\n    sub = new std::string[n];\n  }\n\n  // Allocate memory for the bins array\n  std::array<size_t, 26> *subBins;\n  if (rank == 0) {\n    subBins = new std::array<size_t, 26>;\n  }\n\n  // Copy the subarray from the complete array\n  MPI_Scatter(s.data(), N, MPI_STRING, sub, N, MPI_STRING, 0, MPI_COMM_WORLD);\n  if (rank!= 0) {\n    sub = new std::string[N + rem];\n    MPI_Scatter(s.data(), N + rem, MPI_STRING, sub, N + rem, MPI_STRING, 0, MPI_COMM_WORLD);\n  }\n\n  // Initialise bins array\n  if (rank == 0) {\n    std::fill(subBins->begin(), subBins->end(), 0);\n  }\n  else {\n    std::fill(subBins->begin(), subBins->end(), 0);\n  }\n\n  // Count the letters\n  for (int i = 0; i < N + rem; i++) {\n    char c = sub[i][0];\n    (*subBins)[c - 97]++;\n  }\n\n  // Combine all the bins arrays into a single one on rank 0\n  MPI_Reduce(subBins, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Cleanup\n  delete[] sub;\n  if (rank!= 0) {\n    delete[] subBins;\n  }\n}",
            "int my_rank, comm_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    int bins_per_rank = 26 / comm_size;\n    int rest = 26 % comm_size;\n\n    std::vector<size_t> my_bins(bins_per_rank);\n\n    int count = 0;\n    for (std::string& word : s) {\n        if (count % comm_size == my_rank) {\n            int index = word[0] - 'a';\n            if (index >= 0 && index <= 25) {\n                ++my_bins[index];\n            }\n        }\n        ++count;\n    }\n\n    std::vector<size_t> global_bins(bins_per_rank);\n\n    MPI_Gather(my_bins.data(), bins_per_rank, MPI_UNSIGNED,\n                global_bins.data(), bins_per_rank, MPI_UNSIGNED,\n                0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        for (int i = 0; i < rest; ++i) {\n            bins[i] = global_bins[i];\n        }\n\n        int sum = 0;\n        for (int i = 0; i < rest; ++i) {\n            sum += bins[i];\n            bins[i] = sum;\n        }\n\n        for (int i = 1; i < comm_size; ++i) {\n            for (int j = 0; j < bins_per_rank; ++j) {\n                global_bins[j] = global_bins[j + bins_per_rank * i];\n            }\n\n            for (int j = 0; j < bins_per_rank; ++j) {\n                bins[j + bins_per_rank * i] += sum;\n            }\n\n            sum += bins[25 + bins_per_rank * i];\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::array<size_t, 26> myBins;\n    myBins.fill(0);\n    if (rank == 0) {\n        for (int i = 0; i < s.size(); ++i) {\n            if (i % size == rank)\n                myBins[s[i][0] - 'a']++;\n        }\n    } else {\n        for (int i = 0; i < s.size(); ++i) {\n            if (i % size == rank)\n                myBins[s[i][0] - 'a']++;\n        }\n    }\n    MPI_Reduce(myBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel\n    {\n        // Initialize the bin count to zero.\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = 0;\n        }\n\n        // Update the bins in parallel.\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); ++i) {\n            int letter = s[i][0] - 'a';\n            if (letter < 0 || letter > 25) {\n                throw \"invalid input\";\n            }\n            bins[letter]++;\n        }\n    }\n}",
            "int numOfThreads, threadID;\n\n  // Your code goes here\n\n}",
            "int mpi_size, mpi_rank, i;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    std::vector<std::string> myStrings;\n\n    if(mpi_rank == 0){\n        for (size_t i = mpi_rank*s.size()/mpi_size; i < (mpi_rank+1)*s.size()/mpi_size; i++)\n            myStrings.push_back(s[i]);\n    }\n    else\n        for (size_t i = mpi_rank*s.size()/mpi_size; i < (mpi_rank+1)*s.size()/mpi_size; i++)\n            myStrings.push_back(s[i]);\n\n    int num_threads;\n    #pragma omp parallel\n    num_threads = omp_get_num_threads();\n\n    int num_threads_per_rank = num_threads / mpi_size;\n\n    #pragma omp parallel num_threads(num_threads_per_rank)\n    {\n        int local_rank = omp_get_thread_num();\n        int global_rank = omp_get_thread_num() + local_rank * num_threads_per_rank;\n        char startChar = s[global_rank].front();\n        if(startChar > 90 || startChar < 65)\n            startChar = 65;\n        bins[startChar - 65]++;\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n    MPI_Bcast(s.data(), s.size(), MPI_CXX_STRING, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (auto str : s) {\n            if (str.size() > 0 && (int)str[0] - 'a' == i)\n                bins[i]++;\n        }\n    }\n\n    // TODO: Your code here\n\n}",
            "int size, rank, num_threads;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_threads);\n    std::array<std::vector<std::string>, 26> temp;\n    std::array<size_t, 26> temp_res;\n    // std::array<size_t, 26> temp_res;\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            temp_res[i] = 0;\n        }\n    }\n    if (rank == 0) {\n        for (int i = 0; i < s.size(); i++) {\n            temp[s[i][0] - 97].push_back(s[i]);\n        }\n    }\n    MPI_Bcast(&(temp[0]), 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 26; i++) {\n        if (temp[i].size()!= 0) {\n            #pragma omp parallel for\n            for (int j = 0; j < temp[i].size(); j++) {\n                for (int k = 1; k < temp[i][j].length(); k++) {\n                    if (temp[i][j][k] == 'a') {\n                        temp_res[0]++;\n                    } else if (temp[i][j][k] == 'b') {\n                        temp_res[1]++;\n                    } else if (temp[i][j][k] == 'c') {\n                        temp_res[2]++;\n                    } else if (temp[i][j][k] == 'd') {\n                        temp_res[3]++;\n                    } else if (temp[i][j][k] == 'e') {\n                        temp_res[4]++;\n                    } else if (temp[i][j][k] == 'f') {\n                        temp_res[5]++;\n                    } else if (temp[i][j][k] == 'g') {\n                        temp_res[6]++;\n                    } else if (temp[i][j][k] == 'h') {\n                        temp_res[7]++;\n                    } else if (temp[i][j][k] == 'i') {\n                        temp_res[8]++;\n                    } else if (temp[i][j][k] == 'j') {\n                        temp_res[9]++;\n                    } else if (temp[i][j][k] == 'k') {\n                        temp_res[10]++;\n                    } else if (temp[i][j][k] == 'l') {\n                        temp_res[11]++;\n                    } else if (temp[i][j][k] =='m') {\n                        temp_res[12]++;\n                    } else if (temp[i][j][k] == 'n') {\n                        temp_res[13]++;\n                    } else if (temp[i][j][k] == 'o') {\n                        temp_res[14]++;\n                    } else if (temp[i][j][k] == 'p') {\n                        temp_res[15]++;\n                    } else if (temp[i][j][k] == 'q') {\n                        temp_res[16]++;\n                    } else if (temp[i][j][k] == 'r') {\n                        temp_res[17]++;\n                    } else if (temp[i][j][k] =='s') {\n                        temp_res[18]++;\n                    } else if (temp[i][j][k] == 't') {\n                        temp_res[19]++;\n                    } else if (temp[i][j][k] == 'u') {\n                        temp_res[20]++;\n                    } else if (temp[i][j][k] == 'v') {\n                        temp_",
            "// YOUR CODE GOES HERE\n    int nprocs, myrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    std::vector<std::string> local(s.size()/nprocs);\n    if(myrank == 0)\n    {\n        for(int i = 0; i < s.size(); i++)\n        {\n            int proc = i%nprocs;\n            MPI_Send(&s[i], 1, MPI_CHAR, proc, 0, MPI_COMM_WORLD);\n        }\n    }\n    else\n    {\n        for(int i = 0; i < s.size()/nprocs; i++)\n        {\n            MPI_Recv(&local[i], 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    std::array<size_t, 26> local_bin;\n    if(myrank!= 0)\n    {\n        for(int i = 0; i < local.size(); i++)\n        {\n            char first = local[i][0];\n            int index = (int)(first - 97);\n            local_bin[index] += 1;\n        }\n    }\n    MPI_Gather(local_bin.data(), 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "/* Your code goes here */\n\n}",
            "bins.fill(0);\n   const size_t size = s.size();\n   const size_t rank = omp_get_thread_num();\n   const size_t num_threads = omp_get_num_threads();\n   const size_t chunk_size = size / num_threads;\n   const size_t remainder = size % num_threads;\n   const size_t low = rank * chunk_size;\n   const size_t high = (rank == num_threads - 1)? size : (rank + 1) * chunk_size;\n   for (size_t i = low; i < high; i++) {\n       char c = s[i][0];\n       if (c >= 'a' && c <= 'z') {\n           bins[c - 'a']++;\n       }\n   }\n}",
            "int worldSize;\n  int worldRank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n  int numberOfThreads;\n  omp_set_num_threads(worldSize);\n  omp_get_num_threads(&numberOfThreads);\n\n  if (worldRank == 0) {\n    for (size_t i = 0; i < s.size(); i++) {\n      if (s[i].size() > 0) {\n        bins[s[i][0] - 'a']++;\n      }\n    }\n  }\n\n  #pragma omp parallel for\n  for (int i = 1; i < worldSize; i++) {\n    for (size_t j = 0; j < s.size(); j++) {\n      if (s[j].size() > 0) {\n        bins[s[j][0] - 'a']++;\n      }\n    }\n  }\n}",
            "// TODO\n    int n = s.size();\n    int rank = 0, size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n_per_rank = n / size;\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(s.data() + i * n_per_rank, n_per_rank, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 0; i < n_per_rank; i++) {\n            bins[s[i][0] - 'a'] += 1;\n        }\n    } else {\n        std::vector<std::string> local_s(n_per_rank);\n        MPI_Recv(local_s.data(), n_per_rank, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (int i = 0; i < n_per_rank; i++) {\n            bins[local_s[i][0] - 'a'] += 1;\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    if (rank!= 0) {\n        MPI_Send(bins.data(), 26, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 26> other_bins;\n            MPI_Recv(other_bins.data(), 26, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 26; j++) {\n                bins[j] += other_bins[j];\n            }\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char letter = s[i][0];\n        ++bins[letter - 'a'];\n    }\n}",
            "int npes, rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &npes);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO:\n  // - Compute how many strings are stored locally in s.\n  // - Count the number of strings starting with each letter in parallel using OpenMP.\n  //   Use the prefix sum to do it in one loop.\n  // - Use MPI_Reduce to sum the results on rank 0.\n\n  MPI_Finalize();\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int chunkSize = s.size() / size;\n  int remainder = s.size() % size;\n\n  std::vector<std::string> localCopy(chunkSize + (rank < remainder? 1 : 0));\n\n  // TODO: Use OpenMP to compute the counts in parallel\n\n  if(rank == 0)\n  {\n      for(int i = 0; i < size; i++)\n      {\n          if(i < remainder)\n          {\n              localCopy[i].resize(chunkSize + 1);\n              MPI_Recv(&localCopy[i][0], chunkSize + 1, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n          }\n          else\n          {\n              localCopy[i].resize(chunkSize);\n              MPI_Recv(&localCopy[i][0], chunkSize, MPI_CHAR, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n          }\n\n      }\n\n  }\n  else\n  {\n      if(rank < remainder)\n      {\n          MPI_Send(&localCopy[0][0], chunkSize + 1, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n      }\n      else\n      {\n          MPI_Send(&localCopy[0][0], chunkSize, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n      }\n  }\n\n  // TODO: Use OpenMP to compute the counts in parallel\n\n}",
            "// TODO\n}",
            "// TODO\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      MPI_Send(&s[i], 1, MPI_C_CHAR, i, i, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Recv(&s[0], 1, MPI_C_CHAR, 0, rank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  int index = 0;\n  for (int i = 0; i < size; i++) {\n    if (rank == i) {\n      // for (auto it = s[i].begin(); it!= s[i].end(); it++) {\n      //   bins[(*it)-'a']++;\n      // }\n      if (index < s.size()) {\n        for (int j = 0; j < s[index].length(); j++) {\n          bins[s[index][j]-'a']++;\n        }\n      }\n    }\n  }\n\n  // if (rank == 0) {\n  //   for (int i = 0; i < size; i++) {\n  //     MPI_Recv(&bins, 26, MPI_INT, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  //   }\n  // } else {\n  //   MPI_Send(&bins, 26, MPI_INT, 0, rank, MPI_COMM_WORLD);\n  // }\n\n  // if (rank == 0) {\n  //   for (int i = 0; i < size; i++) {\n  //     if (i!= 0) {\n  //       MPI_Recv(&bins, 26, MPI_INT, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  //     }\n  //   }\n  // } else {\n  //   MPI_Send(&bins, 26, MPI_INT, 0, rank, MPI_COMM_WORLD);\n  // }\n\n  // if (rank == 0) {\n  //   for (int i = 0; i < size; i++) {\n  //     if (i!= 0) {\n  //       MPI_Recv(&bins, 26, MPI_INT, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  //     }\n  //   }\n  // } else {\n  //   MPI_Send(&bins, 26, MPI_INT, 0, rank, MPI_COMM_WORLD);\n  // }\n\n  MPI_Reduce(&bins, &bins, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code goes here!\n  // TODO: add code\n}",
            "// Fill in this function\n}",
            "#pragma omp parallel for schedule(dynamic, 1)\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "}",
            "#pragma omp parallel\n    {\n        // Use bins as shared memory, and each thread should have its own copy of `s`.\n        #pragma omp for nowait\n        for (auto i = 0u; i < s.size(); ++i) {\n            auto const& word = s[i];\n            if (!word.empty()) {\n                auto const& c = word[0];\n                if (c >= 'a' && c <= 'z') {\n                    bins[c - 'a']++;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    int my_rank = omp_get_thread_num();\n    int n_threads = omp_get_num_threads();\n    int n_total = s.size();\n    int chunk = n_total/n_threads;\n    int start = my_rank * chunk;\n    int end = (my_rank+1) * chunk;\n    if (my_rank == n_threads-1)\n      end = n_total;\n\n    std::array<size_t, 26> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    for (int i=start; i<end; i++) {\n      int letter = s[i][0] - 'a';\n      local_bins[letter] += 1;\n    }\n\n    #pragma omp critical\n    for (int i=0; i<26; i++)\n      bins[i] += local_bins[i];\n\n  }\n}",
            "// Your code goes here\n}",
            "int np, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &np);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    // You may assume that the vector is not empty.\n    size_t n = s.size();\n    // First calculate the number of words per process.\n    // (We could use floor and ceil, but that would make the code less readable)\n    size_t nper = n / np;\n    size_t nleft = n % np;\n    for (int i = 1; i < np; i++) {\n      if (i <= nleft) {\n        MPI_Send(&s[i*nper], nper + 1, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n      } else {\n        MPI_Send(&s[nleft*nper + (i - nleft)*(nper - 1)], nper - 1, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n      }\n    }\n    // Then do the count for rank 0.\n    for (int i = 0; i < nper; i++) {\n      countLetter(s[i], bins);\n    }\n  } else {\n    // Now do the count for the other processes.\n    std::vector<std::string> slocal;\n    slocal.resize(1000);\n    MPI_Status stat;\n    MPI_Recv(&slocal[0], 1000, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &stat);\n    countLetter(slocal, bins);\n  }\n}",
            "constexpr char firstLetter = 'a';\n    constexpr size_t numRanks = 4;\n    constexpr size_t rank = 0;\n\n    #pragma omp parallel\n    {\n        auto s_local = s;\n        size_t start = (omp_get_thread_num() * s.size()) / omp_get_num_threads();\n        size_t end = start + s.size() / omp_get_num_threads();\n        end = end > s.size()? s.size() : end;\n        std::array<size_t, 26> bins_local;\n        bins_local.fill(0);\n\n        for(size_t i = start; i < end; ++i)\n            bins_local[s_local[i][0] - firstLetter]++;\n\n        // TODO: use MPI to communicate bins_local among all ranks and compute the final result in bins\n    }\n}",
            "}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < s.size(); i++) {\n    char c = s[i][0];\n    if(c >= 'a' && c <= 'z') {\n      #pragma omp atomic\n      bins[c - 'a']++;\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // The size of the subvector for each rank\n    int subSize = s.size() / size;\n\n    // If the number of elements is not divisible by the number of ranks,\n    // then add the leftover to the first few ranks\n    if (rank < s.size() % size) {\n        subSize++;\n    }\n\n    // The subvector for each rank\n    std::vector<std::string> subVec(s.begin() + subSize * rank, s.begin() + subSize * (rank + 1));\n\n    // Number of letters in each bin\n    std::array<size_t, 26> binsLocal;\n\n    // Count the number of strings in each bin\n    if (rank == 0) {\n        // Use the first rank to initialize the bins array\n        for (size_t i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n    }\n\n#pragma omp parallel for\n    for (size_t i = 0; i < subVec.size(); i++) {\n        // Use the first letter of the string as the index\n        binsLocal[subVec[i][0] - 'a']++;\n    }\n\n    MPI_Reduce(binsLocal.data(), bins.data(), 26, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int size = s.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    std::vector<std::string> local_s(s.size() / nproc);\n\n    if (rank == 0) {\n        for (int i = 1; i < nproc; ++i) {\n            MPI_Send(s.data() + i * size / nproc, size / nproc, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n        local_s = std::vector<std::string>(s.begin(), s.begin() + size / nproc);\n    }\n    else {\n        MPI_Recv(local_s.data(), size / nproc, MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    std::array<size_t, 26> local_bins = {};\n    for (const std::string& str : local_s) {\n        if (str.size() > 0) {\n            local_bins[str.front() - 'a'] += 1;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: your code here\n  int my_rank;\n  int p;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n  std::vector<std::string> my_string;\n  if(my_rank == 0) {\n      my_string.resize(s.size()/p);\n  }\n  MPI_Scatter(s.data(), s.size()/p, MPI_CHAR, my_string.data(), s.size()/p, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 26> my_bins;\n  std::fill(my_bins.begin(), my_bins.end(), 0);\n\n#pragma omp parallel for\n  for (int i = 0; i < my_string.size(); i++) {\n    int char_code = my_string[i][0];\n    my_bins[char_code-97]++;\n  }\n\n  std::vector<std::array<size_t, 26>> my_bins_vector;\n  my_bins_vector.resize(p);\n\n  MPI_Gather(my_bins.data(), my_bins.size(), MPI_UNSIGNED_LONG_LONG, my_bins_vector.data(), my_bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  if(my_rank == 0) {\n      for (int i = 0; i < p; i++) {\n          for (int j = 0; j < my_bins_vector[i].size(); j++) {\n              bins[j] += my_bins_vector[i][j];\n          }\n      }\n  }\n\n}",
            "// You need to write this function\n}",
            "// TODO: replace with your code\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<std::string> s_rank;\n    int start = 0;\n    int end = 0;\n\n    for (int i = 0; i < size; ++i) {\n        start = end;\n        end = ((s.size() / size) * (i + 1));\n        if (i == rank) {\n            s_rank = std::vector<std::string>(s.begin() + start, s.begin() + end);\n        }\n        MPI_Bcast(s_rank.data(), s_rank.size(), MPI_CHAR, i, MPI_COMM_WORLD);\n    }\n\n    #pragma omp parallel\n    {\n        for (size_t i = 0; i < s_rank.size(); ++i) {\n            char c = s_rank[i][0];\n            #pragma omp atomic\n            bins[c - 'a']++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    int const size = omp_get_max_threads();\n    int const rank = omp_get_thread_num();\n\n    #pragma omp parallel for schedule(static) num_threads(size)\n    for (int i = 0; i < s.size(); i++) {\n        char c = s[i][0];\n        int bin = c - 'a';\n        bins[bin]++;\n    }\n\n    MPI_Reduce(bins.data(), NULL, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char letter = s[i][0];\n        if (letter >= 'a' && letter <= 'z') {\n            int index = s[i][0] - 'a';\n            #pragma omp atomic\n            bins[index]++;\n        }\n    }\n}",
            "int world_size = omp_get_num_threads();\n    int rank = omp_get_thread_num();\n    int tag = 11;\n    int dest = 0;\n\n    int count_per_thread = s.size() / world_size;\n    std::vector<std::string> sub_s(s.begin() + count_per_thread * rank,\n                                   s.begin() + count_per_thread * (rank + 1));\n\n    int sub_s_size = sub_s.size();\n    std::array<int, 26> sub_bins{};\n\n    // count the number of strings that start with each letter in the alphabet\n#pragma omp parallel for\n    for (int i = 0; i < sub_s_size; ++i) {\n        if (sub_s[i].size() > 0) {\n            ++sub_bins[sub_s[i][0] - 'a'];\n        }\n    }\n\n    // MPI_Reduce(const void* sendbuf, void* recvbuf, int count, MPI_Datatype datatype,\n    // MPI_Op op, int root, MPI_Comm comm)\n    MPI_Reduce(sub_bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, dest, MPI_COMM_WORLD);\n}",
            "// Your code here\n\n    MPI_Status status;\n\n    int my_id, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_id);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    std::vector<std::string> all_strings;\n    if (my_id == 0)\n        all_strings = s;\n\n    for (int i = 1; i < num_procs; i++) {\n        MPI_Send(all_strings.data(), all_strings.size(), MPI_CHAR, i, 0, MPI_COMM_WORLD);\n    }\n\n    std::vector<std::string> local_all_strings;\n\n    if (my_id!= 0) {\n        MPI_Recv(all_strings.data(), all_strings.size(), MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n        local_all_strings = all_strings;\n    } else {\n        local_all_strings = std::vector<std::string>(all_strings.begin() + my_id, all_strings.end() + my_id);\n    }\n\n    std::array<size_t, 26> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    int num_threads;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_all_strings.size(); i++) {\n        char letter = local_all_strings[i][0];\n        local_bins[letter - 'a']++;\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (my_id == 0)\n        for (int i = 1; i < num_procs; i++) {\n            std::array<size_t, 26> recv_bins;\n            MPI_Recv(recv_bins.data(), 26, MPI_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int i = 0; i < 26; i++)\n                bins[i] += recv_bins[i];\n        }\n    else {\n        MPI_Send(local_bins.data(), 26, MPI_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Your code here\n}",
            "size_t numOfThreads = omp_get_max_threads();\n    std::vector<size_t> localBins(26, 0);\n    #pragma omp parallel num_threads(numOfThreads)\n    {\n        #pragma omp for\n        for (auto i = 0; i < s.size(); i++) {\n            char c = s[i].front();\n            if (c >= 'a' && c <= 'z') {\n                #pragma omp atomic\n                localBins[c - 'a']++;\n            }\n        }\n    }\n\n    for (int i = 0; i < 26; i++) {\n        #pragma omp atomic\n        bins[i] += localBins[i];\n    }\n}",
            "const size_t s_size = s.size();\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  const size_t chunk_size = s_size / size;\n  size_t start = rank * chunk_size;\n  size_t end = std::min(s_size, start + chunk_size);\n\n  #pragma omp parallel for\n  for (size_t i = start; i < end; i++) {\n    bins[s[i][0] - 'a']++;\n  }\n\n  MPI_Reduce(&bins[0], &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int numRanks, myRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    int numThreads;\n#pragma omp parallel\n    {\n        numThreads = omp_get_num_threads();\n    }\n    std::array<std::vector<std::string>, numThreads> sPerThread;\n    std::array<std::array<size_t, 26>, numThreads> binsPerThread;\n    if (myRank == 0) {\n        int numWords = s.size();\n        std::vector<int> numWordsPerThread(numThreads);\n        int start = 0, end = numWords;\n        for (int i = 0; i < numThreads; i++) {\n            if (i == numThreads - 1) {\n                numWordsPerThread[i] = end - start;\n            }\n            else {\n                numWordsPerThread[i] = (end - start) / numThreads;\n            }\n            start += numWordsPerThread[i];\n        }\n        start = 0;\n        for (int i = 0; i < numThreads; i++) {\n            int newEnd = start + numWordsPerThread[i];\n            sPerThread[i] = std::vector<std::string>(s.begin() + start, s.begin() + newEnd);\n            binsPerThread[i] = std::array<size_t, 26>{};\n            start = newEnd;\n        }\n    }\n    MPI_Bcast(&numThreads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(s.data(), numThreads, MPI_DOUBLE, sPerThread.data(), numThreads, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    int threadID = omp_get_thread_num();\n    for (int i = 0; i < numThreads; i++) {\n        if (myRank == 0) {\n            for (int j = 0; j < 26; j++) {\n                for (int k = 0; k < sPerThread[i].size(); k++) {\n                    if (sPerThread[i][k][0] == 'a' + j) {\n                        binsPerThread[i][j]++;\n                    }\n                }\n            }\n        }\n        else {\n            for (int j = 0; j < 26; j++) {\n                for (int k = 0; k < sPerThread[i].size(); k++) {\n                    if (sPerThread[i][k][0] == 'a' + j) {\n                        bins[j]++;\n                    }\n                }\n            }\n        }\n    }\n    if (myRank == 0) {\n        for (int i = 0; i < numThreads; i++) {\n            for (int j = 0; j < 26; j++) {\n                bins[j] += binsPerThread[i][j];\n            }\n        }\n    }\n}",
            "int const NWORDS = s.size();\n\n\tstd::vector<int> counts(NWORDS, 0);\n\t#pragma omp parallel for schedule(static)\n\tfor (int i = 0; i < NWORDS; ++i) {\n\t\tint c = s[i][0] - 'a';\n\t\tcounts[i] = c;\n\t}\n\n\t#pragma omp parallel for schedule(static)\n\tfor (int i = 0; i < NWORDS; ++i) {\n\t\tbins[counts[i]]++;\n\t}\n}",
            "/* Your solution goes here */\n\n}",
            "// TODO\n}",
            "// TODO: fill in your code here\n}",
            "// TODO\n\n}",
            "// Count the number of elements for each letter\n    for (std::string const& string : s) {\n        char letter = string[0];\n        // TODO\n    }\n}",
            "std::array<size_t, 26> bins_private;\n    bins_private.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i=0; i<s.size(); ++i) {\n        const char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            const int j = c - 'a';\n            ++bins_private[j];\n        }\n    }\n\n    MPI_Reduce(bins_private.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n   for(int i = 0; i < 26; i++){\n        bins[i] = 0;\n   }\n\n   #pragma omp parallel for\n   for(int i = 0; i < s.size(); i++){\n        bins[s[i][0] - 'a'] += 1;\n   }\n}",
            "// Implement here\n\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // TODO: Your code here\n  // Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n  // Every rank has a complete copy of s. The result is stored in bins on rank 0.\n\n  //int num_threads = 0;\n  //#pragma omp parallel\n  //{\n  //  num_threads = omp_get_num_threads();\n  //}\n\n  //printf(\"Hello from thread %d out of %d\\n\", omp_get_thread_num(), num_threads);\n\n  //printf(\"Hello from rank %d out of %d\\n\", world_rank, world_size);\n\n  //char firstChar;\n  //std::vector<char> firstChars;\n  //for (int i = 0; i < s.size(); i++) {\n  //  firstChar = s[i][0];\n  //  firstChars.push_back(firstChar);\n  //  printf(\"i = %d, firstChar = %c\\n\", i, firstChar);\n  //}\n  //printf(\"done\\n\");\n\n  //int chunk = s.size() / world_size;\n  //printf(\"world_size = %d\\n\", world_size);\n  //printf(\"chunk = %d\\n\", chunk);\n  //printf(\"s.size() = %d\\n\", s.size());\n  //printf(\"firstChar = %c\\n\", firstChars[chunk*world_rank]);\n\n  //int localSize = 0;\n  //if (world_rank == world_size-1) {\n  //  localSize = s.size() - chunk*world_rank;\n  //}\n  //else {\n  //  localSize = chunk;\n  //}\n  //printf(\"localSize = %d\\n\", localSize);\n\n  //std::vector<char> firstCharsLocal(localSize);\n  //for (int i = 0; i < localSize; i++) {\n  //  firstCharsLocal[i] = firstChars[chunk*world_rank + i];\n  //}\n  //printf(\"done\\n\");\n\n  //printf(\"firstCharsLocal[0] = %c\\n\", firstCharsLocal[0]);\n  //printf(\"firstCharsLocal[1] = %c\\n\", firstCharsLocal[1]);\n  //printf(\"firstCharsLocal[2] = %c\\n\", firstCharsLocal[2]);\n\n  if (world_rank == 0) {\n    int localSize = 0;\n    for (int i = 1; i < world_size; i++) {\n      MPI_Recv(&localSize, 1, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    //printf(\"localSize = %d\\n\", localSize);\n\n    int count = 0;\n    int localIndex = 0;\n    int firstLetterIndex;\n    for (int i = 0; i < world_size; i++) {\n      if (i == 0) {\n        MPI_Send(&localSize, 1, MPI_INT, i, 1, MPI_COMM_WORLD);\n      }\n      else {\n        //printf(\"before MPI_Recv\\n\");\n        MPI_Recv(&localSize, 1, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        //printf(\"after MPI_Recv\\n\");\n\n        std::vector<char> firstCharsLocal(localSize);\n        MPI_Recv(&firstCharsLocal[0], localSize, MPI_CHAR, i, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        //printf(\"after MPI_Recv\\n\");\n\n        for (int j = 0; j < localSize; j++) {\n          firstLetterIndex = firstCharsLocal[j] - 'a';",
            "}",
            "// Your code goes here!\n    int rank, npes;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &npes);\n    int nStrings = s.size();\n    int nStringsPerPE = nStrings/npes;\n    int extraStrings = nStrings % npes;\n    std::vector<std::string> s_perPE(nStringsPerPE);\n    for (int i = 0; i < nStringsPerPE; i++) {\n        s_perPE[i] = s[i + nStringsPerPE * rank];\n    }\n    if (rank == 0) {\n        #pragma omp parallel for\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n            for (int j = 0; j < nStringsPerPE * npes; j++) {\n                if (s[j][0] == 'a' + i) {\n                    bins[i]++;\n                }\n            }\n        }\n    } else if (rank < npes - 1) {\n        #pragma omp parallel for\n        for (int i = 0; i < 26; i++) {\n            int count = 0;\n            for (int j = 0; j < nStringsPerPE; j++) {\n                if (s_perPE[j][0] == 'a' + i) {\n                    count++;\n                }\n            }\n            MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        #pragma omp parallel for\n        for (int i = 0; i < 26; i++) {\n            int count = 0;\n            for (int j = 0; j < nStringsPerPE + extraStrings; j++) {\n                if (s_perPE[j][0] == 'a' + i) {\n                    count++;\n                }\n            }\n            MPI_Send(&count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n    if (rank!= 0) {\n        MPI_Recv(&bins, 26, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "// TODO 1\n    // Initialize bins to 0\n\n    // TODO 2\n    // Add 1 to the bin that corresponds to the first letter of each string\n\n    // TODO 3\n    // Add 1 to the bin that corresponds to the first letter of each string\n\n    // TODO 4\n    // Add 1 to the bin that corresponds to the first letter of each string\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < bins.size(); ++i) {\n        for (int j = 0; j < s.size(); ++j) {\n            if (s[j].substr(0, 1).compare(char(65 + i)) == 0) {\n                #pragma omp atomic\n                bins[i]++;\n            }\n        }\n    }\n}",
            "int const nranks = mpi::size();\n    int const myrank = mpi::rank();\n    int const nthreads = omp_get_num_threads();\n    std::vector<std::vector<size_t>> bin_rank(nranks, std::vector<size_t>(26));\n\n    if(myrank == 0){\n        for(int i = 0; i < nranks; i++) mpi::broadcast(s, i);\n        #pragma omp parallel for\n        for (auto &t: s) {\n            bin_rank[myrank][t[0] - 'a']++;\n        }\n    } else {\n        #pragma omp parallel for\n        for (auto &t: s) {\n            bin_rank[myrank][t[0] - 'a']++;\n        }\n        mpi::broadcast(bin_rank[myrank], 0);\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (int j = 0; j < nranks; j++) {\n            bins[i] += bin_rank[j][i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    size_t letter = s[i][0] - 'a';\n    bins[letter]++;\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            // initialize local array\n            std::array<size_t, 26> localBins{};\n\n            // fill the array\n            for (size_t i = 0; i < s.size(); ++i) {\n                #pragma omp task shared(s, i)\n                {\n                    char firstLetter = s[i][0];\n                    if (firstLetter >= 'a' && firstLetter <= 'z') {\n                        localBins[firstLetter - 'a']++;\n                    }\n                }\n            }\n\n            // merge results\n            for (size_t i = 0; i < localBins.size(); ++i) {\n                #pragma omp atomic\n                bins[i] += localBins[i];\n            }\n        }\n    }\n}",
            "auto nthreads = omp_get_max_threads();\n    auto chunksize = s.size() / nthreads;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::array<size_t, 26> my_bins;\n\n    if (rank == 0) {\n        for (auto i = 0; i < 26; ++i) {\n            my_bins[i] = 0;\n        }\n    }\n\n    #pragma omp parallel for\n    for (auto i = 0; i < nthreads; i++) {\n        auto start = i * chunksize;\n        auto end = (i + 1) * chunksize;\n\n        for (auto j = start; j < end; j++) {\n            if (rank == 0) {\n                my_bins[s[j][0] - 'a'] += 1;\n            }\n        }\n    }\n\n    MPI_Reduce(my_bins.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Fill in your solution here\n    bins.fill(0);\n}",
            "#pragma omp parallel for schedule(dynamic)\n  for (size_t i = 0; i < s.size(); ++i) {\n    if (s[i].empty()) { continue; }\n    auto const bin = std::tolower(s[i].front()) - 'a';\n    #pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "// YOUR CODE GOES HERE\n\n}",
            "// ----- Your code starts here -----\n  if (s.empty()) return;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int blockSize = s.size() / size;\n  std::vector<int> localCounts(26, 0);\n  for (int i = 0; i < s.size(); i++){\n    if (i % size == rank){\n      localCounts[s[i][0] - 'a']++;\n    }\n  }\n  // ----- Your code ends here -----\n\n}",
            "// TODO\n}",
            "}",
            "const size_t letters = 26;\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i) {\n        if (s[i].length() > 0) {\n            #pragma omp atomic\n            bins[s[i][0] - 'a'] += 1;\n        }\n    }\n}",
            "int size, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n\n        std::array<size_t, 26> bins_0;\n\n        std::vector<std::string> s_0;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&s_0, s.size(), MPI_STRING, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            #pragma omp parallel for\n            for (int j = 0; j < s.size(); ++j) {\n                bins_0[s_0[j].at(0) - 'a'] += 1;\n            }\n        }\n\n        #pragma omp parallel for\n        for (int j = 0; j < s.size(); ++j) {\n            bins[s[j].at(0) - 'a'] += 1;\n        }\n\n        #pragma omp parallel for\n        for (int j = 0; j < 26; ++j) {\n            bins[j] += bins_0[j];\n        }\n    }\n    else {\n\n        MPI_Send(&s, s.size(), MPI_STRING, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&bins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n    else {\n\n        MPI_Send(&bins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[(s[i][0]) - 97]++;\n    }\n}",
            "// TODO\n\n}",
            "// your code here\n\n}",
            "// Your code here.\n    int size, rank, nthreads;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    omp_get_num_threads(&nthreads);\n    #pragma omp parallel for num_threads(nthreads)\n    for (int i = 0; i < s.size(); i++)\n    {\n        char c = tolower(s[i].at(0));\n        if (c >= 'a' && c <= 'z')\n            #pragma omp critical\n            bins[c - 'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  // TODO\n\n  // End of TODO\n}",
            "// Create a local version of the array\n  std::array<size_t, 26> bins_local;\n\n  // Reset the counters\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins_local[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins_local[s[i][0] - 'a'] += 1;\n  }\n\n  // Do reduction\n  for (size_t i = 1; i < omp_get_num_threads(); ++i) {\n    for (size_t j = 0; j < bins_local.size(); ++j) {\n      bins_local[j] += bins[i][j];\n    }\n  }\n\n  // Transfer the results to rank 0\n  if (omp_get_thread_num() == 0) {\n    for (int i = 1; i < omp_get_num_threads(); ++i) {\n      for (size_t j = 0; j < bins_local.size(); ++j) {\n        bins[j] += bins_local[j];\n      }\n    }\n  }\n}",
            "// TODO: your code here\n\n}",
            "// TODO: Your code here.\n\n}",
            "// YOUR CODE HERE\n\n}",
            "// Use a parallel for loop to compute the histogram of the first letters in each string\n  // The histogram is stored in the bins array.\n  // HINT: You need to use MPI to communicate the histogram counts between ranks.\n  // HINT: For the final histogram, you need to combine the local histograms from all\n  // ranks into a single histogram\n\n  #pragma omp parallel\n  {\n\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int nb_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nb_procs);\n\n    std::array<size_t, 26> local_bins{};\n    int chunk = s.size() / nb_procs;\n\n    if (my_rank == 0) {\n      for (int i = 1; i < nb_procs; ++i) {\n        MPI_Send(&s[i * chunk], chunk, MPI_INT, i, 0, MPI_COMM_WORLD);\n      }\n    }\n\n    int start, end;\n    if (my_rank == 0) {\n      start = 0;\n      end = chunk;\n    } else {\n      start = my_rank * chunk;\n      end = start + chunk;\n      MPI_Recv(&s[0], chunk, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    // Compute local histogram\n    #pragma omp for\n    for (int i = start; i < end; ++i) {\n      ++local_bins[s[i][0] - 'a'];\n    }\n\n    // Combine histograms from all ranks\n    std::array<size_t, 26> global_bins{};\n    MPI_Reduce(&local_bins, &global_bins, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Copy histogram from rank 0 to bins\n    if (my_rank == 0) {\n      bins = global_bins;\n    }\n  }\n}",
            "std::vector<std::string> sLocal;\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        sLocal = s;\n    }\n\n    int numStringsPerRank = s.size() / size;\n    int numStringsInLastRank = s.size() - numStringsPerRank * (size - 1);\n\n    int begin = numStringsPerRank * rank;\n    int end = begin + numStringsPerRank;\n    if (rank == size - 1) {\n        end += numStringsInLastRank;\n    }\n\n    for (int i = begin; i < end; ++i) {\n        sLocal.push_back(s[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < sLocal.size(); ++i) {\n        char letter = tolower(sLocal[i][0]);\n        if (letter >= 'a' && letter <= 'z') {\n            #pragma omp atomic update\n            bins[letter - 'a'] += 1;\n        }\n    }\n\n    // Reduce each rank's bins on rank 0\n    MPI_Reduce(bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    //TODO: your code here\n}",
            "int num_procs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int num_procs_per_core = num_procs/omp_get_num_procs();\n   int num_procs_per_core_rank = rank%num_procs_per_core;\n   int thread_id = omp_get_thread_num();\n   int core_id = rank/num_procs_per_core;\n   int core_rank_id = num_procs_per_core_rank+thread_id*num_procs_per_core;\n   // your code here\n\n}",
            "// Use 26 threads in a parallel region\n    #pragma omp parallel num_threads(26)\n    {\n        #pragma omp for\n        // Compute the bin for each letter of the alphabet\n        for (size_t i = 0; i < 26; ++i)\n        {\n            // Initialize the bin to zero\n            bins[i] = 0;\n            // Iterate over all strings in s\n            for (auto const &str : s)\n            {\n                // The first letter of the string is i\n                if (str.front() == 'a' + i)\n                {\n                    // Increment the bin\n                    bins[i] += 1;\n                }\n            }\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Insert code here to do the work.\n    // Make sure to use a parallel for loop.\n}",
            "// Your code here\n  Kokkos::parallel_for(x.size(), [=](const size_t i) {\n    size_t bin = size_t(x(i) / 10);\n    Kokkos::atomic_fetch_add(&bins[bin], 1);\n  });\n}",
            "// TODO: implement me!\n}",
            "// Your code here\n\n}",
            "size_t num_inputs = x.size();\n  Kokkos::parallel_for(num_inputs, [&](size_t i) {\n    int bin = static_cast<int>(std::floor(x(i) / 10));\n    Kokkos::atomic_increment(&bins[bin]);\n  });\n  Kokkos::fence();\n}",
            "// your code here\n}",
            "// your code here\n}",
            "// TODO: Use parallel_for with Kokkos to count the numbers of x values in each bin\n    //       Hint: the 1D index i corresponds to bins[i]\n}",
            "// TODO: Fill in this function.\n}",
            "/* Insert your code here */\n}",
            "// TODO: Implement me!\n}",
            "// Your code here\n}",
            "}",
            "// Fill this in\n}",
            "Kokkos::parallel_for(\n    \"bins_by_10\",\n    Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ExecutionPolicy>>(\n      /* begin */ 0,\n      /* end */ x.extent(0),\n      /* stride */ 1\n    ),\n    KOKKOS_LAMBDA(const int i) {\n      double x_i = x(i);\n      // TODO: use x_i to update bins\n    }\n  );\n  Kokkos::fence();\n}",
            "const size_t n = x.extent(0);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(size_t i) {\n        // TODO: Your code here\n    });\n    Kokkos::fence(); // Wait for all previous parallel_for to finish\n}",
            "Kokkos::View<double*> bins_d(\"bins_d\", 10);\n  auto vx = Kokkos::subview(x, Kokkos::ALL());\n  Kokkos::parallel_for(\n    \"binsBy10Count\",\n    Kokkos::RangePolicy<Kokkos::Threads>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      int which_bin = vx(i) / 10;\n      if (which_bin >= 0 && which_bin <= 9) {\n        Kokkos::atomic_add(&(bins_d(which_bin)), 1);\n      }\n    });\n  Kokkos::deep_copy(bins, bins_d);\n}",
            "// TODO: Fill this in\n}",
            "// TODO\n\n}",
            "// Implement me!\n}",
            "Kokkos::parallel_for(\n    \"histogram\",\n    Kokkos::RangePolicy<Kokkos::ExecSpace>(0, x.size()),\n    KOKKOS_LAMBDA(const size_t idx) {\n      const int i = static_cast<int>(x[idx] / 10);\n      bins(i)++;\n    });\n  Kokkos::fence();\n}",
            "const size_t n = x.extent(0);\n\n  // TODO: fill in the code\n}",
            "// TODO\n}",
            "// TODO\n}",
            "/*\n   * Fill in the code here.\n   * You can use the `range` and `parallel_for` functions\n   * from Kokkos. For example:\n   *\n   * Kokkos::parallel_for(\n   *   Kokkos::Range(0, x.size()),\n   *   [&](int i) {\n   *    ...\n   *   }\n   * );\n   */\n\n  // HINT: You can use the C++11 remainder operator to get the index of the bin\n  // you should add to:\n  // int bin_index = x[i] % 10;\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(int i) {\n    bins(i) = Kokkos::count(x, [=](double xi){ return xi >= i * 10 && xi < (i + 1) * 10; });\n  });\n  Kokkos::fence();\n}",
            "const size_t n = x.extent(0);\n  Kokkos::View<double[10]> x_local(\"x_local\", 10);\n\n  // TODO: Set up the workspace of the Kokkos::parallel_for\n  // - Create a Kokkos::View of size 10, which will store the counts\n  // - Make sure the parallel_for works over the entire array `x`\n  // - Inside the parallel_for, fill the `x_local` view with the elements of `x`\n\n  // TODO: Compute the counts with the Kokkos::parallel_reduce\n  // - Make sure the parallel_reduce works over the entire array `x`\n  // - Inside the parallel_reduce, compute the counts of the `x_local` view into the counts array\n  // - Use Kokkos::atomic_add to do the reduction safely\n\n  // TODO: Copy the results into `bins` using Kokkos::deep_copy\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\"binsBy10Count\", x.extent(0), KOKKOS_LAMBDA(int i) {\n    // TODO\n  });\n}",
            "/* Your code goes here */\n\n}",
            "// Fill bins in parallel using Kokkos.\n}",
            "// Write your code here\n    //\n    //  You will need to:\n    //    - Write a parallel for loop with Kokkos to count the numbers in each bin\n    //    - Run a parallel reduction to sum the counts of each bin\n    //    - Use the Kokkos::View::assign_atomic method to assign the counts to the\n    //      corresponding bin in bins\n    //\n    //  Do not use any of the STL algorithms or data structures.\n}",
            "constexpr int n = 10;\n  const double factor = 10.;\n  const double lower_bound = 0.;\n  Kokkos::parallel_for(\n      \"by10Count\",\n      Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(\n          Kokkos::DefaultHostExecutionSpace(), 0, x.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n        double x_i = x(i);\n        int idx = static_cast<int>((x_i - lower_bound) / factor);\n        if (idx >= 0 && idx < n) {\n          Kokkos::atomic_increment(Kokkos::atomic_ref(bins(idx)));\n        }\n      });\n}",
            "// TODO: Implement me\n    Kokkos::parallel_for(\n        Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n        KOKKOS_LAMBDA(const int i) {\n            // TODO: Compute the bin for `x[i]` and increment it by 1\n        }\n    );\n    Kokkos::DefaultHostExecutionSpace::fence();\n}",
            "Kokkos::RangePolicy<> policy(0, x.extent(0));\n  Kokkos::parallel_for(\n    policy,\n    KOKKOS_LAMBDA(size_t i) {\n      int index = (int) (x(i) / 10.0);\n      Kokkos::atomic_increment(&bins(index));\n    });\n  Kokkos::fence();\n}",
            "// your code here\n\n  // TODO\n\n  Kokkos::View<size_t*> bins_h(\"bins_h\",10);\n  Kokkos::parallel_for(\"count\", Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0,x.size()),\n    KOKKOS_LAMBDA(const int& i) {\n      const int bin = (int) (x(i) / 10);\n      Kokkos::atomic_increment(&bins_h[bin]);\n  });\n\n  Kokkos::deep_copy(bins,bins_h);\n}",
            "/* Step 1: Initialize the output array `bins`. We will fill it with the\n       counts later. */\n    Kokkos::parallel_for(\n        \"InitBins\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 10),\n        KOKKOS_LAMBDA(int i) {\n            bins(i) = 0;\n        }\n    );\n\n    /* Step 2: Compute the counts in parallel. */\n    Kokkos::parallel_for(\n        \"Count\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n        KOKKOS_LAMBDA(int i) {\n            // For each element in `x`, compute its bin and increment the count\n            int bin = (int)x(i) / 10;\n            if (bin < 0 || bin >= 10) {\n                // FIXME: this shouldn't happen\n                printf(\"ERROR: bin is out of range\\n\");\n                exit(-1);\n            }\n            Kokkos::atomic_increment(&bins(bin));\n        }\n    );\n\n    /* Step 3: Synchronize Kokkos so the main thread can read the results. */\n    Kokkos::DefaultExecutionSpace::fence();\n\n    /* Step 4: Print the results. */\n    for (int i = 0; i < 10; i++) {\n        printf(\"%d-%d: %lu\\n\", i * 10, (i+1) * 10, bins(i));\n    }\n}",
            "// TODO: Fill in this function\n    // (1) Create a reduction variable that uses a lambda function as its\n    //     reduction operator.\n    // (2) Create a parallel_reduce to calculate the number of values in each\n    //     bin.\n    // (3) Copy the results to the bins View.\n}",
            "// TODO: your code goes here\n}",
            "// Create views on the device\n  auto x_d = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_d, x);\n  auto bins_d = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_d, bins);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Schedule<Kokkos::StaticChunked>>>(x.extent(0), Kokkos::AUTO),\n                       [=] (const int& i) {\n    double value = x_d(i);\n    int bin = value/10;\n    if (bin < 0) bin = 0;\n    if (bin > 9) bin = 9;\n    bins_d[bin]++;\n  });\n\n  Kokkos::deep_copy(bins, bins_d);\n}",
            "// Kokkos::parallel_for is an efficient parallel iteration construct that\n  // automatically parallelizes execution of loops over the indices.\n  Kokkos::parallel_for(x.size(), [&](size_t i) {\n    // This lambda function will be called with the index of the loop as\n    // the parameter `i`.\n    //\n    // Example: For x = [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n    //\n    // i    x[i]\n    // 0     7\n    // 1    32\n    // 2    95\n    // 3    12\n    // 4    39\n    // 5    32\n    // 6    11\n    // 7    71\n    // 8    70\n    // 9    66\n\n    // The range [0, 10)\n    constexpr int low = 0;\n    // The range [10, 20)\n    constexpr int high = 10;\n    // The index into the `bins` array\n    constexpr int index = 0;\n\n    // Compute the index into the `bins` array\n    // Use the integer division operator to convert x[i] to the range [0, 9].\n    // This is also the index into the `bins` array.\n    const int bin_index = (x[i] - low) / (high - low);\n\n    // Increment the corresponding element of `bins`.\n    // Use the atomic increment function.\n    Kokkos::atomic_fetch_add(&bins[bin_index], 1);\n  });\n\n  // Make sure all threads have finished before the next parallel region.\n  Kokkos::fence();\n}",
            "}",
            "const double min = 0.0;\n    const double max = 100.0;\n    const double bin_width = (max - min) / 10;\n    const size_t N = x.extent(0);\n    // Use a parallel_for here to do the work in parallel.\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, N),\n                         KOKKOS_LAMBDA(const int i) {\n        const double x_i = x[i];\n        if (x_i >= min && x_i < max) {\n            // Find which bin it goes in.\n            // e.g., x_i=32 -> bin_idx = 3\n            int bin_idx = (int) (x_i / bin_width);\n            // Increment the appropriate bin.\n            Kokkos::atomic_increment(&bins[bin_idx]);\n        }\n    });\n}",
            "size_t bins_size = bins.size();\n\n    /* The below lambda is the actual work to be done in parallel. */\n    auto count_by_10 = KOKKOS_LAMBDA(size_t i) {\n        /* The actual work to be done on each element. */\n        auto value = x(i);\n        auto bin_index = int(value / 10);\n        bins(bin_index) += 1;\n    };\n\n    /* Execute the lambda on every element of `x` in parallel. */\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), count_by_10);\n\n    /* Synchronize to make sure all elements of `bins` have been updated by the\n       parallel_for. */\n    Kokkos::fence();\n}",
            "/* TODO: Your code here */\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n\n    Kokkos::parallel_for(\"binsBy10Count\", ExecPolicy(0, x.size()),\n    [&](int i) {\n        auto val = x(i);\n        // compute which bin the value belongs to\n        auto bin = val / 10;\n        // update bins\n        Kokkos::atomic_add(&bins(bin), 1);\n    });\n}",
            "// Create a Kokkos policy to distribute the work among 4 threads\n  Kokkos::parallel_for(\"count_by_10\", Kokkos::TeamPolicy<>(4, 4),\n                       [=](const Kokkos::TeamPolicy<>::member_type& team) {\n\n                         // Create a local view for the bin counts that will be\n                         // accessible by every thread\n                         Kokkos::View<size_t[10]> local_bins(\"local_bins\", 10);\n\n                         // Use `team_reduce` to initialize all bin counts to 0\n                         Kokkos::parallel_for(\n                             Kokkos::TeamThreadRange(team, 10), [=](const int i) {\n                               local_bins(i) = 0;\n                             });\n                         team.team_barrier();\n\n                         // Compute the bin for each element in `x`\n                         Kokkos::parallel_for(\n                             Kokkos::TeamThreadRange(team, x.extent(0)), [=](const int i) {\n                               const int bin = (int)(x(i) / 10);\n                               Kokkos::atomic_fetch_add(&local_bins(bin), 1);\n                             });\n                         team.team_barrier();\n\n                         // Add the local bin counts to the global bin counts\n                         Kokkos::parallel_for(\n                             Kokkos::TeamThreadRange(team, 10), [=](const int i) {\n                               Kokkos::atomic_fetch_add(&bins(i), local_bins(i));\n                             });\n                       });\n}",
            "// TODO: Implement this function!\n}",
            "using bin_type = Kokkos::View<size_t[10]>;\n  // Compute the number of bins (10) in parallel\n  size_t num_bins = Kokkos::Experimental::required_vector_size<bin_type>() *\n                    Kokkos::Experimental::required_vector_alignment<bin_type>();\n\n  // Initialize the bins to 0.\n  Kokkos::fill_view(bins, 0);\n\n  // Make a Kokkos policy for 1 thread per bin.\n  Kokkos::RangePolicy<Kokkos::Experimental::ROCm> bin_policy(0, num_bins);\n\n  // Compute the bin numbers for all of the input values.\n  Kokkos::View<size_t*> bin_numbers(\"bin_numbers\", x.extent(0));\n  auto bin_numbers_functor = KOKKOS_LAMBDA(size_t i) {\n    // The bin number is the value divided by 10, rounded down.\n    bin_numbers(i) = size_t(x(i) / 10);\n  };\n  Kokkos::parallel_for(bin_policy, bin_numbers_functor);\n\n  // Count the number of elements in each bin.\n  auto bins_functor = KOKKOS_LAMBDA(size_t i) {\n    // Loop over all of the input values and count how many are in the ith bin.\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (bin_numbers(j) == i) {\n        bins(i)++;\n      }\n    }\n  };\n  Kokkos::parallel_for(bin_policy, bins_functor);\n}",
            "// TODO: Implement.\n\n}",
            "// Write your code here\n}",
            "/* TODO: fill in */\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n  Kokkos::parallel_for(ExecPolicy{0, x.size()}, KOKKOS_LAMBDA(const size_t& i) {\n    auto x_i = (size_t)x[i];\n    assert(x_i < 100);\n    bins[x_i/10] += 1;\n  });\n}",
            "using Device = Kokkos::DefaultExecutionSpace;\n  using View = Kokkos::View<size_t[10]>;\n\n  // Initialize bins to zero.\n  Kokkos::parallel_for( \"Init\", Kokkos::RangePolicy<Device>(0, 10),\n    KOKKOS_LAMBDA(int bin) {\n      bins(bin) = 0;\n    }\n  );\n\n  // Count the number of values in each bin.\n  // We will use this device view to share data between host and device.\n  Kokkos::View<size_t[10], Device> bins_device(\"bins_device\", 10);\n  Kokkos::deep_copy(bins_device, bins);\n\n  Kokkos::parallel_for( \"Bins\", Kokkos::RangePolicy<Device>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      const auto bin = std::min(10, std::max(0, static_cast<int>(x(i) / 10)));\n      Kokkos::atomic_increment( &bins_device(bin) );\n    }\n  );\n\n  // Copy back to host.\n  Kokkos::deep_copy(bins, bins_device);\n\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0));\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(int i) {\n    // TODO: Your code here!\n    int x_i = x(i);\n    for (int j = 0; j < 10; ++j) {\n      if (x_i >= 10*j && x_i < 10*j+10) {\n        Kokkos::atomic_add(&bins(j), 1);\n        break;\n      }\n    }\n  });\n  Kokkos::fence();\n}",
            "// Your code goes here\n}",
            "// TODO: Implement this function. You should not modify the prototype.\n}",
            "// TODO: Your code here!\n}",
            "// Your code here.\n\n}",
            "using namespace Kokkos;\n  const size_t n = x.extent_int(0);\n\n  using range = RangePolicy<size_t>;\n  using member = RangePolicy<size_t>::member_type;\n\n  auto x_h = create_mirror_view(x);\n  auto bins_h = create_mirror_view(bins);\n\n  // Copy data to host space\n  Kokkos::deep_copy(x_h, x);\n  // Initialize to zero\n  Kokkos::deep_copy(bins_h, 0);\n\n  parallel_for(range(0, n), KOKKOS_LAMBDA(const member& i) {\n    // Compute bin index\n    auto idx = std::floor(x_h(i) / 10);\n    if (idx < 10) bins_h(idx)++;\n  });\n\n  Kokkos::deep_copy(bins, bins_h);\n}",
            "// TODO: Implement\n}",
            "// Fill in the body of this function.\n}",
            "// TODO\n}",
            "// TODO: Fill this in.\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceMode::Reduce>>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      bins(x[i] / 10) += 1;\n    });\n  Kokkos::fence();\n}",
            "const size_t x_size = x.extent(0);\n    Kokkos::View<size_t[10]> counts_host(\"counts\", 10);\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace>(0, x_size), [=](size_t i) {\n        size_t x_i = (size_t) x[i];\n        counts_host[x_i/10] += 1;\n    });\n    Kokkos::deep_copy(bins, counts_host);\n}",
            "auto const n = x.extent(0);\n  size_t bin_size = 10;\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n    KOKKOS_LAMBDA(size_t i) {\n      size_t bin = x(i)/bin_size;\n      if (bin >= 10) bin = 9;\n      Kokkos::atomic_increment(&bins[bin]);\n    }\n  );\n}",
            "// Your code goes here\n}",
            "// TODO\n}",
            "// your code here\n}",
            "/* Your code here */\n}",
            "//...\n}",
            "const size_t n = x.extent(0);\n    const double* x_data = x.data();\n    size_t* bins_data = bins.data();\n\n    // Add your code here.\n\n    /* The code above will compile to a CUDA kernel that looks like:\n\n    __global__ void binBy10Count(const double *x_data, size_t *bins_data) {\n      const size_t n =...;\n      const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n      if (i < n) {\n        int which_bin = (int) (x_data[i] / 10);\n        atomicAdd(&bins_data[which_bin], 1);\n      }\n    }\n\n    The kernel is launched with a block size of BLOCK_SIZE and grid size\n    ((n + BLOCK_SIZE - 1) / BLOCK_SIZE). This means that there are (n +\n    BLOCK_SIZE - 1) / BLOCK_SIZE blocks, each with BLOCK_SIZE threads. Each\n    thread will access x_data[i] where 0 <= i < n.\n\n    Kokkos takes care of creating the device pointer x_data and host pointer\n    bins_data. It also takes care of synchronizing data between the host and\n    device. bins_data is allocated on the host and then copied to the device.\n\n    The code above is safe for both multi-threaded CPU and GPU execution.\n    However, there is still a race condition when there are multiple blocks\n    executing at the same time.\n\n    To resolve this race condition, replace\n\n    atomicAdd(&bins_data[which_bin], 1);\n\n    with\n\n    atomicAdd(&bins_data[which_bin], 1);\n    while (atomicCAS(&bins_data[which_bin], old_val, old_val + 1)!= old_val);\n\n    atomicAdd and atomicCAS are atomic operations that are guaranteed to\n    execute atomically. atomicAdd returns the value of bins_data[which_bin]\n    before the atomic operation. atomicCAS sets bins_data[which_bin] to the\n    value of old_val only if bins_data[which_bin] is equal to old_val. Thus, if\n    bins_data[which_bin] is equal to old_val, then the loop will terminate.\n    Otherwise, it will execute atomicCAS again, which will again set\n    bins_data[which_bin] to old_val.\n\n    Atomic operations are expensive, so it is best to minimize the number of\n    atomic operations. In this case, it is best to use one atomicAdd call per\n    bin.\n    */\n\n    const size_t BLOCK_SIZE = 512;\n\n    // Replace the body of the kernel with a call to this function.\n    // This is a good way to get started, but you'll want to optimize it later.\n    void count1Bin(const size_t which_bin, const double *x_data, size_t *bins_data) {\n        for (size_t i = 0; i < n; ++i) {\n            double x_val = x_data[i];\n            if (x_val >= which_bin * 10 && x_val < (which_bin + 1) * 10) {\n                atomicAdd(&bins_data[which_bin], 1);\n            }\n        }\n    }\n\n    Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n, BLOCK_SIZE),\n      [&](const size_t i) {\n        for (size_t which_bin = 0; which_bin < 10; ++which_bin) {\n            count1Bin(which_bin, x_data, bins_data);\n        }\n      }\n    );\n}",
            "// TODO\n}",
            "// TODO: complete this function\n}",
            "// TODO: Implement this\n}",
            "/* TODO: Your code goes here */\n\n}",
            "}",
            "// TODO\n}",
            "// Implement this!\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::ThreadVectorRange, Kokkos::Schedule<Kokkos::Static>>;\n  Kokkos::parallel_for(ExecPolicy(0, bins.size() + 1), KOKKOS_LAMBDA(const int& i) {\n    const auto bin = i / 10;\n    const auto offset = i % 10;\n    auto& x_bin = bins(bin);\n    Kokkos::atomic_fetch_add(&x_bin, static_cast<size_t>(Kokkos::count(Kokkos::subview(x, Kokkos::ALL(), bin), offset * 10)));\n  });\n}",
            "using ExecutionSpace = typename Kokkos::DefaultExecutionSpace;\n  constexpr size_t vec_length = 4; // 4 is the minimum vector length that a GPU hardware\n  // typically has\n  Kokkos::View<size_t[vec_length][10]> bins_vec_local(\"bins_vec_local\", 1);\n  const size_t n = x.size();\n  const size_t n_vec = (n + vec_length - 1) / vec_length;\n  Kokkos::parallel_for(\n      Kokkos::RangePolicy<ExecutionSpace>(0, n_vec),\n      KOKKOS_LAMBDA(const size_t &i) {\n        double data[vec_length];\n        for (size_t j = 0; j < vec_length; ++j) {\n          if (i * vec_length + j < n) {\n            data[j] = x(i * vec_length + j);\n          } else {\n            data[j] = 0;\n          }\n        }\n        for (size_t j = 0; j < vec_length; ++j) {\n          if (i * vec_length + j < n) {\n            auto k = (data[j] / 10) % 10;\n            Kokkos::atomic_increment(bins_vec_local.data() + i * vec_length + j);\n          }\n        }\n      });\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<ExecutionSpace>(0, n_vec),\n      KOKKOS_LAMBDA(const size_t &i, double &val) {\n        for (size_t j = 0; j < vec_length; ++j) {\n          if (i * vec_length + j < n) {\n            val += bins_vec_local(i * vec_length + j);\n          }\n        }\n      },\n      Kokkos::Sum<double>(bins.data()));\n}",
            "// TODO: Your code here!\n  Kokkos::parallel_for(x.extent(0),\n  [&](const int i) {\n    int index = x(i) / 10;\n    Kokkos::atomic_increment(&bins(index));\n  });\n  Kokkos::fence();\n}",
            "// Your code here.\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n  const size_t num_values = x.extent(0);\n\n  Kokkos::parallel_for(ExecPolicy{0,num_values}, [=](const int i) {\n    auto xi = x(i);\n    auto bucket = static_cast<int>(xi / 10);\n    Kokkos::atomic_increment<size_t>(&bins(bucket));\n  });\n}",
            "// TODO\n}",
            "/* Your solution goes here */\n}",
            "/*\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, x.extent(0));\n  auto lambda = KOKKOS_LAMBDA(const size_t& i) {\n    // insert code here to update bins\n  };\n  Kokkos::parallel_for(policy, lambda);\n  */\n\n  /*\n  int *ptr;\n  int myIdx;\n  int sum;\n  int numElements = 10;\n  Kokkos::View<int *> bins_new(\"bins\", numElements);\n  Kokkos::View<int **> d_bins_new(\"bins\", numElements, 1);\n\n  Kokkos::deep_copy(bins_new, 0);\n  Kokkos::deep_copy(d_bins_new, bins_new);\n\n  Kokkos::parallel_for(numElements, [&](int myIdx) {\n    int sum = 0;\n    for(int i = 0; i < x.extent(0); i++) {\n      sum += (int)x(i);\n    }\n    d_bins_new(myIdx, 0) = sum;\n  });\n\n  Kokkos::deep_copy(bins, d_bins_new);\n  */\n\n  Kokkos::parallel_for(x.extent(0), [&](size_t i) {\n    int myIdx = ((int)x(i) / 10);\n    bins(myIdx)++;\n  });\n\n  /*\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(size_t i) {\n    ptr = Kokkos::atomic_fetch_add(&bins(0), 1);\n  });\n  */\n\n  /*\n  int binSize = (int) (100 / 10);\n  Kokkos::parallel_for(10, KOKKOS_LAMBDA(int myIdx) {\n    int sum = 0;\n    for (int i = 0; i < x.extent(0); i++) {\n      if (((int)x(i) / binSize) == myIdx) {\n        sum++;\n      }\n    }\n    Kokkos::atomic_fetch_add(&bins(myIdx), sum);\n  });\n  */\n}",
            "using ExecutionSpace = typename Kokkos::DefaultExecutionSpace;\n\n  // Create Kokkos policy and launch Kokkos kernel.\n  auto policy = Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0));\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int& i) {\n    const int bin = static_cast<int>(std::floor(x(i) / 10));\n    Kokkos::atomic_increment(&bins[bin]);\n  });\n\n  // Sync to ensure kernel completes.\n  Kokkos::fence();\n\n  // Check the result on the host.\n  for (int i = 0; i < 10; ++i) {\n    printf(\"Bin %d: %zu\\n\", i, bins[i]);\n  }\n}",
            "// Your code goes here\n}",
            "// your code goes here\n\n}",
            "// Your code here\n\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::Cuda>;\n\n  // Create a Kokkos kernel for assigning values in `bins`.\n  Kokkos::parallel_for(ExecPolicy(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i) {\n        // TODO: Compute the bin to which x[i] belongs and assign it a value of 1.\n      });\n\n  // TODO: Replace `0` with a reduction to compute the number of values in each bin.\n  //       Use the `Kokkos::atomic_fetch_add` primitive.\n  Kokkos::parallel_reduce(ExecPolicy(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i, int &binCount) {\n        // TODO: Compute the bin to which x[i] belongs and add 1 to the appropriate bin.\n      }, bins);\n}",
            "// Put your implementation here.\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(x.size()),\n      [=] (int i) {\n        int bin = (int)x(i) / 10;\n        if (bin > 9) bin = 9;\n        Kokkos::atomic_fetch_add(&bins[bin], 1);\n      });\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// Your code goes here.\n\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(size_t i) {\n    auto val = x[i];\n    if (val < 0 || val >= 100) return;\n    bins[int(val / 10)]++;\n  });\n  Kokkos::fence();\n}",
            "// Implementation here\n}",
            "/* TODO: Put your solution here */\n}",
            "// Your code goes here\n  //...\n}",
            "/* CODE GOES HERE */\n\n}",
            "// TODO: Fill in the body of the parallel for loop using the Kokkos::atomic functions\n}",
            "/* ************************************************************************ */\n  /*\n   * You may use the following code to construct x, but feel free to modify it\n   * or not use it at all.\n   */\n  Kokkos::View<double*> x_h(\"x\", 10);\n  double x_h_data[] = {7, 32, 95, 12, 39, 32, 11, 71, 70, 66};\n  Kokkos::parallel_for(10, [&] (int i) {\n    x_h(i) = x_h_data[i];\n  });\n  x_h.deep_copy(x);\n  /* ************************************************************************ */\n\n  Kokkos::View<size_t[10]> bins_h(\"bins_h\", 10);\n  // Use a parallel_for to count the number of elements in each range\n  Kokkos::parallel_for(10, [&] (int i) {\n    // Count the number of elements in [i * 10, i * 10 + 10)\n    int count = 0;\n    for (int j = 0; j < 10; j++) {\n      if (x(j) >= i * 10 && x(j) < i * 10 + 10) {\n        count++;\n      }\n    }\n    bins_h(i) = count;\n  });\n  bins_h.deep_copy(bins);\n}",
            "// TODO: fill in body of function\n}",
            "// TODO:\n  // 1. Use the provided `bins` array as an atomic accumulator.\n  // 2. Use the atomic accumulator to increment the bin that a given `x` value\n  //    belongs to.\n  // 3. For example, x[3] is 12, so we need to increment the 10's bin (at index\n  //    1). This is done with `Kokkos::atomic_fetch_add`.\n  // 4. Since `bins` is a `View` of size 10, the index into `bins` needs to\n  //    scale the `x` value by 10 before indexing. For example, 12/10 = 1.2, so\n  //    the 10's bin is at `bins[1]`.\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using mem_space = Kokkos::MemoryTraits<Kokkos::Unmanaged>;\n\n  auto count = Kokkos::View<int[10], mem_space>(bins.data());\n\n  Kokkos::parallel_for(\n      Kokkos::RangePolicy<ExecutionSpace>(0, x.extent(0)),\n      KOKKOS_LAMBDA(int i) {\n        auto bin = int(x(i) / 10);\n        if (bin < 10) {\n          Kokkos::atomic_fetch_add(&(count[bin]), 1);\n        }\n      });\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(size_t const& i) {\n    size_t const bin = (x(i) / 10);\n    Kokkos::atomic_fetch_add(&bins[bin], 1);\n  });\n}",
            "// Fill in this function\n}",
            "/* TODO: Fill this in */\n  const size_t num_elems = x.extent(0);\n  Kokkos::View<size_t*, Kokkos::HostSpace> bins_host(\"bins_host\", 10);\n  Kokkos::parallel_for(num_elems, KOKKOS_LAMBDA(const size_t& i) {\n    const auto x_i = x[i];\n    const auto bin_idx = (x_i < 100.0)? (size_t) (x_i / 10.0) : 9;\n    Kokkos::atomic_increment<Kokkos::View<size_t*, Kokkos::HostSpace>>(&bins_host(bin_idx));\n  });\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "/*... */\n}",
            "// TODO\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 10), [&] (int i) {\n    for (int j = 0; j < x.size(); ++j) {\n      if (x(j) >= i * 10 && x(j) < (i + 1) * 10) {\n        bins(i)++;\n      }\n    }\n  });\n\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// TODO: your code here\n}",
            "// TODO: Fill in the implementation here\n\n}",
            "// TODO: add code\n\n\n}",
            "// Implement this function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0, x.extent(0), 100), KOKKOS_LAMBDA (const int& idx) {\n        const int b = (int) x(idx);\n        const int bin = b / 10;\n        Kokkos::atomic_increment(&bins[bin]);\n    });\n    Kokkos::fence();\n}",
            "// Your code goes here\n}",
            "Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::Cuda>> policy(0, x.size());\n  Kokkos::parallel_reduce(policy, KOKKOS_LAMBDA(const size_t& i, size_t& update) {\n    size_t bin = (size_t)(x(i) / 10);\n    if (bin < 10) {\n      ++update(bin);\n    }\n  }, bins);\n  Kokkos::fence();\n}",
            "// Your code goes here\n\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(size_t const i) {\n      // Fill in the body of the kernel\n    });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"binsBy10Count\", x.extent(0), KOKKOS_LAMBDA(int i) {\n    const double xi = x(i);\n    if (0 <= xi && xi < 10) {\n      Kokkos::atomic_increment(&bins(0));\n    } else if (10 <= xi && xi < 20) {\n      Kokkos::atomic_increment(&bins(1));\n    } else if (20 <= xi && xi < 30) {\n      Kokkos::atomic_increment(&bins(2));\n    } else if (30 <= xi && xi < 40) {\n      Kokkos::atomic_increment(&bins(3));\n    } else if (40 <= xi && xi < 50) {\n      Kokkos::atomic_increment(&bins(4));\n    } else if (50 <= xi && xi < 60) {\n      Kokkos::atomic_increment(&bins(5));\n    } else if (60 <= xi && xi < 70) {\n      Kokkos::atomic_increment(&bins(6));\n    } else if (70 <= xi && xi < 80) {\n      Kokkos::atomic_increment(&bins(7));\n    } else if (80 <= xi && xi < 90) {\n      Kokkos::atomic_increment(&bins(8));\n    } else if (90 <= xi && xi <= 100) {\n      Kokkos::atomic_increment(&bins(9));\n    }\n  });\n  Kokkos::fence();\n}",
            "/* Your code here */\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0, x.size()),\n                         KOKKOS_LAMBDA(const int i) {\n                             bins[static_cast<size_t>((x(i) / 10))] += 1;\n                         });\n    Kokkos::fence();\n}",
            "// Your code goes here!\n}",
            "/* TODO: Your code here */\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)), [&](int i) {\n    size_t bin = 0;\n    if (x(i) < 10) bin = 0;\n    else if (x(i) < 20) bin = 1;\n    else if (x(i) < 30) bin = 2;\n    else if (x(i) < 40) bin = 3;\n    else if (x(i) < 50) bin = 4;\n    else if (x(i) < 60) bin = 5;\n    else if (x(i) < 70) bin = 6;\n    else if (x(i) < 80) bin = 7;\n    else if (x(i) < 90) bin = 8;\n    else if (x(i) < 100) bin = 9;\n    Kokkos::atomic_add(&bins(bin), 1);\n  });\n  Kokkos::fence();\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "const size_t n = x.extent(0);\n\n  // TODO: set up the parallel_for.\n  // TODO: use the Kokkos::parallel_for to iterate over the values of `x` and count\n  //       the number of values in each bin.\n  // TODO: use the Kokkos::single to print the counts out.\n\n}",
            "// your code here\n}",
            "using namespace Kokkos;\n\n    /*\n    Kokkos::parallel_for(10, [&](size_t i) {\n        bins(i) = 0;\n    });\n    */\n    Kokkos::parallel_reduce(\n        x.extent(0),\n        [&](size_t i, size_t &update) {\n            const int b = static_cast<int>(x(i) / 10);\n            Atomic<size_t *> bin_val(bins.data() + b);\n            bin_val() += 1;\n        },\n        bins.data()\n    );\n}",
            "// Your code here\n}",
            "// TODO: Implement using the `binsByXCount` function.\n\n}",
            "size_t bin_size = x.extent(0);\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, bin_size);\n\n  Kokkos::parallel_for(\n    policy,\n    KOKKOS_LAMBDA (int i) {\n      // Compute the index of the bin this value should go in\n      int bin_index = x(i) / 10;\n\n      // Write the value to the correct bin\n      Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    }\n  );\n\n  // Wait for all the work to finish\n  Kokkos::fence();\n}",
            "// TODO: Your code here\n}",
            "Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    int bin = static_cast<int>(x(i)) / 10;\n    bins(bin)++;\n  });\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> >\n        policy(0, bins.extent(0));\n\n    Kokkos::parallel_for(\"bin_by_10_count\", policy, KOKKOS_LAMBDA(const int i) {\n        for(int j=0;j<bins.extent(1);j++)\n            bins(i,j) = 0;\n    });\n\n    Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static> >\n        policy2(0, x.extent(0));\n\n    Kokkos::parallel_for(\"bin_by_10_count2\", policy2, KOKKOS_LAMBDA(const int i) {\n        double a = x(i);\n        int b = (int)a/10;\n        Kokkos::atomic_increment(&bins(b));\n    });\n}",
            "/* Your code here */\n}",
            "// Your code here\n\n}",
            "// TODO: Create a Kokkos lambda to do this.\n    // Hint: use `parallel_for` and use a `TeamPolicy` with 10 teams of 1 thread\n    // each.\n    // Your lambda should take 2 arguments: an integer `i` and a `TeamPolicy`\n    // `team`.\n    // TODO: Define a lambda.\n    // Hint: use the `Kokkos::parallel_for` function and pass it a policy and your\n    // lambda.\n    // Your lambda should have 2 arguments: an integer `i` and a `TeamPolicy`\n    // `team`.\n    // TODO: Inside the lambda, you will need to define a local variable to\n    // capture `i`.\n    // Hint: use the `Kokkos::parallel_for` function and pass it a policy and your\n    // lambda.\n    // Your lambda should have 2 arguments: an integer `i` and a `TeamPolicy`\n    // `team`.\n    // TODO: Inside the lambda, you will need to define a local variable to\n    // capture `i`.\n\n}",
            "// Your code here.\n}",
            "// TODO: Fill in\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA (const int i) {\n    const int bin = x[i] / 10;\n    Kokkos::atomic_increment(&bins[bin]);\n  });\n  Kokkos::fence();\n}",
            "// YOUR CODE GOES HERE\n}",
            "// Your code here!\n}",
            "// Your code here!\n\n}",
            "// Your code goes here\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using IndexType = Kokkos::IndexType<int>;\n  Kokkos::parallel_for(\n    \"BinsBy10\", Kokkos::RangePolicy<ExecutionSpace, IndexType>(0, x.size()),\n    [=](IndexType i) {\n      const double x_i = x(i);\n      bins(int(x_i / 10.0)) += 1;\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      size_t bin_index = size_t(x(i) / 10.0);\n      Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n    });\n}",
            "/* Your code here */\n}",
            "Kokkos::parallel_for(\"bins_by_10_count\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n    size_t bin_id = size_t(x(i) / 10.);\n    if (bin_id >= 10) {\n      bin_id = 9;\n    }\n    Kokkos::atomic_increment(&bins[bin_id]);\n  });\n}",
            "// TODO\n}",
            "using namespace Kokkos;\n    using Policy = Kokkos::RangePolicy<typename Kokkos::DefaultExecutionSpace>;\n    const int n = x.size();\n    int bins_data[10];\n    auto bins_host = View<size_t[10]>(&bins_data[0]);\n    Kokkos::parallel_for(Policy(0, n), KOKKOS_LAMBDA (const int i) {\n        int bucket = (int)x(i) / 10;\n        bins_data[bucket]++;\n    });\n    deep_copy(bins, bins_host);\n}",
            "// Write your code here\n}",
            "Kokkos::RangePolicy<Kokkos::OpenMP> range(0, x.size());\n    Kokkos::parallel_for(\"bin_x\", range, KOKKOS_LAMBDA(int const i) {\n        double const x_i = x(i);\n        size_t const bin = static_cast<size_t>(x_i / 10);\n        Kokkos::atomic_increment(&bins[bin]);\n    });\n    Kokkos::fence(); // ensure that bins are up to date when returning\n}",
            "using Kokkos::RangePolicy;\n    using Kokkos::Schedule;\n\n    // Your code here\n    // Assume the following declarations\n    int n = x.extent(0);\n    Kokkos::parallel_for(\n        \"histogram\",\n        RangePolicy<>(0, n),\n        KOKKOS_LAMBDA(const int i) {\n            const int val = static_cast<int>(x[i]) / 10;\n            Kokkos::atomic_increment(&bins[val]);\n        });\n}",
            "// 1. Define the Kokkos parallel_for functor\n  struct CountBy10 {\n    const Kokkos::View<const double*> x;\n    Kokkos::View<size_t[10]> bins;\n\n    KOKKOS_INLINE_FUNCTION\n    void operator() (size_t i) const {\n      const double x_i = x[i];\n      const size_t bin_i = static_cast<size_t>(x_i/10);\n      Kokkos::atomic_increment(&bins[bin_i]);\n    }\n  };\n\n  // 2. Execute the parallel_for\n  Kokkos::parallel_for(CountBy10{x, bins});\n\n  // 3. Synchronize the execution space\n  Kokkos::fence();\n}",
            "// your code here\n}",
            "// Insert your code here\n\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "// TODO: Use Kokkos to compute the correct `bins`.\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using TeamPolicy = Kokkos::TeamPolicy<ExecutionSpace>;\n\n  const int num_teams = 10;\n  const int team_size = 10;\n  const int vector_length = 1;\n\n  TeamPolicy policy(num_teams, team_size, vector_length);\n  Kokkos::parallel_for(\n    \"bin-count\",\n    policy,\n    KOKKOS_LAMBDA(const Kokkos::TeamPolicy<ExecutionSpace>::member_type &team) {\n      const int i = team.league_rank();\n      auto count = 0;\n      Kokkos::parallel_reduce(\n        Kokkos::TeamThreadRange(team, x.extent(0)),\n        [=](const int &j, size_t &count) {\n          if (x(j) / 10.0 == i) {\n            count += 1;\n          }\n        },\n        count\n      );\n      bins(i) = count;\n    }\n  );\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const size_t i) {\n    bins(i) = std::count_if(x.begin(), x.end(), [i](double val) {\n      return val >= 10*i && val < 10*(i+1);\n    });\n  });\n\n  Kokkos::fence();\n}",
            "using Policy = Kokkos::RangePolicy<Kokkos::RoundRobin>;\n\n  Kokkos::parallel_for(\"bin_by_10_count\", Policy(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      int bin = (int)x[i] / 10;\n      bins[bin] += 1;\n    });\n}",
            "// TODO: Add code here to call Kokkos parallel_for to implement\n  //       the algorithm described in the function comments.\n\n  // To access the elements of x and bins, use the following syntax:\n  //   x(i) or bins(j)\n  // where i and j are indices in the range [0, x.extent(0)]\n  // where x.extent(0) is the number of elements in x.\n  // For more details, see:\n  // https://kokkos.github.io/docs/reference/View.html#view-subview-accessors\n  // For more details on parallel_for, see:\n  // https://kokkos.github.io/docs/md_containers_parallel_for.html\n\n  // Implementation notes:\n  // 1. You can use the following syntax to compute a bin index:\n  //    bin_index = std::floor(x(i) / 10.0);\n  // 2. To update the bin counts, use the following syntax:\n  //    Kokkos::atomic_fetch_add(&bins(bin_index), 1);\n  // 3. You can use the following to compute the number of threads in\n  //    the parallel_for:\n  //    int n_threads = x.extent(0);\n\n}",
            "// Your code goes here\n}",
            "// TODO\n  // Implement the body of the Kernel. \n  // Use the index type `i` and access the input values by `x(i)`.\n  // Store results in `bins`.\n\n  const int N = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, N), KOKKOS_LAMBDA (const int i) {\n  });\n\n  Kokkos::fence();\n\n  // TODO\n  // Verify that the kernel worked.\n  // The right answer is: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n\n}",
            "// Set the first element of bins to zero.\n  // (Because we are storing the counts in the first 10 elements of bins.)\n  Kokkos::deep_copy(bins.subview(0), 0);\n\n  // Define a parallel_for loop that increments the bin corresponding to an\n  // input value.\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    [=](int i) {\n      double val = x(i);\n      int bin = static_cast<int>(val/10);\n      if (bin > 9) bin = 9;\n      Kokkos::atomic_increment(&bins(bin));\n    });\n\n  // Synchronize before calling deep_copy.\n  Kokkos::fence();\n\n  // Copy the bins back to the host.\n  Kokkos::deep_copy(bins.h_view, bins);\n}",
            "// TODO: Add code to fill bins\n}",
            "// TODO\n}",
            "using namespace Kokkos;\n  using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using Member = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n  using UnorderedTag = Kokkos::UnorderedExecutionTag;\n  using OrderedTag = Kokkos::OrderedExecutionTag;\n  using TeamPolicy = Kokkos::TeamPolicy<ExecutionSpace>;\n  using TeamThreadRange = Kokkos::TeamThreadRange;\n\n  //... write code here...\n}",
            "/* Your code here */\n}",
            "Kokkos::parallel_for(x.extent(0), [=] (size_t i) {\n        // use `x` and `bins` here\n    });\n\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA (const size_t idx) {\n      const size_t x_idx = x(idx);\n      if (x_idx >= 10 && x_idx < 20) {\n        Kokkos::atomic_increment(&bins[1]);\n      } else if (x_idx >= 20 && x_idx < 30) {\n        Kokkos::atomic_increment(&bins[2]);\n      } else if (x_idx >= 30 && x_idx < 40) {\n        Kokkos::atomic_increment(&bins[3]);\n      } else if (x_idx >= 40 && x_idx < 50) {\n        Kokkos::atomic_increment(&bins[4]);\n      } else if (x_idx >= 50 && x_idx < 60) {\n        Kokkos::atomic_increment(&bins[5]);\n      } else if (x_idx >= 60 && x_idx < 70) {\n        Kokkos::atomic_increment(&bins[6]);\n      } else if (x_idx >= 70 && x_idx < 80) {\n        Kokkos::atomic_increment(&bins[7]);\n      } else if (x_idx >= 80 && x_idx < 90) {\n        Kokkos::atomic_increment(&bins[8]);\n      } else if (x_idx >= 90 && x_idx < 100) {\n        Kokkos::atomic_increment(&bins[9]);\n      }\n  });\n  Kokkos::fence();\n}",
            "// TODO: Create a Kokkos view for the input x.\n    // TODO: Use Kokkos parallel_for to do the counting.\n    // TODO: Return the results in the bins view.\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.size()),\n  [&](const int i) {\n    // Determine which bin this value falls into\n    const int bin = static_cast<int>(std::floor(x[i] / 10));\n    Kokkos::atomic_increment(&bins[bin]);\n  });\n}",
            "const auto x_size = x.extent(0);\n    // TODO: implement this\n}",
            "Kokkos::parallel_for(\n        Kokkos::RangePolicy<Kokkos::Rank<2>>(\n            {0, 0}, {x.extent(0), 10}),\n        KOKKOS_LAMBDA(const int i, const int j) {\n            if (j == 0) {\n                if (x(i) < 10.0) { bins(0)++; }\n            } else if (j == 1) {\n                if (x(i) < 20.0) { bins(1)++; }\n            } else if (j == 2) {\n                if (x(i) < 30.0) { bins(2)++; }\n            } else if (j == 3) {\n                if (x(i) < 40.0) { bins(3)++; }\n            } else if (j == 4) {\n                if (x(i) < 50.0) { bins(4)++; }\n            } else if (j == 5) {\n                if (x(i) < 60.0) { bins(5)++; }\n            } else if (j == 6) {\n                if (x(i) < 70.0) { bins(6)++; }\n            } else if (j == 7) {\n                if (x(i) < 80.0) { bins(7)++; }\n            } else if (j == 8) {\n                if (x(i) < 90.0) { bins(8)++; }\n            } else if (j == 9) {\n                if (x(i) < 100.0) { bins(9)++; }\n            }\n        }\n    );\n\n    // Use `Kokkos::deep_copy` to transfer the result back to the CPU:\n    Kokkos::deep_copy(bins, bins);\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      const double val = x(i);\n      if (val >= 0.0 && val < 10.0) {\n        Kokkos::atomic_increment(&bins[0]);\n      }\n      if (val >= 10.0 && val < 20.0) {\n        Kokkos::atomic_increment(&bins[1]);\n      }\n      if (val >= 20.0 && val < 30.0) {\n        Kokkos::atomic_increment(&bins[2]);\n      }\n      if (val >= 30.0 && val < 40.0) {\n        Kokkos::atomic_increment(&bins[3]);\n      }\n      if (val >= 40.0 && val < 50.0) {\n        Kokkos::atomic_increment(&bins[4]);\n      }\n      if (val >= 50.0 && val < 60.0) {\n        Kokkos::atomic_increment(&bins[5]);\n      }\n      if (val >= 60.0 && val < 70.0) {\n        Kokkos::atomic_increment(&bins[6]);\n      }\n      if (val >= 70.0 && val < 80.0) {\n        Kokkos::atomic_increment(&bins[7]);\n      }\n      if (val >= 80.0 && val < 90.0) {\n        Kokkos::atomic_increment(&bins[8]);\n      }\n      if (val >= 90.0 && val <= 100.0) {\n        Kokkos::atomic_increment(&bins[9]);\n      }\n    });\n}",
            "Kokkos::View<size_t*> count(\"count\", 10);\n    Kokkos::parallel_for(\n        \"count_by_10\",\n        Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n        KOKKOS_LAMBDA(const size_t i) {\n            const size_t bin = static_cast<size_t>(x(i) / 10);\n            Kokkos::atomic_increment(&count(bin));\n        }\n    );\n    Kokkos::deep_copy(bins, count);\n}",
            "/* Your code goes here */\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0, x.extent(0)),\n                         KOKKOS_LAMBDA(const int i) {\n        int digit = std::floor(x(i) / 10);\n        atomic_add(&bins[digit], 1);\n    });\n\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"binsBy10Count\",\n    Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0, 1, 1024),\n    KOKKOS_LAMBDA(const int& i) {\n      const int index = (int)(x(i) / 10);\n      Kokkos::atomic_increment(&bins[index]);\n    }\n  );\n}",
            "/* Your solution goes here  */\n\n}",
            "size_t bins_size = bins.size();\n  assert(bins_size == 10);\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_host, 0);\n\n  auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n\n  auto work = [&] (const int i) {\n    size_t xi = x_host(i);\n    size_t bin = (xi / 10) * 10;\n    assert(bin < 100);\n    bins_host[bin / 10]++;\n  };\n  Kokkos::RangePolicy<decltype(work)> policy(0, x.size());\n  Kokkos::parallel_for(policy, work);\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// your code here\n\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    // find the bin this element goes in.\n    size_t bin = (x(i) / 10);\n\n    // atomically increment the bin count\n    Kokkos::atomic_increment(&bins(bin));\n  });\n}",
            "/* Your code goes here */\n}",
            "// Use the Kokkos::parallel_for function to implement the above.\n}",
            "// TODO\n}",
            "// Implement me\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n                       [=] (const int i) {\n                         const double value = x(i);\n                         for (size_t j = 0; j < 10; ++j) {\n                           if (j * 10 <= value && value < (j + 1) * 10)\n                             Kokkos::atomic_increment(&(bins(j)));\n                         }\n                       });\n  Kokkos::DefaultExecutionSpace().fence();\n}",
            "// Fill the body of this function.\n    int N = x.size();\n    auto x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n    Kokkos::parallel_for(\n        \"binsBy10Count\",\n        Kokkos::RangePolicy<Kokkos::Serial>(0, N),\n        KOKKOS_LAMBDA(int i) {\n            // Compute the bin number.\n            int bin_number = x_host(i) / 10;\n            // Update the bin count.\n            bins(bin_number) += 1;\n        }\n    );\n    Kokkos::fence();\n}",
            "//...\n}",
            "using namespace Kokkos;\n  using C = double;\n  using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using BinType = double;\n  using BinViewType = View<BinType[10]>;\n\n  // Allocate space for the reduction\n  BinType bins_host[10] = {};\n  BinViewType bins_view(\"bins\", 10);\n  deep_copy(bins_view, bins_host);\n\n  // Do the reduction\n  ParallelReduce(\n    x.extent(0),\n    KOKKOS_LAMBDA(const int i, BinViewType& bins) {\n      const C x_i = x(i);\n      bins[static_cast<int>(x_i / 10)] += 1;\n    },\n    bins_view\n  );\n\n  // Copy the results back to the host\n  deep_copy(bins, bins_view);\n}",
            "// TODO\n}",
            "// Your code goes here!\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// your code here\n\n}",
            "// Fill bins with zeros.\n    bins.fill(0);\n\n    #pragma omp parallel for schedule(dynamic)\n    for (size_t i=0; i<x.size(); i++) {\n        size_t bin = x[i] / 10; // floor division\n        if (bin < 10)\n            bins[bin]++;\n    }\n}",
            "// TODO: Fill this in\n}",
            "int numThreads = omp_get_num_threads();\n    #pragma omp parallel for\n    for (int i = 0; i < numThreads; i++) {\n        int thisThread = omp_get_thread_num();\n        for (int j = 0; j < 10; j++) {\n            if (x[i] >= j*10 && x[i] < (j + 1)*10) {\n                bins[j]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        for (size_t i = 0; i < x.size(); ++i) {\n            int idx = floor(x[i] / 10.0);\n            #pragma omp atomic update\n            ++bins[idx];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "/* Your code here */\n\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++)\n  {\n      int size = x.size();\n      bins[i] = 0;\n      for (int j = 0; j < size; j++)\n      {\n          if (x[j] >= i*10 && x[j] < (i+1)*10)\n          {\n              bins[i]++;\n          }\n      }\n  }\n  return;\n}",
            "}",
            "size_t n = x.size();\n  bins.fill(0);\n#pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    size_t bin = (size_t)(x[i] / 10.);\n    if (bin < bins.size()) {\n      bins[bin]++;\n    }\n  }\n}",
            "size_t const n = x.size();\n#pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    int bin = (int) std::floor(x[i] / 10.0);\n    bins[bin] += 1;\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO\n    int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int digit = floor(x[i] / 10);\n        bins[digit] += 1;\n    }\n}",
            "bins = {0};\n    // TODO\n}",
            "for (int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    int n = x.size();\n#pragma omp parallel for\n    for (int i = 0; i < n; ++i) {\n        if (x[i] >= 0 && x[i] < 10) {\n            ++bins[0];\n        } else if (x[i] >= 10 && x[i] < 20) {\n            ++bins[1];\n        } else if (x[i] >= 20 && x[i] < 30) {\n            ++bins[2];\n        } else if (x[i] >= 30 && x[i] < 40) {\n            ++bins[3];\n        } else if (x[i] >= 40 && x[i] < 50) {\n            ++bins[4];\n        } else if (x[i] >= 50 && x[i] < 60) {\n            ++bins[5];\n        } else if (x[i] >= 60 && x[i] < 70) {\n            ++bins[6];\n        } else if (x[i] >= 70 && x[i] < 80) {\n            ++bins[7];\n        } else if (x[i] >= 80 && x[i] < 90) {\n            ++bins[8];\n        } else if (x[i] >= 90 && x[i] < 100) {\n            ++bins[9];\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for default(shared)\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin = 0;\n        if (x[i] < 10) {\n            bin = 0;\n        }\n        else if (x[i] < 20) {\n            bin = 1;\n        }\n        else if (x[i] < 30) {\n            bin = 2;\n        }\n        else if (x[i] < 40) {\n            bin = 3;\n        }\n        else if (x[i] < 50) {\n            bin = 4;\n        }\n        else if (x[i] < 60) {\n            bin = 5;\n        }\n        else if (x[i] < 70) {\n            bin = 6;\n        }\n        else if (x[i] < 80) {\n            bin = 7;\n        }\n        else if (x[i] < 90) {\n            bin = 8;\n        }\n        else {\n            bin = 9;\n        }\n        #pragma omp atomic update\n        bins[bin] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i){\n        int bin = std::min(static_cast<int>(x[i]) / 10, 9);\n        #pragma omp atomic\n        ++bins[bin];\n    }\n}",
            "bins.fill(0);\n\n    #pragma omp parallel\n    {\n        // TODO: count the number of values in x in each bin in parallel\n    }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < bins.size(); i++) {\n        for(size_t j = 0; j < x.size(); j++) {\n            if(x[j] >= i*10 && x[j] < (i + 1)*10) {\n                #pragma omp atomic\n                bins[i]++;\n            }\n        }\n    }\n}",
            "// Implement this function\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    size_t bin = x[i] / 10;\n    #pragma omp atomic\n    ++bins[bin];\n  }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        bins[x[i] / 10]++;\n    }\n}",
            "#pragma omp parallel\n    {\n        int numThreads = omp_get_num_threads();\n        int threadNum = omp_get_thread_num();\n        int numPerThread = x.size() / numThreads;\n        int start = numPerThread * threadNum;\n        int end = (numPerThread * (threadNum + 1)) - 1;\n        for (int i = start; i <= end; ++i) {\n            int bin = int(x[i] / 10);\n            #pragma omp atomic\n            bins[bin]++;\n        }\n    }\n}",
            "/* Your solution goes here  */\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int which_bin = x[i] / 10;\n    #pragma omp critical\n    bins[which_bin] += 1;\n  }\n}",
            "size_t n = x.size();\n#pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    double const &x_i = x[i];\n    int const bin = std::min(9, std::max(0, int(std::floor(x_i / 10))));\n    bins[bin]++;\n  }\n}",
            "/* Your code goes here */\n#pragma omp parallel for\n    for (size_t i=0; i<10; i++) {\n        bins[i] = 0;\n        for (auto val : x)\n            if (val >= 10*i && val < 10*(i+1))\n                bins[i]++;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    const size_t index = (x[i] / 10) % 10;\n    bins[index]++;\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i=0; i < x.size(); ++i) {\n            int bin = (int)x[i] / 10;\n            bins[bin]++;\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static, 1)\n    for (size_t i = 0; i < bins.size(); ++i) {\n        size_t bin_size = 0;\n        for (double xi : x) {\n            if (xi >= i * 10 && xi < (i + 1) * 10) {\n                ++bin_size;\n            }\n        }\n        bins[i] = bin_size;\n    }\n}",
            "#pragma omp parallel for schedule(static)\n    for (auto const& value : x) {\n        int const binIndex = (value / 10.0) - 0.001;\n        assert(0 <= binIndex && binIndex < 10);\n        #pragma omp atomic\n        bins[binIndex] += 1;\n    }\n\n}",
            "}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++) {\n            // TODO\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        int index = floor(x[i] / 10);\n        if (index == 9) index = 8;\n        #pragma omp atomic\n        bins[index]++;\n    }\n}",
            "// Insert your code here\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int bin = floor(x[i] / 10);\n    bins[bin]++;\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        auto v = x[i];\n        // What is the correct way to update bins?\n        bins[1] += v >= 10 && v < 20;\n        //...\n    }\n}",
            "// TODO: Replace this with your solution\n}",
            "// YOUR CODE GOES HERE\n\n  // 1. get the number of thread\n  int num_thread = omp_get_num_threads();\n  // 2. find the start index of the vector x\n  int start = num_thread * x.size() / 10;\n  // 3. find the end index of the vector x\n  int end = start + x.size() / 10;\n  // 4. iterate the vector\n  #pragma omp for\n  for (int i = 0; i < 10; i++) {\n    for (int j = start; j < end; j++) {\n      if (x[j] >= i*10 && x[j] < (i+1) * 10) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "bins.fill(0);\n\n  // TODO: implement\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double const v = x[i];\n    // What's a more concise way of doing this?\n    if (v < 10) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (v < 20) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (v < 30) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (v < 40) {\n      #pragma omp atomic\n      bins[3]++;\n    } else if (v < 50) {\n      #pragma omp atomic\n      bins[4]++;\n    } else if (v < 60) {\n      #pragma omp atomic\n      bins[5]++;\n    } else if (v < 70) {\n      #pragma omp atomic\n      bins[6]++;\n    } else if (v < 80) {\n      #pragma omp atomic\n      bins[7]++;\n    } else if (v < 90) {\n      #pragma omp atomic\n      bins[8]++;\n    } else {\n      #pragma omp atomic\n      bins[9]++;\n    }\n  }\n}",
            "// Start 10 threads and distribute the vector elements to the threads\n  // each thread processes a portion of the vector.\n  // When you divide the elements into 10 parts and each part contains at\n  // most 1 element, then it would be easy to get the counts.\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    // compute the index of bin for each element in x\n    // if the element in x is less than 0, 1, 2,..., 9, 10, then it is in\n    // the 0th bin, the 1st bin,..., the 9th bin, the 10th bin, respectively\n    size_t binIndex = std::floor(x[i] / 10.0);\n    ++bins[binIndex];\n  }\n}",
            "#pragma omp parallel for default(none) shared(x, bins)\n    for (size_t i = 0; i < x.size(); ++i) {\n        const double x_i = x[i];\n        const size_t bin = (x_i / 10.0);\n        if (bin <= 9) {\n            #pragma omp atomic\n            bins[bin]++;\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < 10; i++) {\n            bins[i] = 0;\n        }\n\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            // TODO\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        //...\n    }\n}",
            "}",
            "}",
            "int nthreads = 0;\n    #pragma omp parallel\n    {\n        nthreads = omp_get_num_threads();\n    }\n    printf(\"nthreads=%d\\n\", nthreads);\n\n    // TODO: your code here\n\n}",
            "// TODO: Fill in your code here\n}",
            "// TODO: add your code here\n}",
            "bins.fill(0);\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = static_cast<size_t>(x[i] / 10);\n        if (bin < 10) {\n            #pragma omp atomic\n            ++bins[bin];\n        }\n    }\n}",
            "omp_set_num_threads(8);\n    size_t const SIZE = x.size();\n    for (size_t i = 0; i < SIZE; i++) {\n        bins[static_cast<size_t>(x[i]/10)]++;\n    }\n}",
            "// TODO: fill this in\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int idx = (int) std::floor(x[i] / 10.0);\n    bins[idx]++;\n  }\n}",
            "size_t n = x.size();\n    bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i=0; i<n; i++) {\n        size_t bin = x[i] / 10;\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    int bin = (int)(value / 10.0);\n    omp_set_lock(&lock);\n    bins[bin]++;\n    omp_unset_lock(&lock);\n  }\n}",
            "// Initialize bins to zero\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Initialize a temporary bins array that has 100 entries\n    std::vector<size_t> tempBins(100, 0);\n\n    /*\n     * Your code goes here\n     *\n     * Note: You should create a temporary array that has 100 entries, fill it\n     * with zeroes, and store the counts in this array.\n     */\n\n    // Copy tempBins to bins\n    for (size_t i = 0; i < 10; i++) {\n        bins[i] = tempBins[i * 10];\n        for (size_t j = 1; j < 10; j++) {\n            bins[i] += tempBins[i * 10 + j];\n        }\n    }\n\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        int bin = std::min((int)std::floor(x[i] / 10), 9);\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "// TODO: Implement this\n}",
            "std::array<size_t, 10> count = {};\n\n    for(int i=0; i<bins.size(); i++){\n\n        #pragma omp parallel for\n        for(size_t j=0; j<x.size(); j++){\n\n            if (x[j] >= i*10 && x[j] < (i+1)*10){\n                #pragma omp critical\n                count[i]++;\n            }\n        }\n    }\n    bins = count;\n}",
            "int n_threads = omp_get_max_threads();\n  std::vector<std::vector<double>> x_threads(n_threads);\n  std::vector<size_t> bins_threads(n_threads, 0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int i_thread = omp_get_thread_num();\n    x_threads[i_thread].push_back(x[i]);\n  }\n\n  for (size_t i = 0; i < x_threads.size(); i++) {\n    for (size_t j = 0; j < x_threads[i].size(); j++) {\n      int bin = x_threads[i][j] / 10;\n      bins_threads[i] += bin;\n    }\n  }\n\n  for (size_t i = 0; i < bins_threads.size(); i++) {\n    bins[i] = bins_threads[i];\n  }\n}",
            "size_t const n = x.size();\n\n  // Your code goes here!\n  omp_set_num_threads(8);\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    size_t bin = size_t(x[i] / 10);\n    if (bin < 10) {\n      #pragma omp atomic\n      bins[bin]++;\n    }\n  }\n}",
            "// YOUR CODE GOES HERE\n  int n = bins.size();\n  std::vector<int> bins_new(n, 0);\n#pragma omp parallel for\n  for (int i = 0; i < n; ++i)\n  {\n      for (int j = 0; j < x.size(); ++j)\n      {\n          if (x[j] >= i * 10 && x[j] < (i + 1) * 10)\n          {\n              ++bins_new[i];\n          }\n      }\n  }\n  for (int i = 0; i < n; ++i)\n  {\n      bins[i] = bins_new[i];\n  }\n}",
            "// Insert your solution here\n\n}",
            "/* YOUR CODE GOES HERE */\n  omp_set_num_threads(10);\n#pragma omp parallel for\n  for (size_t i = 0; i < 10; ++i)\n  {\n    for (size_t j = 0; j < x.size(); ++j)\n    {\n      if (x[j] >= i * 10 && x[j] < (i+1)*10)\n      {\n        ++bins[i];\n      }\n    }\n  }\n\n  // This is for debug only\n  // for (int i = 0; i < 10; ++i)\n  // {\n  //   std::cout << bins[i] << std::endl;\n  // }\n}",
            "// TODO: Fill this in\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        // TODO: Fill this in\n    }\n}",
            "}",
            "// 1. Your code here\n  // std::cout << \"Hello, World!\" << std::endl;\n  size_t count_by_10[10] = {0};\n  size_t index;\n  double start = 0.0, end = 10.0;\n\n  omp_set_num_threads(16);\n\n  #pragma omp parallel for private(index) shared(x, count_by_10, start, end) default(none)\n  for (int i = 0; i < x.size(); i++) {\n    index = static_cast<size_t>(x[i] / 10);\n    if (x[i] >= start && x[i] < end) {\n      count_by_10[index] += 1;\n    }\n  }\n\n  for (int i = 0; i < 10; i++) {\n    bins[i] = count_by_10[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        bins[(int)(x[i] / 10)]++;\n    }\n}",
            "// The code below is not correct. Fix it!\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int binNumber = x[i] / 10;\n        bins[binNumber]++;\n    }\n\n}",
            "omp_set_num_threads(8);\n\n    bins.fill(0);\n    size_t bin = 0;\n\n    #pragma omp parallel for schedule(guided)\n    for (size_t i = 0; i < x.size(); i++) {\n        bin = static_cast<size_t>(x[i] / 10);\n        #pragma omp critical\n        bins[bin] += 1;\n    }\n}",
            "//#pragma omp parallel for\n    for (int i = 0; i < bins.size(); i++) {\n      bins[i] = 0;\n      for (int j = 0; j < x.size(); j++) {\n        if (x[j] >= i * 10 && x[j] < (i + 1) * 10) {\n          bins[i]++;\n        }\n      }\n    }\n}",
            "int threads = omp_get_max_threads();\n  #pragma omp parallel num_threads(threads)\n  {\n    int rank = omp_get_thread_num();\n    for (int i = 0; i < x.size(); i++) {\n      int bin = x[i] / 10;\n      if (bin < 0 || bin >= 10)\n        continue;\n      #pragma omp atomic update\n      bins[bin]++;\n    }\n  }\n}",
            "#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < bins.size(); ++i) {\n        for (auto n : x) {\n            if (n < i * 10 && n >= i * 10 + 10) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "}",
            "// TODO: replace this with your code\n  // bins.fill(0); // Set all the values to zero\n  // std::vector<double>::iterator begin = x.begin();\n  // std::vector<double>::iterator end = x.end();\n  // #pragma omp parallel for\n  // for (auto it = begin; it!= end; ++it) {\n  //   int index = (*it / 10);\n  //   if (index < 0) {\n  //     index = 0;\n  //   } else if (index > 9) {\n  //     index = 9;\n  //   }\n  //   bins[index]++;\n  // }\n\n}",
            "#pragma omp parallel for\n    for(size_t i=0; i<bins.size(); i++){\n        bins[i] = 0;\n    }\n    for(size_t i=0; i<x.size(); i++) {\n        #pragma omp critical\n        {\n            bins[x[i] / 10]++;\n        }\n    }\n}",
            "// Your code here!\n\n}",
            "// TODO: add code here.\n}",
            "}",
            "for (int i = 0; i < 10; i++) bins[i] = 0;\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++)\n    #pragma omp atomic\n      ++bins[std::floor(x[i]/10.0)];\n}",
            "size_t binCounts[10] = {};\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    // We know that `x[i]` is in [0, 100]\n    double index = std::min(static_cast<int>(std::floor(x[i] / 10.0)), 9);\n    binCounts[static_cast<int>(index)] += 1;\n  }\n\n  for (int i = 0; i < 10; i++) {\n    bins[i] = binCounts[i];\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); ++i) {\n        // write your code here\n    }\n}",
            "// Insert your code here\n\n}",
            "/* YOUR CODE HERE */\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            int key = floor(x[i] / 10);\n            if (key >= 0 && key < 10) {\n                #pragma omp atomic\n                bins[key]++;\n            }\n        }\n    }\n\n    /* END OF YOUR CODE */\n}",
            "// Add your code here!\n}",
            "// TODO: implement this function\n\n}",
            "}",
            "// Write code here\n  bins.fill(0);\n  size_t size = x.size();\n  size_t chunk = size / 10;\n\n  #pragma omp parallel for num_threads(10)\n  for (size_t i = 0; i < size; ++i) {\n    int index = x[i] / 10;\n    if (index >= 0 && index < 10) {\n      ++bins[index];\n    }\n  }\n}",
            "/* Insert your code here */\n}",
            "// TODO\n}",
            "/* Your solution goes here. */\n\n}",
            "bins.fill(0);\n\n    // ****** INSERT YOUR CODE HERE ******\n    // Get number of threads\n    int num_threads;\n#pragma omp parallel\n    {\n#pragma omp single\n        num_threads = omp_get_num_threads();\n    }\n    // Split the vector\n    int split_size = x.size() / num_threads;\n    int remain = x.size() % num_threads;\n    int start = 0, end = 0;\n\n    // Initialize vector of sums\n    std::vector<size_t> sum(num_threads);\n    sum.assign(num_threads, 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        int thread_id = omp_get_thread_num();\n        if (i < split_size) {\n            if (x[i] < 10)\n                sum[thread_id] += 1;\n            else if (x[i] >= 10 && x[i] < 20)\n                sum[thread_id] += 2;\n            else if (x[i] >= 20 && x[i] < 30)\n                sum[thread_id] += 3;\n            else if (x[i] >= 30 && x[i] < 40)\n                sum[thread_id] += 4;\n            else if (x[i] >= 40 && x[i] < 50)\n                sum[thread_id] += 5;\n            else if (x[i] >= 50 && x[i] < 60)\n                sum[thread_id] += 6;\n            else if (x[i] >= 60 && x[i] < 70)\n                sum[thread_id] += 7;\n            else if (x[i] >= 70 && x[i] < 80)\n                sum[thread_id] += 8;\n            else if (x[i] >= 80 && x[i] < 90)\n                sum[thread_id] += 9;\n            else if (x[i] >= 90 && x[i] <= 100)\n                sum[thread_id] += 10;\n        } else if (i > x.size() - remain) {\n            if (x[i] < 10)\n                sum[thread_id] += 1;\n            else if (x[i] >= 10 && x[i] < 20)\n                sum[thread_id] += 2;\n            else if (x[i] >= 20 && x[i] < 30)\n                sum[thread_id] += 3;\n            else if (x[i] >= 30 && x[i] < 40)\n                sum[thread_id] += 4;\n            else if (x[i] >= 40 && x[i] < 50)\n                sum[thread_id] += 5;\n            else if (x[i] >= 50 && x[i] < 60)\n                sum[thread_id] += 6;\n            else if (x[i] >= 60 && x[i] < 70)\n                sum[thread_id] += 7;\n            else if (x[i] >= 70 && x[i] < 80)\n                sum[thread_id] += 8;\n            else if (x[i] >= 80 && x[i] < 90)\n                sum[thread_id] += 9;\n            else if (x[i] >= 90 && x[i] <= 100)\n                sum[thread_id] += 10;\n        }\n    }\n\n    // Add sums\n    for (int i = 1; i < num_threads; ++i)\n        sum[0] += sum[i];\n\n    for (int i = 0; i < num_threads; ++i)\n        sum[i] = 0;\n\n    // Initialize thread id\n    int thread_id;\n    // Initialize start and end\n    start = 0;\n    end = 0;\n\n#",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (auto& e : bins) {\n      e = 0;\n    }\n\n    // Count the number of values in the range [i*10, (i+1)*10)\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      auto const& val = x[i];\n      if (val >= 0 && val < 10) {\n        ++bins[0];\n      } else if (val >= 10 && val < 20) {\n        ++bins[1];\n      } else if (val >= 20 && val < 30) {\n        ++bins[2];\n      } else if (val >= 30 && val < 40) {\n        ++bins[3];\n      } else if (val >= 40 && val < 50) {\n        ++bins[4];\n      } else if (val >= 50 && val < 60) {\n        ++bins[5];\n      } else if (val >= 60 && val < 70) {\n        ++bins[6];\n      } else if (val >= 70 && val < 80) {\n        ++bins[7];\n      } else if (val >= 80 && val < 90) {\n        ++bins[8];\n      } else if (val >= 90 && val < 100) {\n        ++bins[9];\n      }\n    }\n  }\n}",
            "for (int i = 0; i < 10; i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++)\n        bins[int(x[i] / 10)]++;\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin = std::min(10 * std::floor(x[i] / 10), 9);\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n    for (size_t i = 0; i < x.size(); i++) {\n        bins[((x[i] / 10) > 10? 9 : x[i] / 10)]++;\n    }\n}",
            "// TODO: implement me!\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        size_t bin = static_cast<size_t>(x[i] / 10);\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "omp_set_num_threads(omp_get_max_threads());\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        int bin = (int) (x[i] / 10);\n        bins[bin]++;\n    }\n}",
            "#pragma omp parallel for shared(x, bins)\n    for (int i=0; i<x.size(); ++i) {\n        bins[(int)x[i]/10]++;\n    }\n}",
            "/* Your solution goes here */\n    size_t const size = x.size();\n    size_t const per_thread = size/omp_get_num_threads();\n    size_t const rem = size % omp_get_num_threads();\n\n    std::array<size_t, 10> thread_bins;\n    #pragma omp parallel for\n    for (size_t i = 0; i < size; i++) {\n        size_t const thread = i / per_thread;\n        int const bin = (int)x[i] / 10;\n\n        thread_bins[bin]++;\n    }\n\n    bins = thread_bins;\n}",
            "// ***********************************************************************\n    // * Replace this comment with your solution.\n    // ***********************************************************************\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 10; ++i) {\n    for (auto v: x) {\n      if (v >= i*10 && v < (i+1)*10) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n    for (size_t i=0; i < bins.size(); ++i) {\n        // Count the number of values in the range [i * 10, (i + 1) * 10)\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int idx = (int) (x[i] / 10.0);\n    if (idx > 9)\n      bins[9] += 1;\n    else\n      bins[idx] += 1;\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto j = static_cast<int>(x[i] / 10);\n    if (j >= 0 && j < 10) {\n      #pragma omp atomic\n      ++bins[j];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (auto xi : x) {\n        // TODO: count xi in bins\n    }\n}",
            "#pragma omp parallel for default(none) shared(x, bins)\n    for (size_t i = 0; i < x.size(); i++) {\n        int bin_num = static_cast<int>(std::floor(x[i] / 10.0));\n        if ((bin_num >= 0) && (bin_num <= 9)) {\n            #pragma omp atomic update\n            bins[bin_num]++;\n        }\n    }\n}",
            "// TODO: Fill in this function\n\n}",
            "for (size_t i=0; i<x.size(); i++)\n        bins[x[i] / 10]++;\n\n}",
            "// TODO: fill this in\n}",
            "// YOUR CODE HERE\n\n}",
            "// TODO: Fill in the missing code\n\n    #pragma omp parallel for\n    for (int i=0; i<x.size(); i++) {\n        int index = x[i]/10;\n        if (index >= 0 && index < 10) {\n            #pragma omp atomic\n            bins[index]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = (size_t)(x[i] / 10.0);\n        #pragma omp atomic update\n        bins[bin]++;\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code here\n#pragma omp parallel\n  {\n    size_t local_id = omp_get_thread_num();\n    size_t local_size = omp_get_num_threads();\n\n    #pragma omp for\n    for(size_t i = 0; i < x.size(); i++)\n    {\n      double cur = x[i];\n      cur = (cur - (cur % 10)) / 10;\n\n      bins[cur] += 1;\n    }\n  }\n}",
            "for(int i = 0; i < 10; i++)\n        bins[i] = 0;\n\n    // TODO: implement me!\n}",
            "// You can use the following to check how many threads OpenMP has created\n  //#pragma omp parallel\n  //{\n  //  std::cout << \"Hello, world! I am thread \" << omp_get_thread_num() << '\\n';\n  //}\n\n  // TODO: Use omp_get_num_threads() to get the number of threads\n  // TODO: Use omp_get_thread_num() to get the thread id\n  // TODO: Use omp_get_num_procs() to get the number of processors\n  // TODO: Use omp_in_parallel() to check if it's in parallel\n  // TODO: Use omp_set_num_threads(n) to set the number of threads\n  // TODO: Use omp_get_dynamic() to get the dynamic setting\n  // TODO: Use omp_set_dynamic(0) to set the dynamic setting to false\n\n  // TODO: Use omp_set_num_threads(4) to set the number of threads to 4\n  // TODO: Use omp_get_dynamic() to get the dynamic setting\n  // TODO: Use omp_set_dynamic(1) to set the dynamic setting to true\n\n  // TODO: Use omp_set_num_threads(8) to set the number of threads to 8\n  // TODO: Use omp_get_dynamic() to get the dynamic setting\n  // TODO: Use omp_set_dynamic(0) to set the dynamic setting to false\n\n  // TODO: Use omp_set_dynamic(1) to set the dynamic setting to true\n  // TODO: Use omp_get_num_threads() to get the number of threads\n\n  // TODO: Use omp_set_dynamic(0) to set the dynamic setting to false\n  // TODO: Use omp_get_num_threads() to get the number of threads\n\n  // TODO: Use omp_set_num_threads(4) to set the number of threads to 4\n  // TODO: Use omp_get_num_procs() to get the number of processors\n  // TODO: Use omp_set_num_threads(8) to set the number of threads to 8\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_get_dynamic() to get the dynamic setting\n  // TODO: Use omp_set_dynamic(0) to set the dynamic setting to false\n\n  // TODO: Use omp_set_dynamic(1) to set the dynamic setting to true\n  // TODO: Use omp_get_num_threads() to get the number of threads\n\n  // TODO: Use omp_set_dynamic(0) to set the dynamic setting to false\n  // TODO: Use omp_get_num_threads() to get the number of threads\n\n  // TODO: Use omp_set_num_threads(4) to set the number of threads to 4\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_set_num_threads(8) to set the number of threads to 8\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_set_num_threads(16) to set the number of threads to 16\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_set_num_threads(32) to set the number of threads to 32\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_set_num_threads(64) to set the number of threads to 64\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_set_num_threads(128) to set the number of threads to 128\n  // TODO: Use omp_get_num_procs() to get the number of processors\n\n  // TODO: Use omp_set_num_threads(256) to set the number of threads to 256\n  // TODO: Use omp_get_num_procs() to get the number",
            "#pragma omp parallel for\n    for (auto& bin : bins) {\n        bin = 0;\n        for (auto& xi : x) {\n            if (xi >= 0 && xi < 10) {\n                bin += 1;\n            }\n        }\n    }\n}",
            "// TODO: implement this\n\n}",
            "// TODO: Fill in the body of the function\n    // Note: Use `omp_get_thread_num()` to figure out which thread you are\n\n    // TODO: What is the correct syntax for this loop?\n    for (size_t i = 0; i < x.size(); i++) {\n        // TODO: What is the correct syntax for this loop?\n        for (size_t j = 0; j < 10; j++) {\n            if (j < x[i] && x[i] < (j + 1) * 10) {\n                bins[j]++;\n            }\n        }\n    }\n}",
            "}",
            "// Your code goes here!\n\n\n\n}",
            "// TODO\n}",
            "size_t i, nthreads, tid;\n    nthreads = omp_get_num_threads();\n    tid = omp_get_thread_num();\n    #pragma omp for\n    for (i = 0; i < x.size(); i++) {\n        size_t bin = static_cast<size_t>(x[i] / 10);\n        bins[bin]++;\n    }\n}",
            "/* Insert your code here */\n}",
            "// TODO: implement parallelization\n\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < x.size(); ++i) {\n        int bin = x[i] / 10;\n        if(bin > 9)\n            bin = 9;\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "}",
            "}",
            "#pragma omp parallel for schedule(dynamic)\n  for (int i = 0; i < x.size(); i++){\n\n    int j = static_cast<int>(x[i]);\n    j = j/10;\n\n    #pragma omp atomic\n    bins[j]++;\n  }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < 10; i++) {\n        // TODO\n    }\n}",
            "// TODO\n}",
            "// Your code here!\n  int num = 0;\n  for (int i = 0; i < 10; i++)\n  {\n    num = 0;\n    #pragma omp parallel for reduction (+: num)\n    for (int j = 0; j < x.size(); j++)\n    {\n      if (x[j] >= i * 10 && x[j] < (i + 1) * 10)\n      {\n        num++;\n      }\n    }\n    bins[i] = num;\n  }\n}",
            "//#pragma omp parallel for\n  for(size_t i = 0; i < 10; ++i){\n    bins[i] = 0;\n    for(auto e: x)\n      if (e >= i * 10 && e < (i + 1) * 10)\n        bins[i]++;\n  }\n}",
            "// write your solution here\n}",
            "/* YOUR CODE HERE */\n\n  bins.fill(0);\n\n  for (auto const &val : x) {\n    auto idx = static_cast<int>(val / 10.0);\n    bins.at(idx)++;\n  }\n\n\n}",
            "int n_threads = omp_get_num_threads();\n  std::cout << n_threads << \" threads\" << std::endl;\n  for (int i = 0; i < 10; i++)\n    bins[i] = 0;\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int idx = x[i] / 10;\n    bins[idx]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int idx = x[i] / 10;\n        if (idx < 10) {\n            // TODO: use an atomic increment (see `incBin` below)\n            bins[idx]++;\n        }\n    }\n}",
            "for (size_t i=0; i<bins.size(); i++) {\n    bins[i] = 0;\n  }\n#pragma omp parallel for reduction(+: bins)\n  for (auto xx : x) {\n    size_t bin = xx / 10;\n    bins[bin]++;\n  }\n}",
            "for(size_t i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i) {\n        ++bins[int(x[i] / 10.0) % 10];\n    }\n}",
            "// TODO\n}",
            "// YOUR CODE GOES HERE\n\n}",
            "/* Your solution goes here  */\n\n}",
            "}",
            "// Use omp_get_thread_num() to get a number between 0 and omp_get_num_threads()-1\n  // Use omp_get_num_threads() to get the number of threads\n  // Use omp_get_max_threads() to get the number of threads that are available\n  // Use omp_get_num_procs() to get the number of cores on the computer\n\n  // Your code here\n}",
            "// write code here\n}",
            "// TODO: Implement\n}",
            "}",
            "// TODO: fill bins with the right values\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        int count = int(x[i] / 10);\n        if (count >= 10) {\n            count = 9;\n        }\n        ++bins[count];\n    }\n\n}",
            "size_t numThreads = omp_get_num_threads();\n  std::vector<std::array<size_t, 10>> counts(numThreads);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int index = (int)x[i]/10;\n    ++counts[omp_get_thread_num()][index];\n  }\n\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n  for (int i = 0; i < numThreads; ++i) {\n    for (int j = 0; j < 10; ++j) {\n      bins[j] += counts[i][j];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i=0; i < x.size(); i++) {\n        // Find which bin each element of x belongs to\n        size_t binIdx = static_cast<size_t>(x[i] / 10);\n        // Increment the bin count\n        bins[binIdx]++;\n    }\n}",
            "// TODO: Fill in the code here\n    // You can use #pragma omp parallel for to parallelize this loop.\n    // You will need to declare `bins` as shared, but do not use any locks!\n    //\n    // Hint: you should use the `<algorithm>` header and `std::transform`.\n    // You might find `std::round` helpful.\n}",
            "// TODO: fill in the bins\n    int n_threads = omp_get_max_threads();\n    int n_bins = 10;\n    #pragma omp parallel for\n    for (int i = 0; i < n_bins; i++) {\n        bins[i] = 0;\n    }\n    for (auto const& i : x) {\n        int bin = (int) (i/10);\n        #pragma omp atomic\n        bins[bin] += 1;\n    }\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i=0; i<x.size(); i++) {\n        int val = (int)x[i];\n        int bin = (int)x[i]/10;\n        #pragma omp critical\n        bins[bin]++;\n    }\n}",
            "// Your code here\n    int count[10] = {0};\n    omp_set_num_threads(8);\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if ((int)x[i] < 10)\n            count[0]++;\n        else if ((int)x[i] < 20)\n            count[1]++;\n        else if ((int)x[i] < 30)\n            count[2]++;\n        else if ((int)x[i] < 40)\n            count[3]++;\n        else if ((int)x[i] < 50)\n            count[4]++;\n        else if ((int)x[i] < 60)\n            count[5]++;\n        else if ((int)x[i] < 70)\n            count[6]++;\n        else if ((int)x[i] < 80)\n            count[7]++;\n        else if ((int)x[i] < 90)\n            count[8]++;\n        else if ((int)x[i] < 100)\n            count[9]++;\n    }\n\n    bins[0] = count[0];\n    bins[1] = count[1];\n    bins[2] = count[2];\n    bins[3] = count[3];\n    bins[4] = count[4];\n    bins[5] = count[5];\n    bins[6] = count[6];\n    bins[7] = count[7];\n    bins[8] = count[8];\n    bins[9] = count[9];\n}",
            "}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        int j = x[i]/10;\n        bins[j]++;\n    }\n}",
            "// TODO: replace this line with the code to count bins in parallel\n    // using OpenMP\n}",
            "omp_set_num_threads(4);\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < 10; ++i){\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i){\n        int bin = std::round(x[i] / 10);\n        bins[bin] += 1;\n    }\n}",
            "// Your code here\n}",
            "// Fill in the code here\n    #pragma omp parallel for\n    for (auto& i : bins) {\n        i = 0;\n    }\n    for (auto const& val : x) {\n        bins[val / 10] += 1;\n    }\n}",
            "int num_threads = omp_get_max_threads();\n  int thread_id = omp_get_thread_num();\n  // Your code here.\n}",
            "// TODO\n}",
            "size_t const n = x.size();\n  //size_t const nthreads = omp_get_num_threads();\n  //printf(\"%d %d\\n\", n, nthreads);\n\n  for (size_t i = 0; i < n; ++i) {\n    double xi = x[i];\n    int binIndex = (int) (xi / 10);\n    if (xi < 10) {\n      ++bins[0];\n    }\n    if (xi < 20 && xi > 10) {\n      ++bins[1];\n    }\n    if (xi < 30 && xi > 20) {\n      ++bins[2];\n    }\n    if (xi < 40 && xi > 30) {\n      ++bins[3];\n    }\n    if (xi < 50 && xi > 40) {\n      ++bins[4];\n    }\n    if (xi < 60 && xi > 50) {\n      ++bins[5];\n    }\n    if (xi < 70 && xi > 60) {\n      ++bins[6];\n    }\n    if (xi < 80 && xi > 70) {\n      ++bins[7];\n    }\n    if (xi < 90 && xi > 80) {\n      ++bins[8];\n    }\n    if (xi < 100 && xi > 90) {\n      ++bins[9];\n    }\n  }\n}",
            "// your code here\n}",
            "#pragma omp parallel for schedule(static, 4)\n  for (int i = 0; i < 10; i++) {\n    for (int j = 0; j < x.size(); j++) {\n      if ((x[j] >= i * 10) && (x[j] < i * 10 + 10)) {\n        bins[i]++;\n      }\n    }\n  }\n\n}",
            "// TODO: YOUR CODE HERE\n}",
            "}",
            "// Initialize bins\n  for(auto& bin : bins) {\n    bin = 0;\n  }\n\n  // TODO: count the number of values in each bin.\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int binNum = x[i] / 10;\n    #pragma omp atomic\n    bins[binNum]++;\n  }\n}",
            "// Initialize bins to zero\n    for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n\n    // Count in parallel\n    #pragma omp parallel\n    {\n        // Each thread will count its own bins\n        std::array<size_t, 10> privateBins;\n\n        // Initialize bins to zero\n        for (size_t i = 0; i < privateBins.size(); ++i) {\n            privateBins[i] = 0;\n        }\n\n        // Count in parallel\n        #pragma omp for schedule(dynamic, 1) nowait\n        for (int i = 0; i < x.size(); ++i) {\n            int bin = static_cast<int>(x[i] / 10);\n\n            if (bin < 10) {\n                privateBins[bin]++;\n            }\n        }\n\n        // Combine all private bins into global bins\n        #pragma omp critical\n        {\n            for (int i = 0; i < privateBins.size(); ++i) {\n                bins[i] += privateBins[i];\n            }\n        }\n    }\n}",
            "// std::array<size_t, 10> bins{};\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    // add to corresponding bin\n    // std::cout << x[i] << std::endl;\n    auto bin_idx = std::min(x[i] / 10, 9);\n    // std::cout << bin_idx << std::endl;\n    #pragma omp atomic update\n    bins[bin_idx]++;\n  }\n}",
            "for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = 0; j < bins.size(); j++) {\n            if (x[i] > j * 10 && x[i] < (j + 1) * 10) {\n                bins[j]++;\n            }\n        }\n    }\n}",
            "//...\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  int i;\n  #pragma omp parallel for private(i)\n  for (i = 0; i < x.size(); i++) {\n    if (x[i] < 10)\n      bins[0]++;\n    else if (x[i] < 20)\n      bins[1]++;\n    else if (x[i] < 30)\n      bins[2]++;\n    else if (x[i] < 40)\n      bins[3]++;\n    else if (x[i] < 50)\n      bins[4]++;\n    else if (x[i] < 60)\n      bins[5]++;\n    else if (x[i] < 70)\n      bins[6]++;\n    else if (x[i] < 80)\n      bins[7]++;\n    else if (x[i] < 90)\n      bins[8]++;\n    else if (x[i] < 100)\n      bins[9]++;\n  }\n}",
            "// TODO: implement me\n}",
            "size_t bins_array_size = 10;\n  size_t n = x.size();\n\n  #pragma omp parallel for shared(bins) private(i)\n  for (int i = 0; i < n; i++) {\n    // Assign an initial value of 0 to each element of `bins`\n    //\n    // The following lines of code can be replaced by `memset(&bins, 0,\n    // bins_array_size * sizeof(size_t));`. However, memset is deprecated in\n    // C++ and it will be removed in a future version of C++.\n    for (int i = 0; i < bins_array_size; i++) {\n      bins[i] = 0;\n    }\n\n    // Find which bin to add the value to, and increment the count.\n    if (x[i] < 10) {\n      bins[0] += 1;\n    }\n    else if (x[i] < 20) {\n      bins[1] += 1;\n    }\n    else if (x[i] < 30) {\n      bins[2] += 1;\n    }\n    else if (x[i] < 40) {\n      bins[3] += 1;\n    }\n    else if (x[i] < 50) {\n      bins[4] += 1;\n    }\n    else if (x[i] < 60) {\n      bins[5] += 1;\n    }\n    else if (x[i] < 70) {\n      bins[6] += 1;\n    }\n    else if (x[i] < 80) {\n      bins[7] += 1;\n    }\n    else if (x[i] < 90) {\n      bins[8] += 1;\n    }\n    else if (x[i] < 100) {\n      bins[9] += 1;\n    }\n  }\n}",
            "}",
            "omp_set_num_threads(omp_get_num_procs());\n\n  #pragma omp parallel\n  {\n\n    // Each thread needs a private copy of the vector for its own exclusive use.\n    std::vector<double> thread_x = x;\n\n    // Each thread needs its own private copy of the bins.\n    std::array<size_t, 10> thread_bins;\n\n    // Reset the thread_bins vector.\n    #pragma omp for\n    for (size_t i = 0; i < thread_bins.size(); ++i) {\n        thread_bins[i] = 0;\n    }\n\n    // Iterate over the values of x, counting them in the appropriate bin.\n    #pragma omp for\n    for (size_t i = 0; i < thread_x.size(); ++i) {\n        size_t bin = std::floor(thread_x[i] / 10);\n        ++thread_bins[bin];\n    }\n\n    // Merge the results into the global vector.\n    #pragma omp critical\n    for (size_t i = 0; i < thread_bins.size(); ++i) {\n        bins[i] += thread_bins[i];\n    }\n  }\n}",
            "// insert your code here\n    // #pragma omp parallel for\n    // for (size_t i = 0; i < 10; i++)\n    // {\n    //     bins[i] = 0;\n    // }\n    // for (size_t i = 0; i < x.size(); i++)\n    // {\n    //     #pragma omp critical\n    //     {\n    //         if (x[i] < 10)\n    //         {\n    //             bins[0] += 1;\n    //         }\n    //         else if (x[i] < 20)\n    //         {\n    //             bins[1] += 1;\n    //         }\n    //         else if (x[i] < 30)\n    //         {\n    //             bins[2] += 1;\n    //         }\n    //         else if (x[i] < 40)\n    //         {\n    //             bins[3] += 1;\n    //         }\n    //         else if (x[i] < 50)\n    //         {\n    //             bins[4] += 1;\n    //         }\n    //         else if (x[i] < 60)\n    //         {\n    //             bins[5] += 1;\n    //         }\n    //         else if (x[i] < 70)\n    //         {\n    //             bins[6] += 1;\n    //         }\n    //         else if (x[i] < 80)\n    //         {\n    //             bins[7] += 1;\n    //         }\n    //         else if (x[i] < 90)\n    //         {\n    //             bins[8] += 1;\n    //         }\n    //         else\n    //         {\n    //             bins[9] += 1;\n    //         }\n    //     }\n    // }\n\n    // #pragma omp parallel for\n    // for (size_t i = 0; i < 10; i++)\n    // {\n    //     bins[i] = 0;\n    // }\n    // for (size_t i = 0; i < x.size(); i++)\n    // {\n    //     if (x[i] < 10)\n    //     {\n    //         bins[0] += 1;\n    //     }\n    //     else if (x[i] < 20)\n    //     {\n    //         bins[1] += 1;\n    //     }\n    //     else if (x[i] < 30)\n    //     {\n    //         bins[2] += 1;\n    //     }\n    //     else if (x[i] < 40)\n    //     {\n    //         bins[3] += 1;\n    //     }\n    //     else if (x[i] < 50)\n    //     {\n    //         bins[4] += 1;\n    //     }\n    //     else if (x[i] < 60)\n    //     {\n    //         bins[5] += 1;\n    //     }\n    //     else if (x[i] < 70)\n    //     {\n    //         bins[6] += 1;\n    //     }\n    //     else if (x[i] < 80)\n    //     {\n    //         bins[7] += 1;\n    //     }\n    //     else if (x[i] < 90)\n    //     {\n    //         bins[8] += 1;\n    //     }\n    //     else\n    //     {\n    //         bins[9] += 1;\n    //     }\n    // }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++)\n    {\n        #pragma omp critical\n        {\n            if (x[i] < 10)\n            {\n                bins[0] += 1;\n            }\n            else if (x[i]",
            "/* YOUR CODE HERE */\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i)\n    {\n        int j = (x[i] - 1) / 10;\n        bins[j]++;\n    }\n}",
            "// Initialize the bins to 0\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n\n  // Your code goes here\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for(size_t i=0; i<x.size(); i++)\n    bins[x[i]/10]++;\n}",
            "size_t nthreads;\n  #pragma omp parallel\n  {\n    #pragma omp single\n    nthreads = omp_get_num_threads();\n  }\n  std::cout << \"Parallelizing with \" << nthreads << \" threads\" << std::endl;\n\n  size_t num_bins = 10;\n  size_t num_bins_per_thread = num_bins / nthreads;\n  size_t remainder = num_bins % nthreads;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int bin = x[i] / 10;\n\n    if (bin >= 0 && bin < num_bins) {\n      #pragma omp atomic\n      bins[bin]++;\n    }\n  }\n\n  std::cout << \"Bins: \";\n  for (size_t i = 0; i < bins.size(); i++) {\n    std::cout << bins[i] << \" \";\n  }\n  std::cout << std::endl;\n}",
            "size_t const n = x.size();\n    // TODO\n}",
            "//#pragma omp parallel for\n    for(int i=0; i<x.size(); i++){\n        if(x[i]>=0 && x[i]<10){\n            bins[0]++;\n        } else if(x[i]>=10 && x[i]<20){\n            bins[1]++;\n        } else if(x[i]>=20 && x[i]<30){\n            bins[2]++;\n        } else if(x[i]>=30 && x[i]<40){\n            bins[3]++;\n        } else if(x[i]>=40 && x[i]<50){\n            bins[4]++;\n        } else if(x[i]>=50 && x[i]<60){\n            bins[5]++;\n        } else if(x[i]>=60 && x[i]<70){\n            bins[6]++;\n        } else if(x[i]>=70 && x[i]<80){\n            bins[7]++;\n        } else if(x[i]>=80 && x[i]<90){\n            bins[8]++;\n        } else if(x[i]>=90 && x[i]<100){\n            bins[9]++;\n        }\n    }\n}",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        auto val = x[i];\n        if (val < 10) {\n            bins[0]++;\n        } else if (val < 20) {\n            bins[1]++;\n        } else if (val < 30) {\n            bins[2]++;\n        } else if (val < 40) {\n            bins[3]++;\n        } else if (val < 50) {\n            bins[4]++;\n        } else if (val < 60) {\n            bins[5]++;\n        } else if (val < 70) {\n            bins[6]++;\n        } else if (val < 80) {\n            bins[7]++;\n        } else if (val < 90) {\n            bins[8]++;\n        } else if (val < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "// Your code goes here!\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel\n    {\n        // Use the first thread to compute the histogram\n        if (omp_get_thread_num() == 0) {\n            for (auto const& v : x) {\n                int idx = int(v / 10);\n                if (idx >= 0 && idx < 10) {\n                    bins[idx]++;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = 0;\n        for (auto& v : x) {\n            if (v >= i * 10 && v < (i + 1) * 10) {\n                ++bins[i];\n            }\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n        int digit = x[i]/10;\n        if (digit >= 0 && digit <= 9)\n            bins[digit]++;\n    }\n}",
            "// TODO: Replace this line with your solution\n}",
            "}",
            "// TODO: Your code goes here!\n}",
            "size_t n = x.size();\n  int nthreads = omp_get_num_threads();\n  int tid = omp_get_thread_num();\n  auto chunkSize = (n - 1) / nthreads;\n  auto start = chunkSize * tid;\n  auto end = start + chunkSize;\n  end = end >= n? n - 1 : end;\n\n  for (size_t i = start; i < end; i++) {\n    auto bin = x[i] / 10.0;\n    if (bin < 1) {\n      bins[0] += 1;\n    } else if (bin < 2) {\n      bins[1] += 1;\n    } else if (bin < 3) {\n      bins[2] += 1;\n    } else if (bin < 4) {\n      bins[3] += 1;\n    } else if (bin < 5) {\n      bins[4] += 1;\n    } else if (bin < 6) {\n      bins[5] += 1;\n    } else if (bin < 7) {\n      bins[6] += 1;\n    } else if (bin < 8) {\n      bins[7] += 1;\n    } else if (bin < 9) {\n      bins[8] += 1;\n    } else if (bin < 10) {\n      bins[9] += 1;\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "serial",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "//...\n}",
            "// TODO: implement\n}",
            "bins.fill(0);\n    for (auto v : x) {\n        size_t bin = static_cast<size_t>(v/10.);\n        ++bins[bin];\n    }\n}",
            "for (auto d : x) {\n        bins[static_cast<size_t>(std::floor(d / 10.0))]++;\n    }\n}",
            "bins = std::array<size_t, 10>{};\n    for(auto x_i: x) {\n        if(0 <= x_i && x_i < 10) {\n            bins[0]++;\n        }\n        else if(10 <= x_i && x_i < 20) {\n            bins[1]++;\n        }\n        else if(20 <= x_i && x_i < 30) {\n            bins[2]++;\n        }\n        else if(30 <= x_i && x_i < 40) {\n            bins[3]++;\n        }\n        else if(40 <= x_i && x_i < 50) {\n            bins[4]++;\n        }\n        else if(50 <= x_i && x_i < 60) {\n            bins[5]++;\n        }\n        else if(60 <= x_i && x_i < 70) {\n            bins[6]++;\n        }\n        else if(70 <= x_i && x_i < 80) {\n            bins[7]++;\n        }\n        else if(80 <= x_i && x_i < 90) {\n            bins[8]++;\n        }\n        else if(90 <= x_i && x_i <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "}",
            "for (auto i : x) {\n        int binNum = (int)i / 10;\n        bins[binNum]++;\n    }\n}",
            "for(auto xi : x) {\n        int bucketIndex = (int)xi / 10;\n        assert(bucketIndex >= 0 && bucketIndex < 10);\n        bins[bucketIndex]++;\n    }\n}",
            "for (auto xv : x) {\n    // Find the bin, and count it\n    size_t ix = static_cast<size_t>(std::floor(xv / 10));\n    bins[ix]++;\n  }\n}",
            "bins.fill(0);\n    for (auto xx : x) {\n        if (xx >= 0 && xx < 10) {\n            ++bins[0];\n        } else if (xx >= 10 && xx < 20) {\n            ++bins[1];\n        } else if (xx >= 20 && xx < 30) {\n            ++bins[2];\n        } else if (xx >= 30 && xx < 40) {\n            ++bins[3];\n        } else if (xx >= 40 && xx < 50) {\n            ++bins[4];\n        } else if (xx >= 50 && xx < 60) {\n            ++bins[5];\n        } else if (xx >= 60 && xx < 70) {\n            ++bins[6];\n        } else if (xx >= 70 && xx < 80) {\n            ++bins[7];\n        } else if (xx >= 80 && xx < 90) {\n            ++bins[8];\n        } else if (xx >= 90 && xx <= 100) {\n            ++bins[9];\n        }\n    }\n}",
            "for (auto val : x) {\n        // Note: The first bin should be [0, 10), the second bin is [10, 20), etc.\n        bins[static_cast<size_t>(val / 10.0)]++;\n    }\n}",
            "for (auto xx : x) {\n        size_t i = (size_t)(xx / 10);\n        bins[i]++;\n    }\n}",
            "for (double x_val : x) {\n    size_t bin_idx = static_cast<size_t>(x_val / 10);\n    bins[bin_idx]++;\n  }\n}",
            "//TODO\n}",
            "assert(x.size() == bins.size());\n  // TODO\n}",
            "for (auto i: x) {\n    int bin = static_cast<int>(std::floor(i / 10.0));\n    bins[bin] += 1;\n  }\n}",
            "// The bins array is initialized to zero.\n  // Replace the zeros with the counts in `x`.\n  for (size_t i = 0; i < x.size(); ++i) {\n    //...\n  }\n}",
            "// 0. Set all bins to 0.\n\n\tstd::fill(bins.begin(), bins.end(), 0);\n\n\t// 1. Create a vector to hold the count for each element in x.\n\t// 2. Count how many of the elements in x are in the range 0 - 10, 10 - 20,\n\t//    etc. using a for loop.\n\t// 3. Store the result in `bins`.\n\n\t// IMPLEMENT THIS\n\n}",
            "for(size_t i=0; i < bins.size(); ++i)\n    bins[i] = 0;\n\n  for(auto &v : x) {\n    auto bin_no = std::floor(v/10.0);\n    bins[bin_no]++;\n  }\n}",
            "for (auto i : x) {\n        bins[i/10]++;\n    }\n}",
            "for(auto const& el: x)\n    ++bins[size_t(std::floor(el / 10))];\n}",
            "// Replace this code with your implementation.\n    for (size_t i = 0; i < x.size(); ++i) {\n        bins[((int)x[i] / 10) % 10]++;\n    }\n}",
            "//...\n}",
            "bins.fill(0);\n  for (auto const& val : x) {\n    // Count the number of values in [0,10), [10, 20), [20, 30),...\n    int const bin = std::lround(val/10);\n    bins.at(bin)++;\n  }\n}",
            "for (auto const& v : x) {\n        auto bin = (size_t)(((v / 10) + 0.5) + 1);\n        ++bins[bin - 1];\n    }\n}",
            "for (auto v : x) {\n    if (v < 10.0) {\n      ++bins[0];\n    } else if (v < 20.0) {\n      ++bins[1];\n    } else if (v < 30.0) {\n      ++bins[2];\n    } else if (v < 40.0) {\n      ++bins[3];\n    } else if (v < 50.0) {\n      ++bins[4];\n    } else if (v < 60.0) {\n      ++bins[5];\n    } else if (v < 70.0) {\n      ++bins[6];\n    } else if (v < 80.0) {\n      ++bins[7];\n    } else if (v < 90.0) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "for (auto v : x) {\n        auto bin = static_cast<size_t>(floor(v/10));\n        ++bins[bin];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (double xi: x) {\n    int i = xi / 10;\n    if (i >= 0 && i < 10)\n      ++bins[i];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto xi : x) {\n        // Which bin is this value in?\n        size_t bin = std::floor(xi / 10);\n        ++bins[bin];\n    }\n}",
            "assert(x.size() > 0);\n  assert(bins.size() == 10);\n\n  /* Replace this with your solution */\n  // std::fill(bins.begin(), bins.end(), 0);\n  // for (auto const& v : x) {\n  //   int i = (v / 10);\n  //   bins[i]++;\n  // }\n  std::fill(bins.begin(), bins.end(), 0);\n  for (size_t i = 0; i < x.size(); i++) {\n    int v = x[i];\n    int i = (v / 10);\n    bins[i]++;\n  }\n}",
            "bins.fill(0);\n    for (double xv: x) {\n        int i = xv / 10.0;\n        bins[i] += 1;\n    }\n}",
            "for (double value : x) {\n        int bin = static_cast<int>(value) / 10;\n        assert(bin >= 0 && bin < 10);\n        bins[bin]++;\n    }\n}",
            "for (auto v : x) {\n        int i = static_cast<int>(v/10.0);\n        if (i >= 0 && i < 10) {\n            bins[i]++;\n        }\n    }\n}",
            "// TODO: Fill in this function\n}",
            "for (size_t bin = 0; bin < 10; bin++) {\n    bins[bin] = 0;\n  }\n\n  for (size_t i = 0; i < x.size(); i++) {\n    int bin = int(x[i] / 10);\n    if (bin < 10 && bin >= 0) {\n      bins[bin]++;\n    }\n  }\n}",
            "for (int i = 0; i < 10; ++i) bins[i] = 0;\n  for (auto const& xi : x) {\n    bins[std::min(static_cast<size_t>(std::floor(xi / 10)), static_cast<size_t>(9))]++;\n  }\n}",
            "// Your code here\n}",
            "for (auto const& xx : x) {\n        auto bin = floor(xx / 10);\n        bins[bin]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto val : x) {\n    auto bin = static_cast<size_t>(val / 10);\n    ++bins[bin];\n  }\n}",
            "for(double v : x) {\n        int index = floor(v/10);\n        if (index >= 0 && index < 10) {\n            bins[index]++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (double v : x) {\n        int b = int(v) / 10;\n        if (b < 0) b = 0;\n        if (b > 9) b = 9;\n        ++bins[b];\n    }\n}",
            "for (auto it = x.begin(); it!= x.end(); it++) {\n        int index = (int)(*it / 10.0);\n        bins[index]++;\n    }\n}",
            "bins.fill(0);\n\n   for (double value : x)\n   {\n      if (value >= 0 && value < 10) {\n         bins[0]++;\n      } else if (value >= 10 && value < 20) {\n         bins[1]++;\n      } else if (value >= 20 && value < 30) {\n         bins[2]++;\n      } else if (value >= 30 && value < 40) {\n         bins[3]++;\n      } else if (value >= 40 && value < 50) {\n         bins[4]++;\n      } else if (value >= 50 && value < 60) {\n         bins[5]++;\n      } else if (value >= 60 && value < 70) {\n         bins[6]++;\n      } else if (value >= 70 && value < 80) {\n         bins[7]++;\n      } else if (value >= 80 && value < 90) {\n         bins[8]++;\n      } else if (value >= 90 && value <= 100) {\n         bins[9]++;\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto value : x) {\n    size_t index = size_t(value / 10.0);\n    if (index < 10)\n      ++bins[index];\n  }\n}",
            "for (auto const& value : x) {\n        if (value >= 0 && value <= 10) {\n            bins[0]++;\n        } else if (value > 10 && value <= 20) {\n            bins[1]++;\n        } else if (value > 20 && value <= 30) {\n            bins[2]++;\n        } else if (value > 30 && value <= 40) {\n            bins[3]++;\n        } else if (value > 40 && value <= 50) {\n            bins[4]++;\n        } else if (value > 50 && value <= 60) {\n            bins[5]++;\n        } else if (value > 60 && value <= 70) {\n            bins[6]++;\n        } else if (value > 70 && value <= 80) {\n            bins[7]++;\n        } else if (value > 80 && value <= 90) {\n            bins[8]++;\n        } else if (value > 90 && value <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n    size_t n = x.size();\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] < 10.0) {\n            bins[0] += 1;\n        } else if (x[i] < 20.0) {\n            bins[1] += 1;\n        } else if (x[i] < 30.0) {\n            bins[2] += 1;\n        } else if (x[i] < 40.0) {\n            bins[3] += 1;\n        } else if (x[i] < 50.0) {\n            bins[4] += 1;\n        } else if (x[i] < 60.0) {\n            bins[5] += 1;\n        } else if (x[i] < 70.0) {\n            bins[6] += 1;\n        } else if (x[i] < 80.0) {\n            bins[7] += 1;\n        } else if (x[i] < 90.0) {\n            bins[8] += 1;\n        } else {\n            bins[9] += 1;\n        }\n    }\n}",
            "std::fill(std::begin(bins), std::end(bins), 0);\n\n    for (auto const xi : x) {\n        auto const bin = static_cast<size_t>(xi / 10);\n        if (bin < bins.size()) {\n            bins[bin]++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (double const &v : x) {\n        int i = 0;\n        for (double bin_start = 0; bin_start < 100; ++i, bin_start += 10) {\n            if (v < bin_start) {\n                break;\n            }\n        }\n        bins[i] += 1;\n    }\n}",
            "for (double v : x) {\n        int bin = static_cast<int>(v / 10.0);\n        if (bin < 0)\n            bin = 0;\n        else if (bin > 9)\n            bin = 9;\n\n        bins[bin]++;\n    }\n}",
            "bins.fill(0);\n  for(auto& xi: x) {\n    size_t bin = (xi / 10.0);\n    bins[bin] += 1;\n  }\n}",
            "// TODO: Your code goes here\n}",
            "for(size_t i=0; i<bins.size(); ++i) {\n        bins[i] = 0;\n    }\n    for(size_t i=0; i<x.size(); ++i) {\n        int bin = static_cast<int>(std::floor(x[i] / 10));\n        bins[bin] += 1;\n    }\n}",
            "for (auto i : x) {\n        auto bin = static_cast<int>(std::floor(i / 10));\n        bins[bin]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto i = 0u; i < x.size(); ++i) {\n    auto bin_number = x[i] / 10.0;\n    bins[bin_number]++;\n  }\n}",
            "// TODO: Your code here\n    std::vector<size_t> v(10);\n    for(auto i = 0; i < x.size(); i++) {\n        int j = std::floor(x[i] / 10);\n        v[j]++;\n    }\n\n    for(auto i = 0; i < bins.size(); i++) {\n        bins[i] = v[i];\n    }\n}",
            "bins.fill(0);\n    for (auto v : x) {\n        auto index = static_cast<size_t>(v / 10);\n        bins[index]++;\n    }\n}",
            "// TODO: implement\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin_index = x[i] / 10;\n        bins[bin_index]++;\n    }\n}",
            "for (auto v : x) {\n    bins[static_cast<size_t>(v / 10)]++;\n  }\n}",
            "bins.fill(0);\n  for (auto const &xi: x) {\n    ++bins[std::floor(xi / 10.0)];\n  }\n}",
            "for (double const& d : x) {\n    if (d < 10) {\n      bins[0]++;\n    } else if (d < 20) {\n      bins[1]++;\n    } else if (d < 30) {\n      bins[2]++;\n    } else if (d < 40) {\n      bins[3]++;\n    } else if (d < 50) {\n      bins[4]++;\n    } else if (d < 60) {\n      bins[5]++;\n    } else if (d < 70) {\n      bins[6]++;\n    } else if (d < 80) {\n      bins[7]++;\n    } else if (d < 90) {\n      bins[8]++;\n    } else if (d < 100) {\n      bins[9]++;\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n  for (double n : x) {\n    bins[static_cast<size_t>(n / 10.0)] += 1;\n  }\n}",
            "/* YOUR CODE HERE */\n\n    for (auto const& i : x) {\n        if (i >= 0.0 && i < 10.0) {\n            bins[0] += 1;\n        } else if (i >= 10.0 && i < 20.0) {\n            bins[1] += 1;\n        } else if (i >= 20.0 && i < 30.0) {\n            bins[2] += 1;\n        } else if (i >= 30.0 && i < 40.0) {\n            bins[3] += 1;\n        } else if (i >= 40.0 && i < 50.0) {\n            bins[4] += 1;\n        } else if (i >= 50.0 && i < 60.0) {\n            bins[5] += 1;\n        } else if (i >= 60.0 && i < 70.0) {\n            bins[6] += 1;\n        } else if (i >= 70.0 && i < 80.0) {\n            bins[7] += 1;\n        } else if (i >= 80.0 && i < 90.0) {\n            bins[8] += 1;\n        } else if (i >= 90.0 && i <= 100.0) {\n            bins[9] += 1;\n        }\n    }\n}",
            "for(auto xv : x) {\n        if (xv < 0 || xv > 100) throw std::range_error{\"x must be in [0, 100]\"};\n        // `bins` has 10 elements, so this is safe\n        bins[static_cast<size_t>(xv/10)]++;\n    }\n}",
            "for (double value : x) {\n    bins[static_cast<size_t>(value / 10)] += 1;\n  }\n}",
            "/* YOUR CODE HERE */\n  for (size_t i = 0; i < x.size(); i++) {\n      int bin = x[i] / 10;\n      bins[bin]++;\n  }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        auto value = x[i];\n        auto binIndex = static_cast<int>(std::floor(value / 10));\n        // if the bin index is out of bounds (i.e., > 9)\n        if (binIndex > 9) {\n            // then set it to 9\n            binIndex = 9;\n        }\n        bins[binIndex] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = x[i] / 10;\n        if (bin >= 10) {\n            bin = 9;\n        }\n        ++bins[bin];\n    }\n}",
            "// TODO: Implement this function\n}",
            "// your code here\n    int i = 0;\n    for(int j = 0; j < x.size(); j++){\n        int a = x[j]/10;\n        bins[a]++;\n    }\n\n}",
            "for (int i = 0; i < 10; i++)\n    bins[i] = 0;\n\n  for (int i = 0; i < x.size(); i++) {\n    int bin = x[i] / 10;\n    if (bin == 10) {\n      bin = 9;\n    }\n    bins[bin]++;\n  }\n}",
            "for (auto const& xval: x) {\n        size_t index = static_cast<size_t>(std::floor(xval / 10.0));\n        bins[index]++;\n    }\n}",
            "for (double x_value : x)\n   {\n      int bin = (int)x_value / 10;\n      bins[bin]++;\n   }\n}",
            "for(double xi: x) {\n        size_t bin = xi / 10;\n        bins[bin]++;\n    }\n}",
            "// TODO: Your code goes here!\n  bins = {};\n  size_t value = 0;\n  for(auto& element : x){\n    value = (element/10);\n    if(value == 10){\n      bins.at(9)++;\n    }else{\n      bins.at(value)++;\n    }\n  }\n}",
            "bins = std::array<size_t, 10> {};\n    for (auto v: x) {\n        int bin = std::floor(v / 10);\n        ++bins[bin];\n    }\n}",
            "for(auto v : x) {\n        bins[static_cast<size_t>(v/10)]++;\n    }\n}",
            "// The function is already defined. You don't need to change it.\n    assert(x.size() == bins.size());\n    for (int i = 0; i < x.size(); i++) {\n        int idx = static_cast<int>(std::floor(x[i]/10.0));\n        if (idx < 0)\n            idx = 0;\n        if (idx > 9)\n            idx = 9;\n        bins[idx]++;\n    }\n}",
            "size_t count = 0;\n\n  for (auto value : x) {\n    bins[value/10]++;\n  }\n}",
            "// YOUR CODE HERE\n}",
            "for (auto const& v : x) {\n        if (v >= 0 && v < 10) {\n            ++bins[0];\n        } else if (v >= 10 && v < 20) {\n            ++bins[1];\n        } else if (v >= 20 && v < 30) {\n            ++bins[2];\n        } else if (v >= 30 && v < 40) {\n            ++bins[3];\n        } else if (v >= 40 && v < 50) {\n            ++bins[4];\n        } else if (v >= 50 && v < 60) {\n            ++bins[5];\n        } else if (v >= 60 && v < 70) {\n            ++bins[6];\n        } else if (v >= 70 && v < 80) {\n            ++bins[7];\n        } else if (v >= 80 && v < 90) {\n            ++bins[8];\n        } else if (v >= 90 && v < 100) {\n            ++bins[9];\n        }\n    }\n}",
            "for (size_t i{}; i < x.size(); ++i) {\n    int bin{static_cast<int>(std::floor(x[i] / 10.0))};\n    ++bins[bin];\n  }\n}",
            "// TODO\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (double const& v : x) {\n      ++bins[static_cast<size_t>(std::floor(v / 10.0))];\n   }\n}",
            "/* YOUR CODE HERE */\n    int bin_size = 10;\n    for(int i = 0; i < x.size(); i++){\n        int bin_index = int(x[i] / bin_size);\n        bins[bin_index]++;\n    }\n}",
            "for(const auto& x_i : x) {\n      bins[static_cast<int>(x_i / 10)]++;\n   }\n}",
            "bins = {0};\n\n    for (auto v : x) {\n        int b = (int) (v / 10);\n        if (b >= 0 && b < 10) {\n            bins[b] += 1;\n        }\n    }\n}",
            "for (size_t i = 0; i < bins.size(); ++i)\n    bins[i] = 0;\n\n  for (auto v : x) {\n    size_t i = size_t(v / 10);\n    ++bins[i];\n  }\n}",
            "for(auto i : x) {\n    auto bin = static_cast<size_t>(std::floor(i / 10));\n    assert(bin < bins.size());\n    bins[bin]++;\n  }\n}",
            "// YOUR CODE HERE\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto v : x) {\n        int i = int(v / 10.0);\n        if (i >= 0 && i < 10) {\n            ++bins[i];\n        }\n    }\n}",
            "// Step 1: Declare the variables you need.\n  // (Note that all the variables you declared above still exist and have the same\n  // value as in the previous step)\n\n  // Step 2: Iterate through the input vector and compute the number of values in each\n  // bin (in the range [0,10), [10, 20), [20, 30),...). You should use the following\n  // function to compute the bin:\n\n  //     int binOf = (int) std::floor(x / 10.0);\n\n  // (HINT: std::floor takes a double and returns an integer.\n  //        When you divide a double by an integer, it automatically gets\n  //        converted to a double.\n  //        Therefore, you can write `x / 10.0` instead of `(double) x / 10`.\n  //        You can also write `std::floor(x / 10.0)` instead of\n  //        `std::floor((double) x / 10)`.\n  //        You can also write `std::floor(x / 10.0) + 1` instead of\n  //        `std::floor((double) x / 10) + 1`.)\n\n  // Step 3: Store the bins in the vector `bins`.\n}",
            "assert(x.size() >= 10);\n\n    bins.fill(0);\n    for (size_t i = 0; i < 10; i++) {\n        for (size_t j = 0; j < x.size(); j++) {\n            if (x[j] > i * 10.0 && x[j] <= (i + 1) * 10.0) {\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        double n = x[i] / 10.0;\n        size_t bin = (n >= 10)? 9 : static_cast<size_t>(n);\n        ++bins[bin];\n    }\n}",
            "// TODO\n}",
            "bins.fill(0);\n\n  for (double xx : x) {\n    auto i = static_cast<size_t>(xx/10.0);\n    assert(i < 10);\n    ++bins[i];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    int bin_id = (int) floor(x[i] / 10.0);\n    bins[bin_id] += 1;\n  }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n  for (double value : x) {\n    bins[10 * int(value / 10)]++;\n  }\n}",
            "/* YOUR CODE HERE */\n}",
            "// Replace this with your implementation.\n    bins = std::array<size_t, 10>{};\n\n    for (const auto& value : x) {\n        const auto bucket = static_cast<size_t>(value / 10);\n        ++bins[bucket];\n    }\n}",
            "for(double v : x) {\n    bins[(size_t) floor(v / 10.0)]++;\n  }\n}",
            "// Initialize all bins to 0\n  std::fill(std::begin(bins), std::end(bins), 0);\n\n  // Fill bins\n  for (auto val : x) {\n    auto index = static_cast<size_t>(std::floor(val / 10.0));\n    bins[index]++;\n  }\n}",
            "for (auto const& val : x) {\n    if (val < 10) {\n      ++bins[0];\n    } else if (val < 20) {\n      ++bins[1];\n    } else if (val < 30) {\n      ++bins[2];\n    } else if (val < 40) {\n      ++bins[3];\n    } else if (val < 50) {\n      ++bins[4];\n    } else if (val < 60) {\n      ++bins[5];\n    } else if (val < 70) {\n      ++bins[6];\n    } else if (val < 80) {\n      ++bins[7];\n    } else if (val < 90) {\n      ++bins[8];\n    } else if (val < 100) {\n      ++bins[9];\n    }\n  }\n}",
            "// YOUR CODE HERE\n   bins = {0};\n   for (auto const& val : x) {\n      bins[static_cast<size_t>(val / 10)] += 1;\n   }\n}",
            "for (auto xx : x) {\n    if (xx < 10)\n      bins[0] += 1;\n    else if (xx < 20)\n      bins[1] += 1;\n    else if (xx < 30)\n      bins[2] += 1;\n    else if (xx < 40)\n      bins[3] += 1;\n    else if (xx < 50)\n      bins[4] += 1;\n    else if (xx < 60)\n      bins[5] += 1;\n    else if (xx < 70)\n      bins[6] += 1;\n    else if (xx < 80)\n      bins[7] += 1;\n    else if (xx < 90)\n      bins[8] += 1;\n    else\n      bins[9] += 1;\n  }\n}",
            "for (double x_i: x) {\n        if (x_i >= 0 and x_i < 10) {\n            bins[0]++;\n        } else if (x_i >= 10 and x_i < 20) {\n            bins[1]++;\n        } else if (x_i >= 20 and x_i < 30) {\n            bins[2]++;\n        } else if (x_i >= 30 and x_i < 40) {\n            bins[3]++;\n        } else if (x_i >= 40 and x_i < 50) {\n            bins[4]++;\n        } else if (x_i >= 50 and x_i < 60) {\n            bins[5]++;\n        } else if (x_i >= 60 and x_i < 70) {\n            bins[6]++;\n        } else if (x_i >= 70 and x_i < 80) {\n            bins[7]++;\n        } else if (x_i >= 80 and x_i < 90) {\n            bins[8]++;\n        } else if (x_i >= 90 and x_i < 100) {\n            bins[9]++;\n        } else {\n            throw std::invalid_argument(\"Invalid value in vector\");\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& xx : x) {\n    size_t binIdx = std::floor(xx / 10);\n    ++bins.at(binIdx);\n  }\n}",
            "for (auto i = 0u; i < x.size(); ++i) {\n        bins[10*((x[i]-0)/10)]++;\n    }\n}",
            "for (double i : x) {\n        int x = std::floor(i/10);\n        bins[x]++;\n    }\n}",
            "for (double val : x)\n        bins[std::floor(val / 10.0)]++;\n}",
            "// TODO: your code here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto val : x) {\n        auto index = static_cast<size_t>(val / 10.);\n        if (index < 10) {\n            ++bins[index];\n        }\n    }\n}",
            "bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    for (double i : x) {\n        auto bin = static_cast<size_t>(i/10.0);\n        bins[bin]++;\n    }\n}",
            "bins.fill(0);\n  for(double const& value: x) {\n    // value is already in range [0,100] from previous call.\n    // 0-9 goes into bin 0, 10-19 into bin 1, etc.\n    bins[int(value) / 10]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (double val: x) {\n        size_t bin = floor(val / 10.0);\n        ++bins[bin];\n    }\n}",
            "// Write your code here.\n}",
            "assert(x.size() > 0);\n  assert(bins.size() == 10);\n\n  std::fill(bins.begin(), bins.end(), 0);\n\n  for(const auto& d : x) {\n    int bin_id = std::floor(d / 10.0);\n    assert(bin_id >= 0);\n    assert(bin_id <= 9);\n    bins[bin_id]++;\n  }\n}",
            "// TODO: Write code to populate bins\n\n  // example of a way to access the elements in bins\n  bins[0] = 0;\n\n}",
            "for (auto& val : bins) {\n    val = 0;\n  }\n  for (auto val : x) {\n    auto bin = static_cast<size_t>(std::floor(val / 10));\n    bins[bin]++;\n  }\n}",
            "for (auto const& n : x) {\n        size_t i = (n / 10.0);\n        bins[i]++;\n    }\n}",
            "// TODO: implement this function\n\n  for (auto xi : x)\n    bins[std::min(size_t(10*xi/100), 9)]++;\n\n}",
            "for (double d : x) {\n      // truncate d to an integer multiple of 10 and add it to the corresponding\n      // bin.\n   }\n}",
            "// bins[i] contains the count for values in the range [i*10, (i+1)*10)\n    for (auto const& elem : x) {\n        int bucket = std::floor(elem / 10);\n        bins[bucket]++;\n    }\n}",
            "for (double xi : x) {\n      double d = xi/10.0;\n      size_t i = d;\n      if (i < 10) {\n         ++bins[i];\n      }\n   }\n}",
            "bins.fill(0);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = x[i] / 10;\n        bins[bin] += 1;\n    }\n}",
            "for (auto e: x) {\n        bins[size_t(e / 10)] += 1;\n    }\n}",
            "/* YOUR CODE HERE */\n}",
            "for (double val : x) {\n        if (val >= 0 && val < 10) {\n            bins[0]++;\n        }\n        else if (val >= 10 && val < 20) {\n            bins[1]++;\n        }\n        else if (val >= 20 && val < 30) {\n            bins[2]++;\n        }\n        else if (val >= 30 && val < 40) {\n            bins[3]++;\n        }\n        else if (val >= 40 && val < 50) {\n            bins[4]++;\n        }\n        else if (val >= 50 && val < 60) {\n            bins[5]++;\n        }\n        else if (val >= 60 && val < 70) {\n            bins[6]++;\n        }\n        else if (val >= 70 && val < 80) {\n            bins[7]++;\n        }\n        else if (val >= 80 && val < 90) {\n            bins[8]++;\n        }\n        else if (val >= 90 && val < 100) {\n            bins[9]++;\n        }\n        else {\n            std::cout << \"error: \" << val << \" is out of range\\n\";\n        }\n    }\n}",
            "/* YOUR CODE HERE */\n}",
            "// Fill this in\n\n}",
            "for (auto v : x) {\n        auto b = v < 10? 0 : 1 + (v / 10);\n        bins[b] += 1;\n    }\n}",
            "for (auto value : x) {\n        int bin = static_cast<int>(value / 10);\n        if (bin < 0) {\n            bin = 0;\n        } else if (bin > 9) {\n            bin = 9;\n        }\n        ++bins[bin];\n    }\n}",
            "// TODO: implement\n}",
            "bins.fill(0);\n  for (auto const& n : x) {\n    bins[static_cast<int>(n / 10)]++;\n  }\n}",
            "// The code in this function is for illustration purposes.\n    // You may replace it with your own code, but you may not delete it.\n    // This function is already tested and graded, do not modify it.\n    throw std::runtime_error(\"You have not implemented this function yet.\");\n}",
            "for (size_t i = 0; i < x.size(); ++i)\n        bins[static_cast<size_t>(x[i] / 10)]++;\n}",
            "for(auto& b : bins)\n    b = 0;\n\n  for(auto const& v : x)\n    bins[std::min(9, size_t(v/10))]++;\n}",
            "/* Insert your code here */\n}",
            "for (auto& i : bins) {\n        i = 0;\n    }\n    for (auto const& xi : x) {\n        ++bins[size_t(xi / 10.)];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto xx: x)\n        bins[(size_t) std::floor(xx / 10)]++;\n}",
            "bins.fill(0);\n\n  for (double value : x) {\n    int bin = std::max(0, std::min(9, int(value / 10)));\n    bins[bin] += 1;\n  }\n}",
            "for (auto const &val : x) {\n        double x10 = std::floor(val / 10);\n        bins[static_cast<size_t>(x10)]++;\n    }\n}",
            "for (double d: x) {\n    size_t bin_idx = std::min(std::max(size_t(d / 10.0), 0UL), 9UL);\n    ++bins[bin_idx];\n  }\n}",
            "for (auto val : x)\n    bins[static_cast<size_t>(val / 10)]++;\n}",
            "for (auto& i : bins)\n        i = 0;\n    for (auto d : x) {\n        auto idx = (size_t) (d * 0.1);\n        ++bins[idx];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   for (auto x_val : x) {\n      if (x_val < 0 || x_val > 100) {\n         throw std::invalid_argument(\"vector element out of range\");\n      }\n      auto bin = static_cast<size_t>(x_val / 10);\n      ++bins[bin];\n   }\n}",
            "for (auto const& value : x) {\n        auto const bin = static_cast<size_t>(value / 10);\n        assert(bin <= 10);\n        ++bins[bin];\n    }\n}",
            "for(auto i: x) {\n        bins[static_cast<size_t>(i/10)]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (double value : x) {\n    size_t index = static_cast<size_t>(value / 10.0);\n    if (index > 9) {\n      index = 9;\n    }\n    ++bins[index];\n  }\n}",
            "for (auto value : x) {\n        bins[value/10]++;\n    }\n}",
            "for (auto v : x) {\n    if (v >= 0 && v <= 10)\n      bins[0]++;\n    else if (v >= 11 && v <= 20)\n      bins[1]++;\n    else if (v >= 21 && v <= 30)\n      bins[2]++;\n    else if (v >= 31 && v <= 40)\n      bins[3]++;\n    else if (v >= 41 && v <= 50)\n      bins[4]++;\n    else if (v >= 51 && v <= 60)\n      bins[5]++;\n    else if (v >= 61 && v <= 70)\n      bins[6]++;\n    else if (v >= 71 && v <= 80)\n      bins[7]++;\n    else if (v >= 81 && v <= 90)\n      bins[8]++;\n    else if (v >= 91 && v <= 100)\n      bins[9]++;\n  }\n}",
            "/*\n       TODO:\n       - count the number of occurrences of values in each of the bins.\n       - bins[0] should contain the number of occurrences of values\n         between 0 and 10\n       - bins[1] should contain the number of occurrences of values\n         between 10 and 20\n       -...\n       - bins[9] should contain the number of occurrences of values\n         between 90 and 100\n     */\n\n    int low = 0;\n    int high = 10;\n\n    for(int i = 0; i < 10; i++){\n        for(int j = 0; j < x.size(); j++){\n            if(x[j] >= low && x[j] < high){\n                bins[i]++;\n            }\n        }\n        low = low + 10;\n        high = high + 10;\n    }\n\n    //std::cout << \"output: [\" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \", \" << bins[4] << \", \" << bins[5] << \", \" << bins[6] << \", \" << bins[7] << \", \" << bins[8] << \", \" << bins[9] << \"]\" << std::endl;\n\n}",
            "for (auto i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n    for (auto v : x) {\n        int bin = std::floor(v / 10);\n        bins[bin]++;\n    }\n}",
            "size_t binIndex = 0;\n    for (auto value : x) {\n        auto lower = 10 * binIndex;\n        auto upper = 10 * (binIndex + 1);\n        if (value >= lower && value < upper) {\n            bins[binIndex]++;\n        }\n        binIndex++;\n    }\n}",
            "std::array<size_t, 10> counts;\n    // Replace this code with your solution\n    for (auto v : x) {\n        int binIdx = v / 10;\n        counts[binIdx]++;\n    }\n\n    for (int i = 0; i < 10; i++) {\n        bins[i] = counts[i];\n    }\n}",
            "for (auto xx: x)\n    bins[std::min<size_t>(xx/10, 9)]++;\n}",
            "// TODO\n}",
            "for (auto xx : x) {\n        if (xx >= 0 && xx < 10) {\n            bins[0]++;\n        }\n        else if (xx >= 10 && xx < 20) {\n            bins[1]++;\n        }\n        else if (xx >= 20 && xx < 30) {\n            bins[2]++;\n        }\n        else if (xx >= 30 && xx < 40) {\n            bins[3]++;\n        }\n        else if (xx >= 40 && xx < 50) {\n            bins[4]++;\n        }\n        else if (xx >= 50 && xx < 60) {\n            bins[5]++;\n        }\n        else if (xx >= 60 && xx < 70) {\n            bins[6]++;\n        }\n        else if (xx >= 70 && xx < 80) {\n            bins[7]++;\n        }\n        else if (xx >= 80 && xx < 90) {\n            bins[8]++;\n        }\n        else if (xx >= 90 && xx <= 100) {\n            bins[9]++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& v : x) {\n        double floorVal = floor(v / 10);\n        int binIndex = floorVal;\n        bins[binIndex]++;\n    }\n}",
            "size_t index = 0;\n   size_t i = 0;\n   while (i < x.size()) {\n      size_t count = 0;\n      while (i < x.size() && x[i] < 10) {\n         count++;\n         i++;\n      }\n      bins[index++] = count;\n   }\n}",
            "for (double val : x) {\n        bins[std::min<size_t>(9, std::floor(val / 10))]++;\n    }\n}",
            "bins = {0};\n\n    for (double value : x) {\n        double bin_idx = value / 10;\n        int index = int(bin_idx);\n        ++bins[index];\n    }\n}",
            "bins.fill(0);\n    std::for_each(x.begin(), x.end(), [&bins](auto i) {\n        int bin = static_cast<int>(i) / 10;\n        ++bins[bin];\n    });\n}",
            "//...\n}",
            "// TODO: implement me\n\n}",
            "}",
            "// Replace the statement below with your implementation\n   for (size_t i = 0; i < x.size(); ++i) {\n    int bin = (int) floor(x[i] / 10);\n    bins[bin]++;\n  }\n}",
            "// You can use binsBy10Count(x, bins) to get the counts in each bin\n  // for a vector x of doubles. bins is an array of size 10 (use std::array)\n  // that contains the counts in each bin.\n\n  // For example, if the input is:\n  // [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n  // bins is:\n  // [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n  // which means that we have 1 value in [0, 10), 2 values in [10, 20)\n  // and so on.\n\n  // You can assume that 0 <= x[i] <= 100 for all 0 <= i < x.size().\n\n  // Fill in the missing code here\n\n  for (double v : x){\n    int idx = static_cast<int>(v/10.0);\n    bins[idx]++;\n  }\n}",
            "bins.fill(0);\n  for (auto v : x) {\n    int bin = v / 10;\n    ++bins[bin];\n  }\n}",
            "bins.fill(0);\n    for (double elem : x) {\n        int index = int(elem) / 10;\n        bins[index]++;\n    }\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n    for (auto val : x) {\n        auto index = static_cast<int>(std::floor(val / 10.0));\n        if (index >= 0 && index < 10) {\n            bins[index] += 1;\n        }\n    }\n}",
            "for (auto v : x) {\n        int i = int(v/10);\n        if (0 <= i && i < 10) {\n            ++bins[i];\n        }\n    }\n}",
            "// TODO\n\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] < 10) {\n      bins[0]++;\n    }\n    else if (x[i] >= 10 && x[i] < 20) {\n      bins[1]++;\n    }\n    else if (x[i] >= 20 && x[i] < 30) {\n      bins[2]++;\n    }\n    else if (x[i] >= 30 && x[i] < 40) {\n      bins[3]++;\n    }\n    else if (x[i] >= 40 && x[i] < 50) {\n      bins[4]++;\n    }\n    else if (x[i] >= 50 && x[i] < 60) {\n      bins[5]++;\n    }\n    else if (x[i] >= 60 && x[i] < 70) {\n      bins[6]++;\n    }\n    else if (x[i] >= 70 && x[i] < 80) {\n      bins[7]++;\n    }\n    else if (x[i] >= 80 && x[i] < 90) {\n      bins[8]++;\n    }\n    else if (x[i] >= 90 && x[i] < 100) {\n      bins[9]++;\n    }\n  }\n}",
            "bins.fill(0);\n  for (auto xi: x) {\n    // cast xi to an integer and compute the bin index\n    size_t binIndex = (int)(xi / 10);\n    // update the count of the bin\n    bins[binIndex]++;\n  }\n}",
            "// TODO: write this!\n}",
            "for (double value : x) {\n        if (value >= 0 && value < 10) {\n            bins[0]++;\n        }\n        if (value >= 10 && value < 20) {\n            bins[1]++;\n        }\n        if (value >= 20 && value < 30) {\n            bins[2]++;\n        }\n        if (value >= 30 && value < 40) {\n            bins[3]++;\n        }\n        if (value >= 40 && value < 50) {\n            bins[4]++;\n        }\n        if (value >= 50 && value < 60) {\n            bins[5]++;\n        }\n        if (value >= 60 && value < 70) {\n            bins[6]++;\n        }\n        if (value >= 70 && value < 80) {\n            bins[7]++;\n        }\n        if (value >= 80 && value < 90) {\n            bins[8]++;\n        }\n        if (value >= 90 && value < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "for (auto const& v : x) {\n        int bucket = static_cast<int>(v / 10);\n        if (bucket < 0)\n            bucket = 0;\n        if (bucket > 9)\n            bucket = 9;\n        bins[bucket]++;\n    }\n}",
            "for (double xx : x) {\n        int binIndex = static_cast<int>(xx / 10);\n        if (binIndex < 0) {\n            binIndex = 0;\n        }\n        if (binIndex > 9) {\n            binIndex = 9;\n        }\n        bins[binIndex]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto val : x) {\n    size_t idx = static_cast<size_t>(val / 10);\n    if (idx < 10) {\n      bins[idx]++;\n    }\n  }\n}",
            "//...\n}",
            "for (auto xx : x)\n        bins[int(xx / 10)]++;\n}",
            "/* Your code here */\n\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        bins[(size_t) (x[i] / 10)]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (double value : x) {\n    double bin_index = value / 10;\n    size_t bin = std::floor(bin_index);\n    if (bin < 10) bins[bin]++;\n  }\n}",
            "for(size_t i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n\n  for(auto xx : x) {\n    size_t bucket = xx / 10;\n    if(bucket >= 10) {\n      bucket = 9;\n    }\n    bins[bucket]++;\n  }\n}",
            "/* Your code here! */\n\n}",
            "for (auto v : x) {\n    int bin = v / 10;\n    if (bin < 0)\n      bin = 0;\n    else if (bin > 9)\n      bin = 9;\n    ++bins[bin];\n  }\n}",
            "// YOUR CODE HERE\n\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    size_t b = (x[i] / 10);\n    bins[b] += 1;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto xi : x) {\n        bins[static_cast<size_t>(std::floor(xi / 10)) % 10]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& v: x)\n        bins[static_cast<int>(v / 10.0)]++;\n}",
            "// Write your code here\n  \n}",
            "for (double value : x) {\n        int i = floor(value / 10);\n        bins[i]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& e : x) {\n    int binIndex = static_cast<int>(e / 10);\n    if (binIndex < 10) {\n      ++bins[binIndex];\n    }\n  }\n}",
            "// TODO: Fill this in\n\n  for(int i=0; i<10; i++)\n  {\n    bins[i]=0;\n  }\n  for(int i=0; i<x.size(); i++)\n  {\n    int binIndex = x[i]/10;\n    bins[binIndex] = bins[binIndex]+1;\n  }\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "hip",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: Implement\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if(tid < N) {\n      size_t bin = (size_t)(x[tid] / 10); // bin is 0,1,2,...,9\n      atomicAdd(bins + bin, 1);\n   }\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if(idx >= N) return;\n  const size_t bin = (x[idx] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        size_t bin = x[idx] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t tid = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  if (tid < N) {\n    bins[x[tid]/10]++;\n  }\n}",
            "__shared__ size_t temp[10];\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int bin = (int)(x[tid] / 10);\n  if (bin == 9) bin = 8;\n  atomicAdd(temp + bin, 1);\n  __syncthreads();\n  if (threadIdx.x == 0)\n    for (int i = 0; i < 10; ++i) bins[i] = temp[i];\n}",
            "// Compute a bins index and use it as an array index\n    size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx >= N) return;\n    size_t i = (size_t)floor(x[idx] / 10.0);\n    atomicAdd(&bins[i], 1);\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if(index < N) {\n    double value = x[index];\n    size_t bin = (size_t)(value / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n  const size_t stride = blockDim.x * gridDim.x;\n\n  for (size_t i = start; i < N; i += stride) {\n    size_t bin = floor(x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        int bin = int(x[tid] / 10.);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid >= N) return;\n    int i = (int) (10 * x[tid] / 100);\n    atomicAdd(&bins[i], 1);\n}",
            "// TODO\n}",
            "unsigned int index = threadIdx.x;\n    while (index < N) {\n        bins[(x[index] / 10)]++;\n        index += blockDim.x;\n    }\n}",
            "size_t binNumber = threadIdx.x / 10;\n  size_t bin = min(max((size_t)x[threadIdx.x], 0), 9);\n  atomicAdd(&bins[bin], 1);\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N)\n        return;\n\n    int binIdx = static_cast<int>(x[idx] / 10);\n    if (binIdx >= 0 && binIdx < 10)\n        atomicAdd(&bins[binIdx], 1);\n}",
            "// Initialize bins to zero\n  for (int j = 0; j < 10; j++)\n    bins[j] = 0;\n\n  // Use a shared memory array to accumulate counts for each thread's chunk\n  __shared__ size_t sh_bins[10];\n  for (int j = 0; j < 10; j++)\n    sh_bins[j] = 0;\n\n  // Loop over values in x\n  int chunk_size = N / blockDim.x;\n  int start_index = chunk_size * threadIdx.x;\n  for (int i = start_index; i < start_index + chunk_size; i++) {\n    if (i < N) {\n      // Compute bin index for this value\n      int bin = (int)((x[i] +.01) / 10);\n\n      // Atomically increment that bin's count\n      atomicAdd(&sh_bins[bin], 1);\n    }\n  }\n\n  // Wait for all threads to finish updating sh_bins\n  __syncthreads();\n\n  // Copy shared memory back to global memory\n  for (int j = 0; j < 10; j++) {\n    atomicAdd(&bins[j], sh_bins[j]);\n  }\n}",
            "size_t thread = hipThreadIdx_x;\n    size_t stride = hipBlockDim_x;\n\n    for (size_t i = thread; i < N; i += stride) {\n        size_t bin = 0;\n        if (x[i] >= 0. && x[i] < 10.)\n            bin = 0;\n        else if (x[i] >= 10. && x[i] < 20.)\n            bin = 1;\n        else if (x[i] >= 20. && x[i] < 30.)\n            bin = 2;\n        else if (x[i] >= 30. && x[i] < 40.)\n            bin = 3;\n        else if (x[i] >= 40. && x[i] < 50.)\n            bin = 4;\n        else if (x[i] >= 50. && x[i] < 60.)\n            bin = 5;\n        else if (x[i] >= 60. && x[i] < 70.)\n            bin = 6;\n        else if (x[i] >= 70. && x[i] < 80.)\n            bin = 7;\n        else if (x[i] >= 80. && x[i] < 90.)\n            bin = 8;\n        else if (x[i] >= 90. && x[i] < 100.)\n            bin = 9;\n        __atomic_fetch_add(&bins[bin], 1, __ATOMIC_SEQ_CST);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N)\n    return;\n  size_t bin = x[i] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "auto i = threadIdx.x;\n  auto N_threads = blockDim.x;\n  if (i < N) {\n    auto x_i = x[i];\n    auto i_bin = int(x_i / 10.0);\n    atomicAdd(bins + i_bin, 1);\n  }\n}",
            "size_t gid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (gid >= N) return;\n\n  bins[int(x[gid] / 10)]++;\n}",
            "// The thread index corresponds to the index of the value in x\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    // Divide i by 10, rounding down to the nearest integer\n    int bin = i / 10;\n    // Increment the bin count\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "auto idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    size_t bin = x[idx] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N)\n    atomicAdd(&bins[int(x[idx] / 10)], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n\n    while (tid < N) {\n        int i = floor(x[tid] / 10);\n        atomicAdd(&(bins[i]), 1);\n        tid += stride;\n    }\n}",
            "// Your code here\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N)\n    return;\n  // Determine which bin to increment\n  double d = floor((x[i] + 0.5) / 10.0);\n  atomicAdd(&bins[(int)d], 1);\n}",
            "// Get my global thread ID\n    size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n    // Compute the modulus and use it as an index into bins\n    if (i < N) {\n        bins[x[i] / 10]++;\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        const int bin = int(x[i] / 10);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    // Note: the bins are 0-based, so the index can be 0-9.\n    if (index < N) {\n        double value = x[index];\n        size_t bin = size_t(value / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Compute number of values in each bin in parallel.\n    // Use a shared memory array of 10 ints to store counts for each bin.\n    // Use atomicAdd to add to the count for each bin.\n    // Remember to index into `bins` with a threadIdx.x.\n}",
            "// TODO: Your code here\n}",
            "const int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    const int bin = int(x[index] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: Implement me!\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int bin = (int)x[idx] / 10;\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const size_t stride = blockDim.x * gridDim.x;\n\n    for (size_t i = tid; i < N; i += stride) {\n        size_t bin = (int)(x[i] / 10);\n        if (bin >= 10) bin = 9;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// Implement this kernel\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        size_t idx = (size_t) (x[i] / 10);\n        atomicAdd(bins + idx, 1);\n    }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index >= N) { return; }\n    int bin = (int)(x[index] / 10.);\n    atomicAdd(bins + bin, 1);\n}",
            "size_t idx = threadIdx.x;\n    size_t stride = blockDim.x;\n\n    __shared__ int sharedMem[10];\n    memset(sharedMem, 0, 10 * sizeof(int));\n    __syncthreads();\n\n    for (size_t i = idx; i < N; i += stride)\n    {\n        int bin = int(x[i] / 10);\n        atomicAdd(&sharedMem[bin], 1);\n    }\n\n    __syncthreads();\n    for (int i = 0; i < 10; i++)\n    {\n        atomicAdd(&bins[i], sharedMem[i]);\n    }\n}",
            "__shared__ size_t s_bins[10];\n  const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  //initialize shared memory\n  if (threadIdx.x < 10) {\n    s_bins[threadIdx.x] = 0;\n  }\n\n  __syncthreads();\n\n  if (idx < N) {\n    size_t i = (size_t) floor(x[idx] / 10.0);\n    atomicAdd(&s_bins[i], 1);\n  }\n\n  __syncthreads();\n\n  //copy data from shared memory to global memory\n  if (threadIdx.x < 10) {\n    bins[threadIdx.x] = s_bins[threadIdx.x];\n  }\n}",
            "// Your code here.\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[x[i] / 10]++;\n    }\n}",
            "// Your code goes here.\n    __shared__ size_t temp[10];\n    for (size_t i = 0; i < 10; i++) {\n        temp[i] = 0;\n    }\n    size_t tid = threadIdx.x;\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        for (size_t i = 0; i < 10; i++) {\n            if (x[idx] >= (i * 10) && x[idx] < (i * 10 + 10)) {\n                atomicAdd(&temp[i], 1);\n            }\n        }\n    }\n    __syncthreads();\n    if (tid < 10) {\n        atomicAdd(&bins[tid], temp[tid]);\n    }\n}",
            "/* Declare a shared memory array of size 10 for the histogram.\n       Each thread block processes a portion of the input.\n    */\n    extern __shared__ size_t shmem[];\n    for(size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n        shmem[i] = 0;\n    }\n    __syncthreads();\n\n    /* Each thread computes the index of its value in x, and increments the corresponding\n       element in the shared memory array.\n    */\n    size_t index = (size_t)(10 * x[blockIdx.x * blockDim.x + threadIdx.x] / 100);\n    atomicAdd(shmem + index, 1);\n    __syncthreads();\n\n    /* Copy the shared memory array to the output array.\n       This is done by the first thread in each thread block.\n    */\n    if(threadIdx.x == 0) {\n        for(size_t i = 0; i < 10; i++) {\n            bins[i] = shmem[i];\n        }\n    }\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (gid >= N) return;\n  int bin = x[gid] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "/* Fill in the code here */\n}",
            "size_t index = threadIdx.x;\n  size_t stride = blockDim.x;\n  while (index < N) {\n    size_t bin = size_t(x[index] / 10);\n    atomicAdd(&bins[bin], 1);\n    index += stride;\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int bin = (int)floorf(x[idx] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n    if (idx < N) {\n        int bin_idx = (int)(x[idx] / 10);\n        atomicAdd(&bins[bin_idx], 1);\n    }\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   const size_t stride = blockDim.x * gridDim.x;\n   size_t value;\n\n   for(size_t i = idx; i < N; i += stride) {\n      value = x[i] / 10.0;\n      atomicAdd(bins + value, 1);\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    int bin = x[idx] / 10;\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  size_t bin = (x[i] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        size_t bin = (size_t) x[idx] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int bin = ((int) x[threadIdx.x]) / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "__shared__ double sdata[256];\n    __shared__ double b[10];\n\n    int tid = threadIdx.x;\n    int i = blockIdx.x*blockDim.x + tid;\n    int bin;\n\n    sdata[tid] = (i < N)? (int(x[i] / 10)) : 0;\n    __syncthreads();\n\n    // Reduction\n    for (unsigned int s=1; s<blockDim.x; s*=2) {\n        if (tid % (2*s) == 0) {\n            sdata[tid] += sdata[tid + s];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        for (int i=0; i < 10; i++) {\n            b[i] = 0;\n        }\n        for (int i=0; i < 10; i++) {\n            bin = (int)sdata[i];\n            if (bin < 10) {\n                atomicAdd(&(b[bin]), 1);\n            }\n        }\n        __syncthreads();\n        for (int i=0; i < 10; i++) {\n            bins[i] = (size_t)b[i];\n        }\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    while (i < N) {\n        int bin = floor(x[i] / 10);\n        atomicAdd(&(bins[bin]), 1);\n        i += blockDim.x * gridDim.x;\n    }\n}",
            "// Initialize the bins to 0\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    // Determine the index into the bins based on the value of x\n    // (Note that this does not handle out-of-bounds values)\n    int bin = x[blockIdx.x] / 10;\n\n    // Increment the appropriate bin\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    bins[int(x[idx] / 10.0)]++;\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    int binId = (int) (x[i] / 10);\n    atomicAdd(&(bins[binId]), 1);\n  }\n}",
            "// TODO: Compute the number of values in [0, 10), [10, 20), [20, 30), etc.\n    // and store in bins.  You may assume that 0 <= x[i] <= 100.\n    // You may use `if` or integer division to determine in which range x[i]\n    // belongs.\n    // You may need to use a for-loop.\n    //\n    // Use a parallel reduction to compute the final histogram.\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    bins[static_cast<size_t>(x[idx] / 10)] += 1;\n  }\n}",
            "const size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i >= N) { return; }\n    bins[x[i] / 10] += 1;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  int idx = x[i] / 10;\n  atomicAdd(&bins[idx], 1);\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int i = index / 10;\n  int j = index % 10;\n  if (i < N) {\n    if (j == 0 && x[i] < 10) bins[0] += 1;\n    if (j == 1 && x[i] >= 10 && x[i] < 20) bins[1] += 1;\n    if (j == 2 && x[i] >= 20 && x[i] < 30) bins[2] += 1;\n    if (j == 3 && x[i] >= 30 && x[i] < 40) bins[3] += 1;\n    if (j == 4 && x[i] >= 40 && x[i] < 50) bins[4] += 1;\n    if (j == 5 && x[i] >= 50 && x[i] < 60) bins[5] += 1;\n    if (j == 6 && x[i] >= 60 && x[i] < 70) bins[6] += 1;\n    if (j == 7 && x[i] >= 70 && x[i] < 80) bins[7] += 1;\n    if (j == 8 && x[i] >= 80 && x[i] < 90) bins[8] += 1;\n    if (j == 9 && x[i] >= 90 && x[i] < 100) bins[9] += 1;\n  }\n}",
            "auto idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx >= N)\n    return;\n  auto xi = static_cast<int>(x[idx]);\n  if (xi >= 0 && xi < 100)\n    atomicAdd(&bins[xi / 10], 1);\n}",
            "// TODO:\n    // For each value `x[i]` in `x`, count it in the appropriate bin.\n    // Hint: You can use the modulo operator `%` to find the remainder.\n    // Example: `x[i] = 23` =>  `23 % 10 = 3`.\n}",
            "// TODO\n}",
            "__shared__ int nthreads;\n    __shared__ double *shared_x;\n    int tid = threadIdx.x;\n    if (tid == 0) {\n        // Initialize bins to zero\n        for (int i = 0; i < 10; i++)\n            bins[i] = 0;\n        // get the number of threads\n        nthreads = blockDim.x;\n        // allocate memory in shared memory for `x`\n        shared_x = (double *)malloc(nthreads * sizeof(double));\n    }\n    __syncthreads();\n\n    for (int i = 0; i < N; i += nthreads) {\n        // get value of x to the thread's position in the array\n        int index = tid + i;\n        // bound checking\n        if (index < N)\n            shared_x[tid] = x[index];\n        else\n            shared_x[tid] = 0;\n        __syncthreads();\n\n        // get the bins that the values belong to\n        int bin = shared_x[tid] / 10;\n        __syncthreads();\n\n        // increment the value in `bins` at `bin`\n        atomicAdd(&bins[bin], 1);\n        __syncthreads();\n    }\n}",
            "unsigned int i = threadIdx.x;\n\n  while (i < N) {\n    bins[(int)(x[i] / 10)]++;\n    i += blockDim.x;\n  }\n}",
            "__shared__ int partials[1024];\n\n  unsigned tid = threadIdx.x;\n  unsigned i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    int bin = x[i] / 10;\n    atomicAdd(&partials[bin], 1);\n  }\n\n  __syncthreads();\n\n  // accumulate bin counts using a single thread\n  if (tid == 0) {\n    for (int bin = 0; bin < 10; bin++) {\n      bins[bin] = partials[bin];\n      for (int i = 1; i < 1024 / 10; i++) {\n        bins[bin] += partials[i * 10 + bin];\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        int bin = (int)x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    // This is a race, but we are only using this for illustration\n    bins[x[i] / 10]++;\n  }\n}",
            "size_t t = hipThreadIdx_x;\n  if (t < N) {\n    // Determine the bin by integer division, then decrement.\n    // If the value is 0, decrementing will move it into the previous bin.\n    bins[x[t] / 10] -= 1;\n  }\n}",
            "// Initialize the bins to 0.\n  for (size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n    bins[i] = 0;\n  }\n\n  __syncthreads();\n\n  // Increment the appropriate bin for each value in x.\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    bins[int(x[i] / 10)]++;\n  }\n}",
            "unsigned int tIdx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tIdx < N)\n        atomicAdd(&bins[int(x[tIdx] / 10)], 1);\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) return;\n\n  double value = x[i];\n\n  size_t bin = static_cast<size_t>(value / 10);\n  if (bin >= 10) bin = 9;\n  atomicAdd(&bins[bin], 1);\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        int bin = floor(x[index] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    size_t bin = (int)(x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const int i = blockDim.x*blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  const int bin = (int)x[i] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "/*\n  * Add your code here!\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *\n  *",
            "__shared__ size_t shBins[10];\n    for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n        shBins[i] = 0;\n    }\n    __syncthreads();\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n        int bin = 10 * floor(x[i] / 10);\n        atomicAdd(shBins + bin, 1);\n    }\n    __syncthreads();\n    for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n        atomicAdd(bins + i, shBins[i]);\n    }\n}",
            "__shared__ size_t shared[10];\n  for(int i=0; i<10; i++) shared[i] = 0;\n  __syncthreads();\n\n  int tId = threadIdx.x;\n  int b = x[tId] / 10;\n  atomicAdd(&shared[b], 1);\n  __syncthreads();\n\n  for(int i=0; i<10; i++) {\n    atomicAdd(&bins[i], shared[i]);\n  }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    const size_t bin = static_cast<size_t>(x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "auto tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    size_t bin = static_cast<size_t>(x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N)\n        atomicAdd(&bins[(size_t)floor(x[tid] / 10.0)], 1);\n}",
            "int ix = threadIdx.x + blockIdx.x * blockDim.x;\n  int myBin = (int)x[ix] / 10;\n  if (ix < N && myBin >= 0 && myBin < 10) {\n    atomicAdd(&bins[myBin], 1);\n  }\n}",
            "//...\n}",
            "// TODO\n}",
            "int tid = blockDim.x*blockIdx.x + threadIdx.x;\n    if(tid >= N) return;\n\n    int bin = int(x[tid]/10.0);\n    atomicAdd(&(bins[bin]), 1);\n}",
            "size_t threadId = blockIdx.x*blockDim.x + threadIdx.x;\n    //printf(\"threadId = %d\\n\", threadId);\n\n    if(threadId < N){\n        size_t bin = floor(x[threadId]/10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "}",
            "/* TODO */\n}",
            "__shared__ size_t s[10];\n  auto tid = threadIdx.x;\n  // initialize shared memory\n  if (tid < 10) {\n    s[tid] = 0;\n  }\n  __syncthreads();\n\n  // count in shared memory\n  for (size_t i = 0; i < N; i += blockDim.x) {\n    if (tid + i < N) {\n      auto idx = ((int)x[tid + i]) / 10;\n      atomicAdd(&s[idx], 1);\n    }\n  }\n\n  // merge thread results into global memory\n  __syncthreads();\n  if (tid < 10) {\n    atomicAdd(&bins[tid], s[tid]);\n  }\n}",
            "__shared__ size_t sBins[10];\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  /* Initialize shared memory for each block to 0.\n     This only needs to be done once per block.\n     The following `if` statement will only execute for the first thread in a block.\n  */\n  if (threadIdx.x == 0) {\n    for (size_t i = 0; i < 10; ++i)\n      sBins[i] = 0;\n  }\n\n  __syncthreads();\n\n  if (i < N) {\n    int bin = x[i] / 10;\n    atomicAdd(&sBins[bin], 1);\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    for (size_t i = 0; i < 10; ++i)\n      atomicAdd(&bins[i], sBins[i]);\n  }\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) return;\n  int index = floor(x[i] / 10);\n  __atomic_fetch_add(&bins[index], 1, __ATOMIC_RELAXED);\n}",
            "// TODO\n\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[x[i] / 10] += 1;\n    }\n}",
            "__shared__ size_t count[10];\n    count[threadIdx.x] = 0;\n    for (int i = threadIdx.x; i < N; i += blockDim.x) {\n        auto j = static_cast<int>(x[i] / 10.0);\n        atomicAdd(&count[j], 1);\n    }\n    __syncthreads();\n    for (int j = 0; j < 10; ++j) {\n        atomicAdd(&bins[j], count[j]);\n    }\n}",
            "int index = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if (index < N) {\n        int bin = (int) (x[index] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = threadIdx.x;\n    if (i < N) {\n        size_t bin = x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "//TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  int bin = int(x[i] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "//TODO\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N) return;\n\n  // compute a histogram with 10 bins.\n  // the index of the bin is the integer part of x[index]/10.\n  bins[int(x[index] / 10)]++;\n}",
            "__shared__ size_t[10] binsLocal;\n  memset(binsLocal, 0, sizeof(binsLocal));\n  int tid = threadIdx.x;\n  size_t i = (blockIdx.x * blockDim.x) + tid;\n  if (i < N) {\n    size_t bin = floor(x[i]/10.0);\n    atomicAdd(&(binsLocal[bin]), 1);\n  }\n  __syncthreads();\n  // Each thread copies its local value to the shared memory, so that it can be reduced\n  if (tid < 10) {\n    size_t old = atomicAdd(&(bins[tid]), binsLocal[tid]);\n  }\n  __syncthreads();\n}",
            "// Each thread computes one histogram bin.\n  const size_t bin = blockIdx.x;\n\n  size_t count = 0;\n  for (size_t i = 0; i < N; i++)\n    if (x[i] >= bin*10 && x[i] < (bin+1)*10)\n      count++;\n\n  atomicAdd(&bins[bin], count);\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    const int xi = (int)x[i];\n    const int bin = xi/10;\n    atomicAdd(&bins[bin], 1);\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  const int bin = (int)(x[tid] / 10.0);\n  atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n  int global_idx = threadIdx.x + blockIdx.x * blockDim.x;\n  int num_ele = gridDim.x * blockDim.x;\n  if (global_idx >= N) return;\n  int tmp = x[global_idx] / 10;\n  atomicAdd(&bins[tmp], 1);\n}",
            "// Get index of current thread (0 to N)\n    int i = threadIdx.x + blockIdx.x*blockDim.x;\n\n    // Each thread i counts the number of values in [i, i+10)\n    if( i < N ){\n        int binIndex = (int)(x[i]/10);\n        atomicAdd(&(bins[binIndex]), 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    int bin = x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int index = threadIdx.x;\n  if (index >= N)\n    return;\n  size_t bin = 10 * (int) (x[index] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[i / 10], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      size_t bin = size_t(x[i]) / 10;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO: Fill in this function\n}",
            "auto idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N) return;\n\n    int bin = floor(x[idx] / 10);\n    atomicAdd(bins + bin, 1);\n}",
            "int idx = threadIdx.x;\n    double value = x[idx];\n    // TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int bin = (int) floor(x[i] / 10.0f);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N)\n        atomicAdd(bins + (x[i] / 10), 1);\n}",
            "// TODO\n}",
            "size_t t = blockDim.x * blockIdx.x + threadIdx.x;\n  if (t < N) {\n    size_t bin = floor(x[t] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "__shared__ size_t sharedBins[10];\n\n    // Index into global memory\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // Each thread computes one value of bins\n    if (index < N) {\n        int binIndex = int(floor(x[index] / 10));\n        // Add to shared memory\n        atomicAdd(&sharedBins[binIndex], 1);\n    }\n\n    // Make sure we are done with shared memory\n    __syncthreads();\n\n    // Copy shared memory to global memory\n    if (threadIdx.x < 10) {\n        bins[threadIdx.x] = sharedBins[threadIdx.x];\n    }\n}",
            "// __shared__ size_t bin_counts[10];\n    // __shared__ size_t i;\n    int bin = (int) ((*x)/10); // 0..9 inclusive\n    atomicAdd(&bins[bin], 1);\n    // __syncthreads();\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t bin;\n    if (tid < N) {\n        bin = (x[tid] / 10) % 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  // TODO: check that i is in range\n\n  // TODO: count values between [0,10), [10, 20),...\n  size_t binIndex =???;\n  atomicAdd(&(bins[binIndex]), 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (; i < N; i += stride) {\n    size_t bin = (size_t) floor(x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        size_t bin = (size_t) floor((x[i] * 10.0) / 10.0);\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "unsigned int global_thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // Use min(global_thread_id, N - 1) to avoid accessing x[N].\n  if (global_thread_id < N) {\n    unsigned int bin_idx = (int)(x[global_thread_id] / 10);\n    atomicAdd(&bins[bin_idx], 1);\n  }\n}",
            "// TODO\n}",
            "const size_t stride = gridDim.x * blockDim.x;\n    const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    for (size_t i = tid; i < N; i += stride) {\n        const size_t bin = (int) (x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) {\n    return;\n  }\n\n  const int bin = x[i] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "// Get the ID of the thread, which corresponds to the value at x[i]\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // Check whether the thread ID is within the range of the values in x.\n  if (i >= N) return;\n\n  // Put the value of the thread in the corresponding bin.\n  atomicAdd(&(bins[(int)(x[i] / 10)]), 1);\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t idx = (size_t)((x[i] / 10.0) + 0.001);\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n\n  size_t idx = (size_t)floor(x[i] / 10);\n  atomicAdd(&(bins[idx]), 1);\n}",
            "int gid = blockIdx.x*blockDim.x + threadIdx.x;\n  if (gid < N) {\n    int bin = min(10, max(0, int(x[gid]/10)));\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const int tid = threadIdx.x;\n    const int bid = blockIdx.x;\n\n    __shared__ size_t s[BLOCKSIZE];\n\n    // Initialize s, which will be used as a shared memory buffer for \n    // temporary results, to zero.\n    if(tid < 10) s[tid] = 0;\n    __syncthreads();\n\n    // Each thread will be responsible for processing multiple input\n    // values, depending on the total number of values (N) and the\n    // number of threads per block (BLOCKSIZE)\n    const int nThreads = BLOCKSIZE;\n    const int nBlocks = N / BLOCKSIZE;\n    for(int i = bid; i < nBlocks; i += gridDim.x) {\n        for(int j = tid; j < BLOCKSIZE; j += nThreads) {\n            int index = i * BLOCKSIZE + j;\n            if(index < N) {\n                int bin = x[index] / 10;\n                atomicAdd(s + bin, 1);\n            }\n        }\n    }\n\n    // Wait until all threads in this block have finished\n    __syncthreads();\n\n    // Copy s to bins\n    if(tid < 10) bins[tid] = s[tid];\n}",
            "// 1. Get thread index.\n  size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N) {\n    return;\n  }\n  // 2. Compute bin index for value in `x`.\n  double value = x[index];\n  size_t bin = (int)floor(value / 10);\n  // 3. Use atomic add to increment bin count.\n  atomicAdd(&bins[bin], 1);\n}",
            "int tId = threadIdx.x + blockIdx.x * blockDim.x;\n  if(tId >= N)\n    return;\n  int val = (int)x[tId];\n  atomicAdd(&bins[val/10], 1);\n}",
            "size_t bin = blockDim.x * blockIdx.x + threadIdx.x;\n  if(bin < N)\n    atomicAdd(&bins[bin/10], 1);\n}",
            "const int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (threadIdx < N) {\n        const int bin = (int) x[threadIdx] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    int bin = x[index] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = threadIdx.x; // thread id\n  size_t stride = blockDim.x; // number of threads\n  size_t tid = i + stride * blockIdx.x; // global thread id\n\n  // Make sure we don't go out of bounds\n  if (tid >= N)\n    return;\n\n  double n = x[tid];\n  if (n >= 0 && n < 10)\n    atomicAdd(bins, 1);\n  else if (n >= 10 && n < 20)\n    atomicAdd(bins + 1, 1);\n  else if (n >= 20 && n < 30)\n    atomicAdd(bins + 2, 1);\n  else if (n >= 30 && n < 40)\n    atomicAdd(bins + 3, 1);\n  else if (n >= 40 && n < 50)\n    atomicAdd(bins + 4, 1);\n  else if (n >= 50 && n < 60)\n    atomicAdd(bins + 5, 1);\n  else if (n >= 60 && n < 70)\n    atomicAdd(bins + 6, 1);\n  else if (n >= 70 && n < 80)\n    atomicAdd(bins + 7, 1);\n  else if (n >= 80 && n < 90)\n    atomicAdd(bins + 8, 1);\n  else if (n >= 90 && n < 100)\n    atomicAdd(bins + 9, 1);\n}",
            "extern __shared__ size_t temp[];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  if (tid < 10) {\n    temp[tid] = 0;\n  }\n  __syncthreads();\n\n  if (bid < N) {\n    size_t bin = floor(x[bid] / 10);\n    atomicAdd(&temp[bin], 1);\n  }\n  __syncthreads();\n\n  if (tid < 10) {\n    atomicAdd(&bins[tid], temp[tid]);\n  }\n}",
            "size_t tid = threadIdx.x;\n\n    __shared__ double x_s[100];\n    x_s[tid] = 0;\n\n    // For each element of x, increment the corresponding bin index in `bins`.\n    // Hint: use `atomicAdd` to avoid race conditions.\n    __syncthreads();\n}",
            "size_t i = threadIdx.x;\n    if (i < N) {\n        size_t bin = floor(x[i] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t t = blockIdx.x * blockDim.x + threadIdx.x;\n  if (t < N) {\n    bins[(size_t)(x[t] / 10)]++;\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if(idx < N) {\n        // Divide the values by 10 to get the 10 bins\n        int bin_idx = (int)(x[idx] / 10);\n        // Add 1 to the bin\n        atomicAdd(&bins[bin_idx], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < N) {\n        int bin = x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x*blockDim.x;\n  if (idx < N) {\n    int bin = x[idx] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int bin = int(x[tid] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// The kernel is initialized with at least as many threads as values in x.\n    size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N) return;\n\n    // Divide the values in x into bins according to the statement in the problem.\n    // (See: http://docs.nvidia.com/hip/index.html#introduction-hip-parallel-for-loops)\n    atomicAdd(&bins[int(x[idx] / 10)], 1);\n}",
            "// TODO\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    int bin = 10 * x[tid] / 100;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n\n  int b = int(x[i] / 10);\n  atomicAdd(&bins[b], 1);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    double i = x[tid];\n    size_t bin = i / 10;\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "const size_t index = blockIdx.x*blockDim.x + threadIdx.x;\n  const size_t stride = gridDim.x*blockDim.x;\n\n  for (size_t i=index; i < N; i += stride)\n    bins[static_cast<size_t>(x[i]/10)]++;\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) {\n    return;\n  }\n  int bin_idx = static_cast<int>(x[idx]) / 10;\n  if (bin_idx < 0 || bin_idx >= 10) {\n    return;\n  }\n  atomicAdd(&bins[bin_idx], 1);\n}",
            "// AMD HIP kernel\n}",
            "// get the index of the current thread\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // make sure thread does not go out of bounds\n    if (i >= N) return;\n\n    // initialize bins to 0\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    bins[4] = 0;\n    bins[5] = 0;\n    bins[6] = 0;\n    bins[7] = 0;\n    bins[8] = 0;\n    bins[9] = 0;\n\n    // find the corresponding bin\n    int bin = floor(x[i]/10);\n\n    // increment the bin\n    atomicAdd(&bins[bin], 1);\n}",
            "int threadID = threadIdx.x;\n    __shared__ int local[10];\n    for (int i = 0; i < 10; i++) {\n        local[i] = 0;\n    }\n\n    while (threadID < N) {\n        if (x[threadID] >= 0 && x[threadID] < 10) {\n            atomicAdd(&local[0], 1);\n        } else if (x[threadID] >= 10 && x[threadID] < 20) {\n            atomicAdd(&local[1], 1);\n        } else if (x[threadID] >= 20 && x[threadID] < 30) {\n            atomicAdd(&local[2], 1);\n        } else if (x[threadID] >= 30 && x[threadID] < 40) {\n            atomicAdd(&local[3], 1);\n        } else if (x[threadID] >= 40 && x[threadID] < 50) {\n            atomicAdd(&local[4], 1);\n        } else if (x[threadID] >= 50 && x[threadID] < 60) {\n            atomicAdd(&local[5], 1);\n        } else if (x[threadID] >= 60 && x[threadID] < 70) {\n            atomicAdd(&local[6], 1);\n        } else if (x[threadID] >= 70 && x[threadID] < 80) {\n            atomicAdd(&local[7], 1);\n        } else if (x[threadID] >= 80 && x[threadID] < 90) {\n            atomicAdd(&local[8], 1);\n        } else if (x[threadID] >= 90 && x[threadID] < 100) {\n            atomicAdd(&local[9], 1);\n        }\n        threadID += blockDim.x;\n    }\n\n    __syncthreads();\n\n    if (threadID < 10) {\n        atomicAdd(&bins[threadID], local[threadID]);\n    }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if(tid < N) {\n    size_t bin = floor(x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i < N) {\n      int bin = static_cast<int>(x[i] / 10);\n      atomicAdd(bins + bin, 1);\n   }\n}",
            "/* Your code goes here */\n}",
            "// TODO: implement kernel for binsBy10Count\n\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N)\n        atomicAdd(&bins[(int) (x[tid] / 10)], 1);\n}",
            "// Use grid stride loop\n  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = gridDim.x * blockDim.x;\n  size_t idx = 0;\n  for (size_t i = tid; i < N; i += stride) {\n    // Find the index corresponding to the bin\n    // Add an extra bin for values above 100\n    idx = (x[i] >= 100)? 10 : x[i] / 10;\n    atomicAdd(bins + idx, 1);\n  }\n}",
            "// This is the index of this thread in the kernel\n  const size_t idx = threadIdx.x;\n\n  // This is the block index in the kernel\n  const size_t bid = blockIdx.x;\n\n  // This is the total number of blocks in the kernel\n  const size_t gid = gridDim.x;\n\n  // This is the total number of threads in the kernel\n  const size_t nt = blockDim.x * gridDim.x;\n\n  for (size_t i=bid*nt + idx; i < N; i += gid * nt) {\n    const int binIdx = x[i]/10;\n    atomicAdd(bins+binIdx, 1);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N) return;\n\n    int bin = static_cast<int>(x[tid]) / 10;\n    atomicAdd(&bins[bin], 1);\n}",
            "// Each thread handles one value of x\n    auto idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    auto xVal = x[idx];\n    auto bin = (xVal / 10) % 10;\n    atomicAdd(bins + bin, 1);\n}",
            "// calculate global thread index\n    const int gid = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // only process valid threads in the range of the input\n    if(gid < N) {\n\n        // initialize thread local bin counts to zero\n        int count[10] = {0};\n\n        // each thread reads a value from x, increments the corresponding bin and stores\n        // the result in the shared memory\n        count[x[gid]/10] = atomicAdd(&bins[x[gid]/10], 1);\n    }\n}",
            "/* Compute this thread's starting position in the array x */\n    const size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n\n    /* The number of values that this block will handle is the number of elements divided by the number of blocks,\n       plus the remainder. */\n    const size_t stride = (N + gridDim.x - 1) / gridDim.x;\n\n    /* Loop over the values assigned to this thread. Compute the value of x mod 10 and use that as the bin number. */\n    size_t bin_num;\n    for (size_t i = start; i < start + stride && i < N; i++) {\n        bin_num = (size_t)x[i] % 10;\n        atomicAdd(&bins[bin_num], 1);\n    }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        int j = (int) x[i] / 10; // j is a bin index [0, 10)\n        atomicAdd(bins + j, 1);\n    }\n}",
            "size_t i = hipThreadIdx_x;\n    if (i >= N)\n        return;\n    size_t j = x[i] / 10;\n    atomicAdd(&bins[j], 1);\n}",
            "// TODO\n}",
            "unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N)\n        return;\n    // TODO: fill in\n}",
            "// Initialize bins to 0.\n  //...\n  // loop over `x` using a grid stride loop\n  //   compute the bin index: x[i] / 10.\n  //   increment the bin count using atomic increment.\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        int bin = (int) floor(x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  __shared__ int sdata[256];\n\n  // each thread loads one element from global to shared mem\n  sdata[tid] = (int) x[bid * blockDim.x + tid];\n  __syncthreads();\n\n  // each warp now has the data it needs -- count in warp using shfl\n  // warp vote\n  int sum = 0;\n  for(int d = 0; d < 32; d += 1) {\n    int vote = __ballot_sync(__activemask(), (sdata[tid] >= d * 10) && (sdata[tid] < (d + 1) * 10));\n    int count = __popc(vote);\n    if(tid % 32 == 0) {\n      sdata[tid / 32] = count;\n    }\n    __syncthreads();\n  }\n\n  if(tid % 32 == 0) {\n    bins[tid / 32] = sdata[0];\n  }\n}",
            "// compute bin index from value\n  auto computeBinIndex = [](double val) {\n    return static_cast<size_t>(val / 10);\n  };\n\n  // shared memory holds values for all threads in the block\n  extern __shared__ double temp[];\n\n  // get value at this thread's index\n  double v = x[hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x];\n\n  // compute bin index for this value\n  auto i = computeBinIndex(v);\n\n  // put value into shared memory\n  temp[hipThreadIdx_x] = v;\n\n  // let all threads in this block work together\n  // to reduce values in shared memory\n  for (size_t stride = 1; stride < hipBlockDim_x; stride *= 2) {\n    size_t index = 2 * hipThreadIdx_x + (stride - 1);\n\n    // if `index` is in bounds, compute the value for `index`\n    // and add it to the value at `hipThreadIdx_x`\n    if (index < hipBlockDim_x) {\n      auto v1 = temp[hipThreadIdx_x];\n      auto v2 = temp[index];\n      temp[hipThreadIdx_x] = v1 + v2;\n    }\n\n    // wait until all values in shared memory have been computed\n    __syncthreads();\n  }\n\n  // `v` is now the sum of the values in the block\n  // `temp[0]` contains the sum of all values in the block\n  // `i` is the bin index corresponding to the bin we want to count\n  //\n  // the only work to do is to count `temp[0]` if it is in the correct bin\n  if (i < 10) {\n    atomicAdd(&(bins[i]), temp[0]);\n  }\n}",
            "int idx = threadIdx.x;\n    int stride = blockDim.x;\n    int warpSize = 32;\n    // Avoids warp divergence\n    int warpIdx = idx & (warpSize - 1);\n    __shared__ int sharedCounts[10];\n\n    // Initialize shared memory array\n    for (int i = 0; i < 10; ++i) {\n        sharedCounts[i] = 0;\n    }\n\n    // Count the number of values in each bin\n    for (int i = idx; i < N; i += stride) {\n        if (x[i] >= 0 && x[i] < 10) {\n            atomicAdd(&sharedCounts[0], 1);\n        } else if (x[i] >= 10 && x[i] < 20) {\n            atomicAdd(&sharedCounts[1], 1);\n        } else if (x[i] >= 20 && x[i] < 30) {\n            atomicAdd(&sharedCounts[2], 1);\n        } else if (x[i] >= 30 && x[i] < 40) {\n            atomicAdd(&sharedCounts[3], 1);\n        } else if (x[i] >= 40 && x[i] < 50) {\n            atomicAdd(&sharedCounts[4], 1);\n        } else if (x[i] >= 50 && x[i] < 60) {\n            atomicAdd(&sharedCounts[5], 1);\n        } else if (x[i] >= 60 && x[i] < 70) {\n            atomicAdd(&sharedCounts[6], 1);\n        } else if (x[i] >= 70 && x[i] < 80) {\n            atomicAdd(&sharedCounts[7], 1);\n        } else if (x[i] >= 80 && x[i] < 90) {\n            atomicAdd(&sharedCounts[8], 1);\n        } else if (x[i] >= 90 && x[i] < 100) {\n            atomicAdd(&sharedCounts[9], 1);\n        }\n    }\n\n    // Wait for all threads to reach this point\n    __syncthreads();\n\n    // Reduce the block into a single value per bin.\n    for (unsigned int s = blockDim.x / 2; s > warpSize; s >>= 1) {\n        if (idx < s) {\n            for (int i = 0; i < 10; ++i) {\n                sharedCounts[i] += sharedCounts[i + s];\n            }\n        }\n        __syncthreads();\n    }\n\n    // Reduce warp\n    for (unsigned int s = warpSize / 2; s > 0; s >>= 1) {\n        if (warpIdx < s) {\n            for (int i = 0; i < 10; ++i) {\n                sharedCounts[i] += sharedCounts[i + s];\n            }\n        }\n    }\n\n    // Write result for this block to global memory\n    if (idx == 0) {\n        for (int i = 0; i < 10; ++i) {\n            bins[i] = sharedCounts[i];\n        }\n    }\n}",
            "size_t tid = threadIdx.x;\n    if(tid < N) {\n        int bin = min(9, max(0, (int) (x[tid] / 10)));\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n    if (tid < N) {\n        size_t bin = x[tid]/10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = threadIdx.x;\n    int gid = tid + blockIdx.x * blockDim.x;\n    if (gid >= N) return;\n    size_t idx = 0;\n    while (gid >= 10) {\n        idx++;\n        gid -= 10;\n    }\n    atomicAdd(&bins[idx], 1);\n}",
            "size_t threadID = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n  if (threadID >= N)\n    return;\n  size_t xValue = size_t(x[threadID]);\n  if (xValue > 100)\n    return;\n  atomicAdd(&bins[xValue / 10], 1);\n  for (size_t i = threadID + stride; i < N; i += stride) {\n    xValue = size_t(x[i]);\n    if (xValue > 100)\n      continue;\n    atomicAdd(&bins[xValue / 10], 1);\n  }\n}",
            "int index = threadIdx.x;\n\n  for (int i = index; i < N; i += blockDim.x) {\n    bins[floor(x[i] / 10.0)]++;\n  }\n}",
            "// Each thread counts one element.\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N)\n    return;\n  size_t bin = (int)x[i] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N) return;\n    double xi = x[tid];\n    bins[xi/10] += 1;\n}",
            "size_t thread_idx = hipThreadIdx_x;\n\n    // Initialize `bins` to all zeros\n    for (size_t i = thread_idx; i < 10; i += hipBlockDim_x)\n        bins[i] = 0;\n\n    __syncthreads();\n\n    // This loop is executed only once, so thread_idx == 0\n    for (size_t i = thread_idx; i < N; i += hipBlockDim_x) {\n        size_t bin = x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t index = blockIdx.x*blockDim.x + threadIdx.x;\n\n  while (index < N) {\n    size_t bin = (size_t) x[index] / 10;\n    atomicAdd(&bins[bin], 1);\n    index += blockDim.x * gridDim.x;\n  }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[(x[idx] / 10)], 1);\n  }\n}",
            "__shared__ double cache[1024];\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  size_t i = bid * blockDim.x + tid;\n  if (i < N) cache[tid] = (int) floor(x[i] / 10);\n  __syncthreads();\n  if (tid < 10) {\n    size_t sum = 0;\n    for (int j = 0; j < N / blockDim.x + 1; j++)\n      if (j * blockDim.x + tid < N) sum += cache[j * blockDim.x + tid];\n    atomicAdd(&bins[tid], sum);\n  }\n}",
            "const size_t tid = hipThreadIdx_x;\n  if (tid >= N) {\n    return;\n  }\n  double val = x[tid];\n  int bin = floor(val / 10.0);\n  if (bin < 0 || bin > 9) {\n    return;\n  }\n  atomicAdd(&bins[bin], 1);\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) {\n        return;\n    }\n\n    size_t bin = floor(x[idx] / 10.0);\n    atomicAdd(&bins[bin], 1);\n}",
            "// Declare and set a private variable for each bin, initialized to zero.\n  size_t bin0=0, bin1=0, bin2=0, bin3=0, bin4=0, bin5=0, bin6=0, bin7=0, bin8=0, bin9=0;\n\n  // Get the global thread ID.\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Make sure we do not go out of bounds.\n  if (gid < N) {\n\n    // Get the bin index.\n    size_t bin_index = floor(x[gid] / 10.0);\n\n    // Increment the private variable corresponding to bin_index.\n    if (bin_index == 0) { ++bin0; }\n    if (bin_index == 1) { ++bin1; }\n    if (bin_index == 2) { ++bin2; }\n    if (bin_index == 3) { ++bin3; }\n    if (bin_index == 4) { ++bin4; }\n    if (bin_index == 5) { ++bin5; }\n    if (bin_index == 6) { ++bin6; }\n    if (bin_index == 7) { ++bin7; }\n    if (bin_index == 8) { ++bin8; }\n    if (bin_index == 9) { ++bin9; }\n  }\n\n  // Compute and store the sum of all private variables.\n  size_t sum = bin0 + bin1 + bin2 + bin3 + bin4 + bin5 + bin6 + bin7 + bin8 + bin9;\n\n  // Write back to global memory.\n  if (threadIdx.x == 0) {\n    bins[0] = bin0;\n    bins[1] = bin1;\n    bins[2] = bin2;\n    bins[3] = bin3;\n    bins[4] = bin4;\n    bins[5] = bin5;\n    bins[6] = bin6;\n    bins[7] = bin7;\n    bins[8] = bin8;\n    bins[9] = bin9;\n  }\n}",
            "/* Initialize the bins. */\n    for (int i = 0; i < 10; i++)\n        bins[i] = 0;\n\n    /* Loop over all values in `x` that this thread has access to. */\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n         i < N;\n         i += blockDim.x * gridDim.x) {\n        /* Count each value into a bin. */\n        int bin = int(x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n  if(i < N) {\n    size_t bin = size_t(x[i]/10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// your code here\n}",
            "const size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N) return;\n\n    int bin = floor((x[idx] / 10.0));\n    if (bin < 0) bin = 0;\n    else if (bin >= 10) bin = 9;\n\n    atomicAdd(&bins[bin], 1);\n}",
            "// use an atomic operation to increment the appropriate bin for this thread\n  const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N)\n    atomicAdd(bins + (x[i] / 10), 1);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (int rank = 0; rank < worldSize; rank++) {\n    // TODO: split input data into equal chunks and send to each rank\n\n    // TODO: send the data to the appropriate rank\n\n    // TODO: receive the data from the appropriate rank\n\n    // TODO: compute the counts in parallel\n  }\n}",
            "// TODO: Your code goes here.\n    MPI_Bcast(x.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (0 == MPI_COMM_WORLD.rank) {\n        for (auto& bin : bins) {\n            bin++;\n        }\n    }\n}",
            "// TODO: write this\n}",
            "// TODO\n}",
            "bins.fill(0);\n  int n = x.size();\n  // TODO: \n}",
            "auto const n = x.size();\n\n    // Step 1: count the elements in each bin on this rank\n    std::array<size_t, 10> localBins;\n    localBins.fill(0);\n    for (double const elem : x) {\n        localBins[static_cast<size_t>(elem / 10)] += 1;\n    }\n\n    // Step 2: broadcast the counts to the other ranks\n    std::array<size_t, 10> globalBins;\n    globalBins.fill(0);\n    MPI_Reduce(localBins.data(), globalBins.data(), localBins.size(),\n        MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Step 3: copy back into bins if this is the root\n    if (MPI_COMM_WORLD.Get_rank() == 0) {\n        bins = globalBins;\n    }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n  // Hints:\n  // - To check whether a value belongs to a certain bin, use `v % 10`.\n  // - To compute the sum of all elements in a range of `std::vector`, use\n  //   `std::accumulate`.\n}",
            "int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO:\n  // Count the number of values in each [0,10) range on this process\n  // Store the counts in `bins`\n\n  // TODO:\n  // Use MPI_Reduce to collect the counts into `bins` on rank 0\n\n}",
            "// TODO\n}",
            "/* TODO: Fill the function with your implementation */\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    std::array<size_t, 10> local_bins{};\n    for (double e: x) {\n      auto bin = std::min(static_cast<int>(e / 10), 9);\n      ++local_bins[bin];\n    }\n\n    for (int r = 1; r < size; ++r) {\n      MPI_Recv(local_bins.data(), local_bins.size(), MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int i = 0; i < 10; ++i) {\n        bins[i] += local_bins[i];\n      }\n    }\n  } else {\n    std::array<size_t, 10> local_bins{};\n    for (double e: x) {\n      auto bin = std::min(static_cast<int>(e / 10), 9);\n      ++local_bins[bin];\n    }\n    MPI_Send(local_bins.data(), local_bins.size(), MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// your code here\n}",
            "// Replace this comment with your implementation\n}",
            "int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  // TODO: fill the bins vector\n  // hint: use std::upper_bound to find the index of a value in a sorted array\n  // hint: use std::array to access the elements of bins by index\n\n  if (my_rank == 0) {\n    // reduce the results from all ranks\n    std::vector<size_t> counts_from_all_ranks(10 * MPI_Size);\n    std::copy(bins.begin(), bins.end(), counts_from_all_ranks.begin());\n\n    int root_rank = 0;\n    MPI_Reduce(&counts_from_all_ranks[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, root_rank, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(MPI_IN_PLACE, MPI_IN_PLACE, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n}",
            "/* TODO */\n}",
            "auto size = x.size();\n    auto sizePerRank = (size + MPI::COMM_WORLD.Get_size() - 1) / MPI::COMM_WORLD.Get_size();\n    auto rank = MPI::COMM_WORLD.Get_rank();\n    auto rankOffset = rank * sizePerRank;\n    auto rankCount = std::min(sizePerRank, size - rankOffset);\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    std::vector<size_t> rankBins(10, 0);\n    for (int i = 0; i < rankCount; i++) {\n        auto bin = x[rankOffset + i] / 10;\n        rankBins[bin]++;\n    }\n\n    MPI_Reduce(rankBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n    int size,rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    //MPI_Status status;\n    //MPI_Request request;\n\n    std::array<size_t, 10> localBins;\n    localBins.fill(0);\n\n    std::vector<double>::const_iterator first = x.begin();\n    std::vector<double>::const_iterator last = x.end();\n\n    // size_t i = 0;\n    int offset = 0;\n    while(first!= last) {\n        if(*first < 10) {\n            localBins[0] += 1;\n        }\n        else if(*first >= 10 && *first < 20) {\n            localBins[1] += 1;\n        }\n        else if(*first >= 20 && *first < 30) {\n            localBins[2] += 1;\n        }\n        else if(*first >= 30 && *first < 40) {\n            localBins[3] += 1;\n        }\n        else if(*first >= 40 && *first < 50) {\n            localBins[4] += 1;\n        }\n        else if(*first >= 50 && *first < 60) {\n            localBins[5] += 1;\n        }\n        else if(*first >= 60 && *first < 70) {\n            localBins[6] += 1;\n        }\n        else if(*first >= 70 && *first < 80) {\n            localBins[7] += 1;\n        }\n        else if(*first >= 80 && *first < 90) {\n            localBins[8] += 1;\n        }\n        else {\n            localBins[9] += 1;\n        }\n        first++;\n        offset++;\n    }\n    // printf(\"rank %d: offset = %d\\n\",rank,offset);\n\n    int total = offset;\n    MPI_Reduce(&offset, &total, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // printf(\"rank %d: total = %d\\n\",rank,total);\n\n    // if(rank == 0) {\n    //     printf(\"rank %d: total = %d\\n\",rank,total);\n    // }\n\n    if(rank == 0) {\n        bins.fill(0);\n        int i;\n        for(i = 0; i < size; i++) {\n            MPI_Status status;\n            int msg;\n            MPI_Recv(&msg, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            // printf(\"rank %d: offset = %d\\n\",i,msg);\n            // int offset = msg;\n            int j;\n            for(j = 0; j < 10; j++) {\n                bins[j] += localBins[j];\n            }\n        }\n        // printf(\"rank 0: bins = [%d, %d, %d, %d, %d, %d, %d, %d, %d, %d]\\n\",\n        //         bins[0],bins[1],bins[2],bins[3],bins[4],bins[5],\n        //         bins[6],bins[7],bins[8],bins[9]);\n        // printf(\"rank 0: total = %d\\n\",total);\n    }\n    else {\n        MPI_Send(&offset, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Replace this with your solution.\n\n  // Useful functions:\n  // int MPI::COMM_WORLD.Get_rank() const;\n  // int MPI::COMM_WORLD.Get_size() const;\n  // int MPI::COMM_WORLD.Rank() const;\n  // int MPI::COMM_WORLD.Size() const;\n  // void MPI::COMM_WORLD.Barrier();\n\n  // int MPI::COMM_WORLD.Send(const void*, int, MPI::Datatype, int, int, MPI::Comm);\n  // int MPI::COMM_WORLD.Send(const void*, int, MPI::Datatype, int, int);\n  // int MPI::COMM_WORLD.Recv(void*, int, MPI::Datatype, int, int, MPI::Comm);\n  // int MPI::COMM_WORLD.Recv(void*, int, MPI::Datatype, int, int);\n  // int MPI::COMM_WORLD.Bcast(void*, int, MPI::Datatype, int);\n  // int MPI::COMM_WORLD.Bcast(void*, int, MPI::Datatype);\n  // int MPI::COMM_WORLD.Gather(const void*, int, MPI::Datatype, void*, int, MPI::Datatype, int);\n  // int MPI::COMM_WORLD.Gather(const void*, int, MPI::Datatype, void*, int, MPI::Datatype);\n  // int MPI::COMM_WORLD.Scatter(const void*, int, MPI::Datatype, void*, int, MPI::Datatype, int);\n  // int MPI::COMM_WORLD.Scatter(const void*, int, MPI::Datatype, void*, int, MPI::Datatype);\n  // int MPI::COMM_WORLD.Allgather(const void*, int, MPI::Datatype, void*, int, MPI::Datatype);\n  // int MPI::COMM_WORLD.Allgather(const void*, int, MPI::Datatype, void*);\n  // int MPI::COMM_WORLD.Allgatherv(const void*, int, MPI::Datatype, void*, const int*, const int*, MPI::Datatype, int);\n  // int MPI::COMM_WORLD.Allgatherv(const void*, int, MPI::Datatype, void*, const int*, const int*);\n  // int MPI::COMM_WORLD.Alltoall(const void*, int, MPI::Datatype, void*, int, MPI::Datatype, int);\n  // int MPI::COMM_WORLD.Alltoall(const void*, int, MPI::Datatype, void*, int, MPI::Datatype);\n  // int MPI::COMM_WORLD.Alltoallv(const void*, const int*, const int*, MPI::Datatype, void*, const int*, const int*, MPI::Datatype, int);\n  // int MPI::COMM_WORLD.Alltoallv(const void*, const int*, const int*, MPI::Datatype, void*, const int*, const int*);\n  // int MPI::COMM_WORLD.Reduce(const void*, void*, int, MPI::Datatype, MPI::Op, int, MPI::Comm);\n  // int MPI::COMM_WORLD.Reduce(const void*, void*, int, MPI::Datatype, MPI::Op, int);\n  // int MPI::COMM_WORLD.Reduce(const void*, void*, int, MPI::Datatype, MPI::Op);\n  // int MPI::COMM_WORLD.Allreduce(const void*, void*, int, MPI::Datatype, MPI::Op, MPI::Comm);\n  // int MPI::COMM_WORLD.Allreduce(const void*, void*, int, MPI::Datatype, MPI::Op);\n  // int MPI::COMM_WORLD.Reduce_scatter(const void*, void*, const int*, MPI::Datatype, MPI::Op, MPI::Comm);\n  // int MPI::COMM_WORLD.Reduce_scatter(const void*, void*, const int*,",
            "// Your code goes here.\n\n  // std::array<size_t, 10> bins = {0};\n\n  // bins[static_cast<size_t>(x / 10.0)]++;\n\n  // std::cout << \"bins: \" << bins << std::endl;\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO\n}",
            "}",
            "// Your code here\n}",
            "// TODO\n}",
            "// TODO: complete this function\n}",
            "size_t size = x.size();\n    // TODO\n}",
            "// TODO\n}",
            "auto const localSize = x.size();\n\n  // TODO\n\n}",
            "// TODO: Your code here\n\n}",
            "}",
            "if (bins.size()!= 10) {\n    throw std::runtime_error(\"binsBy10Count expects 10 bins\");\n  }\n\n  // This vector will store the number of items in each bin\n  std::vector<size_t> bin_counts(10, 0);\n\n  // TODO: Count the number of items in each bin using bin_counts and x\n  // Hint: use `std::count_if` to count the number of elements that meet\n  //       a condition. For example,\n  //\n  //     count_if(x.begin(), x.end(), [](double val){ return val > 3; })\n  //\n  // counts how many elements in `x` are greater than 3.\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_per_proc = x.size()/size;\n  int remainder = x.size() % size;\n  int start = rank * num_per_proc;\n  int end = start + num_per_proc;\n  if (rank == 0) {\n    end += remainder;\n  }\n  if (rank == size-1) {\n    end = x.size();\n  }\n\n  //for (int i = start; i < end; i++) {\n  //  if (x[i] >= 10 * rank && x[i] < 10 * (rank + 1)) {\n  //    bin_counts[rank]++;\n  //  }\n  //}\n  std::vector<double> my_x;\n  for (int i = start; i < end; i++) {\n    my_x.push_back(x[i]);\n  }\n  bin_counts[rank] = std::count_if(my_x.begin(), my_x.end(),\n    [](double val){ return val >= 10 * rank && val < 10 * (rank + 1); });\n\n  // TODO: Combine the counts into bins on rank 0 using MPI.\n  // Hint: use `MPI_Reduce` to send the bin counts from all ranks to rank 0\n  //       (which will be the first rank)\n  // Hint: use `MPI_REDUCE_SUM` as the operation to sum the counts\n  if (rank == 0) {\n    std::array<int, 10> counts = {0};\n    MPI_Reduce(bin_counts.data(), counts.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < 10; i++) {\n      bins[i] = counts[i];\n    }\n  }\n}",
            "// TODO: add your code here\n    //...\n}",
            "// Your code goes here.\n}",
            "/* Your code goes here */\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  // (Hint: Use `std::partition` and `std::distance`)\n}",
            "// YOUR CODE HERE\n}",
            "int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  int n_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n  int bins_per_rank = 10 / n_ranks;\n  int bins_on_last_rank = 10 - bins_per_rank * (n_ranks - 1);\n  int my_start = bins_per_rank * my_rank;\n  int my_end = my_start + (my_rank == n_ranks - 1? bins_on_last_rank : bins_per_rank);\n\n  // Initialize local bins to zeros\n  std::array<size_t, 10> local_bins = {};\n\n  // Count bins in local copy\n  for (double x_i : x) {\n    if (x_i >= my_start && x_i < my_end) {\n      local_bins[x_i / 10]++;\n    }\n  }\n\n  // Reduce bins across all ranks\n  std::vector<int> counts(10);\n  MPI_Reduce(local_bins.data(), counts.data(), counts.size(),\n             MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Copy result to bins array\n  for (int i = 0; i < counts.size(); ++i) {\n    bins[i] = counts[i];\n  }\n}",
            "// Your code goes here.\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    auto const local_x = x;\n\n    for (auto &val : local_x) {\n        int bin = static_cast<int>(val / 10);\n        MPI_Request request;\n        MPI_Isend(&bin, 1, MPI_INT, 0, bin, MPI_COMM_WORLD, &request);\n        MPI_Wait(&request, MPI_STATUS_IGNORE);\n    }\n\n    if (rank == 0) {\n        for (auto &bin : bins) {\n            MPI_Request request;\n            MPI_Irecv(&bin, 1, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &request);\n            MPI_Wait(&request, MPI_STATUS_IGNORE);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "}",
            "// CODE HERE\n}",
            "// your code here\n}",
            "// You need to compute bins on rank 0 only\n    MPI_Reduce(&bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Replace this with your code\n    if (x.size() == 0) {\n        return;\n    }\n\n    for (auto i : x) {\n        if (i >= 0 && i < 10) {\n            bins[0]++;\n        } else if (i >= 10 && i < 20) {\n            bins[1]++;\n        } else if (i >= 20 && i < 30) {\n            bins[2]++;\n        } else if (i >= 30 && i < 40) {\n            bins[3]++;\n        } else if (i >= 40 && i < 50) {\n            bins[4]++;\n        } else if (i >= 50 && i < 60) {\n            bins[5]++;\n        } else if (i >= 60 && i < 70) {\n            bins[6]++;\n        } else if (i >= 70 && i < 80) {\n            bins[7]++;\n        } else if (i >= 80 && i < 90) {\n            bins[8]++;\n        } else if (i >= 90 && i < 100) {\n            bins[9]++;\n        }\n    }\n\n    if (bins[0] == 0) {\n        bins[0] = 0;\n    }\n    if (bins[1] == 0) {\n        bins[1] = 0;\n    }\n    if (bins[2] == 0) {\n        bins[2] = 0;\n    }\n    if (bins[3] == 0) {\n        bins[3] = 0;\n    }\n    if (bins[4] == 0) {\n        bins[4] = 0;\n    }\n    if (bins[5] == 0) {\n        bins[5] = 0;\n    }\n    if (bins[6] == 0) {\n        bins[6] = 0;\n    }\n    if (bins[7] == 0) {\n        bins[7] = 0;\n    }\n    if (bins[8] == 0) {\n        bins[8] = 0;\n    }\n    if (bins[9] == 0) {\n        bins[9] = 0;\n    }\n}",
            "// This function should not modify bins.\n  // Use MPI to divide the work.\n  // Rank 0 is responsible for initializing the array bins.\n  // Don't use global variables.\n  // You're allowed to use `std::array<double, 10> const&` instead of\n  // `std::array<double, 10> &bins` as a parameter, but don't.\n  // You're also allowed to use `std::vector<double> const& x` instead of\n  // `std::vector<double> const& x` as a parameter, but don't.\n  // Don't use functions outside std, such as atoi.\n  // Don't use loops or ifs.\n  // You should use MPI_Reduce instead of MPI_Allreduce.\n  // Don't use `bins[bin] += 1` or `bins[bin] = bins[bin] + 1`.\n  // You should use `bins[bin] = MPI_SUM(1, bins[bin])` instead of `bins[bin] =\n  // bins[bin] + 1` and `bins[bin] += 1`.\n  // You should use `bins[bin] = MPI_SUM(0, bins[bin])` instead of `bins[bin] =\n  // 0` and `bins[bin] += bins[bin]` (note the use of `bins[bin]`).\n  // You should use `bins[bin] = MPI_MAX(0, bins[bin])` instead of `bins[bin] =\n  // 0` and `bins[bin] = std::max(bins[bin], 0)`.\n  // You should use `MPI_REDUCE_SCATTER_BLOCK` instead of `MPI_Gather`.\n  // You should use `MPI_Reduce` instead of `MPI_Scan`.\n  // Don't use `MPI_Bcast` or `MPI_Gather` or `MPI_Bcast`.\n  // Don't use `MPI_Scatter` or `MPI_Scatterv`.\n  // Don't use `MPI_Exscan`.\n  // Don't use `MPI_Reduce_scatter_block`\n  // Don't use `MPI_Reduce_scatter`\n  // Don't use `MPI_Reduce`\n  // Don't use `MPI_Scan`\n  // Don't use `MPI_Reduce_local`\n}",
            "bins = {};\n\n    // Your code goes here.\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  // TODO: Your code here!\n}",
            "// TODO\n}",
            "const size_t n = x.size();\n  // TODO: implement this function\n}",
            "// Put your code here\n}",
            "bins.fill(0);\n\n    // TODO\n\n}",
            "// TODO\n}",
            "if (x.size() < 1) {\n    throw std::invalid_argument(\"vector must have length at least 1\");\n  }\n\n  // your code goes here!\n\n  MPI_Bcast(&bins, 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n}",
            "/* Your solution goes here  */\n\n}",
            "// TODO \n    const int num_ranks = 1;\n    int my_rank = 0;\n    int root = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    int bin_size = x.size() / num_ranks;\n    std::vector<double> my_bin(bin_size);\n\n    for (int i = 0; i < bin_size; i++){\n        my_bin[i] = x[i];\n    }\n\n    MPI_Reduce(&my_bin[0], &bins[0], bin_size, MPI_INT, MPI_SUM, root, MPI_COMM_WORLD);\n}",
            "/* TODO */\n}",
            "size_t const rank = MPI::COMM_WORLD.Get_rank();\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  for (double value : x) {\n    int bucket = (value / 10);\n    MPI::COMM_WORLD.Send(&bucket, 1, MPI::INT, 0, 1);\n  }\n\n  if (rank == 0) {\n    MPI::COMM_WORLD.Recv(&bins[0], 10, MPI::INT, MPI::ANY_SOURCE, 1,\n                         MPI::STATUS_IGNORE);\n  }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<size_t> bins_v(bins.size());\n    std::vector<double> x_rank(x.size());\n\n    if (rank == 0) {\n        std::copy(x.begin(), x.end(), x_rank.begin());\n        bins_v = bins;\n    }\n\n    MPI_Bcast(x_rank.data(), x_rank.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(bins_v.data(), bins_v.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < bins.size(); ++i) {\n        int lower = i * 10;\n        int upper = lower + 10;\n        for (int j = 0; j < x.size(); ++j) {\n            if (x_rank[j] >= lower && x_rank[j] < upper) {\n                bins_v[i] += 1;\n            }\n        }\n    }\n\n    if (rank == 0) {\n        bins = bins_v;\n    }\n\n}",
            "// TODO\n}",
            "// Your code goes here!\n  MPI_Reduce(&x, &bins, 10, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: compute the result\n\n}",
            "int worldSize, worldRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n    //...\n}",
            "// YOUR CODE HERE\n\n}",
            "// TODO: implement\n}",
            "// TODO: add the rest of the code to finish the function.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size() / size;\n  std::vector<size_t> bins_rank;\n  for (size_t i = 0; i < 10; i++)\n    bins_rank.push_back(0);\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] >= 10 * (rank) && x[i] < 10 * (rank + 1))\n      bins_rank[rank]++;\n  }\n  MPI_Gather(bins_rank.data(), 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this\n    MPI_Bcast(&bins, 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "/* your code here */\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    // Your code here\n}",
            "// Your code goes here!\n\n}",
            "// TODO\n}",
            "// your code here\n}",
            "// Your code here.\n}",
            "const int rank = MPI::COMM_WORLD.Get_rank();\n  const int nproc = MPI::COMM_WORLD.Get_size();\n\n  /* Your code goes here */\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::array<size_t, 10> local_bins;\n  if (rank == 0) {\n    // Rank 0 can see all of x. Copy it locally.\n    local_bins = bins;\n  }\n\n  // Count the bins locally.\n  for (auto val : x) {\n    int bin = val / 10;\n    // Check that the value is in the range.\n    if (0 <= bin && bin < 10) {\n      local_bins[bin]++;\n    }\n  }\n\n  // Send the local bins to rank 0.\n  MPI_Gather(local_bins.data(), 10, MPI_UNSIGNED_LONG,\n             bins.data(), 10, MPI_UNSIGNED_LONG,\n             0, MPI_COMM_WORLD);\n}",
            "// TODO: Replace this line with your code\n  throw std::runtime_error(\"binsBy10Count not implemented\");\n}",
            "MPI_Datatype mydatatype;\n    MPI_Type_vector(1, 10, 1, MPI_DOUBLE, &mydatatype);\n    MPI_Type_commit(&mydatatype);\n    std::array<size_t, 10> localBins;\n    for (size_t i = 0; i < 10; i++) {\n        localBins[i] = 0;\n    }\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 10.0) {\n            localBins[0]++;\n        } else if (x[i] >= 10.0 && x[i] < 20.0) {\n            localBins[1]++;\n        } else if (x[i] >= 20.0 && x[i] < 30.0) {\n            localBins[2]++;\n        } else if (x[i] >= 30.0 && x[i] < 40.0) {\n            localBins[3]++;\n        } else if (x[i] >= 40.0 && x[i] < 50.0) {\n            localBins[4]++;\n        } else if (x[i] >= 50.0 && x[i] < 60.0) {\n            localBins[5]++;\n        } else if (x[i] >= 60.0 && x[i] < 70.0) {\n            localBins[6]++;\n        } else if (x[i] >= 70.0 && x[i] < 80.0) {\n            localBins[7]++;\n        } else if (x[i] >= 80.0 && x[i] < 90.0) {\n            localBins[8]++;\n        } else if (x[i] >= 90.0 && x[i] < 100.0) {\n            localBins[9]++;\n        }\n    }\n    std::array<size_t, 10> totalBins;\n    if (MPI_Rank == 0) {\n        MPI_Reduce(localBins.data(), totalBins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        bins = totalBins;\n    } else {\n        MPI_Reduce(localBins.data(), NULL, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    MPI_Type_free(&mydatatype);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int bin;\n  size_t count = 0;\n\n  for(auto i: x){\n    bin = (int)(i/10);\n    count = count + bin;\n  }\n  MPI_Reduce(&count, &bins[0], 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: Complete this function\n}",
            "auto const n = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  int n_per_proc = n / nprocs;\n  int n_remainder = n % nprocs;\n  int start, end;\n  if (rank < n_remainder) {\n    start = rank * (n_per_proc + 1);\n    end = (rank + 1) * (n_per_proc + 1);\n  }\n  else {\n    start = rank * n_per_proc + n_remainder;\n    end = (rank + 1) * n_per_proc + n_remainder;\n  }\n\n  std::vector<double> local_x(x.begin() + start, x.begin() + end);\n\n  auto local_bins = std::array<size_t, 10>();\n\n  for (auto x_i : local_x) {\n    if (x_i < 10) {\n      local_bins[0] += 1;\n    }\n    else if (x_i < 20) {\n      local_bins[1] += 1;\n    }\n    else if (x_i < 30) {\n      local_bins[2] += 1;\n    }\n    else if (x_i < 40) {\n      local_bins[3] += 1;\n    }\n    else if (x_i < 50) {\n      local_bins[4] += 1;\n    }\n    else if (x_i < 60) {\n      local_bins[5] += 1;\n    }\n    else if (x_i < 70) {\n      local_bins[6] += 1;\n    }\n    else if (x_i < 80) {\n      local_bins[7] += 1;\n    }\n    else if (x_i < 90) {\n      local_bins[8] += 1;\n    }\n    else {\n      local_bins[9] += 1;\n    }\n  }\n\n  std::array<int, 10> recv_counts;\n  int offset = 0;\n  for (auto r = 0; r < nprocs; r++) {\n    if (r < n_remainder) {\n      recv_counts[r] = n_per_proc + 1;\n      offset += 1;\n    }\n    else {\n      recv_counts[r] = n_per_proc;\n    }\n  }\n  std::array<int, 10> displs;\n  displs[0] = 0;\n  for (auto r = 1; r < nprocs; r++) {\n    displs[r] = displs[r - 1] + recv_counts[r - 1];\n  }\n\n  if (rank == 0) {\n    std::vector<size_t> recv_bins(10 * nprocs);\n    MPI_Gatherv(local_bins.data(), 10, MPI_LONG,\n                recv_bins.data(), recv_counts.data(), displs.data(),\n                MPI_LONG, 0, MPI_COMM_WORLD);\n    bins = std::array<size_t, 10>();\n    for (auto r = 0; r < nprocs; r++) {\n      for (auto i = 0; i < 10; i++) {\n        bins[i] += recv_bins[r * 10 + i];\n      }\n    }\n  }\n  else {\n    MPI_Gatherv(local_bins.data(), 10, MPI_LONG,\n                nullptr, recv_counts.data(), displs.data(),\n                MPI_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "std::array<size_t, 10> local_bins{};\n  for (auto i = 0; i < x.size(); i++) {\n    auto const idx = static_cast<int>(x[i] / 10);\n    local_bins.at(idx) += 1;\n  }\n\n  MPI_Reduce(&local_bins, &bins, 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "// YOUR CODE HERE\n}",
            "bins = {0};\n    for (auto const& v : x) {\n        auto bin = std::min(static_cast<size_t>(std::floor(v / 10)), 9);\n        bins[bin]++;\n    }\n}",
            "if (MPI_COMM_WORLD == nullptr) {\n    throw std::runtime_error(\"MPI_COMM_WORLD is null. Did you call MPI_Init?\");\n  }\n\n  int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  std::array<size_t, 10> loc_bins;\n  std::fill(loc_bins.begin(), loc_bins.end(), 0);\n\n  for (auto const& num : x) {\n    int bucket = int(num / 10);\n    if (bucket >= 0 && bucket < 10) {\n      ++loc_bins[bucket];\n    }\n  }\n\n  if (rank == 0) {\n    MPI_Reduce(&loc_bins, &bins, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(MPI_IN_PLACE, &loc_bins, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "/* your code here */\n}",
            "// TODO: Implement this function\n}",
            "if (MPI::COMM_WORLD.Get_size() > 1) {\n    // TODO: parallel code here\n  }\n  else {\n    // TODO: single-process code here\n  }\n}",
            "const size_t n = x.size();\n  const size_t nperrank = n / MPI::COMM_WORLD.Get_size();\n\n  // Your code here\n}",
            "// TODO: Your code here!\n  size_t size = x.size();\n  bins = std::array<size_t, 10>({0});\n  std::vector<size_t> bins_rank;\n\n  int rank, num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  MPI_Bcast(&size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // if not rank 0, receive all x from rank 0\n  if (rank!= 0) {\n    std::vector<double> x_rank;\n    x_rank.resize(size);\n\n    MPI_Bcast(&x_rank[0], size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < x_rank.size(); i++) {\n      int bin = x_rank[i] / 10;\n      bins_rank.push_back(bin);\n    }\n  } else { // rank 0\n    for (int i = 0; i < x.size(); i++) {\n      int bin = x[i] / 10;\n      bins_rank.push_back(bin);\n    }\n  }\n\n  int bins_size = bins_rank.size();\n  int size_per_rank = bins_size / num_ranks;\n  int remainder = bins_size % num_ranks;\n  int start = rank * size_per_rank;\n  int end = start + size_per_rank;\n  if (rank == num_ranks - 1) {\n    end += remainder;\n  }\n\n  for (int i = start; i < end; i++) {\n    bins[bins_rank[i]]++;\n  }\n\n  MPI_Reduce(\n      &bins[0],\n      &bins[0],\n      bins.size(),\n      MPI_UNSIGNED_LONG,\n      MPI_SUM,\n      0,\n      MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  std::vector<size_t> sendcounts(size);\n  std::vector<size_t> displs(size);\n\n  size_t n = x.size();\n  size_t chunk = (n + size - 1) / size; // ceil(n/size)\n  std::vector<double> my_x(chunk);\n\n  // calculate the start and end indices of each rank's portion of x\n  size_t my_start = rank*chunk;\n  size_t my_end = my_start + chunk;\n\n  // copy a portion of x to my_x\n  for(size_t i = 0; i < chunk; i++){\n    if (my_start + i < n){\n      my_x[i] = x[my_start + i];\n    }\n  }\n\n  // calculate the number of values to send to each rank\n  // sendcounts[rank] = my_x.size();\n  for(size_t i = 0; i < chunk; i++){\n    if (my_start + i < n){\n      sendcounts[rank] += 1;\n    }\n  }\n\n  // calculate the displacement of each rank's portion of my_x\n  // displs[rank] = 0;\n  for(int i = 0; i < rank; i++){\n    displs[rank] += sendcounts[i];\n  }\n\n  // MPI_Barrier(comm);\n\n  // receive counts from all ranks\n  // int recvcount;\n  // MPI_Gather(&sendcounts[rank], 1, MPI_INT, &recvcount, 1, MPI_INT, 0, comm);\n\n  // MPI_Barrier(comm);\n\n  // calculate displacements of each rank's portion of recvcount\n  // std::vector<int> recvcounts(size);\n  // for(int i = 0; i < size; i++){\n  //   displs[i] = 0;\n  //   for(int j = 0; j < i; j++){\n  //     displs[i] += recvcounts[j];\n  //   }\n  // }\n\n  // MPI_Barrier(comm);\n\n  // receive recvcounts from all ranks\n  // int total = 0;\n  // int* buf = new int[size];\n  // for(int i = 0; i < size; i++){\n  //   total += recvcounts[i];\n  // }\n  // MPI_Gather(&recvcounts[rank], 1, MPI_INT, buf, 1, MPI_INT, 0, comm);\n\n  // MPI_Barrier(comm);\n\n  // allocate memory for recvbuf on rank 0\n  // int* recvbuf;\n  // if (rank == 0){\n  //   recvbuf = new int[total];\n  // }\n\n  // MPI_Barrier(comm);\n\n  // gather all values in recvbuf on rank 0\n  // MPI_Gatherv(my_x.data(), sendcounts[rank], MPI_DOUBLE, recvbuf, sendcounts, displs, MPI_DOUBLE, 0, comm);\n\n  // MPI_Barrier(comm);\n\n  // copy to bins\n  if (rank == 0){\n    for(int i = 0; i < size; i++){\n      for(size_t j = 0; j < sendcounts[i]; j++){\n        int bin = (int)(((double)buf[displs[i] + j] + 0.01) / 10);\n        bins[bin] += 1;\n      }\n    }\n  }\n\n  MPI_Barrier(comm);\n\n  // deallocate memory for recvbuf on rank 0\n  // if (rank == 0){\n  //   delete[] recvbuf;",
            "// Fill this in!\n}",
            "// TODO: fill this in\n}",
            "// YOUR CODE GOES HERE\n  // NOTE: You can add helper functions if you want\n}",
            "bins.fill(0);\n    auto binIndex = [&x](size_t index){\n        return static_cast<size_t>(std::floor(x[index] / 10));\n    };\n    // Loop over x to compute bin counts\n    for (size_t index = 0; index < x.size(); index++) {\n        bins[binIndex(index)]++;\n    }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n    size_t const rank = MPI::COMM_WORLD.Get_rank();\n    size_t const size = MPI::COMM_WORLD.Get_size();\n\n    std::vector<size_t> localBins(10, 0);\n    size_t const n = x.size() / size;\n    size_t const m = x.size() % size;\n    size_t begin = rank * n;\n    size_t end = begin + n;\n    if (rank == size - 1) {\n        end += m;\n    }\n    for (size_t i = begin; i < end; ++i) {\n        ++localBins[x[i] / 10];\n    }\n    MPI::COMM_WORLD.Allreduce(localBins.data(), bins.data(), 10, MPI::INT, MPI::SUM);\n}",
            "// TODO: your code here\n}",
            "// TODO: insert your code here\n  if(bins.size()!= 10){\n      throw std::logic_error(\"size of bins is not 10\");\n  }\n\n  for(int i = 0; i < x.size(); i++){\n    if(x[i] >= 0 && x[i] < 10){\n      bins[0] += 1;\n    } else if(x[i] >= 10 && x[i] < 20){\n      bins[1] += 1;\n    } else if(x[i] >= 20 && x[i] < 30){\n      bins[2] += 1;\n    } else if(x[i] >= 30 && x[i] < 40){\n      bins[3] += 1;\n    } else if(x[i] >= 40 && x[i] < 50){\n      bins[4] += 1;\n    } else if(x[i] >= 50 && x[i] < 60){\n      bins[5] += 1;\n    } else if(x[i] >= 60 && x[i] < 70){\n      bins[6] += 1;\n    } else if(x[i] >= 70 && x[i] < 80){\n      bins[7] += 1;\n    } else if(x[i] >= 80 && x[i] < 90){\n      bins[8] += 1;\n    } else if(x[i] >= 90 && x[i] < 100){\n      bins[9] += 1;\n    }\n  }\n\n}",
            "// TODO\n}",
            "std::array<size_t, 10> counts;\n    counts.fill(0);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    for (size_t i = 0; i < x.size(); i++) {\n        counts[static_cast<size_t>(x[i] / 10)]++;\n    }\n    std::array<size_t, 10> temp_counts;\n    MPI_Reduce(&counts, &temp_counts, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = temp_counts[i];\n        }\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n\n    // Number of elements per rank:\n    size_t n = x.size();\n    size_t q = n/size;\n    size_t r = n%size;\n\n    // Start and end indices for each rank:\n    size_t s = rank*q + std::min(r, rank);\n    size_t e = (rank+1)*q + std::min(r, rank+1) - 1;\n\n    if (rank == 0)\n        std::fill(bins.begin(), bins.end(), 0);\n\n    // Compute counts on each rank:\n    for (size_t i = s; i <= e; ++i) {\n        size_t bin = (int)x[i] / 10;\n        bins[bin]++;\n    }\n\n    // Reduce counts from all ranks to rank 0:\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n}",
            "size_t n = x.size();\n  bins.fill(0);\n\n  //TODO: insert code here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // Your code here!\n}",
            "// Fill code\n}",
            "// TODO: Your code goes here\n}",
            "auto const n = x.size();\n  int const rank = 0;\n  int const root = 0;\n\n  std::vector<size_t> counts(10);\n  std::fill(counts.begin(), counts.end(), 0);\n\n  for (size_t i = 0; i < n; ++i) {\n    auto const value = x[i];\n    auto const bin = static_cast<size_t>(value / 10);\n    if (bin < counts.size()) {\n      ++counts[bin];\n    }\n  }\n\n  MPI_Gather(&counts, counts.size(), MPI_INT, bins.data(), counts.size(), MPI_INT, root, MPI_COMM_WORLD);\n}",
            "auto const n = x.size();\n\n  // Your code here\n\n}",
            "// TODO\n}",
            "// Your code goes here\n}",
            "int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * (x.size() / size);\n    int end = (rank + 1) * (x.size() / size);\n    int myBin[10];\n\n    for (int i = 0; i < 10; i++) {\n        myBin[i] = 0;\n    }\n\n    // Count the number of values in [0,10), [10, 20), [20, 30),...\n    // and store the counts in myBin.\n    // You may assume the input x is between 0 and 100.\n    //...\n\n    // Send myBin to rank 0.\n    MPI_Send(&myBin[0], 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\n    // Receive the counts from rank 0 and add them up.\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            int recv[10];\n            MPI_Recv(&recv[0], 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 10; j++) {\n                bins[j] += recv[j];\n            }\n        }\n    }\n}",
            "int my_rank = 0;\n    int n_ranks = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    auto local_x = x;\n    std::array<size_t, 10> local_bins{};\n\n    // TODO: implement\n\n    // Reduce results on rank 0\n    if (my_rank == 0) {\n        std::array<size_t, 10> results{};\n        for (int i = 0; i < n_ranks; ++i) {\n            // TODO: implement\n        }\n        bins = results;\n    }\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// Put your code here\n}",
            "/* Your code goes here. */\n}",
            "// Your code here\n}",
            "// TODO: implement me!\n  // Hint: use std::lower_bound to compute the bin index for a given value.\n  //       Use MPI_Reduce to sum the bin counts across ranks.\n}",
            "std::array<size_t, 10> counts;\n    counts.fill(0);\n\n    for (auto const& el : x) {\n        auto bin = std::floor(el / 10);\n        ++counts[bin];\n    }\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    MPI_Reduce(counts.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int nprocs, rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n_per_proc = x.size() / nprocs;\n  int remainder = x.size() % nprocs;\n\n  int start_index = rank * n_per_proc;\n  int end_index = rank * n_per_proc + n_per_proc;\n\n  if (remainder > 0) {\n    if (rank < remainder) {\n      start_index += rank;\n      end_index += rank + 1;\n    } else {\n      start_index += remainder;\n      end_index += remainder;\n    }\n  }\n\n  int bins_per_proc[10];\n  std::fill(bins_per_proc, bins_per_proc + 10, 0);\n\n  // count bins\n  for (int i = start_index; i < end_index; i++) {\n    int bin_num = std::floor(x[i] / 10.0);\n    bins_per_proc[bin_num]++;\n  }\n\n  // send bins to rank 0\n  int *bins_per_proc_send = new int[10];\n  std::copy(bins_per_proc, bins_per_proc + 10, bins_per_proc_send);\n\n  if (rank!= 0) {\n    MPI_Send(bins_per_proc_send, 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  } else {\n    int *bins_per_proc_recv = new int[10];\n    for (int i = 0; i < nprocs; i++) {\n      if (i!= 0) {\n        MPI_Recv(bins_per_proc_recv, 10, MPI_INT, i, 0, MPI_COMM_WORLD,\n                 MPI_STATUS_IGNORE);\n        for (int j = 0; j < 10; j++) {\n          bins[j] += bins_per_proc_recv[j];\n        }\n      }\n    }\n\n    delete[] bins_per_proc_recv;\n  }\n\n  delete[] bins_per_proc_send;\n}",
            "// TODO: write your solution here\n  //...\n}",
            "std::array<size_t, 10> counts;\n  counts.fill(0);\n  size_t const n = x.size();\n  for (size_t i = 0; i < n; ++i) {\n    int index = (x[i] / 10.0);\n    if (index >= 10)\n      index = 9;\n    counts[index]++;\n  }\n\n  // TODO\n\n  int const rank = 0;\n  int const nranks = 1;\n  int const source = 0;\n  int const tag = 1;\n\n  MPI_Comm comm = MPI_COMM_WORLD;\n\n  if (rank == 0) {\n    MPI_Status status;\n    for (int i = 1; i < nranks; ++i) {\n      MPI_Recv(&counts[0], counts.size(), MPI_SIZE_T, source, tag, comm, &status);\n    }\n  }\n  else if (rank == 1) {\n    MPI_Send(&counts[0], counts.size(), MPI_SIZE_T, source, tag, comm);\n  }\n\n  bins = counts;\n}",
            "std::vector<size_t> localCounts(10, 0);\n    // Compute the counts on this rank\n    for (auto xVal: x) {\n        auto bin = static_cast<size_t>(xVal / 10);\n        if (bin < 10) {\n            ++localCounts[bin];\n        }\n    }\n    // Gather the counts from all ranks to rank 0\n    MPI_Reduce(localCounts.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "/*... */\n}",
            "// TODO: write your solution here\n    size_t size = x.size();\n    int rank, size_proc;\n    std::array<size_t, 10> local_bins;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size_proc);\n    MPI_Status status;\n    std::vector<double> x_proc(size/size_proc, 0);\n    std::array<size_t, 10> local_bins_proc;\n    for (int i = 0; i < size_proc; i++) {\n        if (i == rank) {\n            for (int j = 0; j < 10; j++) {\n                local_bins_proc[j] = 0;\n            }\n            for (int j = 0; j < size; j++) {\n                if (0 <= x[j] && x[j] <= 10) {\n                    local_bins_proc[0]++;\n                } else if (10 < x[j] && x[j] <= 20) {\n                    local_bins_proc[1]++;\n                } else if (20 < x[j] && x[j] <= 30) {\n                    local_bins_proc[2]++;\n                } else if (30 < x[j] && x[j] <= 40) {\n                    local_bins_proc[3]++;\n                } else if (40 < x[j] && x[j] <= 50) {\n                    local_bins_proc[4]++;\n                } else if (50 < x[j] && x[j] <= 60) {\n                    local_bins_proc[5]++;\n                } else if (60 < x[j] && x[j] <= 70) {\n                    local_bins_proc[6]++;\n                } else if (70 < x[j] && x[j] <= 80) {\n                    local_bins_proc[7]++;\n                } else if (80 < x[j] && x[j] <= 90) {\n                    local_bins_proc[8]++;\n                } else if (90 < x[j] && x[j] <= 100) {\n                    local_bins_proc[9]++;\n                }\n            }\n        }\n        MPI_Scatter(&x[0], size/size_proc, MPI_DOUBLE, &x_proc[0], size/size_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        for (int j = 0; j < 10; j++) {\n            local_bins_proc[j] = 0;\n        }\n        for (int j = 0; j < size/size_proc; j++) {\n            if (0 <= x_proc[j] && x_proc[j] <= 10) {\n                local_bins_proc[0]++;\n            } else if (10 < x_proc[j] && x_proc[j] <= 20) {\n                local_bins_proc[1]++;\n            } else if (20 < x_proc[j] && x_proc[j] <= 30) {\n                local_bins_proc[2]++;\n            } else if (30 < x_proc[j] && x_proc[j] <= 40) {\n                local_bins_proc[3]++;\n            } else if (40 < x_proc[j] && x_proc[j] <= 50) {\n                local_bins_proc[4]++;\n            } else if (50 < x_proc[j] && x_proc[j] <= 60) {\n                local_bins_proc[5]++;\n            } else if (60 < x_proc[j] && x_proc[j] <= 70) {\n                local_bins_proc[6]++;\n            } else if (70 < x_proc[j] && x_proc[j] <= 80) {\n                local",
            "// Your code here\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  const int N = x.size();\n  const int block_size = N / size;\n\n  std::vector<size_t> bin_values(10, 0);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(&bin_values[0], 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; ++j)\n        bin_values[j] += bin_values[j];\n    }\n    MPI_Send(&bin_values[0], 10, MPI_UNSIGNED_LONG, 1, 0, MPI_COMM_WORLD);\n  } else {\n    for (int i = rank * block_size; i < (rank + 1) * block_size; ++i)\n      bin_values[static_cast<int>(x[i] / 10)]++;\n    MPI_Send(&bin_values[0], 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    MPI_Recv(&bin_values[0], 10, MPI_UNSIGNED_LONG, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  if (rank == 0)\n    bins = bin_values;\n}",
            "// TODO: Implement this function.\n  // Hint: To find the bin for a value, use:\n  //   index = (value / 10) - 1;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (double val : x) {\n    int idx = std::floor(val / 10);\n    bins[idx] += 1;\n  }\n\n}",
            "int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // TODO: Implement this function\n    return;\n}",
            "// Your code here\n\n}",
            "std::array<size_t, 10> localBins;\n  for (int i = 0; i < 10; ++i) localBins[i] = 0;\n\n  // TODO: count the number of values in x that are in each interval\n  // [i * 10, (i + 1) * 10).\n\n  // TODO: Reduce localBins into bins using MPI_Reduce.\n}",
            "// Add your code here\n}",
            "// TODO: Your code here\n\n    for (int i = 0; i < 10; i++)\n        bins[i] = 0;\n    for (int i = 0; i < x.size(); i++)\n    {\n        if (x[i] >= 0 && x[i] < 10)\n        {\n            bins[0]++;\n        }\n        else if (x[i] >= 10 && x[i] < 20)\n        {\n            bins[1]++;\n        }\n        else if (x[i] >= 20 && x[i] < 30)\n        {\n            bins[2]++;\n        }\n        else if (x[i] >= 30 && x[i] < 40)\n        {\n            bins[3]++;\n        }\n        else if (x[i] >= 40 && x[i] < 50)\n        {\n            bins[4]++;\n        }\n        else if (x[i] >= 50 && x[i] < 60)\n        {\n            bins[5]++;\n        }\n        else if (x[i] >= 60 && x[i] < 70)\n        {\n            bins[6]++;\n        }\n        else if (x[i] >= 70 && x[i] < 80)\n        {\n            bins[7]++;\n        }\n        else if (x[i] >= 80 && x[i] < 90)\n        {\n            bins[8]++;\n        }\n        else if (x[i] >= 90 && x[i] < 100)\n        {\n            bins[9]++;\n        }\n    }\n    return;\n}",
            "// TODO: Implement this function.\n}",
            "int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    int N = x.size();\n    int n = N / nprocs;\n    int m = N % nprocs;\n    std::vector<int> counts;\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; ++i) {\n            counts.push_back(0);\n        }\n\n    }\n\n    for (int i = 0; i < n; ++i) {\n        int bin = int(x[i] / 10);\n        counts[bin]++;\n    }\n\n    for (int i = 0; i < m; ++i) {\n        int bin = int(x[n + i] / 10);\n        counts[bin]++;\n    }\n\n    MPI_Reduce(&counts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return;\n}",
            "auto const size = x.size();\n  auto const chunk_size = size / MPI::COMM_WORLD.size();\n  auto const chunk_remainder = size % MPI::COMM_WORLD.size();\n  auto const chunk_start = chunk_size * MPI::COMM_WORLD.rank();\n  auto const chunk_end = chunk_start + chunk_size +\n                         (MPI::COMM_WORLD.rank() < chunk_remainder);\n  // Count bins.\n  std::array<size_t, 10> bins_local = {};\n  for (size_t i = chunk_start; i < chunk_end; ++i) {\n    bins_local[static_cast<size_t>(std::floor(x[i] / 10)) % 10]++;\n  }\n  // Gather the counts from all ranks.\n  std::array<size_t, 10> bins_temp = {};\n  MPI::COMM_WORLD.Reduce(&bins_local, &bins_temp, bins_local.size(),\n                         MPI_LONG, MPI_SUM, 0);\n  // Copy result into `bins`.\n  if (MPI::COMM_WORLD.rank() == 0) {\n    bins = bins_temp;\n  }\n}",
            "// TODO\n\n}",
            "// TODO: your code goes here.\n}",
            "if (x.size() == 0)\n        return;\n\n    int n = x.size();\n    int rank = 0;\n    int size = 0;\n\n    std::vector<size_t> bins_all(10);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // for every rank, compute the number of values that belong to a bin\n    for (int i = 0; i < n; i++) {\n        bins[int(x[i] / 10.0)] += 1;\n    }\n\n    // gather all bins and sum them up\n    MPI_Gather(&bins[0], 10, MPI_UNSIGNED_LONG_LONG, &bins_all[0], 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // sum up all bins on rank 0\n        for (int i = 0; i < 10; i++) {\n            bins[i] = 0;\n            for (int j = 0; j < size; j++) {\n                bins[i] += bins_all[10*j + i];\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// This function should be filled in with your implementation.\n}",
            "constexpr size_t bin_count = 10;\n\n  /*\n     Your code here\n  */\n}",
            "// your code here\n}",
            "int rank, nranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  //...\n}",
            "// Count bins per element\n  std::array<size_t, 10> per_element_counts;\n\n  // Communication:\n  MPI_Reduce(MPI_IN_PLACE, per_element_counts.data(), per_element_counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Add up bins per element\n  std::copy(per_element_counts.begin(), per_element_counts.end(), bins.begin());\n\n  // Calculate prefix sums\n  std::array<size_t, 10> partial_sums;\n  std::partial_sum(bins.begin(), bins.end(), partial_sums.begin());\n\n  // Communication:\n  MPI_Reduce(MPI_IN_PLACE, partial_sums.data(), partial_sums.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Calculate counts per bin\n  std::copy(partial_sums.begin(), partial_sums.end(), bins.begin());\n\n  // Subtract last count from each bin (only for last bin)\n  std::transform(bins.begin(), bins.end(), bins.end(), [&](auto i) { return i - bins.back(); });\n}",
            "if (bins.size()!= 10) {\n    throw std::runtime_error(\"bins array must be 10 elements\");\n  }\n\n  // initialize bins to 0\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // TODO: implement this function\n  // Hint: `std::count_if` and `std::function` might be useful.\n  // Hint: each rank must only count the elements that are local to that rank.\n  // Hint: use `MPI_Reduce` to combine the bins on each rank into a single\n  // result.\n  // Hint: the \"reduction\" operation that you pass to `MPI_Reduce` should be\n  // a function that adds two bins together, where each bin is an array of 10\n  // elements\n}",
            "// TODO\n}",
            "int numRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n  std::array<std::array<size_t, 10>, 10> local_bins;\n  for (auto& local_bin : local_bins) {\n    local_bin.fill(0);\n  }\n\n  for (auto elem : x) {\n    int binIndex = static_cast<int>(elem / 10);\n    int rankIndex = binIndex / 10;\n    int offset = binIndex % 10;\n    local_bins[rankIndex][offset]++;\n  }\n\n  // TODO: use MPI_Gather to get the counts into bins\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "if (x.empty()) return;\n\n  /* YOUR CODE HERE */\n\n  /*... */\n\n}",
            "// TODO\n}",
            "// Your code here\n\n}",
            "// YOUR CODE HERE\n}",
            "// your code here\n}",
            "// TODO\n}",
            "size_t n_local = x.size();\n  std::array<size_t, 10> local_bins = {0};\n\n  for (size_t i = 0; i < n_local; ++i) {\n    local_bins[std::floor(x[i] / 10)] += 1;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n    const size_t xSize = x.size();\n    // Create the bins\n    std::array<size_t, 10> localBins = {};\n\n    // Fill the bins\n    for (const auto& value : x) {\n        const int bin = static_cast<int>(value / 10);\n        ++localBins[bin];\n    }\n\n    // Combine the bins\n    if (MPI_RANK == 0) {\n        MPI_Reduce(&localBins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(&localBins[0], nullptr, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "}",
            "/* Your code here */\n}",
            "const int num_ranks = 4;\n\n  // TODO\n}",
            "// TODO\n}",
            "// TODO\n    // Your code goes here.\n}",
            "int num_procs;\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  std::vector<size_t> partialBins(10, 0);\n  // TODO: fill in this function\n  if (rank == 0) {\n    int chunkSize = x.size() / num_procs;\n    int chunkRem = x.size() % num_procs;\n    std::vector<int> chunkId(num_procs, chunkSize);\n    for (int i = 0; i < chunkRem; ++i) {\n      ++chunkId[i];\n    }\n    for (int i = 0; i < num_procs; ++i) {\n      for (int j = 0; j < chunkId[i]; ++j) {\n        int bin = x[j] / 10;\n        if (bin == 10) bin = 9;\n        ++partialBins[bin];\n      }\n      MPI_Send(partialBins.data(), partialBins.size(), MPI_UNSIGNED, i, 0, MPI_COMM_WORLD);\n      partialBins.assign(10, 0);\n    }\n  } else {\n    std::vector<int> temp(10, 0);\n    MPI_Recv(temp.data(), temp.size(), MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    partialBins = temp;\n  }\n  MPI_Reduce(partialBins.data(), bins.data(), partialBins.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype MPI_ARRAY_SIZE_T =...;\n\n  MPI_Request request;\n  int rc = MPI_Iscatter(x.data(),\n                       ...,\n                       ...,\n                       ...,\n                       ...,\n                        &request);\n\n  MPI_Wait(&request, MPI_STATUS_IGNORE);\n\n  size_t n =...;\n  for (size_t i = 0; i < n; ++i)\n    ++bins[i/10];\n\n  std::array<size_t, 10> temp{};\n  MPI_Request request2;\n  rc = MPI_Iallreduce(bins.data(),\n                      temp.data(),\n                     ...,\n                     ...,\n                      &request2);\n\n  MPI_Wait(&request2, MPI_STATUS_IGNORE);\n  bins = temp;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // allocate local array\n    std::array<size_t, 10> localBins = {0};\n\n    // compute local counts\n    for (auto i : x) {\n        if (i < 10) {\n            ++localBins[0];\n        }\n        else if (i < 20) {\n            ++localBins[1];\n        }\n        else if (i < 30) {\n            ++localBins[2];\n        }\n        else if (i < 40) {\n            ++localBins[3];\n        }\n        else if (i < 50) {\n            ++localBins[4];\n        }\n        else if (i < 60) {\n            ++localBins[5];\n        }\n        else if (i < 70) {\n            ++localBins[6];\n        }\n        else if (i < 80) {\n            ++localBins[7];\n        }\n        else if (i < 90) {\n            ++localBins[8];\n        }\n        else {\n            ++localBins[9];\n        }\n    }\n\n    // allocate buffer for all results\n    auto bufferSize = localBins.size() * size;\n    std::vector<int> buffer(bufferSize);\n\n    // collect all results\n    MPI_Gather(localBins.data(), localBins.size(), MPI_INT,\n            buffer.data(), localBins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    // gather all results\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < localBins.size(); ++j) {\n                bins[j] += buffer[i * localBins.size() + j];\n            }\n        }\n    }\n\n}",
            "// TODO: your code here\n}",
            "if (x.size() == 0) {\n        for (auto& bin : bins)\n            bin = 0;\n        return;\n    }\n\n    // TODO: compute the number of elements in x that are in each bin\n    //       and store the result in bins.\n\n    // (1) Count number of elements in each bin\n    std::array<size_t, 10> binCounts;\n    // TODO: replace this line with the appropriate code\n    binCounts = {0};\n    // (2) Replace binCounts with the final result\n    bins = binCounts;\n}",
            "auto const size = x.size();\n  auto const rank = MPI_Comm_rank(MPI_COMM_WORLD, nullptr);\n  auto const worldSize = MPI_Comm_size(MPI_COMM_WORLD, nullptr);\n\n  std::vector<size_t> binsLocal(10, 0);\n  for (auto it = x.begin() + rank * size / worldSize; it!= x.begin() + (rank + 1) * size / worldSize; ++it) {\n    auto bin = 10 * (*it) / 100;\n    if (bin >= 10) {\n      bin = 9;\n    }\n    ++binsLocal[bin];\n  }\n\n  std::vector<size_t> binsGlobal(10, 0);\n  MPI_Reduce(&binsLocal[0], &binsGlobal[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = std::array<size_t, 10>(binsGlobal.begin(), binsGlobal.end());\n  }\n}",
            "auto const n = x.size();\n    bins.fill(0);\n\n    //...\n}",
            "/* Add your code here */\n}",
            "auto const n = x.size();\n  auto const chunk = n / 10;\n\n  // TODO\n\n  MPI_Finalize();\n}",
            "for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n    int size;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunkSize = x.size() / size;\n    std::vector<double> local_x(chunkSize);\n    for (int i = 0; i < chunkSize; i++) {\n        local_x[i] = x[rank * chunkSize + i];\n    }\n    std::array<size_t, 10> local_bins;\n    for (int i = 0; i < 10; i++) {\n        local_bins[i] = 0;\n    }\n    for (int i = 0; i < chunkSize; i++) {\n        int bin = local_x[i] / 10;\n        local_bins[bin] += 1;\n    }\n    std::array<size_t, 10> global_bins;\n    MPI_Reduce(&local_bins[0], &global_bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = global_bins[i];\n        }\n    }\n}",
            "// TODO: Fill in your code here\n}",
            "auto const& rank = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    auto const& size = MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int chunkSize = x.size() / size;\n    std::vector<size_t> chunk(chunkSize);\n    MPI_Scatter(MPI_IN_PLACE, chunkSize, MPI_UNSIGNED_LONG_LONG, chunk.data(), chunkSize, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    for (auto const& i : chunk) {\n        if (i < 10) {\n            bins[0]++;\n        } else if (i < 20) {\n            bins[1]++;\n        } else if (i < 30) {\n            bins[2]++;\n        } else if (i < 40) {\n            bins[3]++;\n        } else if (i < 50) {\n            bins[4]++;\n        } else if (i < 60) {\n            bins[5]++;\n        } else if (i < 70) {\n            bins[6]++;\n        } else if (i < 80) {\n            bins[7]++;\n        } else if (i < 90) {\n            bins[8]++;\n        } else if (i < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "//...\n}",
            "// TODO: your code here\n\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk_size = x.size() / size;\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      int i = rank;\n      int start = chunk_size * i;\n      int end = start + chunk_size;\n      MPI_Send(&x[start], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Recv(&x[chunk_size * rank], chunk_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  for (auto v : x) {\n    int bin = int(v / 10);\n    bins[bin]++;\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      int i = rank;\n      int start = chunk_size * i;\n      int end = start + chunk_size;\n      MPI_Recv(&x[start], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(&x[chunk_size * rank], chunk_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "bins.fill(0);\n  for (double v : x) {\n    if (v >= 0 && v < 10) {\n      bins[0]++;\n    } else if (v >= 10 && v < 20) {\n      bins[1]++;\n    } else if (v >= 20 && v < 30) {\n      bins[2]++;\n    } else if (v >= 30 && v < 40) {\n      bins[3]++;\n    } else if (v >= 40 && v < 50) {\n      bins[4]++;\n    } else if (v >= 50 && v < 60) {\n      bins[5]++;\n    } else if (v >= 60 && v < 70) {\n      bins[6]++;\n    } else if (v >= 70 && v < 80) {\n      bins[7]++;\n    } else if (v >= 80 && v < 90) {\n      bins[8]++;\n    } else if (v >= 90 && v <= 100) {\n      bins[9]++;\n    }\n  }\n}",
            "size_t n = x.size();\n\n  // Add up bins in all processes\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Add up bins in rank 0 (this is not necessary, it is just for comparison)\n  std::array<size_t, 10> bins_local{};\n  for (auto i = 0; i < n; ++i) {\n    auto bin = static_cast<size_t>(std::floor(x[i] / 10.0));\n    bins_local[bin]++;\n  }\n  for (size_t i = 0; i < bins_local.size(); ++i) {\n    bins[i] += bins_local[i];\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    const int rank = getRank(comm);\n    const int nproc = getNproc(comm);\n\n    // Your code here\n\n}",
            "const int rank = MPI_RANK;\n  const int size = MPI_SIZE;\n\n  std::vector<size_t> bins_of_rank;\n  bins_of_rank.assign(10, 0);\n  // TODO: Implement binsBy10Count\n\n  // Send the data to the rank 0.\n  if (rank!= 0) {\n    MPI_Send(&bins_of_rank[0], 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::vector<size_t> bins_of_other_rank;\n      MPI_Recv(&bins_of_other_rank[0], 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      for (int j = 0; j < 10; j++) {\n        bins[j] += bins_of_other_rank[j];\n      }\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n    return;\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t numRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n  size_t size = x.size();\n  size_t mySize = size / numRanks;\n  size_t myOffset = mySize * MPI_Comm_rank(MPI_COMM_WORLD);\n  std::vector<double> myX(myX.begin() + myOffset, myX.begin() + myOffset + mySize);\n\n  size_t binsPerRank = 10 / numRanks;\n  size_t myBinStart = binsPerRank * MPI_Comm_rank(MPI_COMM_WORLD);\n  std::vector<double> myBins(binsPerRank);\n\n  // TODO: Compute local counts\n\n  // TODO: Combine the results of all the ranks into `bins`\n}",
            "if(x.size() < 100) {\n    // TODO: write this code\n  }\n  else {\n    // TODO: write this code\n  }\n}",
            "if (x.size()!= bins.size()) {\n    throw std::invalid_argument(\"Number of items must be equal\");\n  }\n\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank!= 0) {\n    MPI_Send(&x.front(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  } else {\n    std::array<double, x.size()> x_recv;\n\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x_recv.front(), x_recv.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < x_recv.size(); j++) {\n        if (x_recv[j] >= 0 && x_recv[j] < 10) {\n          bins[0] += 1;\n        } else if (x_recv[j] >= 10 && x_recv[j] < 20) {\n          bins[1] += 1;\n        } else if (x_recv[j] >= 20 && x_recv[j] < 30) {\n          bins[2] += 1;\n        } else if (x_recv[j] >= 30 && x_recv[j] < 40) {\n          bins[3] += 1;\n        } else if (x_recv[j] >= 40 && x_recv[j] < 50) {\n          bins[4] += 1;\n        } else if (x_recv[j] >= 50 && x_recv[j] < 60) {\n          bins[5] += 1;\n        } else if (x_recv[j] >= 60 && x_recv[j] < 70) {\n          bins[6] += 1;\n        } else if (x_recv[j] >= 70 && x_recv[j] < 80) {\n          bins[7] += 1;\n        } else if (x_recv[j] >= 80 && x_recv[j] < 90) {\n          bins[8] += 1;\n        } else if (x_recv[j] >= 90 && x_recv[j] < 100) {\n          bins[9] += 1;\n        } else {\n          throw std::invalid_argument(\"Data value out of range [0, 100]\");\n        }\n      }\n    }\n  }\n}",
            "// Your code here\n}",
            "//TODO\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "std::array<size_t, 10> myBins{};\n  for (size_t i = 0; i < x.size(); ++i) {\n    // TODO: find the correct bin for x[i] and increment the counter\n  }\n  // TODO: send myBins to rank 0\n  // TODO: receive from rank 0 and add to myBins\n  // TODO: add up myBins from all ranks\n}",
            "MPI_Datatype type;\n    MPI_Type_vector(x.size(), 1, 1, MPI_DOUBLE, &type);\n    MPI_Type_commit(&type);\n\n    MPI_Bcast(x.data(), x.size(), type, 0, MPI_COMM_WORLD);\n    MPI_Gather(bins.data(), x.size(), type, bins.data(), x.size(), type, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n  for (auto i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  if (x.size() == 0) {\n    return;\n  }\n\n  auto localSize = x.size();\n  auto rank = 0;\n  auto commSize = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  auto blockSize = localSize / commSize;\n\n  auto subBins = std::array<size_t, 10>();\n  for (auto i = 0; i < 10; i++) {\n    subBins[i] = 0;\n  }\n  for (auto i = 0; i < localSize; i++) {\n    auto index = static_cast<size_t>((x[i] / 10));\n    subBins[index] += 1;\n  }\n\n  std::vector<size_t> recvData(10);\n  MPI_Gather(subBins.data(), 10, MPI_UNSIGNED_LONG, recvData.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (auto i = 0; i < 10; i++) {\n      bins[i] = recvData[i];\n    }\n  }\n}",
            "}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "cuda",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "}",
            "}",
            "}",
            "__shared__ size_t sharedBins[10];\n    size_t threadId = threadIdx.x;\n    if (threadId < 10) sharedBins[threadId] = 0;\n    __syncthreads();\n\n    // Each thread will use a different index.\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = floor(x[i] / 10.0);\n        atomicAdd(&sharedBins[bin], 1);\n    }\n    __syncthreads();\n\n    if (threadId < 10) {\n        atomicAdd(&bins[threadId], sharedBins[threadId]);\n    }\n}",
            "// TODO\n}",
            "__shared__ int shared_mem[32];\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] >= 0.0 && x[i] < 10.0) {\n      atomicAdd(shared_mem, 1);\n    }\n  }\n  __syncthreads();\n  bins[threadIdx.x] = shared_mem[threadIdx.x];\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (tid < N) {\n        int binIndex = 10 * (int) (x[tid] / 10);\n        atomicAdd(&bins[binIndex], 1);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n  if (i < N) {\n    double val = x[i];\n    size_t bin = (size_t)(val / 10);\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "size_t i = threadIdx.x;\n    if (i < N)\n        atomicAdd(&bins[(int)floor(x[i] / 10)], 1);\n}",
            "int index = threadIdx.x;\n\t__shared__ double x_shared[THREADS];\n\n\t// copy values into shared memory\n\tif (index < N)\n\t\tx_shared[index] = x[index];\n\t__syncthreads();\n\n\t// sum the elements\n\tfor (int i = 5; i > 0; i >>= 1) {\n\t\tif (index < i)\n\t\t\tx_shared[index] += x_shared[index + i];\n\t\t__syncthreads();\n\t}\n\n\t// find the sum\n\tif (index == 0) {\n\t\tint bucket = (int)x_shared[0] / 10;\n\t\tatomicAdd(&bins[bucket], 1);\n\t}\n}",
            "// Initialize thread with 0\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  bins[4] = 0;\n  bins[5] = 0;\n  bins[6] = 0;\n  bins[7] = 0;\n  bins[8] = 0;\n  bins[9] = 0;\n\n  // Loop through all values in x\n  for (size_t i = 0; i < N; i++) {\n    double val = x[i];\n\n    // If value is within the range, increment the corresponding bin\n    if (val >= 0 && val <= 10)\n      bins[0] = bins[0] + 1;\n\n    else if (val >= 10 && val <= 20)\n      bins[1] = bins[1] + 1;\n\n    else if (val >= 20 && val <= 30)\n      bins[2] = bins[2] + 1;\n\n    else if (val >= 30 && val <= 40)\n      bins[3] = bins[3] + 1;\n\n    else if (val >= 40 && val <= 50)\n      bins[4] = bins[4] + 1;\n\n    else if (val >= 50 && val <= 60)\n      bins[5] = bins[5] + 1;\n\n    else if (val >= 60 && val <= 70)\n      bins[6] = bins[6] + 1;\n\n    else if (val >= 70 && val <= 80)\n      bins[7] = bins[7] + 1;\n\n    else if (val >= 80 && val <= 90)\n      bins[8] = bins[8] + 1;\n\n    else if (val >= 90 && val <= 100)\n      bins[9] = bins[9] + 1;\n\n  }\n\n}",
            "}",
            "// Your code here\n\n}",
            "// Your code here\n\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double i = x[tid];\n    if (i < 10.0) {\n      atomicAdd(&bins[0], 1);\n    } else if (i < 20.0) {\n      atomicAdd(&bins[1], 1);\n    } else if (i < 30.0) {\n      atomicAdd(&bins[2], 1);\n    } else if (i < 40.0) {\n      atomicAdd(&bins[3], 1);\n    } else if (i < 50.0) {\n      atomicAdd(&bins[4], 1);\n    } else if (i < 60.0) {\n      atomicAdd(&bins[5], 1);\n    } else if (i < 70.0) {\n      atomicAdd(&bins[6], 1);\n    } else if (i < 80.0) {\n      atomicAdd(&bins[7], 1);\n    } else if (i < 90.0) {\n      atomicAdd(&bins[8], 1);\n    } else {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "/*\n     * Compute the linear index corresponding to the block and thread\n     */\n    size_t linearIdx = blockIdx.x * blockDim.x + threadIdx.x;\n    /*\n     * Do not do any work if the linear index is larger than the number of elements\n     * in the vector\n     */\n    if (linearIdx >= N) return;\n\n    /*\n     * Compute the bin based on the value in x\n     */\n    int bin = (int)floor(x[linearIdx] / 10);\n    /*\n     * Increment the appropriate bin\n     */\n    atomicAdd(&bins[bin], 1);\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) return;\n  const int bin = (int)x[tid] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "int id = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (id < N) {\n    int digit = (int)(x[id] / 10);\n    atomicAdd(&bins[digit], 1);\n  }\n}",
            "}",
            "int tid = threadIdx.x;\n  size_t i = tid;\n  while (i < N) {\n    int bin = int(x[i] / 10.0);\n    atomicAdd(&(bins[bin]), 1);\n    i += blockDim.x;\n  }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        // TODO\n    }\n}",
            "// TODO: implement me\n}",
            "/* YOUR CODE HERE */\n}",
            "int i = threadIdx.x;\n  if (i < N) {\n    if (x[i] < 10) bins[0]++;\n    else if (x[i] < 20) bins[1]++;\n    else if (x[i] < 30) bins[2]++;\n    else if (x[i] < 40) bins[3]++;\n    else if (x[i] < 50) bins[4]++;\n    else if (x[i] < 60) bins[5]++;\n    else if (x[i] < 70) bins[6]++;\n    else if (x[i] < 80) bins[7]++;\n    else if (x[i] < 90) bins[8]++;\n    else if (x[i] < 100) bins[9]++;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t bin = (size_t) x[i] / 10;\n    atomicAdd(bins + bin, 1);\n}",
            "// TODO\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const size_t stride = blockDim.x * gridDim.x;\n    for (size_t i = idx; i < N; i += stride) {\n        bins[x[i] / 10]++;\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = (x[i] + 9) / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n  int idx = threadIdx.x;\n\n  if (idx < N) {\n    int bin_idx = (int)x[idx] / 10;\n    atomicAdd(&bins[bin_idx], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[x[i] / 10]++;\n    }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        size_t bin = (size_t)x[idx] / 10;\n        atomicAdd(bins+bin, 1);\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n}",
            "const size_t thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (thread_id >= N) return;\n\n    const size_t bin = (size_t)(x[thread_id] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if(tid < N) {\n    size_t bin = (size_t) floor(x[tid] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const size_t idx = threadIdx.x;\n  // your code here\n  if (idx < N) {\n    if (x[idx] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[idx] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[idx] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[idx] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[idx] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[idx] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[idx] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[idx] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[idx] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[idx] < 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "int tid = threadIdx.x;\n    int numThreads = blockDim.x;\n    int start = tid;\n    int stride = numThreads;\n\n    __shared__ size_t sharedBins[10];\n    for (int i = 0; i < 10; i++) sharedBins[i] = 0;\n    __syncthreads();\n\n    while (start < N) {\n        if (start < N) {\n            int bin = floor(x[start] / 10.0);\n            atomicAdd(&sharedBins[bin], 1);\n        }\n        start += stride;\n    }\n    __syncthreads();\n\n    for (int i = 0; i < 10; i++) atomicAdd(&bins[i], sharedBins[i]);\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        int value = floor(x[index] / 10);\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        unsigned int bin = x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: implement\n}",
            "int tIdx = threadIdx.x;\n\tint bIdx = tIdx / 10;\n\tif (bIdx >= 10)\n\t\treturn;\n\tbins[bIdx] += (tIdx % 10) == (int)x[tIdx];\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        // Put the value in `x` into the corresponding bin.\n        // Remember that `x` contains values between 0 and 100, inclusive.\n        bins[0] = 0; // TODO\n        bins[1] = 0; // TODO\n        bins[2] = 0; // TODO\n        bins[3] = 0; // TODO\n        bins[4] = 0; // TODO\n        bins[5] = 0; // TODO\n        bins[6] = 0; // TODO\n        bins[7] = 0; // TODO\n        bins[8] = 0; // TODO\n        bins[9] = 0; // TODO\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int index = int(x[i] / 10);\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    int i = int(x[idx] / 10);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i >= N)\n    return;\n\n  double xi = x[i];\n\n  if (xi < 10)\n    atomicAdd(&bins[0], 1);\n  else if (xi < 20)\n    atomicAdd(&bins[1], 1);\n  else if (xi < 30)\n    atomicAdd(&bins[2], 1);\n  else if (xi < 40)\n    atomicAdd(&bins[3], 1);\n  else if (xi < 50)\n    atomicAdd(&bins[4], 1);\n  else if (xi < 60)\n    atomicAdd(&bins[5], 1);\n  else if (xi < 70)\n    atomicAdd(&bins[6], 1);\n  else if (xi < 80)\n    atomicAdd(&bins[7], 1);\n  else if (xi < 90)\n    atomicAdd(&bins[8], 1);\n  else\n    atomicAdd(&bins[9], 1);\n}",
            "/* Your code goes here */\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // make sure the index is in range\n   if (index < N) {\n     int bin = (int)x[index]/10;\n     atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO\n  // hint:\n  // 1. How do you convert the vector index to a bin index?\n  // 2. How do you update the `bins` array?\n  // 3. What are the implications of having multiple threads accessing the same bin?\n  //    (e.g., if one thread increments the bin count, does that change what the next thread\n  //    reads back?)\n  // 4. How do you deal with values greater than 100?\n  // 5. How do you deal with values less than 0?\n  // 6. How do you handle values that are exactly 10? (e.g., 10.0000000000000000000)\n  //    Can you handle this case without testing the value?\n  // 7. Why is it ok for multiple threads to be accessing the same bin?\n  //    Is there a better way?\n  // 8. Can you think of a way to avoid hard-coding the number of bins?\n}",
            "// Your code goes here.\n\n}",
            "// TODO: add code\n\n}",
            "// TODO\n}",
            "int i = threadIdx.x;\n  size_t c[10] = {0};\n  for (size_t j = i; j < N; j += blockDim.x) {\n    size_t bin = x[j] / 10;\n    assert(bin < 10);\n    atomicAdd(c + bin, 1);\n  }\n  for (size_t j = 0; j < 10; j++) {\n    atomicAdd(bins + j, c[j]);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t stride = gridDim.x * blockDim.x;\n  for (size_t i = tid; i < N; i += stride) {\n    size_t bin = (size_t)x[i] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "__shared__ size_t binCounts[10];\n    __shared__ size_t count;\n\n    size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        int bin = floor(x[i] / 10.0);\n        if (bin < 0 || bin > 9)\n            // TODO: Handle the edge case of x[i] being outside the range 0-100\n            return;\n        atomicAdd(&binCounts[bin], 1);\n    }\n\n    // TODO: At this point, all threads have counted the bin for x[i]. Use an atomicAdd to sum the counts\n    // into bins[bin].\n}",
            "}",
            "unsigned int id = blockDim.x * blockIdx.x + threadIdx.x;\n  if (id < N) {\n    int i = (int)(x[id] / 10);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        int bin = 1 + (int)((x[idx] + 0.001) / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    __shared__ size_t s[10];\n    if (threadIdx.x < 10) s[threadIdx.x] = 0;\n    __syncthreads();\n    size_t j = (size_t)floor(x[i] / 10);\n    atomicAdd(bins + j, 1);\n}",
            "// TODO\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        const int bin_index = floor(x[i] / 10);\n        atomicAdd(&bins[bin_index], 1);\n    }\n}",
            "/*\n   * your code goes here\n   */\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    int bin = (int) (x[i] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "__shared__ size_t partial[10];\n\n  partial[threadIdx.x] = 0;\n  size_t start = 10 * blockIdx.x;\n  size_t stride = 10 * gridDim.x;\n  size_t idx = start + threadIdx.x;\n  size_t end = start + stride;\n  for (; idx < end; idx += blockDim.x) {\n    if (idx < N) {\n      int bin = (int)(x[idx] / 10.0);\n      atomicAdd(&partial[bin], 1);\n    }\n  }\n  __syncthreads();\n\n  size_t offset = 0;\n  for (int i = 0; i < 10; i++) {\n    size_t tmp = partial[i];\n    partial[i] = atomicAdd(&bins[i], tmp);\n    offset += tmp;\n  }\n}",
            "// TODO\n}",
            "// The index of this thread in the input vector `x`\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check if the current thread is valid for the input vector\n  if (i >= N)\n    return;\n\n  // Compute the index of the bin that should hold the current value `x[i]`\n  size_t bin = int(x[i] / 10);\n\n  // Atomically increment the count for the correct bin\n  atomicAdd(bins + bin, 1);\n}",
            "// TODO: implement\n}",
            "// This kernel has one thread per value in `x`.\n  // The thread's index in the array corresponds to the value in the array.\n\n  // `threadIdx.x` is the thread's index in the array.\n  // `blockIdx.x` is the block's index in the grid.\n\n  // Find the index into the `bins` array that corresponds to the bin containing the value.\n  int binIdx = (int)((x[threadIdx.x] - 0) / 10);\n\n  // If the value is in the range for the bin, atomically increment the bin's count.\n  if (binIdx >= 0 && binIdx < 10) atomicAdd(&bins[binIdx], 1);\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  const int bin = static_cast<int>(x[i] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "// Your code here\n}",
            "const double binSize = 10.0;\n    const size_t binCount = 10;\n    size_t tid = threadIdx.x;\n\n    // TODO: compute the bin number for x[tid] and store it in `bin`\n    size_t bin;\n    double value = x[tid];\n    bin = value / binSize;\n\n    if(tid < N)\n        atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i >= N) { return; }\n\n    int index = (int) (x[i] / 10.0);\n    atomicAdd(&bins[index], 1);\n}",
            "int idx = threadIdx.x;\n    size_t i = idx;\n    while (i < N) {\n        if (x[i] < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[i] < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (x[i] < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (x[i] < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (x[i] < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (x[i] < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (x[i] < 90) {\n            atomicAdd(&bins[8], 1);\n        } else {\n            atomicAdd(&bins[9], 1);\n        }\n        i += blockDim.x;\n    }\n}",
            "// TODO: implement\n\n}",
            "// TODO: Your code here!\n}",
            "int ix = threadIdx.x + blockIdx.x * blockDim.x;\n    if (ix < N)\n    {\n        int bin_ix = floor(x[ix] / 10);\n        atomicAdd(&bins[bin_ix], 1);\n    }\n}",
            "// TODO: Fill in the body of the function.\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if(tid < N)\n   {\n      bins[int(x[tid]/10.0)]++;\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        const size_t value = x[idx] / 10;\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    // Your code goes here!\n}",
            "/*\n   *\n   * TODO 1\n   * - Loop over all elements in the input vector x.\n   * - Compute the bin index using `floor(x[i] / 10)`.\n   * - Increment the `bins[binIndex]` by one.\n   *\n   * Example:\n   *\n   * input: [10.3, 13.7, 14.9, 12.0, 13.9, 11.1, 12.4]\n   * output: [1, 1, 0, 1, 2, 0, 0]\n   */\n}",
            "// TODO\n}",
            "}",
            "__shared__ size_t shared_bins[10];\n  const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  for (size_t j = bid; j < 10; j += gridDim.x) {\n    shared_bins[j] = 0;\n  }\n  __syncthreads();\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    int bin = x[i] / 10;\n    atomicAdd(&shared_bins[bin], 1);\n  }\n  for (size_t j = tid; j < 10; j += blockDim.x) {\n    atomicAdd(&bins[j], shared_bins[j]);\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        size_t bin = x[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx >= N) {\n        return;\n    }\n\n    size_t val = (size_t)x[idx];\n    bins[val/10]++;\n}",
            "const int i = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (i < N) {\n\t\tconst size_t bin = x[i] / 10;\n\t\tatomicAdd(bins + bin, 1);\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "extern __shared__ int sh[];\n\n  // TODO: Implement\n  // Hint: You can use modulo (%) and division (/) operators, which both work on\n  // double-precision floating point numbers.\n}",
            "/* TODO: Implement a parallel kernel that computes the number of values\n     in each 10-bin for the input vector `x`. Results should be stored in the\n     `bins` array, which has 10 elements. The kernel is initialized with\n     at least as many threads as values in `x`.\n  */\n}",
            "// TODO: Implement this kernel\n  __shared__ size_t blockBins[10];\n\n  int local_idx = threadIdx.x;\n  int block_idx = blockIdx.x;\n  int block_size = blockDim.x;\n\n  if (local_idx == 0) {\n    for (int i = 0; i < 10; i++) {\n      blockBins[i] = 0;\n    }\n  }\n\n  __syncthreads();\n\n  // Find the appropriate bin for the current element\n  int bin_idx = (int)floor(x[block_idx*block_size + local_idx] / 10);\n  __syncthreads();\n\n  // Increment the bin's count\n  atomicAdd(&blockBins[bin_idx], 1);\n  __syncthreads();\n\n  // Update global bin count from local thread count\n  if (local_idx == 0) {\n    for (int i = 0; i < 10; i++) {\n      atomicAdd(&bins[i], blockBins[i]);\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid >= N) return;\n\n  size_t bin = (size_t)floor(x[tid] / 10.0);\n  atomicAdd(&bins[bin], 1);\n}",
            "}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        size_t bin = 10 * floor(x[idx] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t idx = threadIdx.x;\n  if (idx >= N) return;\n  int binIdx = floor((x[idx] + 0.5) / 10);\n  atomicAdd(bins + binIdx, 1);\n}",
            "const size_t tid = threadIdx.x;\n  if (tid < N) {\n    const int i = floor(x[tid] / 10);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "size_t i = threadIdx.x;\n  if (i < N) {\n    double value = x[i];\n    int bin = (int)(value / 10.0);\n    if (bin > 9) bin = 9;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    size_t bin = floor(x[idx] / 10.0);\n    atomicAdd(&bins[bin], 1);\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        int bin = int(x[i] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N)\n        return;\n    size_t i = (size_t)x[idx];\n    if (i >= 100)\n        return;\n    i = i / 10;\n    atomicAdd(&bins[i], 1);\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        size_t bin = (size_t)(x[i] / 10);\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "int idx = threadIdx.x;\n    int stride = blockDim.x;\n    __shared__ double mymin, mymax;\n    __shared__ double local_x;\n\n    double *local_x_ptr = &local_x;\n    if (idx == 0) {\n        mymin = x[0];\n        mymax = x[0];\n    }\n    __syncthreads();\n\n    while (idx < N) {\n        if (x[idx] < mymin)\n            mymin = x[idx];\n        else if (x[idx] > mymax)\n            mymax = x[idx];\n        idx += stride;\n    }\n    __syncthreads();\n\n    if (threadIdx.x == 0) {\n        *local_x_ptr = mymin;\n    }\n    __syncthreads();\n    local_x = *local_x_ptr;\n    __syncthreads();\n\n    while (local_x < mymax) {\n        local_x_ptr[threadIdx.x] = local_x;\n        __syncthreads();\n\n        // Count the number of items in the bucket.\n        int bin = floor((local_x + 5) / 10);\n        for (int i = 0; i < stride; i++) {\n            if (x[i] >= local_x && x[i] < local_x + 10)\n                atomicAdd(&bins[bin], 1);\n        }\n        __syncthreads();\n\n        local_x_ptr[threadIdx.x] = local_x;\n        __syncthreads();\n        local_x = local_x_ptr[threadIdx.x + 1];\n        __syncthreads();\n    }\n}",
            "// 1. Your code here\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[i] < 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// 1. Compute index in global memory\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  // 2. Make sure that thread is not accessing out-of-bound element of input vector\n  if (i >= N)\n    return;\n  // 3. Count the number of elements in the range [0, 10)\n  if (x[i] >= 0.0 && x[i] < 10.0) {\n    atomicAdd(&bins[0], 1);\n  }\n  if (x[i] >= 10.0 && x[i] < 20.0) {\n    atomicAdd(&bins[1], 1);\n  }\n  if (x[i] >= 20.0 && x[i] < 30.0) {\n    atomicAdd(&bins[2], 1);\n  }\n  if (x[i] >= 30.0 && x[i] < 40.0) {\n    atomicAdd(&bins[3], 1);\n  }\n  if (x[i] >= 40.0 && x[i] < 50.0) {\n    atomicAdd(&bins[4], 1);\n  }\n  if (x[i] >= 50.0 && x[i] < 60.0) {\n    atomicAdd(&bins[5], 1);\n  }\n  if (x[i] >= 60.0 && x[i] < 70.0) {\n    atomicAdd(&bins[6], 1);\n  }\n  if (x[i] >= 70.0 && x[i] < 80.0) {\n    atomicAdd(&bins[7], 1);\n  }\n  if (x[i] >= 80.0 && x[i] < 90.0) {\n    atomicAdd(&bins[8], 1);\n  }\n  if (x[i] >= 90.0 && x[i] < 100.0) {\n    atomicAdd(&bins[9], 1);\n  }\n}",
            "int tId = threadIdx.x;\n\n    if (tId < N) {\n        int bin = floor(x[tId] / 10);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "// The first call to atomicAdd will increment bins[0] by one and return the previous value\n  // of bins[0]. If the previous value was zero, the second call will return one and increment\n  // bins[1] by one. If the previous value was 1, the second call will return 2 and increment\n  // bins[2] by one. If the previous value was 2, the second call will return 3 and increment\n  // bins[3] by one. And so on...\n  //\n  // The final return value is the number of times that the atomicAdd was called with the\n  // value `1`.\n  const size_t binIndex = 10 * atomicAdd(bins, 0) / N;\n  atomicAdd(bins + binIndex, 1);\n}",
            "// The bins array is allocated in shared memory\n  extern __shared__ size_t sBins[];\n\n  // The index of the current thread\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Initialize the shared memory array\n  if (threadIdx.x < 10) {\n    sBins[threadIdx.x] = 0;\n  }\n\n  // Ensure all threads have finished writing to the shared memory array\n  __syncthreads();\n\n  // Add 1 to the appropriate bin if i is in the given range\n  if (i < N) {\n    int bin = floor(x[i] / 10.0);\n    atomicAdd(&sBins[bin], 1);\n  }\n\n  // Wait for all threads to finish writing to shared memory\n  __syncthreads();\n\n  // Copy the values from shared memory to the bins array\n  if (threadIdx.x < 10) {\n    bins[threadIdx.x] = sBins[threadIdx.x];\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        int bin = min(int(x[i] / 10), 9);\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    __shared__ size_t shared[10];\n\n    if (i < N) {\n        size_t bin_index = (size_t)x[i] / 10;\n        atomicAdd(&shared[bin_index], 1);\n    }\n    __syncthreads();\n\n    if (threadIdx.x < 10) {\n        atomicAdd(&bins[threadIdx.x], shared[threadIdx.x]);\n    }\n}",
            "__shared__ size_t sharedBins[10];\n\n    // Each thread sets the value of one bin\n    size_t threadIdx = threadIdx.x;\n    if (threadIdx >= 10) {\n        return;\n    }\n\n    size_t binIdx = 0;\n    if (threadIdx < 10) {\n        sharedBins[threadIdx] = 0;\n    }\n\n    // Iterate over the input values\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n        size_t value = (size_t)x[i];\n        if (value >= binIdx * 10 && value < binIdx * 10 + 10) {\n            atomicAdd(&(sharedBins[binIdx]), 1);\n        }\n        binIdx = (binIdx + 1) % 10;\n    }\n\n    __syncthreads();\n\n    // Add up the shared memory values\n    if (threadIdx < 10) {\n        atomicAdd(&(bins[threadIdx]), sharedBins[threadIdx]);\n    }\n}",
            "/* Your code goes here */\n}",
            "int my_thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Iterate over values in x. \n  // Note that my_thread_id is used here instead of a for loop. \n  // This allows us to run a different kernel for each value in x. \n  // In the original example, N is a large number and we need to split the processing into multiple kernel calls. \n  while (my_thread_id < N) {\n    // Get value of x at my_thread_id\n    double xi = x[my_thread_id];\n    size_t bin = (size_t) floor(xi / 10);\n\n    // Update the correct bin\n    atomicAdd(&bins[bin], 1);\n\n    // Move on to next x value\n    my_thread_id += blockDim.x * gridDim.x;\n  }\n}",
            "unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N)\n        return;\n    const double val = x[idx];\n    const unsigned int bin = (unsigned int) (val / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    int bin = (int)(x[i] / 10.0);\n    atomicAdd(&bins[bin], 1);\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // TODO: Your code here\n}",
            "// Use grid/block/thread information to index into the data\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Check the index against the size of the input\n  if (i < N) {\n    // Use a modulo to map the input data to the bin\n    size_t bin = x[i] / 10;\n    // Ensure we're not overflowing our bin count\n    if (bin < 10) {\n      // Use atomic add to increment the bin value\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) { return; }\n    const int bin = int(x[tid]) / 10;\n    atomicAdd(&bins[bin], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  int bin = floor(x[i]/10);\n  atomicAdd(&bins[bin], 1);\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  // Convert each value to its corresponding bin\n  int bin = floor(x[i] / 10.0);\n\n  // Update the corresponding bin with an atomic increment\n  atomicAdd(&bins[bin], 1);\n}",
            "// TODO: Implement\n}",
            "// TODO: write kernel code\n}",
            "// TODO\n}",
            "// TODO: implement this\n  // Use modular arithmetic to put values in the right bin.\n  //  The following is an example of the binning algorithm.\n  //  int index = 0;\n  //  for (int i = 0; i < 10; i++) {\n  //    if (x[blockIdx.x] >= i * 10 && x[blockIdx.x] < i * 10 + 10) {\n  //      index = i;\n  //      break;\n  //    }\n  //  }\n  //  bins[index] += 1;\n}",
            "// TODO:\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "// You need to use `atomicAdd` to assign values to the `bins` array.\n    // See the atomicAdd documentation for an example:\n    // http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomicadd\n    int idx = threadIdx.x;\n    if (idx < 10) {\n        atomicAdd(&bins[idx], 0);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n\n  int bin = floor((*x) / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "// TODO: Your code here\n  int idx = threadIdx.x;\n  int blockIdx_x = blockIdx.x;\n  int numThreads = gridDim.x * blockDim.x;\n  int numBlocks = gridDim.x;\n  __shared__ size_t count[10];\n  if (blockIdx_x == 0 && idx == 0) {\n    for (int i = 0; i < 10; i++) {\n      count[i] = 0;\n    }\n  }\n  __syncthreads();\n  for (int i = blockIdx_x * blockDim.x + idx; i < N; i += numBlocks * numThreads) {\n    for (int j = 0; j < 10; j++) {\n      if (x[i] < 10 * (j + 1) && x[i] >= 10 * j) {\n        count[j] += 1;\n        break;\n      }\n    }\n  }\n  __syncthreads();\n  for (int i = 0; i < 10; i++) {\n    atomicAdd(&bins[i], count[i]);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int bin = (int)floor(x[i] / 10.0) - 1;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "extern __shared__ int s[];\n    int tid = threadIdx.x;\n\n    // initialize shared memory\n    if (tid < 10) s[tid] = 0;\n    __syncthreads();\n\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        size_t bin = (size_t) x[i] / 10;\n        atomicAdd(&s[bin], 1);\n    }\n    __syncthreads();\n    for (size_t i = 0; i < 10; i++) {\n        atomicAdd(&bins[i], s[i]);\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (tid < N) {\n\t\tsize_t bin = x[tid] / 10;\n\t\tatomicAdd(&bins[bin], 1);\n\t}\n}",
            "extern __shared__ double local[]; // 48B\n    unsigned int myIdx = blockDim.x * blockIdx.x + threadIdx.x;\n\n    for (size_t i = myIdx; i < N; i += blockDim.x * gridDim.x) {\n        double xi = x[i];\n        size_t bin = (size_t) (10 * xi);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Implement kernel\n}",
            "size_t i = threadIdx.x;\n    if (i >= N) return;\n    const double d = floor(x[i] / 10);\n    const int bin = min(9, max(0, d));\n    atomicAdd(&bins[bin], 1);\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) { return; }\n  const int bin = x[tid] / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "// TODO: implement me\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "// Fill in this function.\n}",
            "// each thread is assigned an index into the array\n    size_t idx = threadIdx.x;\n    if (idx >= N) return;\n\n    // compute bin index based on value\n    size_t binIdx = (int)x[idx] / 10;\n\n    // only increment if in-bounds\n    if (binIdx < 10) {\n        atomicAdd(&bins[binIdx], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n    if (tid < N) {\n        size_t bin = floor(x[tid] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  __shared__ size_t shm[BLOCK_SIZE];\n  if (idx < N)\n    shm[threadIdx.x] = 1;\n\n  __syncthreads();\n\n  for (size_t i = BLOCK_SIZE / 2; i > 0; i >>= 1) {\n    if (idx < N)\n      if (threadIdx.x < i)\n        shm[threadIdx.x] += shm[threadIdx.x + i];\n    __syncthreads();\n  }\n\n  if (idx < N)\n    atomicAdd(bins + (int) (x[idx] / 10.0), shm[0]);\n}",
            "}",
            "/* Your code goes here */\n}",
            "// TODO: implement kernel.\n    size_t thread_idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int bin = x[thread_idx];\n    atomicAdd(&bins[bin / 10], 1);\n}",
            "//\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N)\n    return;\n  size_t bin = floor(x[i] / 10.0);\n  atomicAdd(&bins[bin], 1);\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    int bin = (int)floor(x[index] / 10);\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement binsBy10Count.\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    int bin = (x[tid] + 0.5) / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  size_t bin = (size_t) (x[i] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n}",
            "__shared__ int count[10];\n\tsize_t bin = threadIdx.x / 10;\n\tsize_t countIndex = bin;\n\tsize_t threadIdx10 = threadIdx.x % 10;\n\tint startIndex = bin * 10;\n\tcount[countIndex] = 0;\n\t__syncthreads();\n\tif (threadIdx10 <= 9) {\n\t\tint count = 0;\n\t\tfor (int i = startIndex; i < N; i += blockDim.x) {\n\t\t\tif ((i < N) && (x[i] >= startIndex) && (x[i] < startIndex + 10)) {\n\t\t\t\tcount += 1;\n\t\t\t}\n\t\t}\n\t\tatomicAdd(&count[countIndex], count);\n\t}\n\t__syncthreads();\n\tfor (int i = 5; i > 0; i /= 2) {\n\t\tif (threadIdx.x < i) {\n\t\t\tatomicAdd(&count[countIndex], count[countIndex + i]);\n\t\t}\n\t\t__syncthreads();\n\t}\n\tif (threadIdx.x == 0) {\n\t\tbins[bin] = count[0];\n\t}\n}",
            "// Compute the number of values in each bin, using an atomicAdd\n  //\n  // Hint:\n  //   * Use modulo to determine which bin the value should go in.\n  //   * Use atomicAdd to add one to the bin value.\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //\n  //",
            "// TODO: Implement this kernel.\n}",
            "// TODO\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n\n    for (int i = tid; i < N; i += stride) {\n        // TODO\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (idx >= N) {\n    return;\n  }\n\n  size_t bin = static_cast<size_t>(x[idx] / 10);\n\n  atomicAdd(bins + bin, 1);\n}",
            "// Insert your code here.\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N) return;\n    double val = x[idx];\n    if (val >= 0 && val < 10) atomicAdd(&bins[0], 1);\n    else if (val >= 10 && val < 20) atomicAdd(&bins[1], 1);\n    else if (val >= 20 && val < 30) atomicAdd(&bins[2], 1);\n    else if (val >= 30 && val < 40) atomicAdd(&bins[3], 1);\n    else if (val >= 40 && val < 50) atomicAdd(&bins[4], 1);\n    else if (val >= 50 && val < 60) atomicAdd(&bins[5], 1);\n    else if (val >= 60 && val < 70) atomicAdd(&bins[6], 1);\n    else if (val >= 70 && val < 80) atomicAdd(&bins[7], 1);\n    else if (val >= 80 && val < 90) atomicAdd(&bins[8], 1);\n    else if (val >= 90 && val < 100) atomicAdd(&bins[9], 1);\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t bin;\n\n  if (tid < N) {\n    // calculate the bin number\n    bin = (int)(x[tid] / 10.0);\n\n    // atomically increment the count of values in the bin\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Implement here\n}",
            "// TODO:\n\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        bins[x[idx] / 10]++;\n    }\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    int bin = (int) (x[i] / 10);\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "/*\n    YOUR CODE HERE\n  */\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid >= N) {\n    return;\n  }\n\n  int bin = x[tid] / 10;\n\n  if (bin < 0) {\n    bin = 0;\n  }\n\n  if (bin >= 10) {\n    bin = 9;\n  }\n\n  atomicAdd(&bins[bin], 1);\n}",
            "/* YOUR CODE HERE */\n}",
            "extern __shared__ size_t *shared_bins;\n    size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= N)\n        return;\n    int bin = x[i] / 10;\n    atomicAdd(&shared_bins[bin], 1);\n}",
            "}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n  for (; idx < N; idx += stride)\n    atomicAdd(&bins[10 * x[idx]], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx >= N) return;\n\n    int x_val = (int) x[idx];\n\n    if (x_val >= 0 && x_val <= 10)\n        atomicAdd(bins, 1);\n}",
            "const size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        int bin = floor((x[idx])/10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  double value = x[i];\n  size_t bin = static_cast<size_t>(value / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    int i = x[tid] / 10;\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "// TODO\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        int i = x[tid] / 10;\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "// This function has a hardcoded number of bins (10)\n  // The number of blocks is 10\n  // The number of threads per block is N / 10\n  // Make sure the number of threads per block is a multiple of 32\n  // (if not, adjust the number of blocks to be a multiple of 32)\n\n  // You may find it useful to use the modulus (%) and division (/) operators\n  // Example:\n  //   int a = 12;\n  //   int b = 3;\n  //   int r = a % b; // r == 0\n  //   int q = a / b; // q == 4\n  //   int qr = q * b + r; // qr == 12\n\n  // Use gridDim to find the number of threads\n  int threadId = threadIdx.x;\n  int blockId = blockIdx.x;\n  int nThreadsPerBlock = blockDim.x;\n  int nBlocks = gridDim.x;\n\n  // Use blockIdx and threadIdx to find the index into the vector x\n  // int x_idx =???\n\n  // Use x_idx to read the value from the vector x\n  // double x_value = x[x_idx];\n\n  // Use x_value to find the bin\n  // int bin =???;\n\n  // Use atomicAdd to update the count in bins[bin]\n  atomicAdd(&bins[bin], 1);\n\n  // You do not need to return a value from this kernel\n}",
            "// Add your code here.\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  extern __shared__ size_t mybins[];\n  mybins[tid] = 0;\n\n  // We are calculating only for the \n  // values which are not out of range\n  if (bid*blockDim.x + tid < N) {\n    size_t bin = floor(x[bid * blockDim.x + tid] / 10);\n    atomicAdd(&(mybins[bin]), 1);\n  }\n\n  __syncthreads();\n\n  // Now adding the local bins to the global bins\n  if (tid < 10) {\n    for (int i = 0; i < blockDim.x; i++) {\n      atomicAdd(&bins[tid], mybins[i]);\n    }\n  }\n}",
            "// __shared__ size_t sh_bins[10];\n\n  __shared__ size_t sh_bins[20];\n\n  int idx = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (idx < 10) {\n    sh_bins[idx] = 0;\n  }\n  __syncthreads();\n\n  if (i < N) {\n    sh_bins[int((x[i] / 10.0))] += 1;\n  }\n  __syncthreads();\n\n  for (int d = 5; d > 0; d >>= 1) {\n    if (idx < d) {\n      sh_bins[idx] += sh_bins[idx + d];\n    }\n    __syncthreads();\n  }\n\n  if (idx == 0) {\n    bins[blockIdx.x] = sh_bins[0];\n  }\n}",
            "int i = threadIdx.x;\n  if (i >= N) return;\n  __syncthreads();\n  atomicAdd(&bins[(int)x[i] / 10], 1);\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    int bin = (int) (x[idx] / 10);\n    if (bin < 10) atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < N) {\n        // Write your code here.\n    }\n}",
            "}",
            "const int tid = threadIdx.x;\n    const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (gid < N) {\n        const double val = x[gid];\n        // use bins[floor(val / 10)] to store count of elements in bin\n        atomicAdd(&bins[floor(val / 10.0)], 1);\n    }\n}",
            "const size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id >= N) return;\n  const int bin = floor(x[id] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "int index = threadIdx.x;\n  if (index < N) {\n    int bin = (int)(x[index] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "/* TODO: your code here */\n}",
            "// TODO\n    // Determine the index of the current thread\n    int thread_index = threadIdx.x;\n\n    // TODO\n    // Determine if the current thread is responsible for a valid element of x.\n    // Use this condition to determine if the thread should perform the necessary calculations.\n    if (thread_index < N) {\n\n        // TODO\n        // Determine the bin index for the current element of x.\n        int bin_index = 0;\n\n        // TODO\n        // Update the corresponding bin index in `bins` to increment it by 1.\n        atomicAdd(&bins[bin_index], 1);\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i >= N) {\n        return;\n    }\n    size_t j = floor(x[i] / 10);\n    atomicAdd(&bins[j], 1);\n}",
            "int idx = threadIdx.x;\n  while (idx < N) {\n    int bin = (int)x[idx] / 10;\n    atomicAdd(&bins[bin], 1);\n    idx += blockDim.x;\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (i < N) {\n\t\tint bin = static_cast<int>(x[i] / 10);\n\t\tatomicAdd(&bins[bin], 1);\n\t}\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "// TODO\n}",
            "// your code here\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            bins.fill(0);\n        }\n\n        #pragma omp for schedule(static)\n        for (auto val : x) {\n            bins[int(floor(val / 10.0)) % 10] += 1;\n        }\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Each rank computes its own counts\n  std::array<size_t, 10> counts;\n  counts.fill(0);\n\n  #pragma omp parallel for\n  for(auto &v : x) {\n    counts[(int)(v / 10)]++;\n  }\n\n  // Combine counts from all ranks\n  if(rank == 0) {\n    std::array<size_t, 10> globalCounts;\n    globalCounts.fill(0);\n    for(int i = 1; i < size; i++) {\n      MPI_Recv(globalCounts.data(), 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int j = 0; j < 10; j++) {\n        globalCounts[j] += counts[j];\n      }\n    }\n    for(int i = 0; i < 10; i++) {\n      counts[i] = globalCounts[i];\n    }\n  } else {\n    MPI_Send(counts.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // Return counts on rank 0\n  if(rank == 0) {\n    bins = counts;\n  }\n\n}",
            "if (x.empty()) return;\n\n  int numRanks = 0;\n  int rank = 0;\n  int const numThreads = omp_get_max_threads();\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int const chunkSize = x.size() / numRanks;\n  std::vector<size_t> localBins(10, 0);\n\n  #pragma omp parallel for\n  for (size_t i = rank*chunkSize; i < (rank+1)*chunkSize; i++) {\n    size_t val = x[i] / 10;\n    localBins[val]++;\n  }\n\n  std::array<size_t, 10> recvBins;\n  if (rank == 0) {\n    for (int i = 1; i < numRanks; i++) {\n      MPI_Recv(&recvBins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        localBins[j] += recvBins[j];\n      }\n    }\n  }\n  else {\n    MPI_Send(&localBins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    bins = localBins;\n  }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            bins.fill(0);\n        }\n\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++) {\n            bins[x[i] / 10]++;\n        }\n    }\n}",
            "// Implement this function in parallel\n}",
            "// TODO\n}",
            "int world_size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  constexpr double lower_bound = 0;\n  constexpr double upper_bound = 100;\n  constexpr double bin_width = 10;\n\n  const size_t n = x.size();\n\n  if (rank == 0) {\n    // count local bins\n    std::array<size_t, 10> local_bins{};\n#pragma omp parallel for schedule(guided)\n    for (size_t i = 0; i < n; ++i) {\n      const int bin = std::floor((x[i] - lower_bound) / bin_width);\n      if (bin >= 0 && bin < 10) {\n        ++local_bins[bin];\n      }\n    }\n\n    // sum up counts for all bins\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    // count local bins\n    std::array<size_t, 10> local_bins{};\n#pragma omp parallel for schedule(guided)\n    for (size_t i = 0; i < n; ++i) {\n      const int bin = std::floor((x[i] - lower_bound) / bin_width);\n      if (bin >= 0 && bin < 10) {\n        ++local_bins[bin];\n      }\n    }\n\n    // sum up counts for all bins\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Partition the range of indexes across the ranks\n  std::vector<double> indexes = getIndexesOfBins(x);\n  std::vector<double> myIndexes;\n\n  if (rank == 0) {\n    myIndexes.insert(myIndexes.end(), indexes.begin(), indexes.begin() + size);\n  } else {\n    myIndexes.insert(myIndexes.end(), indexes.begin() + (rank - 1),\n                     indexes.begin() + (rank - 1) + size);\n  }\n\n  std::array<size_t, 10> myBins;\n\n  // Populate myBins\n  for (size_t i = 0; i < 10; i++) {\n    myBins[i] = 0;\n  }\n  std::vector<int> myBinsList(10, 0);\n\n  std::vector<double> myBinsKeys;\n  std::vector<double> myBinsValues;\n\n  // Count values in x that fall in the range of bins for each thread\n#pragma omp parallel for\n  for (int i = 0; i < myIndexes.size(); i++) {\n    double start = myIndexes[i];\n    double end = myIndexes[i] + 10;\n    for (size_t j = 0; j < x.size(); j++) {\n      if (x[j] >= start && x[j] < end) {\n        myBins[getBin(x[j])] += 1;\n      }\n    }\n  }\n\n  // Reduce myBins to bins\n  MPI_Reduce(myBins.data(), bins.data(), 10, MPI_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n}",
            "// TODO: Implement me\n}",
            "// TODO\n}",
            "const int size = x.size();\n  const int rank = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int r, rnk;\n  std::vector<double> x_loc(size);\n  std::array<size_t, 10> bins_loc = {};\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      x_loc[i] = x[i];\n    }\n  }\n  MPI_Bcast(x_loc.data(), size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::cout << \"Rank: \" << rank << \", size: \" << size << std::endl;\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    int bucket = int(x_loc[i] / 10);\n    if (bucket < 10) {\n      #pragma omp atomic\n      bins_loc[bucket] += 1;\n    }\n  }\n  MPI_Reduce(bins_loc.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "int numprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  size_t num = x.size();\n  size_t chunk_size = num / numprocs;\n  size_t remainder = num % numprocs;\n  \n  size_t my_num_elems = chunk_size;\n  if (rank < remainder) {\n    my_num_elems++;\n  }\n  \n  std::vector<size_t> my_bins(10, 0);\n  \n  // count values in my chunk of x\n  for (int i = 0; i < my_num_elems; i++) {\n    int bin = std::min(9, std::floor(x[i] / 10));\n    my_bins[bin]++;\n  }\n  \n  // collect counts from all processes\n  std::vector<size_t> recv_counts(numprocs, 0);\n  for (int r = 0; r < numprocs; r++) {\n    if (r < remainder) {\n      recv_counts[r] = 10;\n    } else {\n      recv_counts[r] = 10;\n    }\n  }\n  int disp = 0;\n  std::vector<size_t> my_bins_flat(10 * numprocs, 0);\n  MPI_Gatherv(my_bins.data(), 10, MPI_UNSIGNED_LONG, my_bins_flat.data(),\n    recv_counts.data(), &disp, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  \n  // store result in bins on rank 0\n  if (rank == 0) {\n    std::copy(my_bins_flat.begin(), my_bins_flat.end(), bins.begin());\n  }\n}",
            "// TODO\n}",
            "}",
            "int nRanks;\n  int myRank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n  int chunk = x.size() / nRanks;\n  int remainder = x.size() % nRanks;\n  int myStart = myRank * chunk + std::min(myRank, remainder);\n  int myStop = myStart + chunk + (myRank < remainder? 1 : 0);\n\n  std::array<size_t, 10> counts;\n  counts.fill(0);\n\n  for(int i = myStart; i < myStop; ++i) {\n    counts[x[i] / 10]++;\n  }\n\n  // combine all counts into myRank == 0\n  MPI_Reduce(MPI_IN_PLACE, counts.data(), 10, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(myRank == 0) {\n    std::copy(counts.begin(), counts.end(), bins.begin());\n  }\n}",
            "int my_rank;\n    int comm_size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    // your code here\n\n}",
            "// TODO: Implement this function.\n  // Hint: You will need to use std::count_if.\n}",
            "// TODO: Implement the function. Use OpenMP to parallelize the counting and\n  //       MPI to distribute the work.\n  MPI_Init(NULL, NULL);\n\n  int rank, size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int sizeOfChunk = x.size() / size;\n  int lastSize = 0;\n  if(rank == 0) {\n    lastSize = x.size() % size;\n  }\n\n  int first, last;\n  first = rank * sizeOfChunk;\n  last = first + sizeOfChunk;\n  if (rank == 0) {\n    last += lastSize;\n  }\n\n  for (int i = first; i < last; i++) {\n    int bin = (int)x.at(i) / 10;\n    if (bin >= 0 && bin <= 10)\n      bins[bin]++;\n  }\n\n  for (int i = 0; i < 10; i++) {\n    int global = 0;\n    MPI_Reduce(&bins[i], &global, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n      bins[i] = global;\n    }\n  }\n\n  MPI_Finalize();\n\n}",
            "}",
            "int size;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    auto localSize = (x.size() + size - 1) / size;\n    auto localX = x;\n    std::array<size_t, 10> localBins{0};\n\n    if (rank == 0) {\n        localBins = bins;\n    }\n\n    if (rank == 0) {\n        for (auto i = 1; i < size; ++i) {\n            MPI_Send(&localX[i * localSize], localSize, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank > 0) {\n        MPI_Recv(&localX[0], localSize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (auto i = 0; i < localSize; ++i) {\n        if (localX[i] < 10) {\n            localBins[0]++;\n        }\n        else if (localX[i] < 20) {\n            localBins[1]++;\n        }\n        else if (localX[i] < 30) {\n            localBins[2]++;\n        }\n        else if (localX[i] < 40) {\n            localBins[3]++;\n        }\n        else if (localX[i] < 50) {\n            localBins[4]++;\n        }\n        else if (localX[i] < 60) {\n            localBins[5]++;\n        }\n        else if (localX[i] < 70) {\n            localBins[6]++;\n        }\n        else if (localX[i] < 80) {\n            localBins[7]++;\n        }\n        else if (localX[i] < 90) {\n            localBins[8]++;\n        }\n        else {\n            localBins[9]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "bins.fill(0);\n\n  // CODE HERE\n  const int nproc = omp_get_num_procs();\n\n  //#pragma omp parallel\n  {\n    //omp_set_num_threads(2);\n    int rank = omp_get_thread_num();\n    int nthreads = omp_get_num_threads();\n    //printf(\"Rank %d with %d threads\\n\",rank,nthreads);\n    int size = x.size();\n    //printf(\"Rank %d has %d elements\\n\",rank,size);\n    //int chunk_size = size/nthreads;\n    //printf(\"Rank %d with chunk %d\\n\",rank,chunk_size);\n    //int my_start_index = rank*chunk_size;\n    //int my_end_index = my_start_index+chunk_size;\n    //printf(\"Rank %d has range %d to %d\\n\",rank,my_start_index,my_end_index);\n    //if(rank == nproc-1) my_end_index = size;\n    //printf(\"Rank %d has range %d to %d\\n\",rank,my_start_index,my_end_index);\n\n    #pragma omp for\n    for (int i = 0; i < size; i++) {\n      //printf(\"Rank %d is at %d\\n\",rank,i);\n      double value = x[i];\n      int bin_index = value / 10;\n      bins[bin_index]++;\n      //printf(\"Rank %d counts %d at %d\\n\",rank,bin_index,i);\n    }\n\n    //int my_start_index = rank*chunk_size;\n    //int my_end_index = my_start_index+chunk_size;\n    //if(rank == nproc-1) my_end_index = size;\n    //printf(\"Rank %d has range %d to %d\\n\",rank,my_start_index,my_end_index);\n    //printf(\"Rank %d has %d elements\\n\",rank,size);\n    //#pragma omp for\n    //for (int i = my_start_index; i < my_end_index; i++) {\n    //  //printf(\"Rank %d is at %d\\n\",rank,i);\n    //  double value = x[i];\n    //  int bin_index = value / 10;\n    //  bins[bin_index]++;\n    //  //printf(\"Rank %d counts %d at %d\\n\",rank,bin_index,i);\n    //}\n  }\n}",
            "// TODO: Your code here\n}",
            "size_t n = x.size();\n  bins.fill(0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int chunk_size = n/size;\n  int remainder = n%size;\n  int chunk_start, chunk_end;\n  chunk_start = chunk_size*rank;\n  chunk_end = chunk_start + chunk_size - 1;\n  if (rank == size - 1) {\n    chunk_end += remainder;\n  }\n  std::vector<int> local_bins(10, 0);\n  if (rank == 0) {\n    std::vector<int> total_bins(10, 0);\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(local_bins.data(), 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        total_bins[j] += local_bins[j];\n      }\n    }\n    for (int i = 0; i < 10; i++) {\n      bins[i] = total_bins[i];\n    }\n  } else {\n    for (int i = chunk_start; i <= chunk_end; i++) {\n      int bin = (int) (x[i]/10.0);\n      local_bins[bin]++;\n    }\n    MPI_Send(local_bins.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int mpi_size, mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  // TODO: Use MPI and OpenMP to calculate the counts.\n\n  // NOTE: \n  // 1. The size of bins should be equal to the number of bins, i.e., 10 in this case\n  // 2. bins[i] should be the number of elements in [i*10, i*10 + 10)\n  //    Note: this is *inclusive*. For example, in [7, 17), 7 is counted!\n  //    This is because of the semantics of std::lower_bound.\n  // 3. You must use std::lower_bound to find the element in bins corresponding to the\n  //    value v. You may want to create a helper function for this.\n}",
            "bins = {};\n\n    auto const n = x.size();\n\n    // TODO 1: create a local copy of bins on each rank\n    #pragma omp parallel\n    {\n        // TODO 2: count on the local copy\n\n        // TODO 3: combine bins from each thread\n    }\n}",
            "//...\n}",
            "// TODO: Fill in the rest of the code here\n\n}",
            "// TODO\n}",
            "// TODO: your code here\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int start = rank * x.size() / size;\n  int end = (rank + 1) * x.size() / size;\n\n  std::vector<double> my_x;\n\n  if (rank == 0)\n  {\n    for (int i = 0; i < size; i++)\n      my_x.resize(x.size() / size);\n  }\n\n  for (int i = start; i < end; i++)\n    my_x.push_back(x.at(i));\n\n  for (int i = 0; i < bins.size(); i++)\n    bins.at(i) = 0;\n\n  int max = 0;\n  int min = 100;\n\n  for (int i = 0; i < my_x.size(); i++)\n  {\n    if (my_x.at(i) > max)\n      max = my_x.at(i);\n\n    if (my_x.at(i) < min)\n      min = my_x.at(i);\n  }\n\n  int part = ceil((double)(max - min) / 10);\n\n  for (int i = 0; i < my_x.size(); i++)\n  {\n    for (int j = 0; j < 10; j++)\n    {\n      if (my_x.at(i) > j * part && my_x.at(i) <= (j + 1) * part)\n        bins.at(j)++;\n    }\n  }\n\n  if (rank == 0)\n  {\n    for (int i = 1; i < size; i++)\n    {\n      MPI_Recv(&bins, 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      for (int j = 0; j < 10; j++)\n        bins.at(j) += bins.at(j);\n    }\n  }\n  else\n  {\n    MPI_Send(&bins, 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "size_t local_size = x.size();\n  double local_min = 0;\n  double local_max = 100;\n  double local_bin_width = (local_max - local_min) / bins.size();\n  std::vector<size_t> local_bins(bins.size(), 0);\n  #pragma omp parallel for schedule(guided)\n  for (size_t i = 0; i < local_size; i++) {\n    int bin_index = (int)((x[i] - local_min) / local_bin_width);\n    if (bin_index >= 0 && bin_index < bins.size()) {\n      #pragma omp atomic update\n      local_bins[bin_index]++;\n    }\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), local_bins.size(), MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/*... */\n\n}",
            "// TODO\n}",
            "/* Your solution goes here. */\n}",
            "// TODO: Implement this function\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// Implement this function\n}",
            "// TODO\n    bins.fill(0);\n}",
            "int size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int* bins_int = new int[10];\n  // count bins in the vector\n  for(int i = 0; i < 10; i++){\n    bins_int[i] = 0;\n    for(int j = 0; j < x.size(); j++){\n      if(x[j] >= i * 10 && x[j] <= i * 10 + 9){\n        bins_int[i]++;\n      }\n    }\n  }\n  // MPI\n  int chunk = x.size() / size;\n  int rem = x.size() % size;\n  double *subvector = new double[chunk];\n  for (int i = 1; i < size; i++){\n    MPI_Send(&bins_int[0], 10, MPI_INT, i, 0, MPI_COMM_WORLD);\n  }\n  // OpenMP\n  #pragma omp parallel for\n  for (int i = 0; i < chunk; i++){\n    for(int j = 0; j < 10; j++){\n      if(subvector[i] >= j * 10 && subvector[i] <= j * 10 + 9){\n        bins_int[j]++;\n      }\n    }\n  }\n\n}",
            "int size = x.size();\n    bins = std::array<size_t, 10>();\n    std::fill(bins.begin(), bins.end(), 0);\n    for (int i=0; i<size; i++) {\n        int bin = x[i] / 10;\n        bins[bin]++;\n    }\n}",
            "/*\n      TODO\n    */\n}",
            "// Your code here\n}",
            "// Fill your code here.\n}",
            "// TODO\n\n  return;\n}",
            "// TODO: replace this with an MPI scatter/allgather\n\n    // TODO: replace this with an OpenMP parallel for\n}",
            "// TODO\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  #pragma omp parallel\n  {\n    int threadId = omp_get_thread_num();\n    int numThreads = omp_get_num_threads();\n    int subArray = x.size() / numThreads;\n    int subStart = subArray * threadId;\n    int subEnd = (subArray * (threadId + 1)) - 1;\n    if(subEnd > x.size() - 1)\n      subEnd = x.size() - 1;\n\n    for(int i = subStart; i <= subEnd; i++){\n      int bin = (x[i] / 10) + 1;\n      #pragma omp atomic update\n      bins[bin]++;\n    }\n  }\n}",
            "// TODO: your code here\n    int size, rank, lsize;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk = x.size() / size;\n    lsize = x.size() - (chunk * (size - 1));\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Send(&x[i * chunk], chunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n        MPI_Send(&x[0], lsize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<double> loc(chunk);\n        if (rank!= 0) {\n            MPI_Recv(&loc[0], chunk, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        int temp = 0;\n        for (int j = 0; j < loc.size(); ++j) {\n            if (loc[j] > 0 && loc[j] < 10) {\n                temp++;\n            }\n        }\n        MPI_Send(&temp, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        std::vector<int> temp(size, 0);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&temp[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        for (int i = 0; i < temp.size(); ++i) {\n            bins[i] = temp[i];\n        }\n    }\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  int numRanks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  const size_t chunkSize = x.size() / numRanks;\n  size_t startIndex = rank * chunkSize;\n  size_t endIndex = startIndex + chunkSize;\n  if (rank == numRanks - 1)\n    endIndex = x.size();\n\n  std::vector<size_t> rankBins(10, 0);\n#pragma omp parallel for\n  for (size_t i = startIndex; i < endIndex; ++i) {\n    size_t bin = std::min(static_cast<size_t>(x[i] / 10), 9u);\n    rankBins[bin]++;\n  }\n\n  MPI_Reduce(rankBins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: fill this in\n}",
            "/*... */\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  /* Your solution goes here. */\n\n}",
            "const int nranks = omp_get_num_threads();\n  const int rank = omp_get_thread_num();\n\n  std::vector<double> local_x;\n  std::vector<size_t> local_bins;\n\n  if (rank == 0) {\n    int size = x.size();\n    int chunk_size = size / nranks;\n    int remain = size % nranks;\n    std::vector<int> counts(nranks, chunk_size);\n    counts[0] += remain;\n\n    MPI_Request* req = new MPI_Request[nranks];\n    int offset = 0;\n    for (int i = 0; i < nranks; i++) {\n      MPI_Isend(&size, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &req[i]);\n    }\n\n    for (int i = 1; i < nranks; i++) {\n      MPI_Recv(&local_x[0], counts[i], MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    MPI_Waitall(nranks, req, MPI_STATUSES_IGNORE);\n\n    for (int i = 1; i < nranks; i++) {\n      MPI_Send(&local_x[0], counts[i], MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    int size = 0;\n    MPI_Recv(&size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    local_x.resize(size);\n\n    MPI_Recv(&local_x[0], size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    MPI_Send(&size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\n    MPI_Send(&local_x[0], size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n  }\n\n  for (double xi : local_x) {\n    int bin = xi / 10;\n    if (bin == 10) {\n      local_bins[9]++;\n    } else {\n      local_bins[bin]++;\n    }\n  }\n\n  if (rank == 0) {\n    int size = local_bins.size();\n    int chunk_size = size / nranks;\n    int remain = size % nranks;\n    std::vector<int> counts(nranks, chunk_size);\n    counts[0] += remain;\n\n    MPI_Request* req = new MPI_Request[nranks];\n    int offset = 0;\n    for (int i = 0; i < nranks; i++) {\n      MPI_Isend(&size, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &req[i]);\n    }\n\n    for (int i = 1; i < nranks; i++) {\n      MPI_Recv(&local_bins[0], counts[i], MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    MPI_Waitall(nranks, req, MPI_STATUSES_IGNORE);\n\n    for (int i = 1; i < nranks; i++) {\n      MPI_Send(&local_bins[0], counts[i], MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    int size = 0;\n    MPI_Recv(&size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    local_bins.resize(size);\n\n    MPI_Recv(&local",
            "// TODO: Implement\n}",
            "// TODO\n  if(x.size() == 0) return;\n\n  const size_t num_threads = omp_get_max_threads();\n\n  // TODO: Initialize bins\n\n  std::vector<size_t> bins_per_thread(num_threads,0);\n  std::vector<size_t> bins_per_thread_offset(num_threads+1);\n  bins_per_thread_offset[0] = 0;\n  for(int i = 1; i <= num_threads; i++)\n  {\n    bins_per_thread_offset[i] = bins_per_thread_offset[i-1] + x.size()/num_threads;\n    if(i == num_threads)\n      bins_per_thread_offset[i] = x.size();\n  }\n\n  #pragma omp parallel for\n  for(int i = 0; i < num_threads; i++)\n  {\n    #pragma omp critical\n    {\n      for(int j = bins_per_thread_offset[i]; j < bins_per_thread_offset[i+1]; j++)\n      {\n        if(x[j] >= 10*i && x[j] < 10*(i+1))\n        {\n          bins_per_thread[i]++;\n        }\n      }\n    }\n  }\n\n  for(int i = 1; i <= num_threads; i++)\n    bins[i-1] = bins_per_thread[i-1];\n\n  if(bins_per_thread_offset[0] > 0)\n  {\n    for(int i = 0; i < bins_per_thread_offset[1]; i++)\n    {\n      if(x[i] >= 0 && x[i] < 10)\n        bins[0]++;\n    }\n  }\n\n  // TODO: gather results from each thread to bins on rank 0\n\n}",
            "std::array<size_t, 10> local_bins;\n\n    //...\n\n    // Send `local_bins` to rank 0.\n    //...\n\n    // Receive `local_bins` from rank 0.\n    //...\n\n    // Merge results.\n    for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = local_bins[i] + bins[i];\n    }\n}",
            "// TODO: Implement this\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int my_bins[10];\n    for (int i = 0; i < 10; i++) my_bins[i] = 0;\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < x.size(); i++)\n        my_bins[std::floor(x[i] / 10.0)]++;\n\n    int recv_bins[10];\n    for (int i = 0; i < 10; i++)\n        recv_bins[i] = 0;\n\n    int* send_bins = (int*) malloc(10 * sizeof(int));\n    for (int i = 0; i < 10; i++)\n        send_bins[i] = my_bins[i];\n\n    int left = rank - 1;\n    int right = rank + 1;\n    if (rank == 0)\n        left = size - 1;\n    if (rank == size - 1)\n        right = 0;\n\n    MPI_Status status;\n    MPI_Sendrecv(send_bins, 10, MPI_INT, left, 0, recv_bins, 10, MPI_INT, right, 0, MPI_COMM_WORLD, &status);\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++)\n            bins[i] = recv_bins[i];\n    }\n\n    for (int i = 0; i < 10; i++)\n        bins[i] += my_bins[i];\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++)\n            std::cout << i * 10 << \" - \" << (i + 1) * 10 - 1 << \": \" << bins[i] << std::endl;\n    }\n}",
            "const int root = 0;\n\n  // your code goes here\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < bins.size(); i++){\n      bins[i] = 0;\n    }\n    #pragma omp for\n    for (int i = 0; i < x.size(); i++){\n      int bin = x[i] / 10;\n      if (bin == 10)\n        bin = 9;\n      #pragma omp atomic\n      bins[bin]++;\n    }\n  }\n}",
            "int numProcessors, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcessors);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  const int x_per_proc = x.size() / numProcessors;\n  const int x_remainder = x.size() % numProcessors;\n\n  // initialize bins to zero\n  std::fill(bins.begin(), bins.end(), 0);\n\n  std::vector<double> x_proc;\n  if (rank == 0) {\n    // if rank 0, initialize x_proc with the last x_remainder elements\n    // of x\n    x_proc = std::vector<double>(x.end() - x_remainder, x.end());\n  } else {\n    // if rank other than 0, initialize x_proc with the first x_per_proc\n    // elements of x\n    x_proc = std::vector<double>(x.begin(), x.begin() + x_per_proc);\n  }\n\n  for (auto i : x_proc) {\n    int bin = i / 10;\n    bins[bin] += 1;\n  }\n\n  // now, gather the bins of all processors into processor 0\n  std::vector<int> bins_mpi(bins.size());\n  MPI_Gather(bins.data(), bins.size(), MPI_INT, bins_mpi.data(), bins.size(),\n             MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < numProcessors; ++i) {\n      for (int j = 0; j < bins_mpi.size(); ++j) {\n        bins[j] += bins_mpi[i * bins.size() + j];\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "const int rank = omp_get_thread_num();\n    if (rank == 0) {\n        const size_t n = x.size();\n        const size_t n_per_thread = n / omp_get_num_threads();\n        const size_t n_remainder = n % omp_get_num_threads();\n        size_t index = 0;\n        for (int tid = 0; tid < omp_get_num_threads(); ++tid) {\n            const size_t begin = tid * n_per_thread;\n            const size_t end = begin + n_per_thread;\n            if (tid == 0) {\n                for (size_t i = 0; i < n_remainder; ++i) {\n                    size_t bin = x[begin + i] / 10;\n                    if (bin < 10) {\n                        bins[bin]++;\n                    }\n                }\n            }\n            else {\n                for (size_t i = begin; i < end; ++i) {\n                    size_t bin = x[i] / 10;\n                    if (bin < 10) {\n                        bins[bin]++;\n                    }\n                }\n            }\n        }\n    }\n    else {\n    }\n}",
            "// TODO\n\n}",
            "/* TODO */\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    int *c = new int[10];\n    for (int i = 0; i < 10; i++)\n      c[i] = 0;\n    for (int i = 0; i < x.size(); i++) {\n      int a = (int)floor((x[i] / 10));\n      c[a]++;\n    }\n    MPI_Scatter(c, 1, MPI_INT, &bins, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    delete c;\n  } else {\n    int *c = new int[10];\n    MPI_Scatter(nullptr, 1, MPI_INT, c, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < x.size(); i++) {\n      int a = (int)floor((x[i] / 10));\n      c[a]++;\n    }\n    MPI_Gather(c, 1, MPI_INT, &bins, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    delete c;\n  }\n}",
            "// TODO: fill in this function\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  int local_size = x.size() / size;\n  int extra = x.size() % size;\n  int first = rank * local_size;\n  int last = first + local_size;\n  std::vector<double> local_x(x.begin()+first, x.begin()+last);\n\n  std::array<size_t, 10> local_bins{};\n\n  #pragma omp parallel for\n  for (size_t i=0; i<local_x.size(); i++) {\n    local_bins[static_cast<int>(local_x[i] / 10)]++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n\n    // TODO\n\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int rank_id;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n    int rank_id_from_right;\n    if(rank_id < num_ranks-1) rank_id_from_right = rank_id + 1;\n    else rank_id_from_right = 0;\n\n    // Partition data\n    std::vector<double> x_rank;\n    for(size_t i = rank_id; i < x.size(); i+=num_ranks) {\n        x_rank.push_back(x[i]);\n    }\n\n    // Count bins locally\n    std::array<size_t, 10> bins_rank;\n    std::fill(bins_rank.begin(), bins_rank.end(), 0);\n    for(double num : x_rank) {\n        if(num >= 10*rank_id && num < 10*rank_id_from_right) {\n            bins_rank[num/10]++;\n        }\n    }\n\n    // Reduce counts to 1 rank\n    std::vector<size_t> bins_size(10);\n    std::fill(bins_size.begin(), bins_size.end(), 0);\n    for(size_t num : bins_rank) bins_size.push_back(num);\n    std::vector<size_t> bins_received;\n    if(rank_id == 0) {\n        bins_received.resize(10*num_ranks);\n        for(int i=1; i<num_ranks; ++i) {\n            MPI_Status status;\n            MPI_Recv(&bins_received[10*i], 10, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n    else {\n        MPI_Send(&bins_size[0], 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Reduce counts\n    if(rank_id == 0) {\n        for(int i=1; i<num_ranks; ++i) {\n            for(int j=0; j<10; ++j) {\n                bins[j] += bins_received[10*i+j];\n            }\n        }\n    }\n\n}",
            "int my_rank, comm_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    auto n = x.size();\n    auto num_bins = 10;\n    auto num_threads = omp_get_max_threads();\n    auto part_size = n / comm_size;\n    auto leftover = n % comm_size;\n\n    std::vector<int> local_bins(num_bins, 0);\n\n    // Partition the input vector for this rank\n    // Use part_size for all ranks except the last one, which will get\n    // part_size + leftover elements.\n    auto start_idx = part_size * my_rank;\n    auto end_idx = start_idx + (my_rank < comm_size - 1? part_size : part_size + leftover);\n\n    // #pragma omp parallel for\n    for (auto i = start_idx; i < end_idx; ++i) {\n        local_bins[x[i] / 10]++;\n    }\n\n    // Now all ranks have their own copy of local_bins\n    // Reduce each local_bins to get bins on rank 0\n    // If n is small, use the built-in MPI_Reduce, otherwise use MPI_Reduce_scatter\n    // to avoid a huge reduction tree.\n    if (n < comm_size * num_threads * 20) {\n        MPI_Reduce(local_bins.data(), bins.data(), num_bins, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        std::vector<int> local_bins_sizes(comm_size, 0);\n        local_bins_sizes[my_rank] = num_bins;\n        MPI_Reduce_scatter(local_bins.data(), bins.data(), local_bins_sizes.data(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    }\n}",
            "auto const n = x.size();\n\n    // count how many values lie in each interval [i, i+10)\n    for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n        // Count the elements between i and i+10, both inclusive.\n        // For example, if x = [1, 3, 4, 12, 35] and i = 0, the bins would contain [1, 3],\n        // which has a size of 2\n    }\n}",
            "size_t const size = x.size();\n    size_t const num_threads = 4;\n\n    // TODO: Implement\n    return;\n}",
            "auto comm = MPI_COMM_WORLD;\n    int rank, size;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n\n    int num_threads = omp_get_max_threads();\n\n    std::vector<size_t> bins_thread(10, 0);\n\n    auto start_rank = rank;\n    auto end_rank = rank + 1;\n    while (end_rank < size) {\n        std::vector<size_t> bins_thread_rank(10, 0);\n        for (int t = 0; t < num_threads; t++) {\n            auto start = x.begin() + (rank * num_threads + t) * x.size() / size / num_threads;\n            auto end = x.begin() + (rank * num_threads + t + 1) * x.size() / size / num_threads;\n            for (auto it = start; it < end; it++) {\n                int bin_id = *it / 10;\n                bins_thread_rank[bin_id]++;\n            }\n        }\n        MPI_Reduce(bins_thread_rank.data(), bins_thread.data(), 10, MPI_INT, MPI_SUM,\n                   start_rank, comm);\n        start_rank = start_rank + 1;\n        end_rank = end_rank + 1;\n    }\n\n    if (rank == 0) {\n        bins = bins_thread;\n    }\n}",
            "// TODO\n}",
            "}",
            "// TODO: implement this function\n    // YOUR CODE HERE\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    for (auto& b : bins) {\n      b = 0;\n    }\n\n    for (int i = 1; i < size; i++) {\n      MPI_Status status;\n      MPI_Recv(&bins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    std::array<size_t, 10> myBins;\n    for (auto& b : myBins) {\n      b = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n      int idx = x[i] / 10;\n      myBins[idx]++;\n    }\n\n    MPI_Send(&myBins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int const num_threads = omp_get_max_threads();\n    int const world_size = 1; // MPI_COMM_WORLD_SIZE;\n    int const world_rank = 0; // MPI_COMM_WORLD_RANK;\n    int const root = 0;\n\n    std::vector<std::array<size_t, 10>> bins_by_thread(num_threads);\n\n    #pragma omp parallel for\n    for(int i = 0; i < x.size(); ++i)\n    {\n        int const thread_num = omp_get_thread_num();\n        std::array<size_t, 10> &local_bins = bins_by_thread[thread_num];\n        size_t const v = x[i];\n\n        if (v >= 0 && v < 10) local_bins[0]++;\n        else if (v >= 10 && v < 20) local_bins[1]++;\n        else if (v >= 20 && v < 30) local_bins[2]++;\n        else if (v >= 30 && v < 40) local_bins[3]++;\n        else if (v >= 40 && v < 50) local_bins[4]++;\n        else if (v >= 50 && v < 60) local_bins[5]++;\n        else if (v >= 60 && v < 70) local_bins[6]++;\n        else if (v >= 70 && v < 80) local_bins[7]++;\n        else if (v >= 80 && v < 90) local_bins[8]++;\n        else if (v >= 90 && v <= 100) local_bins[9]++;\n    }\n\n    if(world_rank == root) {\n        for(int i = 0; i < num_threads; ++i) {\n            for(int j = 0; j < 10; ++j) {\n                bins[j] += bins_by_thread[i][j];\n            }\n        }\n    }\n}",
            "// TODO: Add code here.\n\n}",
            "}",
            "// Your code goes here!\n}",
            "// TODO\n\n}",
            "// Your code goes here!\n}",
            "// Write your code here!\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// TODO: implement me!\n}",
            "bins.fill(0);\n\n    // TODO: implement me\n}",
            "// TODO\n}",
            "const int world_size = omp_get_num_threads();\n    const int rank = omp_get_thread_num();\n    if (rank!= 0)\n    {\n        // count for this rank\n        bins[rank % 10] = countIf(x, [&rank](double x) { return (rank / 10) * 10 <= x && x < (rank / 10) * 10 + 10; });\n\n        MPI_Send(&bins, 1, MPI_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n    else\n    {\n        // sum counts from all ranks\n        for (int i = 1; i < world_size; i++)\n        {\n            MPI_Recv(&bins, 1, MPI_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    }\n}",
            "/* Your code goes here */\n\n}",
            "}",
            "const int size = 10;\n  int my_size = x.size();\n\n  std::array<size_t, 10> my_bins{};\n\n  // Initialize my_bins\n  for(size_t i = 0; i < size; ++i){\n    my_bins[i] = 0;\n  }\n\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int chunk = my_size / num_ranks;\n\n  int start = rank * chunk;\n  int end = (rank + 1) * chunk;\n  if(rank == num_ranks - 1)\n    end = my_size;\n\n  #pragma omp parallel for\n  for(int i = start; i < end; ++i){\n    int idx = x[i] / 10;\n    if(idx < size)\n      ++my_bins[idx];\n  }\n\n  MPI_Reduce(my_bins.data(), bins.data(), size, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::array<size_t, 10>> bins_local(size);\n\n    // use omp for loops\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        int bin_id = x[i] / 10;\n        ++bins_local[rank][bin_id];\n    }\n\n    std::vector<size_t> bins_total(size);\n    for (size_t i = 0; i < size; ++i) {\n        bins_total[i] = bins_local[i][0] + bins_local[i][1] +\n                        bins_local[i][2] + bins_local[i][3] +\n                        bins_local[i][4] + bins_local[i][5] +\n                        bins_local[i][6] + bins_local[i][7] +\n                        bins_local[i][8] + bins_local[i][9];\n    }\n\n    std::array<size_t, 10> bins_total_reduced;\n    MPI_Reduce(&bins_total[0], &bins_total_reduced[0], size, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; ++i) {\n            bins[i] = bins_total_reduced[i];\n        }\n    }\n}",
            "}",
            "/* Implement this function */\n}",
            "/* TODO */\n}",
            "// TODO\n\n}",
            "// TODO: replace this with your code\n  bins.fill(0);\n}",
            "bins = {};\n    std::vector<double> xLocal(x);\n\n    int numberOfRanks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numberOfRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    #pragma omp parallel for\n    for (int i = 0; i < xLocal.size(); ++i) {\n        int bin = xLocal[i] / 10;\n        #pragma omp atomic\n        ++bins[bin];\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "if (x.size() == 0) {\n        return;\n    }\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n        for (size_t i = 0; i < x.size(); i++) {\n            int bin = x[i] / 10;\n            bins[bin] += 1;\n        }\n    }\n\n    if (rank!= 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    // TODO: Implement parallel version of above algorithm.\n    // You can use OpenMP or MPI.\n    // You may assume that x is equally divided between ranks.\n}",
            "// TODO\n}",
            "int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<int> binCounts(nprocs*10);\n    const int N = x.size();\n    const int chunk = N/nprocs;\n    const int rem = N%nprocs;\n    std::vector<double> localX(chunk+rem);\n    if(rank == 0) {\n        for(int i=0; i<N; ++i) {\n            int proc = i/chunk;\n            int index = i%chunk;\n            if(index < chunk) {\n                localX[index] = x[i];\n            } else {\n                localX[index+rem] = x[i];\n            }\n        }\n    }\n    MPI_Scatter(localX.data(), chunk+rem, MPI_DOUBLE, binCounts.data(), chunk+rem, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for(int i=0; i<10; ++i) {\n        binCounts[rank*10+i] = std::count_if(binCounts.begin(), binCounts.end(), [&](double val) { return val >= i*10 && val < (i+1)*10; });\n    }\n\n    MPI_Gather(binCounts.data()+rank*10, 10, MPI_INT, bins.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "/* Implement this function. */\n}",
            "// TODO\n}",
            "int num_ranks, my_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  const int max_bin = 10;\n\n  /* Your code goes here */\n\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n\n  // Hints:\n  // - you must use OpenMP to parallelize the loop over `i`\n  // - if you use OpenMP, you need to decide if you want to parallelize over the\n  //   loop over `j` or over the loop over `k`. It is not possible to parallelize\n  //   both the loops over `j` and `k`.\n  // - you must use MPI to distribute the work between different ranks.\n  //   You must decide how to use MPI to achieve this goal. For example, you\n  //   could use a loop over the rank ids to send data to the other ranks or you\n  //   could use a loop over the data to send it to the other ranks.\n  //   Hint: if you use MPI_Send to send the values of `x` to the other ranks,\n  //   you must ensure that the data is contiguous and the order of the data is\n  //   the same on all ranks.\n  // - you must use MPI_Reduce to reduce the values of `bins` on all ranks to\n  //   the result on rank 0.\n  // - you must use MPI_Reduce on rank 0 to gather the values of `bins` from\n  //   all the other ranks\n  // - you must use MPI_Barrier to wait for all ranks to finish their work\n  //   before starting to do other work\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "// Add your code here\n\n}",
            "// TODO: your code here\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    /* Your solution goes here  */\n}",
            "MPI_Comm world;\n    int rank;\n    MPI_Comm_rank(world, &rank);\n\n    if (rank == 0){\n        std::fill(std::begin(bins), std::end(bins), 0);\n    }\n\n    const int num_threads = omp_get_max_threads();\n\n    size_t local_size = x.size() / num_threads;\n\n    std::vector<size_t> local_bins(10);\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        int thread_id = omp_get_thread_num();\n\n        size_t start = thread_id * local_size;\n        size_t end = start + local_size;\n\n        if (thread_id == num_threads - 1){\n            end = x.size();\n        }\n\n        std::fill(std::begin(local_bins), std::end(local_bins), 0);\n\n        for (size_t i = start; i < end; i++){\n            int bin_id = x[i] / 10;\n            if (bin_id >= 0 && bin_id < 10){\n                local_bins[bin_id]++;\n            }\n        }\n\n        MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, world);\n\n    }\n\n}",
            "/* TODO */\n}",
            "// TODO: Implement the parallel bin count\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int my_bins[10];\n  memset(my_bins, 0, sizeof(my_bins));\n\n  if (rank == 0) {\n    std::copy(std::begin(x), std::end(x), std::begin(bins));\n  } else {\n    std::copy(std::begin(x), std::end(x), std::begin(my_bins));\n  }\n\n  std::array<int, 10> bins_send;\n  memset(bins_send.data(), 0, sizeof(bins_send));\n\n  MPI_Reduce(my_bins, bins_send.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::copy(std::begin(bins_send), std::end(bins_send), std::begin(bins));\n  }\n}",
            "// TODO\n\n}",
            "/* TODO 1: Implement this function to complete the assignment. */\n}",
            "// This is a good place to start. You can change this function definition\n  // to take more parameters as you need them.\n\n  // Your code here\n\n  // We've provided this code to give you an idea of how to use MPI. You can change\n  // this code to accomplish what you need.\n  MPI_Barrier(MPI_COMM_WORLD);\n  // TODO 2: Use an OpenMP parallel loop to compute the histogram.\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++) {\n    // TODO 3: Compute the number of elements in [i*10, (i+1)*10).\n    int count = 0;\n    for (double element : x) {\n      if (element >= (i*10) && element < ((i+1)*10)) {\n        count++;\n      }\n    }\n    // TODO 4: Use MPI_Reduce to reduce the computed counts to rank 0.\n    MPI_Reduce(&count, &(bins[i]), 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// Your code here\n}",
            "// Your code goes here\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunks = x.size() / num_procs;\n    std::vector<double> partial(chunks);\n\n    #pragma omp parallel for\n    for (int i = 0; i < chunks; ++i) {\n        partial[i] = x[i * num_procs + rank];\n    }\n\n    std::vector<int> local_bins(10, 0);\n\n    for (int i = 0; i < chunks; ++i) {\n        int bin = partial[i] / 10;\n        if (bin == 10) bin = 9;\n        ++local_bins[bin];\n    }\n\n    std::vector<int> global_bins(10, 0);\n    MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::copy(global_bins.begin(), global_bins.end(), bins.begin());\n    }\n}",
            "int rank;\n    int procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<size_t> local_bins(10, 0);\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++)\n        local_bins[x[i] / 10]++;\n\n    //reduce all the counts and store the result in bins\n}",
            "// Fill code here\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    int worldSize, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    std::vector<size_t> myBins(10, 0);\n    if (rank == 0) {\n        for (int i = 0; i < n; i++) {\n            int a = (int)x[i];\n            int b = a / 10;\n            myBins[b] += 1;\n        }\n    }\n    else {\n        int s = n / worldSize;\n        int s2 = n % worldSize;\n        int start = (rank - 1) * s;\n        for (int i = 0; i < s; i++) {\n            int a = (int)x[start + i];\n            int b = a / 10;\n            myBins[b] += 1;\n        }\n        if (rank <= s2) {\n            for (int i = 0; i < s2; i++) {\n                int a = (int)x[n - s2 + i];\n                int b = a / 10;\n                myBins[b] += 1;\n            }\n        }\n    }\n\n    MPI_Reduce(myBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO: your code here\n}",
            "int mpi_size, mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  size_t n = x.size();\n  size_t n_per_rank = n/mpi_size;\n  std::vector<double> chunk(n_per_rank);\n  if (mpi_rank == 0) {\n    chunk.insert(chunk.end(), x.begin(), x.end()-n_per_rank);\n  } else {\n    chunk.insert(chunk.end(), x.begin()+mpi_rank*n_per_rank, x.begin()+(mpi_rank+1)*n_per_rank);\n  }\n  std::array<size_t, 10> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n  // TODO\n  for (size_t i = 0; i < n_per_rank; ++i) {\n    if (chunk[i] < 10) {\n      local_bins[0]++;\n    } else if (chunk[i] < 20) {\n      local_bins[1]++;\n    } else if (chunk[i] < 30) {\n      local_bins[2]++;\n    } else if (chunk[i] < 40) {\n      local_bins[3]++;\n    } else if (chunk[i] < 50) {\n      local_bins[4]++;\n    } else if (chunk[i] < 60) {\n      local_bins[5]++;\n    } else if (chunk[i] < 70) {\n      local_bins[6]++;\n    } else if (chunk[i] < 80) {\n      local_bins[7]++;\n    } else if (chunk[i] < 90) {\n      local_bins[8]++;\n    } else if (chunk[i] < 100) {\n      local_bins[9]++;\n    }\n  }\n  // TODO\n  if (mpi_rank == 0) {\n    for (size_t i = 1; i < mpi_size; ++i) {\n      std::vector<size_t> tmp_bins(10, 0);\n      MPI_Recv(&tmp_bins, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < 10; ++j) {\n        local_bins[j] += tmp_bins[j];\n      }\n    }\n    for (size_t i = 0; i < 10; ++i) {\n      bins[i] = local_bins[i];\n    }\n  } else {\n    MPI_Send(&local_bins, 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "/* Your code here */\n  int size;\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  //omp_set_num_threads(4);\n  std::vector<double> data;\n  std::array<size_t, 10> myBins;\n  int chunkSize = x.size() / size;\n  int rest = x.size() % size;\n\n  if(rank == 0){\n    for(int i = 1; i < size; i++){\n      MPI_Send(&x[i * chunkSize], chunkSize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n    for(int i = 0; i < chunkSize; i++){\n      myBins[x[i] / 10]++;\n    }\n    for(int i = 0; i < rest; i++){\n      myBins[x[i + chunkSize * size] / 10]++;\n    }\n\n    for(int i = 1; i < size; i++){\n      MPI_Recv(&data, chunkSize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int j = 0; j < chunkSize; j++){\n        myBins[data[j] / 10]++;\n      }\n    }\n    for(int i = 0; i < 10; i++){\n      bins[i] = 0;\n    }\n    for(int i = 0; i < 10; i++){\n      for(int j = 0; j < size; j++){\n        bins[i] += myBins[i];\n      }\n    }\n  }\n  else{\n    MPI_Recv(&data, chunkSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    for(int i = 0; i < chunkSize; i++){\n      myBins[data[i] / 10]++;\n    }\n    MPI_Send(&myBins, 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "}",
            "const int size = x.size();\n  // std::cout << size << std::endl;\n  const int chunk_size = size/omp_get_num_procs();\n  const int rank = omp_get_thread_num();\n  std::array<size_t, 10> thread_bins;\n  for (int i = 0; i < 10; i++) thread_bins[i] = 0;\n  // std::cout << rank << std::endl;\n  for (int i = 0; i < chunk_size; i++) {\n    int bin = x[rank*chunk_size+i]/10;\n    thread_bins[bin] += 1;\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = 0;\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  // std::cout << \"rank \" << rank << std::endl;\n  // for (int i = 0; i < 10; i++) {\n  //   std::cout << thread_bins[i] << \" \";\n  // }\n  // std::cout << std::endl;\n  MPI_Reduce(&thread_bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // std::cout << \"rank \" << rank << std::endl;\n  // for (int i = 0; i < 10; i++) {\n  //   std::cout << bins[i] << \" \";\n  // }\n  // std::cout << std::endl;\n}",
            "MPI_Comm mpi_comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(mpi_comm, &rank);\n  MPI_Comm_size(mpi_comm, &size);\n  auto const countPerRank = x.size() / size;\n  auto const countPerThread = countPerRank / omp_get_max_threads();\n  bins = {};\n  std::array<size_t, 10> local_bins = {};\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < countPerRank; i++) {\n      local_bins[(int)(x[rank*countPerRank + i] / 10)]++;\n    }\n    // Reduce local_bins over all threads.\n    #pragma omp critical\n    {\n      for (size_t i = 0; i < 10; i++) {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n}",
            "std::array<std::vector<size_t>, 10> binsPerThread;\n    for (auto& b : binsPerThread) {\n        b.resize(x.size());\n    }\n\n    const int threadCount = omp_get_max_threads();\n    int blockSize = x.size()/threadCount;\n    const int lastBlockSize = x.size() - blockSize*(threadCount-1);\n\n    #pragma omp parallel num_threads(threadCount)\n    {\n        #pragma omp for nowait\n        for (int i = 0; i < threadCount-1; i++) {\n            for (int j = 0; j < blockSize; j++) {\n                binsPerThread[i][j] = x[j] < 20;\n            }\n        }\n\n        #pragma omp for nowait\n        for (int i = 0; i < lastBlockSize; i++) {\n            binsPerThread[threadCount-1][i] = x[blockSize*(threadCount-1) + i] < 20;\n        }\n    }\n\n    for (auto &b : binsPerThread) {\n        for (auto &v : b) {\n            if (v!= 0) {\n                bins[v]++;\n            }\n        }\n    }\n}",
            "// Add your code here\n\n}",
            "// TODO:\n\n}",
            "}",
            "}",
            "/* YOUR CODE HERE */\n\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  double local_start = (rank == 0)? 0 : rank * (x.size() / size);\n  double local_end = (rank == size - 1)? x.size() : (rank + 1) * (x.size() / size);\n\n  std::vector<size_t> local_bins(10);\n\n#pragma omp parallel for\n  for (int i = local_start; i < local_end; i++) {\n    int bin_id = x[i] / 10;\n    local_bins[bin_id]++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n}",
            "// Your code here\n\n}",
            "// TODO: insert your solution here\n\n}",
            "// You code goes here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<double> local_x(x.size()/size);\n    std::vector<size_t> local_bins(10);\n    for (int i = 0; i < size; i++){\n        if(i == rank){\n            local_x.assign(x.begin() + i*(x.size()/size), x.begin() + (i+1)*(x.size()/size));\n            #pragma omp parallel for\n            for (int j = 0; j < local_x.size(); j++)\n                local_bins[local_x[j]/10]++;\n        }\n        MPI_Bcast(&local_bins, 10, MPI_INT, i, MPI_COMM_WORLD);\n    }\n    if (rank == 0){\n        for (int i = 0; i < 10; i++)\n            for (int j = 0; j < size; j++)\n                bins[i] += local_bins[i];\n    }\n}",
            "// You must insert code here\n\n}",
            "size_t const n = x.size();\n  bins.fill(0);\n\n  #pragma omp parallel\n  {\n    std::array<size_t, 10> local_bins;\n    local_bins.fill(0);\n\n    #pragma omp for\n    for(size_t i = 0; i < n; ++i)\n      ++local_bins[int(x[i] / 10)];\n\n    // reduce\n    #pragma omp critical\n    for(size_t i = 0; i < 10; ++i)\n      bins[i] += local_bins[i];\n\n  }\n\n}",
            "std::array<size_t, 10> local_bins = {};\n    #pragma omp parallel for\n    for(double v: x) {\n        int index = v / 10;\n        if (index < 0) index = 0;\n        if (index > 10) index = 10;\n        ++local_bins[index];\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* *************************** YOUR CODE HERE *************************** */\n\n  // TODO 0: Use `std::partition` to separate values into bins\n  //   Hint: Use MPI_COMM_WORLD to create sub-communicators, and use `rank` to\n  //   figure out which bin to place values in\n\n\n  // TODO 1: Use MPI_Reduce to sum the counts from the bins\n\n  /* *************************** END OF YOUR CODE ************************* */\n}",
            "}",
            "// TODO: implement\n}",
            "// TODO: your code here\n\n}",
            "// TODO\n\n}",
            "// Your code goes here\n}",
            "// Your code goes here!\n    int n;\n    n = x.size();\n    std::vector<int> bins_temp(10, 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int temp = (int) x[i];\n        bins_temp[temp / 10]++;\n    }\n\n    if (omp_get_thread_num() == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = bins_temp[i];\n        }\n    }\n}",
            "int rank, size;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // 10 bins\n    std::array<size_t, 10> localBins = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n\n    for (int i = 0; i < 10; i++) {\n        if (rank == 0) {\n            // count each value from 0 to 10\n            for (double value : x) {\n                if (value < i * 10) {\n                    localBins[i]++;\n                }\n            }\n        } else {\n            // count each value from 0 to 10\n            for (double value : x) {\n                if (value < i * 10) {\n                    localBins[i]++;\n                }\n            }\n            MPI_Send(localBins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    // sum all bins\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n    size_t N=x.size();\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    const int numT = omp_get_num_threads();\n    const int blockSize = 100;\n    const int numBlocks = (N/blockSize) + 1;\n    const int numData = numBlocks*numT;\n\n    MPI_Request req[numProcs];\n    MPI_Status st[numProcs];\n\n    std::vector<int> recvData(numData);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Irecv(&recvData[0], numData, MPI_INT, 0, 0, MPI_COMM_WORLD, &req[0]);\n    MPI_Send(&x[0], N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    MPI_Wait(&req[0], &st[0]);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        const int tid = omp_get_thread_num();\n\n        // TODO: Implement this function\n        std::vector<int> sendData(numBlocks, 0);\n        std::array<int, 10> localBins(10, 0);\n\n        for (int i = 0; i < N; i++) {\n            int binId = std::floor(x[i]/10);\n            localBins[binId]++;\n        }\n\n        for (int i = 0; i < numBlocks; i++) {\n            sendData[i] = localBins[i];\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n        MPI_Send(&sendData[0], numBlocks, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Barrier(MPI_COMM_WORLD);\n\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Irecv(&recvData[0], numData, MPI_INT, 0, 0, MPI_COMM_WORLD, &req[0]);\n    MPI_Send(&x[0], N, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    MPI_Wait(&req[0], &st[0]);\n\n    // Copy from recvData to bins\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < numData; i++) {\n        bins[i%10] += recvData[i];\n    }\n\n    return;\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "size_t n = x.size();\n    int nRanks;\n    int myRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    // TODO: Implement the counting using the above information.\n    //       For now, set the bins to the correct values on rank 0.\n\n    if(myRank == 0)\n    {\n        for(size_t i = 0; i < bins.size(); i++)\n        {\n            bins[i] = 0;\n        }\n        for (size_t i = 0; i < n; i++)\n        {\n            if (x[i] >= 0 && x[i] < 10)\n            {\n                bins[0]++;\n            }\n            else if (x[i] >= 10 && x[i] < 20)\n            {\n                bins[1]++;\n            }\n            else if (x[i] >= 20 && x[i] < 30)\n            {\n                bins[2]++;\n            }\n            else if (x[i] >= 30 && x[i] < 40)\n            {\n                bins[3]++;\n            }\n            else if (x[i] >= 40 && x[i] < 50)\n            {\n                bins[4]++;\n            }\n            else if (x[i] >= 50 && x[i] < 60)\n            {\n                bins[5]++;\n            }\n            else if (x[i] >= 60 && x[i] < 70)\n            {\n                bins[6]++;\n            }\n            else if (x[i] >= 70 && x[i] < 80)\n            {\n                bins[7]++;\n            }\n            else if (x[i] >= 80 && x[i] < 90)\n            {\n                bins[8]++;\n            }\n            else if (x[i] >= 90 && x[i] <= 100)\n            {\n                bins[9]++;\n            }\n        }\n    }\n    else\n    {\n        for (size_t i = 0; i < bins.size(); i++)\n        {\n            bins[i] = 0;\n        }\n    }\n}",
            "// TODO\n}",
            "if (x.size() == 0) {\n    return;\n  }\n\n  // Your code here.\n  bins.fill(0);\n\n  int size = x.size();\n  int rank = 0, num_proc = 1;\n  int chunk_size;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  chunk_size = size / num_proc;\n\n  if (rank == 0) {\n    for (int i = 1; i < num_proc; i++) {\n      MPI_Send(&x[i * chunk_size], chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Recv(&x[0], chunk_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  #pragma omp parallel\n  {\n    size_t tid = omp_get_thread_num();\n    std::array<size_t, 10> tmp_bins;\n    tmp_bins.fill(0);\n\n    #pragma omp for\n    for (int i = 0; i < chunk_size; i++) {\n      int bin_index = (int)x[i] / 10;\n      if (bin_index < 10) {\n        tmp_bins[bin_index]++;\n      }\n    }\n\n    // reduce thread local results\n    #pragma omp critical\n    {\n      for (int i = 0; i < 10; i++) {\n        bins[i] += tmp_bins[i];\n      }\n    }\n  }\n\n  // TODO: reduce each MPI rank's result into a single result\n  int tmp[10] = {0};\n  MPI_Reduce(bins.data(), tmp, 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  for (int i = 0; i < 10; i++) {\n    bins[i] = tmp[i];\n  }\n\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Add code here\n}",
            "int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    std::vector<size_t> counts(10);\n\n    size_t num_per_rank = x.size() / nprocs;\n    size_t extra = x.size() % nprocs;\n\n    std::vector<size_t> my_counts(10);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < num_per_rank; i++) {\n        size_t my_rank = rank * num_per_rank + i;\n        my_counts[x[my_rank]/10]++;\n    }\n\n    if (rank < extra) {\n        size_t my_rank = (nprocs - 1) * num_per_rank + rank;\n        my_counts[x[my_rank]/10]++;\n    }\n\n    for (size_t i = 0; i < 10; i++) {\n        #pragma omp critical\n        counts[i] += my_counts[i];\n    }\n\n    if (rank == 0) {\n        bins = counts;\n    }\n\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint chunk = x.size() / size;\n\tint i = rank * chunk;\n\tint end = (rank == size - 1)? x.size() : (rank + 1) * chunk;\n\n\tstd::vector<double> x_rank(x.begin() + i, x.begin() + end);\n\tstd::array<size_t, 10> bins_rank;\n\tbins_rank.fill(0);\n\n\t//omp_set_num_threads(size);\n#pragma omp parallel\n\t{\n\t\tstd::array<size_t, 10> bins_temp;\n\t\tbins_temp.fill(0);\n#pragma omp for\n\t\tfor (auto const& item : x_rank) {\n\t\t\tbins_temp[(int)item / 10]++;\n\t\t}\n#pragma omp critical\n\t\t{\n\t\t\tfor (int i = 0; i < 10; ++i) {\n\t\t\t\tbins_rank[i] += bins_temp[i];\n\t\t\t}\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tMPI_Reduce(bins_rank.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t}\n\telse {\n\t\tMPI_Reduce(bins_rank.data(), NULL, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t}\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here\n\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 10> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&bins[0], bins.size(), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD);\n        }\n        for (auto const& xi : x) {\n            int b = (int) (xi / 10);\n            counts[b]++;\n        }\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&counts[0], bins.size(), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD,\n                     MPI_STATUS_IGNORE);\n        }\n    } else {\n        std::vector<double> my_x;\n        std::copy_if(x.begin(), x.end(), std::back_inserter(my_x), [rank](double x) {\n            return x % size == rank;\n        });\n        for (auto const& xi : my_x) {\n            int b = (int) (xi / 10);\n            counts[b]++;\n        }\n        MPI_Send(&counts[0], bins.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&counts[0], bins.size(), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD,\n                     MPI_STATUS_IGNORE);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = counts;\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    const int maxNumThreads = omp_get_max_threads();\n\n    int chunksPerProcess = size / maxNumThreads;\n    int remainder = size % maxNumThreads;\n    int myStart = rank * chunksPerProcess + std::min(rank, remainder);\n    int myEnd = (rank + 1) * chunksPerProcess + std::min(rank + 1, remainder);\n\n    for (int i = myStart; i < myEnd; i++) {\n        int bin = x[i] / 10;\n        bins[bin]++;\n    }\n\n    MPI_Reduce(&bins, &bins, bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n}",
            "int nthreads, threadid;\n    omp_get_num_threads();\n\n    // Count each bin in parallel\n    #pragma omp parallel private(threadid)\n    {\n        threadid = omp_get_thread_num();\n        #pragma omp for\n        for (int i = 0; i < x.size(); i++) {\n            bins[x[i] / 10]++;\n        }\n    }\n\n    // Collect all counts from all threads\n    // We will use this to compute the counts on all ranks\n    int *bins_array = bins.data();\n    MPI_Reduce(bins_array, bins_array, 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for(int i=0; i<bins.size(); i++){\n        bins[i] = 0;\n        for(int j = 0; j<x.size(); j++){\n            if(x[j] >= 10*i && x[j] < 10*(i+1)){\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "}",
            "// TODO: Fill in your code\n}",
            "int nranks, rankid;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rankid);\n  if (rankid == 0)\n  {\n    size_t i = 0;\n    for (double v : x) {\n      if (v >= 10.0 * i && v < 10.0 * (i + 1))\n        bins[i]++;\n      i++;\n    }\n  }\n  else\n  {\n    std::vector<double> local;\n    local.reserve(x.size() / nranks);\n    std::vector<size_t> bins_local;\n    bins_local.reserve(10);\n\n    int local_n = 0;\n    int i = rankid - 1;\n    for (double v : x) {\n      if (v >= 10.0 * i && v < 10.0 * (i + 1)) {\n        local.push_back(v);\n        local_n++;\n      }\n      i++;\n    }\n\n    // Count\n    for (i = 0; i < 10; i++) {\n      for (double v : local) {\n        if (v >= 10.0 * i && v < 10.0 * (i + 1))\n          bins_local[i]++;\n      }\n    }\n\n    // Gather results\n    MPI_Gather(&local_n, 1, MPI_INT, nullptr, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(bins_local.data(), 10, MPI_INT, nullptr, 10, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < x.size(); ++i){\n    int bin = x[i] / 10;\n    #pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "// TODO: Fill in code here to compute bins in parallel\n}",
            "// YOUR CODE HERE\n}",
            "auto rank = omp_get_thread_num();\n\n    auto localBins = std::array<size_t, 10>{0};\n\n    for (auto const& value : x) {\n        localBins[value / 10] += 1;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: use MPI and OpenMP to compute the counts in parallel\n\n    if (rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "//...\n}",
            "// TODO: implement this!\n  bins.fill(0);\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int index = x[i] / 10;\n    if (index == 10) {\n      index = 9;\n    }\n    #pragma omp atomic update\n    bins[index]++;\n  }\n\n}",
            "// Replace this code with your solution\n}",
            "//...\n}",
            "// your code here\n\n}",
            "int numRanks, rank;\n    int rc = MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    if (rc!= MPI_SUCCESS) {\n        std::cerr << \"MPI error \" << rc << std::endl;\n        return;\n    }\n    rc = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rc!= MPI_SUCCESS) {\n        std::cerr << \"MPI error \" << rc << std::endl;\n        return;\n    }\n\n    // TODO: your code here\n}",
            "// TODO\n    int world_rank, world_size;\n    int tag = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    std::vector<int> recvbuf(10, 0);\n    std::vector<int> sendbuf(10, 0);\n\n    if (world_rank == 0)\n    {\n        for (int i = 0; i < x.size(); i++)\n        {\n            int tmp = (int)(x[i] / 10);\n            if (tmp >= 0 && tmp <= 10)\n            {\n                bins[tmp] += 1;\n            }\n        }\n\n        for (int i = 1; i < world_size; i++)\n        {\n            MPI_Recv(&recvbuf, 10, MPI_INT, i, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 10; j++)\n            {\n                bins[j] += recvbuf[j];\n            }\n        }\n    }\n    else\n    {\n        #pragma omp parallel\n        {\n            int myid = omp_get_thread_num();\n            int num_threads = omp_get_num_threads();\n\n            for (int i = myid; i < x.size(); i += num_threads)\n            {\n                int tmp = (int)(x[i] / 10);\n                if (tmp >= 0 && tmp <= 10)\n                {\n                    sendbuf[tmp] += 1;\n                }\n            }\n\n            MPI_Send(&sendbuf, 10, MPI_INT, 0, tag, MPI_COMM_WORLD);\n        }\n    }\n}",
            "if(x.size() == 0) {\n        return;\n    }\n\n    const int rank = omp_get_thread_num();\n    const int size = omp_get_num_threads();\n\n    std::vector<double> rank_x;\n\n    for (int i = rank; i < x.size(); i += size) {\n        rank_x.push_back(x[i]);\n    }\n\n    std::vector<double> rank_bins(10, 0);\n    #pragma omp parallel for\n    for (int i = 0; i < rank_x.size(); ++i) {\n        int index = rank_x[i] / 10;\n        ++rank_bins[index];\n    }\n\n    MPI_Reduce(rank_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // if (rank == 0) {\n    //     std::cout << \"Bins: \";\n    //     for (int i = 0; i < bins.size(); ++i) {\n    //         std::cout << bins[i] << \", \";\n    //     }\n    //     std::cout << std::endl;\n    // }\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: put your code here.\n}",
            "// TODO\n}",
            "const size_t N = x.extent(0);\n  const size_t BLOCK_SIZE = 256;\n  Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::Cuda>(0, N / BLOCK_SIZE), [=] (const int i) {\n    const size_t start = i * BLOCK_SIZE;\n    const size_t end = (i == N / BLOCK_SIZE - 1)? N : (i + 1) * BLOCK_SIZE;\n    size_t bins_local[4] = {0, 0, 0, 0};\n    for (size_t j = start; j < end; j++) {\n      const double frac = x[j] - floor(x[j]);\n      if (frac < 0.25) {\n        bins_local[0]++;\n      } else if (frac < 0.5) {\n        bins_local[1]++;\n      } else if (frac < 0.75) {\n        bins_local[2]++;\n      } else {\n        bins_local[3]++;\n      }\n    }\n    for (int i = 0; i < 4; i++) {\n      atomicAdd(bins.data() + i, bins_local[i]);\n    }\n  });\n}",
            "Kokkos::parallel_for(\n      \"countQuartiles\",\n      Kokkos::RangePolicy<>(0, x.size()),\n      KOKKOS_LAMBDA(int i) {\n        const auto& v = x[i];\n        bins(0) += v >= 0 && v < 0.25;\n        bins(1) += v >= 0.25 && v < 0.5;\n        bins(2) += v >= 0.5 && v < 0.75;\n        bins(3) += v >= 0.75 && v <= 1;\n      });\n}",
            "// TODO: implement\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using RangePolicy = Kokkos::RangePolicy<ExecutionSpace>;\n\n  // TODO: Your code here!\n  //...\n\n}",
            "/*\n   * Your code goes here!\n   */\n}",
            "/* YOUR CODE GOES HERE */\n}",
            "// Put your code here\n}",
            "// TODO: implement this function\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>;\n  // TODO: fill in the range policy and the parallel for loop\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(size_t i) {\n        const double f = x(i) - std::floor(x(i));\n        if (f >= 0 && f < 0.25) ++bins(0);\n        else if (f >= 0.25 && f < 0.5) ++bins(1);\n        else if (f >= 0.5 && f < 0.75) ++bins(2);\n        else if (f >= 0.75 && f < 1) ++bins(3);\n    });\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"quartiles_kernel\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    [&](size_t idx) {\n      const double val = x(idx);\n      const double fractional = val - std::floor(val);\n      if (fractional < 0.25) {\n        ++bins(0);\n      } else if (fractional < 0.5) {\n        ++bins(1);\n      } else if (fractional < 0.75) {\n        ++bins(2);\n      } else {\n        ++bins(3);\n      }\n    }\n  );\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "Kokkos::parallel_for(\n        \"countQuartiles\",\n        Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n        KOKKOS_LAMBDA(int i) {\n            // TODO: implement\n        });\n}",
            "// Your code here.\n}",
            "// Put your solution here.\n  \n}",
            "// TODO: fill in the body of this function\n\n  Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    // TODO: Fill in the body of this kernel\n  });\n\n  Kokkos::fence();\n}",
            "// Fill this in\n}",
            "// TODO: implement the body of this function\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0));\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(int i) {\n    double x_frac = x[i] - std::floor(x[i]);\n    if (x_frac < 0.25) {\n      Kokkos::atomic_add(&bins[0], 1);\n    } else if (x_frac < 0.5) {\n      Kokkos::atomic_add(&bins[1], 1);\n    } else if (x_frac < 0.75) {\n      Kokkos::atomic_add(&bins[2], 1);\n    } else {\n      Kokkos::atomic_add(&bins[3], 1);\n    }\n  });\n  Kokkos::fence();\n}",
            "using exec_space = typename Kokkos::DefaultExecutionSpace;\n\n  // Write your code here\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::ParallelForTag>(0, x.size()),\n    KOKKOS_LAMBDA(int i) {\n      const double frac = x(i) - std::floor(x(i));\n      if (frac < 0.25) ++bins(0);\n      else if (frac < 0.5) ++bins(1);\n      else if (frac < 0.75) ++bins(2);\n      else ++bins(3);\n    }\n  );\n  Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n\n    auto x_at_0 = Kokkos::subview(x, 0, Kokkos::ALL());\n    auto x_at_1 = Kokkos::subview(x, 1, Kokkos::ALL());\n    auto x_at_2 = Kokkos::subview(x, 2, Kokkos::ALL());\n    auto x_at_3 = Kokkos::subview(x, 3, Kokkos::ALL());\n\n    auto policy_0 = Kokkos::RangePolicy<ExecutionSpace>(0, x_at_0.extent(0));\n    auto policy_1 = Kokkos::RangePolicy<ExecutionSpace>(0, x_at_1.extent(0));\n    auto policy_2 = Kokkos::RangePolicy<ExecutionSpace>(0, x_at_2.extent(0));\n    auto policy_3 = Kokkos::RangePolicy<ExecutionSpace>(0, x_at_3.extent(0));\n\n    Kokkos::parallel_for(\"first\", policy_0, KOKKOS_LAMBDA(const int i) {\n        if (std::fmod(x_at_0(i), 1) < 0.25)\n            ++bins(0);\n        else if (std::fmod(x_at_0(i), 1) < 0.5)\n            ++bins(1);\n        else if (std::fmod(x_at_0(i), 1) < 0.75)\n            ++bins(2);\n        else\n            ++bins(3);\n    });\n\n    Kokkos::parallel_for(\"second\", policy_1, KOKKOS_LAMBDA(const int i) {\n        if (std::fmod(x_at_1(i), 1) < 0.25)\n            ++bins(0);\n        else if (std::fmod(x_at_1(i), 1) < 0.5)\n            ++bins(1);\n        else if (std::fmod(x_at_1(i), 1) < 0.75)\n            ++bins(2);\n        else\n            ++bins(3);\n    });\n\n    Kokkos::parallel_for(\"third\", policy_2, KOKKOS_LAMBDA(const int i) {\n        if (std::fmod(x_at_2(i), 1) < 0.25)\n            ++bins(0);\n        else if (std::fmod(x_at_2(i), 1) < 0.5)\n            ++bins(1);\n        else if (std::fmod(x_at_2(i), 1) < 0.75)\n            ++bins(2);\n        else\n            ++bins(3);\n    });\n\n    Kokkos::parallel_for(\"fourth\", policy_3, KOKKOS_LAMBDA(const int i) {\n        if (std::fmod(x_at_3(i), 1) < 0.25)\n            ++bins(0);\n        else if (std::fmod(x_at_3(i), 1) < 0.5)\n            ++bins(1);\n        else if (std::fmod(x_at_3(i), 1) < 0.75)\n            ++bins(2);\n        else\n            ++bins(3);\n    });\n\n    Kokkos::fence();\n}",
            "// Insert code here\n}",
            "// 1. Create a Kokkos parallel_for with a range of 4 iterations\n  // 2. Count the number of elements of x in each bin, and store in bins\n  // 3. Add a Kokkos parallel_for to do the same thing as in count_evens, above\n}",
            "/* TODO: your implementation goes here */\n}",
            "const int n = x.extent(0);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, n),\n    KOKKOS_LAMBDA(const int i) {\n    size_t &bins0 = bins(0);\n    size_t &bins1 = bins(1);\n    size_t &bins2 = bins(2);\n    size_t &bins3 = bins(3);\n    const double x_i = x(i);\n\n    if(x_i < 0.25) bins0++;\n    else if(x_i < 0.50) bins1++;\n    else if(x_i < 0.75) bins2++;\n    else bins3++;\n  });\n}",
            "// Your code here\n}",
            "// Kokkos views require an execution space.\n  // Create a serial execution space here.\n  using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n\n  // Get the number of elements in `x`.\n  const size_t n = x.extent(0);\n\n  // Create a Kokkos parallel_reduce lambda function to perform the computation.\n  // For a parallel_reduce lambda, the template parameters are\n  // <ReductionType, TagType, ExecutionSpace, ValueType>.\n  //   - ReductionType is the type of the reduction variable.\n  //   - TagType is a string describing the lambda function. It is only used for debugging\n  //     purposes.\n  //   - ExecutionSpace is the execution space for parallelism.\n  //   - ValueType is the type of the value to be reduced.\n  Kokkos::parallel_reduce(\n      Kokkos::RangePolicy<ExecutionSpace>(0, n),\n      KOKKOS_LAMBDA(const size_t i, size_t& local_bins) {\n        // The number of bins is 4.\n        const size_t n_bins = 4;\n\n        // Get the fractional part of x(i).\n        const double x_i = x(i);\n        const double x_i_frac = x_i - floor(x_i);\n\n        // Find the bin that x(i) belongs to.\n        const size_t bin = (x_i_frac / 0.25);\n\n        // Increment the bin's count.\n        const size_t bin_idx = bin;\n        Kokkos::atomic_increment(&(local_bins[bin_idx]));\n      },\n      bins);\n\n  // Synchronize the device.\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t& i) {\n    const double& x_i = x(i);\n    const double intpart = std::floor(x_i);\n    const double fracpart = x_i - intpart;\n    const size_t& bins_i = (size_t) (fracpart / 0.25);\n    Kokkos::atomic_increment(&bins(bins_i));\n  });\n  Kokkos::fence();\n}",
            "// TODO: use Kokkos parallel_for and atomic operations to compute the bins\n  size_t bins_count[4] = {0, 0, 0, 0};\n  for (size_t i = 0; i < x.extent(0); ++i) {\n    double v = x(i);\n    size_t bin = 0;\n    if (v < 0.25) {\n      bin = 0;\n    } else if (v < 0.5) {\n      bin = 1;\n    } else if (v < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    Kokkos::atomic_increment(&bins_count[bin]);\n  }\n\n  Kokkos::deep_copy(bins, bins_count);\n}",
            "const auto n = x.size();\n\n    // Declare a Kokkos execution policy for the range of the input vector x\n    using policy_type = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>;\n\n    // Declare and initialize a Kokkos view for the result vector\n    Kokkos::View<size_t[4]> count(\"count\", 4);\n    Kokkos::deep_copy(count, 0);\n\n    // Define a lambda function that increments the count corresponding to the input value\n    auto count_lambda = KOKKOS_LAMBDA(int i) {\n        if (x(i) < 0.25) {\n            Kokkos::atomic_add(&(count[0]), 1);\n        } else if (x(i) < 0.5) {\n            Kokkos::atomic_add(&(count[1]), 1);\n        } else if (x(i) < 0.75) {\n            Kokkos::atomic_add(&(count[2]), 1);\n        } else {\n            Kokkos::atomic_add(&(count[3]), 1);\n        }\n    };\n\n    // Run the lambda function over the input vector\n    Kokkos::parallel_for(policy_type(0, n), count_lambda);\n\n    // Copy the result back to the host\n    Kokkos::deep_copy(bins, count);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Use a lambda to define the kernel\n  auto countQuartiles_kernel = KOKKOS_LAMBDA(const int i) {\n    // TODO: use std::floor on x[i] and then use an if-else chain\n    // to update the bins appropriately\n  };\n\n  // Call the kernel using the parallel_for function provided by Kokkos\n  Kokkos::parallel_for(x.extent(0), countQuartiles_kernel);\n  // This call is needed to synchronize the result of the kernel\n  Kokkos::fence();\n}",
            "// TODO: fill this in\n}",
            "// TODO: replace this with your code\n}",
            "// TODO: Fill this in\n}",
            "/* YOUR CODE HERE */\n\n  Kokkos::View<int[4]> bins_view(\"bins\", 4);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<1>>(0, x.extent(0)), [&] (const int i) {\n    if (x(i) < 0.25) bins_view(0) += 1;\n    if (x(i) >= 0.25 && x(i) < 0.5) bins_view(1) += 1;\n    if (x(i) >= 0.5 && x(i) < 0.75) bins_view(2) += 1;\n    if (x(i) >= 0.75) bins_view(3) += 1;\n  });\n\n  // Copy the contents of bins_view to bins\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<1>>(0, 4), [&] (const int i) {\n    bins(i) = bins_view(i);\n  });\n}",
            "// TODO\n}",
            "Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, x.size());\n  auto f = KOKKOS_LAMBDA (const size_t i) {\n    double quotient;\n    double fraction = std::modf(x(i), &quotient);\n    if (fraction < 0.25) {\n      Kokkos::atomic_increment(&bins[0]);\n    } else if (fraction < 0.5) {\n      Kokkos::atomic_increment(&bins[1]);\n    } else if (fraction < 0.75) {\n      Kokkos::atomic_increment(&bins[2]);\n    } else {\n      Kokkos::atomic_increment(&bins[3]);\n    }\n  };\n  Kokkos::parallel_for(\"quartiles\", policy, f);\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n\n  // TODO: your code here\n  \n  // Deep copy bins to host\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_host, bins);\n  \n  // Print to verify\n  std::cout << \"Result: [\";\n  for (int i = 0; i < 4; i++) {\n    std::cout << bins_host(i) << \", \";\n  }\n  std::cout << \"]\\n\";\n}",
            "// TODO: Your code here\n  auto x_host = Kokkos::create_mirror_view(x);\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(x_host, x);\n  Kokkos::deep_copy(bins_host, bins);\n\n  for(size_t i = 0; i < x.extent(0); i++) {\n    if(x_host(i) < 0.25) {\n      bins_host(0)++;\n    } else if(x_host(i) < 0.5) {\n      bins_host(1)++;\n    } else if(x_host(i) < 0.75) {\n      bins_host(2)++;\n    } else {\n      bins_host(3)++;\n    }\n  }\n\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// Insert your solution here.\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)), [&](int i) {\n        int idx = 4*int(4*x(i));\n        Kokkos::atomic_add(&bins(idx), 1);\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n            KOKKOS_LAMBDA(const int i) {\n                double frac = x[i] - std::floor(x[i]);\n                if (frac < 0.25)\n                    bins[0]++;\n                else if (frac < 0.5)\n                    bins[1]++;\n                else if (frac < 0.75)\n                    bins[2]++;\n                else\n                    bins[3]++;\n            });\n}",
            "// your code goes here\n}",
            "// Create a device-view for bins.\n  auto bins_dv = Kokkos::View<size_t[4]>(\"bins\", Kokkos::LayoutStride(), Kokkos::DefaultHostExecutionSpace());\n\n  // Initialize bins.\n  // Tip: Kokkos::parallel_for might be useful.\n\n  // Copy from bins_dv to bins.\n  // Tip: Kokkos::deep_copy might be useful.\n}",
            "// Your code here.\n}",
            "using Kokkos::Experimental::unique_token;\n  using Kokkos::Experimental::UniqueToken;\n\n  UniqueToken<Kokkos::Cuda> token{};\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, 1, token),\n    KOKKOS_LAMBDA(const int&) {\n      Kokkos::parallel_for(\n        Kokkos::TeamThreadRange(team, x.extent(0)),\n        [=](const int& i) {\n          double v = x(i);\n          v = fmod(v, 1.0);\n          if (v < 0.25) {\n            Kokkos::atomic_increment(&bins(0));\n          } else if (v < 0.5) {\n            Kokkos::atomic_increment(&bins(1));\n          } else if (v < 0.75) {\n            Kokkos::atomic_increment(&bins(2));\n          } else {\n            Kokkos::atomic_increment(&bins(3));\n          }\n        }\n      );\n    }\n  );\n\n  Kokkos::fence();\n}",
            "/* YOUR CODE HERE */\n\n}",
            "const size_t num_entries = x.extent(0);\n  const double q1 = 0.25;\n  const double q2 = 0.5;\n  const double q3 = 0.75;\n  const double q4 = 1.0;\n\n  // TODO: Replace this with your code\n  size_t bins_host[4] = {0,0,0,0};\n  for (int i = 0; i < num_entries; i++){\n    const double frac = std::fmod(x(i),1);\n    if (frac >= q1 && frac < q2){\n      bins_host[0]++;\n    }\n    else if (frac >= q2 && frac < q3){\n      bins_host[1]++;\n    }\n    else if (frac >= q3 && frac < q4){\n      bins_host[2]++;\n    }\n    else {\n      bins_host[3]++;\n    }\n  }\n\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "/* YOUR CODE GOES HERE */\n}",
            "using view_t = Kokkos::View<const double*>;\n    using int_t = Kokkos::View<size_t[4]>;\n    using policy_t = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>;\n    using functor_t = quartiles_count_functor<view_t, int_t>;\n\n    // Execute functor on range [0, x.size())\n    functor_t functor(x, bins);\n    Kokkos::parallel_for(policy_t(0, x.size()), functor);\n}",
            "// TODO: implement this\n}",
            "// TODO\n}",
            "// Kokkos::parallel_for is a loop that runs in parallel.\n  // The third argument is a functor, which defines the loop body.\n  // You can pass functors by value, so you can define them right here.\n  Kokkos::parallel_for(\"countQuartiles\", x.size(),\n    [=](size_t i) {\n      // The functor captures its arguments by reference, so they don't have to be\n      // declared as static variables.\n      // Use the static_assert to check the types of x and bins.\n      // These should be Kokkos::View<const double*> and Kokkos::View<size_t[4]>,\n      // respectively.\n      static_assert(std::is_same<decltype(x), Kokkos::View<const double*>>::value, \"\");\n      static_assert(std::is_same<decltype(bins), Kokkos::View<size_t[4]>>::value, \"\");\n\n      // Kokkos::subview is the way to get a view into a view.\n      // This gets a view of the ith element of bins, which is a size_t.\n      Kokkos::View<size_t> bin = Kokkos::subview(bins, i);\n\n      // If the fractional part of x(i) is between 0 and 0.25, increment bin(0).\n      if (fmod(x(i), 1) < 0.25) bin() += 1;\n\n      // If the fractional part of x(i) is between 0.25 and 0.5, increment bin(1).\n      else if (fmod(x(i), 1) < 0.5) bin() += 2;\n\n      // If the fractional part of x(i) is between 0.5 and 0.75, increment bin(2).\n      else if (fmod(x(i), 1) < 0.75) bin() += 3;\n\n      // If the fractional part of x(i) is between 0.75 and 1, increment bin(3).\n      else bin() += 4;\n    });\n\n  // Make sure the execution finished before returning.\n  // Kokkos::fence is like a barrier.\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "// TODO: replace this implementation with your own\n  bins(0) = std::count_if(\n    Kokkos::subview(x, Kokkos::ALL).data(), \n    Kokkos::subview(x, Kokkos::ALL).data() + x.extent(0),\n    [](double xi) { return 0 <= xi && xi < 0.25; });\n  bins(1) = std::count_if(\n    Kokkos::subview(x, Kokkos::ALL).data(), \n    Kokkos::subview(x, Kokkos::ALL).data() + x.extent(0),\n    [](double xi) { return 0.25 <= xi && xi < 0.5; });\n  bins(2) = std::count_if(\n    Kokkos::subview(x, Kokkos::ALL).data(), \n    Kokkos::subview(x, Kokkos::ALL).data() + x.extent(0),\n    [](double xi) { return 0.5 <= xi && xi < 0.75; });\n  bins(3) = std::count_if(\n    Kokkos::subview(x, Kokkos::ALL).data(), \n    Kokkos::subview(x, Kokkos::ALL).data() + x.extent(0),\n    [](double xi) { return 0.75 <= xi && xi <= 1; });\n}",
            "Kokkos::parallel_for(\n        \"count\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n        KOKKOS_LAMBDA (int i) {\n            const int bin = int(4 * (x[i] - floor(x[i])));\n            Kokkos::atomic_increment(&bins[bin]);\n        }\n    );\n\n    Kokkos::fence();\n}",
            "// Your code here\n    // You may not use CUDA or OpenMP directives\n    // You may not use std::for_each, std::count_if, or any other STL functions\n\n    // size_t size = x.extent(0);\n    // for (int i = 0; i < size; i++){\n    //     if (x[i] < 0.25){\n    //         bins[0] += 1;\n    //     }\n    //     else if (x[i] < 0.5){\n    //         bins[1] += 1;\n    //     }\n    //     else if (x[i] < 0.75){\n    //         bins[2] += 1;\n    //     }\n    //     else if (x[i] < 1){\n    //         bins[3] += 1;\n    //     }\n    // }\n\n    // // Kokkos::View<double*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    // Kokkos::View<size_t*>::HostMirror bins_host = Kokkos::create_mirror_view(bins);\n    // Kokkos::deep_copy(x_host, x);\n    // Kokkos::deep_copy(bins_host, bins);\n\n    // for (int i = 0; i < size; i++){\n    //     if (x_host[i] < 0.25){\n    //         bins_host[0] += 1;\n    //     }\n    //     else if (x_host[i] < 0.5){\n    //         bins_host[1] += 1;\n    //     }\n    //     else if (x_host[i] < 0.75){\n    //         bins_host[2] += 1;\n    //     }\n    //     else if (x_host[i] < 1){\n    //         bins_host[3] += 1;\n    //     }\n    // }\n\n    // Kokkos::deep_copy(bins, bins_host);\n\n\n    // 23 lines of code, 13 for setup and 10 for counting\n    // 30 lines of code, 19 for setup and 11 for counting\n    // 15 lines of code, 13 for setup and 12 for counting\n    // 14 lines of code, 13 for setup and 11 for counting\n    // 15 lines of code, 14 for setup and 11 for counting\n\n\n\n\n    // Kokkos::View<double*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n    // Kokkos::deep_copy(x_host, x);\n\n    // double* x_host = x.data();\n    // size_t* bins_host = bins.data();\n\n    // Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.extent(0)), [=] (int i){\n    //     if (x_host[i] < 0.25){\n    //         bins_host[0] += 1;\n    //     }\n    //     else if (x_host[i] < 0.5){\n    //         bins_host[1] += 1;\n    //     }\n    //     else if (x_host[i] < 0.75){\n    //         bins_host[2] += 1;\n    //     }\n    //     else if (x_host[i] < 1){\n    //         bins_host[3] += 1;\n    //     }\n    // });\n\n    // Kokkos::View<size_t*>::HostMirror bins_host = Kokkos::create_mirror_view(bins);\n    // Kokkos::deep_copy(bins_host, bins);\n\n\n    // Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.extent(0)), [=] (int i",
            "const size_t n = x.extent(0);\n  Kokkos::View<size_t[4]> count(\"count\", 4);\n\n  // Your code here\n\n  // copy count to bins\n  Kokkos::deep_copy(bins, count);\n}",
            "/* Add code here to set the bins appropriately */\n}",
            "// Your code goes here!\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::ReducePolicy<Kokkos::ReduceMax<size_t> > >(0, x.size()),\n    KOKKOS_LAMBDA(int i, size_t& max) {\n      double fract = std::fmod(x[i], 1.0);\n      if(fract < 0.25)\n        bins[0]++;\n      else if(fract < 0.5)\n        bins[1]++;\n      else if(fract < 0.75)\n        bins[2]++;\n      else\n        bins[3]++;\n    }, bins);\n  Kokkos::fence();\n}",
            "using MDRangePolicy = Kokkos::MDRangePolicy<Kokkos::Rank<2>>;\n  using MemberType = typename MDRangePolicy::member_type;\n  // number of bins to use\n  const size_t numBins = 4;\n\n  Kokkos::parallel_for(\"countQuartiles\",\n                       MDRangePolicy( {0, 0}, {x.extent(0), numBins} ),\n                       KOKKOS_LAMBDA( const int& i, const int& j, MemberType& ){\n\n    // TODO: check if we're in the correct bin\n    //       if so, increment the bin count\n    //       hint: the value we're looking at is x(i)\n\n  });\n\n  Kokkos::fence();\n\n  // TODO: check that bins has the expected values\n}",
            "// TODO\n}",
            "// ======================================================================\n  // Your code here!\n  // ======================================================================\n  Kokkos::parallel_for(\"countQuartiles\", x.extent(0),\n  [x, bins](const size_t idx){\n    double y = x(idx);\n    size_t whichBin = 0;\n    if(y >= 0.25 && y < 0.5){\n      whichBin = 1;\n    }\n    else if(y >= 0.5 && y < 0.75){\n      whichBin = 2;\n    }\n    else if(y >= 0.75 && y <= 1){\n      whichBin = 3;\n    }\n    Kokkos::atomic_add(&bins(whichBin), 1);\n  });\n  Kokkos::fence();\n}",
            "auto bin_view = Kokkos::View<size_t*>(&bins(0), 4);\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    [=] (const int i) {\n    const double x_i = x(i);\n    const size_t bin = std::floor(x_i * 4);\n    Kokkos::atomic_increment(&bin_view(bin));\n  });\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "// Your code here.\n\n  // Kokkos::parallel_for(\"count quartiles\", x.extent(0), [=](const int idx) {\n  //   const int i = x.extent(0) - 1 - idx;\n  //   if (i >= 0) {\n  //     // TODO: your code here\n  //   }\n  // });\n\n  // Kokkos::parallel_reduce(\"count quartiles\", x.extent(0), Kokkos::Min<size_t>(0), [=](const int i, size_t &update) {\n  //   if (x(i) < 0.25) {\n  //     update += 1;\n  //   } else if (x(i) < 0.5) {\n  //     update += 2;\n  //   } else if (x(i) < 0.75) {\n  //     update += 3;\n  //   } else {\n  //     update += 4;\n  //   }\n  // }, Kokkos::Sum<size_t>());\n\n  // Kokkos::parallel_reduce(\"count quartiles\", x.extent(0), Kokkos::Min<size_t>(0), [=](const int i, size_t &update) {\n  //   if (x(i) < 0.25) {\n  //     update += 1;\n  //   } else if (x(i) < 0.5) {\n  //     update += 2;\n  //   } else if (x(i) < 0.75) {\n  //     update += 3;\n  //   } else {\n  //     update += 4;\n  //   }\n  // }, Kokkos::Min<size_t>(0));\n\n  Kokkos::parallel_reduce(\"count quartiles\", x.extent(0), Kokkos::Min<size_t>(0), [=](const int i, size_t &update) {\n    if (x(i) < 0.25) {\n      Kokkos::atomic_increment(&bins[0]);\n    } else if (x(i) < 0.5) {\n      Kokkos::atomic_increment(&bins[1]);\n    } else if (x(i) < 0.75) {\n      Kokkos::atomic_increment(&bins[2]);\n    } else {\n      Kokkos::atomic_increment(&bins[3]);\n    }\n  }, Kokkos::Min<size_t>(0));\n\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"count_quartiles\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n    int index = 0;\n    double remainder = 0;\n    double num = x[i] - floor(x[i]);\n    if(num > 0 && num < 0.25) {\n      index = 0;\n    } else if(num >= 0.25 && num < 0.5) {\n      index = 1;\n    } else if(num >= 0.5 && num < 0.75) {\n      index = 2;\n    } else if(num >= 0.75 && num < 1.0) {\n      index = 3;\n    }\n    Kokkos::atomic_add(&bins(index), 1);\n  });\n}",
            "using namespace Kokkos;\n\n  // TODO: Define and initialize an array of size_t to store the counts.\n  // TODO: Define a parallel_for lambda to update the counts.\n  // TODO: Call Kokkos::parallel_for.\n  // TODO: Check that all",
            "// Write your solution here\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        // Add a term to bins[0] if x[i] is between 0 and 0.25 (inclusive)\n        // Add a term to bins[1] if x[i] is between 0.25 and 0.5 (inclusive)\n        // Add a term to bins[2] if x[i] is between 0.5 and 0.75 (inclusive)\n        // Add a term to bins[3] if x[i] is between 0.75 and 1 (inclusive)\n    });\n    Kokkos::fence();\n}",
            "// Your code goes here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()), KOKKOS_LAMBDA(const int i) {\n        // TODO: Fill in code\n    });\n    Kokkos::fence();\n}",
            "// TODO: fill this in\n}",
            "// Your code goes here\n}",
            "// TODO\n}",
            "// TODO: Implement me!\n  // You may want to create a new Kokkos::View, and use\n  // Kokkos::parallel_reduce to compute the counts.\n\n  // Kokkos::parallel_reduce requires a functor with\n  // two methods: operator()(Tag, int, int) and join()\n  // where Tag = void or const Tag &\n  // See https://kokkos.readthedocs.io/en/latest/api/md_kokkos_parallel_reduce.html#example\n}",
            "// Kokkos::View<size_t[4]> bins(\"bins\", 4);\n  Kokkos::parallel_for(4, [=] (size_t i) { bins[i] = 0; });\n\n  // TODO: Implement me\n\n  // Kokkos::parallel_reduce(x.extent(0), [=] (size_t i, size_t& local_sum) {\n  //   if (x[i] < 0.25) local_sum++;\n  //   else if (x[i] < 0.5)...\n  //   else if (x[i] < 0.75)...\n  //   else if (x[i] < 1.0)...\n  // }, bins);\n\n  Kokkos::fence();\n}",
            "Kokkos::View<int[4]> bins_local(\"bins\", 4);\n  Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      double x_i = x(i);\n      double fraction = x_i - static_cast<int>(x_i);\n      int bin_index = (fraction > 0.75)? 3 : (fraction > 0.5)? 2 : (fraction > 0.25)? 1 : 0;\n      Kokkos::atomic_fetch_add(&bins_local(bin_index), 1);\n    });\n  Kokkos::deep_copy(bins, bins_local);\n}",
            "using namespace Kokkos;\n    using MemberType = Kokkos::TeamPolicy<>::member_type;\n    using CountType = int;\n    const int num_bins = 4;\n\n    Kokkos::parallel_for(\n        \"countQuartiles\",\n        TeamPolicy(x.extent_int(0) / 4),\n        KOKKOS_LAMBDA(const MemberType & member) {\n            const int i = member.league_rank() * member.team_size() + member.team_rank();\n            CountType counts[num_bins] = {0};\n            const int num_to_count = (x.extent_int(0) - i) < 4? x.extent_int(0) - i : 4;\n            for (int ib = 0; ib < num_to_count; ++ib) {\n                const auto d = x(i+ib);\n                counts[static_cast<int>(d - std::floor(d) * 4)]++;\n            }\n            for (int ib = 0; ib < num_bins; ++ib) {\n                Kokkos::atomic_add(&bins[ib], counts[ib]);\n            }\n        });\n}",
            "}",
            "// TODO: insert code here.\n}",
            "// TODO: Count the number of doubles in the vector x that have a fractional part\n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n\n  // You may use the following Kokkos functions:\n  // * `size_type` is an unsigned integer type that is equivalent to `std::size_t`\n  // * `parallel_for` executes a functor in parallel over the range [0, N)\n  //   where N is the number of elements in the view\n  // * `size()` returns the number of elements in the view\n\n  // You may use the following C++ functions:\n  // * `floor(double)` returns the largest integer less than or equal to the argument\n  // * `ceil(double)` returns the smallest integer greater than or equal to the argument\n  // * `std::max` returns the largest of the arguments\n  // * `std::min` returns the smallest of the arguments\n\n  // Some helpful constants:\n  // * `double(n)` is equivalent to `static_cast<double>(n)`\n  // * `double(n)` is equivalent to `double{n}`\n  // * `double(n)` is equivalent to `double n`\n  // * `static_cast<double>(n)` is equivalent to `double(n)`\n\n  // You can use the `Kokkos::Experimental::HIP` namespace if you want to use Kokkos with the\n  // HIP backend.\n\n  // TODO: If you have access to the HIP backend, then uncomment the following lines to enable\n  // it.\n  // Kokkos::InitArguments init_args;\n  // init_args.device_id = 0;\n  // Kokkos::initialize(init_args);\n\n  // TODO: Check that Kokkos has been initialized.\n\n  // TODO: If you have access to the HIP backend, then uncomment the following lines to check\n  // that Kokkos is running on the HIP backend.\n  // std::cout << \"Backend: \" << Kokkos::Experimental::HIP::impl_hardware_name() << std::endl;\n}",
            "Kokkos::parallel_for(\n    \"QuartileBinning\",\n    Kokkos::RangePolicy<Kokkos::HostSpace::execution_space>(0, bins.size()),\n    KOKKOS_LAMBDA(size_t i) {\n      for (auto j = 0; j < x.extent(0); ++j) {\n        auto val = x[j];\n        if (val < 0 || val >= 1) continue;\n        auto index = static_cast<size_t>(4 * val);\n        Kokkos::atomic_add(&bins[index], 1);\n      }\n    }\n  );\n  Kokkos::fence();\n}",
            "// TODO: write me\n}",
            "// This will be your parallel kernel\n  Kokkos::parallel_for(\n    \"quartiles\", \n    Kokkos::RangePolicy<>(0, x.size()),\n    KOKKOS_LAMBDA(const int& i) {\n      // Your code here\n    }\n  );\n  Kokkos::fence();\n}",
            "//...\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::ExecutionSpace>>(x.extent(0)),\n        KOKKOS_LAMBDA(const int i) {\n            double x_dbl = x(i);\n            int i_bin = (x_dbl - int(x_dbl)) * 4;\n            Kokkos::atomic_increment(&bins(i_bin));\n        }\n    );\n    Kokkos::fence();\n}",
            "}",
            "size_t N = x.extent(0);\n  Kokkos::View<size_t[4]> bins_kok(\"bins\", 4);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n    [=] (size_t i) {\n      double xi = x(i);\n      size_t ibin = 0;\n      if (xi < 0.25) {\n        ibin = 0;\n      } else if (xi < 0.5) {\n        ibin = 1;\n      } else if (xi < 0.75) {\n        ibin = 2;\n      } else {\n        ibin = 3;\n      }\n      Kokkos::atomic_increment(&bins_kok[ibin]);\n    }\n  );\n\n  Kokkos::deep_copy(bins, bins_kok);\n}",
            "//\n    // Your code here\n    //\n}",
            "// TODO\n}",
            "// TODO: Implement me\n\n\n}",
            "// Fill this in\n  // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,x.size()), [=](int i){\n\n\n  // });\n  // Kokkos::DefaultExecutionSpace::fence();\n}",
            "// Your code goes here!\n}",
            "// Your code goes here\n}",
            "// TODO: Your code here\n}",
            "// Your code goes here!\n}",
            "Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<Kokkos::ReducePolicy<Kokkos::SumReducer<size_t>>>(0, x.extent(0)), KOKKOS_LAMBDA(size_t i, size_t& lsum) {\n    const double frac = x(i) - floor(x(i));\n    if (frac >= 0 && frac < 0.25) {\n      lsum += 1;\n    } else if (frac >= 0.25 && frac < 0.5) {\n      lsum += 2;\n    } else if (frac >= 0.5 && frac < 0.75) {\n      lsum += 3;\n    } else if (frac >= 0.75 && frac < 1) {\n      lsum += 4;\n    }\n  });\n  Kokkos::fence();\n  Kokkos::deep_copy(bins, Kokkos::reduction_identity<size_t>());\n}",
            "// Your code goes here\n}",
            "}",
            "using MDRangePolicy = Kokkos::RangePolicy<Kokkos::Rank<1>, Kokkos::IndexType<unsigned long long>>;\n  using MemberType = typename MDRangePolicy::member_type;\n\n  Kokkos::parallel_for(\n    MDRangePolicy(0, x.size()),\n    KOKKOS_LAMBDA(MemberType, i) {\n      double value = x[i];\n      size_t j = 4 * value; // j is now an integer in [0, 4)\n      size_t frac = value * 4 - j; // j is now an integer in [0, 4)\n      // TODO: increment `bins[j]` by one\n    }\n  );\n}",
            "// TODO: implement me!\n}",
            "// TODO: Implement this function\n  // It is probably easiest to use the Kokkos parallel_for and team_reduce\n  // constructs to do this.\n}",
            "// TODO\n\n}",
            "// YOUR CODE GOES HERE\n}",
            "// Use Kokkos parallel_for to fill bins with the counts of the fractional parts of x.\n    // Do not forget to call Kokkos::fence() when done, so that bins will contain the correct\n    // final results!\n}",
            "Kokkos::parallel_for(\n    \"quartiles\", Kokkos::RangePolicy<>(0, x.extent(0)), KOKKOS_LAMBDA(size_t i) {\n      double x_i = x[i];\n      size_t b = 0;\n      if (x_i >= 0.0 && x_i < 0.25) {\n        b = 0;\n      } else if (x_i >= 0.25 && x_i < 0.5) {\n        b = 1;\n      } else if (x_i >= 0.5 && x_i < 0.75) {\n        b = 2;\n      } else if (x_i >= 0.75 && x_i < 1.0) {\n        b = 3;\n      }\n      Kokkos::atomic_increment(&bins(b));\n    });\n\n  Kokkos::fence();\n}",
            "// your code here\n}",
            "// TODO: your code here\n\n  auto range = Kokkos::RangePolicy<>(0, x.extent(0));\n  Kokkos::parallel_reduce(range, KOKKOS_LAMBDA(const size_t i, size_t (&localBins)[4]) {\n    double fractional = std::fmod(x(i), 1.0);\n    if (fractional < 0.25) {\n      localBins[0]++;\n    } else if (fractional < 0.5) {\n      localBins[1]++;\n    } else if (fractional < 0.75) {\n      localBins[2]++;\n    } else {\n      localBins[3]++;\n    }\n  }, Kokkos::Sum<size_t[4]>(bins));\n}",
            "// TODO: Your code here\n\n}",
            "// Your code goes here\n}",
            "using ExecutionPolicy = Kokkos::RangePolicy<Kokkos::Launch",
            "const int N = x.extent(0);\n  Kokkos::View<size_t*> bins_(\"bins\", 4);\n  Kokkos::parallel_for(\"countQuartiles\", N, KOKKOS_LAMBDA(const int i) {\n    const double val = x(i);\n    const double modval = std::fmod(val, 1.0);\n    if (modval >= 0.0 && modval < 0.25) {\n      Kokkos::atomic_add(&bins_(0), size_t(1));\n    } else if (modval >= 0.25 && modval < 0.5) {\n      Kokkos::atomic_add(&bins_(1), size_t(1));\n    } else if (modval >= 0.5 && modval < 0.75) {\n      Kokkos::atomic_add(&bins_(2), size_t(1));\n    } else if (modval >= 0.75 && modval < 1.0) {\n      Kokkos::atomic_add(&bins_(3), size_t(1));\n    }\n  });\n  Kokkos::fence();\n  Kokkos::deep_copy(bins, bins_);\n}",
            "// TODO: Implement this function\n}",
            "// TODO: your code here\n}",
            "}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size());\n\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int& i) {\n    const double& value = x(i);\n    if (value < 0.25) bins(0)++;\n    else if (value < 0.5) bins(1)++;\n    else if (value < 0.75) bins(2)++;\n    else bins(3)++;\n  });\n\n  Kokkos::fence();\n}",
            "// Your code goes here\n\n}",
            "// TODO: Implement me\n  size_t numDoubles = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0, numDoubles), [&] (const int i) {\n    double tmp = x[i];\n    if(tmp < 0.25) {\n      Kokkos::atomic_increment(&bins[0]);\n    } else if(tmp < 0.5) {\n      Kokkos::atomic_increment(&bins[1]);\n    } else if(tmp < 0.75) {\n      Kokkos::atomic_increment(&bins[2]);\n    } else if(tmp <= 1) {\n      Kokkos::atomic_increment(&bins[3]);\n    } else {\n      printf(\"Error: x[%i] = %.3f is out of bounds\\n\", i, tmp);\n    }\n  });\n}",
            "const auto xSize = x.extent(0);\n    Kokkos::View<double*> y(\"y\", xSize);\n\n    // Use a parallel_for to fill y with the fractional part of each element of x\n    Kokkos::parallel_for(\n        \"fill_fractional_part\", xSize, KOKKOS_LAMBDA (int i) {\n            y(i) = x(i) - std::floor(x(i));\n        }\n    );\n\n    // Use a parallel_reduce to compute the bin counts\n    Kokkos::parallel_reduce(\n        \"count_bins\", xSize, KOKKOS_LAMBDA(int i, size_t& lsum) {\n            if (y(i) < 0.25) {\n                lsum++;\n            } else if (y(i) < 0.5) {\n                lsum += 2;\n            } else if (y(i) < 0.75) {\n                lsum += 3;\n            } else {\n                lsum += 4;\n            }\n        },\n        Kokkos::Sum<size_t>(bins)\n    );\n\n    // Use a parallel_for to copy y to x in device memory\n    Kokkos::parallel_for(\n        \"copy_back\", xSize, KOKKOS_LAMBDA(int i) {\n            x(i) = y(i);\n        }\n    );\n\n    // This is a required call to complete all work queued on the device\n    Kokkos::fence();\n}",
            "// Your code goes here.\n\n}",
            "const size_t n = x.size();\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, n),\n    KOKKOS_LAMBDA(const size_t i) {\n      const double x_i = x(i);\n      const double frac = std::fmod(x_i, 1.0);\n      if (frac >= 0 && frac < 0.25) {\n        Kokkos::atomic_increment<size_t>(&bins[0]);\n      }\n      else if (frac >= 0.25 && frac < 0.5) {\n        Kokkos::atomic_increment<size_t>(&bins[1]);\n      }\n      else if (frac >= 0.5 && frac < 0.75) {\n        Kokkos::atomic_increment<size_t>(&bins[2]);\n      }\n      else if (frac >= 0.75 && frac < 1) {\n        Kokkos::atomic_increment<size_t>(&bins[3]);\n      }\n    });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(x.extent(0), [=](int i) {\n    int which_quartile = (x(i) - std::floor(x(i))) / 0.25;\n    Kokkos::atomic_increment(&bins(which_quartile));\n  });\n  Kokkos::fence();\n}",
            "// Your code here.\n}",
            "// Your code here!\n    Kokkos::View<size_t[4]> bins_result(\"bins_result\", 4);\n    auto bins_result_view = Kokkos::subview(bins_result, Kokkos::ALL);\n    Kokkos::parallel_for(\n        \"count_quartiles\",\n        Kokkos::RangePolicy<Kokkos::Cuda>(0, x.size()),\n        KOKKOS_LAMBDA(int i) {\n            double x_i = x(i);\n            int bin_i = (x_i - std::floor(x_i)) / 0.25;\n            Kokkos::atomic_increment(bins_result_view + bin_i);\n        });\n    Kokkos::fence();\n\n    Kokkos::parallel_for(\n        \"copy_results\",\n        Kokkos::RangePolicy<Kokkos::Cuda>(0, 4),\n        KOKKOS_LAMBDA(int i) {\n            bins(i) = bins_result(i);\n        });\n    Kokkos::fence();\n}",
            "// Your code here.\n\n}",
            "// TODO: Fill in code here.\n\n}",
            "// your code goes here\n}",
            "// Use Kokkos::parallel_for to iterate through `x`. The range is [0, x.extent(0)),\n    // meaning it runs from 0 (inclusive) to the number of elements in `x` (exclusive).\n    // The lambda function is the work to be done. It takes as its arguments a\n    // \"loop index\", called `i`, and a \"thread index\", called `tid`. \n    // The loop index will range from 0 to x.extent(0)-1, and the thread index will\n    // range from 0 to the number of threads available to this Kokkos::parallel_for.\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(size_t i, size_t tid) {\n        // Convert x[i] to a double in [0, 1)\n        double xfrac = x(i) - floor(x(i));\n        // Increment the appropriate bin\n        if (xfrac < 0.25) {\n            // Use the Kokkos atomics to perform this operation atomically.\n            // See https://github.com/kokkos/kokkos/blob/master/source/impl/Kokkos_Atomic.hpp\n            Kokkos::atomic_increment(&bins(0));\n        } else if (xfrac < 0.5) {\n            Kokkos::atomic_increment(&bins(1));\n        } else if (xfrac < 0.75) {\n            Kokkos::atomic_increment(&bins(2));\n        } else {\n            Kokkos::atomic_increment(&bins(3));\n        }\n    });\n}",
            "// TODO: Implement the body of the function\n}",
            "const size_t n = x.extent(0);\n    Kokkos::parallel_for(\"countQuartiles\", n, KOKKOS_LAMBDA(const size_t& i) {\n        const double xi = x(i);\n        const int floorX = std::floor(xi);\n        const int quarter = std::floor((xi - floorX) * 4.0);\n        // FIXME: increment bins[quarter] with an atomic increment\n    });\n    Kokkos::fence();\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using MemberType = Kokkos::TeamPolicy<ExecutionSpace>::member_type;\n  const int numThreads = 128;\n  const int numBlocks = 8;\n  Kokkos::parallel_for(\n    \"countQuartiles\",\n    Kokkos::TeamPolicy<ExecutionSpace>(numBlocks, numThreads)\n , KOKKOS_LAMBDA (const MemberType& teamMember) {\n    const int teamIdx = teamMember.league_rank();\n    const int threadIdx = teamMember.team_rank();\n    // Count the number of elements in the team that are in each quartile\n    int numInQuartile0 = 0;\n    int numInQuartile1 = 0;\n    int numInQuartile2 = 0;\n    int numInQuartile3 = 0;\n    for (int i = threadIdx; i < x.extent(0); i += numThreads) {\n      double remainder = std::fmod(x(i), 1);\n      if (remainder >= 0 && remainder < 0.25) numInQuartile0++;\n      else if (remainder >= 0.25 && remainder < 0.5) numInQuartile1++;\n      else if (remainder >= 0.5 && remainder < 0.75) numInQuartile2++;\n      else if (remainder >= 0.75 && remainder < 1) numInQuartile3++;\n    }\n    // Reduce the counts to the block leader\n    int blockSize = numThreads;\n    while (blockSize > 1) {\n      Kokkos::parallel_reduce(\n        Kokkos::ThreadVectorRange(teamMember, blockSize),\n        [&] (int i, int& update) {\n          update += i % 2 == 0? numInQuartile0 : 0;\n          update += i % 2 == 1? numInQuartile1 : 0;\n          update += i % 2 == 2? numInQuartile2 : 0;\n          update += i % 2 == 3? numInQuartile3 : 0;\n        },\n        [&] (int& update, const int& value) {\n          update += value;\n        }\n      );\n      blockSize /= 2;\n    }\n    // Leader writes the result to global memory\n    if (threadIdx == 0) {\n      Kokkos::atomic_add(&bins(0), numInQuartile0);\n      Kokkos::atomic_add(&bins(1), numInQuartile1);\n      Kokkos::atomic_add(&bins(2), numInQuartile2);\n      Kokkos::atomic_add(&bins(3), numInQuartile3);\n    }\n  });\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()), [=](int i) {\n    double x_i = x(i);\n    double r = x_i - floor(x_i);\n    if (r < 0.25) {\n      Kokkos::atomic_increment(&bins(0));\n    }\n    else if (r < 0.5) {\n      Kokkos::atomic_increment(&bins(1));\n    }\n    else if (r < 0.75) {\n      Kokkos::atomic_increment(&bins(2));\n    }\n    else {\n      Kokkos::atomic_increment(&bins(3));\n    }\n  });\n}",
            "// TODO\n\n}",
            "// TODO: Your code goes here\n  Kokkos::View<size_t[4], Kokkos::HostSpace> tmp(\"tmp\", 4);\n  Kokkos::parallel_for(\n    \"compute bins\",\n    Kokkos::RangePolicy<Kokkos::ThreadVectorRange>(0, x.extent(0)),\n    KOKKOS_LAMBDA (const int i) {\n      // TODO: your code here\n    }\n  );\n  Kokkos::deep_copy(bins, tmp);\n}",
            "/* \n   TODO:\n   - Count the number of doubles in the vector `x` that have a fractional part in \n     [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n   - Store the counts in `bins`.\n   - Use Kokkos to compute in parallel.\n   - You can use the C++ standard library in your implementation.\n   - Assumes that Kokkos has already been initialized.\n   - You can use all C++11 features.\n  */\n}",
            "// YOUR CODE GOES HERE\n}",
            "// your code here\n}",
            "auto bins_host = Kokkos::create_mirror_view(bins);\n  auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n\n  // Your code here\n\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO: Fill this in.\n}",
            "auto bins_host = Kokkos::create_mirror_view(bins);\n    Kokkos::parallel_for(\n        \"countQuartiles\",\n        Kokkos::RangePolicy<Kokkos::ExecSpace>(0, x.size()),\n        KOKKOS_LAMBDA(const size_t i) {\n            size_t remainder = (x(i) * 4.0) - floor(x(i) * 4.0);\n            bins_host(remainder) += 1;\n        }\n    );\n    Kokkos::deep_copy(bins, bins_host);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, bins.extent(0)), KOKKOS_LAMBDA(const int& i) {\n    bins(i) = Kokkos::reduce(x.extent(0), Kokkos::INITIALIZED_FROM_ZERO, [&](const int& j, size_t& count) {\n      double xj = x(j);\n      if ((i == 0 && xj >= 0 && xj < 0.25) ||\n          (i == 1 && xj >= 0.25 && xj < 0.5) ||\n          (i == 2 && xj >= 0.5 && xj < 0.75) ||\n          (i == 3 && xj >= 0.75 && xj < 1)) {\n        return count + 1;\n      }\n      else {\n        return count;\n      }\n    });\n  });\n  Kokkos::fence();\n}",
            "// Fill in the code here\n  \n  // This example uses a parallel_for with a lambda\n  // Kokkos::parallel_for(\"quartiles\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n  //   // Compute the index of the bin into which x[i] falls\n  //   size_t bin = 0;\n  //   // Fill in the code here\n  \n  //   // Atomically increment the bin count\n  //   Kokkos::atomic_increment(&bins(bin));\n  // });\n}",
            "// TODO: Implement me\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  Kokkos::View<size_t[4], ExecutionSpace> bins_device(\"bins\", 4);\n  Kokkos::parallel_for(\n    \"Counting quartiles\",\n    Kokkos::RangePolicy<ExecutionSpace>(0, x.size()),\n    KOKKOS_LAMBDA(const size_t idx) {\n      if (0 <= x[idx] && x[idx] < 0.25) {\n        Kokkos::atomic_increment(&bins_device(0));\n      }\n      else if (0.25 <= x[idx] && x[idx] < 0.5) {\n        Kokkos::atomic_increment(&bins_device(1));\n      }\n      else if (0.5 <= x[idx] && x[idx] < 0.75) {\n        Kokkos::atomic_increment(&bins_device(2));\n      }\n      else { // x[idx] >= 0.75\n        Kokkos::atomic_increment(&bins_device(3));\n      }\n    }\n  );\n  Kokkos::deep_copy(bins, bins_device);\n}",
            "// TODO: Implement me\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    Kokkos::View<const double*>::HostMirror x_host = Kokkos::create_mirror_view(x);\n\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        double temp = x_host[i];\n        if (temp > 0 && temp <= 0.25) {\n            bins[0]++;\n        } else if (temp > 0.25 && temp <= 0.5) {\n            bins[1]++;\n        } else if (temp > 0.5 && temp <= 0.75) {\n            bins[2]++;\n        } else if (temp > 0.75 && temp <= 1) {\n            bins[3]++;\n        }\n    });\n    Kokkos::fence();\n}",
            "}",
            "// Fill in code here\n}",
            "/* Your code goes here */\n}",
            "//TODO\n\n}",
            "// Fill in this function\n    Kokkos::View<size_t[4]> counts(\"counts\", 4);\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)), \n        [=](int i) {\n            const double val = x(i);\n            // Round val to the nearest double with fractional part in [0, 1)\n            double roundVal = std::floor(val);\n            // Calculate the fractional part\n            double frac = val - roundVal;\n            // Map the fractional part to [0, 3)\n            // (Note: the C++ modulo operator computes the remainder after division)\n            size_t whichBin = (size_t)(frac * 4);\n            // Update the corresponding bin\n            Kokkos::atomic_increment(&(counts(whichBin)));\n        }\n    );\n    Kokkos::deep_copy(bins, counts);\n}",
            "// TODO\n}",
            "// Fill in the body of the parallel_for loop here.\n    // Use the Kokkos::parallel_for() function.\n    // Use Kokkos::ThreadVectorRange to compute the thread index in the vector.\n    // Use Kokkos::ThreadVectorLength to compute the vector length.\n    // Use Kokkos::ThreadReduce to compute the thread sums.\n    // Store the thread sums in the View `bins`.\n}",
            "auto countQuartiles_lambda =\n    KOKKOS_LAMBDA(const int i) {\n      double xi = x(i);\n      int lower_bin = static_cast<int>(4*xi);\n      if (lower_bin < 0) lower_bin = 0;\n      else if (lower_bin >= 4) lower_bin = 3;\n      ++bins(lower_bin);\n      ++bins(lower_bin+1);\n    };\n  Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace> policy(0, x.size());\n  Kokkos::parallel_for(policy, countQuartiles_lambda);\n  Kokkos::fence();\n}",
            "using ExecPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n  using BinTag     = Kokkos::View<size_t[4], Kokkos::LayoutLeft, Kokkos::MemoryUnmanaged>;\n\n  // your code goes here\n}",
            "// Your code goes here\n\n    // You may need to use the following variables:\n    //     size_t n = x.extent(0);\n    //     Kokkos::parallel_for(n, [&](size_t i) {\n    //         // Compute the fractional part of x[i]\n    //         // Store the counts in bins\n    //     });\n\n    // The counts will be stored in bins in the following order:\n    // 0th quarter, 1st quarter, 2nd quarter, 3rd quarter\n}",
            "/* YOUR CODE GOES HERE */\n}",
            "// This lambda function computes the count of fractional parts in [0, 0.25), [0.25, 0.5),\n    // [0.5, 0.75), and [0.75, 1) for a given element of x.\n    auto count = KOKKOS_LAMBDA(int i) {\n        const double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            bins[0]++;\n        } else if (frac < 0.5) {\n            bins[1]++;\n        } else if (frac < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    };\n\n    // Count the numbers in the input vector x using Kokkos parallelism.\n    // Note that the lambda function `count` is not run by this call, instead it is just\n    // passed as an argument to the Kokkos function `parallel_for` so that Kokkos knows\n    // what function to execute in parallel.\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()), count);\n\n    // Force Kokkos to synchronize all work onto the default execution space.\n    Kokkos::DefaultExecutionSpace().fence();\n}",
            "//\n  // YOUR CODE HERE\n  //\n\n  // Make sure we're starting from zero.\n  for (int i = 0; i < 4; i++) {\n    bins(i) = 0;\n  }\n\n  // Loop through x and count the number of values in each of the four quartiles\n  for (int i = 0; i < x.extent(0); i++) {\n    if (x(i) < 0.25)\n      bins(0) += 1;\n    else if (x(i) < 0.5)\n      bins(1) += 1;\n    else if (x(i) < 0.75)\n      bins(2) += 1;\n    else\n      bins(3) += 1;\n  }\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::Reduce>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      const auto xi = x(i);\n      // TODO: compute the index of the bin and increment it atomically\n    });\n}",
            "// TODO\n}",
            "// your code here\n}",
            "size_t count_0_25;\n    size_t count_25_50;\n    size_t count_50_75;\n    size_t count_75_100;\n\n    // Fill in the code\n}",
            "size_t n = x.extent(0);\n    Kokkos::View<size_t[4]> counts(\"counts\", 4);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(int i) {\n        if (0.0 <= x(i) && x(i) < 0.25) counts(0)++;\n        else if (0.25 <= x(i) && x(i) < 0.5) counts(1)++;\n        else if (0.5 <= x(i) && x(i) < 0.75) counts(2)++;\n        else if (0.75 <= x(i) && x(i) < 1.0) counts(3)++;\n    });\n    Kokkos::deep_copy(bins, counts);\n}",
            "// Implement here!\n\n  int N = x.extent(0);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, N), [&] (const int& i) {\n    double x_i = x(i);\n    if (x_i >= 0.0 && x_i < 0.25) bins(0)++;\n    else if (x_i >= 0.25 && x_i < 0.5) bins(1)++;\n    else if (x_i >= 0.5 && x_i < 0.75) bins(2)++;\n    else if (x_i >= 0.75 && x_i < 1.0) bins(3)++;\n  });\n\n  Kokkos::fence();\n}",
            "// TODO: Implement this function\n  // Hint: You can use std::floor and std::fmod\n  //       (https://en.cppreference.com/w/cpp/numeric/math/floor)\n  //       (https://en.cppreference.com/w/cpp/numeric/math/fmod)\n\n  // Make sure we have enough bins\n  assert(bins.size() == 4);\n\n  // TODO: Use parallel_for\n  // Hint: You can use std::floor and std::fmod\n  //       (https://en.cppreference.com/w/cpp/numeric/math/floor)\n  //       (https://en.cppreference.com/w/cpp/numeric/math/fmod)\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n    // TODO: write code here\n  });\n  Kokkos::fence();\n}",
            "// TODO: Implement\n}",
            "// TODO: Fill in the body of this function\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)), [=] (int i) {\n    });\n}",
            "// TODO\n}",
            "Kokkos::View<size_t[4]> bins_local(\"bins\", 4);\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(int) {\n    size_t bin_0 = 0;\n    size_t bin_1 = 0;\n    size_t bin_2 = 0;\n    size_t bin_3 = 0;\n    for (size_t i = 0; i < x.extent(0); i++) {\n      double x_i = x(i);\n      double x_frac = x_i - std::floor(x_i);\n      if (0 <= x_frac && x_frac < 0.25) {\n        bin_0 += 1;\n      }\n      else if (0.25 <= x_frac && x_frac < 0.5) {\n        bin_1 += 1;\n      }\n      else if (0.5 <= x_frac && x_frac < 0.75) {\n        bin_2 += 1;\n      }\n      else if (0.75 <= x_frac && x_frac < 1) {\n        bin_3 += 1;\n      }\n    }\n    bins_local[0] = bin_0;\n    bins_local[1] = bin_1;\n    bins_local[2] = bin_2;\n    bins_local[3] = bin_3;\n  });\n  Kokkos::parallel_reduce(1, KOKKOS_LAMBDA(int, long& total) {\n    total += bins_local[0];\n  }, bins[0]);\n  Kokkos::parallel_reduce(1, KOKKOS_LAMBDA(int, long& total) {\n    total += bins_local[1];\n  }, bins[1]);\n  Kokkos::parallel_reduce(1, KOKKOS_LAMBDA(int, long& total) {\n    total += bins_local[2];\n  }, bins[2]);\n  Kokkos::parallel_reduce(1, KOKKOS_LAMBDA(int, long& total) {\n    total += bins_local[3];\n  }, bins[3]);\n}",
            "// TODO: Your code goes here!\n}",
            "// Implement this function\n}",
            "// your code here\n\n}",
            "// Your code here\n}",
            "// TODO: Replace the code below with your solution\n}",
            "using ExecutionSpace = typename Kokkos::DefaultExecutionSpace;\n  using DeviceType = typename Kokkos::Device<ExecutionSpace, Kokkos::HostSpace>;\n  using MemberType = typename ExecutionSpace::member_type;\n\n  // TODO: Implement this function using Kokkos parallel_for\n  //\n  // 1. First, define a functor that does the work. \n  //    You may find the following functions useful:\n  //    - Kokkos::parallel_for (to define the parallel_for)\n  //    - MemberType::team_rank (to compute the team rank)\n  //    - MemberType::team_size (to compute the team size)\n  //    - MemberType::league_rank (to compute the league rank)\n  //    - MemberType::league_size (to compute the league size)\n  //    - Kokkos::subview (to extract the data for the team)\n  //    - Kokkos::parallel_reduce (to compute the total count for the team)\n  //\n  // 2. Next, create a parallel_for that uses the functor defined above.\n  //    - Use the total number of teams as the league size.\n  //    - Use 4 as the team size.\n  //    - Use 1 as the vector length.\n\n  // YOUR CODE HERE\n}",
            "// your code here\n\n  size_t size = x.extent(0);\n  Kokkos::parallel_for(\"countQuartiles\", Kokkos::RangePolicy<Kokkos::OpenMP>(0,size), [&] (size_t i) {\n    if(x[i] < 0.25)\n      Kokkos::atomic_increment<Kokkos::OpenMP>(&bins[0]);\n    else if(x[i] < 0.5)\n      Kokkos::atomic_increment<Kokkos::OpenMP>(&bins[1]);\n    else if(x[i] < 0.75)\n      Kokkos::atomic_increment<Kokkos::OpenMP>(&bins[2]);\n    else if(x[i] <= 1)\n      Kokkos::atomic_increment<Kokkos::OpenMP>(&bins[3]);\n  });\n\n  Kokkos::fence();\n\n}",
            "// Your code goes here\n}",
            "// TODO\n}",
            "Kokkos::View<size_t*> bins_view(\"bins_view\", 4);\n    Kokkos::parallel_for(\"count_quartiles\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        const double v = x(i);\n        if (v > 0 && v < 0.25)\n            Kokkos::atomic_increment(&bins_view(0));\n        else if (v >= 0.25 && v < 0.5)\n            Kokkos::atomic_increment(&bins_view(1));\n        else if (v >= 0.5 && v < 0.75)\n            Kokkos::atomic_increment(&bins_view(2));\n        else if (v >= 0.75 && v <= 1)\n            Kokkos::atomic_increment(&bins_view(3));\n    });\n    Kokkos::deep_copy(bins, bins_view);\n}",
            "// TODO: Your code here!\n}",
            "// Write your code here\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement\n}",
            "// Implement this function\n}",
            "// TODO\n}",
            "//...\n}",
            "// TODO\n}",
            "// YOUR CODE GOES HERE\n    // Replace this with the actual implementation.\n    // You are free to use whatever C++ constructs you like, but you should\n    // not use the Kokkos \"parallel_for\" construct, since we will supply\n    // a Kokkos implementation that uses it for you.\n\n    // TODO: Implement me\n}",
            "// TODO\n  Kokkos::parallel_for(\"\", 0, x.size(), KOKKOS_LAMBDA(size_t i) {\n    if (x[i] > 0 && x[i] < 0.25) {\n      ++bins[0];\n    }\n    else if (x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    }\n    else if (x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    }\n    else if (x[i] >= 0.75 && x[i] <= 1) {\n      ++bins[3];\n    }\n  });\n}",
            "// TODO\n  auto f = KOKKOS_LAMBDA (const int i) {\n    double x_i = x(i);\n    double frac = x_i - floor(x_i);\n    if(frac >= 0.0 && frac < 0.25) {\n      Kokkos::atomic_increment(&bins[0]);\n    }\n    else if(frac >= 0.25 && frac < 0.5) {\n      Kokkos::atomic_increment(&bins[1]);\n    }\n    else if(frac >= 0.5 && frac < 0.75) {\n      Kokkos::atomic_increment(&bins[2]);\n    }\n    else if(frac >= 0.75 && frac < 1.0) {\n      Kokkos::atomic_increment(&bins[3]);\n    }\n    else {\n      printf(\"This should not be possible...\\n\");\n    }\n  };\n  Kokkos::parallel_for(x.extent(0), f);\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, x.extent(0)),\n    KOKKOS_LAMBDA (size_t i) {\n      // Insert code here to compute quartile counts\n    });\n  Kokkos::fence();\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// YOUR CODE HERE\n}",
            "size_t nThreads = omp_get_max_threads();\n\n  std::vector<std::array<size_t, 4>> local_bins(nThreads);\n  #pragma omp parallel\n  {\n    size_t tId = omp_get_thread_num();\n\n    local_bins[tId] = {0, 0, 0, 0};\n\n    for (auto const& x_i : x) {\n      if (x_i < 0.25) {\n        local_bins[tId][0]++;\n      } else if (x_i < 0.5) {\n        local_bins[tId][1]++;\n      } else if (x_i < 0.75) {\n        local_bins[tId][2]++;\n      } else if (x_i <= 1.0) {\n        local_bins[tId][3]++;\n      }\n    }\n  }\n\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto const& local_bin : local_bins) {\n    for (size_t i = 0; i < 4; i++) {\n      bins[i] += local_bin[i];\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement me!\n    // Hint: You can use OpenMP to parallelize this code!\n    // See https://www.openmp.org/specifications/ for a quick reference.\n    int n = x.size();\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        double x_i = x[i];\n        if (x_i < 0.25) bins[0] += 1;\n        else if (x_i < 0.5) bins[1] += 1;\n        else if (x_i < 0.75) bins[2] += 1;\n        else bins[3] += 1;\n    }\n}",
            "// Your code here\n\n}",
            "size_t nthreads = omp_get_num_threads();\n\n    #pragma omp parallel\n    {\n        size_t tid = omp_get_thread_num();\n        size_t start = tid * (x.size() / nthreads);\n        size_t end = (tid + 1) * (x.size() / nthreads);\n\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        // TODO: Your code here\n        for (size_t i = start; i < end; i++) {\n            if (x[i] <= 0.25) {\n                bins[0] += 1;\n            } else if (x[i] <= 0.5) {\n                bins[1] += 1;\n            } else if (x[i] <= 0.75) {\n                bins[2] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        }\n\n        // TODO: Your code here\n\n        if (tid == 0) {\n            for (size_t i = 1; i < nthreads; i++) {\n                bins[0] += bins[0];\n                bins[1] += bins[1];\n                bins[2] += bins[2];\n                bins[3] += bins[3];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            #pragma omp atomic update\n            bins[0]++;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n            #pragma omp atomic update\n            bins[1]++;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n            #pragma omp atomic update\n            bins[2]++;\n        }\n        else if (x[i] >= 0.75 && x[i] < 1) {\n            #pragma omp atomic update\n            bins[3]++;\n        }\n    }\n}",
            "size_t n = x.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            #pragma omp atomic update\n            bins[0]++;\n        } else if (frac < 0.5) {\n            #pragma omp atomic update\n            bins[1]++;\n        } else if (frac < 0.75) {\n            #pragma omp atomic update\n            bins[2]++;\n        } else {\n            #pragma omp atomic update\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i) {\n        auto d = x[i];\n        size_t bin = 0;\n        if(d >= 0 && d < 0.25)\n            bin = 0;\n        else if(d >= 0.25 && d < 0.5)\n            bin = 1;\n        else if(d >= 0.5 && d < 0.75)\n            bin = 2;\n        else if(d >= 0.75 && d < 1)\n            bin = 3;\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "int num_threads = omp_get_num_threads();\n  std::vector<double> count_per_thread(num_threads);\n  omp_set_num_threads(4);\n  #pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    size_t start_ind = thread_id * (x.size() / num_threads);\n    size_t end_ind = std::min(start_ind + (x.size() / num_threads), x.size());\n    double count = 0;\n    #pragma omp for\n    for(size_t i = start_ind; i < end_ind; i++) {\n      if(x[i] < 0.25)\n        count++;\n      else if(x[i] < 0.5)\n        count++;\n      else if(x[i] < 0.75)\n        count++;\n      else\n        count++;\n    }\n    count_per_thread[thread_id] = count;\n  }\n  for(int i = 0; i < num_threads; i++) {\n    bins[0] += count_per_thread[i];\n  }\n}",
            "// Your code here!\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            #pragma omp critical\n            {\n                bins[0]++;\n            }\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n            #pragma omp critical\n            {\n                bins[1]++;\n            }\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n            #pragma omp critical\n            {\n                bins[2]++;\n            }\n        }\n        else if (x[i] >= 0.75 && x[i] < 1) {\n            #pragma omp critical\n            {\n                bins[3]++;\n            }\n        }\n    }\n\n}",
            "int n_threads, my_id;\n    size_t count;\n    // Declare shared variables.\n    #pragma omp threadprivate(count, n_threads, my_id)\n    #pragma omp parallel shared(x, bins)\n    {\n        // Obtain thread number.\n        my_id = omp_get_thread_num();\n        // Find number of threads\n        n_threads = omp_get_num_threads();\n\n        // Compute the count.\n        for (size_t i = 0; i < x.size(); i++)\n        {\n            if (x[i] >= 0 && x[i] < 0.25)\n                count = count + 1;\n            else if (x[i] >= 0.25 && x[i] < 0.50)\n                count = count + 2;\n            else if (x[i] >= 0.50 && x[i] < 0.75)\n                count = count + 3;\n            else if (x[i] >= 0.75 && x[i] < 1.00)\n                count = count + 4;\n        }\n\n        // Assign to output vector.\n        bins[my_id] = count;\n    }\n\n    // Compute sum of the counts.\n    for (size_t i = 1; i < n_threads; i++)\n    {\n        bins[0] += bins[i];\n    }\n}",
            "// Your code here\n  int num_threads, tid;\n\n  #pragma omp parallel shared(x) private(num_threads, tid)\n  {\n  #pragma omp single\n  {\n    num_threads = omp_get_num_threads();\n  }\n  tid = omp_get_thread_num();\n  }\n\n\n  #pragma omp parallel for \n  for (int i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n\n  int i = 0;\n  int n = x.size();\n\n  int q = n/4;\n  #pragma omp parallel for \n  for (int i = 0; i < 4; ++i) {\n    int q = n/4;\n    for (int j = q*i; j < q*(i+1); ++j) {\n      if (x[j] >= 0 && x[j] < 0.25) {\n        bins[0]++;\n      } else if (x[j] >= 0.25 && x[j] < 0.5) {\n        bins[1]++;\n      } else if (x[j] >= 0.5 && x[j] < 0.75) {\n        bins[2]++;\n      } else if (x[j] >= 0.75 && x[j] < 1) {\n        bins[3]++;\n      }\n    }\n  }\n\n\n  #pragma omp parallel for \n  for (int i = 0; i < 4; ++i) {\n    int q = n/4;\n    for (int j = q*i; j < q*(i+1); ++j) {\n      if (x[j] >= 0 && x[j] < 0.25) {\n        bins[0]++;\n      } else if (x[j] >= 0.25 && x[j] < 0.5) {\n        bins[1]++;\n      } else if (x[j] >= 0.5 && x[j] < 0.75) {\n        bins[2]++;\n      } else if (x[j] >= 0.75 && x[j] < 1) {\n        bins[3]++;\n      }\n    }\n  }\n\n\n  #pragma omp parallel for \n  for (int i = 0; i < 4; ++i) {\n    int q = n/4;\n    for (int j = q*i; j < q*(i+1); ++j) {\n      if (x[j] >= 0 && x[j] < 0.25) {\n        bins[0]++;\n      } else if (x[j] >= 0.25 && x[j] < 0.5) {\n        bins[1]++;\n      } else if (x[j] >= 0.5 && x[j] < 0.75) {\n        bins[2]++;\n      } else if (x[j] >= 0.75 && x[j] < 1) {\n        bins[3]++;\n      }\n    }\n  }\n\n  #pragma omp parallel for \n  for (int i = 0; i < 4; ++i) {\n    int q = n/4;\n    for (int j = q*i; j < q*(i+1); ++j) {\n      if (x[j] >= 0 && x[j] < 0.25) {\n        bins[0]++;\n      } else if (x[j] >= 0.25 && x[j] < 0.5) {\n        bins[1]++;\n      } else if (x[j] >= 0.5 && x[j] < 0.75) {\n        bins[2]++;\n      } else if (x[j] >= 0.75 && x[j] < 1) {\n        bins[3]++;\n      }\n    }\n  }\n\n  #pragma omp parallel for \n  for (int i = 0; i < 4; ++i) {\n    int q = n/4",
            "// YOUR CODE HERE\n    size_t n = x.size();\n    size_t bins_total[4] = {0,0,0,0};\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++)\n    {\n        if (x[i] - floor(x[i]) < 0.25)\n        {\n            #pragma omp atomic\n            bins_total[0]++;\n        }\n        else if (x[i] - floor(x[i]) < 0.5)\n        {\n            #pragma omp atomic\n            bins_total[1]++;\n        }\n        else if (x[i] - floor(x[i]) < 0.75)\n        {\n            #pragma omp atomic\n            bins_total[2]++;\n        }\n        else if (x[i] - floor(x[i]) < 1)\n        {\n            #pragma omp atomic\n            bins_total[3]++;\n        }\n    }\n    bins = {bins_total[0], bins_total[1], bins_total[2], bins_total[3]};\n}",
            "size_t num_elements = x.size();\n  size_t num_threads = omp_get_max_threads();\n  size_t num_per_thread = num_elements / num_threads;\n  size_t rest = num_elements % num_threads;\n  bins.fill(0);\n\n  #pragma omp parallel for\n  for (size_t tid = 0; tid < num_threads; tid++) {\n    size_t start = tid * num_per_thread;\n    size_t end = (tid + 1) * num_per_thread;\n    if (tid == num_threads - 1) {\n      end += rest;\n    }\n    for (size_t i = start; i < end; i++) {\n      double val = x[i] - floor(x[i]);\n      if (val < 0.25) {\n        bins[0]++;\n      } else if (val < 0.5) {\n        bins[1]++;\n      } else if (val < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto frac = x[i] - floor(x[i]);\n    if (frac >= 0 && frac < 0.25) {\n      bins[0]++;\n    } else if (frac >= 0.25 && frac < 0.5) {\n      bins[1]++;\n    } else if (frac >= 0.5 && frac < 0.75) {\n      bins[2]++;\n    } else if (frac >= 0.75 && frac <= 1) {\n      bins[3]++;\n    }\n  }\n}",
            "// Fill the bins with zeroes\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel\n    {\n        // count the number of elements in each bin\n        #pragma omp for\n        for (int i = 0; i < x.size(); ++i) {\n            if (x[i] >= 0 && x[i] < 0.25) {\n                bins[0] += 1;\n            } else if (x[i] >= 0.25 && x[i] < 0.5) {\n                bins[1] += 1;\n            } else if (x[i] >= 0.5 && x[i] < 0.75) {\n                bins[2] += 1;\n            } else if (x[i] >= 0.75 && x[i] < 1) {\n                bins[3] += 1;\n            }\n        }\n    }\n}",
            "// TODO: Your code goes here.\n    // Note: omp_get_num_threads() returns the number of currently configured threads.\n    // Note: omp_get_thread_num() returns the ID of the current thread.\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++)\n  {\n\n      if (x[i] < 1.0 && x[i] >= 0.75)\n      {\n\n          #pragma omp atomic\n          bins[3]++;\n\n      }\n      else if (x[i] < 0.75 && x[i] >= 0.5)\n      {\n\n          #pragma omp atomic\n          bins[2]++;\n\n      }\n      else if (x[i] < 0.5 && x[i] >= 0.25)\n      {\n\n          #pragma omp atomic\n          bins[1]++;\n\n      }\n      else if (x[i] < 0.25 && x[i] >= 0)\n      {\n\n          #pragma omp atomic\n          bins[0]++;\n\n      }\n      else\n      {\n\n          // if the value is not in the range of [0, 1),\n          // do not update the bins\n\n      }\n\n  }\n}",
            "const size_t size = x.size();\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < size; ++i) {\n      if (x[i] < 0.25) {\n        #pragma omp atomic\n        bins[0] += 1;\n      }\n      else if (x[i] < 0.5) {\n        #pragma omp atomic\n        bins[1] += 1;\n      }\n      else if (x[i] < 0.75) {\n        #pragma omp atomic\n        bins[2] += 1;\n      }\n      else {\n        #pragma omp atomic\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "// TODO: Replace this comment with your code\n    //std::vector<double> x(10000000);\n    //std::default_random_engine gen(0);\n    //std::uniform_real_distribution<> dis(0, 1);\n    //std::generate(x.begin(), x.end(), [&dis, &gen]() { return dis(gen); });\n    //std::array<size_t, 4> bins;\n    //size_t i = 0;\n    //double quarter = 1 / 4.0;\n    //for (auto const& num : x) {\n    //    size_t index = std::floor(num / quarter);\n    //    bins[index]++;\n    //    i++;\n    //}\n\n\n    std::array<size_t, 4> bins;\n    size_t i = 0;\n    double quarter = 1 / 4.0;\n    #pragma omp parallel for\n    for (auto const& num : x) {\n        size_t index = std::floor(num / quarter);\n        bins[index]++;\n        i++;\n    }\n}",
            "// Use bins[i] to store the number of doubles in x that have a fractional \n  // part in [0.25*i, 0.25*(i+1)).\n  //\n  // Your code goes here!\n}",
            "int n_threads = 1;\n\tsize_t n = x.size();\n\tstd::vector<size_t> my_bins(4, 0);\n\n\t#pragma omp parallel num_threads(n_threads)\n\t{\n\t\tint tid = omp_get_thread_num();\n\t\tint n_threads = omp_get_num_threads();\n\n\t\t#pragma omp for\n\t\tfor (size_t i = 0; i < n; ++i) {\n\t\t\tif (x[i] < 0.25) {\n\t\t\t\tmy_bins[0]++;\n\t\t\t} else if (x[i] < 0.5) {\n\t\t\t\tmy_bins[1]++;\n\t\t\t} else if (x[i] < 0.75) {\n\t\t\t\tmy_bins[2]++;\n\t\t\t} else {\n\t\t\t\tmy_bins[3]++;\n\t\t\t}\n\t\t}\n\n\t\t// #pragma omp critical\n\t\tfor (int i = 0; i < 4; ++i) {\n\t\t\tbins[i] += my_bins[i];\n\t\t}\n\t}\n\n}",
            "// Initialize the count to zero\n    for (auto& bin: bins) {\n        bin = 0;\n    }\n\n    // Declare and initialize a variable to store the count for each thread\n    size_t thread_bins[4] = { 0 };\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            double d = (double)std::fmod(x[i], 1.0);\n            if (d < 0.25) {\n                thread_bins[0]++;\n            } else if (d < 0.5) {\n                thread_bins[1]++;\n            } else if (d < 0.75) {\n                thread_bins[2]++;\n            } else {\n                thread_bins[3]++;\n            }\n        }\n    }\n\n    // Accumulate the counts from each thread\n    for (size_t i = 0; i < 4; ++i) {\n        bins[i] += thread_bins[i];\n    }\n}",
            "// YOUR CODE HERE\n}",
            "#pragma omp parallel for \n    for (auto i = 0; i < x.size(); i++) {\n        auto const xi = x[i];\n        auto const fract = std::modf(xi, &xi);\n\n        if (fract >= 0.75) {\n            #pragma omp atomic\n            bins[3]++;\n        } else if (fract >= 0.5) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (fract >= 0.25) {\n            #pragma omp atomic\n            bins[1]++;\n        } else {\n            #pragma omp atomic\n            bins[0]++;\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    // Initialize local storage for each thread\n    #pragma omp for schedule(static)\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = 0;\n    }\n\n    // Count the number of times each bin is found in the vector `x`.\n    #pragma omp for schedule(static)\n    for (int i = 0; i < x.size(); ++i) {\n      double fractionalPart = x[i] - floor(x[i]);\n      if (fractionalPart < 0.25) {\n        bins[0]++;\n      }\n      else if (fractionalPart < 0.5) {\n        bins[1]++;\n      }\n      else if (fractionalPart < 0.75) {\n        bins[2]++;\n      }\n      else {\n        bins[3]++;\n      }\n    }\n\n    // Use the reduction clause to compute the total number of bins\n    #pragma omp for schedule(static) reduction(+ : bins)\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = 0;\n    }\n  }\n}",
            "#pragma omp parallel for\n   for(int i = 0; i < x.size(); i++) {\n      auto frac = x[i] - std::floor(x[i]);\n      if (0 <= frac && frac < 0.25) {\n         #pragma omp atomic update\n         bins[0] += 1;\n      } else if (0.25 <= frac && frac < 0.5) {\n         #pragma omp atomic update\n         bins[1] += 1;\n      } else if (0.5 <= frac && frac < 0.75) {\n         #pragma omp atomic update\n         bins[2] += 1;\n      } else if (0.75 <= frac && frac < 1) {\n         #pragma omp atomic update\n         bins[3] += 1;\n      }\n   }\n}",
            "int nthreads = omp_get_num_threads();\n  std::cout << \"Num threads: \" << nthreads << std::endl;\n  int num = x.size();\n\n#pragma omp parallel for\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    int n = value - (int)value;\n    if (n >= 0.0 && n < 0.25) {\n#pragma omp atomic\n      bins[0] += 1;\n    } else if (n >= 0.25 && n < 0.5) {\n#pragma omp atomic\n      bins[1] += 1;\n    } else if (n >= 0.5 && n < 0.75) {\n#pragma omp atomic\n      bins[2] += 1;\n    } else if (n >= 0.75 && n < 1.0) {\n#pragma omp atomic\n      bins[3] += 1;\n    }\n  }\n}",
            "bins.fill(0);\n\n# pragma omp parallel for num_threads(4)\n  for(int i = 0; i < x.size(); i++) {\n\n    double frac = std::modf(x[i], &x[i]);\n\n    if(frac >= 0 && frac <= 0.25) {\n      bins[0]++;\n    }\n    else if(frac > 0.25 && frac <= 0.50) {\n      bins[1]++;\n    }\n    else if(frac > 0.50 && frac <= 0.75) {\n      bins[2]++;\n    }\n    else if(frac > 0.75 && frac <= 1.00) {\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < bins.size(); ++i)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i) {\n        double remainder = x[i] - std::floor(x[i]);\n\n        if(remainder >= 0 && remainder < 0.25)\n            #pragma omp atomic\n            bins[0]++;\n        else if(remainder >= 0.25 && remainder < 0.5)\n            #pragma omp atomic\n            bins[1]++;\n        else if(remainder >= 0.5 && remainder < 0.75)\n            #pragma omp atomic\n            bins[2]++;\n        else if(remainder >= 0.75 && remainder < 1)\n            #pragma omp atomic\n            bins[3]++;\n    }\n}",
            "// TODO: Implement this function\n    \n}",
            "// TODO\n}",
            "// Set bins to 0\n    for (size_t i=0; i<4; i++) {\n        bins[i] = 0;\n    }\n    // Use OpenMP to count the number of elements in each bin.\n    // TODO: replace this comment with your code\n    #pragma omp parallel for\n    for (size_t i=0; i<x.size(); i++) {\n        auto xi = x[i];\n        if (xi >= 0 && xi < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (xi >= 0.25 && xi < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (xi >= 0.5 && xi < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (xi >= 0.75 && xi < 1.0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "}",
            "for (size_t i = 0; i < x.size(); ++i) {\n        // TODO: fill in the body of the loop\n    }\n}",
            "size_t const N = x.size();\n  bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n  std::vector<size_t> thread_bins(4, 0);\n  std::vector<std::vector<double>> thread_x(omp_get_max_threads(), std::vector<double>());\n  std::vector<std::vector<double>> thread_x_remain(omp_get_max_threads(), std::vector<double>());\n\n  #pragma omp parallel num_threads(omp_get_max_threads())\n  {\n    int tid = omp_get_thread_num();\n    size_t num_x = N / omp_get_num_threads();\n    size_t start = num_x * tid;\n    if (tid == omp_get_num_threads() - 1)\n      num_x = N - start;\n    thread_x[tid].resize(num_x);\n    for (size_t i = 0; i < num_x; i++)\n      thread_x[tid][i] = x[start + i];\n\n    #pragma omp barrier\n\n    for (size_t i = 0; i < num_x; i++) {\n      if (thread_x[tid][i] >= 0.0 && thread_x[tid][i] < 0.25)\n        thread_bins[0]++;\n      else if (thread_x[tid][i] >= 0.25 && thread_x[tid][i] < 0.5)\n        thread_bins[1]++;\n      else if (thread_x[tid][i] >= 0.5 && thread_x[tid][i] < 0.75)\n        thread_bins[2]++;\n      else if (thread_x[tid][i] >= 0.75 && thread_x[tid][i] < 1.0)\n        thread_bins[3]++;\n    }\n\n    #pragma omp barrier\n\n    #pragma omp critical\n    {\n      for (int i = 0; i < 4; i++)\n        bins[i] += thread_bins[i];\n    }\n  }\n}",
            "const size_t N = x.size();\n\n    #pragma omp parallel\n    {\n        // TODO: set the number of threads\n        #pragma omp single\n        {\n            #pragma omp taskgroup\n            {\n                #pragma omp task\n                {\n                    size_t tmp = 0;\n                    for (size_t i = 0; i < N; i++) {\n                        if (x[i] >= 0 && x[i] < 0.25) tmp++;\n                    }\n                    bins[0] = tmp;\n                }\n\n                #pragma omp task\n                {\n                    size_t tmp = 0;\n                    for (size_t i = 0; i < N; i++) {\n                        if (x[i] >= 0.25 && x[i] < 0.5) tmp++;\n                    }\n                    bins[1] = tmp;\n                }\n\n                #pragma omp task\n                {\n                    size_t tmp = 0;\n                    for (size_t i = 0; i < N; i++) {\n                        if (x[i] >= 0.5 && x[i] < 0.75) tmp++;\n                    }\n                    bins[2] = tmp;\n                }\n\n                #pragma omp task\n                {\n                    size_t tmp = 0;\n                    for (size_t i = 0; i < N; i++) {\n                        if (x[i] >= 0.75 && x[i] < 1) tmp++;\n                    }\n                    bins[3] = tmp;\n                }\n            }\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n  #pragma omp parallel\n  {\n    // TODO: Your code here\n  }\n}",
            "const int num_threads = omp_get_max_threads();\n  const double num_threads_d = static_cast<double>(num_threads);\n  const size_t n = x.size();\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    const double f = x[i] - std::floor(x[i]);\n    int bin = static_cast<int>(num_threads_d * f);\n    bin = std::min(bin, 3);\n    bin = std::max(bin, 0);\n#pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n\t\tsize_t bin = size_t(x[i] * 4);\n\t\t#pragma omp atomic\n\t\tbins[bin]++;\n    }\n\n}",
            "size_t bins_[4] = {0, 0, 0, 0};\n  #pragma omp parallel for\n  for(int i = 0; i < x.size(); ++i) {\n    double frac = fmod(x[i], 1);\n    if(frac >= 0 && frac < 0.25) bins_[0]++;\n    else if(frac >= 0.25 && frac < 0.5) bins_[1]++;\n    else if(frac >= 0.5 && frac < 0.75) bins_[2]++;\n    else if(frac >= 0.75 && frac < 1) bins_[3]++;\n  }\n  bins[0] = bins_[0];\n  bins[1] = bins_[1];\n  bins[2] = bins_[2];\n  bins[3] = bins_[3];\n}",
            "size_t n = x.size();\n\n#pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        double v = x[i];\n\n        /* Your code here */\n    }\n}",
            "// TODO: implement the function\n  omp_set_num_threads(4);\n\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n\n    if (x[i] >= 0 && x[i] < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    }\n    if (x[i] >= 0.25 && x[i] < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    }\n    if (x[i] >= 0.5 && x[i] < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    }\n    if (x[i] >= 0.75 && x[i] < 1) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n\n  //std::cout << \"size = \" << bins.size() << std::endl;\n  //for (size_t i = 0; i < bins.size(); i++) {\n  //  std::cout << bins[i] << \" \";\n  //}\n  //std::cout << std::endl;\n}",
            "#pragma omp parallel\n    {\n        size_t n = x.size();\n        size_t start = n * omp_get_thread_num() / omp_get_num_threads();\n        size_t end = n * (omp_get_thread_num() + 1) / omp_get_num_threads();\n        #pragma omp for\n        for (size_t i = start; i < end; ++i) {\n            // TODO\n        }\n    }\n}",
            "/* TODO */\n\n    // #pragma omp parallel for\n    // for (auto i = 0; i < x.size(); i++) {\n    //     int bin = static_cast<int>((x[i] * 4) % 4);\n    //     if (bin < 0) {\n    //         bin += 4;\n    //     }\n    //     #pragma omp atomic\n    //     bins[bin]++;\n    // }\n    // std::cout << \"bins: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n}",
            "bins = {0, 0, 0, 0};\n    size_t n = x.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n\n        auto const &xi = x[i];\n\n        if (xi < 0.25) {\n            #pragma omp atomic update\n            bins[0] += 1;\n        }\n        else if (xi < 0.50) {\n            #pragma omp atomic update\n            bins[1] += 1;\n        }\n        else if (xi < 0.75) {\n            #pragma omp atomic update\n            bins[2] += 1;\n        }\n        else if (xi < 1.0) {\n            #pragma omp atomic update\n            bins[3] += 1;\n        }\n\n    }\n\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    double x_i = x[i];\n    size_t bin = 0;\n    if (x_i > 0.75) {\n      bin = 3;\n    } else if (x_i > 0.5) {\n      bin = 2;\n    } else if (x_i > 0.25) {\n      bin = 1;\n    }\n    bins[bin]++;\n  }\n}",
            "size_t size = x.size();\n    int numThreads = omp_get_max_threads();\n    int chunkSize = size / numThreads;\n    size_t *counts = (size_t *)malloc(sizeof(size_t)*4);\n    double start = 0, end = 0;\n    size_t pos;\n\n    #pragma omp parallel shared(x, counts, chunkSize, start, end, pos) default(none)\n    {\n        size_t i = 0;\n        int tid = omp_get_thread_num();\n        size_t startChunk = chunkSize * tid;\n        size_t endChunk = startChunk + chunkSize;\n\n        if (endChunk > size)\n            endChunk = size;\n\n        if (tid == 0) {\n            start = 0;\n            end = 0.25;\n        } else if (tid == 1) {\n            start = 0.25;\n            end = 0.5;\n        } else if (tid == 2) {\n            start = 0.5;\n            end = 0.75;\n        } else if (tid == 3) {\n            start = 0.75;\n            end = 1;\n        }\n\n        #pragma omp for\n        for (i = startChunk; i < endChunk; i++) {\n            if (x[i] > start && x[i] < end)\n                counts[tid]++;\n        }\n\n        #pragma omp single\n        {\n            for (i = 1; i < numThreads; i++)\n                counts[0] += counts[i];\n        }\n    }\n    bins[0] = counts[0];\n    bins[1] = counts[1];\n    bins[2] = counts[2];\n    bins[3] = counts[3];\n}",
            "// Set all bins to zero.\n  bins = {0, 0, 0, 0};\n\n  /* Your solution goes here. */\n  size_t i = 0;\n  size_t j = 0;\n  double n = x.size();\n  double x1 = 0.25 * n;\n  double x2 = 0.50 * n;\n  double x3 = 0.75 * n;\n\n  #pragma omp parallel for\n  for (i = 0; i < n; i++)\n    {\n      if (x[i] <= x1)\n\t{\n\t  #pragma omp atomic\n\t  bins[0]++;\n\t}\n      else if (x1 < x[i] <= x2)\n\t{\n\t  #pragma omp atomic\n\t  bins[1]++;\n\t}\n      else if (x2 < x[i] <= x3)\n\t{\n\t  #pragma omp atomic\n\t  bins[2]++;\n\t}\n      else if (x3 < x[i])\n\t{\n\t  #pragma omp atomic\n\t  bins[3]++;\n\t}\n      else\n\t{\n\t  // Do nothing\n\t}\n    }\n\n  #pragma omp critical\n  for (i = 0; i < 4; i++)\n    {\n      printf(\"%lu\\n\", bins[i]);\n    }\n\n  return;\n}",
            "// TODO: fill in the blanks\n    int size = x.size();\n    int i;\n    // initialise the values of bins to 0\n    for (i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n#pragma omp parallel for\n    for (i = 0; i < size; i++) {\n        double element = x[i];\n        if (element >= 0 && element < 0.25) {\n            bins[0]++;\n        }\n        else if (element >= 0.25 && element < 0.5) {\n            bins[1]++;\n        }\n        else if (element >= 0.5 && element < 0.75) {\n            bins[2]++;\n        }\n        else if (element >= 0.75 && element < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "/* === Modify code below === */\n\n    /* === End Modify code below === */\n}",
            "bins = {{0, 0, 0, 0}};\n\n  // YOUR CODE HERE\n  #pragma omp parallel for\n  for(int i=0; i<x.size(); i++){\n    double temp = x[i] - std::floor(x[i]);\n    if (temp < 0.25)\n      bins[0] += 1;\n    else if (temp < 0.5)\n      bins[1] += 1;\n    else if (temp < 0.75)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n}",
            "///////////////////////////////////////////////////////////////////////////////////\n    //\n    // Insert your code here:\n\n\n    ///////////////////////////////////////////////////////////////////////////////////\n}",
            "// TODO: Fill the array `bins` with the counts of the elements in x\n    // that fall in each quartile.\n    int num_threads;\n    int thread_num;\n    double quarter = 1.0 / 4.0;\n\n    #pragma omp parallel shared(x, quarter) private(num_threads, thread_num)\n    {\n        num_threads = omp_get_num_threads();\n        thread_num = omp_get_thread_num();\n        std::cout << \"Hello World from thread \" << thread_num << \" of \" << num_threads << \" threads.\" << std::endl;\n\n        for (auto v : x) {\n            if (v < quarter && v >= 0) {\n                bins[0]++;\n            }\n            else if (v >= quarter && v < 2 * quarter) {\n                bins[1]++;\n            }\n            else if (v >= 2 * quarter && v < 3 * quarter) {\n                bins[2]++;\n            }\n            else if (v >= 3 * quarter && v <= 4 * quarter) {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  size_t num_threads = omp_get_num_threads();\n  std::vector<std::array<size_t, 4>> bins_threads(num_threads);\n\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int threadId = omp_get_thread_num();\n    double fraction = x[i] - floor(x[i]);\n    if (fraction < 0.25)\n      bins_threads[threadId][0]++;\n    else if (fraction < 0.5)\n      bins_threads[threadId][1]++;\n    else if (fraction < 0.75)\n      bins_threads[threadId][2]++;\n    else\n      bins_threads[threadId][3]++;\n  }\n\n  for (int i = 0; i < num_threads; i++) {\n    bins[0] += bins_threads[i][0];\n    bins[1] += bins_threads[i][1];\n    bins[2] += bins_threads[i][2];\n    bins[3] += bins_threads[i][3];\n  }\n}",
            "// Count the number of doubles in the vector x that have a fractional part in [0, 0.25)\n    // store the count in bins[0]\n    // Count the number of doubles in the vector x that have a fractional part in [0.25, 0.5)\n    // store the count in bins[1]\n    // Count the number of doubles in the vector x that have a fractional part in [0.5, 0.75)\n    // store the count in bins[2]\n    // Count the number of doubles in the vector x that have a fractional part in [0.75, 1)\n    // store the count in bins[3]\n    // Use OpenMP to compute in parallel.\n    // HINT: you may want to use the remainder operation (%) to compute the fractional parts of x\n    // HINT: you may want to use the floor() function\n\n}",
            "size_t N = x.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        double remainder = x[i] - (int) x[i];\n        if (remainder >= 0 && remainder < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (remainder >= 0.25 && remainder < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (remainder >= 0.5 && remainder < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (remainder >= 0.75 && remainder < 1) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for num_threads(4)\n    for (size_t i=0; i<x.size(); i++) {\n\n        if (x[i] >= 0 && x[i] < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: count the number of values of x that lie in each of the \n    // four quartiles. Store the results in bins\n\n}",
            "// TODO\n}",
            "size_t const n = x.size();\n  for (size_t i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    double const x_i = x[i];\n    double const fraction = x_i - floor(x_i);\n    size_t const bin = static_cast<size_t>(fraction / 0.25);\n    bins[bin] += 1;\n  }\n}",
            "size_t num_elements = x.size();\n\n    #pragma omp parallel for\n    for (size_t i=0; i<num_elements; i++) {\n        // TODO: Add one to the appropriate bin\n    }\n\n    /*\n    for (size_t i=0; i<num_elements; i++) {\n        // TODO: Add one to the appropriate bin\n    }\n    */\n}",
            "// Initialize the bins to zero\n  bins.fill(0);\n\n#pragma omp parallel\n  {\n    std::array<size_t, 4> private_bins;\n    private_bins.fill(0);\n\n#pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      auto frac = x[i] - std::floor(x[i]);\n      if (frac <= 0.25) {\n        private_bins[0] += 1;\n      } else if (frac <= 0.5) {\n        private_bins[1] += 1;\n      } else if (frac <= 0.75) {\n        private_bins[2] += 1;\n      } else {\n        private_bins[3] += 1;\n      }\n    }\n\n#pragma omp critical\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] += private_bins[i];\n    }\n  }\n}",
            "size_t n = x.size();\n    bins = {0, 0, 0, 0};\n\n    // TODO: insert your code here\n\n    // parallelize with OpenMP!\n    #pragma omp parallel for\n    for(int i=0;i<n;i++)\n    {\n        if(x[i]>=0 && x[i]<0.25) bins[0]++;\n        else if(x[i]>=0.25 && x[i]<0.5) bins[1]++;\n        else if(x[i]>=0.5 && x[i]<0.75) bins[2]++;\n        else bins[3]++;\n    }\n\n}",
            "#pragma omp parallel for \n  for (int i = 0; i < x.size(); i++) {\n    int index = 0;\n    if (x[i] < 0.25) {\n      index = 0;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      index = 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      index = 2;\n    } else if (x[i] >= 0.75 && x[i] < 1.0) {\n      index = 3;\n    }\n    #pragma omp atomic \n    bins[index] += 1;\n  }\n}",
            "size_t n = x.size();\n   for (int i = 0; i < n; ++i) {\n       size_t c = (size_t) ((x[i] + 0.25) / 0.25);\n       if (c > 3)\n           c = 3;\n       bins[c]++;\n   }\n}",
            "int n = x.size();\n  double xmin = x[0];\n  double xmax = x[0];\n  for (auto const& i : x) {\n    xmin = std::min(xmin, i);\n    xmax = std::max(xmax, i);\n  }\n  double dx = (xmax - xmin) / 4;\n  int nthr = omp_get_max_threads();\n  #pragma omp parallel for\n  for (int i = 0; i < nthr; i++) {\n    double q1 = xmin + (i + 0.0) * dx;\n    double q2 = xmin + (i + 1.0) * dx;\n    double q3 = xmin + (i + 2.0) * dx;\n    double q4 = xmin + (i + 3.0) * dx;\n    size_t b1 = std::count_if(x.begin(), x.end(), [=](double v) {\n      return q1 <= v && v < q2;\n    });\n    size_t b2 = std::count_if(x.begin(), x.end(), [=](double v) {\n      return q2 <= v && v < q3;\n    });\n    size_t b3 = std::count_if(x.begin(), x.end(), [=](double v) {\n      return q3 <= v && v < q4;\n    });\n    size_t b4 = std::count_if(x.begin(), x.end(), [=](double v) {\n      return q4 <= v && v <= xmax;\n    });\n    #pragma omp critical\n    {\n      bins[0] += b1;\n      bins[1] += b2;\n      bins[2] += b3;\n      bins[3] += b4;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) bins[0]++;\n        else if (x[i] >= 0.25 && x[i] < 0.5) bins[1]++;\n        else if (x[i] >= 0.5 && x[i] < 0.75) bins[2]++;\n        else if (x[i] >= 0.75 && x[i] < 1) bins[3]++;\n    }\n}",
            "/* Your code here */\n    //omp_set_num_threads(4);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++)\n    {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25)\n            bins[0]++;\n        else if (frac < 0.5)\n            bins[1]++;\n        else if (frac < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double x_frac = x[i] - std::floor(x[i]);\n        if (x_frac < 0.25) {\n            bins[0]++;\n        } else if (x_frac < 0.5) {\n            bins[1]++;\n        } else if (x_frac < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// YOUR CODE HERE\n    //bins = {0,0,0,0};\n    size_t t[4] = {0,0,0,0};\n    //std::vector<double> quartiles;\n    \n    for(int i = 0; i < x.size(); i++){\n        if(x[i] <= 0.25){\n            t[0] = t[0] + 1;\n        }\n        else if(x[i] > 0.25 && x[i] <= 0.5){\n            t[1] = t[1] + 1;\n        }\n        else if(x[i] > 0.5 && x[i] <= 0.75){\n            t[2] = t[2] + 1;\n        }\n        else if(x[i] > 0.75 && x[i] <= 1.0){\n            t[3] = t[3] + 1;\n        }\n        else{\n            std::cout << \"error\" << std::endl;\n        }\n    }\n    bins[0] = t[0];\n    bins[1] = t[1];\n    bins[2] = t[2];\n    bins[3] = t[3];\n    \n}",
            "// TODO\n\t// 1. Initialize `bins` with zeros.\n\t// 2. For each element of `x`, determine which quartile it falls in.\n\t// 3. Increment the corresponding entry of `bins`.\n\t// 4. Use OpenMP to execute the loop above in parallel.\n\t// 5. Make sure to use the `#pragma omp atomic` directive to protect the `bins` updates.\n}",
            "// TODO\n}",
            "// TODO: fill bins with the number of elements in x that fall in each of the 4 quartiles\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (0 < x[i] && x[i] < 0.25) {\n            bins[0]++;\n        } else if (0.25 <= x[i] && x[i] < 0.5) {\n            bins[1]++;\n        } else if (0.5 <= x[i] && x[i] < 0.75) {\n            bins[2]++;\n        } else if (0.75 <= x[i] && x[i] < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// your code here\n  constexpr double start = 0.0;\n  constexpr double end = 1.0;\n  constexpr double step = 0.25;\n\n  std::array<size_t, 4> counts{0, 0, 0, 0};\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double val = x.at(i);\n    if (val > start && val < end) {\n      size_t bin = (val - start) / step;\n      // printf(\"%f %lu\\n\", val, bin);\n      counts.at(bin) += 1;\n    }\n  }\n\n  bins.at(0) = counts.at(0);\n  bins.at(1) = counts.at(1);\n  bins.at(2) = counts.at(2);\n  bins.at(3) = counts.at(3);\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        double decimal = x[i] - std::floor(x[i]);\n        if(decimal >= 0 && decimal < 0.25) bins[0]++;\n        else if(decimal >= 0.25 && decimal < 0.5) bins[1]++;\n        else if(decimal >= 0.5 && decimal < 0.75) bins[2]++;\n        else if(decimal >= 0.75 && decimal < 1) bins[3]++;\n    }\n}",
            "#pragma omp parallel\n    {\n        size_t local_bins[4] = {0};\n\n        #pragma omp for nowait\n        for (size_t i = 0; i < x.size(); i++) {\n            double x_i = x[i] - std::floor(x[i]);\n\n            if (x_i < 0.25) {\n                local_bins[0]++;\n            }\n            else if (x_i < 0.5) {\n                local_bins[1]++;\n            }\n            else if (x_i < 0.75) {\n                local_bins[2]++;\n            }\n            else if (x_i <= 1) {\n                local_bins[3]++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 4; i++) {\n                bins[i] += local_bins[i];\n            }\n        }\n    }\n}",
            "size_t n = x.size();\n\n  // TODO: add your code here!\n  // You are free to use additional arrays/vectors/etc. to store intermediate results.\n  // You can assume the vector `x` contains only non-negative doubles.\n  // Note: the output is stored in `bins`.\n\n}",
            "/* YOUR CODE HERE */\n\n}",
            "for (int i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n    double lower, upper;\n    int num_threads = omp_get_max_threads();\n    std::vector<std::array<size_t, 4>> bins_local(num_threads);\n\n    #pragma omp parallel for num_threads(num_threads)\n    for (size_t i = 0; i < x.size(); ++i) {\n        int thread_id = omp_get_thread_num();\n        if (x[i] >= 0 && x[i] < 1) {\n            bins_local[thread_id][0] += 1;\n        }\n        if (x[i] >= 1 && x[i] < 2) {\n            bins_local[thread_id][1] += 1;\n        }\n        if (x[i] >= 2 && x[i] < 3) {\n            bins_local[thread_id][2] += 1;\n        }\n        if (x[i] >= 3 && x[i] < 4) {\n            bins_local[thread_id][3] += 1;\n        }\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        for (int j = 0; j < num_threads; ++j) {\n            bins[i] += bins_local[j][i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t bin = 0;\n    if (x[i] >= 0.25 && x[i] < 0.5) {\n      bin = 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bin = 2;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bin = 3;\n    }\n    #pragma omp atomic\n    bins[bin]++;\n  }\n}",
            "auto const N = x.size();\n  auto const n_threads = omp_get_max_threads();\n  auto const chunk_size = N / n_threads;\n  #pragma omp parallel for\n  for (size_t tid = 0; tid < n_threads; ++tid) {\n    auto const start = tid * chunk_size;\n    auto const end = (tid == n_threads - 1)? N : (tid + 1) * chunk_size;\n    for (size_t i = start; i < end; ++i) {\n      auto const frac = std::modf(x[i], &x[i]);\n      if (frac >= 0 && frac < 0.25) {\n        bins[0]++;\n      } else if (frac >= 0.25 && frac < 0.5) {\n        bins[1]++;\n      } else if (frac >= 0.5 && frac < 0.75) {\n        bins[2]++;\n      } else if (frac >= 0.75 && frac < 1) {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "bins = std::array<size_t, 4> {0, 0, 0, 0};\n    size_t size = x.size();\n\n    // TODO: use OpenMP to compute in parallel\n    #pragma omp parallel for\n    for (size_t i = 0; i < size; ++i)\n    {\n        int idx = std::floor(x[i] * 4);\n        ++bins[idx];\n    }\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < x.size(); i++) {\n        auto value = x[i];\n        auto index = std::floor(value);\n        auto remainder = value - index;\n        if (remainder < 0.25) {\n            // bin 0\n            #pragma omp atomic\n            bins[0]++;\n        } else if (remainder < 0.5) {\n            // bin 1\n            #pragma omp atomic\n            bins[1]++;\n        } else if (remainder < 0.75) {\n            // bin 2\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            // bin 3\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "/* Your solution goes here */\n}",
            "bins = {0, 0, 0, 0};\n  #pragma omp parallel for reduction(+ : bins[0])\n  for (auto i = 0u; i < x.size(); ++i) {\n    auto v = x[i];\n    if (0 <= v && v < 0.25) {\n      ++bins[0];\n    }\n    else if (0.25 <= v && v < 0.5) {\n      ++bins[1];\n    }\n    else if (0.5 <= v && v < 0.75) {\n      ++bins[2];\n    }\n    else if (0.75 <= v && v <= 1) {\n      ++bins[3];\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto const frac = x[i] - std::floor(x[i]);\n    if (frac >= 0 && frac < 0.25) {\n      #pragma omp atomic\n      bins[0] += 1;\n    }\n    else if (frac >= 0.25 && frac < 0.5) {\n      #pragma omp atomic\n      bins[1] += 1;\n    }\n    else if (frac >= 0.5 && frac < 0.75) {\n      #pragma omp atomic\n      bins[2] += 1;\n    }\n    else if (frac >= 0.75 && frac <= 1.0) {\n      #pragma omp atomic\n      bins[3] += 1;\n    }\n  }\n}",
            "// Write your code here\n\tbins = std::array<size_t, 4>{};\n}",
            "size_t n = x.size();\n    // TODO: initialize bins to 0\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        double x_i = x[i];\n        // TODO: add 1 to the bin for the correct quartile\n    }\n}",
            "size_t n = x.size();\n    size_t n_threads = omp_get_max_threads();\n    std::vector<size_t> counts(n_threads, 0);\n    #pragma omp parallel for\n    for(size_t i = 0; i < n; i++){\n        double fract = x[i] - floor(x[i]);\n        if (fract < 0.25){\n            counts[omp_get_thread_num()]++;\n        }\n        else if (fract < 0.5){\n            counts[omp_get_thread_num()]++;\n        }\n        else if (fract < 0.75){\n            counts[omp_get_thread_num()]++;\n        }\n        else{\n            counts[omp_get_thread_num()]++;\n        }\n    }\n    size_t start = 0;\n    for(size_t i = 0; i < counts.size(); i++){\n        bins[i] = counts[i];\n    }\n}",
            "// Create a private copy of `bins` for each thread.\n  // Initialize the private copies to zero.\n  std::array<size_t, 4> localBins;\n  localBins.fill(0);\n\n  // Use `omp_get_thread_num` to get the current thread's ID.\n  int id = omp_get_thread_num();\n\n  for (size_t i=0; i<x.size(); ++i) {\n    // Each thread counts the number of elements in its private `localBins`\n    double y = x[i];\n    double z = y - std::floor(y);\n    if (z >= 0 && z < 0.25) {\n      localBins[0]++;\n    } else if (z >= 0.25 && z < 0.5) {\n      localBins[1]++;\n    } else if (z >= 0.5 && z < 0.75) {\n      localBins[2]++;\n    } else if (z >= 0.75 && z < 1.0) {\n      localBins[3]++;\n    }\n  }\n\n  // At the end of the parallel region, sum the private `localBins`\n  // into the global `bins`.\n  #pragma omp critical\n  {\n    for (size_t i=0; i<4; ++i) {\n      bins[i] += localBins[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double q = x[i] - std::floor(x[i]);\n    if (q >= 0 && q < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (q >= 0.25 && q < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (q >= 0.5 && q < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (q >= 0.75 && q < 1) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "// Use OpenMP to run the loops in parallel\n    omp_set_num_threads(4);\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            // Your code here\n        }\n    }\n}",
            "// YOUR CODE HERE\n\n}",
            "// TODO: Fill in the code to find the counts of quartiles\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i)\n        {\n            if (x[i] < 0.25) {\n                #pragma omp atomic update\n                bins[0]++;\n            }\n            else if (x[i] >= 0.25 && x[i] < 0.5) {\n                #pragma omp atomic update\n                bins[1]++;\n            }\n            else if (x[i] >= 0.5 && x[i] < 0.75) {\n                #pragma omp atomic update\n                bins[2]++;\n            }\n            else if (x[i] >= 0.75 && x[i] < 1) {\n                #pragma omp atomic update\n                bins[3]++;\n            }\n        }\n    }\n}",
            "size_t n = x.size();\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    #pragma omp parallel for reduction(+ : bins[0]) reduction(+ : bins[1]) reduction(+ : bins[2]) reduction(+ : bins[3])\n    for (size_t i = 0; i < n; ++i) {\n        double frac = x[i] - std::floor(x[i]);\n        if (frac < 0.25) {\n            ++bins[0];\n        }\n        else if (frac < 0.5) {\n            ++bins[1];\n        }\n        else if (frac < 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO: Your code goes here!\n\n}",
            "const int N = x.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        int index = 4 * x[i];\n        #pragma omp atomic\n        bins[index] += 1;\n    }\n}",
            "size_t n = x.size();\n    size_t n_threads = omp_get_num_threads();\n    std::vector<std::array<size_t, 4>> private_bins(n_threads);\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n\n        size_t id = omp_get_thread_num();\n        double frac = x[i] - floor(x[i]);\n\n        if (frac >= 0 && frac < 0.25) {\n            private_bins[id][0]++;\n        } else if (frac >= 0.25 && frac < 0.5) {\n            private_bins[id][1]++;\n        } else if (frac >= 0.5 && frac < 0.75) {\n            private_bins[id][2]++;\n        } else if (frac >= 0.75 && frac < 1) {\n            private_bins[id][3]++;\n        } else {\n            throw std::runtime_error(\"Invalid fraction\");\n        }\n    }\n\n    // sum up the private bins\n    for (size_t i = 1; i < n_threads; ++i) {\n        for (size_t j = 0; j < 4; ++j) {\n            bins[j] += private_bins[i][j];\n        }\n    }\n}",
            "size_t n = x.size();\n#pragma omp parallel for\n    for (size_t i = 0; i < n; i++)\n    {\n        int index = static_cast<int>(x[i] * 4) % 4;\n        #pragma omp atomic\n        bins[index]++;\n    }\n}",
            "// This function should be implemented using OpenMP, without modifying the\n  // for loop\n\n  // Initialize `bins` with 0\n  bins = {0, 0, 0, 0};\n  // Set the number of threads to use to omp_get_max_threads()\n  // #pragma omp parallel num_threads(omp_get_max_threads())\n\n  // Each thread should increment the appropriate element of `bins`\n\n  // #pragma omp for\n  for (size_t i = 0; i < x.size(); i++) {\n    double fractional_part = x[i] - floor(x[i]);\n    if (fractional_part < 0.25) {\n      // #pragma omp atomic\n      // bins[0] += 1;\n      bins[0]++;\n    } else if (fractional_part < 0.5) {\n      // #pragma omp atomic\n      // bins[1] += 1;\n      bins[1]++;\n    } else if (fractional_part < 0.75) {\n      // #pragma omp atomic\n      // bins[2] += 1;\n      bins[2]++;\n    } else {\n      // #pragma omp atomic\n      // bins[3] += 1;\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel\n    {\n        size_t localBins[4] = {0, 0, 0, 0};\n\n        #pragma omp for nowait\n        for (size_t i = 0; i < x.size(); ++i) {\n            double fractional = x[i] - floor(x[i]);\n            if (fractional < 0.25) {\n                ++localBins[0];\n            } else if (fractional < 0.5) {\n                ++localBins[1];\n            } else if (fractional < 0.75) {\n                ++localBins[2];\n            } else {\n                ++localBins[3];\n            }\n        }\n\n        #pragma omp critical\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] += localBins[i];\n        }\n    }\n}",
            "#pragma omp parallel for num_threads(2)\n  for (int i = 0; i < x.size(); i++) {\n    int bin = 0;\n    if (x[i] >= 0.75 && x[i] < 1) {\n      bin = 3;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bin = 2;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bin = 1;\n    } else if (x[i] >= 0 && x[i] < 0.25) {\n      bin = 0;\n    }\n    #pragma omp atomic\n    bins[bin] += 1;\n  }\n}",
            "#pragma omp parallel for schedule(dynamic, 1)\n    for(size_t i = 0; i < x.size(); ++i){\n        int index = static_cast<int>(((x[i] % 1) * 4));\n        if (index < 0 || index > 3) {\n            std::cerr << \"Bad index \" << index << \" for element \" << x[i] << std::endl;\n        }\n        else {\n            #pragma omp atomic\n            ++bins[index];\n        }\n    }\n\n}",
            "size_t n = x.size();\n  #pragma omp parallel\n  {\n    // TODO: Initialize local counters\n    #pragma omp for schedule(static)\n    for (int i = 0; i < n; ++i) {\n      // TODO: Increment the correct bin based on the value of x[i]\n    }\n    #pragma omp critical\n    {\n      // TODO: Add up the local counters to bins\n    }\n  }\n}",
            "// Add your code here\n\n}",
            "// TODO\n}",
            "size_t const n = x.size();\n    bins = {0, 0, 0, 0};\n    // #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        double const xi = x[i];\n        if (xi >= 0.0 && xi < 0.25) {\n            bins[0] += 1;\n        } else if (xi >= 0.25 && xi < 0.5) {\n            bins[1] += 1;\n        } else if (xi >= 0.5 && xi < 0.75) {\n            bins[2] += 1;\n        } else if (xi >= 0.75 && xi <= 1.0) {\n            bins[3] += 1;\n        }\n    }\n    // #pragma omp critical\n    // std::cout << \"bins = \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n    // std::cout << \"n = \" << n << std::endl;\n}",
            "// Add your code here\n    // The code in the following lines is just to test your code.\n    // It should be removed.\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    for (auto x_i: x) {\n        auto d = x_i - std::floor(x_i);\n        if (d < 0.25) {\n            bins[0] += 1;\n        } else if (d < 0.5) {\n            bins[1] += 1;\n        } else if (d < 0.75) {\n            bins[2] += 1;\n        } else if (d <= 1) {\n            bins[3] += 1;\n        }\n    }\n}",
            "// Get the size of x\n    size_t n = x.size();\n\n    // Initialize `bins` to zero.\n    bins = {0, 0, 0, 0};\n\n    // For each element of x, increment the correct bin.\n    for (size_t i = 0; i < n; i++) {\n        double x_i = x[i];\n        int bin_index = 0;\n        if (x_i < 0.25) {\n            bin_index = 0;\n        } else if (x_i < 0.5) {\n            bin_index = 1;\n        } else if (x_i < 0.75) {\n            bin_index = 2;\n        } else {\n            bin_index = 3;\n        }\n        bins[bin_index]++;\n    }\n}",
            "int n_threads = omp_get_num_threads();\n    int thread_id = omp_get_thread_num();\n    //printf(\"Hello, World! from thread %d\\n\", thread_id);\n    //int n = x.size();\n    //int chunk = n / n_threads;\n    //int start = thread_id * chunk;\n    //int end = start + chunk;\n    //if (thread_id == n_threads - 1) {\n    //    end = n;\n    //}\n    //for (int i = start; i < end; i++) {\n    //    double a = x[i];\n    //    int index = static_cast<int>(a) % 4;\n    //    bins[index]++;\n    //}\n    for (double i : x) {\n        int index = static_cast<int>(i) % 4;\n        //printf(\"%d \", index);\n        bins[index]++;\n    }\n    //printf(\"Hello, World! from thread %d\\n\", thread_id);\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] <= 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (x[i] <= 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (x[i] <= 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "size_t n = x.size();\n    size_t max_threads = omp_get_max_threads();\n    std::vector<size_t> counts(n, 0);\n#pragma omp parallel num_threads(max_threads)\n    {\n        size_t i, p;\n#pragma omp for\n        for (i = 0; i < n; ++i) {\n            p = 0;\n            if (x[i] >= 0.75) {\n                p = 3;\n            } else if (x[i] >= 0.5) {\n                p = 2;\n            } else if (x[i] >= 0.25) {\n                p = 1;\n            }\n            counts[p] += 1;\n        }\n    }\n    bins[0] = counts[0];\n    bins[1] = counts[1];\n    bins[2] = counts[2];\n    bins[3] = counts[3];\n}",
            "size_t bins_count[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (0 <= x[i] && x[i] < 0.25)\n            bins_count[0]++;\n        else if (0.25 <= x[i] && x[i] < 0.5)\n            bins_count[1]++;\n        else if (0.5 <= x[i] && x[i] < 0.75)\n            bins_count[2]++;\n        else if (0.75 <= x[i] && x[i] < 1)\n            bins_count[3]++;\n    }\n\n    for (int i = 0; i < 4; i++) {\n        bins[i] = bins_count[i];\n    }\n}",
            "}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for(int i = 0; i < x.size(); i++){\n        if(x[i] > 0 && x[i] < 0.25){\n            bins[0] += 1;\n        }\n        else if(x[i] > 0.25 && x[i] < 0.5){\n            bins[1] += 1;\n        }\n        else if(x[i] > 0.5 && x[i] < 0.75){\n            bins[2] += 1;\n        }\n        else if(x[i] > 0.75 && x[i] < 1){\n            bins[3] += 1;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] <= 1) {\n      bins[0]++;\n    } else if (x[i] <= 2) {\n      bins[1]++;\n    } else if (x[i] <= 3) {\n      bins[2]++;\n    } else if (x[i] <= 4) {\n      bins[3]++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n  size_t n = x.size();\n  size_t num_threads = 4;\n  std::vector<std::array<size_t, 4> > bins_tmp(num_threads);\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (size_t i = 0; i < n; ++i) {\n    double v = x[i];\n    double q = std::fmod(v, 1.0);\n    if (q == 0) {\n      bins_tmp[omp_get_thread_num()][0]++;\n    } else if (q > 0 && q <= 0.25) {\n      bins_tmp[omp_get_thread_num()][1]++;\n    } else if (q > 0.25 && q <= 0.5) {\n      bins_tmp[omp_get_thread_num()][2]++;\n    } else if (q > 0.5 && q <= 0.75) {\n      bins_tmp[omp_get_thread_num()][3]++;\n    }\n  }\n  for (size_t i = 0; i < 4; i++) {\n    for (size_t j = 0; j < num_threads; j++) {\n      bins[i] += bins_tmp[j][i];\n    }\n  }\n}",
            "// TODO\n\n}",
            "// TODO: Your code here\n  // Note that:\n  // - `x` is a const reference, and cannot be modified\n  // - `bins` is a non-const reference, and can be modified\n\n  // **** begin solution ****\n\n  // 1. initialize bins to 0\n  for (auto &val : bins) {\n    val = 0;\n  }\n\n  // 2. loop through x and increment the appropriate bin\n  //    loop in parallel with OpenMP\n  #pragma omp parallel for\n  for (size_t i=0; i<x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bins[3]++;\n    }\n  }\n\n  // **** end solution ****\n\n  return;\n}",
            "bins = {0, 0, 0, 0};\n    auto num_threads = omp_get_max_threads();\n    auto num_elems = x.size();\n    auto per_thread_elems = num_elems / num_threads;\n    auto num_leftover_elems = num_elems - per_thread_elems * num_threads;\n    auto curr_pos = 0;\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n#pragma omp parallel for default(none) shared(x, num_threads, per_thread_elems, num_leftover_elems, local_bins, bins)\n    for (auto i = 0; i < num_threads; ++i) {\n        auto curr_elems = (i < num_leftover_elems)? per_thread_elems + 1 : per_thread_elems;\n        for (auto j = 0; j < curr_elems; ++j) {\n            auto curr_elem = x[curr_pos + j];\n            if (curr_elem >= 0.0 && curr_elem < 0.25) {\n                local_bins[0] += 1;\n            } else if (curr_elem >= 0.25 && curr_elem < 0.5) {\n                local_bins[1] += 1;\n            } else if (curr_elem >= 0.5 && curr_elem < 0.75) {\n                local_bins[2] += 1;\n            } else if (curr_elem >= 0.75 && curr_elem < 1.0) {\n                local_bins[3] += 1;\n            }\n        }\n        curr_pos += curr_elems;\n    }\n\n    for (auto i = 0; i < 4; ++i) {\n        bins[i] = 0;\n        for (auto j = 0; j < num_threads; ++j) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double x_fractional = x[i] - floor(x[i]);\n    if (x_fractional < 0.25) {\n      #pragma omp atomic\n      bins[0] += 1;\n    } else if (x_fractional < 0.5) {\n      #pragma omp atomic\n      bins[1] += 1;\n    } else if (x_fractional < 0.75) {\n      #pragma omp atomic\n      bins[2] += 1;\n    } else {\n      #pragma omp atomic\n      bins[3] += 1;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] > 0 && x[i] < 0.25) {\n      ++bins[0];\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    } else if (x[i] >= 0.75 && x[i] <= 1) {\n      ++bins[3];\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  size_t xSize = x.size();\n  #pragma omp parallel for\n  for (size_t i = 0; i < xSize; i++) {\n    double fraction = x[i] - floor(x[i]);\n    if (fraction < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (fraction < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (fraction < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    } else {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    auto n = x.size();\n    int num_threads = omp_get_max_threads();\n    int chunk = n / num_threads;\n\n    // compute the sum of the integers in the range [start, end)\n    auto addBins = [&x](int start, int end, std::array<size_t, 4> &bins) {\n        for (int i = start; i < end; i++) {\n            if (x[i] < 0.25) bins[0]++;\n            else if (x[i] < 0.5) bins[1]++;\n            else if (x[i] < 0.75) bins[2]++;\n            else bins[3]++;\n        }\n    };\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_threads; i++) {\n        addBins(i * chunk, (i + 1) * chunk, bins);\n    }\n\n    // Add the remaining elements\n    addBins(num_threads * chunk, n, bins);\n}",
            "bins.fill(0);\n    size_t n = x.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        int index = 0;\n        if (x[i] >= 0 && x[i] < 0.25) {\n            index = 0;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n            index = 1;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n            index = 2;\n        }\n        else {\n            index = 3;\n        }\n        #pragma omp atomic\n        bins[index]++;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    auto const value = x[i];\n    auto const fractionalPart = value - floor(value);\n    auto const binIndex = std::min(3, int(4 * fractionalPart));\n    bins[binIndex] += 1;\n  }\n\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++) {\n            // Use if-else cascade to assign bin values\n            if (x[i] >= 0 && x[i] < 0.25)\n                bins[0]++;\n            else if (x[i] >= 0.25 && x[i] < 0.5)\n                bins[1]++;\n            else if (x[i] >= 0.5 && x[i] < 0.75)\n                bins[2]++;\n            else if (x[i] >= 0.75 && x[i] < 1)\n                bins[3]++;\n        }\n    }\n}",
            "// TODO\n\n}",
            "// TODO: Implement\n}",
            "#pragma omp parallel for \n  for (auto it = x.begin(); it < x.end(); it++) {\n    if (*it < 1) {\n      #pragma omp critical\n      bins[0]++;\n    }\n    else if (*it < 2) {\n      #pragma omp critical\n      bins[1]++;\n    }\n    else if (*it < 3) {\n      #pragma omp critical\n      bins[2]++;\n    }\n    else if (*it < 4) {\n      #pragma omp critical\n      bins[3]++;\n    }\n  }\n}",
            "}",
            "// TODO\n\n  bins = {0, 0, 0, 0};\n  size_t n = x.size();\n  int i;\n  int j;\n  double f;\n  int chunkSize;\n\n#pragma omp parallel\n{\n  chunkSize = n/omp_get_num_threads();\n#pragma omp for\n  for (i=0; i<n; i++) {\n    f = modf(x[i], &j);\n    if (f > 0.75) j = 3;\n    else if (f > 0.5) j = 2;\n    else if (f > 0.25) j = 1;\n    else j = 0;\n    #pragma omp atomic\n    bins[j]++;\n  }\n}\n\n}",
            "for (size_t i = 0; i < bins.size(); i++)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double f = x[i] - floor(x[i]);\n\n        // Your code here.\n\n    }\n}",
            "// Set the bins array to zero.\n  bins.fill(0);\n\n  // Create a parallel region.\n  // Use omp_get_num_threads() to obtain the number of threads.\n  // Use omp_get_thread_num() to obtain the thread ID.\n  // Use a parallel for loop with a private thread-local bins variable.\n  // Initialize the bins variable to zero.\n  // Increment the appropriate bin for the current value.\n  // Combine the private bins variables into a single bins variable at the end.\n}",
            "// YOUR CODE HERE\n\n#pragma omp parallel\n  {\n    std::array<size_t, 4> thread_bins{};\n\n#pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] < 0.25) {\n        ++thread_bins[0];\n      } else if (x[i] < 0.5) {\n        ++thread_bins[1];\n      } else if (x[i] < 0.75) {\n        ++thread_bins[2];\n      } else {\n        ++thread_bins[3];\n      }\n    }\n\n#pragma omp critical\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] += thread_bins[i];\n    }\n  }\n}",
            "size_t const n = x.size();\n  #pragma omp parallel for schedule(dynamic)\n  for (size_t i = 0; i < n; ++i) {\n    double const f = x[i] - std::floor(x[i]);\n    if (f >= 0 && f < 0.25) ++bins[0];\n    else if (f >= 0.25 && f < 0.5) ++bins[1];\n    else if (f >= 0.5 && f < 0.75) ++bins[2];\n    else if (f >= 0.75 && f < 1) ++bins[3];\n  }\n}",
            "// Initialize the bins to 0.\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n\n  // Add all the elements to the correct bin.\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    // TODO: Add an element to the correct bin\n  }\n}",
            "#pragma omp parallel for \n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// Your code here!\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        int bin = (int) ((x[i] - floor(x[i]))*4.0);\n        if (bin < 0) bin = 0;\n        if (bin > 3) bin = 3;\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "size_t n = x.size();\n\tint num_threads;\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp single\n\t\tnum_threads = omp_get_num_threads();\n\t}\n\t#pragma omp parallel for schedule(dynamic,1)\n\tfor (int tid = 0; tid < n; tid++) {\n\t\tdouble value = x[tid];\n\t\tif (value < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (value < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if (value < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if (value <= 1.0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "}",
            "int num_threads = omp_get_max_threads();\n  std::vector<size_t> localBins(num_threads * 4);\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) localBins[omp_get_thread_num() * 4]++;\n    else if (x[i] < 0.5) localBins[omp_get_thread_num() * 4 + 1]++;\n    else if (x[i] < 0.75) localBins[omp_get_thread_num() * 4 + 2]++;\n    else localBins[omp_get_thread_num() * 4 + 3]++;\n  }\n\n  for (int i = 1; i < num_threads; i++) {\n    localBins[0] += localBins[i * 4];\n    localBins[1] += localBins[i * 4 + 1];\n    localBins[2] += localBins[i * 4 + 2];\n    localBins[3] += localBins[i * 4 + 3];\n  }\n\n  bins = {localBins[0], localBins[1], localBins[2], localBins[3]};\n}",
            "size_t const num_threads = 4;\n    size_t const num_elements = x.size();\n    size_t const num_elements_per_thread = (num_elements / num_threads);\n\n    // TODO: Implement this function\n}",
            "// TODO: your code here\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++)\n    {\n        int frac = x[i] * 4 - x[i];\n        if(frac >= 0 && frac < 1){\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if(frac >= 1 && frac < 2){\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if(frac >= 2 && frac < 3){\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if(frac >= 3 && frac < 4){\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "}",
            "//TODO: your code goes here\n  \n  const int num_threads = omp_get_num_threads();\n  const int thread_id = omp_get_thread_num();\n\n  // Calculate the bins\n  int const chunk = x.size() / num_threads;\n  int const begin = thread_id * chunk;\n  int const end = (thread_id == num_threads - 1)? x.size() : begin + chunk;\n\n  // Count the quartiles\n  for (int i = begin; i < end; i++) {\n    if (x[i] < 0.25)\n      bins[0]++;\n    else if (x[i] < 0.5)\n      bins[1]++;\n    else if (x[i] < 0.75)\n      bins[2]++;\n    else\n      bins[3]++;\n  }\n}",
            "// Your code goes here\n    size_t n_elements = x.size();\n    int n_threads = omp_get_max_threads();\n    std::array<size_t, 4> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n    std::vector<size_t> bins_sum(n_threads);\n    std::fill(bins_sum.begin(), bins_sum.end(), 0);\n\n#pragma omp parallel for num_threads(n_threads)\n    for (size_t i = 0; i < n_elements; ++i) {\n        size_t index = static_cast<size_t>((x[i] - std::floor(x[i])) / 0.25);\n        local_bins[index] += 1;\n    }\n#pragma omp parallel for num_threads(n_threads)\n    for (size_t i = 0; i < n_threads; ++i) {\n        for (size_t j = 0; j < 4; ++j) {\n            bins_sum[i] += local_bins[j];\n        }\n    }\n\n    for (size_t j = 0; j < 4; ++j) {\n        for (size_t i = 0; i < n_threads; ++i) {\n            bins[j] += bins_sum[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        double d = x[i] - int(x[i]);\n        if (d < 0.25) {\n            #pragma omp atomic update\n            bins[0]++;\n        }\n        else if (d < 0.5) {\n            #pragma omp atomic update\n            bins[1]++;\n        }\n        else if (d < 0.75) {\n            #pragma omp atomic update\n            bins[2]++;\n        }\n        else {\n            #pragma omp atomic update\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if (x[i] < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if (x[i] < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            #pragma omp taskgroup\n            {\n                for (int i = 0; i < 4; i++) {\n                    #pragma omp task shared(bins)\n                    {\n                        double const x_min = i / 4.0;\n                        double const x_max = (i + 1) / 4.0;\n                        size_t count = 0;\n                        for (double x_val : x) {\n                            if (x_val >= x_min && x_val < x_max) {\n                                count += 1;\n                            }\n                        }\n                        bins[i] = count;\n                    }\n                }\n            }\n        }\n    }\n}",
            "size_t const n = x.size();\n    std::array<size_t, 4> binsLocal;\n\n    #pragma omp parallel\n    {\n        size_t const localSize = n / omp_get_num_threads();\n        size_t const localStart = omp_get_thread_num() * localSize;\n        size_t const localEnd = (omp_get_thread_num() + 1) * localSize;\n        size_t const localCount = localEnd - localStart;\n\n        for (size_t i = 0; i < 4; ++i)\n            binsLocal[i] = 0;\n\n        for (size_t i = localStart; i < localEnd; ++i) {\n            size_t bin = std::min(static_cast<int>(x[i] / 0.25), 3);\n            ++binsLocal[bin];\n        }\n\n        #pragma omp critical\n        for (size_t i = 0; i < 4; ++i)\n            bins[i] += binsLocal[i];\n    }\n}",
            "for (int i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n  int num_threads = omp_get_max_threads();\n  std::vector<std::array<size_t, 4> > temp_bins(num_threads);\n  #pragma omp parallel for\n  for (int i = 0; i < (int)x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 1) {\n      temp_bins[omp_get_thread_num()][0] += 1;\n    } else if (x[i] >= 1 && x[i] < 2) {\n      temp_bins[omp_get_thread_num()][1] += 1;\n    } else if (x[i] >= 2 && x[i] < 3) {\n      temp_bins[omp_get_thread_num()][2] += 1;\n    } else if (x[i] >= 3 && x[i] < 4) {\n      temp_bins[omp_get_thread_num()][3] += 1;\n    }\n  }\n  for (int i = 0; i < num_threads; ++i) {\n    for (int j = 0; j < 4; ++j) {\n      bins[j] += temp_bins[i][j];\n    }\n  }\n}",
            "size_t n = x.size();\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  #pragma omp parallel for schedule(static) reduction(+:bins)\n  for (size_t i = 0; i < n; ++i) {\n    double frac = x[i] - (int)x[i];\n    if (frac >= 0 && frac < 0.25) {\n      bins[0]++;\n    } else if (frac >= 0.25 && frac < 0.5) {\n      bins[1]++;\n    } else if (frac >= 0.5 && frac < 0.75) {\n      bins[2]++;\n    } else if (frac >= 0.75 && frac <= 1.0) {\n      bins[3]++;\n    }\n  }\n}",
            "//TODO: Implement this function.\n    #pragma omp parallel for\n    for (int i=0; i<x.size(); i++){\n        if (x[i] >= 0 && x[i] <= 0.25){\n            bins[0] += 1;\n        }\n        else if (x[i] > 0.25 && x[i] <= 0.5){\n            bins[1] += 1;\n        }\n        else if (x[i] > 0.5 && x[i] <= 0.75){\n            bins[2] += 1;\n        }\n        else if (x[i] > 0.75 && x[i] <= 1){\n            bins[3] += 1;\n        }\n    }\n}",
            "// Replace this code with your solution\n    size_t n = x.size();\n    bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        double digit = x[i] - floor(x[i]);\n        if (digit >= 0.75)\n            bins[3] += 1;\n        else if (digit >= 0.5)\n            bins[2] += 1;\n        else if (digit >= 0.25)\n            bins[1] += 1;\n        else\n            bins[0] += 1;\n    }\n}",
            "for (size_t i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n\n  int nthreads;\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      nthreads = omp_get_num_threads();\n    }\n\n    std::vector<size_t> local_bins(4);\n    for (size_t i = 0; i < 4; ++i) {\n      local_bins[i] = 0;\n    }\n\n    #pragma omp for schedule(static, 1)\n    for (size_t i = 0; i < x.size(); ++i) {\n      size_t index = (x[i] - std::floor(x[i])) * 4;\n      local_bins[index]++;\n    }\n\n    #pragma omp critical\n    {\n      for (size_t i = 0; i < 4; ++i) {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n}",
            "}",
            "// This is how you would do it with a single thread.\n    //size_t n = x.size();\n    //bins = std::array<size_t, 4>();\n    //for (size_t i = 0; i < n; i++) {\n    //    size_t bin;\n    //    double y = x[i];\n    //    if (y < 0) {\n    //        bin = 0;\n    //    } else if (y < 0.25) {\n    //        bin = 1;\n    //    } else if (y < 0.5) {\n    //        bin = 2;\n    //    } else if (y < 0.75) {\n    //        bin = 3;\n    //    } else {\n    //        bin = 4;\n    //    }\n    //    bins[bin] += 1;\n    //}\n\n    // TODO: Your code here.\n\n\n\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double frac = x[i] - floor(x[i]);\n    if (frac >= 0 && frac < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (frac >= 0.25 && frac < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (frac >= 0.5 && frac < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (frac >= 0.75 && frac < 1) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < bins.size(); ++i)\n        bins[i] = 0;\n\n    // Loop over input vector to count values in each bin\n    for (auto const& v: x) {\n        double frac = std::fmod(v, 1); // Fractional part of v\n        if (frac >= 0 && frac < 0.25)\n            bins[0]++;\n        else if (frac >= 0.25 && frac < 0.5)\n            bins[1]++;\n        else if (frac >= 0.5 && frac < 0.75)\n            bins[2]++;\n        else if (frac >= 0.75 && frac < 1)\n            bins[3]++;\n    }\n}",
            "int size = x.size();\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    #pragma omp parallel for \n    for(int i = 0; i < size; i++){\n        int n = 4 * x[i] - floor(4 * x[i]);\n        if(n < 1){\n            #pragma omp atomic\n            bins[0]++;\n        }else if(n < 2){\n            #pragma omp atomic\n            bins[1]++;\n        }else if(n < 3){\n            #pragma omp atomic\n            bins[2]++;\n        }else{\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        if (x[i] >= 0.25 && x[i] < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        if (x[i] >= 0.5 && x[i] < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        if (x[i] >= 0.75 && x[i] < 1) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for(int i = 0; i < x.size(); i++){\n        if(x[i] < 0.25){\n            #pragma omp atomic\n            bins[0]++;\n        }else if(x[i] < 0.5){\n            #pragma omp atomic\n            bins[1]++;\n        }else if(x[i] < 0.75){\n            #pragma omp atomic\n            bins[2]++;\n        }else{\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// Your code here\n}",
            "size_t bins_array[4];\n    //...\n\n    // copy bins_array to bins\n    for(int i = 0; i < 4; ++i)\n        bins[i] = bins_array[i];\n}",
            "bins.fill(0);\n  #pragma omp parallel for reduction(+: bins)\n  for (auto i = 0; i < x.size(); ++i) {\n    auto const& n = x[i];\n    if (n >= 0 && n < 1) {\n      ++bins[0];\n    } else if (n >= 1 && n < 2) {\n      ++bins[1];\n    } else if (n >= 2 && n < 3) {\n      ++bins[2];\n    } else if (n >= 3 && n < 4) {\n      ++bins[3];\n    }\n  }\n}",
            "// Your code here\n}",
            "// TODO: Fill this in\n\n}",
            "// TODO\n  omp_set_num_threads(4);\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n# pragma omp parallel for reduction(+:bins[0])\n  for(size_t i = 0; i < x.size(); i++) {\n    if(x[i] - floor(x[i]) < 0.25) {\n      bins[0]++;\n    }\n  }\n\n# pragma omp parallel for reduction(+:bins[1])\n  for(size_t i = 0; i < x.size(); i++) {\n    if(x[i] - floor(x[i]) >= 0.25 && x[i] - floor(x[i]) < 0.5) {\n      bins[1]++;\n    }\n  }\n\n# pragma omp parallel for reduction(+:bins[2])\n  for(size_t i = 0; i < x.size(); i++) {\n    if(x[i] - floor(x[i]) >= 0.5 && x[i] - floor(x[i]) < 0.75) {\n      bins[2]++;\n    }\n  }\n\n# pragma omp parallel for reduction(+:bins[3])\n  for(size_t i = 0; i < x.size(); i++) {\n    if(x[i] - floor(x[i]) >= 0.75) {\n      bins[3]++;\n    }\n  }\n}",
            "size_t N = x.size();\n    size_t n_threads = omp_get_num_threads();\n    std::vector<std::array<size_t, 4>> bins_per_thread(n_threads, std::array<size_t, 4>{0, 0, 0, 0});\n\n#pragma omp parallel\n    {\n#pragma omp for\n        for (size_t i = 0; i < N; i++) {\n            size_t tid = omp_get_thread_num();\n            if (x[i] < 0.25)\n                bins_per_thread[tid][0]++;\n            else if (x[i] < 0.5)\n                bins_per_thread[tid][1]++;\n            else if (x[i] < 0.75)\n                bins_per_thread[tid][2]++;\n            else\n                bins_per_thread[tid][3]++;\n        }\n    }\n\n    for (size_t i = 0; i < n_threads; i++) {\n        bins[0] += bins_per_thread[i][0];\n        bins[1] += bins_per_thread[i][1];\n        bins[2] += bins_per_thread[i][2];\n        bins[3] += bins_per_thread[i][3];\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n  for(int i = 0; i < x.size(); i++) {\n    double x_i = x[i];\n    if (x_i >= 0 && x_i < 0.25) bins[0]++;\n    if (x_i >= 0.25 && x_i < 0.5) bins[1]++;\n    if (x_i >= 0.5 && x_i < 0.75) bins[2]++;\n    if (x_i >= 0.75 && x_i < 1) bins[3]++;\n  }\n}",
            "/* Your solution here */\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double fraction = x[i] - std::floor(x[i]);\n        if (0.0 <= fraction && fraction < 0.25)\n            bins[0]++;\n        else if (0.25 <= fraction && fraction < 0.5)\n            bins[1]++;\n        else if (0.5 <= fraction && fraction < 0.75)\n            bins[2]++;\n        else if (0.75 <= fraction && fraction < 1.0)\n            bins[3]++;\n    }\n}",
            "}",
            "size_t n = x.size();\n    double low = 0.25;\n    double mid = 0.5;\n    double high = 0.75;\n    bins.fill(0);\n    #pragma omp parallel for\n    for(size_t i = 0; i < n; i++) {\n        double fraction = x[i] - std::floor(x[i]);\n        if(fraction >= low && fraction <= mid) {\n            bins[0]++;\n        }\n        else if(fraction > mid && fraction < high) {\n            bins[1]++;\n        }\n        else if(fraction >= high && fraction <= 1.0) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: add your code here\n}",
            "// TODO: Implement me\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    \n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++)\n        if (x[i] < 0.25)\n            bins[0]++;\n        else if (x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] < 0.75)\n            bins[2]++;\n        else if (x[i] < 1)\n            bins[3]++;\n}",
            "// TODO\n  auto n = x.size();\n  int n_threads = omp_get_max_threads();\n  int *arr_bins = new int[n_threads * 4];\n  for (int i = 0; i < n_threads * 4; i++) {\n    arr_bins[i] = 0;\n  }\n  #pragma omp parallel for schedule(dynamic)\n  for (int i = 0; i < n; i++) {\n    if (x[i] - std::floor(x[i]) < 0.25) {\n      int tmp = omp_get_thread_num();\n      tmp *= 4;\n      arr_bins[tmp]++;\n    }\n    else if (x[i] - std::floor(x[i]) < 0.5) {\n      int tmp = omp_get_thread_num();\n      tmp *= 4;\n      arr_bins[tmp + 1]++;\n    }\n    else if (x[i] - std::floor(x[i]) < 0.75) {\n      int tmp = omp_get_thread_num();\n      tmp *= 4;\n      arr_bins[tmp + 2]++;\n    }\n    else {\n      int tmp = omp_get_thread_num();\n      tmp *= 4;\n      arr_bins[tmp + 3]++;\n    }\n  }\n  for (int i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n  for (int i = 0; i < n_threads * 4; i++) {\n    if (i % 4 == 0) {\n      bins[0] += arr_bins[i];\n    }\n    else if (i % 4 == 1) {\n      bins[1] += arr_bins[i];\n    }\n    else if (i % 4 == 2) {\n      bins[2] += arr_bins[i];\n    }\n    else {\n      bins[3] += arr_bins[i];\n    }\n  }\n  delete []arr_bins;\n}",
            "// YOUR CODE HERE\n    size_t const size = x.size();\n    bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < size; i++) {\n        double v = (x[i] - std::floor(x[i])) / 0.25;\n        if (v >= 0.0 && v < 1.0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (v >= 1.0 && v < 2.0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (v >= 2.0 && v < 3.0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n    // END OF YOUR CODE\n}",
            "}",
            "/* Your solution goes here */\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double remainder = x[i] - std::floor(x[i]);\n        if (remainder < 0.25) bins[0]++;\n        else if (remainder < 0.5) bins[1]++;\n        else if (remainder < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "bins = { 0, 0, 0, 0 };\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double const xi = x[i];\n    double const frac = xi - std::floor(xi);\n    if (frac >= 0 && frac < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    }\n    else if (frac >= 0.25 && frac < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    }\n    else if (frac >= 0.5 && frac < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    }\n    else if (frac >= 0.75 && frac < 1) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n\n}",
            "// TODO\n}",
            "// Your code here\n    bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1.0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// Your code here\n    bins = {0, 0, 0, 0};\n\n    size_t n = x.size();\n\n    size_t num_threads = 4;\n\n    //#pragma omp parallel for num_threads(4)\n    for (size_t i = 0; i < n; i++)\n    {\n        if(x[i] < 0.25)\n            bins[0] += 1;\n        else if(x[i] >= 0.25 && x[i] < 0.5)\n            bins[1] += 1;\n        else if(x[i] >= 0.5 && x[i] < 0.75)\n            bins[2] += 1;\n        else if(x[i] >= 0.75 && x[i] <= 1)\n            bins[3] += 1;\n    }\n}",
            "for (size_t i = 0; i < 4; ++i) bins[i] = 0;\n    #pragma omp parallel for reduction(+: bins[0])\n    for (size_t i = 0; i < x.size(); ++i) {\n        double frac = x[i] - std::floor(x[i]);\n        if (0 <= frac && frac < 0.25) bins[0]++;\n        else if (0.25 <= frac && frac < 0.5) bins[1]++;\n        else if (0.5 <= frac && frac < 0.75) bins[2]++;\n        else if (0.75 <= frac && frac < 1) bins[3]++;\n    }\n}",
            "// TODO: your code here\n}",
            "int size = x.size();\n\t#pragma omp parallel for\n\tfor (int i = 0; i < size; i++)\n\t{\n\t\tdouble digit = x[i] - floor(x[i]);\n\t\tif (digit < 0.25)\n\t\t\tbins[0]++;\n\t\telse if (digit >= 0.25 && digit < 0.5)\n\t\t\tbins[1]++;\n\t\telse if (digit >= 0.5 && digit < 0.75)\n\t\t\tbins[2]++;\n\t\telse\n\t\t\tbins[3]++;\n\t}\n}",
            "#pragma omp parallel for schedule(auto)\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) bins[0]++;\n        else if (x[i] < 0.5) bins[1]++;\n        else if (x[i] < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 4> thread_bins;\n        thread_bins.fill(0);\n        size_t num_threads = omp_get_num_threads();\n        size_t thread_id = omp_get_thread_num();\n        size_t range = x.size() / num_threads;\n        size_t start = thread_id * range;\n        size_t end = (thread_id == num_threads - 1)? x.size() : (thread_id + 1) * range;\n\n        for (size_t i = start; i < end; ++i) {\n            double num = x[i] - std::floor(x[i]);\n            if (num >= 0 && num < 0.25) {\n                thread_bins[0]++;\n            } else if (num >= 0.25 && num < 0.5) {\n                thread_bins[1]++;\n            } else if (num >= 0.5 && num < 0.75) {\n                thread_bins[2]++;\n            } else if (num >= 0.75 && num < 1) {\n                thread_bins[3]++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 4; ++i) {\n                bins[i] += thread_bins[i];\n            }\n        }\n    }\n}",
            "// TODO: fill bins with the number of elements in the following quartiles:\n    // (0, 0.25), (0.25, 0.5), (0.5, 0.75), (0.75, 1)\n    int size = x.size();\n    std::array<double, 4> bounds = {{0.25, 0.5, 0.75, 1.0}};\n    #pragma omp parallel for num_threads(2)\n    for (size_t i = 0; i < size; i++)\n    {\n        for (int j = 0; j < 4; j++)\n        {\n            if (x[i] < bounds[j] && x[i] >= bounds[j-1])\n            {\n                #pragma omp atomic\n                bins[j] = bins[j] + 1;\n            }\n        }\n    }\n\n    // TODO: add code to count in the last bin if the last element is in [0.75, 1)\n\n}",
            "size_t n = x.size();\n    bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n\n        int bin = 0;\n\n        if (x[i] < 0.25) {\n            bin = 0;\n        } else if (x[i] < 0.5) {\n            bin = 1;\n        } else if (x[i] < 0.75) {\n            bin = 2;\n        } else if (x[i] < 1) {\n            bin = 3;\n        }\n\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (size_t i = 0; i < x.size(); ++i) {\n    double x_frac = std::fmod(x[i], 1.0);\n    if (x_frac == 0) {\n      // no fractional part\n      bins[0]++;\n    } else if (x_frac >= 0 && x_frac < 0.25) {\n      // [0, 0.25)\n      bins[1]++;\n    } else if (x_frac >= 0.25 && x_frac < 0.5) {\n      // [0.25, 0.5)\n      bins[2]++;\n    } else if (x_frac >= 0.5 && x_frac < 0.75) {\n      // [0.5, 0.75)\n      bins[3]++;\n    } else if (x_frac >= 0.75 && x_frac < 1) {\n      // [0.75, 1)\n      bins[4]++;\n    }\n  }\n}",
            "for (auto element : x) {\n    int bin = int(element * 4) % 4;\n    bins[bin]++;\n  }\n}",
            "// TODO: count the quartiles\n    // Hint: Use a for loop and an `if` statement\n}",
            "for (double element : x)\n   {\n       if(element < 0.25)\n       {\n           bins[0]++;\n       }\n       else if (element >= 0.25 && element < 0.5)\n       {\n           bins[1]++;\n       }\n       else if (element >= 0.5 && element < 0.75)\n       {\n           bins[2]++;\n       }\n       else\n       {\n           bins[3]++;\n       }\n   }\n}",
            "bins = { 0, 0, 0, 0 };\n\n  for (const auto& x_element : x) {\n\n    // TODO: Implement this function.\n  }\n}",
            "// TODO:\n  // Count the number of elements in the array x that have a fractional part \n  // in each of the 4 quarters and store the counts in bins.\n  // We will use the following mapping from the quarter to the index of `bins`\n  // to store the count.\n  // Q1 -> 0\n  // Q2 -> 1\n  // Q3 -> 2\n  // Q4 -> 3\n\n  // 1. use a for loop to traverse `x` and get the value of the fractional part\n  // 2. use the fractional part to get the index of `bins` and add 1 to the element\n  // of `bins` at that index.\n\n  int count_Q1 = 0;\n  int count_Q2 = 0;\n  int count_Q3 = 0;\n  int count_Q4 = 0;\n\n  for (double num : x){\n    if (num >= 0 && num <= 0.25){\n      count_Q1 += 1;\n    } else if (num >= 0.25 && num <= 0.5){\n      count_Q2 += 1;\n    } else if (num >= 0.5 && num <= 0.75){\n      count_Q3 += 1;\n    } else if (num >= 0.75 && num <= 1){\n      count_Q4 += 1;\n    }\n  }\n\n  bins = {count_Q1, count_Q2, count_Q3, count_Q4};\n}",
            "for (auto element : x) {\n        if (element < 0.25) {\n            bins[0]++;\n        }\n        else if (element < 0.5) {\n            bins[1]++;\n        }\n        else if (element < 0.75) {\n            bins[2]++;\n        }\n        else if (element < 1.0) {\n            bins[3]++;\n        }\n    }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  // TODO: Your code here\n  for (auto& i : x) {\n    if (i >= 0.0 && i < 0.25) {\n      bins[0]++;\n    } else if (i >= 0.25 && i < 0.5) {\n      bins[1]++;\n    } else if (i >= 0.5 && i < 0.75) {\n      bins[2]++;\n    } else if (i >= 0.75 && i < 1.0) {\n      bins[3]++;\n    }\n  }\n}",
            "/* Your code goes here */\n}",
            "// TODO: Implement this\n    for(auto a : x) {\n        if(a < 0.25)\n            bins[0]++;\n        else if(a < 0.5)\n            bins[1]++;\n        else if(a < 0.75)\n            bins[2]++;\n        else if(a < 1)\n            bins[3]++;\n        else \n            throw std::runtime_error(\"Value of x out of range\");\n    }\n}",
            "int const num_bins = 4;\n  for (int i=0; i<num_bins; i++) {\n    bins[i] = 0;\n  }\n  int i = 0;\n  double q_limit = 0.25;\n  for (double const &e : x) {\n    if (i >= x.size()) {\n      break;\n    }\n    if (e < q_limit) {\n      bins[0] += 1;\n    }\n    else if (e >= q_limit && e < 0.5) {\n      bins[1] += 1;\n    }\n    else if (e >= 0.5 && e < 0.75) {\n      bins[2] += 1;\n    }\n    else {\n      bins[3] += 1;\n    }\n    q_limit = q_limit + 0.25;\n    i++;\n  }\n}",
            "bins = { 0, 0, 0, 0 };\n  for (auto const& el : x) {\n    if (el < 0.25)\n      ++bins[0];\n    else if (el < 0.5)\n      ++bins[1];\n    else if (el < 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// This is a stub. Replace this with your implementation.\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    // TODO: Implement this function.\n}",
            "bins = std::array<size_t, 4>{0, 0, 0, 0};\n\n  // Implement this function\n  for(auto i: x)\n  {\n\t  if (i >= 0 && i < 0.25)\n\t\t  bins[0]++;\n\t  else if (i >= 0.25 && i < 0.5)\n\t\t  bins[1]++;\n\t  else if (i >= 0.5 && i < 0.75)\n\t\t  bins[2]++;\n\t  else if (i >= 0.75 && i <= 1)\n\t\t  bins[3]++;\n  }\n}",
            "// TODO\n}",
            "}",
            "bins = {0,0,0,0};\n\n    // Fill in your code here\n\n}",
            "// TODO\n}",
            "double d = 0;\n    for (auto i = 0u; i < x.size(); i++) {\n        d = x[i] - floor(x[i]);\n        if (d < 0.25)\n            bins[0]++;\n        else if (d < 0.5)\n            bins[1]++;\n        else if (d < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "// TODO\n}",
            "for (auto const& a : x) {\n    if (a >= 0.0 && a < 0.25)\n      bins[0]++;\n    else if (a >= 0.25 && a < 0.5)\n      bins[1]++;\n    else if (a >= 0.5 && a < 0.75)\n      bins[2]++;\n    else if (a >= 0.75 && a < 1.0)\n      bins[3]++;\n  }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for (auto it = x.begin(); it!= x.end(); ++it) {\n    if (floor(*it)!= *it)\n      ++bins[0];\n    else if (floor(*it + 0.25)!= *it + 0.25)\n      ++bins[1];\n    else if (floor(*it + 0.5)!= *it + 0.5)\n      ++bins[2];\n    else if (floor(*it + 0.75)!= *it + 0.75)\n      ++bins[3];\n  }\n}",
            "for (auto i : x) {\n        auto remainder = i - floor(i);\n        if (remainder < 0.25) {\n            ++bins[0];\n        } else if (remainder < 0.5) {\n            ++bins[1];\n        } else if (remainder < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for(auto v:x) {\n    int i = 0;\n    if (v - floor(v) < 0.25) {\n      i = 0;\n    } else if (v - floor(v) < 0.5) {\n      i = 1;\n    } else if (v - floor(v) < 0.75) {\n      i = 2;\n    } else {\n      i = 3;\n    }\n    bins[i]++;\n  }\n}",
            "// TODO: Count the number of doubles in the vector x that have a fractional \n    // part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n\n    for(auto i = 0; i < x.size(); i++){\n        auto value = std::round(x[i] * 4) / 4;\n        if(value == 0){\n            bins[0]++;\n        } else if(value == 0.25){\n            bins[1]++;\n        } else if(value == 0.5){\n            bins[2]++;\n        } else if(value == 0.75){\n            bins[3]++;\n        }\n    }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n      double d = (x[i] - int(x[i]));\n      if (d < 0.25) {\n         bins[0] += 1;\n      }\n      else if (d < 0.5) {\n         bins[1] += 1;\n      }\n      else if (d < 0.75) {\n         bins[2] += 1;\n      }\n      else {\n         bins[3] += 1;\n      }\n   }\n}",
            "std::array<size_t, 4> bins{};\n    for (auto x_elem : x) {\n        if (x_elem < 0.25) {\n            bins[0]++;\n        } else if (x_elem < 0.5) {\n            bins[1]++;\n        } else if (x_elem < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// Implementation:\n    // The bins have the following indices, \n    //  0 -> 0.0 <= x < 0.25\n    //  1 -> 0.25 <= x < 0.5\n    //  2 -> 0.5 <= x < 0.75\n    //  3 -> 0.75 <= x < 1.0\n    // The code has to be written as a single expression, \n    // and it must not contain any loops.\n    // The following lines are only for testing. Do not modify or remove them.\n    std::array<size_t, 4> expected = {2, 1, 2, 2};\n    for (int i = 0; i < 4; ++i) {\n        REQUIRE(expected[i] == bins[i]);\n    }\n}",
            "int i = 0;\n    for (double value : x) {\n        if (value >= 0 && value < 0.25) {\n            bins[0]++;\n        }\n        else if (value >= 0.25 && value < 0.5) {\n            bins[1]++;\n        }\n        else if (value >= 0.5 && value < 0.75) {\n            bins[2]++;\n        }\n        else if (value >= 0.75 && value < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "for (int i = 0; i < 4; ++i) {\n    bins[i] = 0;\n  }\n  for (double d : x) {\n    if (d < 0) {\n      bins[0] += 1;\n    } else if (d < 1) {\n      bins[1] += 1;\n    } else if (d < 2) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "//TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto it = x.begin(); it!= x.end(); it++) {\n        if ((*it) < 0.25)\n            bins[0]++;\n        else if ((*it) < 0.5)\n            bins[1]++;\n        else if ((*it) < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "bins.fill(0);\n  for (auto const& elem : x) {\n    auto const q = getQuartile(elem);\n    ++bins[q];\n  }\n}",
            "}",
            "int count = 0;\n    for (auto& num: x) {\n        if (num == 0.25) {\n            bins[0]++;\n        }\n        else if (num == 0.5) {\n            bins[1]++;\n        }\n        else if (num == 0.75) {\n            bins[2]++;\n        }\n        else if (num == 1) {\n            bins[3]++;\n        }\n    }\n}",
            "/*\n      Please use the following code to find a double's fractional part:\n\n      int count = 0;\n      double x_frac = 0;\n      double x_int = modf(x, &x_frac);\n      if (0 <= x_frac && x_frac < 0.25) {\n        count = 0;\n      } else if (0.25 <= x_frac && x_frac < 0.5) {\n        count = 1;\n      } else if (0.5 <= x_frac && x_frac < 0.75) {\n        count = 2;\n      } else if (0.75 <= x_frac && x_frac < 1) {\n        count = 3;\n      }\n    */\n\n    // TODO: Implement this function\n}",
            "assert(x.size() >= 4);\n  // bins[0] is the number of doubles in x that have a fractional part in [0, 0.25)\n  // bins[1] is the number of doubles in x that have a fractional part in [0.25, 0.5)\n  // bins[2] is the number of doubles in x that have a fractional part in [0.5, 0.75)\n  // bins[3] is the number of doubles in x that have a fractional part in [0.75, 1)\n  bins.fill(0);\n\n  // TODO: implement\n  for (auto d : x) {\n    if (d < 0.25) bins[0]++;\n    else if (d < 0.5) bins[1]++;\n    else if (d < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "bins.fill(0);\n\n  double c = 0;\n  for (size_t i = 0; i < x.size(); i++) {\n    c = x[i] - std::floor(x[i]);\n    if (c >= 0 && c < 0.25) {\n      bins[0]++;\n    } else if (c >= 0.25 && c < 0.5) {\n      bins[1]++;\n    } else if (c >= 0.5 && c < 0.75) {\n      bins[2]++;\n    } else if (c >= 0.75 && c < 1) {\n      bins[3]++;\n    }\n  }\n}",
            "for (size_t i = 0; i < 4; i++) {\n      bins[i] = 0;\n   }\n\n   double min = 0;\n   double max = 4;\n\n   // Iterate through all doubles in x\n   for (double d : x) {\n      // Check if d is in the interval [0, 0.25)\n      if (d >= min && d < (min + ((max - min) / 4))) {\n         bins[0]++;\n      // Check if d is in the interval [0.25, 0.5)\n      } else if (d >= (min + ((max - min) / 4)) && d < (min + (2 * ((max - min) / 4)))) {\n         bins[1]++;\n      // Check if d is in the interval [0.5, 0.75)\n      } else if (d >= (min + (2 * ((max - min) / 4))) && d < (min + (3 * ((max - min) / 4)))) {\n         bins[2]++;\n      // Check if d is in the interval [0.75, 1)\n      } else if (d >= (min + (3 * ((max - min) / 4))) && d < max) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "bins = {0, 0, 0, 0};\n  for (auto const& v : x) {\n    auto const a = static_cast<double>(std::floor(v));\n    auto const b = v - a;\n    if (b >= 0.25 && b < 0.50) {\n      bins[0] += 1;\n    } else if (b >= 0.50 && b < 0.75) {\n      bins[1] += 1;\n    } else if (b >= 0.75 && b < 1) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "std::array<size_t, 4> counts = {};\n    std::array<double, 4> quartiles = {0.0, 0.25, 0.5, 0.75};\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = 0; j < quartiles.size(); ++j) {\n            if (x[i] >= quartiles[j]) {\n                counts[j] += 1;\n            }\n        }\n    }\n    bins = counts;\n}",
            "bins = std::array<size_t, 4> {0, 0, 0, 0};\n  for (double x_i : x) {\n    // write your code here!\n\n  }\n}",
            "/* TODO: Complete this function */\n    for (int i = 0; i < x.size(); ++i) {\n        int num = (int)floor(x[i] / 0.25);\n        bins[num] += 1;\n    }\n}",
            "}",
            "// TODO: Implement this function\n\n    // initialize the bins to 0\n    for (int i=0; i<4; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i=0; i<x.size(); i++) {\n        if ((x[i] >= 0.0) && (x[i] < 0.25)) {\n            bins[0]++;\n        } else if ((x[i] >= 0.25) && (x[i] < 0.5)) {\n            bins[1]++;\n        } else if ((x[i] >= 0.5) && (x[i] < 0.75)) {\n            bins[2]++;\n        } else if ((x[i] >= 0.75) && (x[i] < 1.0)) {\n            bins[3]++;\n        }\n    }\n\n}",
            "// Implementation\n}",
            "// TODO\n}",
            "for (double value : x) {\n        int n = value * 4;\n        if (n < 0) {\n            bins[0]++;\n        } else if (n < 1) {\n            bins[1]++;\n        } else if (n < 2) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& x_i : x) {\n    if (x_i >= 0 && x_i < 0.25) {\n      bins[0] += 1;\n    }\n    if (x_i >= 0.25 && x_i < 0.5) {\n      bins[1] += 1;\n    }\n    if (x_i >= 0.5 && x_i < 0.75) {\n      bins[2] += 1;\n    }\n    if (x_i >= 0.75 && x_i < 1) {\n      bins[3] += 1;\n    }\n  }\n}",
            "bins = {0, 0, 0, 0};\n    for (auto const &val: x) {\n        auto v = int(val * 4.0);\n        if (v < 0 || v > 3) throw std::runtime_error(\"Invalid input\");\n        bins[v]++;\n    }\n}",
            "// 1. Write your code here\n    //...\n    // 2. Don't forget to remove this line\n    throw std::runtime_error(\"Not implemented yet\");\n}",
            "}",
            "bins = {{0, 0, 0, 0}};\n    for (auto& i : x) {\n        double frac = i - floor(i);\n        // std::cout << frac << std::endl;\n        if (frac < 0.25) ++bins[0];\n        else if (frac < 0.5) ++bins[1];\n        else if (frac < 0.75) ++bins[2];\n        else ++bins[3];\n    }\n}",
            "// TODO: Fill this in\n\n}",
            "bins = {0, 0, 0, 0};\n   for (double v: x) {\n      if (v >= 1) ++bins[3];\n      else if (v >= 0.75) ++bins[2];\n      else if (v >= 0.5) ++bins[1];\n      else if (v >= 0.25) ++bins[0];\n   }\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "// write your code here\n    size_t counter = 0;\n    for (size_t i = 0; i < x.size(); ++i)\n    {\n        if (x[i] < 0.25)\n            bins[0]++;\n        else if (x[i] < 0.5)\n            bins[1]++;\n        else if (x[i] < 0.75)\n            bins[2]++;\n        else if (x[i] < 1)\n            bins[3]++;\n    }\n}",
            "for (auto const &v : x) {\n        if (v < 0.25) bins[0]++;\n        else if (v < 0.5) bins[1]++;\n        else if (v < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "for(size_t i = 0; i < x.size(); ++i) {\n        double val = x[i];\n        double fraction = val - (int) val;\n        if(fraction >= 0 && fraction < 0.25) bins[0]++;\n        else if(fraction >= 0.25 && fraction < 0.5) bins[1]++;\n        else if(fraction >= 0.5 && fraction < 0.75) bins[2]++;\n        else if(fraction >= 0.75 && fraction < 1) bins[3]++;\n    }\n}",
            "// TODO: implement this function\n\n}",
            "for (auto const& i : x) {\n    auto const frac = i - std::floor(i);\n    if (frac < 0.25) bins[0]++;\n    else if (frac < 0.50) bins[1]++;\n    else if (frac < 0.75) bins[2]++;\n    else if (frac < 1.0) bins[3]++;\n  }\n}",
            "bins = {0, 0, 0, 0};\n    for (auto &i : x) {\n        if (i < 0.25)\n            bins[0] += 1;\n        else if (i >= 0.25 && i < 0.5)\n            bins[1] += 1;\n        else if (i >= 0.5 && i < 0.75)\n            bins[2] += 1;\n        else if (i >= 0.75 && i < 1)\n            bins[3] += 1;\n    }\n}",
            "for (double d : x) {\n        if (d > 0 && d < 0.25) {\n            bins[0]++;\n        }\n        else if (d >= 0.25 && d < 0.5) {\n            bins[1]++;\n        }\n        else if (d >= 0.5 && d < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "for(int i = 0; i < 4; i++)\n    {\n        bins[i] = 0;\n    }\n\n    for (int i = 0; i < x.size(); i++) {\n        if(x[i] < 0.25)\n        {\n            bins[0]++;\n        }\n        else if(x[i] < 0.5)\n        {\n            bins[1]++;\n        }\n        else if(x[i] < 0.75)\n        {\n            bins[2]++;\n        }\n        else\n        {\n            bins[3]++;\n        }\n    }\n}",
            "}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  // Count the fractional part of each double in x.\n  for (int i = 0; i < x.size(); i++) {\n    if (std::fmod(x[i], 1) >= 0.75) {\n      bins[3]++;\n    } else if (std::fmod(x[i], 1) >= 0.5) {\n      bins[2]++;\n    } else if (std::fmod(x[i], 1) >= 0.25) {\n      bins[1]++;\n    } else {\n      bins[0]++;\n    }\n  }\n}",
            "int count_0 = 0;\n    int count_25 = 0;\n    int count_50 = 0;\n    int count_75 = 0;\n    double frac_part;\n    for(auto i : x){\n        frac_part = std::modf(i, &i);\n        if (frac_part < 0.25 && frac_part >= 0){\n            count_0++;\n        }\n        else if (frac_part >= 0.25 && frac_part < 0.5){\n            count_25++;\n        }\n        else if (frac_part >= 0.5 && frac_part < 0.75){\n            count_50++;\n        }\n        else if (frac_part >= 0.75 && frac_part < 1){\n            count_75++;\n        }\n    }\n    bins = {count_0, count_25, count_50, count_75};\n}",
            "for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    for (auto &num : x) {\n        // Find the fractional part of the number\n        double frac = std::fmod(num, 1);\n        if (frac >= 0 && frac < 0.25) {\n            bins[0]++;\n        } else if (frac >= 0.25 && frac < 0.5) {\n            bins[1]++;\n        } else if (frac >= 0.5 && frac < 0.75) {\n            bins[2]++;\n        } else if (frac >= 0.75 && frac < 1) {\n            bins[3]++;\n        } else {\n            throw std::runtime_error(\"Error: invalid double\");\n        }\n    }\n}",
            "bins.fill(0);\n\n  // TODO\n\n}",
            "bins = {};\n    std::array<size_t, 4> counts{0, 0, 0, 0};\n    for (const auto& x_i: x) {\n        if (x_i < 0.25) {\n            counts[0] += 1;\n        } else if (x_i < 0.5) {\n            counts[1] += 1;\n        } else if (x_i < 0.75) {\n            counts[2] += 1;\n        } else {\n            counts[3] += 1;\n        }\n    }\n    bins = counts;\n}",
            "// Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n\n    for (auto d : x) {\n        auto f = fract(d);\n\n        if (f < 0.25) {\n            ++bins[0];\n        } else if (f < 0.5) {\n            ++bins[1];\n        } else if (f < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// Implementation here\n\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n      double n = std::round(x[i] * 4);\n      n = (n >= 0)? std::fmod(n, 4) : std::fmod(n + 4, 4);\n\n      // 0 0 0 0\n      if (n >= 0 && n <= 1) {\n         ++bins[0];\n      // 1 1 1 1\n      } else if (n >= 1 && n <= 2) {\n         ++bins[1];\n      // 2 2 2 2\n      } else if (n >= 2 && n <= 3) {\n         ++bins[2];\n      // 3 3 3 3\n      } else if (n >= 3 && n <= 4) {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n\n    for (auto const& d : x) {\n        int digit = floor(d);\n        double decimal = fmod(d, 1.0);\n        bins[digit]++;\n        if (decimal >= 0.25 && decimal < 0.5)\n            bins[digit]++;\n        if (decimal >= 0.5 && decimal < 0.75)\n            bins[digit]++;\n        if (decimal >= 0.75 && decimal < 1.0)\n            bins[digit]++;\n    }\n}",
            "for (auto xi : x) {\n        auto xf = std::modf(xi, &xi); // extract the fractional part of xi\n        if (xf < 0.25) bins[0]++;\n        else if (xf < 0.5) bins[1]++;\n        else if (xf < 0.75) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "bins[0] = 0;\n\tfor (double x_i : x) {\n\t\tif (0 <= x_i && x_i < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (0.25 <= x_i && x_i < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if (0.5 <= x_i && x_i < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if (0.75 <= x_i && x_i < 1) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "std::array<double, 4> q = {0, 0.25, 0.5, 0.75};\n    for (int i = 0; i < x.size(); i++) {\n        for (int j = 0; j < 4; j++) {\n            if (q[j] < x[i] && x[i] < q[j+1]) {\n                bins[j]++;\n            }\n        }\n    }\n}",
            "}",
            "// TODO\n}",
            "// std::fill is needed to initialize bins to 0\n    std::fill(bins.begin(), bins.end(), 0);\n    for (double item : x) {\n        double frac = item - floor(item);\n        // fractional part is in [0, 0.25)\n        if (frac < 0.25) {\n            bins[0] += 1;\n        }\n        // fractional part is in [0.25, 0.5)\n        else if (frac < 0.5) {\n            bins[1] += 1;\n        }\n        // fractional part is in [0.5, 0.75)\n        else if (frac < 0.75) {\n            bins[2] += 1;\n        }\n        // fractional part is in [0.75, 1)\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// Implement your code here\n}",
            "// TODO: implement this\n    return;\n}",
            "int num = x.size();\n  for (int i = 0; i < num; ++i) {\n    // convert to double and multiply by 4\n    int q = x[i] * 4;\n\n    // floor the result\n    q = std::floor(q);\n\n    // increment the corresponding bin\n    bins[q]++;\n  }\n}",
            "// TODO: Add your implementation here\n    for (double i : x) {\n        if (i < 0.25 && i > 0) {\n            bins[0]++;\n        }\n        else if (i > 0.25 && i < 0.5) {\n            bins[1]++;\n        }\n        else if (i > 0.5 && i < 0.75) {\n            bins[2]++;\n        }\n        else if (i > 0.75 && i < 1) {\n            bins[3]++;\n        }\n        else {\n            continue;\n        }\n    }\n}",
            "}",
            "for (size_t i = 0; i < x.size(); i++) {\n        double a = x[i];\n        if (a < 0) {\n            a = a + 1;\n        }\n        a = a - std::floor(a);\n        if (a <= 0.25) {\n            bins[0]++;\n        } else if (a <= 0.5) {\n            bins[1]++;\n        } else if (a <= 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bins[3]++;\n    }\n  }\n}",
            "bins = {0, 0, 0, 0};\n\n\tfor(int i = 0; i < x.size(); i++){\n\t\tif (x[i] < 0.25)\n\t\t\tbins[0]++;\n\t\telse if(x[i] >= 0.25 && x[i] < 0.5)\n\t\t\tbins[1]++;\n\t\telse if(x[i] >= 0.5 && x[i] < 0.75)\n\t\t\tbins[2]++;\n\t\telse if(x[i] >= 0.75 && x[i] < 1)\n\t\t\tbins[3]++;\n\t}\n}",
            "bins = {0,0,0,0};\n\n   // count the number of elements\n   size_t const count = x.size();\n   // count the number of elements below 0.25, 0.5, 0.75, 1\n   for(size_t i = 0; i < count; i++) {\n       auto const& val = x[i];\n       if(val >= 0 && val <= 0.25) bins[0]++;\n       if(val > 0.25 && val <= 0.5) bins[1]++;\n       if(val > 0.5 && val <= 0.75) bins[2]++;\n       if(val > 0.75 && val <= 1)   bins[3]++;\n   }\n}",
            "}",
            "for(auto elem: x)\n  {\n    if (elem < 0.25)\n    {\n      bins[0] += 1;\n    }\n    else if (elem < 0.5)\n    {\n      bins[1] += 1;\n    }\n    else if (elem < 0.75)\n    {\n      bins[2] += 1;\n    }\n    else\n    {\n      bins[3] += 1;\n    }\n  }\n}",
            "// TODO\n\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (auto i = 0; i < x.size(); ++i) {\n\n      double frac = x[i] - floor(x[i]);\n      if (frac >= 0.0 && frac < 0.25) {\n         bins[0]++;\n      } else if (frac >= 0.25 && frac < 0.5) {\n         bins[1]++;\n      } else if (frac >= 0.5 && frac < 0.75) {\n         bins[2]++;\n      } else if (frac >= 0.75 && frac < 1.0) {\n         bins[3]++;\n      }\n   }\n}",
            "double upperBound;\n    double lowerBound;\n    // your code goes here\n\n}",
            "// TODO: fill this in\n}",
            "std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n    for (auto const &num : x) {\n        // TODO: Compute which bin the number belongs to\n        size_t index;\n\n        if (num < 0.25) {\n            index = 0;\n        } else if (num >= 0.25 && num < 0.5) {\n            index = 1;\n        } else if (num >= 0.5 && num < 0.75) {\n            index = 2;\n        } else {\n            index = 3;\n        }\n        counts[index]++;\n    }\n\n    bins = counts;\n}",
            "// TODO\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    for (auto const &x_value : x) {\n        double fractional_part = x_value - std::floor(x_value);\n        if (fractional_part < 0.25)\n            bins[0]++;\n        else if (fractional_part < 0.5)\n            bins[1]++;\n        else if (fractional_part < 0.75)\n            bins[2]++;\n        else if (fractional_part < 1.0)\n            bins[3]++;\n    }\n}",
            "// Count the number of doubles in the vector x that have a fractional part \n   // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.00 && x[i] < 0.25) {\n            bins[0]++;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.50) {\n            bins[1]++;\n        }\n        else if (x[i] >= 0.50 && x[i] < 0.75) {\n            bins[2]++;\n        }\n        else if (x[i] >= 0.75 && x[i] < 1.00) {\n            bins[3]++;\n        }\n    }\n}",
            "for(size_t i = 0; i < x.size(); ++i) {\n        double frac = std::fmod(x[i], 1.0);\n        if (frac < 0.25) {\n            ++bins[0];\n        } else if (frac < 0.5) {\n            ++bins[1];\n        } else if (frac < 0.75) {\n            ++bins[2];\n        } else if (frac < 1.0) {\n            ++bins[3];\n        } else {\n            throw std::runtime_error(\"Error: fractional part of a double is not in the range [0,1).\");\n        }\n    }\n}",
            "for(auto xi : x) {\n        bins[0] += xi < 0.25;\n        bins[1] += xi >= 0.25 && xi < 0.5;\n        bins[2] += xi >= 0.5 && xi < 0.75;\n        bins[3] += xi >= 0.75 && xi < 1;\n    }\n}",
            "for (double el: x) {\n        int index = static_cast<int>(floor(el * 4));\n        if (index > 3) index = 3;\n        bins[index]++;\n    }\n}",
            "/* TODO: implement the method */\n}",
            "}",
            "// TODO: Implement this function\n}",
            "for(size_t i = 0; i < x.size(); i++)\n   {\n      if(x[i] < 0.25)\n      {\n         bins[0]++;\n      }\n      else if(x[i] < 0.5)\n      {\n         bins[1]++;\n      }\n      else if(x[i] < 0.75)\n      {\n         bins[2]++;\n      }\n      else if(x[i] < 1)\n      {\n         bins[3]++;\n      }\n   }\n}",
            "size_t n = x.size();\n    bins = {0, 0, 0, 0};\n    for (size_t i = 0; i < n; ++i) {\n        double fract = std::modf(x[i], &x[i]);\n        if (fract < 0.25) {\n            bins[0]++;\n        } else if (fract < 0.5) {\n            bins[1]++;\n        } else if (fract < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for(auto i = 0; i < x.size(); i++) {\n       if(x[i] < 0) {\n           bins[0]++;\n       }\n       else if(x[i] >= 0 && x[i] < 0.25) {\n           bins[1]++;\n       }\n       else if(x[i] >= 0.25 && x[i] < 0.5) {\n           bins[2]++;\n       }\n       else if(x[i] >= 0.5 && x[i] < 0.75) {\n           bins[3]++;\n       }\n       else {\n           bins[4]++;\n       }\n    }\n}",
            "bins.fill(0);\n\n  for (auto const& elem : x) {\n    int fraction = elem * 4;\n    if (fraction >= 3 && fraction <= 7) {\n      bins[0]++;\n    } else if (fraction >= 1 && fraction <= 5) {\n      bins[1]++;\n    } else if (fraction >= 5 && fraction <= 9) {\n      bins[2]++;\n    } else if (fraction >= 0 && fraction <= 3) {\n      bins[3]++;\n    }\n  }\n}",
            "for(double n: x) {\n       bins[static_cast<size_t>(4 * n - std::floor(4*n))]++;\n   }\n}",
            "// TODO\n}",
            "size_t n = x.size();\n    bins = {0, 0, 0, 0};\n    for (size_t i = 0; i < n; ++i) {\n        double f = frac(x[i]);\n        if (f < 0.25) {\n            bins[0]++;\n        } else if (f < 0.5) {\n            bins[1]++;\n        } else if (f < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins = { 0, 0, 0, 0 };\n\n  // TODO: Replace this with your code\n\n}",
            "// TODO: implement this\n   bins = std::array<size_t, 4>{ 0, 0, 0, 0 };\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        int k = static_cast<int>(4.0*x[i]);\n        bins[k]++;\n    }\n}",
            "std::array<size_t, 4> counts;\n  counts.fill(0);\n\n  for (auto value : x) {\n    int i = std::floor(value * 4) % 4;\n    if (i < 0)\n      i += 4;\n    counts[i]++;\n  }\n\n  bins[0] = counts[0];\n  bins[1] = counts[1];\n  bins[2] = counts[2];\n  bins[3] = counts[3];\n}",
            "size_t b1 = 0, b2 = 0, b3 = 0, b4 = 0;\n\n  for(double i : x) {\n    if(i < 1.0)\n      b1++;\n    else if(i < 2.0)\n      b2++;\n    else if(i < 3.0)\n      b3++;\n    else if(i < 4.0)\n      b4++;\n  }\n  bins[0] = b1; bins[1] = b2; bins[2] = b3; bins[3] = b4;\n}",
            "// TODO\n\n}",
            "// Your code here.\n\tfor(auto i=0; i<x.size(); i++){\n\t\tdouble digit=x[i];\n\t\tdigit=digit-floor(digit);\n\t\tif(digit==0)\n\t\t\tbins[0]++;\n\t\telse if(digit==0.25)\n\t\t\tbins[1]++;\n\t\telse if(digit==0.5)\n\t\t\tbins[2]++;\n\t\telse if(digit==0.75)\n\t\t\tbins[3]++;\n\t}\n}",
            "for (auto i = 0u; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bins[3]++;\n    } else if (x[i] >= 1) {\n      std::cout << \"The input should be a vector of values in [0, 1)\";\n    } else {\n      std::cout << \"The input should be a vector of values in [0, 1)\";\n    }\n  }\n}",
            "for (auto val : x) {\n    if (val >= 0 && val < 0.25) {\n      bins[0] += 1;\n    } else if (val >= 0.25 && val < 0.5) {\n      bins[1] += 1;\n    } else if (val >= 0.5 && val < 0.75) {\n      bins[2] += 1;\n    } else if (val >= 0.75 && val < 1) {\n      bins[3] += 1;\n    }\n  }\n}",
            "bins = {0, 0, 0, 0};\n   //...\n}",
            "// TODO\n    bins.fill(0);\n}",
            "// TODO\n}",
            "for(size_t i = 0; i < x.size(); i++) {\n        // TODO:\n        if(x[i] >= 0 && x[i] < 0.25){\n            bins[0]++;\n        }\n        else if(x[i] >= 0.25 && x[i] < 0.5){\n            bins[1]++;\n        }\n        else if(x[i] >= 0.5 && x[i] < 0.75){\n            bins[2]++;\n        }\n        else if(x[i] >= 0.75 && x[i] < 1){\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "for (int i = 0; i < x.size(); i++) {\n\n        double frac = x[i] - std::floor(x[i]);\n        if (frac >= 0.0 && frac < 0.25) {\n            bins[0] += 1;\n        }\n        else if (frac >= 0.25 && frac < 0.5) {\n            bins[1] += 1;\n        }\n        else if (frac >= 0.5 && frac < 0.75) {\n            bins[2] += 1;\n        }\n        else if (frac >= 0.75 && frac < 1.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "size_t b = 0; // current bin\n    size_t n = x.size(); // the size of the vector x\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] >= 0 && x[i] <= 0.25) {\n            b = 0;\n        }\n        else if (x[i] > 0.25 && x[i] <= 0.5) {\n            b = 1;\n        }\n        else if (x[i] > 0.5 && x[i] <= 0.75) {\n            b = 2;\n        }\n        else if (x[i] > 0.75 && x[i] <= 1) {\n            b = 3;\n        }\n        bins[b]++;\n    }\n}",
            "size_t i = 0;\n    for (auto const& x_i: x) {\n        if (0 <= x_i && x_i < 0.25) {\n            bins[0] += 1;\n        } else if (0.25 <= x_i && x_i < 0.5) {\n            bins[1] += 1;\n        } else if (0.5 <= x_i && x_i < 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n        i += 1;\n    }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    for(int i = 0; i < 4; i++){\n        bins[i] = 0;\n    }\n    for(int i = 0; i < x.size(); i++){\n        if(x[i] < 0.25){\n            bins[0] += 1;\n        }\n        else if(x[i] < 0.5){\n            bins[1] += 1;\n        }\n        else if(x[i] < 0.75){\n            bins[2] += 1;\n        }\n        else{\n            bins[3] += 1;\n        }\n    }\n}",
            "/* Replace this comment with your code */\n}",
            "for(auto i = 0; i < x.size(); i++) {\n\t\tif(x[i] >= 0 && x[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if(x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if(x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO\n}",
            "// TODO: write your code here\n    for (auto const& a : x) {\n        double fractional = std::fmod(a, 1.0);\n        if (fractional >= 0.0 && fractional <= 0.25) {\n            ++bins[0];\n        } else if (fractional > 0.25 && fractional <= 0.5) {\n            ++bins[1];\n        } else if (fractional > 0.5 && fractional <= 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "for (double e: x) {\n        auto frac = e - std::floor(e);\n        auto bin = static_cast<int>(frac * 4);\n        ++bins[bin];\n    }\n}",
            "size_t length = x.size();\n\n    for (size_t i = 0; i < length; i++) {\n        double x_i = x[i];\n\n        if (x_i < 0.25) {\n            bins[0]++;\n        } else if (0.25 <= x_i && x_i < 0.5) {\n            bins[1]++;\n        } else if (0.5 <= x_i && x_i < 0.75) {\n            bins[2]++;\n        } else if (0.75 <= x_i && x_i <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& xi : x) {\n        if (xi < 0.25) {\n            ++bins[0];\n        } else if (xi < 0.5) {\n            ++bins[1];\n        } else if (xi < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    // TODO\n\n}",
            "for (auto const& a : x) {\n        double fractionalPart = fmod(a, 1);\n        if (fractionalPart >= 0 && fractionalPart <= 0.25) {\n            bins[0]++;\n        } else if (fractionalPart >= 0.25 && fractionalPart <= 0.5) {\n            bins[1]++;\n        } else if (fractionalPart >= 0.5 && fractionalPart <= 0.75) {\n            bins[2]++;\n        } else if (fractionalPart >= 0.75 && fractionalPart <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// your code here\n    for (double v : x) {\n        size_t q = (size_t)std::round(v*4);\n        bins[q]++;\n    }\n}",
            "std::array<size_t, 4> temp = {0, 0, 0, 0};\n    for (double val : x) {\n        // TODO: Your code here\n    }\n    bins = temp;\n}",
            "// Fill in the code here\n}",
            "int i=0;\n  for(auto it=x.begin();it<x.end();it++) {\n    int d = (int) ( (*it - (int) *it) / 0.25);\n    bins[d]++;\n  }\n}",
            "for (auto& el : bins) {\n    el = 0;\n  }\n  for (auto& el : x) {\n    auto d = el - std::floor(el);\n    if (d < 0) {\n      std::cerr << \"Invalid double encountered: \" << el << \"\\n\";\n      exit(1);\n    }\n    int i = 0;\n    if (d > 0.75) {\n      i = 3;\n    } else if (d > 0.5) {\n      i = 2;\n    } else if (d > 0.25) {\n      i = 1;\n    }\n    ++bins[i];\n  }\n}",
            "// Write your code here\n  \n}",
            "bins = {0,0,0,0};\n\n  // TODO: Implement this function\n  double lower_quartile = 0.25;\n  double upper_quartile = 0.75;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] < lower_quartile) {\n      bins[0] += 1;\n    } else if (x[i] >= lower_quartile && x[i] < upper_quartile) {\n      bins[1] += 1;\n    } else if (x[i] >= upper_quartile) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "// YOUR CODE HERE\n\t// TODO: Implement\n    // TODO: Implement\n    // TODO: Implement\n}",
            "// TODO\n}",
            "int i;\n    double number;\n    bins = { 0, 0, 0, 0 };\n\n    for (i = 0; i < x.size(); i++)\n    {\n        number = x[i] - std::floor(x[i]);\n\n        if (number >= 0.0 && number < 0.25)\n        {\n            bins[0] += 1;\n        }\n\n        else if (number >= 0.25 && number < 0.5)\n        {\n            bins[1] += 1;\n        }\n\n        else if (number >= 0.5 && number < 0.75)\n        {\n            bins[2] += 1;\n        }\n\n        else if (number >= 0.75 && number < 1.0)\n        {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins = {{0, 0, 0, 0}};\n   for (auto const& i : x) {\n      if (i >= 0.0 && i < 0.25) {\n         bins[0]++;\n      } else if (i >= 0.25 && i < 0.5) {\n         bins[1]++;\n      } else if (i >= 0.5 && i < 0.75) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n  for (auto i: x) {\n    int q = (i - floor(i)) * 4;\n    if (q < 1) {\n      bins[0] += 1;\n    } else if (q < 2) {\n      bins[1] += 1;\n    } else if (q < 3) {\n      bins[2] += 1;\n    } else if (q < 4) {\n      bins[3] += 1;\n    } else {\n      assert(false);\n    }\n  }\n}",
            "// TODO: implement this\n}",
            "std::array<size_t, 4> counts = {0, 0, 0, 0};\n    for(double e: x) {\n        if(e < 0.25) {\n            counts[0]++;\n        } else if(e < 0.5) {\n            counts[1]++;\n        } else if(e < 0.75) {\n            counts[2]++;\n        } else {\n            counts[3]++;\n        }\n    }\n    bins = counts;\n}",
            "// TODO: Your code goes here\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        double value = std::fmod(x[i], 1.0);\n        if (value < 0.25)\n            bins[0]++;\n        else if (value < 0.50)\n            bins[1]++;\n        else if (value < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "// TODO: Implement this function\n  for (double n : x)\n  {\n    //std::cout << \"n = \" << n << std::endl;\n    double n_int = std::trunc(n);\n    double n_frac = n - n_int;\n    if (n_frac >= 0 && n_frac < 0.25)\n    {\n      bins[0]++;\n    }\n    else if (n_frac >= 0.25 && n_frac < 0.5)\n    {\n      bins[1]++;\n    }\n    else if (n_frac >= 0.5 && n_frac < 0.75)\n    {\n      bins[2]++;\n    }\n    else if (n_frac >= 0.75 && n_frac < 1)\n    {\n      bins[3]++;\n    }\n  }\n}",
            "/* Your code here */\n}",
            "bins = {0,0,0,0};\n   for (double val: x) {\n      double fractional = std::fmod(val, 1);\n      if (fractional >= 0.75) {\n         bins[3]++;\n      }\n      else if (fractional >= 0.5) {\n         bins[2]++;\n      }\n      else if (fractional >= 0.25) {\n         bins[1]++;\n      }\n      else {\n         bins[0]++;\n      }\n   }\n}",
            "for (double d : x) {\n      int digit = static_cast<int>(d*4) % 4;\n      ++bins[digit];\n   }\n}",
            "bins.fill(0);\n  for (double value : x) {\n    double tmp = std::floor(value / 0.25);\n    ++bins[tmp];\n  }\n}",
            "size_t binsSum = 0;\n    for (int i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    for (double xi : x) {\n        int index = floor(xi * 4);\n        if (index < 0) {\n            index = 0;\n        } else if (index >= bins.size()) {\n            index = bins.size() - 1;\n        }\n\n        bins[index]++;\n        binsSum++;\n    }\n\n    for (int i = 0; i < bins.size(); i++) {\n        bins[i] = bins[i] * 4 / binsSum;\n    }\n}",
            "}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n  // TODO: replace this loop with your code\n  for (double v : x) {\n    auto frac = std::fmod(v, 1.0);\n    if (frac == 0.0) {\n      bins[0] += 1;\n    } else if (frac >= 0.25 && frac < 0.5) {\n      bins[1] += 1;\n    } else if (frac >= 0.5 && frac < 0.75) {\n      bins[2] += 1;\n    } else if (frac >= 0.75 && frac < 1.0) {\n      bins[3] += 1;\n    }\n  }\n}",
            "}",
            "for (double i : x) {\n        int index = floor(i * 4);\n        bins[index]++;\n    }\n}",
            "for (auto i = 0; i < 4; i++) bins[i] = 0;\n\n\tfor (auto i = 0; i < x.size(); i++) {\n\t\tif (x[i] - floor(x[i]) >= 0 && x[i] - floor(x[i]) <= 0.25) bins[0]++;\n\t\telse if (x[i] - floor(x[i]) >= 0.25 && x[i] - floor(x[i]) <= 0.5) bins[1]++;\n\t\telse if (x[i] - floor(x[i]) >= 0.5 && x[i] - floor(x[i]) <= 0.75) bins[2]++;\n\t\telse if (x[i] - floor(x[i]) >= 0.75 && x[i] - floor(x[i]) <= 1) bins[3]++;\n\t}\n}",
            "for (int i = 0; i < x.size(); i++) {\n    double temp = x[i] % 1;\n    if (temp >= 0 && temp < 0.25)\n      bins[0]++;\n    else if (temp >= 0.25 && temp < 0.5)\n      bins[1]++;\n    else if (temp >= 0.5 && temp < 0.75)\n      bins[2]++;\n    else if (temp >= 0.75 && temp < 1)\n      bins[3]++;\n  }\n}",
            "for (auto const& xi : x) {\n    auto const f = std::fmod(xi, 1);\n    if (f == 0) {\n      ++bins[0];\n    } else if (f <= 0.25) {\n      ++bins[1];\n    } else if (f <= 0.5) {\n      ++bins[2];\n    } else if (f <= 0.75) {\n      ++bins[3];\n    } else {\n      ++bins[0];\n    }\n  }\n}",
            "bins.fill(0);\n  for (auto& num : x) {\n    bins[int((num - floor(num)) * 4)]++;\n  }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] > 0 && x[i] < 0.25)\n\t\t\tbins[0]++;\n\t\telse if (x[i] > 0.25 && x[i] < 0.5)\n\t\t\tbins[1]++;\n\t\telse if (x[i] > 0.5 && x[i] < 0.75)\n\t\t\tbins[2]++;\n\t\telse\n\t\t\tbins[3]++;\n\t}\n}",
            "for (double value: x) {\n        if (value >= 0 && value < 0.25)\n            bins[0]++;\n        else if (value >= 0.25 && value < 0.5)\n            bins[1]++;\n        else if (value >= 0.5 && value < 0.75)\n            bins[2]++;\n        else if (value >= 0.75 && value < 1)\n            bins[3]++;\n    }\n}",
            "bins = std::array<size_t, 4> {0, 0, 0, 0};\n    for (double d: x) {\n        if (d < 0) {\n            throw std::invalid_argument(\"Negative values are not allowed.\");\n        } else if (d >= 1) {\n            throw std::invalid_argument(\"Values should be < 1\");\n        }\n        int i = 4 * d;\n        if (i < 0) {\n            i = 0;\n        } else if (i >= 4) {\n            i = 3;\n        }\n        bins[i]++;\n    }\n}",
            "bins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\tfor (size_t i = 0; i < x.size(); i++)\n\t{\n\t\tint current = x.at(i);\n\t\tdouble currentBin = (current + 0.25) / 0.25;\n\t\tif (currentBin < 1)\n\t\t{\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (currentBin < 2)\n\t\t{\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (currentBin < 3)\n\t\t{\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (currentBin < 4)\n\t\t{\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "for (auto v : x) {\n    int d = 0;\n    if (v >= 0.75)\n      d = 3;\n    else if (v >= 0.5)\n      d = 2;\n    else if (v >= 0.25)\n      d = 1;\n    bins[d]++;\n  }\n}",
            "for (auto const& el : x) {\n        size_t frac = el * 4;\n        frac = frac % 4;\n        bins[frac] += 1;\n    }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for (double d : x) {\n    if (d >= 0.0 && d < 0.25)\n      bins[0]++;\n    else if (d >= 0.25 && d < 0.5)\n      bins[1]++;\n    else if (d >= 0.5 && d < 0.75)\n      bins[2]++;\n    else if (d >= 0.75 && d < 1.0)\n      bins[3]++;\n  }\n}",
            "for (double value : x) {\n        if (value < 0.25) {\n            bins[0]++;\n        } else if (value < 0.5) {\n            bins[1]++;\n        } else if (value < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for(auto& v : bins) {\n      v = 0;\n   }\n   for (auto const& v : x) {\n      if (v < 0.25) {\n         bins[0]++;\n      } else if (v < 0.5) {\n         bins[1]++;\n      } else if (v < 0.75) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n\n  for (auto const& a : x) {\n    if (a < 0.25) {\n      ++bins[0];\n    } else if (a < 0.5) {\n      ++bins[1];\n    } else if (a < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "for(size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    double min = std::numeric_limits<double>::min();\n    double max = std::numeric_limits<double>::max();\n\n    for(size_t i = 0; i < x.size(); i++) {\n        if(x[i] >= 0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if(x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if(x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if(x[i] >= 0.75 && x[i] < 1) {\n            bins[3]++;\n        } else if(x[i] == min) {\n            bins[0]++;\n        } else if(x[i] == max) {\n            bins[3]++;\n        }\n    }\n\n}",
            "}",
            "// TODO: implement this function\n\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        double digit = x[i] - std::floor(x[i]);\n        if (digit >= 0 && digit < 0.25) {\n            bins[0]++;\n        } else if (digit >= 0.25 && digit < 0.5) {\n            bins[1]++;\n        } else if (digit >= 0.5 && digit < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n   for (auto val: x) {\n      int index = static_cast<int>((val - std::floor(val)) * 4);\n      if (index >= 0 && index <= 3) {\n         ++bins[index];\n      }\n   }\n}",
            "//...\n}",
            "for(size_t i = 0; i < x.size(); i++) {\n      auto xi = x[i];\n      auto frac = std::modf(xi, &xi);\n      if(frac >= 0 && frac < 0.25) {\n         bins[0] += 1;\n      } else if(frac >= 0.25 && frac < 0.5) {\n         bins[1] += 1;\n      } else if(frac >= 0.5 && frac < 0.75) {\n         bins[2] += 1;\n      } else if(frac >= 0.75 && frac < 1) {\n         bins[3] += 1;\n      }\n   }\n}",
            "std::fill(std::begin(bins), std::end(bins), 0);\n    for (auto const& v : x) {\n        if (std::modf(v, &v) < 0.25) {\n            bins[0]++;\n        }\n        else if (std::modf(v, &v) < 0.5) {\n            bins[1]++;\n        }\n        else if (std::modf(v, &v) < 0.75) {\n            bins[2]++;\n        }\n        else if (std::modf(v, &v) < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "std::array<double, 4> limits = {0.0, 0.25, 0.5, 0.75};\n    std::fill(bins.begin(), bins.end(), 0);\n    for (double const& d : x) {\n        for (size_t i = 0; i < bins.size(); ++i) {\n            if (d > limits[i] && d <= limits[i+1]) {\n                ++bins[i];\n                break;\n            }\n        }\n    }\n}",
            "std::array<double, 4> intervals = {0, 0.25, 0.5, 0.75};\n\n  std::array<size_t, 4> temp{};\n  for (const auto &i : x) {\n    auto it = std::lower_bound(intervals.begin(), intervals.end(), i);\n    int index = it - intervals.begin();\n    temp[index]++;\n  }\n\n  bins = temp;\n}",
            "//TODO\n\tsize_t n = x.size();\n\tstd::vector<double> fracs;\n\tfor(size_t i = 0; i < n; ++i){\n\t\tdouble frac = x[i] - std::floor(x[i]);\n\t\tfracs.push_back(frac);\n\t}\n\n\tsize_t n0 = 0;\n\tsize_t n1 = 0;\n\tsize_t n2 = 0;\n\tsize_t n3 = 0;\n\tfor(size_t i = 0; i < n; ++i){\n\t\tif(fracs[i] >= 0 && fracs[i] <= 0.25){\n\t\t\tn0++;\n\t\t}\n\t\telse if(fracs[i] > 0.25 && fracs[i] <= 0.5){\n\t\t\tn1++;\n\t\t}\n\t\telse if(fracs[i] > 0.5 && fracs[i] <= 0.75){\n\t\t\tn2++;\n\t\t}\n\t\telse if(fracs[i] > 0.75 && fracs[i] <= 1){\n\t\t\tn3++;\n\t\t}\n\t}\n\n\tbins[0] = n0;\n\tbins[1] = n1;\n\tbins[2] = n2;\n\tbins[3] = n3;\n}",
            "bins = {0, 0, 0, 0};\n\n  for (auto const& xi : x) {\n\n    if (xi == 0.0) {\n      ++bins[0];\n    } else if (xi <= 0.25) {\n      ++bins[1];\n    } else if (xi <= 0.5) {\n      ++bins[2];\n    } else if (xi <= 0.75) {\n      ++bins[3];\n    } else {\n      ++bins[4];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: Implement\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  double a = x[i];\n  int a0 = a;\n  double a1 = a - a0;\n\n  if (a1 < 0.25)\n    atomicAdd(&(bins[0]), 1);\n  else if (a1 < 0.5)\n    atomicAdd(&(bins[1]), 1);\n  else if (a1 < 0.75)\n    atomicAdd(&(bins[2]), 1);\n  else\n    atomicAdd(&(bins[3]), 1);\n}",
            "}",
            "/* Add your code here. */\n\n}",
            "// Your code here\n\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n\n  double remainder = x[idx] - floor(x[idx]);\n\n  if (remainder < 0.25) {\n    atomicAdd(bins, 0, cuda::plus<size_t>{});\n  } else if (remainder < 0.5) {\n    atomicAdd(bins, 1, cuda::plus<size_t>{});\n  } else if (remainder < 0.75) {\n    atomicAdd(bins, 2, cuda::plus<size_t>{});\n  } else {\n    atomicAdd(bins, 3, cuda::plus<size_t>{});\n  }\n}",
            "// Implement this!\n\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (index < N) {\n        // TODO: Implement\n    }\n}",
            "size_t idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (idx >= N) return;\n\n   double mod = fmod(x[idx], 1);\n   if (mod < 0.25)\n      atomicAdd(&bins[0], 1);\n   else if (mod < 0.5)\n      atomicAdd(&bins[1], 1);\n   else if (mod < 0.75)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "// TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= N) return;\n\n    double xi = x[i];\n    if (xi < 0 || xi >= 1) return;\n\n    double fraction = xi - floor(xi);\n\n    if (fraction < 0.25) {\n        atomicAdd(&bins[0], 1);\n    } else if (fraction < 0.5) {\n        atomicAdd(&bins[1], 1);\n    } else if (fraction < 0.75) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    double remainder = x[i] - floor(x[i]);\n    if      (remainder < 0.25)  atomicAdd(&bins[0], 1);\n    else if (remainder < 0.5)   atomicAdd(&bins[1], 1);\n    else if (remainder < 0.75)  atomicAdd(&bins[2], 1);\n    else                        atomicAdd(&bins[3], 1);\n}",
            "//...\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if(i < N) {\n      int i0 = (int)(x[i] * 4.0);\n      atomicAdd(&bins[i0], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double xi = x[i];\n        xi = floor(xi * 4.0);\n        atomicAdd(&bins[(size_t)xi], 1);\n    }\n}",
            "__shared__ size_t sBins[4];\n  // Initialize the shared memory\n  if (threadIdx.x == 0) {\n    sBins[0] = 0;\n    sBins[1] = 0;\n    sBins[2] = 0;\n    sBins[3] = 0;\n  }\n  // Synchronize so that shared memory is initialized.\n  __syncthreads();\n\n  // Loop over the vector x and count the number of values in the bins\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    double frac = fmod(x[i], 1.0);\n    if (frac >= 0.75) {\n      atomicAdd(&sBins[3], 1);\n    } else if (frac >= 0.5) {\n      atomicAdd(&sBins[2], 1);\n    } else if (frac >= 0.25) {\n      atomicAdd(&sBins[1], 1);\n    } else {\n      atomicAdd(&sBins[0], 1);\n    }\n  }\n  __syncthreads();\n\n  // Read shared memory to global memory\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], sBins[0]);\n    atomicAdd(&bins[1], sBins[1]);\n    atomicAdd(&bins[2], sBins[2]);\n    atomicAdd(&bins[3], sBins[3]);\n  }\n}",
            "/*\n   * Replace this with your code\n   *\n   * Hint: You can use the function `fmod` from `<cmath>` to get the\n   * fractional part of a double:\n   *\n   *     double a = 12.23;\n   *     double f = fmod(a, 1.0);  // f = 0.23\n   *\n   *     double b = 1.3;\n   *     double g = fmod(b, 1.0);  // g = 0.3\n   *\n   * Also, note that `x` is a pointer to the first element of `x` and that\n   * `N` is the number of elements in `x`.\n   */\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n      double d = x[tid] - floor(x[tid]);\n      if (d < 0.25) bins[0]++;\n      else if (d < 0.5) bins[1]++;\n      else if (d < 0.75) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "// TODO: implement this\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if(tid >= N) return;\n\n    const double fract = x[tid] - floor(x[tid]);\n    const int q = (fract < 0.25)? 0 : (fract < 0.5)? 1 : (fract < 0.75)? 2 : 3;\n    atomicAdd(&bins[q], 1);\n}",
            "/* YOUR CODE HERE */\n}",
            "__shared__ size_t sdata[N];\n  const size_t tid = threadIdx.x;\n  sdata[tid] = 0;\n\n  __syncthreads();\n\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double d = x[i] - std::floor(x[i]);\n    if (d < 0.25)\n      sdata[tid] += 1;\n    else if (d < 0.5)\n      sdata[tid + 1] += 1;\n    else if (d < 0.75)\n      sdata[tid + 2] += 1;\n    else\n      sdata[tid + 3] += 1;\n  }\n\n  for (size_t i = blockDim.x / 2; i > 0; i >>= 1) {\n    if (tid < i)\n      sdata[tid] += sdata[tid + i];\n    __syncthreads();\n  }\n\n  if (tid == 0) {\n    bins[0] = sdata[0];\n    bins[1] = sdata[1];\n    bins[2] = sdata[2];\n    bins[3] = sdata[3];\n  }\n}",
            "// Add your code here\n\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n\n  if (x[i] < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (x[i] < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (x[i] < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (x[i] < 1) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if(idx < N) {\n      double xd = x[idx];\n\n      // TODO: Determine the index of the bin for xd in [0, 1)\n      int bin = 0;\n\n      // TODO: Use atomicAdd to update the corresponding bin\n   }\n}",
            "// Implement the kernel here\n}",
            "extern __shared__ size_t s_bins[];\n\n    size_t bin = 0;\n    for (size_t i=threadIdx.x; i<N; i+=blockDim.x) {\n        // Determine which bin x[i] belongs to\n        if (x[i] >= 0 && x[i] < 1) bin = 0;\n        else if (x[i] >= 1 && x[i] < 2) bin = 1;\n        else if (x[i] >= 2 && x[i] < 3) bin = 2;\n        else if (x[i] >= 3 && x[i] < 4) bin = 3;\n\n        // Atomic add to bin to avoid data races\n        atomicAdd(&s_bins[bin], 1);\n    }\n\n    __syncthreads();\n\n    // Copy s_bins back to bins\n    for (size_t i=threadIdx.x; i<4; i+=blockDim.x) {\n        atomicAdd(&bins[i], s_bins[i]);\n    }\n}",
            "// Define the shared memory for the warp\n  __shared__ double sharedX[blockDim.x];\n\n  // Determine the warp's id within the block\n  const int warpId = threadIdx.x / warpSize;\n\n  // Determine the thread's id within the warp\n  const int laneId = threadIdx.x % warpSize;\n\n  // Compute the global index for the thread\n  const int globalId = (blockDim.x * blockIdx.x) + threadIdx.x;\n\n  // Check if the thread has valid work to do\n  if (globalId < N) {\n    // Load the data for the warp\n    sharedX[threadIdx.x] = x[globalId];\n  } else {\n    // Set the shared memory to 0 for non-active threads\n    sharedX[threadIdx.x] = 0.0;\n  }\n\n  // Wait for all threads to finish loading data\n  __syncthreads();\n\n  // Count the numbers of values in the range 0.0 to 0.25\n  unsigned int count = 0;\n  for (int i = laneId; i < blockDim.x; i += warpSize) {\n    if (sharedX[i] >= 0 && sharedX[i] < 0.25) {\n      count += 1;\n    }\n  }\n\n  // Wait for all warps to finish counting\n  __syncwarp();\n\n  // Sum the values in the warp\n  count = warpSum(count);\n\n  // If this is the first lane of the warp, add the count to the output array\n  if (laneId == 0) {\n    atomicAdd(&bins[0], count);\n  }\n\n  // Wait for all warps to finish storing their values\n  __syncthreads();\n\n  // Count the numbers of values in the range 0.25 to 0.5\n  count = 0;\n  for (int i = laneId; i < blockDim.x; i += warpSize) {\n    if (sharedX[i] >= 0.25 && sharedX[i] < 0.5) {\n      count += 1;\n    }\n  }\n\n  // Wait for all warps to finish counting\n  __syncwarp();\n\n  // Sum the values in the warp\n  count = warpSum(count);\n\n  // If this is the first lane of the warp, add the count to the output array\n  if (laneId == 0) {\n    atomicAdd(&bins[1], count);\n  }\n\n  // Wait for all warps to finish storing their values\n  __syncthreads();\n\n  // Count the numbers of values in the range 0.5 to 0.75\n  count = 0;\n  for (int i = laneId; i < blockDim.x; i += warpSize) {\n    if (sharedX[i] >= 0.5 && sharedX[i] < 0.75) {\n      count += 1;\n    }\n  }\n\n  // Wait for all warps to finish counting\n  __syncwarp();\n\n  // Sum the values in the warp\n  count = warpSum(count);\n\n  // If this is the first lane of the warp, add the count to the output array\n  if (laneId == 0) {\n    atomicAdd(&bins[2], count);\n  }\n\n  // Wait for all warps to finish storing their values\n  __syncthreads();\n\n  // Count the numbers of values in the range 0.75 to 1.0\n  count = 0;\n  for (int i = laneId; i < blockDim.x; i += warpSize) {\n    if (sharedX[i] >= 0.75 && sharedX[i] < 1.0) {\n      count += 1;\n    }\n  }\n\n  // Wait for all warps to finish counting\n  __syncwarp();\n\n  // Sum the values in the warp\n  count = warpSum(count);\n\n  // If this is the first lane of the warp, add the count to the output array\n  if",
            "/* Add your code here */\n  return;\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  if (tid < N) {\n    double xi = x[tid];\n    double frac = xi - floor(xi);\n\n    if (frac < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    // get fractional part of x[i] (using fmod)\n    double fract = x[i] - floor(x[i]);\n\n    if (fract >= 0.0 && fract < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (fract >= 0.25 && fract < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (fract >= 0.5 && fract < 0.75)\n      atomicAdd(&bins[2], 1);\n    else if (fract >= 0.75 && fract < 1.0)\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "const size_t i = threadIdx.x;\n  if(i >= N)\n    return;\n\n  double f = x[i] - floor(x[i]);\n  if(f >= 0 && f < 0.25)\n    atomicAdd(&bins[0], 1);\n  else if(f >= 0.25 && f < 0.5)\n    atomicAdd(&bins[1], 1);\n  else if(f >= 0.5 && f < 0.75)\n    atomicAdd(&bins[2], 1);\n  else if(f >= 0.75 && f < 1)\n    atomicAdd(&bins[3], 1);\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (id >= N) return;\n\n  double x_i = x[id];\n\n  // Compute the fractional part of x_i\n  double frac_part = x_i - floor(x_i);\n\n  // Determine which bin the value belongs to\n  if (frac_part < 0.25)\n    atomicAdd(&bins[0], 1);\n  else if (frac_part >= 0.25 && frac_part < 0.5)\n    atomicAdd(&bins[1], 1);\n  else if (frac_part >= 0.5 && frac_part < 0.75)\n    atomicAdd(&bins[2], 1);\n  else if (frac_part >= 0.75 && frac_part < 1.0)\n    atomicAdd(&bins[3], 1);\n}",
            "int ix = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Use double to get better accuracy, at the expense of a few extra instructions\n    if (ix < N) {\n        double xi = x[ix];\n\n        // Use integer division to get the index for the bin\n        // Increment if the remainder is greater than or equal to 0.5\n        size_t q = (xi - floor(xi)) >= 0.5? (xi - floor(xi)) + 0.5 : xi - floor(xi);\n        atomicAdd(bins + q, 1);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) return;\n  const double f = x[tid] - (int)x[tid];\n  atomicAdd(&bins[0], f > 0.75);\n  atomicAdd(&bins[1], f > 0.50);\n  atomicAdd(&bins[2], f > 0.25);\n  atomicAdd(&bins[3], f > 0.00);\n}",
            "size_t global_thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (global_thread_id < N) {\n    double x_fract = fmod(x[global_thread_id], 1.0);\n    if (x_fract < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (x_fract < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (x_fract < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   double x_ = (tid < N)? x[tid] : 0;\n   int k = (x_ - floor(x_)) * 4;\n   atomicAdd(bins + k, 1);\n}",
            "// TODO\n}",
            "// This is your code!\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "// Compute an index into the x vector\n   size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n\n   // Count the number of doubles in the vector x that have a fractional part \n   // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   // Use the function fmod(x, 1.0) to compute the fractional part of x.\n   if (i >= N) return;\n   double frac_part = fmod(x[i], 1.0);\n   if (frac_part >= 0.0 && frac_part < 0.25) bins[0]++;\n   else if (frac_part >= 0.25 && frac_part < 0.5) bins[1]++;\n   else if (frac_part >= 0.5 && frac_part < 0.75) bins[2]++;\n   else if (frac_part >= 0.75 && frac_part <= 1.0) bins[3]++;\n}",
            "// TODO: implement me!\n  //...\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i < N) {\n    double f = x[i] - floor(x[i]);\n    if (f < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (f < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (f < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// determine the thread's index in the array\n   size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n   // check that the thread's index is within range\n   if(i < N) {\n\n      // determine the bin that the double should be placed in\n      size_t bin = 0;\n      if(x[i] < 0.25) {\n         bin = 0;\n      } else if(x[i] < 0.5) {\n         bin = 1;\n      } else if(x[i] < 0.75) {\n         bin = 2;\n      } else {\n         bin = 3;\n      }\n\n      // atomically increment the bin's count\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x;\n    double xj = x[j];\n    double xjf = xj - floor(xj);\n    int bin = (xjf >= 0.75)? 3 : (xjf >= 0.5)? 2 : (xjf >= 0.25)? 1 : 0;\n    atomicAdd(&bins[bin], 1);\n}",
            "// Implement\n}",
            "const size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    double xx = x[tid];\n    size_t q = xx < 1;\n    q += xx >= 1 && xx < 2;\n    q += xx >= 2 && xx < 3;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "// TODO\n\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i >= N) return;\n\n  // Your code here\n\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid >= N) return;\n\n  double d = x[tid];\n  d -= floor(d);\n  bins[int(d * 4)]++;\n}",
            "// Fill in the code to count the number of doubles in the vector x that have \n  // a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  // Store the counts in bins. Use atomicAdd to add counts atomically.\n  // The kernel is launched with at least N threads.\n  // You may use only one atomicAdd operation.\n}",
            "// Compute index into x\n  size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n\n  // Compute the range of values to be examined by each thread\n  // This is just a simple example - the real code would use\n  // atomic operations to avoid race conditions.\n  if (threadIdx.x == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n  }\n  if (i < N) {\n    // Compute the double's binary representation\n    // See http://en.wikipedia.org/wiki/Double_precision_floating-point_format\n    unsigned long long bits = ((unsigned long long*)(&x[i]))[0];\n    unsigned long long frac = bits & 0x000fffffffffffff;\n\n    // Count the fractional part in each of the quartiles\n    if (frac >= 0x0000000000000000) bins[0]++;\n    else if (frac < 0x0000000000000080) bins[1]++;\n    else if (frac < 0x0000000000004000) bins[2]++;\n    else if (frac < 0x0000000000008000) bins[3]++;\n  }\n\n  __syncthreads();\n\n  // Add up the counts in the bins to compute the total\n  if (threadIdx.x == 0) {\n    bins[0] += bins[1];\n    bins[2] += bins[3];\n    bins[0] += bins[2];\n  }\n}",
            "int tid = threadIdx.x;\n   int bin = 0;\n   for (int i = tid; i < N; i += blockDim.x) {\n      if (x[i] < 0.25) bin = 0;\n      else if (x[i] < 0.5) bin = 1;\n      else if (x[i] < 0.75) bin = 2;\n      else bin = 3;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// You code here\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n   if (tid >= N) {\n      return;\n   }\n   double fractionalPart = x[tid] - floor(x[tid]);\n   if (fractionalPart < 0.25) {\n      atomicAdd(&bins[0], 1);\n   }\n   else if (fractionalPart < 0.5) {\n      atomicAdd(&bins[1], 1);\n   }\n   else if (fractionalPart < 0.75) {\n      atomicAdd(&bins[2], 1);\n   }\n   else {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "}",
            "// TODO\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n      double frac = x[tid] - floor(x[tid]);\n      int bin = (int)(4.0 * frac);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// Implement this!\n    // AMD HIP: Replace this line with the HIP kernel implementation\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double x_i = x[i];\n    // write code here\n  }\n}",
            "int ix = threadIdx.x + blockDim.x * blockIdx.x;\n\n    // Do not go out of bounds\n    if (ix >= N) {\n        return;\n    }\n\n    // Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    const double val = x[ix];\n    if (val >= 0.0 && val < 0.25) {\n        atomicAdd(&bins[0], 1);\n    }\n    else if (val >= 0.25 && val < 0.5) {\n        atomicAdd(&bins[1], 1);\n    }\n    else if (val >= 0.5 && val < 0.75) {\n        atomicAdd(&bins[2], 1);\n    }\n    else if (val >= 0.75 && val < 1.0) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "int i = threadIdx.x;\n  if (i >= N) return;\n\n  // TODO: add your code here\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  double frac = x[i] - floor(x[i]);\n  if      (frac <  0.25) bins[0]++;\n  else if (frac <  0.5)  bins[1]++;\n  else if (frac <  0.75) bins[2]++;\n  else                   bins[3]++;\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  double frac;\n  if (idx < N) {\n    // Count the number of values in x that are between 0 and 0.25\n    frac = x[idx] - floor(x[idx]);\n    if (frac >= 0 && frac < 0.25)\n      atomicAdd(bins, 1);\n    // Count the number of values in x that are between 0.25 and 0.5\n    frac = x[idx] - floor(x[idx]) - 0.25;\n    if (frac >= 0 && frac < 0.25)\n      atomicAdd(bins + 1, 1);\n    // Count the number of values in x that are between 0.5 and 0.75\n    frac = x[idx] - floor(x[idx]) - 0.5;\n    if (frac >= 0 && frac < 0.25)\n      atomicAdd(bins + 2, 1);\n    // Count the number of values in x that are between 0.75 and 1\n    frac = x[idx] - floor(x[idx]) - 0.75;\n    if (frac >= 0 && frac < 0.25)\n      atomicAdd(bins + 3, 1);\n  }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    const double x_i = x[i];\n    if (i < N) {\n        const double tmp = x_i - floor(x_i);\n        if (tmp < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (tmp < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (tmp < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int tid = blockIdx.x*blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double fract = x[tid] - floor(x[tid]);\n    if (fract < 0.25)\n      atomicAdd(bins, 0, 1);\n    else if (fract < 0.5)\n      atomicAdd(bins, 1, 1);\n    else if (fract < 0.75)\n      atomicAdd(bins, 2, 1);\n    else\n      atomicAdd(bins, 3, 1);\n  }\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  const double d = x[i];\n  if (i < N && d >= 0 && d < 1) {\n    const int q = d * 4;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "extern __shared__ size_t *binsLocal;\n    // Initialize the shared bins to 0\n    for (size_t i = threadIdx.x; i < 4; i += blockDim.x) {\n        binsLocal[i] = 0;\n    }\n    __syncthreads();\n\n    // Increment the correct bin for each element in `x`\n    if (threadIdx.x < N) {\n        int whichBin = 0;\n        double d = x[threadIdx.x];\n        // if (0.0 < d && d < 0.25) {\n        //     whichBin = 0;\n        // } else if (0.25 <= d && d < 0.5) {\n        //     whichBin = 1;\n        // } else if (0.5 <= d && d < 0.75) {\n        //     whichBin = 2;\n        // } else if (0.75 <= d && d < 1.0) {\n        //     whichBin = 3;\n        // }\n        if (d - floor(d) < 0.25) {\n            whichBin = 0;\n        } else if (0.25 <= d && d < 0.5) {\n            whichBin = 1;\n        } else if (0.5 <= d && d < 0.75) {\n            whichBin = 2;\n        } else if (0.75 <= d && d < 1.0) {\n            whichBin = 3;\n        }\n\n        // atomicAdd(&binsLocal[whichBin], 1);\n        atomicAdd(&binsLocal[whichBin], 1);\n    }\n    __syncthreads();\n\n    // Add the local bins to the global memory bins\n    for (size_t i = threadIdx.x; i < 4; i += blockDim.x) {\n        atomicAdd(&bins[i], binsLocal[i]);\n    }\n}",
            "const size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        // Determine the bin to increment\n        size_t bin = (int)((x[i] - floor(x[i])) / 0.25);\n        if (bin > 3) {\n            bin = 3;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  if (x[i] >= 0.0 && x[i] < 0.25) atomicAdd(&bins[0], 1);\n  else if (x[i] >= 0.25 && x[i] < 0.5) atomicAdd(&bins[1], 1);\n  else if (x[i] >= 0.5 && x[i] < 0.75) atomicAdd(&bins[2], 1);\n  else if (x[i] >= 0.75 && x[i] < 1.0) atomicAdd(&bins[3], 1);\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "__shared__ size_t privateBins[4];\n\n  privateBins[0] = 0;\n  privateBins[1] = 0;\n  privateBins[2] = 0;\n  privateBins[3] = 0;\n\n  int tid = threadIdx.x;\n\n  for (int i = tid; i < N; i += blockDim.x) {\n    if (x[i] < 0.25) {\n      atomicAdd(&privateBins[0], 1);\n    } else if (x[i] < 0.5) {\n      atomicAdd(&privateBins[1], 1);\n    } else if (x[i] < 0.75) {\n      atomicAdd(&privateBins[2], 1);\n    } else {\n      atomicAdd(&privateBins[3], 1);\n    }\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    atomicAdd(&bins[0], privateBins[0]);\n    atomicAdd(&bins[1], privateBins[1]);\n    atomicAdd(&bins[2], privateBins[2]);\n    atomicAdd(&bins[3], privateBins[3]);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int q = x[i] - 4 * floor(x[i] / 4);\n    atomicAdd(bins + q, 1);\n  }\n}",
            "}",
            "//TODO: count the number of doubles in the vector x that have a fractional part\n    //  in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    //  Store the counts in `bins`.\n    //  Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Skip any thread that is outside of the range of the vector x\n  if (i >= N)\n    return;\n\n  // Compute the fractional part of x[i]\n  double remainder = x[i] - floor(x[i]);\n\n  // Count the number of elements that fall in each quartile\n  // Note that this will increment the same bin for multiple elements, \n  // which will be compensated for later\n  if (remainder < 0.25)\n    atomicAdd(&(bins[0]), 1);\n  else if (remainder < 0.5)\n    atomicAdd(&(bins[1]), 1);\n  else if (remainder < 0.75)\n    atomicAdd(&(bins[2]), 1);\n  else\n    atomicAdd(&(bins[3]), 1);\n}",
            "size_t thread = blockIdx.x * blockDim.x + threadIdx.x;\n  if (thread < N) {\n    double frac = x[thread] - floor(x[thread]);\n    if (frac >= 0.0 && frac < 0.25) bins[0]++;\n    if (frac >= 0.25 && frac < 0.5) bins[1]++;\n    if (frac >= 0.5 && frac < 0.75) bins[2]++;\n    if (frac >= 0.75 && frac <= 1.0) bins[3]++;\n  }\n}",
            "// TODO\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double fract = x[i] - floor(x[i]);\n    int bin = 0;\n    if (fract > 0.75) {\n      bin = 3;\n    } else if (fract > 0.5) {\n      bin = 2;\n    } else if (fract > 0.25) {\n      bin = 1;\n    }\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    int fractional = 100 * x[i] - floor(100 * x[i]);\n    if (fractional >=  0 && fractional <  25) atomicAdd(&bins[0], 1);\n    if (fractional >= 25 && fractional <  50) atomicAdd(&bins[1], 1);\n    if (fractional >= 50 && fractional <  75) atomicAdd(&bins[2], 1);\n    if (fractional >= 75 && fractional < 100) atomicAdd(&bins[3], 1);\n}",
            "// TODO: your code here\n}",
            "// Initialize bins.\n  for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n    bins[i] = 0;\n  }\n\n  // Sum up the counts of the number of numbers in each quartile.\n  __syncthreads();\n  for (int i = blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x*gridDim.x) {\n    const int quarter = (int) (x[i] - floor(x[i]));\n    atomicAdd(&bins[quarter], 1);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  // Check that the thread index is within the bounds of the array.\n  if (tid >= N) return;\n\n  // Compute which quadrant x[tid] is in.\n  double frac = x[tid] - floor(x[tid]);\n\n  if (frac >= 0 && frac < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (frac >= 0.25 && frac < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (frac >= 0.5 && frac < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (frac >= 0.75 && frac <= 1) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "}",
            "// Compute the index of the thread that has just finished\n  const unsigned int tid = (blockIdx.x * blockDim.x) + threadIdx.x;\n  if (tid >= N) return;\n\n  // Count the number of doubles in x that have a fractional part\n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  int bin = 0;\n  if (x[tid] < 0.25) {\n    bin = 0;\n  } else if (x[tid] < 0.50) {\n    bin = 1;\n  } else if (x[tid] < 0.75) {\n    bin = 2;\n  } else if (x[tid] < 1.00) {\n    bin = 3;\n  }\n\n  atomicAdd(&bins[bin], 1);\n}",
            "unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n  unsigned int startIndex = i * N;\n  unsigned int endIndex = (i + 1) * N;\n\n  // TODO: Fill in this kernel.\n  // Hints:\n  // - You can use `modf(3)` or `frexp(3)` to extract the fractional part of a double.\n  // - You can use `__syncthreads(3)` to wait for all threads in a block to reach the same point.\n  // - You can use `__shfl_sync(3)` to share information between threads.\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  // TODO: compute bins from x[i]\n  int j = (int) (x[i] * 4);\n  if(j >= 0 && j <= 3)\n    atomicAdd(&bins[j], 1);\n}",
            "/* Implementation */\n}",
            "int myId = threadIdx.x + blockIdx.x*blockDim.x;\n  if (myId >= N) return;\n  \n  // A better way to implement the kernel would be to have 4 threads per element \n  // in the vector x. Each thread would increment the counter of the bin whose \n  // quarter range the element is in. However, this could be quite inefficient,\n  // as there would be 4 times more threads than there are elements in the vector.\n  // To avoid this, we can merge the 4 bins into 1, and use an if-elseif-elseif-else \n  // chain to check if the element is in the appropriate bin.\n  \n  // check if the element is in the first bin\n  if ((x[myId] - floor(x[myId])) < 0.25) {\n    atomicAdd(&bins[0], 1);\n  }\n  // check if the element is in the second bin\n  else if ((x[myId] - floor(x[myId])) < 0.5) {\n    atomicAdd(&bins[1], 1);\n  }\n  // check if the element is in the third bin\n  else if ((x[myId] - floor(x[myId])) < 0.75) {\n    atomicAdd(&bins[2], 1);\n  }\n  // check if the element is in the fourth bin\n  else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int index = threadIdx.x;\n  int stride = blockDim.x;\n\n  // TODO\n}",
            "/*\n  - The grid is 1D, with one block per thread block, and one thread per element.\n  - The thread block index (blockIdx.x) is the bin index (0, 1, 2, or 3).\n  - The thread index within the block (threadIdx.x) is the element index.\n  */\n  int binIndex = blockIdx.x;\n  size_t xIndex = threadIdx.x;\n\n  double xValue = x[xIndex];\n  int binValue = 0;\n\n  /*\n  - For each element, calculate the fractional part of the value using the \n    remainder function (%). The value 0.25 corresponds to the fractional part\n    0.25, 0.5, 0.75, and 1.\n  - Then, check if the fractional part is between 0 and 0.25, 0.25 and 0.5, \n    0.5 and 0.75, or 0.75 and 1.\n  - If the fractional part is between the bounds, increment the corresponding bin.\n  */\n  double fractionalPart = fmod(xValue, 1);\n\n  if (fractionalPart >= 0 && fractionalPart < 0.25) {\n    binValue = 1;\n  }\n  else if (fractionalPart >= 0.25 && fractionalPart < 0.5) {\n    binValue = 2;\n  }\n  else if (fractionalPart >= 0.5 && fractionalPart < 0.75) {\n    binValue = 3;\n  }\n  else if (fractionalPart >= 0.75 && fractionalPart < 1) {\n    binValue = 4;\n  }\n\n  atomicAdd(bins + binIndex, binValue);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  // Each thread of the block is assigned one element of x\n  if (tid < N) {\n\n    double frac = x[tid] - floor(x[tid]);\n\n    size_t bin = 0;\n    if (frac < 0.25) bin = 0;\n    else if (frac < 0.50) bin = 1;\n    else if (frac < 0.75) bin = 2;\n    else bin = 3;\n\n    atomicAdd(&bins[bin], 1);\n\n  }\n\n}",
            "// YOUR CODE HERE\n\n  unsigned int index = threadIdx.x + blockIdx.x * blockDim.x;\n  int count = 0;\n  if(index < N) {\n    double fractional = fmod(x[index], 1);\n    if(fractional >= 0 && fractional < 0.25) count = 0;\n    else if(fractional >= 0.25 && fractional < 0.5) count = 1;\n    else if(fractional >= 0.5 && fractional < 0.75) count = 2;\n    else if(fractional >= 0.75 && fractional < 1) count = 3;\n\n    atomicAdd(&bins[count], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // 1. Write code here to count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    // Store the counts in `bins`. Use modulus and division to do this in parallel.\n    // You may not use a for loop or any other loop.\n    // You can use a conditional statement.\n    // You may use the floor function.\n    // For example, if x[tid] = 1.4, bins[0] will be incremented by 1.\n    // You can assume that 0 <= x[tid] <= 1.\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  const double frac = x[tid] - floor(x[tid]);\n  const size_t bucket = 4 * frac;\n  atomicAdd(&bins[bucket], 1);\n}",
            "int tid = threadIdx.x;\n   int nt = blockDim.x;\n   int i;\n   for (i = tid; i < N; i += nt) {\n      double val = x[i];\n      int frac = (int)(val - (int)val)*4;\n      atomicAdd(&bins[frac], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t offset = 4 * i;\n\n  if (i >= N/4) {\n    return;\n  }\n\n  // TODO: replace the code below with the appropriate code.\n  if(x[i] < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (x[i] >= 0.25 && x[i] < 0.50) {\n    atomicAdd(&bins[1], 1);\n  } else if (x[i] >= 0.50 && x[i] < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (x[i] >= 0.75 && x[i] < 1.00) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "//...\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   if (i < N) {\n     // add one to the appropriate bin (if the fractional part is between two values, add to the higher value)\n     bins[0] += (x[i] - floor(x[i]) < 0.25);\n     bins[1] += (x[i] - floor(x[i]) < 0.50) && (x[i] - floor(x[i]) >= 0.25);\n     bins[2] += (x[i] - floor(x[i]) < 0.75) && (x[i] - floor(x[i]) >= 0.50);\n     bins[3] += (x[i] - floor(x[i]) < 1.00) && (x[i] - floor(x[i]) >= 0.75);\n   }\n}",
            "/* YOUR CODE HERE */\n  __shared__ size_t sdata[THREADS_PER_BLOCK];\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n  int tid = threadIdx.x;\n  sdata[tid] = 0;\n\n  if (index < N) {\n    int xId = index % 4;\n    double xVal = x[index];\n    if (xVal >= 0.0 && xVal < 0.25) {\n      atomicAdd(&sdata[0], 1);\n    } else if (xVal >= 0.25 && xVal < 0.5) {\n      atomicAdd(&sdata[1], 1);\n    } else if (xVal >= 0.5 && xVal < 0.75) {\n      atomicAdd(&sdata[2], 1);\n    } else if (xVal >= 0.75 && xVal < 1.0) {\n      atomicAdd(&sdata[3], 1);\n    }\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], sdata[0]);\n    atomicAdd(&bins[1], sdata[1]);\n    atomicAdd(&bins[2], sdata[2]);\n    atomicAdd(&bins[3], sdata[3]);\n  }\n}",
            "}",
            "// Find the index into the x vector of the input values that this thread will handle.\n    // This is the index of the current thread (block.x + thread.x).\n    // If there are more threads than values in the vector, not all threads will have work to do.\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Only do work if the current thread is assigned to a value in the x vector.\n    if (i < N) {\n        // TODO: Find which bin the value in x[i] belongs to and increment the appropriate bin in bins.\n\n        // TODO: This is where you will need to use the `fmod()` function from <cmath> to determine\n        // the fractional part of `x[i]` and find out which bin it belongs in.\n\n        // TODO: Use the atomicAdd() function from <hip/hip_runtime.h> to increment the appropriate bin\n        // in `bins`.\n        //\n        // Note: atomicAdd() is a special function that is only available when compiling with nvcc.\n        // It will add the value of `bin` to `bins[i]`. It should be used in place of `bins[i]++`.\n        //\n        // The syntax is:\n        //     atomicAdd(&bins[i], bin);\n        //\n        // atomicAdd() is atomic in the sense that it is thread safe and can be used in parallel\n        // with other atomicAdd()s, but it is *not* atomic in the sense that if two threads both\n        // call atomicAdd() on the same variable, the result will be the same as if one thread\n        // called atomicAdd() on the variable and the other thread called `bins[i]++`. This is\n        // because atomicAdd() is not an atomic increment, it is an atomic add.\n        //\n        // See: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions\n    }\n}",
            "// Your code here\n}",
            "const size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        const double f = x[tid] - floor(x[tid]);\n        if (f < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (f < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (f < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO\n}",
            "}",
            "}",
            "const int i = blockIdx.x*blockDim.x + threadIdx.x;\n  const double quarter = 0.25;\n\n  if(i < N) {\n    const double y = x[i];\n    const int firstBin = static_cast<int>(y*4);\n    const int secondBin = firstBin < 3? firstBin + 1 : 3;\n    atomicAdd(bins + firstBin, 1);\n    atomicAdd(bins + secondBin, 1);\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n  double frac = fmod(x[i], 1);\n  atomicAdd(&bins[0], frac >= 0 && frac <= 0.25);\n  atomicAdd(&bins[1], frac >  0.25 && frac <= 0.50);\n  atomicAdd(&bins[2], frac >  0.50 && frac <= 0.75);\n  atomicAdd(&bins[3], frac >  0.75 && frac <= 1.00);\n}",
            "/* You must use AMD HIP to implement the kernel, including calling \n     hipThreadIdx_x(), hipBlockDim_x(), and hipBlockIdx_x().\n\n     The variables N and bins must be passed into the kernel\n     from the calling routine.\n\n     The kernel must do a parallel reduction on x to compute the\n     counts in `bins` for the fractional parts in [0, 0.25),\n     [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n\n     See the description of the `bins` array in `countQuartiles()`\n     in quartiles.cpp.\n\n     Note that `bins` is a size_t[4] array. Each thread must compute\n     the count of the fractional part for the relevant quartile and\n     write the result into `bins[quartile]`.\n  */\n  \n  size_t thread_id = hipThreadIdx_x();\n  size_t thread_num = hipBlockDim_x();\n  size_t grid_id = hipBlockIdx_x();\n  size_t i;\n\n  size_t total_size = thread_num*grid_id + thread_id;\n  int quartile;\n  double fraction;\n  \n  for(i = 0; i < N; i += thread_num*grid_id + thread_num) {\n    fraction = x[i] - floor(x[i]);\n    if(fraction >= 0 && fraction < 0.25) {\n      quartile = 0;\n    } else if (fraction >= 0.25 && fraction < 0.5) {\n      quartile = 1;\n    } else if (fraction >= 0.5 && fraction < 0.75) {\n      quartile = 2;\n    } else if (fraction >= 0.75 && fraction < 1) {\n      quartile = 3;\n    } else {\n      quartile = -1;\n    }\n\n    // Use atomicAdd to add 1 to the corresponding element\n    atomicAdd(&(bins[quartile]), 1);\n  }\n}",
            "int thread = hipThreadIdx_x;\n\n  if (thread < N) {\n    double fractional = x[thread] - floor(x[thread]);\n    int bin = fractional * 4;\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  //...\n}",
            "}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25) atomicAdd(&bins[0], 1);\n    else if (frac < 0.5) atomicAdd(&bins[1], 1);\n    else if (frac < 0.75) atomicAdd(&bins[2], 1);\n    else atomicAdd(&bins[3], 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if(i >= N) return;\n  size_t quartile = floor(4 * (x[i] - floor(x[i]))) + 1;\n  atomicAdd(&bins[quartile], 1);\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i >= N) {\n    return;\n  }\n\n  // Insert your code here\n  float fractional = (float)x[i] - (float)floor(x[i]);\n  int bin = floor(fractional * 4.0f);\n  atomicAdd(&bins[bin], 1);\n}",
            "__shared__ double *s_x;\n    if (threadIdx.x == 0) {\n        s_x = x;\n    }\n    __syncthreads();\n    if (threadIdx.x < N) {\n        int q = (int) ((s_x[threadIdx.x] - floor(s_x[threadIdx.x])) / 0.25);\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  const size_t x4 = 4 * i;\n\n  if (i < N / 4) {\n    // get the fractional part of x[4*i]\n    const double x0 = x[x4];\n    const double x1 = x[x4 + 1];\n    const double x2 = x[x4 + 2];\n    const double x3 = x[x4 + 3];\n\n    const double f0 = x0 - floor(x0);\n    const double f1 = x1 - floor(x1);\n    const double f2 = x2 - floor(x2);\n    const double f3 = x3 - floor(x3);\n\n    if (f0 <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    if (f0 > 0.25 && f0 <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    if (f0 > 0.5 && f0 <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    if (f0 > 0.75 && f0 <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n\n    if (f1 <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    if (f1 > 0.25 && f1 <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    if (f1 > 0.5 && f1 <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    if (f1 > 0.75 && f1 <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n\n    if (f2 <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    if (f2 > 0.25 && f2 <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    if (f2 > 0.5 && f2 <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    if (f2 > 0.75 && f2 <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n\n    if (f3 <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    if (f3 > 0.25 && f3 <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    if (f3 > 0.5 && f3 <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    if (f3 > 0.75 && f3 <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "__shared__ size_t tmp[4];\n  size_t tid = threadIdx.x;\n  tmp[0] = tmp[1] = tmp[2] = tmp[3] = 0;\n  __syncthreads();\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    // add this index to the appropriate bin\n    size_t xi = (size_t) x[i];\n    if (xi <= 3) {\n      atomicAdd(tmp + xi, 1);\n    } else {\n      atomicAdd(tmp + 3, 1);\n    }\n  }\n  __syncthreads();\n  for (size_t i = 0; i < 4; ++i) {\n    atomicAdd(bins + i, tmp[i]);\n  }\n}",
            "// Define a shared memory to communicate between the threads\n  __shared__ size_t sharedBins[4];\n\n  // Initialize bins in shared memory to 0.\n  if (threadIdx.x < 4) sharedBins[threadIdx.x] = 0;\n  __syncthreads();\n\n  // Use parallel reduction to count the number of elements in each\n  // quartile\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double v = x[i] - floor(x[i]);\n    if (v < 0.25)\n      atomicAdd(&sharedBins[0], 1);\n    else if (v < 0.5)\n      atomicAdd(&sharedBins[1], 1);\n    else if (v < 0.75)\n      atomicAdd(&sharedBins[2], 1);\n    else\n      atomicAdd(&sharedBins[3], 1);\n  }\n  __syncthreads();\n\n  // Read the summed values back into global memory\n  if (threadIdx.x < 4) atomicAdd(&bins[threadIdx.x], sharedBins[threadIdx.x]);\n}",
            "// TODO\n}",
            "const size_t stride = blockDim.x * gridDim.x;\n    const size_t threadID = blockDim.x * blockIdx.x + threadIdx.x;\n\n    size_t localBins[4] = { 0, 0, 0, 0 };\n    for (size_t i = threadID; i < N; i += stride) {\n        const double remainder = x[i] - floor(x[i]);\n        if (remainder < 0.25) {\n            ++localBins[0];\n        } else if (remainder < 0.5) {\n            ++localBins[1];\n        } else if (remainder < 0.75) {\n            ++localBins[2];\n        } else {\n            ++localBins[3];\n        }\n    }\n\n    atomicAdd(&bins[0], localBins[0]);\n    atomicAdd(&bins[1], localBins[1]);\n    atomicAdd(&bins[2], localBins[2]);\n    atomicAdd(&bins[3], localBins[3]);\n}",
            "// Your code goes here\n}",
            "int tid = blockDim.x*blockIdx.x + threadIdx.x;\n   if (tid >= N) return;\n\n   double fract = x[tid] - floor(x[tid]);\n   if (fract < 0.25) {\n      atomicAdd(&bins[0], 1);\n   } else if (fract >= 0.25 && fract < 0.5) {\n      atomicAdd(&bins[1], 1);\n   } else if (fract >= 0.5 && fract < 0.75) {\n      atomicAdd(&bins[2], 1);\n   } else if (fract >= 0.75 && fract <= 1.0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) return;\n  if (x[tid] - floor(x[tid]) < 0.25)\n    atomicAdd(bins+0, 1);\n  else if (x[tid] - floor(x[tid]) < 0.50)\n    atomicAdd(bins+1, 1);\n  else if (x[tid] - floor(x[tid]) < 0.75)\n    atomicAdd(bins+2, 1);\n  else\n    atomicAdd(bins+3, 1);\n}",
            "extern __shared__ size_t temp[];\n  int tx = threadIdx.x;\n  if (tx < 4) {\n    temp[tx] = 0;\n  }\n  __syncthreads();\n\n  for (size_t i = blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    double f = x[i] - floorf(x[i]);\n    atomicAdd(&temp[0], f == 0.0);\n    atomicAdd(&temp[1], f == 0.25);\n    atomicAdd(&temp[2], f == 0.5);\n    atomicAdd(&temp[3], f == 0.75);\n  }\n  __syncthreads();\n\n  if (tx < 4) {\n    atomicAdd(&bins[tx], temp[tx]);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (frac < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (frac < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        double xi = x[i];\n        int idx = 3;\n        if (xi < 1.0) {\n            idx = 0;\n        } else if (xi < 2.0) {\n            idx = 1;\n        } else if (xi < 3.0) {\n            idx = 2;\n        }\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    double xgid = x[gid];\n    if (gid < N && xgid!= 0) {\n        int bin = (xgid < 1) * (xgid < 0.25) * (xgid < 0.5) * (xgid < 0.75);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  double frac = x[i] - floor(x[i]);\n  int q;\n  if (frac < 0.25) q = 0;\n  else if (frac < 0.5) q = 1;\n  else if (frac < 0.75) q = 2;\n  else q = 3;\n  atomicAdd(&bins[q], 1);\n}",
            "// Replace this kernel by a suitable implementation.\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        const double xi = x[tid];\n        // Your code here\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double x_double = x[tid];\n    int x_int = (int)x_double;\n    double frac = x_double - x_int;\n    if (frac < 0.25 && frac >= 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac < 0.5 && frac >= 0.25) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac < 0.75 && frac >= 0.5) {\n      atomicAdd(&bins[2], 1);\n    } else if (frac < 1 && frac >= 0.75) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    int xi = int(x[i] * 4);\n    atomicAdd(&(bins[xi]), 1);\n  }\n}",
            "int id = threadIdx.x;\n    if (id >= N) return;\n    int bin = (x[id] - floor(x[id]));\n    bins[bin]++;\n}",
            "// TODO\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if(i >= N) return;\n   double f = modf(x[i], &bins[3]);\n   if(f < 0.25) bins[0]++;\n   else if(f < 0.5) bins[1]++;\n   else if(f < 0.75) bins[2]++;\n   else bins[3]++;\n}",
            "}",
            "// TODO\n}",
            "/* Your code here */\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    const double xi = x[tid];\n    const double frac = xi - floor(xi);\n    if (frac < 0.25) bins[0]++;\n    else if (frac < 0.5) bins[1]++;\n    else if (frac < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    const double x_i = x[i];\n    const double frac = x_i - floor(x_i);\n    if (frac <= 0.25)\n        atomicAdd(&bins[0], 1);\n    else if (frac <= 0.5)\n        atomicAdd(&bins[1], 1);\n    else if (frac <= 0.75)\n        atomicAdd(&bins[2], 1);\n    else\n        atomicAdd(&bins[3], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    // TODO: Implement this function\n    int index = (int)((x[i] - 0.0) / (1.0 / 4.0));\n    if (index < 0 || index > 3)\n        printf(\"ERROR: index=%d\\n\", index);\n\n    atomicAdd(&bins[index], 1);\n}",
            "// TODO: Your code here\n   // We have to use size_t to avoid underflow when N is smaller than 2\n   size_t tid = threadIdx.x;\n   size_t gid = blockIdx.x * blockDim.x + tid;\n   int bin = 0;\n\n   while (gid < N) {\n      double num = x[gid];\n\n      // Handle case where num > 1.0\n      if (num > 1.0) {\n         bin = 3;\n      }\n      // Handle case where num < 0.0\n      else if (num < 0.0) {\n         bin = 0;\n      }\n      // Handle case where 0.0 <= num <= 0.25\n      else if (num >= 0.0 && num < 0.25) {\n         bin = 0;\n      }\n      // Handle case where 0.25 <= num <= 0.5\n      else if (num >= 0.25 && num < 0.5) {\n         bin = 1;\n      }\n      // Handle case where 0.5 <= num <= 0.75\n      else if (num >= 0.5 && num < 0.75) {\n         bin = 2;\n      }\n      // Handle case where 0.75 <= num <= 1.0\n      else if (num >= 0.75 && num <= 1.0) {\n         bin = 3;\n      }\n\n      atomicAdd(bins + bin, 1);\n\n      gid += blockDim.x * gridDim.x;\n   }\n}",
            "__shared__ size_t partialBins[4];\n\n    // each thread in the block computes a separate element in bins\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int bin = 0;\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bin = 0;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bin = 1;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n        atomicAdd(&partialBins[bin], 1);\n    }\n\n    __syncthreads();\n\n    // each thread in the block accumulates its partial bin counts into bins\n    if (threadIdx.x < 4) {\n        atomicAdd(&bins[threadIdx.x], partialBins[threadIdx.x]);\n    }\n}",
            "int index = threadIdx.x + blockIdx.x*blockDim.x;\n  if (index >= N) return;\n\n  const double xi = x[index];\n  if (xi >= 0 && xi < 0.25) bins[0]++;\n  else if (xi >= 0.25 && xi < 0.5) bins[1]++;\n  else if (xi >= 0.5 && xi < 0.75) bins[2]++;\n  else if (xi >= 0.75 && xi < 1) bins[3]++;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double frac = x[tid] - floor(x[tid]);\n        if (frac < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (frac < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (frac < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "int gid = blockDim.x * blockIdx.x + threadIdx.x;\n  double xval;\n  // For each element in x, increment the corresponding bin\n  if (gid < N) {\n    xval = x[gid];\n    // First, get the fractional part of the number\n    xval -= (int)(xval);\n    // Now, determine which bin to increment\n    // 0.0 <= x < 0.25\n    if (xval <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    // 0.25 <= x < 0.5\n    else if (xval <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    // 0.5 <= x < 0.75\n    else if (xval <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    // 0.75 <= x < 1.0\n    else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N) return;\n  double d = x[tid];\n  double dd = d - floor(d);\n  if (dd >= 0.0 && dd <= 0.25)\n    atomicAdd(&bins[0], 1);\n  else if (dd > 0.25 && dd <= 0.5)\n    atomicAdd(&bins[1], 1);\n  else if (dd > 0.5 && dd <= 0.75)\n    atomicAdd(&bins[2], 1);\n  else if (dd > 0.75 && dd <= 1.0)\n    atomicAdd(&bins[3], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double d = x[i] - floor(x[i]);\n        if (d < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (d < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (d < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int i = threadIdx.x;\n  int sum = 0;\n  while (i < N) {\n    int quarter = x[i] * 4;\n    int bin = quarter % 4;\n    int quarter_floor = quarter / 4;\n    if (quarter_floor == 0) sum++;\n    else if (quarter_floor == 1) sum++;\n    else if (quarter_floor == 2) sum++;\n    else if (quarter_floor == 3) sum++;\n    i += blockDim.x;\n  }\n  atomicAdd(&bins[0], sum);\n}",
            "// TODO\n}",
            "// TODO\n  unsigned int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n  if(i < N){\n    if(x[i] < 0.25 && x[i] >= 0) bins[0]++;\n    else if(x[i] < 0.5 && x[i] >= 0.25) bins[1]++;\n    else if(x[i] < 0.75 && x[i] >= 0.5) bins[2]++;\n    else if(x[i] < 1 && x[i] >= 0.75) bins[3]++;\n  }\n}",
            "// TODO\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    double frac = fmod(x[tid], 1);\n    if (frac >= 0 && frac < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (frac >= 0.25 && frac < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (frac >= 0.5 && frac < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (frac >= 0.75 && frac < 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (tid >= N) return;\n   if (x[tid] >= 0 && x[tid] < 0.25) atomicAdd(&bins[0], 1);\n   if (x[tid] >= 0.25 && x[tid] < 0.50) atomicAdd(&bins[1], 1);\n   if (x[tid] >= 0.50 && x[tid] < 0.75) atomicAdd(&bins[2], 1);\n   if (x[tid] >= 0.75 && x[tid] < 1.0) atomicAdd(&bins[3], 1);\n}",
            "const size_t tid = threadIdx.x;\n   const size_t blockDim = gridDim.x * blockDim.x;\n\n   __shared__ double min;\n   __shared__ double max;\n   double val;\n   size_t bin;\n\n   if (tid == 0) {\n      min = HIP_INFINITY;\n      max = -HIP_INFINITY;\n   }\n\n   // Find min and max\n   for (size_t i = tid; i < N; i += blockDim) {\n      val = x[i];\n      min = fmin(min, val);\n      max = fmax(max, val);\n   }\n\n   __syncthreads();\n\n   // Compute bin counts\n   if (tid < 4) {\n      double binValue;\n      if (tid == 0) {\n         binValue = floor(min);\n      } else if (tid == 1) {\n         binValue = floor(min + 0.25);\n      } else if (tid == 2) {\n         binValue = floor(min + 0.5);\n      } else {\n         binValue = floor(min + 0.75);\n      }\n\n      bins[tid] = 0;\n      for (size_t i = tid; i < N; i += 4) {\n         val = x[i];\n         bin = val > binValue? 1 : 0;\n         bin += val > binValue + 0.25? 2 : 0;\n         bin += val > binValue + 0.5? 3 : 0;\n         atomicAdd(&bins[bin], 1);\n      }\n   }\n}",
            "// Your code here\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    // 1. Fill in your code here to count the number of doubles in the\n    //    input vector `x` that fall in each of the four quartiles.\n    //    You can use the function `fmodf` to determine whether a double\n    //    is in the first, second, third, or fourth quartile.\n    //    Use a single atomicAdd instruction for each quartile.\n    //    You can use any integer values for the bins, but make sure to\n    //    convert them back to float when calling this function.\n    if (tid < N) {\n        double quartile = fmodf(x[tid], 4);\n        if (quartile == 0 || quartile == 4) {\n            atomicAdd(bins + 0, 1);\n        } else if (quartile == 1 || quartile == 5) {\n            atomicAdd(bins + 1, 1);\n        } else if (quartile == 2 || quartile == 6) {\n            atomicAdd(bins + 2, 1);\n        } else {\n            atomicAdd(bins + 3, 1);\n        }\n    }\n}",
            "/* Each thread will process an element of `x` */\n   int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid >= N) return;\n\n   int which = 0;\n   if (x[tid] >= 0.0 && x[tid] < 0.25) which = 0;\n   else if (x[tid] >= 0.25 && x[tid] < 0.50) which = 1;\n   else if (x[tid] >= 0.50 && x[tid] < 0.75) which = 2;\n   else if (x[tid] >= 0.75 && x[tid] < 1.00) which = 3;\n\n   atomicAdd(bins + which, 1);\n}",
            "const int tid = threadIdx.x;\n  extern __shared__ int bins_s[];\n  size_t sum = 0;\n  for (size_t i = tid; i < 4; i += blockDim.x)\n    bins_s[i] = 0;\n  __syncthreads();\n\n  double val = x[tid];\n  val -= floor(val);\n  int bin = (int) (val * 4.0);\n  if (bin < 4)\n    atomicAdd(&bins_s[bin], 1);\n  __syncthreads();\n\n  for (int i = 0; i < 4; i++)\n    sum += bins_s[i];\n  if (tid == 0)\n    atomicAdd(&bins[0], sum);\n}",
            "const size_t globalIndex = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (globalIndex < N) {\n        int q = static_cast<int>(x[globalIndex] * 4);\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "// Determine the thread ID\n  unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Do nothing if the thread ID exceeds the length of the vector\n  if (tid >= N) return;\n\n  // Load the value of x to the register.\n  // (Note: the following operations are all done in the register)\n  double xx = x[tid];\n\n  // Determine the fractional part of the value of x\n  double frac = xx - floor(xx);\n\n  // Determine the fractional part of the value of x\n  // The four fractional parts are [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n  if (frac >= 0.75) atomicAdd(&bins[0], 1); // [0.75, 1)\n  else if (frac >= 0.5) atomicAdd(&bins[1], 1); // [0.5, 0.75)\n  else if (frac >= 0.25) atomicAdd(&bins[2], 1); // [0.25, 0.5)\n  else atomicAdd(&bins[3], 1); // [0, 0.25)\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    // TODO\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    double dx = x[tid] - floor(x[tid]);\n    if (dx < 0.25)\n      bins[0]++;\n    else if (dx < 0.5)\n      bins[1]++;\n    else if (dx < 0.75)\n      bins[2]++;\n    else\n      bins[3]++;\n  }\n}",
            "}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t v = x[i] - floor(x[i]);\n        atomicAdd(&bins[v], 1);\n    }\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    const double quarter = 0.25;\n    if (i < N) {\n        const double xi = x[i];\n        const double frac = xi - floor(xi);\n        // use xi to compute the fractional part of xi\n        if (frac < quarter) {\n            atomicAdd(&bins[0], 1);\n        } else if (frac < 2*quarter) {\n            atomicAdd(&bins[1], 1);\n        } else if (frac < 3*quarter) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  double xi = x[i];\n  // TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      double v = x[i] - floor(x[i]);\n      if (v < 0.25) atomicAdd(&bins[0], 1);\n      else if (v < 0.5) atomicAdd(&bins[1], 1);\n      else if (v < 0.75) atomicAdd(&bins[2], 1);\n      else atomicAdd(&bins[3], 1);\n   }\n}",
            "unsigned int i = threadIdx.x;\n  unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n  while (tid < N) {\n    if (x[tid] - floor(x[tid]) <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[tid] - floor(x[tid]) <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[tid] - floor(x[tid]) <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n    tid += stride;\n  }\n}",
            "// TODO: Implement in parallel using a reduction\n}",
            "// TODO\n}",
            "// The value of threadIdx.x is within [0, N).\n  size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i < N) {\n\n    // Your code here\n\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    int q = floor(x[i] * 4) + 1;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double d = x[tid];\n    d -= floor(d);\n    if (d <= 0.25) bins[0]++;\n    else if (d <= 0.5) bins[1]++;\n    else if (d <= 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    int index = ((int) (x[i] * 4)) % 4;\n    atomicAdd(&bins[index], 1);\n  }\n\n}",
            "/* Your code here */\n\n}",
            "// The thread index\n  const int i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i < N) {\n    // Compute the fractional part of x[i]\n    const double frac = x[i] - (int) x[i];\n\n    // Determine which bin frac falls into\n    const int bin = frac < 0.25? 0 : (frac < 0.5? 1 : (frac < 0.75? 2 : 3));\n\n    // Use atomic operations to increment the corresponding bin\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  //...\n}",
            "__shared__ size_t partial[256];\n   auto start = N * hipThreadIdx_x / hipBlockDim_x;\n   auto end = N * (hipThreadIdx_x + 1) / hipBlockDim_x;\n\n   partial[hipThreadIdx_x] = 0;\n   for (auto i = start; i < end; ++i) {\n      auto f = frac(x[i]);\n      if (f >= 0 && f < 0.25)\n         ++partial[hipThreadIdx_x];\n      else if (f >= 0.25 && f < 0.5)\n         ++partial[hipThreadIdx_x + 128];\n      else if (f >= 0.5 && f < 0.75)\n         ++partial[hipThreadIdx_x + 256];\n      else if (f >= 0.75 && f < 1)\n         ++partial[hipThreadIdx_x + 384];\n   }\n   __syncthreads();\n\n   if (hipThreadIdx_x < 128) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 128];\n   }\n   if (hipThreadIdx_x < 64) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 64];\n   }\n   if (hipThreadIdx_x < 32) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 32];\n   }\n   if (hipThreadIdx_x < 16) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 16];\n   }\n   if (hipThreadIdx_x < 8) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 8];\n   }\n   if (hipThreadIdx_x < 4) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 4];\n   }\n   if (hipThreadIdx_x < 2) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 2];\n   }\n   if (hipThreadIdx_x == 0) {\n      auto i = hipThreadIdx_x;\n      partial[i] += partial[i + 1];\n   }\n\n   if (hipThreadIdx_x == 0) {\n      bins[0] = partial[0];\n      bins[1] = partial[1];\n      bins[2] = partial[2];\n      bins[3] = partial[3];\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  // Do nothing if `i` is out of bounds\n  if (i >= N) return;\n\n  // Count the double in `x[i]` in the corresponding bin\n  double xi = x[i];\n  size_t remainder = fmod(xi, 1.0);\n  if (remainder < 0.25) bins[0]++;\n  else if (remainder < 0.5) bins[1]++;\n  else if (remainder < 0.75) bins[2]++;\n  else bins[3]++;\n}",
            "/* Your code here */\n}",
            "// TODO\n}",
            "/* \n    YOUR CODE HERE\n    */\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n   // if all threads in the block are out of bounds, return\n   if (tid >= N) return;\n\n   double fraction = x[tid] - floor(x[tid]);\n   atomicAdd(&bins[0], (fraction > 0 && fraction <= 0.25));\n   atomicAdd(&bins[1], (fraction > 0.25 && fraction <= 0.5));\n   atomicAdd(&bins[2], (fraction > 0.5 && fraction <= 0.75));\n   atomicAdd(&bins[3], (fraction > 0.75 && fraction <= 1));\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  double frac = x[i] - floor(x[i]);\n  if (frac >= 0.25 && frac < 0.5)\n    atomicAdd(&bins[0], 1);\n  else if (frac >= 0.5 && frac < 0.75)\n    atomicAdd(&bins[1], 1);\n  else if (frac >= 0.75 && frac < 1.0)\n    atomicAdd(&bins[2], 1);\n  else\n    atomicAdd(&bins[3], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  // TODO\n}",
            "// TODO: Count the number of doubles in the vector x that have a fractional\n  //       part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n  //       Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n  // Hint: Use atomic increments.\n  //\n  // Note: Do not call any HIP runtime API during the kernel.\n}",
            "// Set each thread to work on an element of x.\n  size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Initialize `bins` to 0\n  if (threadIdx.x == 0) {\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n  }\n  __syncthreads();\n\n  // Load x[i] to register\n  double x_i = x[i];\n\n  // Compute the fractional part of x[i]\n  double frac = x_i - floor(x_i);\n\n  // Count the number of elements in each fractional bin\n  if (frac < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (frac < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (frac < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   if (x[i] >= 0 && x[i] < 0.25) atomicAdd(&bins[0], 1);\n   else if (x[i] >= 0.25 && x[i] < 0.5) atomicAdd(&bins[1], 1);\n   else if (x[i] >= 0.5 && x[i] < 0.75) atomicAdd(&bins[2], 1);\n   else if (x[i] >= 0.75 && x[i] < 1) atomicAdd(&bins[3], 1);\n}",
            "// TODO: Determine thread ID\n    size_t tid =?;\n\n    // TODO: Count fractional parts in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    //       You will need to use a branch statement.\n    //       Hint: Do you need a loop?\n    if (x[tid] >= 0.0 && x[tid] < 0.25) {\n        // TODO: Increment appropriate bin\n       ?\n    }\n    else if (x[tid] >= 0.25 && x[tid] < 0.5) {\n       ?\n    }\n    else if (x[tid] >= 0.5 && x[tid] < 0.75) {\n       ?\n    }\n    else {\n       ?\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int rank, size;\n  int i, n, r;\n  int m, m_proc;\n  int my_bins[4];\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  n = x.size();\n  m_proc = n / size;\n  r = n % size;\n  i = 0;\n\n  if (rank == 0) {\n    for (int j = 0; j < 4; j++) my_bins[j] = 0;\n  }\n\n  if (rank == 0) {\n    m = m_proc + r;\n  } else {\n    m = m_proc;\n  }\n\n  for (i = 0; i < m; i++) {\n    if (x[i] - floor(x[i]) < 0.25) {\n      my_bins[0]++;\n    } else if (x[i] - floor(x[i]) < 0.5) {\n      my_bins[1]++;\n    } else if (x[i] - floor(x[i]) < 0.75) {\n      my_bins[2]++;\n    } else {\n      my_bins[3]++;\n    }\n  }\n\n  if (rank == 0) {\n    for (int j = 0; j < 4; j++) bins[j] = 0;\n  }\n\n  MPI_Reduce(my_bins, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// Replace this statement with your implementation.\n    // Make sure to use MPI calls to divide the input among the ranks.\n    // Your code should work even if x.size() is not divisible by the number of ranks.\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Status status;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int count;\n    MPI_Get_count(&status, MPI_DOUBLE, &count);\n    MPI_Scatter(x.data(), count, MPI_DOUBLE, &x[0], count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    bins = {0, 0, 0, 0};\n    for (size_t i = 0; i < x.size(); i++) {\n        int index = (int)((x[i] - std::floor(x[i])) / 0.25);\n        if (index > 3)\n            index = 3;\n        bins[index]++;\n    }\n    MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<double> chunk;\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n  if(rank == 0){\n    chunk.assign(x.begin(), x.end());\n    int n = chunk.size() / size;\n    int m = chunk.size() % size;\n    int start = 0;\n    for (int i = 1; i < size; ++i) {\n      int end = start + n;\n      if(m!= 0){\n        end += 1;\n        m -= 1;\n      }\n      MPI_Send(&chunk[start], end-start, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      start = end;\n    }\n  }else{\n    int n = x.size() / size;\n    int m = x.size() % size;\n    int start = 0;\n    if(rank-1 < m){\n      start = rank-1;\n      MPI_Recv(&chunk[0], n+1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }else{\n      start = m + (rank-1-m)*(n+1);\n      MPI_Recv(&chunk[0], n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n\n  for(double x: chunk){\n    int idx = std::floor(x*4);\n    if(idx >= 0 && idx <= 3){\n      local_bins[idx] += 1;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n    if (x.size() == 0)\n        return;\n    if (x.size() == 1) {\n        if (x[0] < 0.25)\n            ++bins[0];\n        else if (x[0] < 0.5)\n            ++bins[1];\n        else if (x[0] < 0.75)\n            ++bins[2];\n        else if (x[0] < 1)\n            ++bins[3];\n    }\n    else {\n        auto half_size = x.size() / 2;\n        auto left_part = std::vector<double>(x.begin(), x.begin() + half_size);\n        auto right_part = std::vector<double>(x.begin() + half_size, x.end());\n        std::array<size_t, 4> left_bins;\n        std::array<size_t, 4> right_bins;\n        countQuartiles(left_part, left_bins);\n        countQuartiles(right_part, right_bins);\n        MPI_Reduce(&left_bins, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&right_bins, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        if (x.size() % 2) {\n            auto value = x[half_size];\n            if (value < 0.25)\n                ++bins[0];\n            else if (value < 0.5)\n                ++bins[1];\n            else if (value < 0.75)\n                ++bins[2];\n            else if (value < 1)\n                ++bins[3];\n        }\n    }\n}",
            "// TODO\n  // hint: you could just use std::accumulate (from <numeric>) with a lambda as \n  // the binary operation.\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int remainder = x.size() % size;\n  int chunkSize = x.size() / size + (rank < remainder? 1 : 0);\n  int startIndex = rank * chunkSize;\n  if (rank == 0) {\n    // initialize bins to zero on rank 0\n    for (size_t i = 0; i < bins.size(); i++) {\n      bins[i] = 0;\n    }\n  }\n  // use a scratch space to avoid having to do a bunch of conditional adds\n  std::array<size_t, 4> counts = {{0, 0, 0, 0}};\n  for (int i = startIndex; i < startIndex + chunkSize; i++) {\n    double value = x[i] - static_cast<int>(x[i]);\n    if (value < 0.25) {\n      counts[0]++;\n    } else if (value < 0.5) {\n      counts[1]++;\n    } else if (value < 0.75) {\n      counts[2]++;\n    } else {\n      counts[3]++;\n    }\n  }\n  // sum counts across all ranks\n  MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Your code here!\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<size_t> counts(4, 0);\n    counts[0] = std::count_if(x.begin(), x.end(), [](double i){ return i >= 0 && i < 0.25; });\n    counts[1] = std::count_if(x.begin(), x.end(), [](double i){ return i >= 0.25 && i < 0.5; });\n    counts[2] = std::count_if(x.begin(), x.end(), [](double i){ return i >= 0.5 && i < 0.75; });\n    counts[3] = std::count_if(x.begin(), x.end(), [](double i){ return i >= 0.75 && i <= 1; });\n\n    if (rank == 0) {\n        std::vector<size_t> all_counts(size * counts.size());\n        MPI_Gather(counts.data(), counts.size(), MPI_UNSIGNED_LONG, all_counts.data(), counts.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        std::copy(all_counts.begin(), all_counts.begin() + 4, bins.begin());\n    } else {\n        MPI_Gather(counts.data(), counts.size(), MPI_UNSIGNED_LONG, NULL, counts.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int rank, nranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n\n  // YOUR CODE HERE\n}",
            "// Your code goes here.\n\n}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<double, 4> localBins{};\n\n    if (rank == 0) {\n        // Count the number of doubles in the vector x that have a fractional part \n        // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n        // You may assume that the first element of x is a double in [0, 1).\n        int localSize = x.size() / size;\n        for (int i = 0; i < localSize; i++) {\n            if (x[i] - floor(x[i]) < 0.25)\n                localBins[0]++;\n            else if (x[i] - floor(x[i]) < 0.5)\n                localBins[1]++;\n            else if (x[i] - floor(x[i]) < 0.75)\n                localBins[2]++;\n            else\n                localBins[3]++;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO\n}",
            "const int num_ranks = mpi.world.size();\n  const int my_rank = mpi.world.rank();\n  const size_t x_per_rank = x.size() / num_ranks;\n  const size_t start_ind = my_rank * x_per_rank;\n  const size_t end_ind = start_ind + x_per_rank;\n  const size_t size = x.size();\n  std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n\n  for (size_t i = start_ind; i < end_ind; ++i) {\n    const double xi = x[i];\n    const double frac = xi - std::floor(xi);\n    if (0. <= frac && frac < 0.25) {\n      local_bins[0] += 1;\n    }\n    else if (0.25 <= frac && frac < 0.5) {\n      local_bins[1] += 1;\n    }\n    else if (0.5 <= frac && frac < 0.75) {\n      local_bins[2] += 1;\n    }\n    else if (0.75 <= frac && frac < 1) {\n      local_bins[3] += 1;\n    }\n  }\n\n  std::array<size_t, 4> all_bins;\n  if (my_rank == 0) {\n    all_bins = local_bins;\n    mpi.world.reduce_in_place(0, [](std::array<size_t, 4> &b1, std::array<size_t, 4> b2) {\n      for (int i = 0; i < 4; ++i) {\n        b1[i] += b2[i];\n      }\n    }, local_bins);\n  }\n  else {\n    mpi.world.reduce_in_place(0, [](std::array<size_t, 4> &b1, std::array<size_t, 4> b2) {\n      for (int i = 0; i < 4; ++i) {\n        b1[i] += b2[i];\n      }\n    }, local_bins);\n  }\n\n  if (my_rank == 0) {\n    bins = all_bins;\n  }\n}",
            "size_t size = x.size();\n    size_t rank;\n    size_t nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    int n = size / nprocs;\n    int remain = size % nprocs;\n    int left = 0;\n    int right = 0;\n    if (remain!= 0) {\n        if (rank < remain) {\n            n = n + 1;\n        }\n        left = rank * n;\n        if (rank < remain) {\n            right = left + n;\n        } else {\n            right = left + n - 1;\n        }\n    } else {\n        left = rank * n;\n        right = left + n - 1;\n    }\n\n    std::array<size_t, 4> loc_bins = { 0, 0, 0, 0 };\n\n    for (int i = left; i <= right; i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            loc_bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            loc_bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            loc_bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1.0) {\n            loc_bins[3]++;\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < nprocs; i++) {\n            MPI_Status stat;\n            int proc;\n            int count[4];\n            MPI_Probe(i, 0, MPI_COMM_WORLD, &stat);\n            MPI_Get_count(&stat, MPI_INT, &proc);\n            MPI_Recv(count, proc, MPI_INT, i, 0, MPI_COMM_WORLD, &stat);\n            bins[0] += count[0];\n            bins[1] += count[1];\n            bins[2] += count[2];\n            bins[3] += count[3];\n        }\n    } else {\n        MPI_Send(loc_bins.data(), 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// your code here\n}",
            "// YOUR CODE HERE\n  MPI_Status status;\n  MPI_Request request;\n  size_t local_size = x.size();\n  size_t size, rank;\n  std::array<size_t, 4> local_bins{};\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    std::vector<std::array<size_t, 4>> bins_per_rank(size - 1);\n    for (size_t i = 0; i < size - 1; ++i) {\n      MPI_Recv(&bins_per_rank[i], 4, MPI_UNSIGNED_LONG, i + 1, 0, MPI_COMM_WORLD, &status);\n    }\n    for (size_t i = 0; i < size - 1; ++i) {\n      for (size_t j = 0; j < 4; ++j) {\n        bins[j] += bins_per_rank[i][j];\n      }\n    }\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] += local_bins[i];\n    }\n  } else {\n    for (size_t i = 0; i < 4; ++i) {\n      local_bins[i] = std::count_if(x.begin(), x.end(), [i](double x) {\n        return std::floor(x) == i;\n      });\n    }\n    MPI_Send(&local_bins, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int mpi_rank = 0, mpi_size = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    size_t n = x.size();\n    size_t chunkSize = n / mpi_size;\n    size_t remainder = n % mpi_size;\n\n    std::vector<size_t> myChunk;\n    myChunk.resize(4, 0);\n\n    if (mpi_rank == 0) {\n        for (size_t i = 0; i < remainder; ++i) {\n            if (x[i] > 0 && x[i] < 1) ++myChunk[0];\n            if (x[i] > 0.25 && x[i] < 0.5) ++myChunk[1];\n            if (x[i] > 0.5 && x[i] < 0.75) ++myChunk[2];\n            if (x[i] > 0.75 && x[i] < 1) ++myChunk[3];\n        }\n    }\n\n    std::vector<size_t> allChunk(mpi_size * 4, 0);\n    MPI_Gather(&myChunk[0], 4, MPI_UNSIGNED_LONG_LONG,\n               &allChunk[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (mpi_rank == 0) {\n        for (size_t i = 0; i < mpi_size * 4; i += 4) {\n            bins[0] += allChunk[i];\n            bins[1] += allChunk[i + 1];\n            bins[2] += allChunk[i + 2];\n            bins[3] += allChunk[i + 3];\n        }\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  /* add your code here */\n}",
            "// Your code goes here\n  auto n = x.size();\n  if (n == 0) return;\n  std::vector<std::array<size_t, 4>> counts(n);\n  for (auto i = 0; i < n; i++) {\n    auto frac = std::modf(x[i], &x[i]);\n    if (frac < 0.25) counts[i][0]++;\n    else if (frac < 0.5) counts[i][1]++;\n    else if (frac < 0.75) counts[i][2]++;\n    else if (frac < 1) counts[i][3]++;\n  }\n  if (MPI_RANK == 0) {\n    MPI_Reduce(MPI_IN_PLACE, counts.data(), 4 * n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    for (auto i = 0; i < n; i++) {\n      bins[0] += counts[i][0];\n      bins[1] += counts[i][1];\n      bins[2] += counts[i][2];\n      bins[3] += counts[i][3];\n    }\n  }\n  else {\n    MPI_Reduce(counts.data(), nullptr, 4 * n, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size, rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if(rank == 0)\n  {\n    // The root rank will have a complete copy of x, so the loop below won't\n    // compute anything for rank 0.\n    for(int i = 1; i < size; i++)\n    {\n      // TODO: send the bin count to the rank\n    }\n  }\n  else\n  {\n    // TODO: count the number of values in x that fall into each bin\n    // then send the bin count to the root\n  }\n\n  if(rank == 0)\n  {\n    // The root rank will have a complete copy of x, so the loop below won't\n    // compute anything for rank 0.\n    for(int i = 1; i < size; i++)\n    {\n      // TODO: receive the bin count from the rank\n    }\n  }\n\n  // TODO: sum the bin counts from all ranks on the root\n}",
            "int rank, p;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    auto numPerRank = x.size() / p;\n    auto numRemainder = x.size() % p;\n    std::vector<double> localX;\n    if (rank == 0) {\n        localX.resize(numPerRank + numRemainder);\n    } else {\n        localX.resize(numPerRank);\n    }\n\n    // copy the local portion of x into localX\n    MPI_Scatter(x.data(), numPerRank, MPI_DOUBLE, localX.data(), numPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // count the local fractional parts\n    std::array<size_t, 4> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n    for (auto& x_i : localX) {\n        int i = x_i / 1.0;\n        int f = x_i - i;\n        if (f >= 0 && f < 0.25) {\n            localBins[0]++;\n        } else if (f >= 0.25 && f < 0.5) {\n            localBins[1]++;\n        } else if (f >= 0.5 && f < 0.75) {\n            localBins[2]++;\n        } else if (f >= 0.75 && f < 1) {\n            localBins[3]++;\n        }\n    }\n\n    // count the local fractional parts\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO: Implement this function!\n    // Remember that the order of the bins is\n    // [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n}",
            "int num_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t count_size = x.size()/num_procs;\n  size_t remain = x.size()%num_procs;\n\n  if (rank == 0) {\n    for (int i=1; i<num_procs; i++) {\n      MPI_Send(&x[i*count_size], count_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n    if (remain > 0) {\n      MPI_Send(&x[num_procs*count_size], remain, MPI_DOUBLE, num_procs-1, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    std::vector<double> local_x;\n    local_x.resize(count_size + (rank < remain? 1 : 0));\n    MPI_Recv(&local_x[0], count_size + (rank < remain? 1 : 0), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    // do the count\n  }\n\n  if (rank == 0) {\n    // do the count and store the result in bins\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  MPI_Gather(&bins, 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // TODO: Implement this function\n}",
            "const size_t N = x.size();\n  const size_t myrank = 0;\n  const size_t nprocs = 1;\n\n  std::array<size_t, 4> myBins = {0, 0, 0, 0};\n  for (size_t i = 0; i < N; ++i) {\n    double y = x[i] - (int)x[i];\n    if (y < 0.25) myBins[0] += 1;\n    else if (y < 0.5) myBins[1] += 1;\n    else if (y < 0.75) myBins[2] += 1;\n    else myBins[3] += 1;\n  }\n\n  if (myrank == 0) bins = myBins;\n}",
            "//TODO: Your code here\n\n}",
            "const int rank = 0;\n    const int size = 1;\n    int const localSize = x.size();\n    int const localOffset = 0;\n    int const localLowerBound = 0;\n    int const localUpperBound = localSize - 1;\n    std::vector<double> localX;\n    std::vector<double> localBins;\n    localBins = {0, 0, 0, 0};\n    localX = x;\n\n    MPI_Bcast(&localSize, 1, MPI_INT, rank, MPI_COMM_WORLD);\n    MPI_Bcast(&localOffset, 1, MPI_INT, rank, MPI_COMM_WORLD);\n    MPI_Bcast(&localLowerBound, 1, MPI_INT, rank, MPI_COMM_WORLD);\n    MPI_Bcast(&localUpperBound, 1, MPI_INT, rank, MPI_COMM_WORLD);\n    MPI_Bcast(&localX[0], localSize, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n\n    for (int i = localLowerBound; i <= localUpperBound; i++) {\n        double xi = localX[i];\n        if (xi >= 0 && xi < 0.25) {\n            localBins[0] += 1;\n        } else if (xi >= 0.25 && xi < 0.5) {\n            localBins[1] += 1;\n        } else if (xi >= 0.5 && xi < 0.75) {\n            localBins[2] += 1;\n        } else if (xi >= 0.75 && xi < 1) {\n            localBins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(&localBins[0], &bins[0], 4, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 4> local_counts = {0, 0, 0, 0};\n\n    // TODO: add code here\n\n    // TODO: combine counts across ranks\n}",
            "int world_size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Create a buffer of the same size as x, for gathering\n  std::vector<size_t> buffer(x.size());\n  // Compute the local count of each bin in buffer\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      buffer[i] = 0;\n    }\n    else if (x[i] < 0.5) {\n      buffer[i] = 1;\n    }\n    else if (x[i] < 0.75) {\n      buffer[i] = 2;\n    }\n    else {\n      buffer[i] = 3;\n    }\n  }\n\n  // Gather the results from all ranks and put them in bins\n  std::vector<size_t> send_buffer(buffer);\n  std::vector<size_t> recv_buffer(x.size());\n  MPI_Gather(send_buffer.data(), x.size(), MPI_INT, recv_buffer.data(), x.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      for (int j = 0; j < 4; j++) {\n        bins[j] += recv_buffer[i] == j;\n      }\n    }\n  }\n\n}",
            "// TODO: replace this code with your implementation\n  bins.fill(0);\n  for (auto v : x) {\n    int n = std::floor(v * 4);\n    if (n >= 0 && n < 4) {\n      bins[n]++;\n    }\n  }\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  std::array<size_t, 4> recv_bins;\n  recv_bins.fill(0);\n  if (world_size > 1) {\n    // TODO: Replace this code with your own MPI implementation\n    // You will need to replace this with an MPI call to gather the results\n    // from all ranks.\n    MPI_Gather(&bins, 4, MPI_UNSIGNED_LONG_LONG, &recv_bins, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    if (world_rank!= 0) {\n      return;\n    }\n  }\n  bins.fill(0);\n  for (auto rb : recv_bins) {\n    for (int n = 0; n < 4; n++) {\n      bins[n] += rb;\n    }\n  }\n}",
            "// TODO: Implement\n\n}",
            "// TODO\n}",
            "//TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "//TODO\n\n}",
            "int nprocs;\n    int myid;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\n    //...\n\n    MPI_Finalize();\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < bins.size(); i++) {\n            bins[i] = 0;\n        }\n    }\n\n    auto sendBins = bins;\n    auto recvBins = bins;\n\n    auto myX = x;\n    std::vector<double> sendX;\n    std::vector<double> recvX;\n\n    // 4 bins\n    int n = x.size();\n    int n_per_bin = n / 4;\n    int remain = n % 4;\n    int offset = 0;\n\n    for (int i = 0; i < 4; i++) {\n        int begin = i * n_per_bin + offset;\n        int end = begin + n_per_bin + (i == 3? remain : 0);\n\n        if (i == 0) {\n            sendX.clear();\n            for (auto it = myX.begin() + begin; it < myX.begin() + end; it++) {\n                sendX.push_back(*it);\n            }\n        }\n\n        if (i == 3 && rank == 0) {\n            recvX.clear();\n            for (auto it = myX.begin() + begin; it < myX.begin() + end; it++) {\n                recvX.push_back(*it);\n            }\n        }\n\n        if (rank == 0) {\n            MPI_Send(&sendBins[i], 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD);\n            MPI_Recv(&recvBins[i], 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        } else if (rank == i) {\n            MPI_Recv(&sendBins[i], 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (auto it = sendX.begin(); it!= sendX.end(); it++) {\n                if (*it >= 0 && *it < 0.25) {\n                    sendBins[i]++;\n                } else if (*it >= 0.25 && *it < 0.5) {\n                    sendBins[i]++;\n                } else if (*it >= 0.5 && *it < 0.75) {\n                    sendBins[i]++;\n                } else if (*it >= 0.75 && *it <= 1) {\n                    sendBins[i]++;\n                }\n            }\n            MPI_Send(&sendBins[i], 1, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            bins[i] = recvBins[i];\n        }\n    }\n}",
            "// Your code goes here\n\n}",
            "int num_procs;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    /*\n      Your code here\n    */\n}",
            "// TODO\n}",
            "int size = x.size();\n    int rank = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> subX(size);\n    std::vector<size_t> subBins(4,0);\n\n    if(rank == 0) {\n        subX.assign(x.begin(), x.begin()+size);\n    } else {\n        subX.assign(x.begin()+rank*size, x.begin()+rank*size+size);\n    }\n\n    int* counts;\n    counts = new int[size];\n    std::fill(counts, counts+size, 1);\n    int* displs;\n    displs = new int[size];\n    std::fill(displs, displs+size, 0);\n\n    MPI_Gatherv(&subX, size, MPI_DOUBLE, x, counts, displs, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        for(int i = 0; i < size; ++i) {\n            if(x[i] < 0.25) {\n                subBins[0]++;\n            } else if (x[i] < 0.5) {\n                subBins[1]++;\n            } else if (x[i] < 0.75) {\n                subBins[2]++;\n            } else {\n                subBins[3]++;\n            }\n        }\n        bins[0] = subBins[0];\n        bins[1] = subBins[1];\n        bins[2] = subBins[2];\n        bins[3] = subBins[3];\n    }\n\n    delete[] counts;\n    delete[] displs;\n}",
            "bins = {0, 0, 0, 0};\n\n    if (x.empty())\n        return;\n\n    // TODO:\n}",
            "MPI_Status status;\n    MPI_Request request;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Your code goes here\n\n    // MPI_Iprobe(source, tag, comm, &status);\n\n    // MPI_Recv(status, count, datatype, source, tag, comm, &status);\n\n    // MPI_Irecv(buffer, count, datatype, source, tag, comm, &request);\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::array<size_t, 4> counts = {0};\n\n  // TODO: Compute the counts in counts\n\n  MPI_Gather(counts.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "/* YOUR CODE HERE */\n}",
            "// Compute the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    // Note: you may want to use a function that is available in <cmath>.\n\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "int size, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        for (auto &elem: bins) {\n            elem = 0;\n        }\n    }\n\n    // 1) Divide the vector in size/2 parts\n    size_t n = x.size();\n    size_t n_per_rank = n / size;\n    size_t n_remain = n % size;\n\n    // 2) Send to the right\n    for (size_t i = 0; i < size - 1; i++) {\n        MPI_Send(&x[n_per_rank * (i + 1)], n_per_rank + (i < n_remain? 1 : 0), MPI_DOUBLE, i + 1, 0, MPI_COMM_WORLD);\n    }\n\n    // 3) Receive from the left\n    std::vector<double> tmp(n_per_rank + (rank < n_remain? 1 : 0));\n    for (size_t i = 0; i < size - 1; i++) {\n        MPI_Recv(&tmp[0], n_per_rank + (i < n_remain? 1 : 0), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        // 4) Count bins\n        for (auto &elem: tmp) {\n            if (elem < 0.25) {\n                bins[0]++;\n            } else if (elem < 0.5) {\n                bins[1]++;\n            } else if (elem < 0.75) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n\n    // 5) Count bins of the elements of x that belong to this rank\n    for (auto &elem: x) {\n        if (elem < 0.25) {\n            bins[0]++;\n        } else if (elem < 0.5) {\n            bins[1]++;\n        } else if (elem < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n    // 6) Send bins to the root\n    MPI_Reduce(&bins[0], nullptr, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int N = x.size();\n    int N_per_proc = N / size;\n    int rem = N % size;\n    int start = rank * N_per_proc;\n    int end = start + N_per_proc;\n    if (rank == size - 1)\n        end = N;\n\n    std::array<size_t, 4> local_bins{};\n    for (int i = start; i < end; ++i) {\n        int bin = static_cast<int>(x[i] * 4.0);\n        local_bins[bin % 4]++;\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(local_bins.data(), nullptr, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // TODO: Implement me\n}",
            "int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::vector<std::array<size_t, 4>> partialBins(size);\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(&partialBins[i], 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        for (int i = 1; i < size; ++i) {\n            for (int j = 0; j < 4; ++j) {\n                bins[j] += partialBins[i][j];\n            }\n        }\n\n    } else {\n        std::array<size_t, 4> myBins{};\n        for (auto elem : x) {\n            if (elem < 0.25) {\n                myBins[0] += 1;\n            } else if (elem >= 0.25 && elem < 0.5) {\n                myBins[1] += 1;\n            } else if (elem >= 0.5 && elem < 0.75) {\n                myBins[2] += 1;\n            } else if (elem >= 0.75) {\n                myBins[3] += 1;\n            }\n        }\n        MPI_Send(&myBins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t rank = 0, size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: fill in\n  // Hints:\n  // * Use MPI_Exscan to sum the counts in the bins over all ranks.\n  // * Use MPI_Scatterv to distribute x among ranks.\n}",
            "// Your code here.\n}",
            "const int rank = 0;\n  const int numProcs = 1;\n\n  std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n  // TODO: Implement this function\n\n  // Send counts to rank 0\n  if (rank == 0) {\n    for (int i = 1; i < numProcs; i++) {\n      int sendBins[4];\n      MPI_Recv(sendBins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; j++) {\n        counts[j] += sendBins[j];\n      }\n    }\n  } else {\n    MPI_Send(&counts, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // TODO: Implement this function\n\n  // Broadcast counts from rank 0\n  MPI_Bcast(bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm const &comm = MPI_COMM_WORLD;\n    int size, rank;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n    if (rank == 0) {\n        bins = {{0, 0, 0, 0}};\n    }\n\n    // Compute the sum of all values in `x`\n    double sum = 0;\n    for (double d : x) {\n        sum += d;\n    }\n\n    // Broadcast the sum of all values in `x`\n    double b_sum;\n    MPI_Bcast(&sum, 1, MPI_DOUBLE, 0, comm);\n\n    // Compute the sum of the values of `x` on the current rank\n    double sum_local = 0;\n    for (double d : x) {\n        sum_local += d;\n    }\n\n    // Broadcast the sum of the values of `x` on the current rank\n    double b_sum_local;\n    MPI_Bcast(&sum_local, 1, MPI_DOUBLE, 0, comm);\n\n    // Compute the number of values in `x`\n    size_t n = x.size();\n\n    // Broadcast the number of values in `x`\n    size_t b_n;\n    MPI_Bcast(&n, 1, MPI_UNSIGNED_LONG, 0, comm);\n\n    // Compute the number of values in `x` on the current rank\n    size_t n_local = x.size();\n\n    // Broadcast the number of values in `x` on the current rank\n    size_t b_n_local;\n    MPI_Bcast(&n_local, 1, MPI_UNSIGNED_LONG, 0, comm);\n\n    // Compute the counts\n    std::array<size_t, 4> counts = {{0, 0, 0, 0}};\n    for (double d : x) {\n        if (d * 4 < b_sum) {\n            counts[0]++;\n        } else if (d * 4 - 1 < b_sum) {\n            counts[1]++;\n        } else if (d * 4 - 2 < b_sum) {\n            counts[2]++;\n        } else {\n            counts[3]++;\n        }\n    }\n\n    // Compute the counts on the current rank\n    std::array<size_t, 4> counts_local = {{0, 0, 0, 0}};\n    for (double d : x) {\n        if (d * 4 < b_sum_local) {\n            counts_local[0]++;\n        } else if (d * 4 - 1 < b_sum_local) {\n            counts_local[1]++;\n        } else if (d * 4 - 2 < b_sum_local) {\n            counts_local[2]++;\n        } else {\n            counts_local[3]++;\n        }\n    }\n\n    // Send the counts to the root\n    MPI_Gather(counts_local.data(), counts_local.size(), MPI_UNSIGNED_LONG, counts.data(), counts.size(),\n               MPI_UNSIGNED_LONG, 0, comm);\n\n    // Compute the number of values in `x` from the root\n    size_t n_root = 0;\n    for (size_t i = 0; i < size; i++) {\n        if (i == 0) {\n            n_root += b_n;\n        } else {\n            n_root += b_n_local;\n        }\n    }\n\n    // Compute the counts from the root\n    std::array<size_t, 4> counts_root = {{0, 0, 0, 0}};\n    for (size_t i = 0; i < size; i++) {\n        for (size_t j = 0; j < 4; j++) {\n            if (i == 0) {\n                counts_root[j] += b_n_local * counts[j];\n            } else {\n                counts_root[j] += b_n * counts",
            "int num_procs, my_rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  int n = x.size();\n  int n_per_proc = n / num_procs;\n  int n_per_proc_rem = n % num_procs;\n  int n_to_add = n_per_proc_rem > my_rank? 1 : 0;\n  int n_start = my_rank * (n_per_proc + n_to_add) + std::min(n_to_add, my_rank);\n  int n_end = (my_rank + 1) * (n_per_proc + n_to_add) + std::min(n_to_add, my_rank + 1) - 1;\n  if (n_end > n) {\n    n_end = n;\n  }\n  int local_n = n_end - n_start + 1;\n\n  std::vector<int> local_bins(4, 0);\n  std::vector<double> local_x;\n  for (int i = n_start; i <= n_end; ++i) {\n    local_x.push_back(x[i]);\n  }\n\n  for (double a : local_x) {\n    if (a >= 0 && a < 0.25) {\n      local_bins[0]++;\n    } else if (a >= 0.25 && a < 0.5) {\n      local_bins[1]++;\n    } else if (a >= 0.5 && a < 0.75) {\n      local_bins[2]++;\n    } else if (a >= 0.75 && a < 1) {\n      local_bins[3]++;\n    }\n  }\n\n  int global_bins[4] = {0};\n\n  MPI_Reduce(local_bins.data(), global_bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (my_rank == 0) {\n    bins = {global_bins[0], global_bins[1], global_bins[2], global_bins[3]};\n  }\n}",
            "// Fill in your code here (you can use all the methods in utils.hpp)\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            bins[0]++;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        }\n        else if (x[i] >= 0.75 && x[i] < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// Your solution goes here\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  //TODO: Your code here!\n  const size_t num_bins = 4;\n  size_t local_bins[num_bins] = { 0 };\n  int lower = 0, upper = 0, bin = 0;\n\n  // compute the range of indices to be processed by this rank\n  lower = rank * x.size() / size;\n  upper = (rank + 1) * x.size() / size;\n\n  // count the number of elements in each bin\n  for (int i = lower; i < upper; ++i) {\n    bin = static_cast<int>(x[i] * 4) % num_bins;\n    local_bins[bin]++;\n  }\n\n  // compute the total counts\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(local_bins, num_bins, MPI_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < num_bins; ++j) {\n        local_bins[j] += bins[j];\n      }\n    }\n    // assign local_bins to bins\n    for (int j = 0; j < num_bins; ++j) {\n      bins[j] = local_bins[j];\n    }\n  } else {\n    MPI_Send(local_bins, num_bins, MPI_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "auto count_quarter = [](double x){\n        int i = (int)std::floor(x*4);\n        return i;\n    };\n\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n    for(auto xi : x) {\n        local_bins[count_quarter(xi)]++;\n    }\n\n    int n = x.size();\n    int size;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0) {\n        std::array<size_t, 4> recvbuf{0, 0, 0, 0};\n        std::array<size_t, 4> recvbufs;\n        for(int i = 1; i < size; i++) {\n            MPI_Recv(recvbufs.data(), 4, MPI_SIZE_T, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < 4; j++) {\n                recvbuf[j] += recvbufs[j];\n            }\n        }\n\n        for(int i = 0; i < 4; i++) {\n            bins[i] = recvbuf[i];\n        }\n\n        std::cout << \"total: \" << n << std::endl;\n        std::cout << \"bins: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n    } else {\n        MPI_Send(local_bins.data(), 4, MPI_SIZE_T, 0, 1, MPI_COMM_WORLD);\n    }\n\n\n}",
            "const int rank = MPI::COMM_WORLD.Get_rank();\n    const int size = MPI::COMM_WORLD.Get_size();\n\n    if (rank == 0) {\n        // TODO: Count the number of doubles in the vector x that have a fractional part \n        // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) and store the counts in bins.\n\n        // for (size_t i = 0; i < x.size(); ++i) {\n        //     size_t bin = (x[i] < 0.25)? 0 : (x[i] < 0.5)? 1 : (x[i] < 0.75)? 2 : 3;\n        //     bins[bin]++;\n        // }\n\n    } else {\n        // TODO: Count the number of doubles in the vector x that have a fractional part \n        // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1) and store the counts in bins.\n\n        // for (size_t i = 0; i < x.size(); ++i) {\n        //     size_t bin = (x[i] < 0.25)? 0 : (x[i] < 0.5)? 1 : (x[i] < 0.75)? 2 : 3;\n        //     MPI::COMM_WORLD.Send(&bin, 1, MPI_INT, 0, bin);\n        // }\n\n    }\n\n    if (rank == 0) {\n        // TODO: Receive the counts from all other ranks and store them in bins.\n\n        // std::array<MPI_Status, 4> status;\n\n        // for (int i = 1; i < size; ++i) {\n        //     int bin;\n        //     MPI::COMM_WORLD.Recv(&bin, 1, MPI_INT, i, bin);\n        //     bins[bin] += bin;\n        // }\n    }\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = x.size();\n  int countPerRank = count / size;\n  int remainder = count % size;\n\n  // TODO: compute bins for local portion of x\n\n  // TODO: gather all bins from all ranks into bins\n\n  // TODO: broadcast bins from rank 0 to all other ranks\n\n  // TODO: use `rank` to determine what portion of x to compute bins for\n  //       hint: compute the offset of the local portion of x in the global vector\n  //       hint: if (rank == 0) bins = localBins\n  //             else MPI_Recv(localBins, 4, MPI_UNSIGNED_LONG, 0,...)\n  //             MPI_Send(localBins, 4, MPI_UNSIGNED_LONG, 0,...)\n}",
            "// TODO 1: Implement MPI logic to divide x and accumulate `bins`.\n  // Each rank should operate on a subset of x.\n  // You may assume that the length of x is divisible by the number of ranks.\n\n\n\n\n\n  // TODO 2: Use MPI_Gather to collect the results from each rank and store them in bins.\n  // You may assume that `bins` is empty when this is called.\n  // You may assume that the number of elements in `bins` is equal to the number of ranks.\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    int size, rank;\n    int const tags[4] = {1, 2, 3, 4};\n    int tag = 1;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int count = x.size();\n    if (rank == 0) {\n        MPI_Bcast(&count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        std::vector<double> sendCount(count, 0.0);\n        for (int i = 0; i < count; i++) {\n            double value = x[i];\n            double intPart = floor(value);\n            if (intPart < value && value < intPart + 0.25) {\n                sendCount[i] = 1;\n                tag = 1;\n            } else if (intPart + 0.25 <= value && value < intPart + 0.5) {\n                sendCount[i] = 2;\n                tag = 2;\n            } else if (intPart + 0.5 <= value && value < intPart + 0.75) {\n                sendCount[i] = 3;\n                tag = 3;\n            } else if (intPart + 0.75 <= value && value <= intPart + 1.0) {\n                sendCount[i] = 4;\n                tag = 4;\n            }\n        }\n        MPI_Send(sendCount.data(), count, MPI_INT, 0, tags[tag], MPI_COMM_WORLD);\n    } else {\n        MPI_Bcast(&count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        std::vector<double> sendCount(count, 0.0);\n        for (int i = 0; i < count; i++) {\n            double value = x[i];\n            double intPart = floor(value);\n            if (intPart < value && value < intPart + 0.25) {\n                sendCount[i] = 1;\n                tag = 1;\n            } else if (intPart + 0.25 <= value && value < intPart + 0.5) {\n                sendCount[i] = 2;\n                tag = 2;\n            } else if (intPart + 0.5 <= value && value < intPart + 0.75) {\n                sendCount[i] = 3;\n                tag = 3;\n            } else if (intPart + 0.75 <= value && value <= intPart + 1.0) {\n                sendCount[i] = 4;\n                tag = 4;\n            }\n        }\n        MPI_Send(sendCount.data(), count, MPI_INT, 0, tags[tag], MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        std::vector<double> recvCount(count, 0.0);\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(recvCount.data(), count, MPI_INT, i, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < count; j++) {\n                bins[recvCount[j] - 1] += 1;\n            }\n        }\n    }\n}",
            "// TODO\n}",
            "int world_rank, world_size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  std::vector<size_t> localBins(4, 0);\n  int count = x.size();\n\n  for (int i = world_rank; i < count; i += world_size) {\n    double v = x[i];\n    if (v >= 0 && v < 0.25) {\n      localBins[0]++;\n    } else if (v >= 0.25 && v < 0.5) {\n      localBins[1]++;\n    } else if (v >= 0.5 && v < 0.75) {\n      localBins[2]++;\n    } else if (v >= 0.75 && v < 1) {\n      localBins[3]++;\n    }\n  }\n\n  std::vector<size_t> globalBins(4, 0);\n  MPI_Reduce(&localBins[0], &globalBins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    bins = globalBins;\n  }\n}",
            "int size;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int *counts = new int[4];\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            counts[i] = 0;\n        }\n        for (int i = 0; i < x.size(); i++) {\n            double fraction = x[i] - floor(x[i]);\n            if (fraction < 0.25) {\n                counts[0]++;\n            } else if (fraction < 0.5) {\n                counts[1]++;\n            } else if (fraction < 0.75) {\n                counts[2]++;\n            } else {\n                counts[3]++;\n            }\n        }\n    }\n    MPI_Reduce(counts, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n\n}",
            "// Write your code here!\n}",
            "size_t n = x.size();\n    if (n == 0)\n        return;\n    double lower = 0.0;\n    double upper = 1.0;\n    double middle = (lower + upper) / 2;\n\n    size_t bins[4] = { 0, 0, 0, 0 };\n    for (size_t i = 0; i < n; i++) {\n        if (x[i] < lower)\n            bins[0]++;\n        else if (x[i] < middle)\n            bins[1]++;\n        else if (x[i] < upper)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n\n    int p;\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    for (int k = 0; k < p; k++) {\n        if (k == rank) {\n            for (int i = 1; i < p; i++) {\n                size_t recvBins[4] = { 0, 0, 0, 0 };\n                MPI_Recv(&recvBins, 4, MPI_SIZE_T, i, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n                for (int j = 0; j < 4; j++) {\n                    bins[j] += recvBins[j];\n                }\n            }\n        }\n        else {\n            MPI_Send(&bins, 4, MPI_SIZE_T, k, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] >= 0 && x[i] < 1) {\n        bins[0]++;\n      } else if (x[i] >= 1 && x[i] < 2) {\n        bins[1]++;\n      } else if (x[i] >= 2 && x[i] < 3) {\n        bins[2]++;\n      } else if (x[i] >= 3 && x[i] < 4) {\n        bins[3]++;\n      }\n    }\n  }\n\n  // TODO: implement parallel version of countQuartiles\n\n  int n_local = x.size() / size;\n  int remain = x.size() % size;\n\n  for (int r = 1; r < size; r++) {\n    int offset = r * n_local;\n    int local_size = n_local;\n    if (r <= remain) {\n      local_size++;\n    }\n    int start = (offset + rank) * local_size;\n    int end = (offset + rank + 1) * local_size;\n    if (end > x.size()) {\n      end = x.size();\n    }\n    std::vector<double> local_x(x.begin() + start, x.begin() + end);\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (int i = 0; i < local_x.size(); i++) {\n      if (local_x[i] >= 0 && local_x[i] < 1) {\n        local_bins[0]++;\n      } else if (local_x[i] >= 1 && local_x[i] < 2) {\n        local_bins[1]++;\n      } else if (local_x[i] >= 2 && local_x[i] < 3) {\n        local_bins[2]++;\n      } else if (local_x[i] >= 3 && local_x[i] < 4) {\n        local_bins[3]++;\n      }\n    }\n\n    // TODO: implement MPI reduction\n\n    int global_size;\n    MPI_Allreduce(&local_bins, &bins, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const double bin_width = 1.0 / 4.0;\n\n    // Count how many elements are in each bin.\n    // This will be used to determine how many elements each rank will have.\n    std::vector<size_t> bin_counts(4);\n    for (double xi : x) {\n        int bin = static_cast<int>(xi / bin_width);\n        bin_counts[bin]++;\n    }\n\n    // Determine how many elements each rank will have.\n    std::vector<size_t> rank_counts(size);\n    for (int i = 0; i < size; i++) {\n        // How many elements will this rank have?\n        // Use the formula `ceil((n/p)*i) - floor((n/p)*(i-1))` to determine\n        // how many elements a rank will have. \n        rank_counts[i] = static_cast<size_t>(ceil(static_cast<double>(x.size()) / size * i)\n                                           - floor(static_cast<double>(x.size()) / size * (i-1)));\n    }\n\n    // Determine how many elements each rank will have, using the bin counts.\n    for (int i = 1; i < size; i++) {\n        // If rank `i` has more elements than the bin count for rank `i-1`,\n        // then the rank `i-1` will have the bin count, and rank `i` will have\n        // the difference.\n        if (rank_counts[i] > bin_counts[i-1]) {\n            rank_counts[i-1] += bin_counts[i-1];\n            rank_counts[i] -= bin_counts[i-1];\n        }\n    }\n\n    // Send the bin counts to rank 0.\n    MPI_Gather(&bin_counts[0], 4, MPI_UNSIGNED_LONG, &bins[0], 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // Compute the counts for this rank.\n    for (int bin = 0; bin < 4; bin++) {\n        for (double xi : x) {\n            int int_xi = static_cast<int>(xi / bin_width);\n            if (int_xi == bin) {\n                bins[bin]++;\n            }\n        }\n    }\n\n    // Reduce the counts from all ranks onto rank 0.\n    MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t i = rank;\n    size_t j = (rank+1) % size;\n    std::array<size_t, 4> tmp_bins{};\n    for(size_t k=i; k<x.size(); k+=size)\n    {\n        if(x[k]>=0 && x[k]<0.25)\n            tmp_bins[0]++;\n        else if(x[k]>=0.25 && x[k]<0.5)\n            tmp_bins[1]++;\n        else if(x[k]>=0.5 && x[k]<0.75)\n            tmp_bins[2]++;\n        else if(x[k]>=0.75 && x[k]<1)\n            tmp_bins[3]++;\n    }\n\n    if(rank == 0)\n    {\n        for(int i=0; i<4; i++)\n        {\n            MPI_Send(&tmp_bins[i], 1, MPI_INT, j, 0, MPI_COMM_WORLD);\n        }\n        bins = tmp_bins;\n        for(int i=1; i<size; i++)\n        {\n            for(int j=0; j<4; j++)\n            {\n                MPI_Recv(&tmp_bins[j], 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            }\n            for(int j=0; j<4; j++)\n            {\n                bins[j] += tmp_bins[j];\n            }\n        }\n    }\n    else\n    {\n        for(int i=0; i<4; i++)\n        {\n            MPI_Recv(&tmp_bins[i], 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n        bins = tmp_bins;\n    }\n    return;\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int binsPerProc = (int) x.size() / size;\n  int rem = (int) x.size() % size;\n  int start = binsPerProc * rank;\n  if (rank == 0) {\n    start += rem;\n  }\n\n  std::vector<double> myX(x.begin() + start, x.begin() + start + binsPerProc);\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n  int myBins[4] = {0};\n\n  for (double i : myX) {\n    if (i < 0.25) {\n      myBins[0]++;\n    } else if (i >= 0.25 && i < 0.5) {\n      myBins[1]++;\n    } else if (i >= 0.5 && i < 0.75) {\n      myBins[2]++;\n    } else if (i >= 0.75 && i < 1) {\n      myBins[3]++;\n    }\n  }\n\n  int recvBins[4];\n  MPI_Gather(myBins, 4, MPI_INT, recvBins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = recvBins[i];\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this\n  int my_rank, num_procs;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  if (my_rank == 0) {\n\n    // TODO: implement this\n\n  }\n  else {\n\n    // TODO: implement this\n\n  }\n\n  // TODO: implement this\n\n}",
            "/* Your solution here. Note that you can use `MPI_Comm_size` to get the total number of ranks. */\n}",
            "int my_rank;\n    int comm_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    if (my_rank == 0) {\n        bins = {0, 0, 0, 0};\n        double local_x[x.size()];\n        std::copy(x.begin(), x.end(), local_x);\n\n        for (int i = 1; i < comm_size; ++i) {\n            MPI_Recv(local_x, x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < x.size(); ++j) {\n                if (local_x[j] < 1 && local_x[j] >= 0.75) {\n                    bins[3] += 1;\n                } else if (local_x[j] < 0.75 && local_x[j] >= 0.5) {\n                    bins[2] += 1;\n                } else if (local_x[j] < 0.5 && local_x[j] >= 0.25) {\n                    bins[1] += 1;\n                } else if (local_x[j] < 0.25 && local_x[j] >= 0) {\n                    bins[0] += 1;\n                }\n            }\n        }\n    } else {\n        std::vector<double> local_x(x.size());\n        std::copy(x.begin(), x.end(), local_x.begin());\n\n        for (int i = 0; i < local_x.size(); ++i) {\n            if (local_x[i] < 1 && local_x[i] >= 0.75) {\n                bins[3] += 1;\n            } else if (local_x[i] < 0.75 && local_x[i] >= 0.5) {\n                bins[2] += 1;\n            } else if (local_x[i] < 0.5 && local_x[i] >= 0.25) {\n                bins[1] += 1;\n            } else if (local_x[i] < 0.25 && local_x[i] >= 0) {\n                bins[0] += 1;\n            }\n        }\n\n        MPI_Send(local_x.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Comm communicator = MPI_COMM_WORLD;\n  int rank;\n  int numRanks;\n\n  MPI_Comm_rank(communicator, &rank);\n  MPI_Comm_size(communicator, &numRanks);\n\n  if (rank == 0) {\n    for (auto& bin : bins) {\n      bin = 0;\n    }\n  }\n\n  size_t count = x.size() / numRanks;\n  size_t remainder = x.size() % numRanks;\n  size_t start = rank * count;\n  if (rank == 0) {\n    start = 0;\n  }\n\n  std::vector<double> partialX(count + ((rank == 0)? remainder : 0));\n\n  std::copy(x.begin() + start, x.begin() + start + count + remainder, partialX.begin());\n\n  MPI_Barrier(communicator);\n\n  std::array<size_t, 4> partialBins;\n\n  for (auto& bin : partialBins) {\n    bin = 0;\n  }\n\n  for (size_t i = 0; i < partialX.size(); i++) {\n    if (partialX[i] < 0.25) {\n      partialBins[0]++;\n    } else if (partialX[i] < 0.5) {\n      partialBins[1]++;\n    } else if (partialX[i] < 0.75) {\n      partialBins[2]++;\n    } else {\n      partialBins[3]++;\n    }\n  }\n\n  std::vector<size_t> tmp(4);\n  MPI_Reduce(partialBins.data(), tmp.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, communicator);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < bins.size(); i++) {\n      bins[i] = tmp[i];\n    }\n  }\n\n  MPI_Barrier(communicator);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "if(x.empty()) return;\n  const int rank = 0;\n  const int size = 0;\n\n  if (rank == 0) {\n    /*\n    TODO: Your code here!\n    */\n  }\n}",
            "// Your code goes here!\n}",
            "const int size = x.size();\n    const int rank = MPI_Get_rank();\n    const int nprocs = MPI_Get_nprocs();\n\n    int local_size = size / nprocs;\n    int extra = size % nprocs;\n    int local_start = rank * local_size;\n\n    if(rank < extra) {\n        local_size++;\n    } else {\n        local_start += extra;\n    }\n\n    std::vector<double> local_x(local_size);\n    std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n    for(int i = 0; i < local_size; i++) {\n        local_x[i] = x[local_start + i];\n    }\n\n    for(int i = 0; i < local_size; i++) {\n        double frac = local_x[i] - floor(local_x[i]);\n        if(frac >= 0.0 && frac < 0.25) {\n            local_bins[0]++;\n        } else if(frac >= 0.25 && frac < 0.5) {\n            local_bins[1]++;\n        } else if(frac >= 0.5 && frac < 0.75) {\n            local_bins[2]++;\n        } else if(frac >= 0.75 && frac < 1.0) {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int numRanks, myRank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  int rankSize = x.size() / numRanks;\n  int remainder = x.size() % numRanks;\n  std::vector<double> xLocal(rankSize);\n  if (myRank == 0) {\n    xLocal = std::vector<double>(x.begin(), x.begin() + rankSize + remainder);\n    bins.fill(0);\n  } else {\n    xLocal = std::vector<double>(rankSize);\n  }\n  MPI_Scatter(x.data(), rankSize, MPI_DOUBLE, xLocal.data(), rankSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if (myRank > 0) {\n    for (size_t i = 0; i < remainder; ++i) {\n      xLocal.push_back(x[rankSize * myRank + i]);\n    }\n  }\n  std::array<size_t, 4> binsLocal;\n  binsLocal.fill(0);\n  for (auto& element : xLocal) {\n    if (element < 0.25) {\n      ++binsLocal[0];\n    } else if (element < 0.5) {\n      ++binsLocal[1];\n    } else if (element < 0.75) {\n      ++binsLocal[2];\n    } else if (element < 1) {\n      ++binsLocal[3];\n    } else {\n      printf(\"Error: element %f is not in [0, 1)\\n\", element);\n      throw 1;\n    }\n  }\n  std::array<size_t, 4> binsGathered;\n  MPI_Gather(binsLocal.data(), 4, MPI_UNSIGNED_LONG_LONG, binsGathered.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  if (myRank == 0) {\n    bins = binsGathered;\n  }\n}",
            "// TODO: your code goes here\n  std::array<size_t, 4> binsLocal;\n  for (int i = 0; i < 4; i++) {\n      binsLocal[i] = 0;\n  }\n  for (int i = 0; i < x.size(); i++) {\n    if (0 <= x[i] && x[i] < 0.25) {\n        binsLocal[0]++;\n    }\n    else if (0.25 <= x[i] && x[i] < 0.5) {\n        binsLocal[1]++;\n    }\n    else if (0.5 <= x[i] && x[i] < 0.75) {\n        binsLocal[2]++;\n    }\n    else if (0.75 <= x[i] && x[i] < 1) {\n        binsLocal[3]++;\n    }\n  }\n  if (MPI_Rank == 0) {\n      for (int i = 1; i < MPI_Size; i++) {\n          MPI_Status status;\n          MPI_Recv(&binsLocal, 4, MPI_SIZE_T, i, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n          for (int j = 0; j < 4; j++) {\n              bins[j] += binsLocal[j];\n          }\n      }\n  } else {\n      MPI_Send(&binsLocal, 4, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size = x.size();\n  int rank = 0;\n  int n = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &n);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = size/n;\n  std::vector<double> local_x;\n  for(int i = 0; i < chunk; i++) {\n    local_x.push_back(x.at(i + rank*chunk));\n  }\n\n  std::array<size_t, 4> local_bins {0,0,0,0};\n  for(int i = 0; i < chunk; i++) {\n    if(local_x.at(i) >= 0 && local_x.at(i) < 0.25) {\n      local_bins[0] += 1;\n    } else if(local_x.at(i) >= 0.25 && local_x.at(i) < 0.5) {\n      local_bins[1] += 1;\n    } else if(local_x.at(i) >= 0.5 && local_x.at(i) < 0.75) {\n      local_bins[2] += 1;\n    } else {\n      local_bins[3] += 1;\n    }\n  }\n\n  // use reduce to count the quartiles\n  MPI_Reduce(&local_bins, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "}",
            "bins = {0, 0, 0, 0};\n    int const rank = MPI::COMM_WORLD.Get_rank();\n    int const size = MPI::COMM_WORLD.Get_size();\n\n    if (rank == 0) {\n        for (auto const& i : x) {\n            if (i >= 0 && i <= 0.25) bins[0]++;\n            if (i >= 0.25 && i <= 0.5) bins[1]++;\n            if (i >= 0.5 && i <= 0.75) bins[2]++;\n            if (i >= 0.75 && i <= 1.0) bins[3]++;\n        }\n        for (int i = 1; i < size; i++) {\n            int temp[4];\n            MPI::COMM_WORLD.Recv(temp, 4, MPI::INT, i, i);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += temp[j];\n            }\n        }\n    } else {\n        int temp[4] = {0, 0, 0, 0};\n        for (auto const& i : x) {\n            if (i >= 0 && i <= 0.25) temp[0]++;\n            if (i >= 0.25 && i <= 0.5) temp[1]++;\n            if (i >= 0.5 && i <= 0.75) temp[2]++;\n            if (i >= 0.75 && i <= 1.0) temp[3]++;\n        }\n        MPI::COMM_WORLD.Send(temp, 4, MPI::INT, 0, rank);\n    }\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    std::vector<double> local_x;\n    int local_N = (int)x.size() / size;\n    int remainder = (int)x.size() % size;\n    if (rank < remainder) {\n        local_N += 1;\n    }\n    else {\n        local_N += 0;\n    }\n    if (rank < remainder) {\n        for (int i = 0; i < local_N; i++) {\n            local_x.push_back(x[i + (rank * local_N)]);\n        }\n    }\n    else {\n        for (int i = 0; i < local_N; i++) {\n            local_x.push_back(x[i + ((rank - remainder) * local_N)]);\n        }\n    }\n    std::array<size_t, 4> local_bins;\n    local_bins[0] = 0;\n    local_bins[1] = 0;\n    local_bins[2] = 0;\n    local_bins[3] = 0;\n    for (int i = 0; i < local_x.size(); i++) {\n        if (local_x[i] < 0.25) {\n            local_bins[0] += 1;\n        }\n        else if (local_x[i] < 0.5) {\n            local_bins[1] += 1;\n        }\n        else if (local_x[i] < 0.75) {\n            local_bins[2] += 1;\n        }\n        else {\n            local_bins[3] += 1;\n        }\n    }\n    MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins[0] += remainder;\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Your code goes here!\n  \n}",
            "bins = {{0, 0, 0, 0}};\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // Your code goes here\n}",
            "// TODO: your code here\n}",
            "int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  int number_of_processes;\n  MPI_Comm_size(MPI_COMM_WORLD, &number_of_processes);\n\n  // TODO: fill in the code here\n}",
            "int rank;\n    int size;\n    int const ndoubles = x.size();\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int const nperproc = ndoubles / size;\n    int const remainder = ndoubles % size;\n\n    std::vector<double> myx(nperproc);\n    std::array<size_t, 4> mybins = {0, 0, 0, 0};\n    for (int i = 0; i < nperproc; ++i)\n        myx[i] = x[i + rank*nperproc];\n    for (auto const& val : myx)\n        if (val < 0.25) ++mybins[0];\n        else if (val < 0.5) ++mybins[1];\n        else if (val < 0.75) ++mybins[2];\n        else ++mybins[3];\n\n    MPI_Reduce(mybins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    return;\n}",
            "// Your code here!\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<double> l_x;\n  std::array<size_t, 4> l_bins = { 0, 0, 0, 0 };\n\n  if (rank == 0) {\n    for (size_t i = 0; i < x.size(); i += size) {\n      double current = x.at(i);\n      int bin_num = 0;\n\n      if (current >= 0 && current < 0.25) {\n        bin_num = 0;\n      }\n      else if (current >= 0.25 && current < 0.5) {\n        bin_num = 1;\n      }\n      else if (current >= 0.5 && current < 0.75) {\n        bin_num = 2;\n      }\n      else if (current >= 0.75 && current <= 1) {\n        bin_num = 3;\n      }\n      else {\n        std::cout << \"Something went wrong\" << std::endl;\n      }\n      l_bins[bin_num]++;\n    }\n  }\n  else {\n    l_x = x;\n    std::fill(l_bins.begin(), l_bins.end(), 0);\n  }\n\n  std::vector<size_t> counts(4, 0);\n\n  MPI_Gather(&l_bins, 4, MPI_UNSIGNED_LONG_LONG, &counts, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; i++) {\n      bins[i] = counts[i];\n    }\n  }\n}",
            "size_t n = x.size();\n  MPI_Datatype double_array;\n  MPI_Type_contiguous(4, MPI_UNSIGNED, &double_array);\n  MPI_Type_commit(&double_array);\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n  for (size_t i = 0; i < n; i++) {\n    double frac = x[i] - floor(x[i]);\n    if (frac < 0.25) local_bins[0]++;\n    else if (frac < 0.5) local_bins[1]++;\n    else if (frac < 0.75) local_bins[2]++;\n    else local_bins[3]++;\n  }\n  MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Type_free(&double_array);\n}",
            "const size_t n = x.size();\n  if (n == 0) {\n    for (size_t i=0; i<4; i++) {\n      bins[i] = 0;\n    }\n    return;\n  }\n\n  // Part 1: Get the bins of all processors\n  MPI_Datatype mpi_double_array_4;\n  MPI_Type_contiguous(4, MPI_SIZE_T, &mpi_double_array_4);\n  MPI_Type_commit(&mpi_double_array_4);\n\n  std::array<size_t, 4> bins_this_rank;\n  for (size_t i=0; i<4; i++) {\n    bins_this_rank[i] = 0;\n  }\n  for (size_t i=0; i<n; i++) {\n    auto const& x_i = x[i];\n    if (x_i >= 0. && x_i < 0.25) {\n      bins_this_rank[0] += 1;\n    } else if (x_i >= 0.25 && x_i < 0.5) {\n      bins_this_rank[1] += 1;\n    } else if (x_i >= 0.5 && x_i < 0.75) {\n      bins_this_rank[2] += 1;\n    } else if (x_i >= 0.75 && x_i < 1.) {\n      bins_this_rank[3] += 1;\n    }\n  }\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Part 2: Reduce the bins of all processors to 1 processor (rank 0)\n  if (world_rank == 0) {\n    for (int r=1; r<world_size; r++) {\n      std::array<size_t, 4> bins_r;\n      MPI_Status status;\n      MPI_Recv(&bins_r, 1, mpi_double_array_4, r, 0, MPI_COMM_WORLD, &status);\n      for (size_t i=0; i<4; i++) {\n        bins[i] += bins_r[i];\n      }\n    }\n  } else {\n    MPI_Send(&bins_this_rank, 1, mpi_double_array_4, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // Part 3: Broadcast the reduced bins\n  MPI_Bcast(&bins, 4, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n}",
            "// TODO: YOUR CODE HERE\n    int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int index, start, end;\n    start = rank * (x.size() / size);\n    end = start + (x.size() / size);\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (int i = start; i < end; i++){\n        if (x[i] <= 0.25)\n            bins[0]++;\n        else if (x[i] <= 0.5)\n            bins[1]++;\n        else if (x[i] <= 0.75)\n            bins[2]++;\n        else if (x[i] <= 1)\n            bins[3]++;\n    }\n\n    MPI_Reduce(&bins, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: your code here\n    //...\n    MPI_Init(NULL, NULL);\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int number_of_bins = 4;\n    int number_of_elements_per_rank = x.size() / size;\n    int number_of_left_over = x.size() % size;\n    int i;\n    int my_rank_elements_left_over;\n    std::array<size_t, 4> my_rank_bins;\n    if (rank == 0) {\n        my_rank_bins = { 0 };\n    } else {\n        my_rank_bins = { 0, 0, 0, 0 };\n    }\n    int global_rank = 0;\n    MPI_Request request[size];\n    MPI_Status status[size];\n    for (int i = 0; i < size; i++) {\n        if (i == 0) {\n            my_rank_elements_left_over = number_of_left_over;\n        } else {\n            my_rank_elements_left_over = 0;\n        }\n        int my_rank_elements_per_rank = number_of_elements_per_rank + my_rank_elements_left_over;\n        for (int j = 0; j < my_rank_elements_per_rank; j++) {\n            i = global_rank;\n            int element = (int)x[i];\n            if (element >= 0 && element <= 3) {\n                my_rank_bins[0]++;\n            }\n            if (element >= 4 && element <= 7) {\n                my_rank_bins[1]++;\n            }\n            if (element >= 8 && element <= 11) {\n                my_rank_bins[2]++;\n            }\n            if (element >= 12 && element <= 15) {\n                my_rank_bins[3]++;\n            }\n            global_rank++;\n        }\n        MPI_Isend(&my_rank_bins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, &request[i]);\n    }\n    for (int i = 1; i < size; i++) {\n        MPI_Recv(&my_rank_bins, 4, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status[i]);\n    }\n    for (int i = 0; i < 4; i++) {\n        bins[i] = my_rank_bins[i];\n    }\n    for (int i = 1; i < size; i++) {\n        MPI_Wait(&request[i], &status[i]);\n    }\n    MPI_Finalize();\n}",
            "// Your code here\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    int n = x.size();\n    int n_per_proc = n / p;\n    int rest = n % p;\n\n    int start = n_per_proc * rank;\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    std::vector<int> my_bins(4);\n\n    if (rank!= 0) {\n        for (int i = 0; i < n_per_proc; i++) {\n            if (x[i + start] < 0.25) {\n                my_bins[0]++;\n            }\n            else if (x[i + start] < 0.5) {\n                my_bins[1]++;\n            }\n            else if (x[i + start] < 0.75) {\n                my_bins[2]++;\n            }\n            else {\n                my_bins[3]++;\n            }\n        }\n    }\n    else {\n        for (int i = 0; i < start; i++) {\n            if (x[i] < 0.25) {\n                my_bins[0]++;\n            }\n            else if (x[i] < 0.5) {\n                my_bins[1]++;\n            }\n            else if (x[i] < 0.75) {\n                my_bins[2]++;\n            }\n            else {\n                my_bins[3]++;\n            }\n        }\n\n        for (int i = 0; i < n_per_proc; i++) {\n            if (x[i + start] < 0.25) {\n                my_bins[0]++;\n            }\n            else if (x[i + start] < 0.5) {\n                my_bins[1]++;\n            }\n            else if (x[i + start] < 0.75) {\n                my_bins[2]++;\n            }\n            else {\n                my_bins[3]++;\n            }\n        }\n    }\n\n    if (rest > 0 && rank == p - 1) {\n        for (int i = 0; i < rest; i++) {\n            if (x[i + start] < 0.25) {\n                my_bins[0]++;\n            }\n            else if (x[i + start] < 0.5) {\n                my_bins[1]++;\n            }\n            else if (x[i + start] < 0.75) {\n                my_bins[2]++;\n            }\n            else {\n                my_bins[3]++;\n            }\n        }\n    }\n\n    MPI_Gather(&my_bins[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int sub_size = x.size() / size;\n  std::vector<double> sub_x(sub_size);\n  if (rank == 0) {\n    sub_x = std::vector<double>(x.begin(), x.begin() + sub_size);\n  } else {\n    sub_x = std::vector<double>(x.begin() + rank * sub_size, x.begin() + (rank + 1) * sub_size);\n  }\n\n  std::array<size_t, 4> sub_bins = {0, 0, 0, 0};\n\n  for (int i = 0; i < sub_x.size(); i++) {\n    if (sub_x[i] < 0.25) {\n      sub_bins[0]++;\n    } else if (sub_x[i] < 0.5) {\n      sub_bins[1]++;\n    } else if (sub_x[i] < 0.75) {\n      sub_bins[2]++;\n    } else if (sub_x[i] < 1.0) {\n      sub_bins[3]++;\n    }\n  }\n  if (rank == 0) {\n    std::vector<size_t> sub_bins_recv(size * 4);\n    MPI_Gather(sub_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, sub_bins_recv.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    bins[0] = sub_bins_recv[0];\n    bins[1] = sub_bins_recv[4];\n    bins[2] = sub_bins_recv[8];\n    bins[3] = sub_bins_recv[12];\n  } else {\n    MPI_Gather(sub_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<size_t> localBins;\n  if (rank == 0) {\n    localBins.resize(4, 0);\n  }\n  const int chunk_size = x.size() / world_size;\n  std::vector<double> localx(chunk_size);\n\n  MPI_Scatter(&x[0], chunk_size, MPI_DOUBLE, &localx[0], chunk_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank!= 0) {\n    for (int i = 0; i < chunk_size; ++i) {\n      double tmp = localx[i];\n      if (tmp <= 0.25) {\n        localBins[0]++;\n      } else if (tmp > 0.25 && tmp <= 0.5) {\n        localBins[1]++;\n      } else if (tmp > 0.5 && tmp <= 0.75) {\n        localBins[2]++;\n      } else {\n        localBins[3]++;\n      }\n    }\n    MPI_Gather(&localBins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  } else {\n    for (int i = 0; i < x.size(); ++i) {\n      double tmp = x[i];\n      if (tmp <= 0.25) {\n        localBins[0]++;\n      } else if (tmp > 0.25 && tmp <= 0.5) {\n        localBins[1]++;\n      } else if (tmp > 0.5 && tmp <= 0.75) {\n        localBins[2]++;\n      } else {\n        localBins[3]++;\n      }\n    }\n    MPI_Gather(&localBins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n\n\n  if (rank == 0) {\n    std::cout << \"rank=0 bins=[ \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \"]\" << std::endl;\n  }\n}",
            "/* TODO */\n}",
            "size_t n = x.size();\n\n  // TODO: implement\n\n  return;\n}",
            "// Fill in the code here.\n  // It may be helpful to first count the number of elements in each bin, \n  // and then use `std::accumulate` to compute the total count of each bin.\n  // (Note: `std::accumulate` is a C++ standard library function, not a part of MPI. \n  // `std::accumulate` should be called only from rank 0.)\n  \n  \n  // This is a placeholder.\n  // Replace it with the actual code.\n  //\n  // You are not allowed to use OpenMP in this assignment.\n  // You are not allowed to use `std::reduce`.\n  // You are not allowed to use `std::accumulate` from <numeric> except at rank 0.\n  // You are not allowed to use any C++ standard library functions other than those listed above.\n}",
            "// TODO: Fill this in\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute the sizes of the subvectors on each process\n  std::vector<int> subvect_size(size, x.size()/size);\n  for (int i = 0; i < x.size() % size; ++i) ++subvect_size[i];\n\n  // Compute the displacements of the subvectors on each process\n  std::vector<int> subvect_disp(size, 0);\n  for (int i = 1; i < size; ++i) subvect_disp[i] = subvect_disp[i-1] + subvect_size[i-1];\n\n  // Create subvectors on each process\n  std::vector<double> x_subvect(subvect_size[rank]);\n  std::copy(x.begin() + subvect_disp[rank], x.begin() + subvect_disp[rank] + subvect_size[rank], x_subvect.begin());\n\n  // Count bins on each process\n  std::array<int, 4> counts = {0, 0, 0, 0};\n  for (double x_i : x_subvect) {\n    if (x_i - int(x_i) == 0) {\n      counts[0] += 1;\n    } else if (x_i - int(x_i) == 0.25) {\n      counts[1] += 1;\n    } else if (x_i - int(x_i) == 0.5) {\n      counts[2] += 1;\n    } else if (x_i - int(x_i) == 0.75) {\n      counts[3] += 1;\n    } else {\n      std::cout << \"ERROR\" << std::endl;\n    }\n  }\n\n  // Compute the counts on rank 0\n  if (rank == 0) {\n    std::vector<int> counts_all(4*size, 0);\n    MPI_Gather(&counts, 4, MPI_INT, counts_all.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < size; ++i) {\n      bins[0] += counts_all[4*i];\n      bins[1] += counts_all[4*i + 1];\n      bins[2] += counts_all[4*i + 2];\n      bins[3] += counts_all[4*i + 3];\n    }\n  } else {\n    MPI_Gather(&counts, 4, MPI_INT, NULL, 4, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "auto const num_elems = x.size();\n\n    // TODO: Implement this function!\n\n}",
            "// Fill the implementation here!\n    bins = {{0, 0, 0, 0}};\n\n}",
            "/* Your code goes here */\n}",
            "// TODO\n}",
            "size_t len = x.size();\n  std::vector<size_t> localCounts(4, 0);\n\n  // TODO: Fill out the counts for local bins\n\n  MPI_Reduce(&localCounts[0], &bins[0], 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: write this code\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0){\n        int len = x.size();\n        for (int i = 0; i < len; i++){\n            if (x[i] <= 0.25){\n                bins[0] += 1;\n            }\n            else if (x[i] > 0.25 && x[i] <= 0.5){\n                bins[1] += 1;\n            }\n            else if (x[i] > 0.5 && x[i] <= 0.75){\n                bins[2] += 1;\n            }\n            else if (x[i] > 0.75 && x[i] <= 1){\n                bins[3] += 1;\n            }\n            else{\n                bins[0] += 0;\n                bins[1] += 0;\n                bins[2] += 0;\n                bins[3] += 0;\n            }\n        }\n    }\n    else{\n        MPI_Send(&x[0], x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n    if (rank == 0){\n        double* recv = new double[x.size()];\n        MPI_Status status;\n        for (int i = 1; i < size; i++){\n            MPI_Recv(&recv, x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < x.size(); j++){\n                if (recv[j] <= 0.25){\n                    bins[0] += 1;\n                }\n                else if (recv[j] > 0.25 && recv[j] <= 0.5){\n                    bins[1] += 1;\n                }\n                else if (recv[j] > 0.5 && recv[j] <= 0.75){\n                    bins[2] += 1;\n                }\n                else if (recv[j] > 0.75 && recv[j] <= 1){\n                    bins[3] += 1;\n                }\n                else{\n                    bins[0] += 0;\n                    bins[1] += 0;\n                    bins[2] += 0;\n                    bins[3] += 0;\n                }\n            }\n        }\n        delete[] recv;\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  if (rank == 0) {\n    // Root: collect the bins from all the other ranks\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(bins.data(), bins.size(), MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < bins.size(); ++j) {\n        bins[j] += bins[j];\n      }\n    }\n  } else {\n    // Every other rank: calculate the bins\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n    for (auto& d : x) {\n      if (d < 0.25) {\n        localBins[0]++;\n      } else if (d < 0.5) {\n        localBins[1]++;\n      } else if (d < 0.75) {\n        localBins[2]++;\n      } else {\n        localBins[3]++;\n      }\n    }\n    if (rank == 0) {\n      // Rank 0: add the local bins to the global bins\n      for (size_t j = 0; j < bins.size(); ++j) {\n        bins[j] += localBins[j];\n      }\n    } else {\n      // Other ranks: send the local bins to rank 0\n      MPI_Send(localBins.data(), localBins.size(), MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n  }\n}",
            "// TODO\n}",
            "size_t const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    size_t const numRanks = MPI_Comm_size(MPI_COMM_WORLD);\n    size_t const n = x.size();\n    size_t const nPerRank = n / numRanks;\n    size_t const nLastRank = n % numRanks;\n\n    // TODO\n\n}",
            "// TODO: Your code here\n  if (x.empty()) return;\n  size_t num_items = x.size();\n  int num_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // determine how many items we will be counting\n  size_t num_items_per_proc = (num_items + num_procs - 1) / num_procs;\n  size_t first_index = rank * num_items_per_proc;\n  size_t last_index = std::min(first_index + num_items_per_proc, num_items);\n\n  // compute local counts\n  std::array<size_t, 4> local_counts = {};\n  for (size_t i = first_index; i < last_index; i++) {\n    double frac = x[i] - std::floor(x[i]);\n    if (frac < 0.25) local_counts[0]++;\n    else if (frac < 0.5) local_counts[1]++;\n    else if (frac < 0.75) local_counts[2]++;\n    else local_counts[3]++;\n  }\n\n  // gather counts from all processes\n  std::array<size_t, 4> gathered_counts;\n  MPI_Gather(&local_counts, 4, MPI_UNSIGNED_LONG, &gathered_counts, 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // store results\n  if (rank == 0) bins = gathered_counts;\n}",
            "// Your code here!\n}",
            "// YOUR CODE HERE\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> localx;\n    std::array<size_t, 4> localbins{0, 0, 0, 0};\n\n    if (rank == 0) {\n        // Count the number of elements in x and determine the number of parts each rank should work on.\n        auto const n = x.size();\n        auto const nparts = n / size;\n        auto const remainder = n % size;\n\n        // Distribute the elements of x between the processes.\n        // The first `remainder` processes will have one more element than the other processes.\n        for (int i = 0; i < size; ++i) {\n            std::vector<double> localx;\n            auto const nlocal = (i < remainder)? nparts + 1 : nparts;\n            localx.assign(x.begin() + i * nlocal, x.begin() + (i + 1) * nlocal);\n            // Send the data to the respective process.\n            MPI_Send(localx.data(), localx.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n\n        // Receive the results from all the processes.\n        for (int i = 1; i < size; ++i) {\n            std::array<size_t, 4> localbins;\n            MPI_Recv(localbins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += localbins[0];\n            bins[1] += localbins[1];\n            bins[2] += localbins[2];\n            bins[3] += localbins[3];\n        }\n    } else {\n        // Receive the data from rank 0.\n        MPI_Recv(localx.data(), localx.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        // Count the number of elements in localx.\n        for (auto const x : localx) {\n            if (x < 0.25) {\n                localbins[0]++;\n            } else if (x < 0.5) {\n                localbins[1]++;\n            } else if (x < 0.75) {\n                localbins[2]++;\n            } else {\n                localbins[3]++;\n            }\n        }\n        // Send the data to rank 0.\n        MPI_Send(localbins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t size = x.size();\n    bins.fill(0);\n\n    // TODO: Distribute the vector and compute the counts on each rank.\n    // Store the counts in `bins` on rank 0.\n\n\n}",
            "// TODO: write this function\n}",
            "// YOUR CODE HERE\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  auto part = [&](int rank, int size, std::vector<double> const& x) {\n    int size_per_rank = x.size() / size;\n    int part_start = rank * size_per_rank;\n    int part_end = (rank + 1) * size_per_rank;\n    if (rank == size - 1) {\n      part_end = x.size();\n    }\n    std::vector<double> part_x(x.begin() + part_start, x.begin() + part_end);\n    return part_x;\n  };\n\n  auto count = [&](std::vector<double> const& x, std::array<size_t, 4>& bins) {\n    std::array<size_t, 4> counts = {0, 0, 0, 0};\n    for (auto& x_i : x) {\n      auto i = static_cast<int>(x_i * 4);\n      counts[i]++;\n    }\n    return counts;\n  };\n\n  auto gather = [&](int rank, int size, std::array<size_t, 4>& bins,\n      std::array<size_t, 4> const& local_bins) {\n    std::vector<size_t> bins_vector(bins.begin(), bins.end());\n    std::vector<size_t> local_bins_vector(local_bins.begin(), local_bins.end());\n    MPI_Gather(&local_bins_vector[0], 4, MPI_UNSIGNED_LONG, &bins_vector[0], 4,\n               MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n      std::copy(bins_vector.begin(), bins_vector.end(), bins.begin());\n    }\n  };\n\n  auto reduce = [&](int rank, int size, std::array<size_t, 4>& bins) {\n    std::vector<size_t> bins_vector(bins.begin(), bins.end());\n    MPI_Reduce(&bins_vector[0], NULL, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0,\n               MPI_COMM_WORLD);\n    if (rank == 0) {\n      MPI_Gather(&bins_vector[0], 4, MPI_UNSIGNED_LONG, &bins_vector[0], 4,\n                 MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n      std::copy(bins_vector.begin(), bins_vector.end(), bins.begin());\n    }\n  };\n\n  if (rank == 0) {\n    bins = count(x, bins);\n    gather(rank, size, bins, bins);\n  } else {\n    auto part_x = part(rank, size, x);\n    auto local_bins = count(part_x, bins);\n    gather(rank, size, bins, local_bins);\n  }\n\n  reduce(rank, size, bins);\n}",
            "// TODO\n}",
            "int const world_size = MPI::COMM_WORLD.Get_size();\n  int const world_rank = MPI::COMM_WORLD.Get_rank();\n\n  std::vector<size_t> counts(4);\n\n  size_t const step = x.size() / world_size;\n  size_t const start_index = step * world_rank;\n  size_t const end_index = std::min(start_index + step, x.size());\n\n  for (size_t i = start_index; i < end_index; ++i) {\n    auto x_i = x[i];\n    int bin = 0;\n    if (x_i >= 0.75) {\n      bin = 0;\n    } else if (x_i >= 0.5) {\n      bin = 1;\n    } else if (x_i >= 0.25) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    ++counts[bin];\n  }\n\n  std::vector<size_t> counts_all(4 * world_size);\n  MPI::COMM_WORLD.Gather(&counts[0], counts.size(), MPI::INT, &counts_all[0], counts.size(), MPI::INT, 0);\n\n  if (world_rank == 0) {\n    for (int i = 0; i < world_size; ++i) {\n      for (int j = 0; j < 4; ++j) {\n        bins[j] += counts_all[i * 4 + j];\n      }\n    }\n  }\n}",
            "bins.fill(0);\n    for (auto const &e: x) {\n        auto const floorE = static_cast<int>(std::floor(e));\n        if (floorE == 0)\n            ++bins[0];\n        else if (floorE == 1)\n            ++bins[1];\n        else if (floorE == 2)\n            ++bins[2];\n        else if (floorE == 3)\n            ++bins[3];\n    }\n}",
            "// TODO\n}",
            "int num_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    // do something here\n  } else {\n    // do something here\n  }\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Write your code here\n  constexpr auto num_bins = 4;\n  constexpr auto bin_size = 1.0/num_bins;\n  const auto num_elems = x.size();\n\n  auto local_bins = std::array<size_t, num_bins>{};\n  for (const auto& elem: x) {\n    const auto bin_idx = static_cast<int>(elem / bin_size);\n    ++local_bins[bin_idx];\n  }\n\n  // TODO: MPI_Reduce\n  MPI_Reduce(local_bins.data(), bins.data(), num_bins, MPI_UNSIGNED_LONG_LONG,\n             MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  \n  for (int i = 0; i < x.size(); i++)\n  {\n    double fraction = x[i] - std::floor(x[i]);\n    if (fraction < 0.25)\n    {\n      bins[0]++;\n    }\n    else if (fraction >= 0.25 && fraction < 0.5)\n    {\n      bins[1]++;\n    }\n    else if (fraction >= 0.5 && fraction < 0.75)\n    {\n      bins[2]++;\n    }\n    else\n    {\n      bins[3]++;\n    }\n  }\n}",
            "}",
            "// TODO\n}",
            "// TODO\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank;\n  int num_ranks;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &num_ranks);\n\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n\n  // TODO: Implement this function\n\n  MPI_Barrier(comm);\n  if (rank == 0) {\n    std::cout << \"Number of ranks: \" << num_ranks << \"\\n\";\n  }\n}",
            "bins = { 0, 0, 0, 0 };\n\n  if (x.empty()) {\n    return;\n  }\n\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int comm_size;\n  MPI_Comm_size(comm, &comm_size);\n  int my_rank;\n  MPI_Comm_rank(comm, &my_rank);\n\n  // TODO: Fill in this function\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_bins = bins.size();\n\n  // Partition the work among ranks\n  int num_per_rank = x.size() / size;\n  int remainder = x.size() % size;\n  int start = rank*num_per_rank + std::min(rank, remainder);\n  int end = (rank+1)*num_per_rank + std::min(rank+1, remainder);\n\n  // Compute the number of elements in each bin in this part\n  std::array<size_t, 4> counts{};\n  for (size_t i = start; i < end; ++i) {\n    int digit = static_cast<int>(x[i] * 4) % 4;\n    if (digit < 0) {\n      digit += 4;\n    }\n    counts[digit]++;\n  }\n\n  // Gather the counts\n  std::array<size_t, 4> recv{};\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Status status;\n      MPI_Recv(&recv, num_bins, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n      for (int j = 0; j < num_bins; ++j) {\n        counts[j] += recv[j];\n      }\n    }\n    bins = counts;\n  }\n  else {\n    MPI_Send(&counts, num_bins, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code goes here\n\n}",
            "bins = {0,0,0,0};\n    for(double val : x)\n    {\n        if(val < 0.25)\n        {\n            bins[0]++;\n        }\n        else if(val < 0.5)\n        {\n            bins[1]++;\n        }\n        else if(val < 0.75)\n        {\n            bins[2]++;\n        }\n        else\n        {\n            bins[3]++;\n        }\n    }\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if(rank == 0)\n    {\n        std::vector<size_t> otherRanksBins(size-1, 0);\n        MPI_Recv(&otherRanksBins[0], size-1, MPI_LONG, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        bins[0] += otherRanksBins[0];\n        bins[1] += otherRanksBins[1];\n        bins[2] += otherRanksBins[2];\n        bins[3] += otherRanksBins[3];\n    }\n    else\n    {\n        MPI_Send(&bins[0], 4, MPI_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int rcv_size, snd_size;\n    snd_size = x.size();\n    MPI_Allgather(&snd_size, 1, MPI_INT, &rcv_size, 1, MPI_INT, MPI_COMM_WORLD);\n    std::vector<double> rcv(rcv_size);\n    int snd_size_total = snd_size;\n    int rcv_displacement = 0;\n    for (int i = 0; i < size; i++) {\n        if (i!= rank) {\n            MPI_Send(x.data(), snd_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        } else {\n            rcv_displacement = 0;\n            for (int j = 0; j < i; j++) {\n                rcv_displacement += rcv_size[j];\n            }\n        }\n    }\n    MPI_Scatterv(&rcv, &rcv_size, &rcv_displacement, MPI_DOUBLE, x.data(), snd_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int mpi_rank;\n  int mpi_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n  const size_t num_data = x.size();\n  const size_t data_per_rank = num_data / mpi_size;\n  const size_t left_over_data = num_data % mpi_size;\n  const size_t start_data = data_per_rank * mpi_rank + std::min(mpi_rank, left_over_data);\n  const size_t end_data = start_data + data_per_rank + (mpi_rank < left_over_data? 1 : 0);\n\n  std::array<size_t, 4> local_bins{0, 0, 0, 0};\n  for (size_t i = start_data; i < end_data; ++i) {\n    size_t bin = std::min(\n        size_t{3},\n        std::distance(\n            std::begin(std::array<double, 4>{0.0, 0.25, 0.5, 0.75}),\n            std::upper_bound(\n                std::begin(std::array<double, 4>{0.0, 0.25, 0.5, 0.75}),\n                std::end(std::array<double, 4>{0.0, 0.25, 0.5, 0.75}),\n                std::modf(x[i], nullptr))));\n    ++local_bins[bin];\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<double> y(x.size());\n    if(rank == 0){\n        std::copy(x.begin(), x.end(), y.begin());\n    }\n    MPI_Bcast(&y[0], x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    int count = 0;\n    for(int i = 0; i < y.size(); i++){\n        if(y[i] >= 0.0 && y[i] <= 0.25) {\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    count = 0;\n    for(int i = 0; i < y.size(); i++){\n        if(y[i] >= 0.25 && y[i] <= 0.5) {\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &bins[1], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    count = 0;\n    for(int i = 0; i < y.size(); i++){\n        if(y[i] >= 0.5 && y[i] <= 0.75) {\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &bins[2], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    count = 0;\n    for(int i = 0; i < y.size(); i++){\n        if(y[i] >= 0.75 && y[i] <= 1.0) {\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &bins[3], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n}",
            "// TODO\n}",
            "//TODO: Add your code here\n}",
            "int num_procs, proc_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n\n    int chunk_size = x.size() / num_procs;\n    int num_leftover = x.size() % num_procs;\n\n    std::vector<double> my_x;\n    std::array<size_t, 4> my_bins = { 0, 0, 0, 0 };\n    if (proc_rank == 0) {\n        for (size_t i = 0; i < num_procs - 1; i++) {\n            for (int j = 0; j < chunk_size; j++) {\n                if (x[j + i * chunk_size] < 0.25) {\n                    my_bins[0]++;\n                }\n                else if (x[j + i * chunk_size] < 0.5) {\n                    my_bins[1]++;\n                }\n                else if (x[j + i * chunk_size] < 0.75) {\n                    my_bins[2]++;\n                }\n                else if (x[j + i * chunk_size] < 1) {\n                    my_bins[3]++;\n                }\n            }\n        }\n        for (size_t i = 0; i < num_procs - 1; i++) {\n            for (int j = 0; j < chunk_size; j++) {\n                my_x.push_back(x[j + i * chunk_size]);\n            }\n        }\n        for (size_t i = 0; i < num_leftover; i++) {\n            my_x.push_back(x[i + (num_procs - 1) * chunk_size]);\n        }\n    }\n    else {\n        for (size_t i = 0; i < chunk_size; i++) {\n            my_x.push_back(x[i + proc_rank * chunk_size]);\n        }\n        for (size_t i = 0; i < num_leftover; i++) {\n            if (i + num_leftover * proc_rank >= x.size()) {\n                break;\n            }\n            my_x.push_back(x[i + num_leftover * proc_rank]);\n        }\n    }\n\n    int my_size = my_x.size();\n    MPI_Bcast(&my_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int *my_bins_int = new int[4];\n    for (size_t i = 0; i < 4; i++) {\n        my_bins_int[i] = my_bins[i];\n    }\n    MPI_Reduce(my_bins_int, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n_per_proc = x.size()/size;\n  int n_leftover = x.size() % size;\n\n  std::vector<double> x_proc;\n  for (int i = 0; i < n_per_proc; i++) {\n    x_proc.push_back(x[i*size+rank]);\n  }\n  for (int i = 0; i < n_leftover; i++) {\n    if (i == rank) {\n      x_proc.push_back(x[i*size+size]);\n    }\n  }\n\n  int n_per_proc_4 = n_per_proc / 4;\n  int n_leftover_4 = n_per_proc % 4;\n\n  std::array<int, 4> bins_proc;\n  bins_proc[0] = std::count_if(x_proc.begin(), x_proc.end(), [](double x){\n    return x >= 0.0 && x < 0.25;\n  });\n  bins_proc[1] = std::count_if(x_proc.begin(), x_proc.end(), [](double x){\n    return x >= 0.25 && x < 0.5;\n  });\n  bins_proc[2] = std::count_if(x_proc.begin(), x_proc.end(), [](double x){\n    return x >= 0.5 && x < 0.75;\n  });\n  bins_proc[3] = std::count_if(x_proc.begin(), x_proc.end(), [](double x){\n    return x >= 0.75 && x < 1.0;\n  });\n\n  if (rank == 0) {\n    for (int proc = 1; proc < size; proc++) {\n      int bins_proc_0;\n      int bins_proc_1;\n      int bins_proc_2;\n      int bins_proc_3;\n      MPI_Recv(&bins_proc_0, 1, MPI_INT, proc, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&bins_proc_1, 1, MPI_INT, proc, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&bins_proc_2, 1, MPI_INT, proc, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      MPI_Recv(&bins_proc_3, 1, MPI_INT, proc, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[0] += bins_proc_0;\n      bins[1] += bins_proc_1;\n      bins[2] += bins_proc_2;\n      bins[3] += bins_proc_3;\n    }\n  } else {\n    MPI_Send(&bins_proc[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    MPI_Send(&bins_proc[1], 1, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    MPI_Send(&bins_proc[2], 1, MPI_INT, 0, 2, MPI_COMM_WORLD);\n    MPI_Send(&bins_proc[3], 1, MPI_INT, 0, 3, MPI_COMM_WORLD);\n  }\n}",
            "int num_ranks, rank_id;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n\n  // rank 0 keeps track of the counts\n  std::array<size_t, 4> counts{};\n  for (auto const& x_i : x) {\n    if (x_i < 0.25) ++counts[0];\n    else if (x_i < 0.5) ++counts[1];\n    else if (x_i < 0.75) ++counts[2];\n    else ++counts[3];\n  }\n\n  // send the counts to rank 0\n  if (rank_id!= 0) {\n    MPI_Send(counts.data(), counts.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  } else {\n    // rank 0 receives the counts from all the other ranks\n    for (int i = 1; i < num_ranks; ++i) {\n      MPI_Recv(counts.data(), counts.size(), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; ++j) {\n        bins[j] += counts[j];\n      }\n    }\n  }\n\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // TODO: implement\n}",
            "int n = x.size();\n\n  MPI_Bcast(x, n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  MPI_Reduce(x, bins, 4, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* YOUR CODE HERE */\n}",
            "/* TODO */\n\n  int n; // Total number of elements in x.\n  int size; // Size of MPI world.\n  int rank; // Rank of this process.\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  /* If we have 3 processes, 2 processors will get 2 elements each and 1 processor will get 1 element. */\n  if (size > 0) {\n    n = x.size(); // Number of elements of x to assign to this processor.\n  }\n\n  std::vector<size_t> local_bins(4, 0);\n\n  /* Assign to each process elements from x. */\n  for (int i = 0; i < n; i++) {\n    if (0 <= x[i] && x[i] < 0.25) {\n      local_bins[0]++;\n    } else if (0.25 <= x[i] && x[i] < 0.5) {\n      local_bins[1]++;\n    } else if (0.5 <= x[i] && x[i] < 0.75) {\n      local_bins[2]++;\n    } else if (0.75 <= x[i]) {\n      local_bins[3]++;\n    }\n  }\n\n  /* Gather the counts on rank 0. */\n  std::vector<size_t> gather_bins(size * 4, 0);\n\n  MPI_Gather(&local_bins[0], 4, MPI_UNSIGNED_LONG_LONG, &gather_bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  /* Assign the result to bins. */\n  if (rank == 0) {\n    for (int i = 0; i < size * 4; i += 4) {\n      bins[0] += gather_bins[i];\n      bins[1] += gather_bins[i + 1];\n      bins[2] += gather_bins[i + 2];\n      bins[3] += gather_bins[i + 3];\n    }\n  }\n}",
            "// Your code here.\n}",
            "// Replace this code with a parallel implementation\n  std::array<size_t, 4> temp;\n  for (auto i = 0; i < 4; ++i)\n    temp[i] = 0;\n\n  for (auto i = 0; i < x.size(); ++i)\n    temp[(int) (x[i] / 0.25)]++;\n\n  MPI_Reduce(temp.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO\n}",
            "// your code here\n}",
            "/* TODO: Your code goes here */\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    if (world_rank == 0)\n        return;\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    std::vector<double> sub_x;\n    std::vector<size_t> sub_bins;\n    for (int i = 0; i < 4; i++) {\n        sub_bins.push_back(0);\n    }\n    if (x.size() % size!= 0) {\n        sub_x.reserve(x.size() / size + 1);\n    } else {\n        sub_x.reserve(x.size() / size);\n    }\n    for (int i = 0; i < x.size(); i++) {\n        int position = i % size;\n        if (position == 0) {\n            sub_x.push_back(x[i]);\n        } else {\n            sub_x.push_back(x[i]);\n        }\n    }\n    int number_of_bins = sub_x.size();\n    for (int i = 0; i < number_of_bins; i++) {\n        if (sub_x[i] >= 0 && sub_x[i] < 0.25) {\n            sub_bins[0]++;\n        } else if (sub_x[i] >= 0.25 && sub_x[i] < 0.5) {\n            sub_bins[1]++;\n        } else if (sub_x[i] >= 0.5 && sub_x[i] < 0.75) {\n            sub_bins[2]++;\n        } else if (sub_x[i] >= 0.75 && sub_x[i] < 1.0) {\n            sub_bins[3]++;\n        }\n    }\n\n    MPI_Send(sub_bins.data(), sub_bins.size(), MPI_INT, 0, 1, MPI_COMM_WORLD);\n}",
            "int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  auto size = x.size();\n\n  if (rank == 0) {\n    // TODO: Initialize all counts to zero.\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = 0;\n    }\n  }\n\n  auto sizePerProcess = size / (size_t)rank;\n  if (rank!= 0) {\n    sizePerProcess++;\n  }\n\n  // TODO: Loop over the elements of the vector x.\n  // Count the number of elements that fall into each quartile.\n  // TODO: Perform a collective communication to compute the total counts.\n  // Store the result in bins on rank 0.\n\n  if (rank == 0) {\n    std::cout << \"Bins: \" << bins[0] <<'' << bins[1] <<'' << bins[2] <<'' << bins[3] << '\\n';\n  }\n}",
            "/* TODO: Complete this function! */\n\n  int mpi_size, mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n  // each process has a local copy of x and stores the counts of the bin\n  // in this variable\n  std::array<size_t, 4> local_bins{0, 0, 0, 0};\n\n  // calculate the number of elements in x assigned to each process\n  int x_len = x.size();\n  int chunksize = x_len / mpi_size;\n  int remainder = x_len % mpi_size;\n\n  // determine the start and end indices for this process\n  int x_start_idx = mpi_rank * chunksize;\n  int x_end_idx = mpi_rank * chunksize + chunksize - 1;\n  if (mpi_rank == mpi_size - 1) {\n    x_end_idx += remainder;\n  }\n\n  for (int i = x_start_idx; i <= x_end_idx; i++) {\n    double frac = x[i] - std::floor(x[i]);\n    if (frac < 0.25) {\n      local_bins[0]++;\n    } else if (frac < 0.5) {\n      local_bins[1]++;\n    } else if (frac < 0.75) {\n      local_bins[2]++;\n    } else {\n      local_bins[3]++;\n    }\n  }\n\n  // merge the local bins with the global bins on rank 0\n  if (mpi_rank == 0) {\n    MPI_Reduce(MPI_IN_PLACE, &local_bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = local_bins;\n  } else {\n    MPI_Reduce(&local_bins, nullptr, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int nRanks, myRank;\n\n  // TODO: Your code here\n\n}",
            "int num_procs, my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    size_t n = x.size();\n    std::vector<size_t> counts(4, 0);\n    int count = 0;\n\n    // TODO: Implement this function\n\n    if (my_rank == 0) {\n        counts = bins;\n    }\n    MPI_Gather(&count, 4, MPI_INT, &bins[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int myid, numprocs;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myid);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n\tint num_per_proc = x.size() / numprocs;\n\tint num_per_proc_extra = x.size() % numprocs;\n\n\tstd::vector<double> my_x;\n\tif (myid == 0) {\n\t\tfor (size_t i = 0; i < x.size(); i++) {\n\t\t\tmy_x.push_back(x[i]);\n\t\t}\n\t}\n\telse {\n\t\tfor (size_t i = myid * num_per_proc; i < myid * num_per_proc + num_per_proc; i++) {\n\t\t\tif (i + num_per_proc_extra < x.size()) {\n\t\t\t\tmy_x.push_back(x[i + num_per_proc_extra]);\n\t\t\t}\n\t\t\telse {\n\t\t\t\tmy_x.push_back(x[i + num_per_proc_extra - 1]);\n\t\t\t}\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < my_x.size(); i++) {\n\t\tdouble fractional_part = my_x[i] - floor(my_x[i]);\n\t\tif (fractional_part < 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\tif (fractional_part >= 0.25 && fractional_part < 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\tif (fractional_part >= 0.5 && fractional_part < 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\tif (fractional_part >= 0.75 && fractional_part < 1) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\tif (myid!= 0) {\n\t\tMPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n\t}\n\tif (myid == 0) {\n\t\tfor (int i = 1; i < numprocs; i++) {\n\t\t\tMPI_Status status;\n\t\t\tMPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n\t\t}\n\t}\n\n}",
            "// replace this code with your implementation\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = 0;\n    }\n    double quarter = 0.25;\n    double half = 0.5;\n    double threeQuarter = 0.75;\n    double one = 1;\n    for (double d : x) {\n      if (d >= quarter && d < half) {\n        bins[0] += 1;\n      } else if (d >= half && d < threeQuarter) {\n        bins[1] += 1;\n      } else if (d >= threeQuarter && d < one) {\n        bins[2] += 1;\n      } else if (d < quarter) {\n        bins[3] += 1;\n      }\n    }\n}",
            "size_t size = x.size();\n    std::vector<int> local_bins(4, 0);\n\n    int rank, size_;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size_);\n\n    // TODO: your code here\n\n    if (rank == 0) {\n        std::vector<int> sum_bins(4, 0);\n        for (int i = 1; i < size_; i++) {\n            MPI_Status status;\n            MPI_Recv(&local_bins[0], 4, MPI_INT, i, 1, MPI_COMM_WORLD, &status);\n            sum_bins[0] += local_bins[0];\n            sum_bins[1] += local_bins[1];\n            sum_bins[2] += local_bins[2];\n            sum_bins[3] += local_bins[3];\n        }\n        bins[0] = sum_bins[0];\n        bins[1] = sum_bins[1];\n        bins[2] = sum_bins[2];\n        bins[3] = sum_bins[3];\n    }\n    else {\n        MPI_Send(&local_bins[0], 4, MPI_INT, 0, 1, MPI_COMM_WORLD);\n    }\n}",
            "int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  if (world_size == 1) {\n    // If only one rank, do serial computation\n    for (auto i : x) {\n      int bin = 0;\n      if (i >= 0 && i < 1) bin = 0;\n      if (i >= 1 && i < 2) bin = 1;\n      if (i >= 2 && i < 3) bin = 2;\n      if (i >= 3 && i < 4) bin = 3;\n      bins[bin]++;\n    }\n    return;\n  }\n\n  int dataSize = x.size();\n  int sendSize = dataSize/world_size;\n  int sendOffset = world_rank * sendSize;\n  int recvSize = sendSize;\n  int recvOffset = sendOffset;\n  if (world_rank == world_size - 1) recvSize = dataSize - world_rank*sendSize;\n\n  std::vector<int> sendBins(sendSize*4);\n  std::vector<int> recvBins(recvSize*4);\n\n  for (int i = 0; i < sendSize; i++) {\n    double xi = x[i + sendOffset];\n    int bin = 0;\n    if (xi >= 0 && xi < 1) bin = 0;\n    if (xi >= 1 && xi < 2) bin = 1;\n    if (xi >= 2 && xi < 3) bin = 2;\n    if (xi >= 3 && xi < 4) bin = 3;\n    sendBins[i*4+bin]++;\n  }\n\n  MPI_Scatter(&sendBins[0], sendSize*4, MPI_INT, &recvBins[0], recvSize*4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    // Merge\n    std::array<size_t, 4> myBins = {{0, 0, 0, 0}};\n    for (int i = 0; i < sendSize; i++) {\n      myBins[0] += recvBins[i*4+0];\n      myBins[1] += recvBins[i*4+1];\n      myBins[2] += recvBins[i*4+2];\n      myBins[3] += recvBins[i*4+3];\n    }\n    for (int i = sendSize; i < recvSize; i++) {\n      myBins[0] += recvBins[i*4+0];\n      myBins[1] += recvBins[i*4+1];\n      myBins[2] += recvBins[i*4+2];\n      myBins[3] += recvBins[i*4+3];\n    }\n    bins = myBins;\n  }\n}",
            "// TODO: Implement this function\n  // You can assume that x is nonempty, and every value is in [0, 1).\n\n  // Start with zero counts\n  bins = {{0, 0, 0, 0}};\n\n  // TODO: You will need to use MPI to communicate between ranks\n}",
            "auto comm = MPI_COMM_WORLD;\n    auto rank = getRank(comm);\n    auto num_ranks = getNumRanks(comm);\n    if (num_ranks!= 4) {\n        throw std::runtime_error(\"The number of ranks must be 4.\");\n    }\n\n    // TODO: implement this function\n}",
            "auto const n = x.size();\n    auto const rank = MPI::COMM_WORLD.Get_rank();\n    MPI::COMM_WORLD.Bcast(&n, 1, MPI_INT, 0);\n    if (n == 0) {\n        return;\n    }\n    size_t bin[4] = {0};\n    for (size_t i = 0; i < n; ++i) {\n        double q = x[i] - std::floor(x[i]);\n        if (q < 0.25) {\n            ++bin[0];\n        } else if (q < 0.5) {\n            ++bin[1];\n        } else if (q < 0.75) {\n            ++bin[2];\n        } else {\n            ++bin[3];\n        }\n    }\n    auto const size = MPI::COMM_WORLD.Get_size();\n    std::vector<size_t> counts(size);\n    MPI::COMM_WORLD.Gather(&bin, 4, MPI::UNSIGNED_LONG_LONG, &counts[0], 4, MPI::UNSIGNED_LONG_LONG, 0);\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            for (int j = 1; j < size; ++j) {\n                counts[0][i] += counts[j][i];\n            }\n        }\n        bins = {counts[0][0], counts[0][1], counts[0][2], counts[0][3]};\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Your code here\n\n}",
            "// TODO: Add your code here\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Status status;\n\n  int num_local_elements = x.size() / size;\n  int remainder = x.size() % size;\n  std::vector<double> x_local(num_local_elements);\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Send(&x[num_local_elements * i], num_local_elements + (i <= remainder), MPI_DOUBLE, i, 1, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Recv(&x_local[0], num_local_elements + (rank <= remainder), MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, &status);\n  }\n  for (int i = 0; i < x_local.size(); i++) {\n    if (rank == 0) {\n      int num = std::floor(x_local[i]);\n      if (num == 0) {\n        bins[0]++;\n      } else if (num == 1) {\n        bins[1]++;\n      } else if (num == 2) {\n        bins[2]++;\n      } else if (num == 3) {\n        bins[3]++;\n      }\n    }\n  }\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&x_local[0], num_local_elements + (i <= remainder), MPI_DOUBLE, i, 1, MPI_COMM_WORLD, &status);\n      for (int j = 0; j < x_local.size(); j++) {\n        int num = std::floor(x_local[j]);\n        if (num == 0) {\n          bins[0]++;\n        } else if (num == 1) {\n          bins[1]++;\n        } else if (num == 2) {\n          bins[2]++;\n        } else if (num == 3) {\n          bins[3]++;\n        }\n      }\n    }\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: Fill in this function\n\n    // MPI variables\n    const int rank = 0;\n    const int size = 1;\n    const int root = 0;\n\n    // Count the elements in x\n    const size_t N = x.size();\n\n    // Create a work vector to store counts in\n    std::vector<size_t> counts(4, 0);\n\n    // TODO: Implement the MPI\n}",
            "int num_ranks;\n  int rank_id;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n\n  /*\n  Your solution here\n  */\n\n  if (rank_id == 0) {\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunks = size;\n    int chunk_size = x.size() / chunks;\n\n    int start = rank * chunk_size;\n    int end = (rank + 1) * chunk_size;\n    std::vector<double> x_chunk(x.begin() + start, x.begin() + end);\n\n    std::array<size_t, 4> bins_chunk;\n\n    for (size_t i = 0; i < 4; i++) {\n        bins_chunk[i] = 0;\n    }\n\n    for (double element : x_chunk) {\n        int quarter = (element * 4) % 4;\n        bins_chunk[quarter]++;\n    }\n\n    // Now, reduce the counts from each rank to rank 0.\n    std::array<size_t, 4> bins_global;\n    MPI_Reduce(&bins_chunk, &bins_global, 4, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = bins_global;\n    }\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n  // your code here\n  int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int my_size = x.size() / size;\n  std::vector<double> my_x(x.begin() + rank * my_size, x.begin() + rank * my_size + my_size);\n\n  std::array<size_t, 4> my_bins = {0, 0, 0, 0};\n\n  for (auto i = 0; i < my_x.size(); i++) {\n    if (my_x[i] < 0.25) {\n      my_bins[0]++;\n    } else if (my_x[i] < 0.5) {\n      my_bins[1]++;\n    } else if (my_x[i] < 0.75) {\n      my_bins[2]++;\n    } else {\n      my_bins[3]++;\n    }\n  }\n\n  MPI_Reduce(my_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "MPI_Status status;\n  int world_size, world_rank, tag = 1;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  MPI_Request requests[4];\n  int count[4] = {0};\n\n  auto const n = x.size();\n  auto const block_size = n / world_size;\n  auto const extra_count = n % world_size;\n  auto const start = block_size * world_rank;\n  auto const stop = block_size * (world_rank + 1) + std::min(extra_count, world_rank);\n\n  for (auto const& elem : x) {\n    if (elem < 0.25)\n      count[0]++;\n    else if (elem < 0.5)\n      count[1]++;\n    else if (elem < 0.75)\n      count[2]++;\n    else if (elem < 1)\n      count[3]++;\n  }\n\n  if (world_rank == 0) {\n    for (int i = 1; i < world_size; ++i) {\n      MPI_Irecv(count.data(), 4, MPI_INT, i, tag, MPI_COMM_WORLD, &requests[i - 1]);\n    }\n    for (int i = 1; i < world_size; ++i) {\n      MPI_Wait(&requests[i - 1], &status);\n      for (int j = 0; j < 4; ++j) {\n        count[j] += status.MPI_TAG;\n      }\n    }\n  } else {\n    MPI_Send(count.data(), 4, MPI_INT, 0, tag, MPI_COMM_WORLD);\n  }\n\n  MPI_Bcast(count.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (auto const& elem : x) {\n    if (elem < 0.25)\n      bins[0]++;\n    else if (elem < 0.5)\n      bins[1]++;\n    else if (elem < 0.75)\n      bins[2]++;\n    else if (elem < 1)\n      bins[3]++;\n  }\n}",
            "}",
            "// TODO: Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<size_t> localBins(4,0);\n    for (auto i=rank; i<x.size(); i+=size){\n        int b = 0;\n        if (x[i]<0.25){\n            b = 0;\n        } else if (x[i]<0.5){\n            b = 1;\n        } else if (x[i]<0.75){\n            b = 2;\n        } else if (x[i]<=1.0){\n            b = 3;\n        }\n        localBins[b]++;\n    }\n\n    std::vector<size_t> globalBins(4,0);\n    MPI_Reduce(localBins.data(), globalBins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0){\n        bins = std::array<size_t,4>{globalBins[0],globalBins[1],globalBins[2],globalBins[3]};\n    }\n\n}",
            "int rank;\n    int numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    // TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunksize = x.size() / size;\n  int remainder = x.size() % size;\n  int local_size = chunksize + (rank < remainder? 1 : 0);\n  std::vector<double> local_x(local_size);\n  std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n\n  MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < local_size; ++i) {\n    if (local_x[i] < 0.25) {\n      ++local_bins[0];\n    } else if (local_x[i] < 0.5) {\n      ++local_bins[1];\n    } else if (local_x[i] < 0.75) {\n      ++local_bins[2];\n    } else {\n      ++local_bins[3];\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: Add implementation\n}",
            "// TODO: Implement\n    int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n    int quarter = 0;\n    double fraction;\n\n    if (thread_id < N) {\n        fraction = x[thread_id] - floor(x[thread_id]);\n\n        if (fraction >= 0.0 && fraction < 0.25) {\n            quarter = 0;\n        } else if (fraction >= 0.25 && fraction < 0.5) {\n            quarter = 1;\n        } else if (fraction >= 0.5 && fraction < 0.75) {\n            quarter = 2;\n        } else if (fraction >= 0.75 && fraction <= 1.0) {\n            quarter = 3;\n        }\n\n        atomicAdd(&(bins[quarter]), 1);\n    }\n}",
            "// Implement this function in the file countQuartiles.cu\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    /* TODO: Write code here */\n\n}",
            "// Write your CUDA code here\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double x_tid = x[tid];\n        if (x_tid < 1 && x_tid > 0)\n            atomicAdd(&bins[0], 1);\n        else if (x_tid < 2 && x_tid > 1)\n            atomicAdd(&bins[1], 1);\n        else if (x_tid < 3 && x_tid > 2)\n            atomicAdd(&bins[2], 1);\n        else if (x_tid < 4 && x_tid > 3)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  int stride = blockDim.x * gridDim.x;\n \n  __shared__ int shared_bins[4];\n  //initialize shared_bins to zero\n  for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n    shared_bins[i] = 0;\n  }\n  __syncthreads();\n\n  for (int i = thread_id; i < N; i += stride) {\n    double number = x[i];\n    int digit_place = (int) (number * 4);\n    if (digit_place <= 3) {\n      atomicAdd(&(shared_bins[digit_place]), 1);\n    }\n  }\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n    atomicAdd(&(bins[i]), shared_bins[i]);\n  }\n\n}",
            "// Count the number of doubles in the vector x that have a fractional part \n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n  // Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n  // Examples:\n  //\n  // input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  // output: [2, 1, 2, 2]\n  //\n  // input: [1.9, 0.2, 0.6, 10.1, 7.4]\n  // output: [2, 1, 1, 1]\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (x[i] >= 0.0 && x[i] < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "__shared__ size_t local_bins[4];\n    local_bins[0] = 0;\n    local_bins[1] = 0;\n    local_bins[2] = 0;\n    local_bins[3] = 0;\n    size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) {\n        double remainder = fmod(x[id], 1.0);\n        if (remainder >= 0.75) {\n            atomicAdd(&local_bins[0], 1);\n        } else if (remainder >= 0.50) {\n            atomicAdd(&local_bins[1], 1);\n        } else if (remainder >= 0.25) {\n            atomicAdd(&local_bins[2], 1);\n        } else if (remainder >= 0.00) {\n            atomicAdd(&local_bins[3], 1);\n        }\n    }\n    __syncthreads();\n    atomicAdd(&bins[0], local_bins[0]);\n    atomicAdd(&bins[1], local_bins[1]);\n    atomicAdd(&bins[2], local_bins[2]);\n    atomicAdd(&bins[3], local_bins[3]);\n}",
            "// Get the thread index.\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // Check that we are in range.\n  if (i < N) {\n\n    // Get the fractional part.\n    double fractional = x[i] - floor(x[i]);\n\n    // Determine which bin it belongs in.\n    if (fractional < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (fractional < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (fractional < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    double xi = x[i];\n    if (xi < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (xi < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (xi < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) {\n    return;\n  }\n\n  double frac = x[idx] - floor(x[idx]);\n  if (frac >= 0.0 && frac < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (frac >= 0.25 && frac < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (frac >= 0.5 && frac < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (frac >= 0.75 && frac < 1.0) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: Replace this\n  assert(0);\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    double fractionalPart = x[index] - floor(x[index]);\n    if (fractionalPart < 0.25) bins[0]++;\n    else if (fractionalPart < 0.5) bins[1]++;\n    else if (fractionalPart < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        const int q1 = (int)floor(x[i]);\n        const int q2 = (int)floor(x[i] + 0.25);\n        const int q3 = (int)floor(x[i] + 0.5);\n        const int q4 = (int)floor(x[i] + 0.75);\n        atomicAdd(&bins[0], x[i] < 1 && q1 == x[i]? 1 : 0);\n        atomicAdd(&bins[1], x[i] < 1 && q2 == x[i]? 1 : 0);\n        atomicAdd(&bins[2], x[i] < 1 && q3 == x[i]? 1 : 0);\n        atomicAdd(&bins[3], x[i] < 1 && q4 == x[i]? 1 : 0);\n    }\n}",
            "}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  double xi = x[i];\n  // TODO: replace this block with an if statement\n  if (xi < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (xi < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (xi < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// Initialize bin counts to 0\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  \n  // TODO: Write a loop that counts the number of doubles in the vector x \n  // that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  // Store the counts in `bins`.\n  // The loop should run from i = threadIdx.x to N\n  // Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n  // Use threadIdx.x to access the vector x.\n  // Examples:\n\n  // input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  // output: [2, 1, 2, 2]\n\n  // input: [1.9, 0.2, 0.6, 10.1, 7.4]\n  // output: [2, 1, 1, 1]\n}",
            "}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  __shared__ size_t shared_bins[4];\n  \n  if (tid < N) {\n    double frac = fmodf(x[tid], 1.0);\n    if (frac >= 0.75) {\n      atomicAdd(&shared_bins[3], 1);\n    } else if (frac >= 0.5) {\n      atomicAdd(&shared_bins[2], 1);\n    } else if (frac >= 0.25) {\n      atomicAdd(&shared_bins[1], 1);\n    } else {\n      atomicAdd(&shared_bins[0], 1);\n    }\n  }\n  \n  __syncthreads();\n  \n  for (size_t i = threadIdx.x; i < 4; i += blockDim.x) {\n    atomicAdd(&bins[i], shared_bins[i]);\n  }\n  \n}",
            "// This should be the size of the thread blocks\n   const size_t THREADS = 128;\n   // The index of this thread in the block\n   size_t i = threadIdx.x + blockIdx.x * THREADS;\n\n   // Compute the number of doubles with a fractional part in [0, 0.25)\n   double fraction = (x[i] - floor(x[i]));\n   if (0 <= fraction && fraction < 0.25) bins[0]++;\n\n   // Compute the number of doubles with a fractional part in [0.25, 0.5)\n   fraction = (x[i] - floor(x[i]));\n   if (0.25 <= fraction && fraction < 0.5) bins[1]++;\n\n   // Compute the number of doubles with a fractional part in [0.5, 0.75)\n   fraction = (x[i] - floor(x[i]));\n   if (0.5 <= fraction && fraction < 0.75) bins[2]++;\n\n   // Compute the number of doubles with a fractional part in [0.75, 1)\n   fraction = (x[i] - floor(x[i]));\n   if (0.75 <= fraction && fraction < 1) bins[3]++;\n\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (gid < N) {\n        size_t index = (x[gid] - floor(x[gid])) * 4;\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n\n  // TODO\n  bins[0] = (x[tid] >= 0 && x[tid] < 0.25)? bins[0] + 1 : bins[0];\n  bins[1] = (x[tid] >= 0.25 && x[tid] < 0.5)? bins[1] + 1 : bins[1];\n  bins[2] = (x[tid] >= 0.5 && x[tid] < 0.75)? bins[2] + 1 : bins[2];\n  bins[3] = (x[tid] >= 0.75 && x[tid] < 1)? bins[3] + 1 : bins[3];\n}",
            "}",
            "}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    double fraction = x[i] - (int)x[i];\n\n    if(fraction >= 0.0 && fraction < 0.25) bins[0] += 1;\n    else if(fraction >= 0.25 && fraction < 0.5) bins[1] += 1;\n    else if(fraction >= 0.5 && fraction < 0.75) bins[2] += 1;\n    else if(fraction >= 0.75 && fraction < 1.0) bins[3] += 1;\n}",
            "// TODO\n}",
            "// Write your code here.\n}",
            "// Your code here\n}",
            "}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i < N)\n      atomicAdd(&bins[0], (int) (x[i] >= 0.0 && x[i] < 0.25));\n   if (i < N)\n      atomicAdd(&bins[1], (int) (x[i] >= 0.25 && x[i] < 0.5));\n   if (i < N)\n      atomicAdd(&bins[2], (int) (x[i] >= 0.5 && x[i] < 0.75));\n   if (i < N)\n      atomicAdd(&bins[3], (int) (x[i] >= 0.75 && x[i] < 1.0));\n}",
            "// YOUR CODE HERE\n}",
            "int i = threadIdx.x;\n    int blockSize = blockDim.x;\n    int threadId = blockIdx.x*blockSize + threadIdx.x;\n\n    // Count the number of values between the boundaries [0, 0.25), [0.25, 0.5),\n    // [0.5, 0.75), and [0.75, 1) by looping through the elements of the array x.\n    // Do this in parallel.\n\n}",
            "}",
            "/* Fill the code below */\n\n  // The index of the current thread\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Count the number of numbers in each bin\n  for (int i = idx; i < N; i += blockDim.x * gridDim.x) {\n    int q = (int)((x[i] - floor(x[i])) / 0.25);\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "}",
            "int idx = threadIdx.x;\n\tif (idx < N) {\n\t\tint a = (int) (x[idx] / 0.25) % 4;\n\t\tatomicAdd(&(bins[a]), 1);\n\t}\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  double fractional = x[i] - floor(x[i]);\n  if (fractional < 0.25)\n    atomicAdd(&bins[0], 1);\n  else if (fractional < 0.5)\n    atomicAdd(&bins[1], 1);\n  else if (fractional < 0.75)\n    atomicAdd(&bins[2], 1);\n  else\n    atomicAdd(&bins[3], 1);\n}",
            "// TODO: Implement this\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        if (x[i] >= 0.75)\n            atomicAdd(&bins[3], 1);\n        else if (x[i] >= 0.5)\n            atomicAdd(&bins[2], 1);\n        else if (x[i] >= 0.25)\n            atomicAdd(&bins[1], 1);\n        else\n            atomicAdd(&bins[0], 1);\n    }\n}",
            "// TODO: implement\n}",
            "int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadIdx >= N) { return; }\n\n    double x_temp = x[threadIdx];\n    if (x_temp - floor(x_temp) < 0.25) {\n        atomicAdd(&bins[0], 1);\n    }\n    else if (x_temp - floor(x_temp) < 0.5) {\n        atomicAdd(&bins[1], 1);\n    }\n    else if (x_temp - floor(x_temp) < 0.75) {\n        atomicAdd(&bins[2], 1);\n    }\n    else if (x_temp - floor(x_temp) < 1) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    double frac = modf(x[i], &x[i]);\n    int bin = (x[i] < 0.25)? 0\n             : (x[i] < 0.5) ? 1\n             : (x[i] < 0.75)? 2\n                             : 3;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "}",
            "// **** START YOUR CODE HERE ****\n    //\n    // Use atomic functions to increment the bins for each of the \n    // 4 quartiles.\n    //\n    // **** END YOUR CODE HERE ****\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) return;\n  double f = x[i] - floor(x[i]);\n  atomicAdd(&bins[4*f], 1);\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = gridDim.x * blockDim.x;\n  for(; id < N; id += stride) {\n    double frac = x[id] - floor(x[id]);\n    if(frac < 0.25) {\n      atomicAdd(bins + 0, 1);\n    }\n    else if(frac < 0.5) {\n      atomicAdd(bins + 1, 1);\n    }\n    else if(frac < 0.75) {\n      atomicAdd(bins + 2, 1);\n    }\n    else if(frac < 1) {\n      atomicAdd(bins + 3, 1);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  __shared__ size_t counts[4];\n\n  for(int i = 0; i < 4; i++) {\n    counts[i] = 0;\n  }\n\n  while(tid < N) {\n    double num = x[tid];\n    int index = 0;\n    if (num >= 0.75) {\n      index = 3;\n    } else if (num >= 0.5) {\n      index = 2;\n    } else if (num >= 0.25) {\n      index = 1;\n    }\n\n    counts[index] += 1;\n\n    tid += stride;\n  }\n\n  // reduce in shared memory\n  __syncthreads();\n  for(int i = 1; i < blockDim.x; i++) {\n    counts[0] += counts[i];\n    counts[1] += counts[i + 1];\n    counts[2] += counts[i + 2];\n    counts[3] += counts[i + 3];\n  }\n\n  // write to global memory\n  if(threadIdx.x == 0) {\n    bins[0] = counts[0];\n    bins[1] = counts[1];\n    bins[2] = counts[2];\n    bins[3] = counts[3];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i < N) {\n    double fraction = x[i] - floor(x[i]);\n    if (fraction >= 0.75) {\n      atomicAdd(&bins[3], 1);\n    } else if (fraction >= 0.5) {\n      atomicAdd(&bins[2], 1);\n    } else if (fraction >= 0.25) {\n      atomicAdd(&bins[1], 1);\n    } else {\n      atomicAdd(&bins[0], 1);\n    }\n  }\n}",
            "// TODO: Fill this in\n  if (threadIdx.x < N) {\n    int bin = (int)(x[threadIdx.x] * 4) % 4;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Count the number of doubles in the vector x that have a fractional part \n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n\n  // TODO:\n\n  // 1. Make sure that each thread has its own local copy of the input array x.\n  //    Do not share it.\n\n  // 2. Use threadIdx.x to get the index of the current thread.\n  //    Use this index to get the current value of x.\n\n  // 3. Use the modulo operation to compute the fractional part of x, then\n  //    use an if-else statement to count the numbers with a fractional part\n  //    in the interval [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n\n  // 4. Use atomicAdd() to add one to the bin associated with the fractional part of x.\n\n  // Hint: You may find the following functions useful:\n  //       floor() returns the largest integer less than or equal to a number.\n  //       fmod() returns the fractional part of a number.\n  //       atomicAdd() increments a variable by a value.\n  //       __syncthreads() causes all threads to wait for all other threads to finish.\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i < N){\n    if (x[i] < 0.25){\n      atomicAdd(&bins[0], 1);\n    }\n    else if (x[i] < 0.5){\n      atomicAdd(&bins[1], 1);\n    }\n    else if (x[i] < 0.75){\n      atomicAdd(&bins[2], 1);\n    }\n    else{\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double t = x[tid];\n    if (t < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (t < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (t < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO: Implement me!\n}",
            "//TODO\n  \n}",
            "__shared__ size_t temp[4];\n\n    // Initialize shared memory\n    if (threadIdx.x < 4) {\n        temp[threadIdx.x] = 0;\n    }\n\n    __syncthreads();\n\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double d = x[i];\n        int f = (int) (d * 4) % 4;\n        atomicAdd(&temp[f], 1);\n    }\n\n    __syncthreads();\n\n    if (threadIdx.x < 4) {\n        atomicAdd(&bins[threadIdx.x], temp[threadIdx.x]);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        bins[int(x[i] * 4) + 1]++;\n    }\n}",
            "int tid = threadIdx.x;\n    int stride = blockDim.x;\n\n    for (int i = tid; i < N; i += stride) {\n        if (x[i] >= 0 && x[i] < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (x[i] >= 0.75 && x[i] < 1)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// YOUR CODE HERE\n\n}",
            "int i = blockDim.x*blockIdx.x + threadIdx.x;\n  if(i < N){\n    double f = x[i] - floor(x[i]);\n    if(f >= 0.75)\n      atomicAdd(&bins[3], 1);\n    else if(f >= 0.5)\n      atomicAdd(&bins[2], 1);\n    else if(f >= 0.25)\n      atomicAdd(&bins[1], 1);\n    else\n      atomicAdd(&bins[0], 1);\n  }\n}",
            "// Add your code here\n}",
            "// Your code here...\n\n  // The kernel is launched with at least N threads\n  // each thread is assigned to work with x[thread_id]\n  // bins[i] = number of numbers in the ith quarter\n\n  int thread_id = blockIdx.x*blockDim.x + threadIdx.x;\n  if (thread_id >= N)\n    return;\n\n  // each thread works with a number x[thread_id]\n  int q1_idx = 0, q2_idx = 0, q3_idx = 0, q4_idx = 0;\n  if (x[thread_id] < 0.25)\n    q1_idx += 1;\n  else if (x[thread_id] < 0.5)\n    q2_idx += 1;\n  else if (x[thread_id] < 0.75)\n    q3_idx += 1;\n  else\n    q4_idx += 1;\n  atomicAdd(&(bins[q1_idx]), 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) return;\n  int x_int = (int)(x[tid] * 4);\n  atomicAdd(&bins[x_int], 1);\n\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  double remainder;\n\n  if (index < N) {\n    remainder = modf(x[index], &x[index]);\n    if (remainder < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (remainder < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (remainder < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  for (; index < N; index += stride) {\n    double fractional = x[index] - floor(x[index]);\n    // TODO:\n    // Add the thread index to the correct bin depending on the fractional value.\n  }\n}",
            "int global_index = blockIdx.x * blockDim.x + threadIdx.x;\n  int count = 0;\n  int count1 = 0;\n  int count2 = 0;\n  int count3 = 0;\n  int count4 = 0;\n  while (global_index < N) {\n    double x_fract = fracpart(x[global_index]);\n    if (x_fract >= 0 && x_fract < 0.25) {\n      count++;\n    }\n    else if (x_fract >= 0.25 && x_fract < 0.5) {\n      count1++;\n    }\n    else if (x_fract >= 0.5 && x_fract < 0.75) {\n      count2++;\n    }\n    else if (x_fract >= 0.75 && x_fract < 1) {\n      count3++;\n    }\n    else if (x_fract >= 1) {\n      count4++;\n    }\n    global_index += blockDim.x * gridDim.x;\n  }\n  atomicAdd(&bins[0], count);\n  atomicAdd(&bins[1], count1);\n  atomicAdd(&bins[2], count2);\n  atomicAdd(&bins[3], count3);\n  atomicAdd(&bins[4], count4);\n}",
            "/* YOUR CODE HERE */\n}",
            "__shared__ size_t s_bins[4];\n  __shared__ double s_temp[THREADS_PER_BLOCK];\n  size_t thid = threadIdx.x + blockIdx.x * blockDim.x;\n  double x_val;\n\n  if (thid < N) {\n    x_val = x[thid];\n    int pos = (int)floor(x_val * 4);\n    pos = pos < 4? pos : 3;\n    s_temp[threadIdx.x] = x_val;\n    __syncthreads();\n    atomicAdd(&s_bins[pos], 1);\n    __syncthreads();\n    if (threadIdx.x == 0)\n      x[thid] = s_temp[threadIdx.x];\n  }\n\n  if (threadIdx.x == 0)\n    for (int i = 0; i < 4; i++)\n      atomicAdd(&bins[i], s_bins[i]);\n}",
            "// Add your code here\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < N) {\n        double d = x[id] - floor(x[id]);\n        int bin = d < 0.25? 0 : d < 0.5? 1 : d < 0.75? 2 : 3;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double xf = (x[tid] - floor(x[tid])) * 4;\n        atomicAdd(&bins[0], xf < 1? 1 : 0);\n        atomicAdd(&bins[1], xf < 2 && xf >= 1? 1 : 0);\n        atomicAdd(&bins[2], xf < 3 && xf >= 2? 1 : 0);\n        atomicAdd(&bins[3], xf < 4 && xf >= 3? 1 : 0);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      // TODO: count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in bins.\n   }\n}",
            "}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  int c;\n  double mod = fmod(x[tid], 1.0);\n  if (mod >= 0.0 && mod < 0.25) {\n    c = 0;\n  } else if (mod >= 0.25 && mod < 0.5) {\n    c = 1;\n  } else if (mod >= 0.5 && mod < 0.75) {\n    c = 2;\n  } else if (mod >= 0.75 && mod < 1.0) {\n    c = 3;\n  }\n  atomicAdd(&bins[c], 1);\n}",
            "}",
            "__shared__ size_t sh[4];\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // initialize sh to zero\n  sh[0] = 0;\n  sh[1] = 0;\n  sh[2] = 0;\n  sh[3] = 0;\n\n  // check if this thread is valid\n  if (idx < N) {\n    double frac = x[idx] - floor(x[idx]);\n\n    if (frac < 0.25) {\n      // count the number of values in the quarter [0, 0.25)\n      atomicAdd(&sh[0], 1);\n    } else if (frac < 0.5) {\n      // count the number of values in the quarter [0.25, 0.5)\n      atomicAdd(&sh[1], 1);\n    } else if (frac < 0.75) {\n      // count the number of values in the quarter [0.5, 0.75)\n      atomicAdd(&sh[2], 1);\n    } else {\n      // count the number of values in the quarter [0.75, 1)\n      atomicAdd(&sh[3], 1);\n    }\n  }\n\n  // perform the reduction in the shared memory to get the counts for each quarter\n  __syncthreads();\n  reduce(sh, 4);\n\n  // move the result back to the `bins` array\n  if (threadIdx.x == 0) {\n    bins[0] = sh[0];\n    bins[1] = sh[1];\n    bins[2] = sh[2];\n    bins[3] = sh[3];\n  }\n}",
            "size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (gid < N) {\n\n        // Fill in this part\n        if (x[gid] >= 0 && x[gid] < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[gid] >= 0.25 && x[gid] < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[gid] >= 0.5 && x[gid] < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (x[gid] >= 0.75 && x[gid] <= 1.0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  double val = x[i];\n  int bin = 0;\n  if (val >= 0.0 && val < 0.25) bin = 0;\n  else if (val >= 0.25 && val < 0.5) bin = 1;\n  else if (val >= 0.5 && val < 0.75) bin = 2;\n  else if (val >= 0.75 && val <= 1.0) bin = 3;\n\n  atomicAdd(&bins[bin], 1);\n}",
            "// Your code here\n\n}",
            "size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (index < N) {\n        int quater = ((int) (x[index] * 4) % 4);\n        atomicAdd(&bins[quater], 1);\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx >= N) return;\n\n    double fraction = x[idx] - (int)x[idx];\n    if (fraction < 0.25) {\n        atomicAdd(&bins[0], 1);\n    } else if (fraction < 0.5) {\n        atomicAdd(&bins[1], 1);\n    } else if (fraction < 0.75) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "}",
            "/* TODO */\n}",
            "const int i = threadIdx.x + blockDim.x * blockIdx.x;\n\n  // replace this with your implementation\n  if (i < N) {\n    const double frac = x[i] - (int)x[i];\n    if (frac < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (frac < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (frac < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double fraction = x[tid] - floor(x[tid]);\n        if (fraction < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (fraction < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (fraction < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "}",
            "// Your code goes here\n}",
            "/* Your code here */\n}",
            "}",
            "__shared__ double q1; // 0.25\n  __shared__ double q2; // 0.5\n  __shared__ double q3; // 0.75\n\n  if(threadIdx.x == 0) {\n    q1 = floor(0.25 * N);\n    q2 = floor(0.5 * N);\n    q3 = floor(0.75 * N);\n  }\n\n  __syncthreads();\n\n  if(blockIdx.x == 0 && threadIdx.x == 0) {\n    int i = 0;\n    for(i = 0; i < N; i++) {\n      if(x[i] < q1) {\n        atomicAdd(&bins[0], 1);\n      } else if(x[i] < q2) {\n        atomicAdd(&bins[1], 1);\n      } else if(x[i] < q3) {\n        atomicAdd(&bins[2], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    // TODO\n  }\n}",
            "// TODO\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N)\n    return;\n  int idx = int(x[tid] * 4);\n  if (idx > 3)\n    return;\n  atomicAdd(&(bins[idx]), 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        // TODO: determine which bin to increment\n    }\n}",
            "}",
            "/* insert code here */\n}",
            "}",
            "}",
            "// Fill in the code to calculate the number of elements in the array `x` that\n  // fall in each quartile of [0, 1)\n  // \n  // Example:\n  //\n  // input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  // output: [2, 1, 2, 2]\n\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N) return;\n  // Fill in your code here\n\n  // Count in which of the 4 bins the element belongs\n  int bin = 0;\n  if (x[index] < 0.25) bin = 0;\n  else if (x[index] < 0.5) bin = 1;\n  else if (x[index] < 0.75) bin = 2;\n  else bin = 3;\n\n  atomicAdd(&(bins[bin]), 1);\n}",
            "/* YOUR CODE HERE */\n}",
            "// get the index of the current thread\n  int i = threadIdx.x;\n  \n  if(i >= N) {\n    return;\n  }\n\n  double fractional = x[i] - floor(x[i]);\n  if (fractional >= 0.0 && fractional < 0.25) {\n    atomicAdd(&bins[0], 1);\n  }\n  else if (fractional >= 0.25 && fractional < 0.5) {\n    atomicAdd(&bins[1], 1);\n  }\n  else if (fractional >= 0.5 && fractional < 0.75) {\n    atomicAdd(&bins[2], 1);\n  }\n  else if (fractional >= 0.75 && fractional <= 1) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// thread index\n    size_t tid = threadIdx.x;\n    if(tid >= N) return;\n\n    // get the index of x[tid] in the array of 4\n    size_t ind = floor(x[tid]*4);\n    assert(ind < 4);\n    atomicAdd(&bins[ind], 1);\n}",
            "// Replace the following with your code.\n\tint index = threadIdx.x;\n\tdouble value = x[index];\n\tdouble fraction = fmod(value, 1.0);\n\tif(fraction < 0.25) {\n\t\tatomicAdd(&bins[0], 1);\n\t}\n\tif(fraction >= 0.25 && fraction < 0.50) {\n\t\tatomicAdd(&bins[1], 1);\n\t}\n\tif(fraction >= 0.50 && fraction < 0.75) {\n\t\tatomicAdd(&bins[2], 1);\n\t}\n\tif(fraction >= 0.75 && fraction < 1) {\n\t\tatomicAdd(&bins[3], 1);\n\t}\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      bins[0] += ((int) (x[i] * 4)) % 4 == 0;\n      bins[1] += ((int) (x[i] * 4)) % 4 == 1;\n      bins[2] += ((int) (x[i] * 4)) % 4 == 2;\n      bins[3] += ((int) (x[i] * 4)) % 4 == 3;\n   }\n}",
            "// The code below is a template for a kernel.\n   // Replace the lines between the comments with your own code.\n   // You can use the following functions:\n   // - atomicAdd(int* address, int val)\n   // - atomicAdd(unsigned long long int* address, unsigned long long int val)\n   // - atomicAdd(unsigned int* address, unsigned int val)\n\n   // Replace with your code\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= N) return;\n    int idx = 4 * ((int)floor(x[i]));\n    atomicAdd(&bins[idx], 1);\n}",
            "const int tid = threadIdx.x;\n  __shared__ size_t sdata[NUM_THREADS];\n\n  // initialize sdata shared memory\n  if (tid < NUM_THREADS) {\n    sdata[tid] = 0;\n  }\n  __syncthreads();\n\n  // sum quartile counts\n  for (int i = tid; i < N; i += NUM_THREADS) {\n    // if the fractional part of x[i] is in [0, 0.25)\n    if (x[i] - (int)x[i] >= 0 && x[i] - (int)x[i] < 0.25) {\n      atomicAdd(&sdata[0], 1);\n    }\n    // if the fractional part of x[i] is in [0.25, 0.5)\n    else if (x[i] - (int)x[i] >= 0.25 && x[i] - (int)x[i] < 0.5) {\n      atomicAdd(&sdata[1], 1);\n    }\n    // if the fractional part of x[i] is in [0.5, 0.75)\n    else if (x[i] - (int)x[i] >= 0.5 && x[i] - (int)x[i] < 0.75) {\n      atomicAdd(&sdata[2], 1);\n    }\n    // if the fractional part of x[i] is in [0.75, 1)\n    else if (x[i] - (int)x[i] >= 0.75 && x[i] - (int)x[i] < 1) {\n      atomicAdd(&sdata[3], 1);\n    }\n  }\n  __syncthreads();\n\n  // copy sdata to bins\n  if (tid < NUM_THREADS) {\n    bins[tid] = sdata[tid];\n  }\n  __syncthreads();\n}",
            "// TODO: Your code here\n\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if(idx < N){\n    int quartile = 0;\n    if(x[idx] >= 0 && x[idx] < 0.25){\n      quartile = 0;\n    }\n    else if(x[idx] >= 0.25 && x[idx] < 0.5){\n      quartile = 1;\n    }\n    else if(x[idx] >= 0.5 && x[idx] < 0.75){\n      quartile = 2;\n    }\n    else if(x[idx] >= 0.75 && x[idx] <= 1){\n      quartile = 3;\n    }\n\n    atomicAdd(&bins[quartile], 1);\n  }\n\n}",
            "// TODO: Fill in the CUDA kernel code to count the number of\n    // elements of `x` in each quartile. You will need to use an atomic\n    // to increment the correct element in `bins`.\n    //\n    // Use the atomic functions from `cuda.h`, in particular:\n    //   - atomicAdd(uint64_t *address, uint64_t val)\n    //   - atomicAdd(int *address, int val)\n    //   - atomicAdd(unsigned int *address, unsigned int val)\n    //   - atomicCAS(int *address, int compare, int val)\n\n    // Your code here\n\n    // If you don't need to use atomic functions, you can also use\n    // atomic_ref for a more convenient syntax:\n    //   - atomic_ref(bins[0])++;\n    //   - atomic_ref(bins[1])++;\n    //   - atomic_ref(bins[2])++;\n    //   - atomic_ref(bins[3])++;\n}",
            "}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (idx >= N) {\n    return;\n  }\n  double d = x[idx];\n  if (d >= 0 && d < 1) {\n    atomicAdd(&bins[0], 1);\n  } else if (d >= 1 && d < 2) {\n    atomicAdd(&bins[1], 1);\n  } else if (d >= 2 && d < 3) {\n    atomicAdd(&bins[2], 1);\n  } else if (d >= 3 && d < 4) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        double v = x[i];\n        int lower = floor(v);\n        double diff = v - lower;\n        if (diff < 0.25)\n            bins[0]++;\n        else if (diff < 0.5)\n            bins[1]++;\n        else if (diff < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "/* TODO: Your code here */\n  \n}",
            "// TODO: fill in this kernel function\n}",
            "__shared__ double shared[32];\n  __shared__ size_t sh_bins[4];\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) {\n    return;\n  }\n  double d = x[i];\n  double fractional_part = d - floor(d);\n  // Your code here\n  if (threadIdx.x == 0) {\n    sh_bins[0] = sh_bins[1] = sh_bins[2] = sh_bins[3] = 0;\n  }\n  __syncthreads();\n  if (fractional_part < 0.25) {\n    atomicAdd(&sh_bins[0], 1);\n  } else if (fractional_part < 0.5) {\n    atomicAdd(&sh_bins[1], 1);\n  } else if (fractional_part < 0.75) {\n    atomicAdd(&sh_bins[2], 1);\n  } else if (fractional_part <= 1) {\n    atomicAdd(&sh_bins[3], 1);\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], sh_bins[0]);\n    atomicAdd(&bins[1], sh_bins[1]);\n    atomicAdd(&bins[2], sh_bins[2]);\n    atomicAdd(&bins[3], sh_bins[3]);\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // TODO\n    //if ((x[idx] - (int)x[idx]) >= 0 && (x[idx] - (int)x[idx]) < 0.25)\n    //    atomicAdd(&bins[0], 1);\n    //if ((x[idx] - (int)x[idx]) >= 0.25 && (x[idx] - (int)x[idx]) < 0.5)\n    //    atomicAdd(&bins[1], 1);\n    //if ((x[idx] - (int)x[idx]) >= 0.5 && (x[idx] - (int)x[idx]) < 0.75)\n    //    atomicAdd(&bins[2], 1);\n    //if ((x[idx] - (int)x[idx]) >= 0.75)\n    //    atomicAdd(&bins[3], 1);\n    //\n    //return;\n\n    int x_int = (int)x[idx];\n    double x_frac = x[idx] - x_int;\n    if(x_frac >= 0 && x_frac < 0.25)\n        atomicAdd(&bins[0], 1);\n    if(x_frac >= 0.25 && x_frac < 0.5)\n        atomicAdd(&bins[1], 1);\n    if(x_frac >= 0.5 && x_frac < 0.75)\n        atomicAdd(&bins[2], 1);\n    if(x_frac >= 0.75)\n        atomicAdd(&bins[3], 1);\n}",
            "const size_t id = threadIdx.x + blockDim.x*blockIdx.x;\n\n  // TODO: add code here.\n}",
            "// ******* TODO: Implement this function. *******\n  // ******* You may use CUDA intrinsics. *******\n  //\n  // Each thread computes the quartiles of a subset of x. \n  // The size of the subset depends on the block size.\n\n  // Define the shared memory array to store the local values of x\n  __shared__ double sharedX[BLOCK_SIZE];\n\n  // Store the local values of x in the shared memory\n  int index = threadIdx.x + blockDim.x * blockIdx.x;\n  sharedX[threadIdx.x] = (index < N)? x[index] : 0.0;\n\n  // Sync the threads in the block to make sure all the values in sharedX have been stored\n  __syncthreads();\n\n  // Compute the quartiles\n  if (index < N) {\n    if (sharedX[threadIdx.x] >= 0 && sharedX[threadIdx.x] < 0.25)\n      atomicAdd(bins + 0, 1);\n    else if (sharedX[threadIdx.x] >= 0.25 && sharedX[threadIdx.x] < 0.5)\n      atomicAdd(bins + 1, 1);\n    else if (sharedX[threadIdx.x] >= 0.5 && sharedX[threadIdx.x] < 0.75)\n      atomicAdd(bins + 2, 1);\n    else if (sharedX[threadIdx.x] >= 0.75 && sharedX[threadIdx.x] <= 1.0)\n      atomicAdd(bins + 3, 1);\n  }\n\n  // Sync the threads in the block to make sure all the values in bins have been added\n  __syncthreads();\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double frac = x[i] - floor(x[i]);\n        int bucket = 0;\n        if (frac < 0.25) {\n            bucket = 0;\n        }\n        else if (frac < 0.5) {\n            bucket = 1;\n        }\n        else if (frac < 0.75) {\n            bucket = 2;\n        }\n        else {\n            bucket = 3;\n        }\n        atomicAdd(&bins[bucket], 1);\n    }\n}",
            "/* TODO: complete this kernel to count the number of elements in each of the\n            four quartiles.\n\n            The count for a given quartile should be stored in the index of the\n            bins array corresponding to that quartile.\n\n            N.B. the quartiles are numbered as follows:\n            0th quartile (0.0 <= x < 0.25)\n            1st quartile (0.25 <= x < 0.50)\n            2nd quartile (0.50 <= x < 0.75)\n            3rd quartile (0.75 <= x < 1.00)\n\n            Remember that a threadblock is a set of threads with the same index\n            space, while a grid of threadblocks is a set of threadblocks with\n            the same index space. \n\n            For this kernel, the grid has a single block, so it is a 1D index\n            space with N threads. The index of the thread can be accessed using\n            the builtin variable `threadIdx.x`\n\n            HINT: You can use an if-statement to check if a value `x` is in a\n                  given quartile, and then use atomicAdd() to add 1 to the\n                  corresponding bin.\n  */\n   int idx = threadIdx.x;\n   if (idx < N) {\n       if (x[idx] >= 0.0 && x[idx] < 0.25) {\n           atomicAdd(&(bins[0]), 1);\n       } else if (x[idx] >= 0.25 && x[idx] < 0.50) {\n           atomicAdd(&(bins[1]), 1);\n       } else if (x[idx] >= 0.50 && x[idx] < 0.75) {\n           atomicAdd(&(bins[2]), 1);\n       } else if (x[idx] >= 0.75 && x[idx] < 1.00) {\n           atomicAdd(&(bins[3]), 1);\n       }\n   }\n}",
            "// Compute the index of this thread in the vector x.\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N)\n        return;\n\n    // TODO: your code here.\n    // You may use floating-point arithmetic.\n    // Be careful to check for overflow!\n\n    // The following code is an example.\n    // You can use it to test your code, but you should remove it\n    // and write your own code.\n\n    /*\n    if (0.0 <= x[idx] && x[idx] < 0.25)\n        atomicAdd(&bins[0], 1);\n    else if (0.25 <= x[idx] && x[idx] < 0.5)\n        atomicAdd(&bins[1], 1);\n    else if (0.5 <= x[idx] && x[idx] < 0.75)\n        atomicAdd(&bins[2], 1);\n    else if (0.75 <= x[idx] && x[idx] < 1.0)\n        atomicAdd(&bins[3], 1);\n    */\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // TODO: Implement\n  if(i < N){\n    if(x[i] < 0.25){\n      atomicAdd(&bins[0], 1);\n    }\n    else if(x[i] < 0.5){\n      atomicAdd(&bins[1], 1);\n    }\n    else if(x[i] < 0.75){\n      atomicAdd(&bins[2], 1);\n    }\n    else{\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int idx = threadIdx.x;\n  __shared__ size_t s[4];\n  __syncthreads();\n\n  // Fill the shared memory.\n  s[0] = 0;\n  s[1] = 0;\n  s[2] = 0;\n  s[3] = 0;\n\n  __syncthreads();\n\n  if (idx < N) {\n    double f = x[idx] - floor(x[idx]);\n    if (f >= 0 && f < 0.25) {\n      s[0] = s[0] + 1;\n    } else if (f >= 0.25 && f < 0.5) {\n      s[1] = s[1] + 1;\n    } else if (f >= 0.5 && f < 0.75) {\n      s[2] = s[2] + 1;\n    } else if (f >= 0.75 && f < 1) {\n      s[3] = s[3] + 1;\n    }\n  }\n  __syncthreads();\n\n  // Add up the shared memory contents.\n  if (idx < 4) {\n    for (int i = 1; i < blockDim.x; ++i) {\n      s[idx] += s[i + idx];\n    }\n    bins[idx] = s[idx];\n  }\n\n  // printf(\"threadIdx.x: %d, bins[0]: %lu, bins[1]: %lu, bins[2]: %lu, bins[3]: %lu\\n\", threadIdx.x, bins[0], bins[1], bins[2], bins[3]);\n}",
            "// TODO: implement\n}",
            "// Write your code here.\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        const double xx = x[i] - floor(x[i]);\n        int n = xx * 4.0;\n        atomicAdd(&bins[n], 1);\n    }\n}",
            "__shared__ size_t shared_bins[4];\n  shared_bins[0] = 0;\n  shared_bins[1] = 0;\n  shared_bins[2] = 0;\n  shared_bins[3] = 0;\n\n  int tid = threadIdx.x;\n\n  // This loop assumes that blockDim.x >= N.\n  // It will only be executed once.\n  for (int i = tid; i < N; i += blockDim.x) {\n    if (x[i] < 0.25) {\n      atomicAdd(&shared_bins[0], 1);\n    } else if (x[i] < 0.5) {\n      atomicAdd(&shared_bins[1], 1);\n    } else if (x[i] < 0.75) {\n      atomicAdd(&shared_bins[2], 1);\n    } else {\n      atomicAdd(&shared_bins[3], 1);\n    }\n  }\n\n  // Wait until all threads finish their counting.\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    bins[0] = shared_bins[0];\n    bins[1] = shared_bins[1];\n    bins[2] = shared_bins[2];\n    bins[3] = shared_bins[3];\n  }\n}",
            "/* Insert code here */\n}",
            "}",
            "}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  double fraction = x[index];\n  fraction = fmod(fraction, 1);\n  if (fraction < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (fraction < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (fraction < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx >= N) return;\n  double frac = x[idx] - floor(x[idx]);\n  if (frac >= 0.25 && frac < 0.5) {\n    atomicAdd(&(bins[1]), 1);\n  } else if (frac >= 0.5 && frac < 0.75) {\n    atomicAdd(&(bins[2]), 1);\n  } else if (frac >= 0.75 && frac < 1.0) {\n    atomicAdd(&(bins[3]), 1);\n  } else {\n    atomicAdd(&(bins[0]), 1);\n  }\n}",
            "// TODO: implement this kernel using atomics (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions)\n\n  // TODO: remove this once you have implemented your kernel\n  for (size_t i = 0; i < N; i++) {\n    if (x[i] >= 0 && x[i] < 0.25) bins[0]++;\n    if (x[i] >= 0.25 && x[i] < 0.5) bins[1]++;\n    if (x[i] >= 0.5 && x[i] < 0.75) bins[2]++;\n    if (x[i] >= 0.75 && x[i] < 1) bins[3]++;\n  }\n}",
            "// Your code here!\n}",
            "// TODO\n}",
            "__shared__ size_t sharedBins[4];\n  __shared__ bool sharedBinFinished[4];\n\n  if(threadIdx.x == 0) {\n    sharedBins[0] = 0;\n    sharedBins[1] = 0;\n    sharedBins[2] = 0;\n    sharedBins[3] = 0;\n    sharedBinFinished[0] = false;\n    sharedBinFinished[1] = false;\n    sharedBinFinished[2] = false;\n    sharedBinFinished[3] = false;\n  }\n\n  __syncthreads();\n\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  for(; index < N; index += stride) {\n    double remainder = fmod(x[index], 1.0);\n    if(remainder >= 0.75 &&!sharedBinFinished[3]) {\n      atomicAdd(&sharedBins[3], 1);\n    } else if(remainder >= 0.5 &&!sharedBinFinished[2]) {\n      atomicAdd(&sharedBins[2], 1);\n    } else if(remainder >= 0.25 &&!sharedBinFinished[1]) {\n      atomicAdd(&sharedBins[1], 1);\n    } else if(!sharedBinFinished[0]) {\n      atomicAdd(&sharedBins[0], 1);\n    }\n  }\n\n  __syncthreads();\n\n  atomicAdd(&bins[0], sharedBins[0]);\n  atomicAdd(&bins[1], sharedBins[1]);\n  atomicAdd(&bins[2], sharedBins[2]);\n  atomicAdd(&bins[3], sharedBins[3]);\n\n  __syncthreads();\n\n  if(threadIdx.x == 0) {\n    sharedBinFinished[0] = true;\n    sharedBinFinished[1] = true;\n    sharedBinFinished[2] = true;\n    sharedBinFinished[3] = true;\n  }\n\n  __syncthreads();\n\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i >= N) return;\n\n  double val = (double) (int) x[i];\n  if (val == x[i]) {\n    if (val >= 0 && val < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (val >= 0.25 && val < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (val >= 0.5 && val < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (val >= 0.75 && val < 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int tId = threadIdx.x;\n    int nthreads = gridDim.x * blockDim.x;\n    int start = blockIdx.x * blockDim.x;\n    int end = fmin(start + blockDim.x, N);\n    __shared__ size_t binsShared[4];\n    __shared__ double binsSharedD[4];\n\n    if (tId == 0) {\n        binsShared[0] = 0;\n        binsShared[1] = 0;\n        binsShared[2] = 0;\n        binsShared[3] = 0;\n    }\n\n    // Loop over the vector x\n    for (int i = start; i < end; ++i) {\n        double remainder = (x[i] - floor(x[i]));\n        // remainder = fmod(x[i], 1);\n        // remainder = x[i] - int(x[i]);\n        if (remainder >= 0 && remainder < 0.25) {\n            atomicAdd(&binsShared[0], 1);\n        } else if (remainder >= 0.25 && remainder < 0.5) {\n            atomicAdd(&binsShared[1], 1);\n        } else if (remainder >= 0.5 && remainder < 0.75) {\n            atomicAdd(&binsShared[2], 1);\n        } else if (remainder >= 0.75 && remainder < 1) {\n            atomicAdd(&binsShared[3], 1);\n        }\n    }\n\n    __syncthreads();\n    if (tId == 0) {\n        bins[0] = binsShared[0];\n        bins[1] = binsShared[1];\n        bins[2] = binsShared[2];\n        bins[3] = binsShared[3];\n    }\n}",
            "int i = threadIdx.x;\n  double q[4] = {0, 0.25, 0.5, 0.75};\n  // count the number of values in each quarter\n  for (size_t j = 0; j < 4; j++) {\n    bins[j] = 0;\n  }\n\n  if (i < N) {\n    // TODO: compute the quarter of each x[i] and store in bins\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n     double f = x[tid] - floor(x[tid]);\n     if (f < 0.25) bins[0]++;\n     else if (f < 0.5) bins[1]++;\n     else if (f < 0.75) bins[2]++;\n     else bins[3]++;\n   }\n}",
            "}",
            "// Your code here\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i >= N) return;\n\n  if (x[i] >= 0 && x[i] < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (x[i] >= 0.25 && x[i] < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (x[i] >= 0.5 && x[i] < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (x[i] >= 0.75 && x[i] < 1) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// Insert your code here.\n\n    // Find the bin for this thread's double.\n    // You can use a series of if statements.\n    // You can also use the modulo operator:\n    //   int binIdx = threadIdx.x % 4;\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    int frac_idx = 0;\n    double frac = fmod(x[idx], 1.0);\n    if (0 <= frac && frac < 0.25)\n        frac_idx = 0;\n    else if (0.25 <= frac && frac < 0.5)\n        frac_idx = 1;\n    else if (0.5 <= frac && frac < 0.75)\n        frac_idx = 2;\n    else\n        frac_idx = 3;\n\n    atomicAdd(&bins[frac_idx], 1);\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n  \n  if (idx >= N) return;\n  \n  // TODO\n  \n}",
            "// TODO: Implement this function.\n}",
            "// TODO: Write your kernel here. \n\n}",
            "}",
            "// TODO: implement\n}",
            "int thread_id = threadIdx.x + blockIdx.x*blockDim.x;\n  if (thread_id >= N)\n    return;\n  int bin_index = ((int)floor(x[thread_id]*4)) % 4;\n  atomicAdd(&bins[bin_index], 1);\n}",
            "int i = threadIdx.x;\n  if (i < N) {\n    double value = x[i];\n    value = fmod(value, 1.0);\n    if (value >= 0 && value < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (value >= 0.25 && value < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (value >= 0.5 && value < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (value >= 0.75 && value < 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (idx < N) {\n\t\tint pos = 4 * (int) floor(4 * x[idx]);\n\t\tatomicAdd(&bins[pos], 1);\n\t}\n}",
            "}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if (id < N) {\n    // TODO: implement\n  }\n}",
            "size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (thread_id < N) {\n        double x_i = x[thread_id];\n        if (x_i < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x_i >= 0.25 && x_i < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x_i >= 0.5 && x_i < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (x_id >= 0.75) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t i = threadIdx.x;\n  //...\n}",
            "// TODO\n}",
            "// This is the parallelization strategy. \n    //\n    // Use a grid of blocks. \n    // Each block is given 128 threads.\n    // Use the thread index in the block to determine the range of input elements to process.\n    //\n    // Example: If the input has 156 elements, the grid has 1 block. \n    // The block has 128 threads. \n    // The thread with thread index 0 will process elements 0, 1, 2, 3,..., 127.\n    // The thread with thread index 127 will process elements 128, 129, 130, 131,..., 155.\n\n    // TODO: Fill in this function\n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        double x_i = x[idx];\n        int q = 0;\n        if (x_i >= 0.75) {\n            q = 3;\n        } else if (x_i >= 0.5) {\n            q = 2;\n        } else if (x_i >= 0.25) {\n            q = 1;\n        }\n\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "// TODO: implement this function\n}",
            "// Implement me!\n}",
            "const int index = blockDim.x * blockIdx.x + threadIdx.x;\n\n\tif(index < N) {\n\t\tfloat xfloat = x[index];\n\t\tint quartile = (xfloat - (int)xfloat) * 4.0;\n\n\t\tif(quartile < 0.25)\n\t\t\tatomicAdd(&(bins[0]), 1);\n\t\telse if(quartile < 0.5)\n\t\t\tatomicAdd(&(bins[1]), 1);\n\t\telse if(quartile < 0.75)\n\t\t\tatomicAdd(&(bins[2]), 1);\n\t\telse if(quartile <= 1.0)\n\t\t\tatomicAdd(&(bins[3]), 1);\n\t}\n}",
            "// Implement\n}",
            "// TODO\n}",
            "// TODO: Fill this in\n}",
            "const int idx = threadIdx.x;\n\tdouble frac = 0;\n\tif(idx < N)\n\t\tfrac = x[idx] - (int)x[idx];\n\t__syncthreads();\n\n\tint res = 0;\n\tif(idx == 0) {\n\t\tif (frac >= 0 && frac < 0.25)\n\t\t\tres = 0;\n\t\telse if (frac >= 0.25 && frac < 0.5)\n\t\t\tres = 1;\n\t\telse if (frac >= 0.5 && frac < 0.75)\n\t\t\tres = 2;\n\t\telse if (frac >= 0.75 && frac <= 1)\n\t\t\tres = 3;\n\t}\n\tatomicAdd(&(bins[res]), 1);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  // TODO: Use double2 to get the fractional and integral part, and use the\n  // fractional part to compute the quartile bins.\n  __shared__ size_t bins_[4];\n  if (tid < N) {\n    double2 fract = double2 {x[tid] - floor(x[tid]), 0.0};\n    if (fract.x < 0.25) {\n      atomicAdd(bins_ + 0, 1);\n    } else if (fract.x < 0.5) {\n      atomicAdd(bins_ + 1, 1);\n    } else if (fract.x < 0.75) {\n      atomicAdd(bins_ + 2, 1);\n    } else {\n      atomicAdd(bins_ + 3, 1);\n    }\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], bins_[0]);\n    atomicAdd(&bins[1], bins_[1]);\n    atomicAdd(&bins[2], bins_[2]);\n    atomicAdd(&bins[3], bins_[3]);\n  }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[idx] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[idx] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[idx] <= 1.0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    const double xi = x[i];\n    const double frac = xi - floor(xi);\n    if (frac >= 0.75) {\n      atomicAdd(bins + 3, 1);\n    } else if (frac >= 0.5) {\n      atomicAdd(bins + 2, 1);\n    } else if (frac >= 0.25) {\n      atomicAdd(bins + 1, 1);\n    } else {\n      atomicAdd(bins, 1);\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (i < N) {\n        double quarter = x[i] - floor(x[i]);\n        if (quarter < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (quarter >= 0.25 && quarter < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (quarter >= 0.5 && quarter < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      const double x_i = x[i];\n      const int which_bin = 4 * (x_i - floor(x_i));\n      atomicAdd(&bins[which_bin], 1);\n   }\n}",
            "// TODO\n}",
            "// your code here\n    size_t i = threadIdx.x;\n    double x_i;\n\n    if(i<N) {\n        x_i = x[i];\n\n        if(x_i<=1 && x_i>0.75)\n            atomicAdd(&bins[0], 1);\n\n        else if(x_i>0.75 && x_i<=0.5)\n            atomicAdd(&bins[1], 1);\n\n        else if(x_i>0.5 && x_i<=0.25)\n            atomicAdd(&bins[2], 1);\n\n        else if(x_i>0.25 && x_i<=0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int stride = blockDim.x * gridDim.x;\n    while (index < N) {\n        size_t q = ((x[index] - floor(x[index])) * 4) + 0.5;\n        atomicAdd(&bins[q], 1);\n        index += stride;\n    }\n}",
            "int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n\n  double val = x[idx];\n  size_t qua = 0;\n  if (val > 0.75)\n    qua = 0;\n  else if (val > 0.5)\n    qua = 1;\n  else if (val > 0.25)\n    qua = 2;\n  else if (val > 0)\n    qua = 3;\n\n  atomicAdd(&bins[qua], 1);\n}",
            "size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (thread_id < N) {\n    double y = x[thread_id];\n    size_t f = frac(y);\n    atomicAdd(&bins[f], 1);\n  }\n}",
            "// Fill this in\n}",
            "}",
            "// TODO:\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  if (x[idx] >= 0.0 && x[idx] < 0.25) {\n    atomicAdd(&bins[0], 1);\n  }\n  else if (x[idx] >= 0.25 && x[idx] < 0.5) {\n    atomicAdd(&bins[1], 1);\n  }\n  else if (x[idx] >= 0.5 && x[idx] < 0.75) {\n    atomicAdd(&bins[2], 1);\n  }\n  else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if(i < N) {\n        const double y = x[i] - (double)((int)x[i]);\n        if(y < 0.25) bins[0]++;\n        else if(y < 0.5) bins[1]++;\n        else if(y < 0.75) bins[2]++;\n        else if(y < 1.0) bins[3]++;\n    }\n}",
            "// Initialize all 4 bins to 0.\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  // Loop through `x` and increment the appropriate bin.\n  // Make sure that all threads are synchronized!\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (x[i] < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    int t = 0;\n    if(i < N){\n        double q1 = x[i] - int(x[i]);\n        if(q1 >= 0.0 && q1 <= 0.25){\n            t = 0;\n        }\n        if(q1 >= 0.25 && q1 <= 0.5){\n            t = 1;\n        }\n        if(q1 >= 0.5 && q1 <= 0.75){\n            t = 2;\n        }\n        if(q1 >= 0.75 && q1 <= 1.0){\n            t = 3;\n        }\n    }\n    atomicAdd(&bins[t], 1);\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  if (x[i] < 0.25) bins[0]++;\n  else if (x[i] < 0.5) bins[1]++;\n  else if (x[i] < 0.75) bins[2]++;\n  else if (x[i] < 1.0) bins[3]++;\n}",
            "}",
            "int x_idx = blockIdx.x * blockDim.x + threadIdx.x;\n\t//int y_idx = blockIdx.y * blockDim.y + threadIdx.y;\n\n\tif (x_idx < N) {\n\t\tdouble x_val = x[x_idx];\n\t\tdouble val = x_val - (int) x_val;\n\t\tint bin = 0;\n\t\tif (val < 0.25) bin = 0;\n\t\telse if (val < 0.5) bin = 1;\n\t\telse if (val < 0.75) bin = 2;\n\t\telse bin = 3;\n\n\t\tatomicAdd(&bins[bin], 1);\n\t}\n\n\t\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    // Write the code to compute the fractional part of x[i]\n    // If the fractional part is in [0, 0.25) then set bins[0]\n    // If the fractional part is in [0.25, 0.5) then set bins[1]\n    // If the fractional part is in [0.5, 0.75) then set bins[2]\n    // If the fractional part is in [0.75, 1) then set bins[3]\n  }\n}",
            "/* You need to compute the number of values in [0, 0.25), [0.25, 0.5),..., [0.75, 1) \n   */\n}",
            "// TODO: Complete this function\n}",
            "}",
            "int globalIdx = blockIdx.x * blockDim.x + threadIdx.x;\n  __shared__ size_t partialBins[4];\n  partialBins[0] = 0;\n  partialBins[1] = 0;\n  partialBins[2] = 0;\n  partialBins[3] = 0;\n\n  for (size_t i = globalIdx; i < N; i += gridDim.x * blockDim.x) {\n    size_t quartile = x[i] - floor(x[i]);\n    atomicAdd(&partialBins[quartile], 1);\n  }\n\n  __syncthreads();\n  atomicAdd(&bins[0], partialBins[0]);\n  atomicAdd(&bins[1], partialBins[1]);\n  atomicAdd(&bins[2], partialBins[2]);\n  atomicAdd(&bins[3], partialBins[3]);\n}",
            "//...\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  const size_t n_per_rank = x.size() / size;\n  const size_t n_remainder = x.size() % size;\n\n  std::vector<size_t> bins_per_rank(4);\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&bins_per_rank, 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; j++) bins[j] += bins_per_rank[j];\n    }\n  } else {\n    std::vector<double> local_vector(n_per_rank + (rank <= n_remainder? 1 : 0));\n    #pragma omp parallel for\n    for (size_t i = 0; i < n_per_rank + (rank <= n_remainder? 1 : 0); i++) {\n      local_vector[i] = x[i * size + rank];\n    }\n    for (size_t i = 0; i < 4; i++) bins_per_rank[i] = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < n_per_rank + (rank <= n_remainder? 1 : 0); i++) {\n      if (local_vector[i] >= 0.0 && local_vector[i] < 0.25) bins_per_rank[0]++;\n      else if (local_vector[i] >= 0.25 && local_vector[i] < 0.5) bins_per_rank[1]++;\n      else if (local_vector[i] >= 0.5 && local_vector[i] < 0.75) bins_per_rank[2]++;\n      else if (local_vector[i] >= 0.75 && local_vector[i] < 1.0) bins_per_rank[3]++;\n    }\n    MPI_Send(&bins_per_rank, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int size = x.size();\n    int rank = 0, num_ranks = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    std::array<size_t, 4> temp{};\n    #pragma omp parallel for default(none) shared(temp, x, rank, size, num_ranks)\n    for (int i = 0; i < size; ++i) {\n        if (0 <= x[i] && x[i] < 0.25)\n            temp[0] += 1;\n        else if (0.25 <= x[i] && x[i] < 0.5)\n            temp[1] += 1;\n        else if (0.5 <= x[i] && x[i] < 0.75)\n            temp[2] += 1;\n        else if (0.75 <= x[i])\n            temp[3] += 1;\n    }\n\n    std::vector<size_t> counts(num_ranks, 0);\n    std::vector<size_t> displs(num_ranks, 0);\n    counts[rank] = temp.size();\n    MPI_Gather(&counts[rank], 1, MPI_INT, &counts[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 1; i < num_ranks; ++i)\n            displs[i] = displs[i-1] + counts[i-1];\n    }\n    MPI_Gatherv(&temp[0], temp.size(), MPI_INT, &bins[0], &counts[0], &displs[0], MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int const rank = omp_get_thread_num();\n\tint const size = omp_get_num_threads();\n\tstd::array<size_t, 4> bins_local;\n\n\tsize_t start = rank * x.size() / size;\n\tsize_t end = (rank + 1) * x.size() / size;\n\tfor (size_t i = start; i < end; ++i) {\n\t\tif (x[i] >= 0 && x[i] < 0.25)\n\t\t\tbins_local[0]++;\n\t\telse if (x[i] >= 0.25 && x[i] < 0.5)\n\t\t\tbins_local[1]++;\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75)\n\t\t\tbins_local[2]++;\n\t\telse if (x[i] >= 0.75 && x[i] < 1)\n\t\t\tbins_local[3]++;\n\t}\n\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < 4; ++i) {\n\t\t\tbins[i] = 0;\n\t\t\tfor (int j = 0; j < size; ++j) {\n\t\t\t\tbins[i] += bins_local[i];\n\t\t\t}\n\t\t}\n\t}\n}",
            "int numprocs, rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n  // TODO: use MPI to distribute the vector among processes\n  // TODO: use OpenMP to parallelize this loop over `x`\n  // TODO: use MPI to combine the results from all processes\n  for (size_t i = 0; i < x.size(); ++i) {\n    // TODO: your code here\n  }\n\n  if (rank == 0) {\n    // TODO: combine the `bins` vectors from all processes\n  }\n}",
            "// TODO\n}",
            "int size, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int local_size = x.size()/size;\n    int remaining = x.size()%size;\n\n    std::vector<double> local_x(local_size);\n\n    if (rank!= 0)\n        MPI_Send(&x.at(0), local_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    else\n    {\n        for(int i = 0; i < size-1; i++)\n        {\n            MPI_Recv(&local_x.at(0), local_size, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins = local_x;\n        }\n    }\n\n    MPI_Recv(&local_x.at(0), local_size+remaining, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    double* ptr = new double[local_x.size()];\n\n    for(int i = 0; i < local_x.size(); i++)\n    {\n        double y = (local_x.at(i) - floor(local_x.at(i)))*4;\n\n        if(y < 1)\n            ptr[i] = 0;\n        else if(y < 2)\n            ptr[i] = 1;\n        else if(y < 3)\n            ptr[i] = 2;\n        else\n            ptr[i] = 3;\n    }\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for(int i = 0; i < local_x.size(); i++)\n    {\n        #pragma omp atomic\n        local_bins[ptr[i]] += 1;\n    }\n\n    if(rank!= 0)\n        MPI_Send(&local_bins.at(0), 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    else\n    {\n        for(int i = 0; i < size-1; i++)\n        {\n            MPI_Recv(&local_bins.at(0), 4, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins = local_bins;\n        }\n    }\n\n    MPI_Recv(&local_bins.at(0), 4, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    for(int i = 0; i < 4; i++)\n    {\n        #pragma omp atomic\n        bins[i] += local_bins[i];\n    }\n\n    delete ptr;\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this\n}",
            "int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n\n        int const n = x.size();\n        int const n_per_proc = n / size;\n        int const rem = n % size;\n        int const start = rank * n_per_proc + std::min(rank, rem);\n        int const end = start + n_per_proc + (rank < rem? 1 : 0);\n        std::vector<double> local_x = std::vector<double>(x.begin() + start, x.begin() + end);\n\n        #pragma omp parallel for\n        for (int i = 0; i < 4; i++) {\n            for (auto const &x_i : local_x) {\n                if (x_i > i * 0.25 && x_i <= (i + 1) * 0.25) {\n                    bins[i]++;\n                }\n            }\n        }\n    } else {\n        int const n = x.size();\n        int const n_per_proc = n / size;\n        int const rem = n % size;\n        int const start = rank * n_per_proc + std::min(rank, rem);\n        int const end = start + n_per_proc + (rank < rem? 1 : 0);\n        std::vector<double> local_x = std::vector<double>(x.begin() + start, x.begin() + end);\n\n        #pragma omp parallel for\n        for (int i = 0; i < 4; i++) {\n            for (auto const &x_i : local_x) {\n                if (x_i > i * 0.25 && x_i <= (i + 1) * 0.25) {\n                    bins[i]++;\n                }\n            }\n        }\n        MPI_Send(bins.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank!= 0) {\n        MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n    // Hint: consider using OpenMP and `omp_get_num_threads()`, `omp_get_thread_num()`,\n    // and `omp_get_num_procs()` to compute in parallel\n}",
            "// TODO\n\n    // Count quartile for rank 0\n    size_t count[4] = {0};\n    for (double value : x) {\n        if (value < 0.25) {\n            count[0]++;\n        } else if (value < 0.5) {\n            count[1]++;\n        } else if (value < 0.75) {\n            count[2]++;\n        } else {\n            count[3]++;\n        }\n    }\n\n    // Communicate results\n    std::array<size_t, 4> global_count{0, 0, 0, 0};\n    MPI_Reduce(count, global_count.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Store results on rank 0\n    if (MPI_Comm_rank(MPI_COMM_WORLD, &rank) == MPI_SUCCESS && rank == 0) {\n        bins = global_count;\n    }\n}",
            "// YOUR CODE HERE\n\n}",
            "int nthreads;\n    int nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &nthreads);\n    int count_per_thread = x.size() / nprocs;\n    int last_thread_count = x.size() % nprocs;\n    int start_index = nthreads * count_per_thread;\n    int end_index = start_index + count_per_thread;\n\n    if (nthreads == nprocs - 1) {\n        end_index += last_thread_count;\n    }\n\n    std::vector<int> local_count(4);\n    for (int i = start_index; i < end_index; i++) {\n        if (x[i] >= 0 && x[i] < 1) {\n            local_count[0] += 1;\n        } else if (x[i] >= 1 && x[i] < 2) {\n            local_count[1] += 1;\n        } else if (x[i] >= 2 && x[i] < 3) {\n            local_count[2] += 1;\n        } else if (x[i] >= 3 && x[i] < 4) {\n            local_count[3] += 1;\n        }\n    }\n\n    int counts[4];\n    MPI_Reduce(&local_count[0], &counts[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (nthreads == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = counts[i];\n        }\n    }\n}",
            "// TODO\n}",
            "if (bins.size()!= 4) {\n        throw std::runtime_error(\"`bins` must be of length 4\");\n    }\n    // TODO: your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<double> local;\n  std::vector<std::array<size_t, 4>> results(size);\n\n  // Split the vector x into the subvectors local and x_rest\n  size_t x_rest_start = x.size() / size * rank;\n  size_t x_rest_size = x.size() / size;\n  if (rank == size - 1) {\n    x_rest_size += x.size() % size;\n  }\n\n  local.assign(x.begin() + x_rest_start, x.begin() + x_rest_start + x_rest_size);\n\n  std::array<size_t, 4> local_results;\n\n#pragma omp parallel for\n  for (int i = 0; i < local.size(); ++i) {\n    size_t a = local[i] / 1.0;\n    local_results[a]++;\n  }\n\n  // Put the local results into the results array\n  MPI_Gather(&local_results, 4, MPI_UNSIGNED_LONG, results.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      for (int j = 0; j < 4; ++j) {\n        results[0][j] += results[i][j];\n      }\n    }\n    bins = results[0];\n  }\n}",
            "int num_rank, rank, i;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_rank);\n\n    bins = {0, 0, 0, 0};\n    if (rank!= 0) {\n        std::array<size_t, 4> local_bins;\n        local_bins = {0, 0, 0, 0};\n        size_t size = x.size() / num_rank;\n        double first = size * rank;\n        double last = first + size;\n        if (rank == num_rank - 1) last = x.size();\n\n#pragma omp parallel for\n        for (i = first; i < last; i++) {\n            if (x[i] >= 0 && x[i] < 0.25) local_bins[0] += 1;\n            else if (x[i] >= 0.25 && x[i] < 0.5) local_bins[1] += 1;\n            else if (x[i] >= 0.5 && x[i] < 0.75) local_bins[2] += 1;\n            else local_bins[3] += 1;\n        }\n\n        MPI_Send(&local_bins[0], 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for (int r = 1; r < num_rank; r++) {\n            MPI_Recv(&bins[0], 4, MPI_UNSIGNED_LONG, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            bins[0] += bins[0];\n            bins[1] += bins[1];\n            bins[2] += bins[2];\n            bins[3] += bins[3];\n        }\n    }\n}",
            "}",
            "// TODO: Your code goes here\n\n}",
            "bins = std::array<size_t, 4>({0, 0, 0, 0});\n    int n_threads;\n    int n_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &n_ranks);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_threads);\n    if(n_ranks == 0)\n        printf(\"threads: %d, ranks: %d\\n\", omp_get_max_threads(), n_threads);\n\n#pragma omp parallel num_threads(n_threads)\n    {\n        auto local_bins = std::array<size_t, 4>({0, 0, 0, 0});\n        int rank = omp_get_thread_num();\n        int num = x.size() / n_threads;\n        int start = rank * num;\n        int end = rank == n_threads - 1? x.size() : (rank + 1) * num;\n\n        if (start >= x.size()) {\n            return;\n        }\n\n#pragma omp for schedule(static)\n        for (int i = start; i < end; ++i) {\n            if (x[i] >= 0 && x[i] < 1)\n                local_bins[0]++;\n            else if (x[i] >= 1 && x[i] < 2)\n                local_bins[1]++;\n            else if (x[i] >= 2 && x[i] < 3)\n                local_bins[2]++;\n            else if (x[i] >= 3 && x[i] < 4)\n                local_bins[3]++;\n        }\n\n        for (int i = 0; i < local_bins.size(); ++i) {\n            MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0,\n                       MPI_COMM_WORLD);\n        }\n    }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Count the number of doubles in x that have a fractional part in [0, 0.25)\n    // Store the number in local_bins[0]\n    // std::array<size_t, 4> local_bins{0, 0, 0, 0};\n    // #pragma omp parallel for\n    // for (int i = 0; i < x.size(); i++) {\n    //     double frac = std::fmod(x[i], 1.0);\n    //     if (frac >= 0.0 && frac < 0.25) local_bins[0]++;\n    //     else if (frac >= 0.25 && frac < 0.5) local_bins[1]++;\n    //     else if (frac >= 0.5 && frac < 0.75) local_bins[2]++;\n    //     else if (frac >= 0.75 && frac < 1.0) local_bins[3]++;\n    // }\n\n    // Reduce local_bins[0], local_bins[1], local_bins[2], and local_bins[3] on rank 0\n    // bins[0] += local_bins[0];\n    // bins[1] += local_bins[1];\n    // bins[2] += local_bins[2];\n    // bins[3] += local_bins[3];\n\n    // MPI_Reduce(&bins, &local_bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // bins = local_bins;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t x_size = x.size();\n    int nbins = 4;\n    int x_per_thread = x_size / size;\n    std::array<size_t, nbins> thread_bins = {0};\n\n    // Compute counts on each thread\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static, x_per_thread)\n        for (size_t i = 0; i < x_size; i++) {\n            double frac = x[i] - floor(x[i]);\n            if (frac < 0.25) thread_bins[0]++;\n            else if (frac < 0.5) thread_bins[1]++;\n            else if (frac < 0.75) thread_bins[2]++;\n            else thread_bins[3]++;\n        }\n\n        // Reduce counts from all threads\n        #pragma omp critical\n        {\n            for (int i = 0; i < nbins; i++) {\n                bins[i] += thread_bins[i];\n            }\n        }\n    }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank!= 0) {\n        MPI_Send(x.data(), x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        return;\n    }\n\n    std::vector<std::array<size_t, 4>> results(size - 1);\n    for (int r = 1; r < size; ++r) {\n        MPI_Recv(results[r - 1].data(), results[r - 1].size(), MPI_UNSIGNED, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (auto const& result : results) {\n        for (size_t i = 0; i < bins.size(); ++i) {\n            bins[i] += result[i];\n        }\n    }\n}",
            "/* YOUR CODE HERE */\n}",
            "int numproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t n = x.size();\n\n    // Divide x into chunks\n    size_t numChunks = numproc;\n    size_t chunkSize = (n + numChunks - 1) / numChunks; // round up to an integer\n    size_t start = rank*chunkSize;\n    size_t end = std::min(start + chunkSize, n);\n\n    // Count in each chunk\n    int bins_local[4] = {0};\n    for (size_t i = start; i < end; i++) {\n        if (x[i] < 0.25) bins_local[0]++;\n        else if (x[i] < 0.5) bins_local[1]++;\n        else if (x[i] < 0.75) bins_local[2]++;\n        else bins_local[3]++;\n    }\n\n    // Sum bins from all chunks\n    int bins_all[4];\n    MPI_Reduce(bins_local, bins_all, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Copy back to bins\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            bins[i] = bins_all[i];\n        }\n    }\n}",
            "// TODO\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int tag = 1;\n  int source = 0;\n  int dest = 0;\n\n  // MPI_COMM_WORLD.root is only valid if the MPI_COMM_WORLD rank is 0\n  // Therefore, use MPI_COMM_WORLD.rank = 0 to indicate which rank is 0\n  if (rank == 0) {\n    // Calculate the number of elements that will be sent to each process\n    int num_per_process = x.size() / size;\n    // Remaining elements will be sent to the first (size - 1) processes\n    int remaining = x.size() % size;\n\n    // Initialize array with zeroes\n    for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n    }\n\n    // For each remaining process\n    for (int i = 1; i < size; i++) {\n      // Determine how many elements to send\n      int elements_to_send = num_per_process;\n      if (i <= remaining) {\n        elements_to_send++;\n      }\n\n      // Send elements\n      MPI_Send(&x[i * num_per_process], elements_to_send, MPI_DOUBLE, i, tag, MPI_COMM_WORLD);\n    }\n\n    // Calculate elements to send to rank 0\n    int elements_to_send = num_per_process;\n    if (rank <= remaining) {\n      elements_to_send++;\n    }\n\n    // Allocate space for data from other processes\n    std::vector<double> data(elements_to_send);\n\n    // Receive elements\n    for (int i = 1; i < size; i++) {\n      MPI_Recv(&data[0], elements_to_send, MPI_DOUBLE, i, tag, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      // Count the number of elements that each process sent\n      for (int j = 0; j < elements_to_send; j++) {\n        double fractional = data[j] - floor(data[j]);\n        if (fractional < 0.25) {\n          bins[0]++;\n        } else if (fractional < 0.5) {\n          bins[1]++;\n        } else if (fractional < 0.75) {\n          bins[2]++;\n        } else {\n          bins[3]++;\n        }\n      }\n    }\n  } else {\n    int elements_to_send = x.size() / size;\n    if (rank <= x.size() % size) {\n      elements_to_send++;\n    }\n\n    // Allocate space for data to be sent to rank 0\n    std::vector<double> data(elements_to_send);\n\n    // Copy data\n    for (int i = 0; i < elements_to_send; i++) {\n      data[i] = x[i];\n    }\n\n    // Send data\n    MPI_Send(&data[0], elements_to_send, MPI_DOUBLE, 0, tag, MPI_COMM_WORLD);\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t bins_rank_0[4] = {0};\n    size_t bins_rank_i[4] = {0};\n    size_t i = 0;\n    size_t j = 0;\n    double bin = 0.0;\n    int nthreads = omp_get_max_threads();\n\n    for (size_t i = 0; i < x.size(); i++) {\n        bin = x[i] - floor(x[i]);\n\n        if (bin > 0.0 && bin < 0.25) {\n            bins_rank_0[0]++;\n        }\n        else if (bin >= 0.25 && bin < 0.5) {\n            bins_rank_0[1]++;\n        }\n        else if (bin >= 0.5 && bin < 0.75) {\n            bins_rank_0[2]++;\n        }\n        else if (bin >= 0.75 && bin < 1.0) {\n            bins_rank_0[3]++;\n        }\n    }\n\n    #pragma omp parallel num_threads(nthreads) private(i, j) shared(bins_rank_i, bins_rank_0)\n    {\n        i = omp_get_thread_num();\n\n        for (j = 0; j < 4; j++) {\n            bins_rank_i[j] = bins_rank_0[j];\n        }\n\n        #pragma omp barrier\n        #pragma omp single\n        {\n            for (j = 0; j < 4; j++) {\n                for (i = 1; i < nthreads; i++) {\n                    bins_rank_0[j] = bins_rank_0[j] + bins_rank_i[j];\n                }\n            }\n        }\n    }\n\n    bins = {bins_rank_0[0], bins_rank_0[1], bins_rank_0[2], bins_rank_0[3]};\n}",
            "bins.fill(0);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // TODO: use OpenMP to split the vector into nthreads chunks\n    // TODO: use MPI to distribute the chunks to the ranks\n    // TODO: sum the results\n}",
            "// TODO\n  int const size = x.size();\n  int const rank = omp_get_thread_num();\n  \n  std::vector<int> data(size);\n  for(int i=0; i<size; ++i)\n    data[i] = int(x[i]*4.0);\n  \n  #pragma omp barrier\n  #pragma omp critical\n  {\n    if(rank == 0){\n      int i;\n      for(i = 0; i < size; ++i)\n\t++bins[data[i]];\n    }\n  }\n  #pragma omp barrier\n  \n}",
            "/* TODO */\n}",
            "// Your code here\n\n}",
            "// your code here\n}",
            "int rank, numproc;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n\n  auto x_copy = x;\n  auto bins_copy = bins;\n\n  int blocksize = x_copy.size() / numproc;\n  int leftover = x_copy.size() % numproc;\n\n  if (rank == 0) {\n    for (int i = 1; i < numproc; i++) {\n      MPI_Send(&x_copy[i * blocksize], blocksize + (i - 1 < leftover), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Recv(&x_copy[0], blocksize + (rank - 1 < leftover), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  std::array<size_t, 4> thread_bins_copy;\n\n  if (rank == 0) {\n    thread_bins_copy = bins_copy;\n    for (int i = 1; i < numproc; i++) {\n      MPI_Recv(&thread_bins_copy, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; j++) {\n        thread_bins_copy[j] += bins_copy[j];\n      }\n    }\n  } else {\n    thread_bins_copy = bins_copy;\n  }\n\n  if (rank == 0) {\n    for (int j = 0; j < 4; j++) {\n      for (int i = 0; i < thread_bins_copy[j]; i++) {\n        bins[j]++;\n      }\n    }\n  }\n\n  if (rank == 0) {\n    std::cout << \"Counts: \" << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n  }\n}",
            "// TODO: implement me\n}",
            "// TODO\n}",
            "if (x.size() % 2!= 0) {\n    std::cerr << \"This program assumes an even number of elements in the vector.\" << std::endl;\n    std::exit(1);\n  }\n\n  // TODO: Your code here\n  \n}",
            "int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<size_t> local_bins(4, 0);\n  if (rank == 0) {\n    local_bins = {0, 0, 0, 0};\n  }\n#pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    int nthreads = omp_get_num_threads();\n    // Divide the vector into nthreads pieces\n    size_t step = x.size() / nthreads;\n    size_t start = tid * step;\n    size_t end = (tid == nthreads - 1)? x.size() : (tid + 1) * step;\n#pragma omp for\n    for (size_t i = start; i < end; i++) {\n      double fractional_part = x[i] - (int) x[i];\n      if (fractional_part < 0.25) {\n        local_bins[0]++;\n      } else if (fractional_part < 0.5) {\n        local_bins[1]++;\n      } else if (fractional_part < 0.75) {\n        local_bins[2]++;\n      } else {\n        local_bins[3]++;\n      }\n    }\n#pragma omp critical\n    for (size_t i = 0; i < 4; i++) {\n      bins[i] += local_bins[i];\n    }\n  }\n}",
            "int numThreads = omp_get_max_threads();\n    int myRank = 0;\n    int numRanks = 0;\n    int chunkSize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    chunkSize = x.size()/numRanks;\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n    int start = myRank*chunkSize;\n    int end = (myRank+1)*chunkSize;\n    if(myRank == numRanks-1){\n        end = x.size();\n    }\n    for(int i = start; i<end; i++){\n        if(x[i] < 0.25){\n            localBins[0]++;\n        }\n        else if(x[i] < 0.5){\n            localBins[1]++;\n        }\n        else if(x[i] < 0.75){\n            localBins[2]++;\n        }\n        else{\n            localBins[3]++;\n        }\n    }\n    if(myRank == 0){\n        std::array<size_t, 4> globalBins = {0, 0, 0, 0};\n        std::array<size_t, 4> partialBins = {0, 0, 0, 0};\n        for(int i = 1; i<numRanks; i++){\n            MPI_Status status;\n            MPI_Recv(&partialBins, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for(int j = 0; j<4; j++){\n                globalBins[j] += partialBins[j];\n            }\n        }\n        for(int i = 0; i<4; i++){\n            globalBins[i] += localBins[i];\n        }\n        bins = globalBins;\n    }\n    else{\n        MPI_Send(&localBins, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// Your code goes here\n\n\n  int size;\n  int rank;\n  int i;\n  int count;\n  int max_count;\n  std::array<size_t, 4> local_bins;\n  std::array<size_t, 4> bins_partial;\n  MPI_Status status;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if(rank == 0)\n  {\n    for(i = 0; i < 4; i++)\n      bins[i] = 0;\n\n    for(i = 0; i < size; i++)\n    {\n      if(i == 0)\n        count = x.size() / size;\n      else\n        count = x.size() / size + 1;\n\n      MPI_Recv(&bins_partial, 4, MPI_INT, i, i, MPI_COMM_WORLD, &status);\n\n      for(int j = 0; j < 4; j++)\n        bins[j] += bins_partial[j];\n    }\n  }\n  else\n  {\n    local_bins[0] = 0;\n    local_bins[1] = 0;\n    local_bins[2] = 0;\n    local_bins[3] = 0;\n\n    for(i = 0; i < x.size(); i++)\n    {\n      if((x[i] > 0.0) && (x[i] <= 0.25))\n        local_bins[0]++;\n      else if((x[i] > 0.25) && (x[i] <= 0.5))\n        local_bins[1]++;\n      else if((x[i] > 0.5) && (x[i] <= 0.75))\n        local_bins[2]++;\n      else if((x[i] > 0.75) && (x[i] <= 1.0))\n        local_bins[3]++;\n    }\n\n    MPI_Send(&local_bins, 4, MPI_INT, 0, rank, MPI_COMM_WORLD);\n  }\n\n  return;\n}",
            "// Your code here\n\n}",
            "auto localBins = std::array<size_t, 4>{};\n    localBins.fill(0);\n    auto n = x.size();\n    auto numThreads = omp_get_max_threads();\n    auto rank = 0;\n    auto numRanks = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    auto start = rank*n/numRanks;\n    auto end = (rank+1)*n/numRanks;\n#pragma omp parallel num_threads(numThreads)\n    {\n        auto threadId = omp_get_thread_num();\n#pragma omp for\n        for (size_t i=start; i<end; ++i) {\n            auto digit = static_cast<size_t>(10.0*x[i] - 10.0*std::floor(10.0*x[i]));\n            auto bin = (digit <= 2? 0 : 1) + (digit <= 4? 0 : 2);\n            ++localBins[bin];\n        }\n    }\n\n    auto globalBins = std::array<size_t, 4>{};\n    globalBins.fill(0);\n    MPI_Reduce(localBins.data(), globalBins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0)\n        bins = globalBins;\n}",
            "bins = {0, 0, 0, 0};\n    auto local_bins = std::array<size_t, 4>{0, 0, 0, 0};\n    auto size = x.size();\n    auto chunk = size / 4;\n    auto start = chunk * omp_get_thread_num();\n    auto end = chunk * (omp_get_thread_num() + 1);\n\n#pragma omp parallel for\n    for (auto i = start; i < end; i++) {\n        if (x[i] < 0.25) {\n            local_bins[0]++;\n        } else if (x[i] < 0.5) {\n            local_bins[1]++;\n        } else if (x[i] < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins = { 0, 0, 0, 0 };\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0]++;\n    } else if (x[i] < 0.5) {\n      bins[1]++;\n    } else if (x[i] < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Your code here\n\n}",
            "auto const n = x.size();\n    auto const bins_per_rank = n / omp_get_num_threads();\n\n    std::array<size_t, 4> local_bins;\n\n    #pragma omp parallel\n    {\n        std::vector<double> local_x;\n        local_x.reserve(bins_per_rank);\n        for (size_t i = 0; i < n; ++i) {\n            if (omp_get_thread_num() == i % omp_get_num_threads())\n                local_x.emplace_back(x[i]);\n        }\n\n        std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n        #pragma omp for\n        for (size_t i = 0; i < bins_per_rank; ++i) {\n            if (local_x[i] < 0.25)\n                ++local_bins[0];\n            else if (local_x[i] < 0.5)\n                ++local_bins[1];\n            else if (local_x[i] < 0.75)\n                ++local_bins[2];\n            else\n                ++local_bins[3];\n        }\n\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 4; ++i)\n                local_bins[i] += bins[i];\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int nRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  #pragma omp parallel\n  {\n  size_t localCounts[4] = {0};\n  #pragma omp for nowait\n  for (size_t i = 0; i < x.size(); ++i) {\n    double const modulo = std::fmod(x[i], 1.0);\n    if (modulo >= 0 && modulo < 0.25) localCounts[0]++;\n    else if (modulo >= 0.25 && modulo < 0.5) localCounts[1]++;\n    else if (modulo >= 0.5 && modulo < 0.75) localCounts[2]++;\n    else if (modulo >= 0.75 && modulo < 1.0) localCounts[3]++;\n  }\n\n  size_t globalCounts[4] = {0};\n  MPI_Reduce(localCounts, globalCounts, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) bins = {{globalCounts[0], globalCounts[1], globalCounts[2], globalCounts[3]}};\n  }\n}",
            "// TODO: your code here\n    int nranks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 4> count;\n    count.fill(0);\n    int nthreads = omp_get_max_threads();\n    #pragma omp parallel num_threads(nthreads)\n    {\n        // Get the thread id\n        int threadid = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n\n        // Count how many numbers in the vector x fall into each bin\n        #pragma omp for schedule(static)\n        for(size_t i = threadid; i < x.size(); i += nthreads){\n            if (x[i] < 0.25) {\n                count[0]++;\n            }\n            else if (x[i] < 0.5) {\n                count[1]++;\n            }\n            else if (x[i] < 0.75) {\n                count[2]++;\n            }\n            else {\n                count[3]++;\n            }\n        }\n    }\n    MPI_Reduce(&count, &bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  int const nranks = omp_get_num_threads();\n  int const rank = omp_get_thread_num();\n  int const rank_size = x.size() / nranks;\n  int const remainder = x.size() % nranks;\n  int const rank_start = rank * rank_size;\n  int const rank_end = rank_start + rank_size + (rank < remainder? 1 : 0);\n\n  // TODO: count the fractional part of the doubles in `x` in parallel\n\n  // TODO: sum the counts from all threads into the `bins` vector\n}",
            "// TODO: Your code goes here\n\n}",
            "const int numThreads = omp_get_max_threads();\n\n  // TODO 1: create bins\n  // TODO 2: create a loop that partitions `x` and calls `countQuartilesThread` for each partition\n  // TODO 3: implement the `countQuartilesThread` function\n\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t n = x.size();\n  std::vector<size_t> nlocal(size);\n  std::vector<size_t> displs(size);\n\n  if (rank == 0) {\n    size_t count = 0;\n    for (int i = 0; i < size; ++i) {\n      count += (n - count) / (size - i);\n      nlocal[i] = count;\n      displs[i] = count - nlocal[i];\n    }\n  }\n  MPI_Bcast(&nlocal[0], size, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&displs[0], size, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank!= 0) x.resize(nlocal[rank]);\n  std::vector<size_t> localBins(4);\n  std::vector<double> localX(nlocal[rank]);\n  localX = x;\n\n  #pragma omp parallel for schedule(static)\n  for (int i = 0; i < nlocal[rank]; ++i) {\n    if (0 <= localX[i] && localX[i] < 1) {\n      ++localBins[0];\n    } else if (1 <= localX[i] && localX[i] < 2) {\n      ++localBins[1];\n    } else if (2 <= localX[i] && localX[i] < 3) {\n      ++localBins[2];\n    } else {\n      ++localBins[3];\n    }\n  }\n  size_t tmp[4];\n  MPI_Reduce(&localBins[0], &tmp[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) bins[0] = tmp[0];\n  MPI_Reduce(&localBins[1], &tmp[1], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) bins[1] = tmp[1];\n  MPI_Reduce(&localBins[2], &tmp[2], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) bins[2] = tmp[2];\n  MPI_Reduce(&localBins[3], &tmp[3], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) bins[3] = tmp[3];\n}",
            "}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t n_bins = x.size();\n  int const rank = omp_get_thread_num();\n  int const size = omp_get_num_threads();\n  MPI_Comm const comm = MPI_COMM_WORLD;\n\n  // Compute the number of doubles that are in each bin.\n  int bin_counts[4];\n  for (int i = 0; i < 4; ++i) {\n    bin_counts[i] = 0;\n  }\n  for (auto const& item: x) {\n    double const frac_part = item - std::floor(item);\n    int const bin_index = frac_part / 0.25;\n    ++bin_counts[bin_index];\n  }\n\n  // Add up the number of doubles from each rank.\n  int all_bin_counts[4];\n  MPI_Reduce(bin_counts, all_bin_counts, 4, MPI_INT, MPI_SUM, 0, comm);\n\n  // Store the results in the output array.\n  if (rank == 0) {\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = all_bin_counts[i];\n    }\n  }\n}",
            "// Your code here\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // split the work into `size` chunks of roughly the same size\n  size_t chunk_size = x.size() / size + 1;\n  size_t start = rank * chunk_size;\n  size_t end = std::min(start + chunk_size, x.size());\n\n  // count the number of integers in [0, 1)\n  size_t local_count[4] = {};\n  for (auto xi = x.begin() + start; xi!= x.begin() + end; ++xi) {\n    int bin = (int) (*xi / 0.25);\n    if (bin > 3)\n      bin = 3;\n    ++local_count[bin];\n  }\n\n  std::array<size_t, 4> all_counts;\n  MPI_Reduce(local_count, &all_counts[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0)\n    bins = all_counts;\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement this\n\n}",
            "// Your code goes here\n}",
            "// TODO: replace this code\n    size_t bins_[4] = {0, 0, 0, 0};\n    for (auto i : x) {\n        if (i < 0.25)\n            bins_[0]++;\n        else if (i < 0.5)\n            bins_[1]++;\n        else if (i < 0.75)\n            bins_[2]++;\n        else\n            bins_[3]++;\n    }\n    bins = {bins_[0], bins_[1], bins_[2], bins_[3]};\n}",
            "}",
            "}",
            "size_t const n = x.size();\n    bins.fill(0);\n\n    int world_size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t chunk_size = n / world_size;\n    size_t remainder = n % world_size;\n    size_t start_idx = rank * chunk_size + (rank < remainder? rank : remainder);\n    size_t end_idx = (rank+1) * chunk_size + (rank+1 < remainder? rank+1 : remainder);\n    size_t local_n = end_idx - start_idx;\n\n    // OMP\n    #pragma omp parallel\n    {\n        // OMP\n        #pragma omp for\n        for (size_t i = 0; i < local_n; i++)\n        {\n            double x_i = x[start_idx + i];\n            int bin_index = std::floor(x_i * 4) % 4;\n            #pragma omp atomic\n            bins[bin_index] += 1;\n        }\n    }\n\n    if (rank == 0)\n    {\n        std::vector<size_t> bins_v(bins.begin(), bins.end());\n        MPI_Reduce(bins_v.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else\n    {\n        MPI_Reduce(MPI_IN_PLACE, MPI_IN_PLACE, 0, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "const int rank = omp_get_thread_num();\n  const int size = omp_get_num_threads();\n  auto start = x.begin();\n  auto end = x.end();\n  std::advance(start, rank*x.size()/size);\n  std::advance(end, (rank+1)*x.size()/size);\n  auto count = std::count_if(start, end, [](double d) { return d > 0 && d < 0.25; });\n  MPI_Reduce(&count, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  count = std::count_if(start, end, [](double d) { return d > 0.25 && d < 0.5; });\n  MPI_Reduce(&count, &bins[1], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  count = std::count_if(start, end, [](double d) { return d > 0.5 && d < 0.75; });\n  MPI_Reduce(&count, &bins[2], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  count = std::count_if(start, end, [](double d) { return d > 0.75 && d < 1; });\n  MPI_Reduce(&count, &bins[3], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    const int local_size = x.size() / num_procs;\n    const int rest = x.size() % num_procs;\n    const int offset = local_size * rank + std::min(rest, rank);\n    const int size = (rank < rest)? local_size + 1 : local_size;\n\n    if (rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n\n    std::array<size_t, 4> local_bins{};\n\n    #pragma omp parallel for\n    for (int i = 0; i < size; ++i) {\n        double xi = x[i + offset];\n        double frac = std::fmod(xi, 1);\n        if (frac <= 0.25) {\n            ++local_bins[0];\n        } else if (frac <= 0.5) {\n            ++local_bins[1];\n        } else if (frac <= 0.75) {\n            ++local_bins[2];\n        } else if (frac <= 1) {\n            ++local_bins[3];\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n    std::vector<int> counts(4, 0);\n    // TODO: Fill counts using OpenMP for-loop\n#pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n      double x_i = x[i];\n      int index = 0;\n      if (x_i >= 0 && x_i < 0.25)\n        index = 0;\n      else if (x_i >= 0.25 && x_i < 0.5)\n        index = 1;\n      else if (x_i >= 0.5 && x_i < 0.75)\n        index = 2;\n      else if (x_i >= 0.75 && x_i < 1)\n        index = 3;\n      #pragma omp atomic\n      counts[index]++;\n    }\n\n    // TODO: Reduce counts using MPI's MPI_Reduce to bins on rank 0\n    int counts_reduce[4];\n    int counts_reduce_recv[4];\n    MPI_Reduce(counts.data(), counts_reduce, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (MPI_COMM_WORLD.rank == 0) {\n      std::copy(counts_reduce, counts_reduce + 4, bins.data());\n    } else {\n      MPI_Reduce(counts.data(), counts_reduce_recv, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "}",
            "if (x.empty()) {\n        throw std::invalid_argument(\"Cannot count quartiles in empty vector\");\n    }\n\n    auto num_procs = omp_get_num_threads();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<size_t> thread_counts(num_procs, 0);\n    auto const x_per_thread = x.size() / num_procs;\n    auto const x_extra = x.size() % num_procs;\n    auto const start = x_per_thread * rank + std::min(rank, x_extra);\n    auto const end = start + x_per_thread + (rank < x_extra);\n\n    for (size_t i = start; i < end; ++i) {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            thread_counts[rank] += 1;\n        }\n        else if (frac < 0.5) {\n            thread_counts[rank] += 2;\n        }\n        else if (frac < 0.75) {\n            thread_counts[rank] += 3;\n        }\n        else if (frac < 1) {\n            thread_counts[rank] += 4;\n        }\n    }\n\n    std::vector<size_t> counts(num_procs, 0);\n    MPI_Gather(&thread_counts[rank], 1, MPI_UNSIGNED_LONG,\n               counts.data(), 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < counts.size(); ++i) {\n            bins[0] += counts[i] % 4;\n            bins[1] += (counts[i] / 4) % 4;\n            bins[2] += (counts[i] / 16) % 4;\n            bins[3] += (counts[i] / 64) % 4;\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: YOUR CODE HERE\n\n}",
            "}",
            "bins = std::array<size_t, 4>();\n  // TODO\n}",
            "//TODO\n\n}",
            "std::array<double, 4> quartiles = {0.0, 0.25, 0.5, 0.75};\n    // fill bins with zeroes\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel\n    {\n        std::array<size_t, 4> private_bins;\n        for (size_t i = 0; i < 4; i++) {\n            private_bins[i] = 0;\n        }\n\n        // count the number of elements that belong to each quartile\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++) {\n            // TODO\n        }\n\n        // add up the bins\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 4; i++) {\n                bins[i] += private_bins[i];\n            }\n        }\n    }\n}",
            "// your code here\n}",
            "size_t mySize = x.size();\n    int myRank = omp_get_thread_num();\n    int numProc = omp_get_num_threads();\n    std::vector<std::vector<int>> localBins;\n    //for (int i = 0; i < numProc; i++) localBins.push_back(std::vector<int> (4, 0));\n    //for (int i = 0; i < 4; i++) {\n    //    localBins[myRank][i] = bins[i];\n    //}\n    localBins.push_back(std::vector<int> (4, 0));\n    for (int i = 0; i < 4; i++) {\n        localBins[myRank][i] = bins[i];\n    }\n    int chunkSize = mySize / numProc;\n    int remainder = mySize % numProc;\n    int start = myRank * chunkSize;\n    int end = start + chunkSize;\n    if (myRank == numProc - 1) {\n        end = end + remainder;\n    }\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (x[i] < 1 && x[i] >= 0.75) {\n            localBins[myRank][0]++;\n        } else if (x[i] < 0.75 && x[i] >= 0.5) {\n            localBins[myRank][1]++;\n        } else if (x[i] < 0.5 && x[i] >= 0.25) {\n            localBins[myRank][2]++;\n        } else if (x[i] < 0.25 && x[i] >= 0) {\n            localBins[myRank][3]++;\n        }\n    }\n    //localBins[myRank][0] = 2;\n    //localBins[myRank][1] = 1;\n    //localBins[myRank][2] = 2;\n    //localBins[myRank][3] = 2;\n    if (myRank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n    MPI_Reduce(localBins[0].data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int myRank, nRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    std::vector<size_t> myBins(4, 0);\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); i++) {\n        double fraction = x[i] - std::floor(x[i]);\n        if (fraction >= 0 && fraction < 0.25) {\n            myBins[0]++;\n        } else if (fraction >= 0.25 && fraction < 0.5) {\n            myBins[1]++;\n        } else if (fraction >= 0.5 && fraction < 0.75) {\n            myBins[2]++;\n        } else if (fraction >= 0.75 && fraction <= 1) {\n            myBins[3]++;\n        }\n    }\n\n    // gather all myBins on rank 0\n    std::vector<size_t> allBins;\n    if (myRank == 0) {\n        allBins.resize(4 * nRanks);\n    }\n    MPI_Gather(&myBins[0], 4, MPI_INT, allBins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (myRank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            for (int j = 1; j < nRanks; j++) {\n                allBins[i] += allBins[4*j+i];\n            }\n            bins[i] = allBins[i];\n        }\n    }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // TODO: insert code here\n    // 4) Use OpenMP to compute the count in parallel.\n\n\n    // 5) Use MPI to merge the counts from each rank.\n}",
            "int rank;\n  int np;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n  // TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int nthreads = omp_get_max_threads();\n    std::vector<std::array<size_t, 4>> bins_rank(nthreads);\n    int nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    if (nprocs!= nthreads) {\n        throw std::runtime_error(\"MPI communicator has not been initialized with \" + std::to_string(nthreads) + \" processes.\");\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    #pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        size_t start = rank * (x.size() / nthreads);\n        size_t end = (rank + 1) * (x.size() / nthreads);\n        size_t count = 0;\n        for (size_t i = start; i < end; ++i) {\n            size_t bin = size_t(x[i] * 4.0);\n            if (bin > 3) {\n                bin = 3;\n            }\n            ++bins_rank[rank][bin];\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Reduce(MPI_IN_PLACE, &bins_rank[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (nprocs == 1) {\n        bins = bins_rank[0];\n    }\n}",
            "// your code here\n}",
            "/* Your solution goes here! */\n}",
            "// TODO: implement\n}",
            "if (x.size() == 0)\n        return;\n\n    // TODO: write your solution here\n}",
            "// TODO\n\n}",
            "bins.fill(0);\n  size_t n = x.size();\n  int num_threads = omp_get_max_threads();\n  int num_procs = 0;\n  int my_rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  int num_blocks = n / num_threads;\n  std::array<int, 4> local_bins{};\n  #pragma omp parallel\n  {\n    #pragma omp single\n    {\n      for (int i = 0; i < num_procs; ++i)\n        MPI_Send(&num_blocks, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n    size_t start = omp_get_thread_num() * num_blocks;\n    size_t end = start + num_blocks;\n    if (end > n) end = n;\n    for (size_t i = start; i < end; ++i) {\n      double fractional = x[i] - std::floor(x[i]);\n      if (fractional < 0.25) ++local_bins[0];\n      else if (fractional < 0.5) ++local_bins[1];\n      else if (fractional < 0.75) ++local_bins[2];\n      else ++local_bins[3];\n    }\n    for (int i = 0; i < num_procs; ++i)\n      MPI_Send(local_bins.data(), 4, MPI_INT, i, 1, MPI_COMM_WORLD);\n  }\n\n  if (my_rank == 0) {\n    for (int i = 1; i < num_procs; ++i) {\n      MPI_Recv(local_bins.data(), 4, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; ++j) bins[j] += local_bins[j];\n    }\n  }\n  else {\n    MPI_Recv(local_bins.data(), 4, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n}",
            "// Your code here\n\n}",
            "const int rank = 0;\n    const int size = 1;\n    const int num_threads = 1;\n\n    const int num_elements = x.size();\n\n    if (rank == 0) {\n        #pragma omp parallel for num_threads(num_threads)\n        for (int i = 0; i < num_elements; i++) {\n            double x_i = x[i];\n            if (x_i >= 0 && x_i < 0.25) {\n                bins[0] += 1;\n            } else if (x_i >= 0.25 && x_i < 0.5) {\n                bins[1] += 1;\n            } else if (x_i >= 0.5 && x_i < 0.75) {\n                bins[2] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        }\n    }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // YOUR CODE HERE\n}",
            "bins = {{0, 0, 0, 0}};\n    size_t local_bins[4] = {0, 0, 0, 0};\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t N = x.size();\n    size_t part_size = N / size;\n    size_t remainder = N - size * part_size;\n    size_t start, end;\n\n    if (rank < remainder) {\n        start = rank * (part_size + 1);\n        end = start + part_size + 1;\n    } else {\n        start = rank * part_size + remainder;\n        end = start + part_size;\n    }\n\n    size_t local_remainder = end - start;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_remainder; ++i) {\n        size_t position = static_cast<size_t>(x[start + i] * 4);\n        ++local_bins[position];\n    }\n\n    // Gather the counts from each rank\n    MPI_Gather(local_bins, 4, MPI_UNSIGNED_LONG,\n            bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "if (bins.size()!= 4) {\n    throw std::domain_error(\"the size of bins must be 4\");\n  }\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 4> localBins {0, 0, 0, 0};\n  int chunks = size;\n  int chunksize = x.size() / chunks;\n  int remainder = x.size() % chunks;\n\n  if (rank == 0) {\n    std::vector<size_t> bins(4);\n    std::vector<std::vector<double>> xs(chunks);\n\n    for (int i = 1; i < chunks; i++) {\n      MPI_Recv(&xs[i][0], chunksize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i = 0; i < chunksize + remainder; i++) {\n      xs[0].push_back(x[i]);\n    }\n    for (int i = 0; i < chunks; i++) {\n      for (double num : xs[i]) {\n        if (num < 0.25)\n          bins[0]++;\n        else if (num < 0.5)\n          bins[1]++;\n        else if (num < 0.75)\n          bins[2]++;\n        else\n          bins[3]++;\n      }\n    }\n    MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    for (int i = 1; i < chunks; i++) {\n      MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Send(&x[0], chunksize + remainder, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    MPI_Recv(&localBins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  for (int i = 0; i < 4; i++) {\n    bins[i] += localBins[i];\n  }\n}",
            "// Your code here\n    //...\n    //...\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int i;\n  int * bins_local = new int[4];\n  for (i = 0; i < 4; i++)\n  {\n    bins_local[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (i = 0; i < x.size(); i++)\n  {\n    if (x[i] < 1.0)\n    {\n      bins_local[0] += 1;\n    }\n    else if (x[i] >= 1.0 && x[i] < 1.25)\n    {\n      bins_local[1] += 1;\n    }\n    else if (x[i] >= 1.25 && x[i] < 1.5)\n    {\n      bins_local[2] += 1;\n    }\n    else if (x[i] >= 1.5 && x[i] < 1.75)\n    {\n      bins_local[3] += 1;\n    }\n  }\n\n  int * bins_reduce = new int[4];\n  for (i = 0; i < 4; i++)\n  {\n    bins_reduce[i] = 0;\n  }\n\n  MPI_Reduce(bins_local, bins_reduce, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0)\n  {\n    for (i = 0; i < 4; i++)\n    {\n      bins[i] = bins_reduce[i];\n    }\n  }\n}",
            "// TODO: your code here\n    if(x.size() == 0) return;\n    int size, rank;\n    double *data_x = new double[x.size()];\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = x.size() / size;\n    int num_chunks = 0;\n    double *y = new double[chunk_size];\n\n    if (rank == 0) {\n        bins = std::array<size_t, 4> {0, 0, 0, 0};\n    }\n\n    #pragma omp parallel num_threads(size)\n    {\n        int id = omp_get_thread_num();\n        int num_threads = omp_get_num_threads();\n        int chunk = id * chunk_size;\n        int rest = x.size() % num_threads;\n        int start_pos = id * chunk_size;\n\n        if (id < rest) {\n            y = new double[chunk_size+1];\n            for (int i = 0; i < chunk_size+1; i++) {\n                y[i] = x[i+start_pos];\n            }\n        } else {\n            y = new double[chunk_size];\n            for (int i = 0; i < chunk_size; i++) {\n                y[i] = x[i+start_pos];\n            }\n        }\n        #pragma omp barrier\n        if (id == 0) {\n            int sum = 0;\n            for (int i = 0; i < num_threads; i++) {\n                if (i < rest) {\n                    for (int j = 0; j < chunk_size+1; j++) {\n                        bins[0] += (int)y[j] % 1;\n                    }\n                    for (int j = 0; j < chunk_size+1; j++) {\n                        bins[1] += ((int)y[j] / 4) % 1;\n                    }\n                    for (int j = 0; j < chunk_size+1; j++) {\n                        bins[2] += ((int)y[j] / 2) % 1;\n                    }\n                    for (int j = 0; j < chunk_size+1; j++) {\n                        bins[3] += ((int)y[j] / 4) % 1;\n                    }\n                } else {\n                    for (int j = 0; j < chunk_size; j++) {\n                        bins[0] += (int)y[j] % 1;\n                    }\n                    for (int j = 0; j < chunk_size; j++) {\n                        bins[1] += ((int)y[j] / 4) % 1;\n                    }\n                    for (int j = 0; j < chunk_size; j++) {\n                        bins[2] += ((int)y[j] / 2) % 1;\n                    }\n                    for (int j = 0; j < chunk_size; j++) {\n                        bins[3] += ((int)y[j] / 4) % 1;\n                    }\n                }\n            }\n        }\n    }\n    delete[] y;\n}",
            "int numprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: your solution here\n}",
            "if (bins.size()!= 4) {\n        throw std::invalid_argument(\"The length of bins must be equal to 4.\");\n    }\n\n    // TODO: your code here\n}",
            "size_t const num_elements = x.size();\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n  // Your code goes here.\n\n\n  // Reduce and broadcast.\n\n\n}",
            "int my_rank, comm_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n  size_t n = x.size();\n  size_t local_n = n/comm_size;\n  size_t start = local_n*my_rank;\n  size_t end = start + local_n;\n  if(my_rank == comm_size-1) end = n;\n  if(my_rank == 0) bins = {0,0,0,0};\n\n  // TODO: your code here\n  for(size_t i = start; i < end; i++) {\n    if(fmod(x[i], 4) < 1) {\n      #pragma omp atomic\n      bins[0]++;\n    }\n    else if(fmod(x[i], 4) < 2) {\n      #pragma omp atomic\n      bins[1]++;\n    }\n    else if(fmod(x[i], 4) < 3) {\n      #pragma omp atomic\n      bins[2]++;\n    }\n    else if(fmod(x[i], 4) < 4) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  bins = std::array<size_t, 4>{};\n\n  std::vector<size_t> bins_rank(4);\n  if (rank == 0) {\n    bins_rank = std::vector<size_t>{0,0,0,0};\n  } else {\n    bins_rank = std::vector<size_t>{0,0,0,0};\n    size_t bins_size = bins.size();\n    size_t x_size = x.size();\n    size_t x_rank_size = x_size/size;\n    size_t x_rank_rest = x_size%size;\n    size_t x_rank_start;\n    if (rank > x_rank_rest) {\n      x_rank_start = (rank - x_rank_rest)*x_rank_size + x_rank_rest;\n    } else {\n      x_rank_start = rank*x_rank_size;\n    }\n    for (size_t i = 0; i < x_rank_size; ++i) {\n      if (x[x_rank_start + i] >= 0 && x[x_rank_start + i] < 0.25)\n        ++bins_rank[0];\n      else if (x[x_rank_start + i] >= 0.25 && x[x_rank_start + i] < 0.5)\n        ++bins_rank[1];\n      else if (x[x_rank_start + i] >= 0.5 && x[x_rank_start + i] < 0.75)\n        ++bins_rank[2];\n      else if (x[x_rank_start + i] >= 0.75 && x[x_rank_start + i] < 1)\n        ++bins_rank[3];\n    }\n  }\n\n  MPI_Gather(&bins_rank[0], bins_rank.size(), MPI_UNSIGNED_LONG, &bins[0], bins_rank.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] /= size;\n  }\n}",
            "int mpi_size;\n  int mpi_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n  bins = {0, 0, 0, 0};\n  auto size = x.size();\n  auto count = size/mpi_size;\n  auto rem = size%mpi_size;\n  auto start = count*mpi_rank + (mpi_rank < rem? mpi_rank : rem);\n  auto end = count*(mpi_rank + 1) + (mpi_rank + 1 < rem? mpi_rank + 1 : rem);\n\n  // count the bins in this partition of x\n  auto local_bins = std::array<size_t, 4>{0, 0, 0, 0};\n  #pragma omp parallel for\n  for(auto i = start; i < end; ++i) {\n    if(x[i] >= 0 && x[i] < 0.25)\n      ++local_bins[0];\n    else if(x[i] >= 0.25 && x[i] < 0.5)\n      ++local_bins[1];\n    else if(x[i] >= 0.5 && x[i] < 0.75)\n      ++local_bins[2];\n    else if(x[i] >= 0.75 && x[i] <= 1)\n      ++local_bins[3];\n  }\n\n  // sum the counts across ranks\n  std::array<size_t, 4> bins_sum{0, 0, 0, 0};\n  MPI_Reduce(local_bins.data(), bins_sum.data(), bins_sum.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(mpi_rank == 0)\n    bins = bins_sum;\n}",
            "int n = x.size();\n  int num_ranks, rank_id;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n  std::array<double, 4> bins_local;\n  for (int i = 0; i < 4; ++i) bins_local[i] = 0;\n  for (int i = 0; i < n; ++i) {\n    double frac = fmod(x[i], 1);\n    for (int j = 0; j < 4; ++j) {\n      if (j == 0) {\n        if (frac >= 0.0 && frac < 0.25) bins_local[j] += 1;\n      } else if (j == 1) {\n        if (frac >= 0.25 && frac < 0.5) bins_local[j] += 1;\n      } else if (j == 2) {\n        if (frac >= 0.5 && frac < 0.75) bins_local[j] += 1;\n      } else if (j == 3) {\n        if (frac >= 0.75 && frac < 1.0) bins_local[j] += 1;\n      }\n    }\n  }\n  MPI_Gather(bins_local.data(), 4, MPI_DOUBLE, bins.data(), 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n\n}",
            "// TODO: Fill this in with your code\n  bins = {0, 0, 0, 0};\n}",
            "std::vector<int> local_bins(4, 0);\n  int n = x.size();\n  int num_threads = omp_get_max_threads();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int mpi_local_size;\n  int mpi_local_rank;\n  int mpi_total_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_total_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpi_local_rank);\n  mpi_local_size = (n + mpi_total_size - 1) / mpi_total_size;\n  std::vector<double> local_x(mpi_local_size);\n  MPI_Scatter(x.data(), mpi_local_size, MPI_DOUBLE, local_x.data(), mpi_local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  int local_n = mpi_local_size;\n#pragma omp parallel num_threads(num_threads)\n  {\n    int local_n_per_thread = (local_n + omp_get_num_threads() - 1) / omp_get_num_threads();\n    int local_thread_id = omp_get_thread_num();\n    int global_thread_id = rank * omp_get_num_threads() + local_thread_id;\n    int local_start = local_n_per_thread * local_thread_id;\n    int local_end = local_start + local_n_per_thread;\n    local_end = local_end > local_n? local_n : local_end;\n    for (int i = local_start; i < local_end; i++) {\n      double val = local_x[i];\n      int bin = 0;\n      if (val >= 0 && val < 0.25) {\n        bin = 0;\n      } else if (val >= 0.25 && val < 0.5) {\n        bin = 1;\n      } else if (val >= 0.5 && val < 0.75) {\n        bin = 2;\n      } else if (val >= 0.75 && val < 1) {\n        bin = 3;\n      }\n      local_bins[bin]++;\n    }\n  }\n  int mpi_bins[4];\n  MPI_Reduce(local_bins.data(), mpi_bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (mpi_local_rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = mpi_bins[i];\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Your code here\n\n  // rank 0\n  std::array<size_t, 4> temp;\n  for (int i = 0; i < 4; ++i) {\n    temp[i] = 0;\n  }\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int start = rank * x.size() / size;\n  int end = (rank + 1) * x.size() / size;\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n  for (int i = start; i < end; ++i) {\n    double x_i = x[i];\n    int i_bin = 0;\n    if (x_i >= 0 && x_i < 0.25) {\n      i_bin = 0;\n    } else if (x_i >= 0.25 && x_i < 0.5) {\n      i_bin = 1;\n    } else if (x_i >= 0.5 && x_i < 0.75) {\n      i_bin = 2;\n    } else if (x_i >= 0.75 && x_i < 1) {\n      i_bin = 3;\n    }\n    #pragma omp critical\n    {\n      temp[i_bin]++;\n    }\n  }\n  MPI_Reduce(&temp, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement me!\n\n    return;\n}",
            "int rank;\n  int nproc;\n  int my_n = x.size();\n  int nproc_row;\n  int my_id_row;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  nproc_row = sqrt(nproc);\n  my_id_row = rank / nproc_row;\n  int my_n_local;\n  int my_first_local;\n  double my_x[my_n];\n  int local_bins[4];\n  int global_bins[4];\n  if (rank == 0) {\n    for (int i = 0; i < nproc_row; i++) {\n      for (int j = 0; j < nproc_row; j++) {\n        if (i == 0 && j == 0) {\n          local_bins[0] = 0;\n          local_bins[1] = 0;\n          local_bins[2] = 0;\n          local_bins[3] = 0;\n        } else {\n          MPI_Recv(local_bins, 4, MPI_INT, i * nproc_row + j, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n      }\n    }\n  } else {\n    local_bins[0] = 0;\n    local_bins[1] = 0;\n    local_bins[2] = 0;\n    local_bins[3] = 0;\n    int global_n = x.size();\n    int global_n_local;\n    int global_first_local;\n    MPI_Scatter(&global_n, 1, MPI_INT, &global_n_local, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(&global_first, 1, MPI_INT, &global_first_local, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (global_n_local!= 0) {\n      MPI_Scatter(&global_x[0], global_n_local, MPI_DOUBLE, &my_x[0], global_n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      #pragma omp parallel for reduction(+ : local_bins[0], local_bins[1], local_bins[2], local_bins[3])\n      for (int i = 0; i < my_n_local; i++) {\n        if (my_x[i] >= 0 && my_x[i] < 0.25) {\n          local_bins[0] += 1;\n        } else if (my_x[i] >= 0.25 && my_x[i] < 0.5) {\n          local_bins[1] += 1;\n        } else if (my_x[i] >= 0.5 && my_x[i] < 0.75) {\n          local_bins[2] += 1;\n        } else if (my_x[i] >= 0.75 && my_x[i] < 1) {\n          local_bins[3] += 1;\n        }\n      }\n      MPI_Send(local_bins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n  }\n  if (rank == 0) {\n    for (int i = 0; i < nproc_row; i++) {\n      for (int j = 0; j < nproc_row; j++) {\n        if (i == 0 && j == 0) {\n          global_bins[0] = 0;\n          global_bins[1] = 0;\n          global_bins[2] = 0;\n          global_bins[3] = 0;\n        } else {\n          MPI_Recv(local_bins, 4, MPI_INT, i * nproc_",
            "// Your code here!\n}",
            "int n_total = x.size();\n  int n_local = x.size();\n  int rank = 0;\n  int n_ranks = 1;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n_local_rank = n_total / n_ranks;\n  int n_local_remainder = n_total % n_ranks;\n  if (rank < n_local_remainder) {\n    n_local_rank += 1;\n  }\n  int n_local_start = n_local_rank * rank;\n  if (rank >= n_local_remainder) {\n    n_local_start += n_local_remainder;\n  }\n  int n_local_end = n_local_start + n_local_rank - 1;\n  if (rank == n_ranks - 1) {\n    n_local_end = n_total - 1;\n  }\n  int n_local_local = n_local_end - n_local_start + 1;\n\n  auto bins_local = std::array<size_t, 4>{};\n  for (size_t i = 0; i < n_local; i++) {\n    if (i >= n_local_start && i <= n_local_end) {\n      int b = 0;\n      double m = x[i];\n      while (m > 0.75) {\n        m -= 0.25;\n        b++;\n      }\n      bins_local[b] += 1;\n    }\n  }\n  std::vector<int> bins_vec(4);\n  for (int i = 0; i < 4; i++) {\n    bins_vec[i] = bins_local[i];\n  }\n  std::vector<int> bins_vec_final(4);\n  MPI_Reduce(&bins_vec[0], &bins_vec_final[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = bins_vec_final[i];\n    }\n  }\n}",
            "int my_rank;\n    int comm_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    int my_size = x.size();\n    int my_size_per_rank = my_size / comm_size;\n    int my_offset = my_rank * my_size_per_rank;\n\n    std::array<size_t, 4> my_bins = { 0, 0, 0, 0 };\n    #pragma omp parallel for\n    for (int i = 0; i < my_size; i++) {\n        double fractional = x[i] - floor(x[i]);\n        if (fractional < 0.25) my_bins[0]++;\n        else if (fractional < 0.5) my_bins[1]++;\n        else if (fractional < 0.75) my_bins[2]++;\n        else if (fractional < 1) my_bins[3]++;\n    }\n\n    // combine the data from different ranks using MPI\n    std::vector<size_t> bins_from_all_ranks(comm_size * 4);\n    MPI_Gather(&my_bins, 4, MPI_SIZE_T, bins_from_all_ranks.data(), 4, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        for (int i = 0; i < comm_size * 4; i += 4) {\n            bins[0] += bins_from_all_ranks[i];\n            bins[1] += bins_from_all_ranks[i+1];\n            bins[2] += bins_from_all_ranks[i+2];\n            bins[3] += bins_from_all_ranks[i+3];\n        }\n    }\n}",
            "int nthreads, tid;\n  int count = 0, count0 = 0, count1 = 0, count2 = 0, count3 = 0;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &nthreads);\n  MPI_Comm_rank(MPI_COMM_WORLD, &tid);\n\n  int num_elem = x.size();\n  int elem_per_thread = num_elem / nthreads;\n  int num_elem_last_thread = num_elem % nthreads;\n\n  std::vector<double> local_x(elem_per_thread);\n\n  for (int i = 0; i < elem_per_thread; i++) {\n    local_x[i] = x[tid * elem_per_thread + i];\n  }\n\n  if (num_elem_last_thread!= 0 && tid == nthreads - 1) {\n    for (int i = elem_per_thread * tid; i < num_elem; i++) {\n      local_x[i - tid * elem_per_thread] = x[i];\n    }\n  }\n\n  #pragma omp parallel for shared(count, local_x) reduction(+:count0, count1, count2, count3) private(tid)\n  for (int i = 0; i < local_x.size(); i++) {\n    double fraction = local_x[i] - (int)local_x[i];\n    if (fraction < 0.25) count0 += 1;\n    else if (fraction < 0.5) count1 += 1;\n    else if (fraction < 0.75) count2 += 1;\n    else if (fraction < 1) count3 += 1;\n  }\n\n  MPI_Gather(&count0, 1, MPI_INT, &count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Gather(&count1, 1, MPI_INT, &count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Gather(&count2, 1, MPI_INT, &count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Gather(&count3, 1, MPI_INT, &count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (tid == 0) {\n    bins[0] = count0;\n    bins[1] = count1;\n    bins[2] = count2;\n    bins[3] = count3;\n  }\n}",
            "int num_processes, my_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  // compute the number of doubles in x that are assigned to each rank\n  size_t elements_per_rank = x.size() / num_processes;\n  size_t remainder = x.size() % num_processes;\n  size_t my_offset = my_rank * elements_per_rank + std::min(my_rank, remainder);\n  size_t my_length = elements_per_rank + (my_rank < remainder? 1 : 0);\n\n  // count the number of doubles in my chunk that belong in each quartile\n  std::array<size_t, 4> my_bins{0, 0, 0, 0};\n  size_t i;\n#pragma omp parallel for private(i)\n  for (i = 0; i < my_length; ++i) {\n    double d = x[my_offset + i] - std::floor(x[my_offset + i]);\n    if (d < 0.25) my_bins[0]++;\n    else if (d < 0.5) my_bins[1]++;\n    else if (d < 0.75) my_bins[2]++;\n    else my_bins[3]++;\n  }\n\n  // send my bins to rank 0 and wait until the other ranks are done\n  if (my_rank == 0) {\n    for (i = 1; i < num_processes; ++i) {\n      MPI_Status status;\n      MPI_Recv(my_bins.data(), 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    MPI_Send(my_bins.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n  }\n\n  if (my_rank == 0) {\n    // combine the bins from each rank\n    for (i = 0; i < 4; ++i) bins[i] = std::accumulate(my_bins.begin(), my_bins.end(), bins[i]);\n  }\n}",
            "if (bins.size()!= 4) {\n        throw std::runtime_error(\"bins must have 4 elements.\");\n    }\n    int num_ranks;\n    int rank_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n\n    // Compute the counts\n    std::array<size_t, 4> counts{};\n    for (auto const& v : x) {\n        int bin_id = v > 0.75? 3 : (v > 0.5? 2 : (v > 0.25? 1 : 0));\n        ++counts[bin_id];\n    }\n\n    // Combine the counts\n    std::array<size_t, 4> recv_counts{};\n    if (rank_id == 0) {\n        // Root has to broadcast the result\n        for (int r = 1; r < num_ranks; ++r) {\n            MPI_Recv(recv_counts.data(), 4, MPI_UNSIGNED_LONG_LONG, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int i = 0; i < 4; ++i) {\n                counts[i] += recv_counts[i];\n            }\n        }\n    } else {\n        // Non-roots send their results\n        MPI_Send(counts.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Copy the result to `bins`\n    if (rank_id == 0) {\n        bins = counts;\n    }\n}",
            "// your code goes here\n\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int num_partitions = size;\n    int num_elements_per_partition = x.size() / num_partitions;\n    int num_elements_per_rank = x.size() / size;\n\n    std::vector<size_t> counts(4, 0);\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        double elem = x[i];\n        int which_quartile = 0;\n        if (elem < 0.25) {\n            which_quartile = 0;\n        } else if (elem < 0.50) {\n            which_quartile = 1;\n        } else if (elem < 0.75) {\n            which_quartile = 2;\n        } else {\n            which_quartile = 3;\n        }\n        #pragma omp atomic\n        counts[which_quartile]++;\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, counts.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = counts;\n    }\n}",
            "// TODO: Implement this function\n}",
            "}",
            "// YOUR CODE HERE\n}",
            "bins = {0, 0, 0, 0};\n\n    // TODO: your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_per_process = n / size;\n    int n_leftover = n % size;\n\n    std::vector<double> y;\n    if (rank == 0) {\n        y = std::vector<double>(n_per_process + n_leftover);\n    } else {\n        y = std::vector<double>(n_per_process);\n    }\n\n#pragma omp parallel for\n    for (int i = 0; i < n_per_process; i++) {\n        y[i] = x[rank * n_per_process + i];\n    }\n    if (rank < n_leftover) {\n        y[n_per_process] = x[n_per_process * size + rank];\n    }\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n#pragma omp parallel for\n    for (int i = 0; i < y.size(); i++) {\n        if (y[i] < 0.25) {\n            local_bins[0]++;\n        } else if (y[i] >= 0.25 && y[i] < 0.5) {\n            local_bins[1]++;\n        } else if (y[i] >= 0.5 && y[i] < 0.75) {\n            local_bins[2]++;\n        } else if (y[i] >= 0.75 && y[i] < 1.0) {\n            local_bins[3]++;\n        }\n    }\n\n    std::array<size_t, 4> global_bins;\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "// TODO: insert your code here\n\n}",
            "if (x.size() < 1) return;\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (auto const& value : x) {\n        if (value < 0.25) {\n            local_bins[0]++;\n        } else if (value < 0.5) {\n            local_bins[1]++;\n        } else if (value < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> global_bins;\n    if (rank == 0) {\n        global_bins = {0, 0, 0, 0};\n    }\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "/* your implementation here */\n\n\n\n\n\n\n\n}",
            "int const n = x.size();\n  int const my_rank = MPI_Comm_rank(MPI_COMM_WORLD, NULL);\n  int const comm_size = MPI_Comm_size(MPI_COMM_WORLD, NULL);\n\n  int const num_elems_per_rank = n / comm_size;\n  int const extra_elems_per_rank = n % comm_size;\n\n  std::vector<size_t> counts(4, 0);\n\n  if (my_rank == 0) {\n    // Rank 0 gets all the extra elements\n    counts = bins;\n  }\n\n  std::vector<double> local_x;\n\n  if (my_rank == 0) {\n    local_x.reserve(num_elems_per_rank + extra_elems_per_rank);\n    // Rank 0 gets the extra elements first\n    for (int i = 0; i < extra_elems_per_rank; ++i) {\n      local_x.push_back(x[i]);\n    }\n  } else {\n    local_x.reserve(num_elems_per_rank);\n  }\n\n  // Copy all elements for this rank\n  for (int i = 0; i < num_elems_per_rank; ++i) {\n    local_x.push_back(x[extra_elems_per_rank + i + my_rank * num_elems_per_rank]);\n  }\n\n  // Do the computation on this rank\n#pragma omp parallel for schedule(dynamic)\n  for (int i = 0; i < local_x.size(); ++i) {\n    if (local_x[i] >= 0.0 && local_x[i] <= 0.25) {\n      counts[0]++;\n    } else if (local_x[i] > 0.25 && local_x[i] <= 0.5) {\n      counts[1]++;\n    } else if (local_x[i] > 0.5 && local_x[i] <= 0.75) {\n      counts[2]++;\n    } else if (local_x[i] > 0.75 && local_x[i] <= 1.0) {\n      counts[3]++;\n    }\n  }\n\n  // Now aggregate the result\n  std::vector<size_t> result(4, 0);\n  std::vector<MPI_Request> requests;\n\n  if (my_rank == 0) {\n    for (int i = 1; i < comm_size; ++i) {\n      // Request for result from rank i\n      requests.push_back(MPI_Request());\n      MPI_Irecv(result.data(), 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD,\n                requests.data() + i - 1);\n    }\n    // Aggregate from rank 1 to rank 0\n    for (int i = 1; i < comm_size; ++i) {\n      MPI_Wait(requests.data() + i - 1, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 4; ++j) {\n        counts[j] += result[j];\n      }\n    }\n    // Store the result\n    bins = {counts[0], counts[1], counts[2], counts[3]};\n  } else {\n    // Send the result to rank 0\n    MPI_Send(counts.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n  \n}",
            "int myRank, commSize;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n\n    auto localCount = std::array<size_t, 4>{0, 0, 0, 0};\n\n    if (x.size() % commSize!= 0) {\n        if (myRank == 0) {\n            std::cerr << \"MPI ranks do not match the number of items in the vector.\\n\";\n        }\n        MPI_Abort(MPI_COMM_WORLD, 1);\n    }\n\n    if (myRank == 0) {\n        auto localChunk = x.size() / commSize;\n        std::vector<double> buffer;\n        for (int i = 1; i < commSize; ++i) {\n            buffer.resize(localChunk);\n            MPI_Recv(buffer.data(), localChunk, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            std::copy(buffer.begin(), buffer.end(), std::back_inserter(x));\n        }\n    }\n    else {\n        auto localChunk = x.size() / commSize;\n        auto start = localChunk * myRank;\n        auto stop = start + localChunk;\n\n        MPI_Send(&x[start], localChunk, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (myRank == 0) {\n        for (auto&& el : x) {\n            auto q = (el - std::floor(el)) * 4;\n\n            if (q >= 0 && q < 1) ++localCount[0];\n            else if (q >= 1 && q < 2) ++localCount[1];\n            else if (q >= 2 && q < 3) ++localCount[2];\n            else ++localCount[3];\n        }\n    }\n    else {\n        std::vector<size_t> chunkCounts;\n        chunkCounts.resize(commSize);\n\n        for (auto&& el : x) {\n            auto q = (el - std::floor(el)) * 4;\n\n            if (q >= 0 && q < 1) ++localCount[0];\n            else if (q >= 1 && q < 2) ++localCount[1];\n            else if (q >= 2 && q < 3) ++localCount[2];\n            else ++localCount[3];\n        }\n\n        MPI_Gather(&localCount, localCount.size(), MPI_UNSIGNED_LONG, chunkCounts.data(),\n                   localCount.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n        if (myRank == 0) {\n            for (int i = 1; i < commSize; ++i) {\n                for (int j = 0; j < localCount.size(); ++j) {\n                    bins[j] += chunkCounts[i * localCount.size() + j];\n                }\n            }\n        }\n    }\n}",
            "// TODO: implement\n    MPI_Status status;\n    const int master = 0;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    auto count = [&](int i) {\n        return std::count_if(x.begin(), x.end(), [i](double x) { return x >= i && x < i + 0.25; });\n    };\n\n    // Compute local counts\n    std::array<size_t, 4> local_bins{};\n    for (int i = 0; i < 4; i++) {\n        local_bins[i] = count(i);\n    }\n\n    // Send counts to master\n    if (rank!= master) {\n        MPI_Send(&local_bins, 4, MPI_UNSIGNED_LONG, master, 0, MPI_COMM_WORLD);\n    } else {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&bins, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += local_bins[j];\n            }\n        }\n    }\n}",
            "// TODO: implement\n}",
            "// TODO\n}",
            "/*\n    Fill in your code here.\n    */\n}",
            "// TODO\n}",
            "// Your code here.\n}",
            "// TODO\n}",
            "auto n = x.size();\n\n\tstd::vector<size_t> local_count(4, 0);\n\n\t#pragma omp parallel for\n\tfor (int i = 0; i < n; ++i) {\n\t\tauto fract = x[i] - std::floor(x[i]);\n\t\tif (fract < 0.25) {\n\t\t\tlocal_count[0]++;\n\t\t} else if (fract < 0.5) {\n\t\t\tlocal_count[1]++;\n\t\t} else if (fract < 0.75) {\n\t\t\tlocal_count[2]++;\n\t\t} else {\n\t\t\tlocal_count[3]++;\n\t\t}\n\t}\n\n\tMPI_Reduce(local_count.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "/* YOUR CODE HERE */\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    const size_t N = x.size();\n    std::array<size_t, 4> counts;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; ++i) {\n        if (x[i] < 0.25)\n            counts[0]++;\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n            counts[1]++;\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n            counts[2]++;\n        else\n            counts[3]++;\n    }\n\n    // TODO: collect the results of counts from every process\n    // and store it in bins\n    // Note: counts is not a vector, so it can't be passed to MPI_Gather.\n    // Instead, the contents of counts should be copied into a temporary buffer,\n    // then that buffer should be passed to MPI_Gather\n}",
            "// TODO: your implementation here\n}",
            "int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   // Add code here:\n   int bins_size = 4;\n   double max, min;\n   int* s = new int[bins_size];\n   int* r = new int[bins_size];\n\n   for(int i = 0; i < bins_size; i++){\n      s[i] = 0;\n   }\n\n   if(world_rank == 0){\n      max = *std::max_element(x.begin(), x.end());\n      min = *std::min_element(x.begin(), x.end());\n      MPI_Bcast(&max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   } else {\n      MPI_Bcast(&max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n\n   int len = x.size() / world_size;\n   int remaining = x.size() % world_size;\n   int s_len = x.size();\n   int s_remaining = 0;\n   double* local_x = new double[len];\n   if(world_rank == 0){\n      local_x = new double[len + remaining];\n   } else {\n      local_x = new double[len];\n   }\n\n   #pragma omp parallel for\n   for(int i = 0; i < len; i++){\n      local_x[i] = x[i + world_rank * len];\n   }\n   if(world_rank == 0){\n      #pragma omp parallel for\n      for(int i = 0; i < remaining; i++){\n         local_x[i + len] = x[i + len * world_size];\n      }\n   }\n\n   #pragma omp parallel for\n   for(int i = 0; i < len; i++){\n      if(local_x[i] > min && local_x[i] <= max){\n         if(local_x[i] - floor(local_x[i]) <= 0.25){\n            s[0]++;\n         } else if(local_x[i] - floor(local_x[i]) <= 0.5){\n            s[1]++;\n         } else if(local_x[i] - floor(local_x[i]) <= 0.75){\n            s[2]++;\n         } else {\n            s[3]++;\n         }\n      }\n   }\n   MPI_Reduce(s, r, bins_size, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if(world_rank == 0){\n      for(int i = 0; i < bins_size; i++){\n         bins[i] = r[i];\n      }\n   }\n\n   delete[] local_x;\n   delete[] s;\n   delete[] r;\n}",
            "// TODO\n}",
            "// Replace this with your code.\n  // (You may need to define additional local variables.)\n}",
            "/* Your code goes here */\n}",
            "size_t n = x.size();\n  size_t n_per_rank = n/size;\n  std::vector<double> part_x(n_per_rank);\n  MPI_Request reqs[3];\n  double bins_arr[4];\n  if (rank == 0) {\n    MPI_Recv(&bins_arr, 4, MPI_INT, 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    bins[0] = bins_arr[0];\n    bins[1] = bins_arr[1];\n    bins[2] = bins_arr[2];\n    bins[3] = bins_arr[3];\n  }\n\n  // std::vector<double> part_x = {x.begin(), x.begin() + n_per_rank};\n  // #pragma omp parallel for reduction(+:bins)\n  // for (size_t i = 0; i < n_per_rank; i++) {\n  //   if (part_x[i] < 0.25) {\n  //     bins[0]++;\n  //   } else if (part_x[i] < 0.5) {\n  //     bins[1]++;\n  //   } else if (part_x[i] < 0.75) {\n  //     bins[2]++;\n  //   } else {\n  //     bins[3]++;\n  //   }\n  // }\n\n  if (rank == 0) {\n    std::vector<double> part_x(x.begin(), x.begin() + n_per_rank);\n    #pragma omp parallel for reduction(+:bins)\n    for (size_t i = 0; i < n_per_rank; i++) {\n      if (part_x[i] < 0.25) {\n        bins[0]++;\n      } else if (part_x[i] < 0.5) {\n        bins[1]++;\n      } else if (part_x[i] < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n\n    MPI_Send(&bins, 4, MPI_INT, 1, 0, MPI_COMM_WORLD);\n\n    for (int r = 1; r < size; r++) {\n      MPI_Recv(&bins_arr, 4, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[0] += bins_arr[0];\n      bins[1] += bins_arr[1];\n      bins[2] += bins_arr[2];\n      bins[3] += bins_arr[3];\n    }\n  } else {\n    std::vector<double> part_x(x.begin() + n_per_rank, x.end());\n    #pragma omp parallel for reduction(+:bins)\n    for (size_t i = 0; i < n_per_rank; i++) {\n      if (part_x[i] < 0.25) {\n        bins[0]++;\n      } else if (part_x[i] < 0.5) {\n        bins[1]++;\n      } else if (part_x[i] < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n\n    MPI_Send(&bins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "/* CODE HERE */\n  int n = x.size();\n  int p = bins.size();\n  int num_processes, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int chunk = n / num_processes;\n  int remainder = n % num_processes;\n  std::vector<int> local_bins(p, 0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < chunk; i++) {\n    int index = rank * chunk + i;\n    if (x[index] >= 0 && x[index] < 0.25) {\n      #pragma omp critical\n      {\n        local_bins[0]++;\n      }\n    } else if (x[index] >= 0.25 && x[index] < 0.5) {\n      #pragma omp critical\n      {\n        local_bins[1]++;\n      }\n    } else if (x[index] >= 0.5 && x[index] < 0.75) {\n      #pragma omp critical\n      {\n        local_bins[2]++;\n      }\n    } else {\n      #pragma omp critical\n      {\n        local_bins[3]++;\n      }\n    }\n  }\n\n  // Last chunk\n  for (int i = 0; i < remainder; i++) {\n    int index = rank * chunk + chunk + i;\n    if (x[index] >= 0 && x[index] < 0.25) {\n      #pragma omp critical\n      {\n        local_bins[0]++;\n      }\n    } else if (x[index] >= 0.25 && x[index] < 0.5) {\n      #pragma omp critical\n      {\n        local_bins[1]++;\n      }\n    } else if (x[index] >= 0.5 && x[index] < 0.75) {\n      #pragma omp critical\n      {\n        local_bins[2]++;\n      }\n    } else {\n      #pragma omp critical\n      {\n        local_bins[3]++;\n      }\n    }\n  }\n\n  // Copy local bins to global bins\n  MPI_Reduce(local_bins.data(), bins.data(), p, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n\n}",
            "MPI_Comm communicator = MPI_COMM_WORLD;\n\n\tint rank;\n\tMPI_Comm_rank(communicator, &rank);\n\n\tint size;\n\tMPI_Comm_size(communicator, &size);\n\n\tint lowerBound = rank * x.size() / size;\n\tint upperBound = (rank + 1) * x.size() / size;\n\n\tstd::array<size_t, 4> binsLocal;\n\tstd::fill(binsLocal.begin(), binsLocal.end(), 0);\n\n\t#pragma omp parallel for\n\tfor (int i = lowerBound; i < upperBound; i++) {\n\t\tif (x[i] < 0.25) {\n\t\t\t#pragma omp atomic\n\t\t\tbinsLocal[0]++;\n\t\t}\n\t\tif (x[i] < 0.5 && x[i] >= 0.25) {\n\t\t\t#pragma omp atomic\n\t\t\tbinsLocal[1]++;\n\t\t}\n\t\tif (x[i] < 0.75 && x[i] >= 0.5) {\n\t\t\t#pragma omp atomic\n\t\t\tbinsLocal[2]++;\n\t\t}\n\t\tif (x[i] < 1 && x[i] >= 0.75) {\n\t\t\t#pragma omp atomic\n\t\t\tbinsLocal[3]++;\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tstd::array<size_t, 4> binsAll = {0, 0, 0, 0};\n\t\tMPI_Gather(binsLocal.data(), 4, MPI_UNSIGNED_LONG, binsAll.data(), 4, MPI_UNSIGNED_LONG, 0, communicator);\n\t\tbins = binsAll;\n\t}\n\telse {\n\t\tMPI_Gather(binsLocal.data(), 4, MPI_UNSIGNED_LONG, NULL, 4, MPI_UNSIGNED_LONG, 0, communicator);\n\t}\n\n}",
            "int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int n_local = n / size;\n    int n_local_rem = n % size;\n\n    std::vector<int> bins_local(4);\n    std::vector<int> bins_global(4);\n\n    int start = rank * n_local + std::min(rank, n_local_rem);\n    int end = (rank + 1) * n_local + std::min(rank + 1, n_local_rem);\n\n    for (int i = start; i < end; i++) {\n        int bin = 0;\n        if (x[i] > 0.75) bin = 3;\n        else if (x[i] > 0.5) bin = 2;\n        else if (x[i] > 0.25) bin = 1;\n        bins_local[bin] += 1;\n    }\n\n    MPI_Reduce(&bins_local[0], &bins_global[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) bins = bins_global;\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int my_rank, comm_size;\n  MPI_Comm_rank(comm, &my_rank);\n  MPI_Comm_size(comm, &comm_size);\n\n  std::array<size_t, 4> my_bins{0, 0, 0, 0};\n\n  // TODO: Implement the parallel algorithm.\n  // Hint: Use MPI_Scatter to distribute x among the processes and use OpenMP\n  // to compute the counts in parallel.\n  // Use MPI_Gather to collect the counts on rank 0.\n\n  // TODO: Check for errors\n\n  // TODO: Copy result back to bins\n  bins = my_bins;\n}",
            "// TODO\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> x_rank;\n    size_t elements_per_rank = (size_t) std::ceil((double)x.size() / size);\n    size_t start_idx = rank*elements_per_rank;\n    if (rank < size - 1) {\n        x_rank.assign(x.begin() + start_idx, x.begin() + start_idx + elements_per_rank);\n    }\n    else {\n        x_rank.assign(x.begin() + start_idx, x.end());\n    }\n\n    std::array<size_t, 4> bins_rank;\n    for (auto& bin : bins_rank)\n        bin = 0;\n\n    size_t elements_per_thread = (size_t) std::ceil((double)x_rank.size() / omp_get_num_threads());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x_rank.size(); ++i) {\n        double x_i = x_rank[i];\n        double mod = std::fmod(x_i, 1);\n        if (mod < 0.25)\n            bins_rank[0] += 1;\n        else if (mod < 0.5)\n            bins_rank[1] += 1;\n        else if (mod < 0.75)\n            bins_rank[2] += 1;\n        else\n            bins_rank[3] += 1;\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(bins_rank.data(), bins.data(), bins_rank.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Reduce(bins_rank.data(), NULL, bins_rank.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "const int n = x.size();\n\n    bins = {0, 0, 0, 0};\n\n    // TODO: your code here\n}",
            "// TODO: Implement this function.\n    // Note: You can use the MPI function MPI_Reduce to combine bins from each rank.\n}",
            "int rank = 0;\n    int nproc = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // TODO: Implement this function\n\n}",
            "const int numRanks = omp_get_num_threads();\n\tconst int rank = omp_get_thread_num();\n\n\tif (rank == 0)\n\t{\n\t\tfor (int i = 0; i < numRanks; ++i)\n\t\t{\n\t\t\tstd::array<size_t, 4> counts;\n\t\t\tMPI_Recv(&counts, 4, MPI_SIZE_T, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tfor (int j = 0; j < 4; ++j)\n\t\t\t{\n\t\t\t\tbins[j] += counts[j];\n\t\t\t}\n\t\t}\n\t}\n\telse\n\t{\n\t\tstd::array<size_t, 4> counts;\n\t\tfor (size_t i = rank; i < x.size(); i += numRanks)\n\t\t{\n\t\t\tdouble num = x[i];\n\t\t\tif (num >= 0 && num < 0.25)\n\t\t\t{\n\t\t\t\tcounts[0] += 1;\n\t\t\t}\n\t\t\telse if (num >= 0.25 && num < 0.5)\n\t\t\t{\n\t\t\t\tcounts[1] += 1;\n\t\t\t}\n\t\t\telse if (num >= 0.5 && num < 0.75)\n\t\t\t{\n\t\t\t\tcounts[2] += 1;\n\t\t\t}\n\t\t\telse if (num >= 0.75 && num < 1)\n\t\t\t{\n\t\t\t\tcounts[3] += 1;\n\t\t\t}\n\t\t}\n\t\tMPI_Send(&counts, 4, MPI_SIZE_T, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "// TODO: Implement me!\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t start = rank * x.size() / size;\n    size_t end = (rank + 1) * x.size() / size;\n\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n    // TODO: Use MPI and OpenMP to count the fractions in parallel.\n    #pragma omp parallel for\n    for (size_t i = start; i < end; ++i) {\n        if (x[i] < 0.25)\n            local_bins[0]++;\n        else if (x[i] < 0.5)\n            local_bins[1]++;\n        else if (x[i] < 0.75)\n            local_bins[2]++;\n        else if (x[i] < 1)\n            local_bins[3]++;\n    }\n\n    if (rank == 0)\n        std::fill(bins.begin(), bins.end(), 0);\n\n    MPI_Reduce(&local_bins, &bins, bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here.\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num = x.size();\n  int chunk_size = num / size;\n  int remainder = num % size;\n  int start = rank * chunk_size;\n  int end = (rank + 1) * chunk_size;\n\n  if (rank == 0) {\n    end += remainder;\n  } else if (rank == size - 1) {\n    start -= remainder;\n  }\n\n  // count\n  std::array<size_t, 4> localBins{0, 0, 0, 0};\n  for (int i = start; i < end; ++i) {\n    double n = x[i] - std::floor(x[i]);\n    if (n < 0.25) {\n      ++localBins[0];\n    } else if (n < 0.5) {\n      ++localBins[1];\n    } else if (n < 0.75) {\n      ++localBins[2];\n    } else {\n      ++localBins[3];\n    }\n  }\n\n  // sum\n  MPI_Reduce(localBins.data(), bins.data(), 4, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* your code here */\n\n}",
            "// YOUR CODE HERE\n  \n  bins = {0, 0, 0, 0}; // initialise bins\n\n  if (bins.size()!= 4) {\n    std::cout << \"ERROR: bins.size() must be 4\" << std::endl;\n    exit(1);\n  }\n\n  int num_threads;\n  int my_rank;\n  int comm_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n  // set number of threads per rank\n  num_threads = omp_get_max_threads();\n  omp_set_num_threads(num_threads);\n\n  if (my_rank == 0) {\n    std::cout << \"Number of threads per rank: \" << num_threads << std::endl;\n  }\n\n  size_t start = 0;\n  size_t end = x.size() / comm_size;\n\n  if (x.size() < comm_size) {\n    std::cout << \"ERROR: x.size() < comm_size\" << std::endl;\n    exit(1);\n  }\n\n  // check if there is a remainder\n  if ((x.size() % comm_size)!= 0) {\n    end += 1;\n  }\n\n  // compute total count of elements per rank\n  int count[comm_size];\n  MPI_Gather(&end, 1, MPI_INT, count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (my_rank == 0) {\n    int temp = 0;\n    for (int i=0; i < comm_size; ++i) {\n      count[i] += temp;\n      temp = count[i];\n    }\n  }\n\n  // define local arrays to store counts\n  std::array<size_t, 4> local_bins;\n  for (size_t i = 0; i < 4; ++i) {\n    local_bins[i] = 0;\n  }\n\n  // define a local array to store values of x on each rank\n  std::vector<double> local_x;\n  for (size_t i = 0; i < end; ++i) {\n    local_x.push_back(x[i + start]);\n  }\n\n  // compute local counts\n  #pragma omp parallel for\n  for (size_t i = 0; i < end; ++i) {\n    if (local_x[i] >= 0 && local_x[i] <= 0.25) {\n      local_bins[0] += 1;\n    }\n    else if (local_x[i] > 0.25 && local_x[i] <= 0.5) {\n      local_bins[1] += 1;\n    }\n    else if (local_x[i] > 0.5 && local_x[i] <= 0.75) {\n      local_bins[2] += 1;\n    }\n    else if (local_x[i] > 0.75 && local_x[i] <= 1) {\n      local_bins[3] += 1;\n    }\n  }\n\n  // compute global counts\n  if (my_rank == 0) {\n    for (int i=0; i < comm_size; ++i) {\n      for (int j=0; j < 4; ++j) {\n        bins[j] += local_bins[j] * count[i];\n      }\n    }\n  }\n\n  // gather counts from other ranks to rank 0\n  if (my_rank!= 0) {\n    MPI_Gather(&local_bins, 4, MPI_UNSIGNED_LONG, NULL, 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  }\n  else {\n    MPI_Gather(&local_bins, 4, MPI_UNSIGNED_LONG, bins.data(), 4,",
            "// TODO: implement\n}",
            "}",
            "}",
            "int numThreads;\n    MPI_Comm_rank(MPI_COMM_WORLD, &numThreads);\n    int numProcesses;\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    int const N = x.size();\n\n    /* Your code here */\n}",
            "bins = {{0, 0, 0, 0}};\n   int size, rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int num_bins = size;\n   int local_size = x.size() / num_bins;\n   int local_start = local_size * rank;\n   int local_end = local_start + local_size;\n\n   // Your code goes here\n   //std::vector<double> my_x = std::vector<double>(x.begin() + local_start, x.begin() + local_end);\n   std::array<size_t, 4> my_bins = {{0, 0, 0, 0}};\n\n   for (size_t i = local_start; i < local_end; i++)\n   {\n      double fractionalPart = x[i] - std::floor(x[i]);\n      if (fractionalPart < 0.25)\n      {\n         my_bins[0]++;\n      }\n      else if (fractionalPart < 0.5)\n      {\n         my_bins[1]++;\n      }\n      else if (fractionalPart < 0.75)\n      {\n         my_bins[2]++;\n      }\n      else\n      {\n         my_bins[3]++;\n      }\n   }\n\n   MPI_Reduce(&my_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\n\n   // Your code goes here\n}",
            "// TODO\n}",
            "int rank = 0, size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  //printf(\"rank %d, size %d\\n\", rank, size);\n\n  size_t chunkSize = x.size() / size;\n  size_t remainder = x.size() % size;\n  size_t local_size = chunkSize + ((rank < remainder)? 1 : 0);\n  size_t offset = chunkSize * rank + ((rank < remainder)? rank : remainder);\n\n  double local_bins[4] = {0};\n  #pragma omp parallel for\n  for (size_t i = 0; i < local_size; i++) {\n    double fraction = x[offset + i] - std::floor(x[offset + i]);\n    if (fraction >= 0 && fraction < 0.25) {\n      local_bins[0]++;\n    } else if (fraction >= 0.25 && fraction < 0.5) {\n      local_bins[1]++;\n    } else if (fraction >= 0.5 && fraction < 0.75) {\n      local_bins[2]++;\n    } else {\n      local_bins[3]++;\n    }\n  }\n  MPI_Reduce(&local_bins, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Fill in this function\n\n}",
            "int rank = 0;\n  int size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: Use MPI and OpenMP to compute `bins` in parallel\n\n}",
            "// TODO: add your code here\n    int my_rank, size;\n    int n;\n    int num_threads;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    n = x.size();\n    // printf(\"n is %d, my_rank is %d\\n\", n, my_rank);\n\n    num_threads = omp_get_max_threads();\n    int local_n = n / size;\n    // printf(\"local_n is %d\\n\", local_n);\n    int start = my_rank * local_n;\n    int end = (my_rank + 1) * local_n;\n    // printf(\"my_rank is %d, start is %d, end is %d\\n\", my_rank, start, end);\n\n    int tmp_bin[4];\n    std::fill(tmp_bin, tmp_bin + 4, 0);\n    double *local_x = &x[0] + start;\n    // printf(\"my_rank is %d, x[0] is %f\\n\", my_rank, x[0]);\n    // printf(\"my_rank is %d, x[5] is %f\\n\", my_rank, x[5]);\n    // printf(\"my_rank is %d, x[3] is %f\\n\", my_rank, x[3]);\n    // printf(\"my_rank is %d, x[6] is %f\\n\", my_rank, x[6]);\n    // printf(\"my_rank is %d, x[7] is %f\\n\", my_rank, x[7]);\n    // printf(\"my_rank is %d, local_x[0] is %f\\n\", my_rank, local_x[0]);\n    // printf(\"my_rank is %d, local_x[5] is %f\\n\", my_rank, local_x[5]);\n    // printf(\"my_rank is %d, local_x[3] is %f\\n\", my_rank, local_x[3]);\n    // printf(\"my_rank is %d, local_x[6] is %f\\n\", my_rank, local_x[6]);\n    // printf(\"my_rank is %d, local_x[7] is %f\\n\", my_rank, local_x[7]);\n    // printf(\"my_rank is %d, x.size() is %d\\n\", my_rank, x.size());\n    // printf(\"my_rank is %d, n is %d\\n\", my_rank, n);\n    // printf(\"my_rank is %d, local_n is %d\\n\", my_rank, local_n);\n    // printf(\"my_rank is %d, start is %d\\n\", my_rank, start);\n    // printf(\"my_rank is %d, end is %d\\n\", my_rank, end);\n\n    double local_bins[4] = {0};\n    int index = 0;\n    omp_set_num_threads(num_threads);\n#pragma omp parallel shared(x, local_bins) private(index)\n    {\n        index = omp_get_thread_num();\n        // printf(\"my_rank is %d, index is %d\\n\", my_rank, index);\n        // printf(\"my_rank is %d, num_threads is %d\\n\", my_rank, num_threads);\n        // printf(\"my_rank is %d, n is %d\\n\", my_rank, n);\n        // printf(\"my_rank is %d, local_n is %d\\n\", my_rank, local_n);\n        // printf(\"my_rank is %d, start is %d\\n\", my_rank, start);\n        // printf(\"my_rank is %d, end is %d\\n\", my_rank, end);\n        // printf(\"my_rank is %d, x.size() is %d\\n\", my_rank, x.size());\n#pragma omp for\n        for (int i = start; i < end; ++i) {\n            // printf(\"my_rank is %d, x[%d] is %f\\n\",",
            "// TODO: implement\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n}",
            "/* Your code goes here */\n\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n#pragma omp parallel\n    {\n        std::array<size_t, 4> privateBins;\n        privateBins[0] = privateBins[1] = privateBins[2] = privateBins[3] = 0;\n#pragma omp for nowait\n        for (double val : x) {\n            if (val < 1.0 && val >= 0.75)\n                privateBins[3] += 1;\n            else if (val < 0.75 && val >= 0.5)\n                privateBins[2] += 1;\n            else if (val < 0.5 && val >= 0.25)\n                privateBins[1] += 1;\n            else if (val < 0.25 && val >= 0)\n                privateBins[0] += 1;\n        }\n#pragma omp critical\n        for (int i = 0; i < 4; i++) {\n            bins[i] += privateBins[i];\n        }\n    }\n}",
            "size_t const num_threads = omp_get_max_threads();\n    std::vector<size_t> counts(4*num_threads, 0);\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        size_t const thread_id = omp_get_thread_num();\n        size_t const chunk_size = (x.size()+num_threads-1)/num_threads;\n        size_t const start = thread_id*chunk_size;\n        size_t const end = std::min(start+chunk_size, x.size());\n\n        for (size_t i=start; i<end; ++i) {\n            double const frac = x[i] - floor(x[i]);\n            if (frac < 0.25) {\n                counts[thread_id]++;\n            } else if (frac < 0.5) {\n                counts[thread_id+num_threads]++;\n            } else if (frac < 0.75) {\n                counts[thread_id+2*num_threads]++;\n            } else {\n                counts[thread_id+3*num_threads]++;\n            }\n        }\n    }\n\n    #pragma omp parallel for num_threads(num_threads)\n    for (size_t i=0; i<4; ++i) {\n        size_t sum = 0;\n        for (size_t j=0; j<num_threads; ++j) {\n            sum += counts[i*num_threads+j];\n        }\n        bins[i] = sum;\n    }\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (size == 1) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = 0;\n         for (int j = 0; j < x.size(); j++) {\n            if (x[j] >= i/4.0 && x[j] < (i+1)/4.0) {\n               bins[i] += 1;\n            }\n         }\n      }\n   }\n   else {\n      int chunkSize = x.size()/size;\n      std::vector<double> chunk;\n      std::array<size_t, 4> tempBins;\n\n      #pragma omp parallel for\n      for (int i = 0; i < size; i++) {\n         chunk.assign(x.begin() + i*chunkSize, x.begin() + (i+1)*chunkSize);\n         for (int j = 0; j < 4; j++) {\n            tempBins[j] = 0;\n            for (int k = 0; k < chunk.size(); k++) {\n               if (chunk[k] >= j/4.0 && chunk[k] < (j+1)/4.0) {\n                  tempBins[j] += 1;\n               }\n            }\n         }\n         MPI_Send(tempBins.data(), 4, MPI_SIZE_T, 0, i, MPI_COMM_WORLD);\n      }\n\n      if (rank == 0) {\n         for (int i = 0; i < size; i++) {\n            MPI_Recv(tempBins.data(), 4, MPI_SIZE_T, i, i, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++) {\n               bins[j] += tempBins[j];\n            }\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int num_processes, rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n\n  // get the size of the vector\n  int size = x.size();\n\n  // get the number of doubles in x that have a fractional part in [0, 0.25)\n  int size1;\n  #pragma omp parallel for\n  for (size_t i = 0; i < size; i++){\n    if (x[i] >= 0 && x[i] < 0.25)\n      size1++;\n  }\n\n  // get the number of doubles in x that have a fractional part in [0.25, 0.5)\n  int size2;\n  #pragma omp parallel for\n  for (size_t i = 0; i < size; i++){\n    if (x[i] >= 0.25 && x[i] < 0.5)\n      size2++;\n  }\n\n  // get the number of doubles in x that have a fractional part in [0.5, 0.75)\n  int size3;\n  #pragma omp parallel for\n  for (size_t i = 0; i < size; i++){\n    if (x[i] >= 0.5 && x[i] < 0.75)\n      size3++;\n  }\n\n  // get the number of doubles in x that have a fractional part in [0.75, 1)\n  int size4;\n  #pragma omp parallel for\n  for (size_t i = 0; i < size; i++){\n    if (x[i] >= 0.75 && x[i] < 1)\n      size4++;\n  }\n\n  int local_sizes[4] = {size1, size2, size3, size4};\n  int global_sizes[4] = {};\n  MPI_Gather(&local_sizes, 4, MPI_INT, &global_sizes, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0){\n    bins[0] = global_sizes[0];\n    bins[1] = global_sizes[1];\n    bins[2] = global_sizes[2];\n    bins[3] = global_sizes[3];\n  }\n}",
            "/* YOUR CODE HERE */\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<size_t> bin_local(4, 0);\n  int i;\n\n  #pragma omp parallel for\n  for (i = 0; i < x.size(); i++) {\n    int bin_num = 0;\n    if (x[i] >= 0 && x[i] < 0.25) {\n      bin_num = 0;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bin_num = 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bin_num = 2;\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      bin_num = 3;\n    } else {\n      bin_num = -1;\n    }\n    bin_local[bin_num]++;\n  }\n\n  std::array<size_t, 4> bin_global = {0, 0, 0, 0};\n  MPI_Reduce(bin_local.data(), bin_global.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = bin_global;\n  }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: compute the number of doubles in the input vector x that fall in the\n    //       four quartiles. Use MPI and OpenMP to do this efficiently.\n    //\n    //       Store the counts in `bins`.\n    //\n    //       You may assume that MPI has already been initialized.\n    //       Every rank has a complete copy of x. The result is stored in bins on rank 0.\n    //\n    //       Examples:\n    //\n    //       input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n    //       output: [2, 1, 2, 2]\n    //\n    //       input: [1.9, 0.2, 0.6, 10.1, 7.4]\n    //       output: [2, 1, 1, 1]\n\n}",
            "// **************************************************************************\n    // **************************************************************************\n    // Your code goes here!\n    // **************************************************************************\n    // **************************************************************************\n}",
            "// fill in your code here\n\n}",
            "// Your code here\n}",
            "// TODO: Your code goes here.\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = 0; i < x.size(); i++)\n    {\n        if (x[i] < 0.25)\n        {\n            bins[0] += 1;\n        }\n        else if (x[i] >= 0.25 && x[i] < 0.5)\n        {\n            bins[1] += 1;\n        }\n        else if (x[i] >= 0.5 && x[i] < 0.75)\n        {\n            bins[2] += 1;\n        }\n        else if (x[i] >= 0.75)\n        {\n            bins[3] += 1;\n        }\n    }\n}",
            "std::array<size_t, 4> thread_bins = {0, 0, 0, 0};\n  auto const num_procs = static_cast<size_t>(MPI::COMM_WORLD.Get_size());\n  auto const my_rank = static_cast<size_t>(MPI::COMM_WORLD.Get_rank());\n  auto const num_threads = static_cast<size_t>(omp_get_num_threads());\n  auto const num_per_thread = x.size() / num_threads;\n\n  // #pragma omp parallel for\n  // for (size_t i = my_rank * num_per_thread; i < (my_rank + 1) * num_per_thread; ++i) {\n  //   if (x[i] < 0.25)\n  //     ++thread_bins[0];\n  //   else if (x[i] < 0.5)\n  //     ++thread_bins[1];\n  //   else if (x[i] < 0.75)\n  //     ++thread_bins[2];\n  //   else\n  //     ++thread_bins[3];\n  // }\n\n  // MPI::COMM_WORLD.Reduce(&thread_bins, &bins, 4, MPI::UNSIGNED_LONG, MPI::SUM, 0);\n\n  MPI::COMM_WORLD.Reduce(thread_bins.data(), bins.data(), bins.size(), MPI::UNSIGNED_LONG, MPI::SUM, 0);\n}",
            "int rank, nproc, n;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  double* bin;\n  if (rank == 0) {\n    bin = new double[4]{};\n  }\n\n  if (rank == 0) {\n    n = x.size();\n  }\n\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int nl, nr;\n  nl = n/nproc;\n  nr = n%nproc;\n\n  int l, r;\n  if (rank < nr) {\n    l = rank * (nl + 1);\n    r = l + nl + 1;\n  } else {\n    l = rank * nl + nr;\n    r = l + nl;\n  }\n\n  double* xrank;\n  if (rank == 0) {\n    xrank = new double[r-l];\n  }\n\n  std::vector<double> xtemp(xrank, xrank + r-l);\n  std::copy(x.begin() + l, x.begin() + r, xrank);\n\n  MPI_Scatter(xrank, r-l, MPI_DOUBLE, bin, r-l, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    delete[] xrank;\n  }\n\n  delete[] bin;\n}",
            "const int numThreads = omp_get_max_threads();\n  std::vector<size_t> privateCounts(numThreads * 4, 0);\n  const int numProcesses = 4;\n  const int rank = 1;\n  const int localN = x.size();\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < localN; i++) {\n    const int threadNum = omp_get_thread_num();\n    double frac = x[i] - std::floor(x[i]);\n    if (frac < 0.25) {\n      privateCounts[4*threadNum]++;\n    } else if (frac < 0.5) {\n      privateCounts[4*threadNum + 1]++;\n    } else if (frac < 0.75) {\n      privateCounts[4*threadNum + 2]++;\n    } else {\n      privateCounts[4*threadNum + 3]++;\n    }\n  }\n\n  std::vector<size_t> globalCounts(numProcesses * 4, 0);\n\n  MPI_Reduce(privateCounts.data(), globalCounts.data(), numThreads * 4, MPI_UNSIGNED_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n\n  for (int i = 0; i < 4; i++) {\n    bins[i] = globalCounts[i];\n  }\n\n}",
            "int num_procs;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n_elems = x.size();\n  int elems_per_proc = n_elems/num_procs;\n  int remainder = n_elems%num_procs;\n\n  std::vector<double> x_part(elems_per_proc);\n\n  if(rank == 0) {\n    int sum = 0;\n    for(int i = 0; i < num_procs-1; i++) {\n      MPI_Recv(&x_part[0], elems_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      sum += x_part[elems_per_proc - 1];\n    }\n\n    for(int i = 0; i < num_procs-1; i++) {\n      int start = i*elems_per_proc;\n      MPI_Send(&x[start], elems_per_proc, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n\n    MPI_Send(&x[num_procs*elems_per_proc - remainder], remainder, MPI_DOUBLE, num_procs-1, 0, MPI_COMM_WORLD);\n    sum += x[num_procs*elems_per_proc - remainder - 1];\n\n    bins[0] = static_cast<size_t>(std::floor(sum - 3*elems_per_proc/4));\n    bins[1] = static_cast<size_t>(std::floor(sum - 1*elems_per_proc/2));\n    bins[2] = static_cast<size_t>(std::floor(sum + 1*elems_per_proc/4));\n    bins[3] = static_cast<size_t>(std::floor(sum + 3*elems_per_proc/4));\n  }\n  else {\n    int start = rank*elems_per_proc;\n    MPI_Send(&x[start], elems_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    MPI_Recv(&x_part[0], elems_per_proc, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n}",
            "int myrank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n    // MPI_Bcast(int *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm)\n    // MPI_Send(void *buf, int count, MPI_Datatype datatype, int dest, int tag, MPI_Comm comm)\n    // MPI_Recv(void *buf, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm comm, MPI_Status *status)\n\n    // 1. \u6bcf\u4e2a\u8fdb\u7a0b\u8ba1\u7b97\u81ea\u5df1\u7684\u533a\u95f4\n    auto lower_bound = x.begin();\n    auto upper_bound = x.end();\n    size_t n = x.size();\n    size_t num_each = n / size;\n    int remain = n % size;\n    if (myrank!= 0)\n        lower_bound += num_each * myrank + (myrank - 1);\n    else\n        lower_bound += remain;\n    if (myrank!= size - 1)\n        upper_bound = lower_bound + num_each;\n    else\n        upper_bound = lower_bound + num_each + remain;\n    auto local_x = std::vector<double>(lower_bound, upper_bound);\n\n    // 2. \u8ba1\u7b97\u6bcf\u4e2a\u533a\u95f4\u7684\u7ed3\u679c\n    int num_threads = omp_get_max_threads();\n    auto local_bins = std::vector<int>(4, 0);\n    #pragma omp parallel for num_threads(num_threads)\n    for (size_t i = 0; i < local_x.size(); i++) {\n        int digit = local_x[i] / 0.25;\n        if (digit < 4) {\n            #pragma omp atomic\n            local_bins[digit]++;\n        }\n    }\n\n    // 3. \u8fdb\u884c\u540c\u6b65\n    int *local_bin_pointer = local_bins.data();\n    int *bin_pointer = bins.data();\n    int count = sizeof(int) * 4;\n    MPI_Reduce(local_bin_pointer, bin_pointer, count, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Status status;\n\n    std::array<std::vector<size_t>, 4> localBins;\n    std::array<size_t, 4> globalBins;\n\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            localBins[i].resize(size);\n        }\n    }\n\n    for (size_t i = 0; i < 4; i++) {\n        if (rank == 0) {\n            std::fill(localBins[i].begin(), localBins[i].end(), 0);\n        }\n    }\n\n    size_t n = x.size();\n    size_t num_per_proc = n / size;\n\n    size_t start = rank * num_per_proc;\n    size_t end = start + num_per_proc;\n\n#pragma omp parallel for\n    for (size_t i = start; i < end; i++) {\n        double xi = x[i];\n        double frac = xi - floor(xi);\n        if (frac >= 0.75) {\n            localBins[3][rank]++;\n        } else if (frac >= 0.5) {\n            localBins[2][rank]++;\n        } else if (frac >= 0.25) {\n            localBins[1][rank]++;\n        } else {\n            localBins[0][rank]++;\n        }\n    }\n\n    MPI_Gather(localBins[0].data(), 4, MPI_UNSIGNED_LONG, globalBins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < 4; i++) {\n            for (size_t j = 0; j < size; j++) {\n                bins[i] += globalBins[i * size + j];\n            }\n        }\n    }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "const int size = points.size();\n\n    /*\n        In Kokkos, parallel-for loops are expressed using functors. \n        Functors are classes that define the member functions operator()().\n        The operator()() function is executed by each thread on the parallel-for loop.\n        \n        Here is an example functor that counts the number of points with x <= 0 and y <= 0:\n        */\n    struct CountQuadrantsFunctor {\n        Kokkos::View<const Point*> const& points;\n        Kokkos::View<size_t[4]> &bins;\n\n        // The constructor of the functor. We can access the values in the view `bins`\n        CountQuadrantsFunctor(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins)\n            : points(points), bins(bins) {\n        }\n\n        // This operator is called by each thread. We count the number of points with x <= 0 and y <= 0\n        KOKKOS_INLINE_FUNCTION\n        void operator()(const int i) const {\n            size_t x = points[i].x <= 0? 0 : 1;\n            size_t y = points[i].y <= 0? 0 : 1;\n            bins(x + y * 2)++;\n        }\n    };\n\n    Kokkos::parallel_for(\n        Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, size),\n        CountQuadrantsFunctor(points, bins));\n\n    // This copies data back to the host.\n    Kokkos::deep_copy(bins, bins);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()), [=] (const int i) {\n        // TODO: increment bins[0], bins[1], bins[2], or bins[3], depending on which quadrant points[i] is in.\n    });\n    Kokkos::fence(); // Wait for the parallel_for to finish.\n}",
            "// Your code goes here!\n}",
            "// Your code goes here\n}",
            "// write your solution here\n}",
            "// TODO: implement this function\n}",
            "// TODO: Create a Kokkos parallel_for here that computes bins[i]\n   // Hint: Use a lambda\n   Kokkos::parallel_for(points.size(), KOKKOS_LAMBDA(const int& i) {\n   \tauto point = points[i];\n     if (point.x >= 0 && point.y >= 0) {\n    \t Kokkos::atomic_increment(&bins(0));\n     }\n     else if (point.x < 0 && point.y >= 0) {\n    \t Kokkos::atomic_increment(&bins(1));\n     }\n     else if (point.x >= 0 && point.y < 0) {\n    \t Kokkos::atomic_increment(&bins(2));\n     }\n     else if (point.x < 0 && point.y < 0) {\n    \t Kokkos::atomic_increment(&bins(3));\n     }\n   });\n\n   // TODO: Kokkos::fence();\n   // Hint: Kokkos::fence();\n   Kokkos::fence();\n}",
            "// Your code here\n}",
            "// Your code here\n}",
            "// your code goes here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, bins.extent(0)), KOKKOS_LAMBDA(size_t i) {\n      bins(i) = 0;\n   });\n\n   Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(size_t i) {\n      if (points(i).x < 0) {\n         if (points(i).y < 0) {\n            bins(0)++;\n         } else if (points(i).y > 0) {\n            bins(1)++;\n         }\n      } else if (points(i).x > 0) {\n         if (points(i).y < 0) {\n            bins(2)++;\n         } else if (points(i).y > 0) {\n            bins(3)++;\n         }\n      }\n   });\n}",
            "size_t numPoints = points.extent(0);\n\n   Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, numPoints),\n      KOKKOS_LAMBDA(const int i) {\n         if (points(i).x >= 0 && points(i).y >= 0) {\n            Kokkos::atomic_increment(&bins[0]);\n         }\n         else if (points(i).x < 0 && points(i).y >= 0) {\n            Kokkos::atomic_increment(&bins[1]);\n         }\n         else if (points(i).x >= 0 && points(i).y < 0) {\n            Kokkos::atomic_increment(&bins[2]);\n         }\n         else {\n            Kokkos::atomic_increment(&bins[3]);\n         }\n      }\n   );\n   Kokkos::fence();\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n    int q = 0;\n    if (points(i).x < 0) {\n      q += 1;\n    }\n    if (points(i).y < 0) {\n      q += 2;\n    }\n    Kokkos::atomic_increment(bins.data() + q);\n  });\n  Kokkos::fence();\n}",
            "// TODO: implement this function\n   const size_t npoints = points.extent(0);\n   const size_t nquadrants = bins.extent(0);\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, npoints),\n      [=] (const int i) {\n\n         // TODO: implement this lambda\n      }\n   );\n\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, nquadrants),\n      [=] (const int i) {\n         // TODO: implement this lambda\n      }\n   );\n\n   Kokkos::DefaultHostExecutionSpace::fence();\n}",
            "// Use Kokkos::RangePolicy\n   Kokkos::parallel_for(\n      // TODO: Fill in the range.\n      // Use Kokkos::CountTag to count the number of points in each quadrant.\n      // Use Kokkos::Sum to sum the counts.\n      // Hint: use Kokkos::Sum as a reduction tag.\n      // TODO: Add the right operator.\n   );\n\n   Kokkos::fence(); // Ensure that the above parallel_for completes before the next Kokkos::fence() call.\n\n   // Read the results and store them in bins.\n   Kokkos::parallel_for(\n      // TODO: Fill in the range.\n      // Use Kokkos::CountTag to count the number of points in each quadrant.\n      // Use Kokkos::Sum to sum the counts.\n      // Hint: use Kokkos::Sum as a reduction tag.\n      // TODO: Add the right operator.\n   );\n}",
            "// TODO: Use Kokkos::parallel_for to fill the `bins` array\n    // with the counts of each quadrant.\n    // Use Kokkos::single to update each bin from different threads\n    // and avoid race conditions.\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n   [&] (int i) {\n      int bin;\n      if (points(i).x < 0 && points(i).y < 0)\n         bin = 0;\n      else if (points(i).x < 0 && points(i).y >= 0)\n         bin = 1;\n      else if (points(i).x >= 0 && points(i).y >= 0)\n         bin = 2;\n      else\n         bin = 3;\n      Kokkos::atomic_increment(&bins(bin));\n   });\n}",
            "const size_t num_points = points.extent(0);\n   //...\n}",
            "// TODO: Replace this code with a parallel Kokkos::parallel_for loop that fills `bins`\n   // Hint: you'll need to use the following functions to fill `bins`:\n   // Kokkos::parallel_for, Kokkos::Atomic<T>::fetch_add\n\n   size_t h_bins[4] = {0, 0, 0, 0};\n\n   for (size_t i = 0; i < points.size(); ++i) {\n      const auto& p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         ++h_bins[0];\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         ++h_bins[1];\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         ++h_bins[2];\n      }\n      else {\n         ++h_bins[3];\n      }\n   }\n\n   Kokkos::deep_copy(bins, h_bins);\n}",
            "// TODO: Implement this\n}",
            "using ExecutionPolicy = Kokkos::TeamPolicy<Kokkos::VectorTag<2>>;\n   using MemberType = Kokkos::TeamPolicy<Kokkos::VectorTag<2>>::member_type;\n\n   Kokkos::parallel_for(ExecutionPolicy(1, Kokkos::AUTO),\n                        KOKKOS_LAMBDA (MemberType& team) {\n      const int i = team.league_rank();\n\n      size_t counts[4] = {0,0,0,0};\n\n      for (int j = 0; j < points.extent(0); ++j) {\n         const auto& p = points(j);\n         if (p.x > 0 && p.y > 0) {\n            counts[0]++;\n         } else if (p.x <= 0 && p.y > 0) {\n            counts[1]++;\n         } else if (p.x <= 0 && p.y <= 0) {\n            counts[2]++;\n         } else if (p.x > 0 && p.y <= 0) {\n            counts[3]++;\n         } else {\n            throw std::runtime_error(\"Unexpected quadrant\");\n         }\n      }\n\n      Kokkos::atomic_add(&bins(0), counts[0]);\n      Kokkos::atomic_add(&bins(1), counts[1]);\n      Kokkos::atomic_add(&bins(2), counts[2]);\n      Kokkos::atomic_add(&bins(3), counts[3]);\n   });\n   Kokkos::fence();\n}",
            "// TODO: use Kokkos parallel_for() to count the points in each quadrant\n}",
            "// TODO: Define a Kokkos parallel for loop with 4 threads\n   // Hint: Use Kokkos::RangePolicy\n   // Hint: Use Kokkos::Single\n   // Hint: Use Kokkos::parallel_for\n\n   // TODO: Use Kokkos::atomic_fetch_add to add to the correct bin\n   // Hint: Use Kokkos::atomic\n   // Hint: Use Kokkos::atomic_fetch_add\n   // Hint: Use Kokkos::atomic_add\n\n}",
            "//...\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Schedule<Kokkos::Dynamic> > >(0, points.extent(0)),\n      KOKKOS_LAMBDA(const int i) {\n         // Use the following to store in a 4-dimensional array.\n         // Kokkos::View<size_t[4][2]> counts;\n         // counts(points[i].x > 0? 0 : 1, points[i].y > 0? 0 : 1) += 1;\n\n         // Use the following to store in a 2-dimensional array.\n         // Note that this does not work if the number of quadrants is not 2.\n         // Kokkos::View<size_t[2]> counts;\n         // counts[points[i].x > 0? 0 : 1] += 1;\n\n         if (points[i].x > 0 && points[i].y > 0) {\n            bins(0) += 1;\n         } else if (points[i].x <= 0 && points[i].y > 0) {\n            bins(1) += 1;\n         } else if (points[i].x > 0 && points[i].y <= 0) {\n            bins(2) += 1;\n         } else {\n            bins(3) += 1;\n         }\n      }\n   );\n}",
            "Kokkos::parallel_for(\"count_quadrants\", points.extent(0), KOKKOS_LAMBDA (const int i) {\n        Point p = points(i);\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    });\n    Kokkos::fence();\n}",
            "// Write your code here\n\n}",
            "// Add code here\n}",
            "// TODO\n}",
            "// TODO: create a parallel_for loop, where each thread counts the number of points in each quadrant\n   // \n   // NOTE: bins is a Kokkos::View that is already initialized to 0s.\n   //       Each thread should only write to its own bin, not to the bins of other threads.\n   //\n   // HINT: Use a parallel_for loop over points.\n   //       To access an element of bins, use bins[k], where k is the index of the quadrant (0, 1, 2, 3).\n   //       To access an element of points, use points[i].\n\n}",
            "const size_t numPoints = points.size();\n   // TODO: Implement using Kokkos' parallel_for.\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()),\n    KOKKOS_LAMBDA(const size_t i) {\n      const auto& p = points(i);\n      if (p.x >= 0 && p.y >= 0) {\n        bins(0)++;\n      } else if (p.x < 0 && p.y >= 0) {\n        bins(1)++;\n      } else if (p.x < 0 && p.y < 0) {\n        bins(2)++;\n      } else if (p.x >= 0 && p.y < 0) {\n        bins(3)++;\n      }\n    });\n  Kokkos::DefaultExecutionSpace::fence();\n}",
            "using View1D = Kokkos::View<size_t*, Kokkos::LayoutStride, Kokkos::HostSpace>;\n   using View2D = Kokkos::View<Point**, Kokkos::LayoutStride, Kokkos::HostSpace>;\n   View2D pts(points.data(), points.extent(0), 2);\n   View1D counts(\"\", 4);\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace>(0, 4),\n                        [=] (int q) {\n                           size_t count = 0;\n                           for (int i = 0; i < points.extent(0); i++) {\n                              if ((q == 0 && pts(i,0) >= 0) ||\n                                  (q == 1 && pts(i,0) < 0 && pts(i,1) >= 0) ||\n                                  (q == 2 && pts(i,0) < 0 && pts(i,1) < 0) ||\n                                  (q == 3 && pts(i,0) >= 0 && pts(i,1) < 0)) {\n                                  count++;\n                              }\n                           }\n                           counts(q) = count;\n                        });\n   Kokkos::fence();\n   for (int i = 0; i < 4; i++) {\n      bins(i) = counts(i);\n   }\n}",
            "// TODO: Use a parallel_reduce to fill bins with the counts\n  // Use the helper functions below to implement this function.\n}",
            "const int N = points.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N), [&points, &bins] (const int i) {\n    if(points[i].x > 0 && points[i].y > 0) bins[0]++;\n    else if(points[i].x < 0 && points[i].y > 0) bins[1]++;\n    else if(points[i].x < 0 && points[i].y < 0) bins[2]++;\n    else if(points[i].x > 0 && points[i].y < 0) bins[3]++;\n  });\n  Kokkos::fence();\n}",
            "}",
            "// fill in the body\n}",
            "// your code here\n   size_t const N = points.extent(0);\n   Kokkos::parallel_for(\"countQuadrants\", N, KOKKOS_LAMBDA(const size_t i) {\n      auto const& p = points(i);\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   });\n   Kokkos::fence();\n}",
            "// This lambda function takes a `Point` and increments the corresponding bin in `bins`.\n   auto countInBin = [bins](const Point &pt) {\n      if (pt.x >= 0.0 && pt.y >= 0.0) {\n         bins[0] += 1;\n      } else if (pt.x < 0.0 && pt.y >= 0.0) {\n         bins[1] += 1;\n      } else if (pt.x < 0.0 && pt.y < 0.0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   };\n   // Use parallel for to call `countInBin` on each point\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, points.extent(0)), countInBin);\n}",
            "Kokkos::parallel_for(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA(int i) {\n      const auto point = points(i);\n      int index;\n      if(point.x > 0 && point.y > 0) index = 0;\n      else if(point.x < 0 && point.y > 0) index = 1;\n      else if(point.x < 0 && point.y < 0) index = 2;\n      else index = 3;\n\n      bins[index]++;\n    });\n\n    Kokkos::fence();\n}",
            "size_t n = points.extent(0);\n\n  // TODO\n  // replace this with the real implementation\n  Kokkos::parallel_for(\"quadrant\", n, KOKKOS_LAMBDA(const int i) {\n      const Point &p = points[i];\n      if (p.x > 0 && p.y > 0) bins[0]++;\n      else if (p.x < 0 && p.y > 0) bins[1]++;\n      else if (p.x > 0 && p.y < 0) bins[2]++;\n      else if (p.x < 0 && p.y < 0) bins[3]++;\n  });\n  Kokkos::fence();\n\n  return;\n}",
            "// Put your Kokkos code here\n}",
            "//...\n}",
            "// TODO: replace the code below with an efficient Kokkos parallel algorithm\n  size_t bins_host[4] = {0, 0, 0, 0};\n  for (size_t i = 0; i < points.extent(0); ++i) {\n    Point pt = points(i);\n    int q = 0;\n    if (pt.x < 0) {\n      ++q;\n      if (pt.y < 0) {\n        ++q;\n      }\n    } else {\n      if (pt.y > 0) {\n        ++q;\n      }\n    }\n    bins_host[q] += 1;\n  }\n\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// your code here\n}",
            "// Your code goes here\n}",
            "// your code here\n}",
            "// This lambda will be executed on all threads, including the main thread\n   Kokkos::parallel_for(\n      Kokkos::RangePolicy<>(points.extent(0)),\n      KOKKOS_LAMBDA(int i) {\n         Point const p = points[i];\n         if(p.x > 0 && p.y > 0)\n            bins(0) += 1;\n         else if(p.x < 0 && p.y > 0)\n            bins(1) += 1;\n         else if(p.x < 0 && p.y < 0)\n            bins(2) += 1;\n         else if(p.x > 0 && p.y < 0)\n            bins(3) += 1;\n      });\n}",
            "// TODO: Fill this in\n\n}",
            "using namespace Kokkos;\n   Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(size_t i) {\n      //...\n   });\n   Kokkos::fence();\n}",
            "// The code goes here\n}",
            "using ExecSpace = Kokkos::DefaultExecutionSpace;\n\n   Kokkos::parallel_for(\n      \"countQuadrants\",\n      Kokkos::RangePolicy<ExecSpace>(0, points.size()),\n      [=](int i) {\n         const Point& pt = points[i];\n         if (pt.x >= 0.0 && pt.y >= 0.0) ++bins[0];\n         if (pt.x < 0.0 && pt.y >= 0.0) ++bins[1];\n         if (pt.x < 0.0 && pt.y < 0.0) ++bins[2];\n         if (pt.x >= 0.0 && pt.y < 0.0) ++bins[3];\n      }\n   );\n}",
            "// Add your code here\n}",
            "// TODO: Replace the following line with your code.\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, points.extent(0)), KOKKOS_LAMBDA(const int i) {\n    bins[0] += ((points[i].x >= 0) && (points[i].y >= 0));\n    bins[1] += ((points[i].x < 0) && (points[i].y >= 0));\n    bins[2] += ((points[i].x < 0) && (points[i].y < 0));\n    bins[3] += ((points[i].x >= 0) && (points[i].y < 0));\n  });\n}",
            "// TODO\n}",
            "// TODO: fill in this function\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// TODO: your code here\n\n   const size_t n = points.extent(0);\n   Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::HostSpace>(0, n), [&](const size_t& i) {\n         const auto& p = points(i);\n         int q = (p.x >= 0? 1 : 0) + (p.y >= 0? 2 : 0);\n         Kokkos::atomic_increment(&bins(q));\n      });\n   Kokkos::fence();\n}",
            "using ExecutionPolicy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>;\n  const auto n = points.extent(0);\n  Kokkos::parallel_for(\"countQuadrants\", ExecutionPolicy(0, n), KOKKOS_LAMBDA(const int i) {\n    const Point& pt = points(i);\n    int x = (pt.x < 0.0)? 0 : ((pt.x < 1.0)? 1 : 2);\n    int y = (pt.y < 0.0)? 0 : ((pt.y < 1.0)? 1 : 2);\n    size_t& bin = bins(x + 2*y);\n    Kokkos::atomic_fetch_add(&bin, 1);\n  });\n}",
            "// TODO: Implement using Kokkos parallel_for and range policies\n}",
            "}",
            "auto policy = Kokkos::RangePolicy<Kokkos::Rank<2>>(0, points.extent(0), 16);\n    Kokkos::parallel_for(policy, [&] (int64_t i) {\n        if (points[i].x >= 0.0 && points[i].y >= 0.0) bins[0] += 1;\n        else if (points[i].x < 0.0 && points[i].y >= 0.0) bins[1] += 1;\n        else if (points[i].x < 0.0 && points[i].y < 0.0) bins[2] += 1;\n        else bins[3] += 1;\n    });\n    Kokkos::fence();\n}",
            "// Your code here!\n\n}",
            "// TODO: Count the points.\n   Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceSum>>(0, points.size()),\n      KOKKOS_LAMBDA (const size_t idx, Kokkos::Reduce<Kokkos::ReduceSum>& rs) {\n         const Point& p = points(idx);\n         const size_t q = p.x > 0? (p.y > 0? 0 : 1) : (p.y > 0? 2 : 3);\n         rs.reduce(q, 1);\n      },\n      bins\n   );\n}",
            "Kokkos::parallel_for(\"count quadrants\", points.size(), [=](size_t index) {\n    // Get the x and y coordinates of the point\n    const double x = points(index).x;\n    const double y = points(index).y;\n    // Count the point based on quadrant\n    if (x >= 0) {\n      if (y >= 0) {\n        // 1st quadrant\n        Kokkos::atomic_increment<size_t>(&bins[0]);\n      } else {\n        // 4th quadrant\n        Kokkos::atomic_increment<size_t>(&bins[3]);\n      }\n    } else {\n      if (y >= 0) {\n        // 2nd quadrant\n        Kokkos::atomic_increment<size_t>(&bins[1]);\n      } else {\n        // 3rd quadrant\n        Kokkos::atomic_increment<size_t>(&bins[2]);\n      }\n    }\n  });\n}",
            "/*\n      Your code goes here. Use a parallel_for, and the functor `quadrantAssignment`.\n      The functor should take the point and the bins, and determine which quadrant\n      the point is in. Then increment the appropriate bin.\n   */\n   // TODO: Implement\n}",
            "const int n = points.extent(0);\n   // Your code goes here\n   // Fill the bins array with the number of points in each quadrant\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::Serial>(Kokkos::AUTO, 1)>(0, points.size()),\n      KOKKOS_LAMBDA (const int i) {\n\n   });\n}",
            "//TODO implement me\n}",
            "// Your code here.\n}",
            "// TODO: complete this function\n}",
            "// TODO\n}",
            "/* Your solution goes here  */\n}",
            "/* Your code goes here */\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, points.size()), [&](const int i) {\n      auto point = points(i);\n      if (point.x >= 0 && point.y >= 0) {\n         Kokkos::atomic_increment(&bins[0]);\n      } else if (point.x < 0 && point.y >= 0) {\n         Kokkos::atomic_increment(&bins[1]);\n      } else if (point.x < 0 && point.y < 0) {\n         Kokkos::atomic_increment(&bins[2]);\n      } else if (point.x >= 0 && point.y < 0) {\n         Kokkos::atomic_increment(&bins[3]);\n      }\n   });\n}",
            "// TODO: Write code here to count the number of points in each quadrant.\n\n   // The solution is based on the idea of parallel reduction, where each thread will count the number of points in a given quadrant.\n   // Then, the intermediate results are combined and stored in the `bins` vector.\n\n   // First, create a device-side view to count the number of points in each quadrant.\n   Kokkos::View<size_t[4]> d_bins(\"bins\", 4);\n\n   // Initialize each bin to 0.\n   Kokkos::deep_copy(d_bins, 0);\n\n   // Create a parallel_for lambda that counts the number of points in each quadrant, based on the point coordinates.\n   // The parallel_for lambda will use the \"bin index\" as an index into the `d_bins` view, and the point as a parameter.\n   // Hint: Use the ternary operator: `b > 0? 1 : 0` is either `1` or `0`, depending on the value of `b`.\n   // Hint: To check if a point is in the first quadrant, use `p.x > 0 && p.y > 0`.\n   // Hint: To check if a point is in the second quadrant, use `p.x < 0 && p.y > 0`.\n   // Hint: To check if a point is in the third quadrant, use `p.x < 0 && p.y < 0`.\n   // Hint: To check if a point is in the fourth quadrant, use `p.x > 0 && p.y < 0`.\n   // Hint: To get the index of the first quadrant, use `p.x > 0? 0 : 1`.\n   // Hint: To get the index of the second quadrant, use `p.x < 0? 0 : 1`.\n   // Hint: To get the index of the third quadrant, use `p.x < 0? 0 : 1`.\n   // Hint: To get the index of the fourth quadrant, use `p.x > 0? 0 : 1`.\n\n   // Call parallel_for.\n   // Hint: You can use `Kokkos::RangePolicy` to create a range of indexes.\n   // Hint: You can use `Kokkos::AtomicFetch<T, Kokkos::MemoryOrder::memory_order_relaxed>` to atomically increment a value.\n\n   // Copy the result from the device to the host.\n   Kokkos::deep_copy(bins, d_bins);\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for( \"Count quadrants\", \n                          points.extent(0), \n                          KOKKOS_LAMBDA(const int& i){\n        const auto& point = points(i);\n        // TODO: replace this with your solution\n        if (point.x > 0 && point.y > 0){\n          bins(0) = bins(0) + 1;\n        }\n        if (point.x < 0 && point.y > 0){\n          bins(1) = bins(1) + 1;\n        }\n        if (point.x < 0 && point.y < 0){\n          bins(2) = bins(2) + 1;\n        }\n        if (point.x > 0 && point.y < 0){\n          bins(3) = bins(3) + 1;\n        }\n    });\n}",
            "// Your code here\n}",
            "// Your code goes here\n}",
            "// your code here\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Reduce<Kokkos::ReduceMax<size_t>>, Kokkos::Schedule<Kokkos::Static>>(0, points.extent(0)), [&](const int i) {\n        size_t x = 0;\n        if(points(i).x > 0) {\n            if(points(i).y > 0) {\n                x = 0;\n            }\n            else {\n                x = 3;\n            }\n        }\n        else if(points(i).x < 0) {\n            if(points(i).y > 0) {\n                x = 1;\n            }\n            else {\n                x = 2;\n            }\n        }\n        else if(points(i).y > 0) {\n            x = 1;\n        }\n        else {\n            x = 3;\n        }\n        Kokkos::atomic_increment(bins.data() + x);\n    });\n\n    Kokkos::fence();\n}",
            "// TODO\n\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(const int i) {\n    int idx = 0;\n    if (points(i).x < 0 && points(i).y < 0) {\n      idx = 0;\n    }\n    else if (points(i).x < 0 && points(i).y >= 0) {\n      idx = 1;\n    }\n    else if (points(i).x >= 0 && points(i).y < 0) {\n      idx = 2;\n    }\n    else if (points(i).x >= 0 && points(i).y >= 0) {\n      idx = 3;\n    }\n    Kokkos::atomic_increment(&bins(idx));\n  });\n  Kokkos::fence();\n}",
            "/* Your code goes here */\n}",
            "//...\n}",
            "// Your code goes here\n}",
            "//...\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Threads>(0, points.extent(0)), [&](int i) {\n    const int x = points[i].x > 0? 0 : points[i].x < 0? 1 : 2;\n    const int y = points[i].y > 0? 1 : points[i].y < 0? 3 : 2;\n    Kokkos::atomic_increment(&bins[x + 4*y]);\n  });\n  Kokkos::fence();\n}",
            "}",
            "// Your code here.\n}",
            "//TODO: implement me\n}",
            "// Your code here\n}",
            "const auto n = points.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,n),\n                       [&](const int i) {\n    Point p = points(i);\n    if(p.x < 0) {\n      if(p.y < 0) {\n        Kokkos::atomic_increment(&bins(0));\n      } else {\n        Kokkos::atomic_increment(&bins(1));\n      }\n    } else {\n      if(p.y < 0) {\n        Kokkos::atomic_increment(&bins(2));\n      } else {\n        Kokkos::atomic_increment(&bins(3));\n      }\n    }\n  });\n  Kokkos::fence();\n\n  // TODO: use the atomic functions to store the bin counts in the bins vector.\n  // You will need to use Kokkos::atomic_fetch_add or Kokkos::atomic_increment.\n}",
            "/* TODO: Define a parallel Kokkos::RangePolicy to use for this algorithm.\n    */\n\n    /* TODO: Define a parallel Kokkos::RangePolicy to use for this algorithm.\n    */\n    auto policy = Kokkos::RangePolicy<>(0, points.extent(0));\n\n    /* TODO: Define a Kokkos::parallel_for lambda that performs the same operation as the serial code.\n    */\n    Kokkos::parallel_for(policy, [=] (const int i) {\n        /* TODO: count the points that fall in each quadrant */\n        int x = (points[i].x > 0)? 1 : 0;\n        int y = (points[i].y > 0)? 2 : 0;\n        bins[x+y] += 1;\n    });\n\n}",
            "// create a parallel for loop over the points, and count the number of points in each quadrant\n   Kokkos::parallel_for(\"countQuadrants\", Kokkos::RangePolicy<Kokkos::Rank<2>>({0, 0}, {points.extent(0), 4}),\n                        KOKKOS_LAMBDA(const Kokkos::Rank<2>& i) {\n                           auto& point = points(i[0]);\n                           if (point.x >= 0 && point.y >= 0) bins[0]++;\n                           else if (point.x < 0 && point.y >= 0) bins[1]++;\n                           else if (point.x < 0 && point.y < 0) bins[2]++;\n                           else if (point.x >= 0 && point.y < 0) bins[3]++;\n                        });\n\n   // wait for the parallel loop to finish before returning\n   Kokkos::fence();\n}",
            "using ExecSpace = Kokkos::OpenMP;\n\n   auto point_count = points.size();\n\n   // Set each bin to zero\n   Kokkos::parallel_for(\"Initialize bins\", ExecSpace(0, 4), KOKKOS_LAMBDA(int i) {\n      bins(i) = 0;\n   });\n\n   // Count points in each quadrant, in parallel\n   Kokkos::parallel_for(\"Count bins\", ExecSpace(0, point_count), KOKKOS_LAMBDA(int i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         Kokkos::atomic_increment<size_t>(&bins(0));\n      } else if (points[i].x <= 0 && points[i].y > 0) {\n         Kokkos::atomic_increment<size_t>(&bins(1));\n      } else if (points[i].x <= 0 && points[i].y <= 0) {\n         Kokkos::atomic_increment<size_t>(&bins(2));\n      } else if (points[i].x > 0 && points[i].y <= 0) {\n         Kokkos::atomic_increment<size_t>(&bins(3));\n      }\n   });\n}",
            "using ExecPolicy = Kokkos::TeamPolicy<Kokkos::Schedule<Kokkos::Dynamic>>;\n   using MemberType = Kokkos::TeamPolicy<Kokkos::Schedule<Kokkos::Dynamic>>::member_type;\n   constexpr size_t QUADRANTS = 4;\n\n   Kokkos::parallel_for(\"Count quadrants\",\n                        ExecPolicy(points.extent(0), Kokkos::AUTO),\n                        KOKKOS_LAMBDA(const MemberType &teamMember) {\n                           int i = teamMember.league_rank();\n                           double x = points(i).x, y = points(i).y;\n                           int quad = ((x > 0) << 0) + ((y > 0) << 1);\n                           Kokkos::atomic_increment(&bins[quad]);\n                        });\n}",
            "// Fill in this function!\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0, points.size()),\n      [&](int i) {\n         if (points(i).x >= 0 && points(i).y >= 0) {\n            Kokkos::atomic_increment(&bins(0));\n         } else if (points(i).x < 0 && points(i).y >= 0) {\n            Kokkos::atomic_increment(&bins(1));\n         } else if (points(i).x < 0 && points(i).y < 0) {\n            Kokkos::atomic_increment(&bins(2));\n         } else {\n            Kokkos::atomic_increment(&bins(3));\n         }\n      }\n   );\n   Kokkos::fence();\n}",
            "// Loop over points\n   // TODO: Write Kokkos parallel for loop\n   // Hint:\n   // - use a `Kokkos::RangePolicy`\n   // - use a `Kokkos::parallel_for` loop\n   // - use a lambda function for the loop body\n\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n\n  // -----^---- STOP\n}",
            "auto const n = points.extent(0);\n   auto const maxThreads = Kokkos::ThreadVectorRange(n);\n   Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int i) {\n      int x = 0, y = 0;\n      if (points(i).x > 0) x = 1;\n      else if (points(i).x < 0) x = 2;\n      if (points(i).y > 0) y = 1;\n      else if (points(i).y < 0) y = 2;\n      auto const bin = x + 2*y;\n      Kokkos::atomic_increment(&bins[bin]);\n   });\n   Kokkos::fence();\n}",
            "// Create a parallel kernel for each quadrant\n   Kokkos::parallel_for(\"quadrants\", points.extent(0), KOKKOS_LAMBDA(const int i) {\n      // Use bins as a 1D array of size 4 to count in each quadrant\n      bins[(points[i].x >= 0 && points[i].y >= 0) + (points[i].x >= 0 && points[i].y < 0) + (points[i].x < 0 && points[i].y >= 0) + (points[i].x < 0 && points[i].y < 0)] += 1;\n   });\n   Kokkos::fence(); // Ensure writes to bins are finished before exiting\n}",
            "Kokkos::parallel_for(\"countQuadrants\", Kokkos::RangePolicy<Kokkos::Rank<1>>{points.extent(0)}, [=] (size_t i) {\n      Point p = points[i];\n      size_t x = (p.x < 0? 1 : 0) + (p.x < 0? 2 : 0);\n      size_t y = (p.y < 0? 1 : 0) + (p.y < 0? 2 : 0);\n      Kokkos::atomic_add(&bins[x + y * 2], 1);\n   });\n}",
            "using execution_space = Kokkos::DefaultHostExecutionSpace;\n   using range_policy = Kokkos::RangePolicy<execution_space>;\n   // TODO: use parallel_for and parallel_reduce to fill bins\n}",
            "Kokkos::parallel_for(\"count_quadrants\", points.extent(0), KOKKOS_LAMBDA(int i) {\n        int x = points(i).x > 0? 0 : 1;\n        int y = points(i).y > 0? 0 : 1;\n        Kokkos::atomic_increment(&bins(x + 2 * y));\n    });\n}",
            "// TODO: Implement me\n}",
            "// TODO: count the number of points in each quadrant\n}",
            "// TODO: Write the code here\n}",
            "// Implement this function\n\n}",
            "// Your code goes here\n\n}",
            "/* Your code here */\n  // constexpr size_t n = 4;\n  // constexpr size_t n_tot = 6;\n  // Kokkos::View<size_t[4]> bins(\"bins\", n);\n  // Kokkos::parallel_for(n, KOKKOS_LAMBDA(const size_t i) {\n  //   bins(i) = 0;\n  // });\n\n  // Kokkos::parallel_for(n_tot, KOKKOS_LAMBDA(const size_t i) {\n  //   if (points(i).x > 0 && points(i).y > 0) {\n  //     bins(0) += 1;\n  //   }\n  //   else if (points(i).x < 0 && points(i).y > 0) {\n  //     bins(1) += 1;\n  //   }\n  //   else if (points(i).x < 0 && points(i).y < 0) {\n  //     bins(2) += 1;\n  //   }\n  //   else if (points(i).x > 0 && points(i).y < 0) {\n  //     bins(3) += 1;\n  //   }\n  // });\n\n}",
            "auto binsHost = Kokkos::create_mirror_view(bins);\n   Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (points(i).x >= 0 && points(i).y >= 0) {\n         binsHost(0)++;\n      } else if (points(i).x < 0 && points(i).y >= 0) {\n         binsHost(1)++;\n      } else if (points(i).x < 0 && points(i).y < 0) {\n         binsHost(2)++;\n      } else if (points(i).x >= 0 && points(i).y < 0) {\n         binsHost(3)++;\n      } else {\n         printf(\"Something wrong!\\n\");\n      }\n   });\n   Kokkos::deep_copy(bins, binsHost);\n}",
            "// TODO: Use Kokkos parallel_for to count points in each quadrant\n   // For each point, use the Point.x and Point.y to determine which quadrant to increment bins[0]-bins[3].\n   // Don't forget to use Kokkos::parallel_reduce to sum the bins\n   // For example, to add 1 to the 2nd quadrant bin use:\n   //\n   //     Kokkos::atomic_fetch_add(&bins(1), 1);\n   //\n   // to add 3 to the 0th quadrant bin:\n   //\n   //     Kokkos::atomic_fetch_add(&bins(0), 3);\n   //\n   // Don't forget to add the Kokkos::parallel_for to the body of this function.\n}",
            "// Your code here\n\n}",
            "/* Your code here */\n}",
            "/*\n   * Fill this in.\n   */\n}",
            "auto n = points.extent(0);\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n), KOKKOS_LAMBDA(int i) {\n      if (points[i].x < 0 && points[i].y < 0) bins[0] += 1;\n      else if (points[i].x >= 0 && points[i].y < 0) bins[1] += 1;\n      else if (points[i].x >= 0 && points[i].y >= 0) bins[2] += 1;\n      else bins[3] += 1;\n   });\n}",
            "// write your code here\n}",
            "using policy_t = Kokkos::RangePolicy<Kokkos::Rank<1>>;\n  using execution_space = Kokkos::DefaultExecutionSpace;\n  using functor_t = Kokkos::RangePolicy<Kokkos::Rank<1>>;\n\n  Kokkos::parallel_for(\"CountQuadrants\", policy_t(0, points.extent(0)), functor_t([] (int i) {\n    int x = points[i].x > 0? 1 : 0;\n    int y = points[i].y > 0? 2 : 0;\n\n    Kokkos::atomic_increment(&bins[x + y]);\n  }));\n}",
            "// Your code here\n  // \n}",
            "// your code here\n}",
            "Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, points.size());\n   Kokkos::parallel_for(\"countQuadrants\", policy, KOKKOS_LAMBDA(const int i) {\n     int q = 0;\n     if (points(i).x < 0) q += 1;\n     if (points(i).y < 0) q += 2;\n     Kokkos::atomic_increment(&bins(q));\n   });\n}",
            "const size_t num_points = points.extent(0);\n\n  // TODO: fill in the body of this function\n\n  // Example:\n  //\n  //  auto h_bins = Kokkos::create_mirror_view(bins);\n  //  Kokkos::deep_copy(h_bins, bins);\n  //  for (size_t i = 0; i < 4; ++i) {\n  //    std::cout << h_bins[i] << \" \";\n  //  }\n  //  std::cout << std::endl;\n}",
            "// TODO\n}",
            "// fill in the code\n}",
            "// TODO\n}",
            "// TODO: Count the number of points in each quadrant\n\n}",
            "// TODO: implement this function\n}",
            "}",
            "Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA (size_t i) {\n      // TODO: compute the quadrant for points[i] and increment the corresponding counter in bins\n   });\n   Kokkos::fence();\n}",
            "// TODO\n}",
            "/* Your code goes here */\n\n}",
            "Kokkos::View<size_t[4]> bins_host = Kokkos::create_mirror_view(bins);\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(points.size()), [=] __device__ (int i) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins_host(0) += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins_host(1) += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins_host(2) += 1;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins_host(3) += 1;\n        }\n    });\n    Kokkos::deep_copy(bins, bins_host);\n}",
            "// BEGIN CODE\n  Kokkos::parallel_for(\n      \"Counting bins\",\n      Kokkos::RangePolicy<Kokkos::",
            "// Create a parallel Kokkos range 0 to points.size().\n   auto r = Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Rank<1> > > (0, points.size());\n\n   // Create a parallel Kokkos team policy with 4 teams.\n   // The team size should be equal to the number of cores on your machine.\n   auto t = Kokkos::TeamPolicy<Kokkos::Rank<1> >(4, Kokkos::AUTO, 0);\n\n   // Create a parallel Kokkos reduction with the range and team policy.\n   // Each team should use its local memory to hold the counts and then\n   // reduce them into the bins at the end of the parallel region.\n   Kokkos::parallel_reduce(t, r, Kokkos::Minimal<size_t[4]>(bins), [&] (const int &i, size_t bins[4]) const {\n      // Determine which quadrant this point is in.\n      // The quadrants are numbered as follows:\n      //\n      //     |    1    |    2    |\n      //     |         |         |\n      //     |         |         |\n      // -------------------------\n      //     |    3    |    4    |\n      //     |         |         |\n      //     |         |         |\n      //\n      const size_t quadrant = (points(i).y > 0.0? (points(i).x > 0.0? 1 : 4) : (points(i).x > 0.0? 2 : 3));\n\n      // Increment the bin for the appropriate quadrant.\n      bins[quadrant]++;\n   });\n}",
            "auto count_functor = KOKKOS_LAMBDA (const int &i) {\n    auto& point = points(i);\n    if (point.x < 0 && point.y < 0) {\n      Kokkos::atomic_increment(&bins(0));\n    } else if (point.x < 0 && point.y >= 0) {\n      Kokkos::atomic_increment(&bins(1));\n    } else if (point.x >= 0 && point.y < 0) {\n      Kokkos::atomic_increment(&bins(2));\n    } else if (point.x >= 0 && point.y >= 0) {\n      Kokkos::atomic_increment(&bins(3));\n    }\n  };\n\n  Kokkos::RangePolicy<Kokkos::OpenMP> policy(0, points.extent(0));\n  Kokkos::parallel_for(\"countQuadrants\", policy, count_functor);\n\n  Kokkos::fence();\n}",
            "// Your code here\n}",
            "/* YOUR CODE HERE */\n   Kokkos::parallel_for(\"Count points\", 4, [&](int i) {\n        bins(i) = 0;\n        for (int j = 0; j < points.extent(0); ++j) {\n            if ((i == 0 && points(j).x >= 0 && points(j).y >= 0) ||\n                (i == 1 && points(j).x <  0 && points(j).y >= 0) ||\n                (i == 2 && points(j).x <  0 && points(j).y <  0) ||\n                (i == 3 && points(j).x >= 0 && points(j).y <  0)) {\n                bins(i) += 1;\n            }\n        }\n    });\n}",
            "const size_t npoints = points.extent(0);\n\n   Kokkos::parallel_for(npoints, [&](const int i) {\n      Point p = points(i);\n      // Use the sign of the x and y coordinates to determine which bin this point is in.\n      int xbin = (p.x >= 0)? 0 : 1;\n      int ybin = (p.y >= 0)? 2 : 3;\n      // Increment the bin count\n      bins(xbin, ybin) += 1;\n   });\n\n   // Wait for the parallel count to finish.\n   Kokkos::fence();\n}",
            "const size_t n = points.extent(0);\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, n), [&] (int i) {\n      auto p = points(i);\n      int bin = (p.x >= 0? (p.y >= 0? 0 : 3) : (p.y >= 0? 1 : 2));\n      Kokkos::atomic_increment(&bins[bin]);\n   });\n   Kokkos::fence();\n}",
            "size_t num_points = points.extent(0);\n\n   // Your solution goes here\n   Kokkos::parallel_for(\"count_quadrants\", Kokkos::RangePolicy<Kokkos::Rank<2>>(num_points), KOKKOS_LAMBDA (int i) {\n       if (points[i].x > 0 && points[i].y > 0) {\n           bins(0)++;\n       } else if (points[i].x <= 0 && points[i].y > 0) {\n           bins(1)++;\n       } else if (points[i].x > 0 && points[i].y <= 0) {\n           bins(2)++;\n       } else {\n           bins(3)++;\n       }\n   });\n\n   Kokkos::fence();\n}",
            "// Your code here\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Implement this\n}",
            "// TODO: fill this in\n}",
            "// YOUR CODE HERE\n}",
            "// Insert code here\n}",
            "Kokkos::parallel_for(points.extent(0), [&](size_t i){\n        // TODO: Fill this in\n    });\n    Kokkos::fence();\n}",
            "auto bins_h = Kokkos::create_mirror_view(bins);\n   auto points_h = Kokkos::create_mirror_view(points);\n   Kokkos::deep_copy(points_h, points);\n   Kokkos::parallel_for(points_h.extent(0), KOKKOS_LAMBDA(const int i) {\n      if (points_h(i).x > 0 && points_h(i).y > 0)\n         bins_h(0)++;\n      else if (points_h(i).x < 0 && points_h(i).y > 0)\n         bins_h(1)++;\n      else if (points_h(i).x < 0 && points_h(i).y < 0)\n         bins_h(2)++;\n      else if (points_h(i).x > 0 && points_h(i).y < 0)\n         bins_h(3)++;\n   });\n   Kokkos::deep_copy(bins, bins_h);\n}",
            "// TODO: Your code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::Cuda>>(0,points.extent(0)),\n      KOKKOS_LAMBDA(int i) {\n         if (points(i).x >= 0) {\n            if (points(i).y >= 0) {\n               Kokkos::atomic_increment(&bins(0));\n            }\n            else {\n               Kokkos::atomic_increment(&bins(2));\n            }\n         }\n         else {\n            if (points(i).y >= 0) {\n               Kokkos::atomic_increment(&bins(1));\n            }\n            else {\n               Kokkos::atomic_increment(&bins(3));\n            }\n         }\n      });\n   Kokkos::fence();\n}",
            "// Kokkos::parallel_for with lambda\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Cuda>(0, points.extent(0)),\n                       [=](const int& i) {\n    int x = (points[i].x >= 0)? 1 : 0;\n    int y = (points[i].y >= 0)? 1 : 0;\n    bins[x+y] += 1;\n  });\n  Kokkos::fence();\n}",
            "// insert code here.\n\n    // A parallel Kokkos::parallel_for loop to fill the bins\n}",
            "// Set the number of points in each quadrant to 0\n  Kokkos::parallel_for(\n    \"fill bins\",\n    Kokkos::RangePolicy<Kokkos::HostSpace>(0, bins.size()),\n    KOKKOS_LAMBDA(size_t i) {\n      bins[i] = 0;\n    });\n\n  // Use a parallel for loop to count the number of points in each quadrant\n  Kokkos::parallel_for(\n    \"count points\",\n    Kokkos::RangePolicy<Kokkos::HostSpace>(0, points.size()),\n    KOKKOS_LAMBDA(size_t i) {\n      auto p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n        Kokkos::atomic_increment(&bins[0]);\n      } else if (p.x < 0 && p.y >= 0) {\n        Kokkos::atomic_increment(&bins[1]);\n      } else if (p.x < 0 && p.y < 0) {\n        Kokkos::atomic_increment(&bins[2]);\n      } else if (p.x >= 0 && p.y < 0) {\n        Kokkos::atomic_increment(&bins[3]);\n      }\n    });\n}",
            "// TODO\n  size_t size = points.size();\n  Kokkos::parallel_for(\n    size,\n    KOKKOS_LAMBDA(const size_t i) {\n      size_t x = (points(i).x > 0.0)? 0 : 1;\n      size_t y = (points(i).y > 0.0)? 2 : 3;\n      Kokkos::atomic_increment<Kokkos::MemoryTraits<Kokkos::Unordered>>(&bins(x + y));\n    }\n  );\n  Kokkos::fence();\n}",
            "auto const policy = Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Static>>(0, points.extent(0));\n  Kokkos::parallel_for(\"countQuadrants\", policy, KOKKOS_LAMBDA(const size_t& i) {\n    auto& p = points[i];\n    size_t bin = p.y >= 0? p.x >= 0? 1 : 2 : p.x >= 0? 3 : 4;\n    Kokkos::atomic_fetch_add(&bins[bin], 1);\n  });\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "// Your code goes here\n   // Kokkos will automatically parallelize over the points\n   // Kokkos::parallel_for(points.extent(0), [&](int i) {\n   //    Point point = points[i];\n   //    if (point.x >= 0 && point.y >= 0) {\n   //       bins[0] += 1;\n   //    }\n   //    else if (point.x < 0 && point.y >= 0) {\n   //       bins[1] += 1;\n   //    }\n   //    else if (point.x < 0 && point.y < 0) {\n   //       bins[2] += 1;\n   //    }\n   //    else if (point.x >= 0 && point.y < 0) {\n   //       bins[3] += 1;\n   //    }\n   // });\n\n   Kokkos::parallel_reduce(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA(int i, size_t& sum) {\n      const Point& point = points[i];\n      sum += (point.x >= 0 && point.y >= 0);\n      sum += (point.x < 0 && point.y >= 0);\n      sum += (point.x < 0 && point.y < 0);\n      sum += (point.x >= 0 && point.y < 0);\n   }, Kokkos::Sum<size_t>(bins));\n\n   Kokkos::fence();\n}",
            "}",
            "// Your code here\n    // Hint: Use a Kokkos parallel_for to loop over the points\n}",
            "//\n    // Your code here\n    //\n}",
            "// Your code goes here\n}",
            "// Fill this in\n}",
            "// Initialize bins to 0\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, bins.size()), KOKKOS_LAMBDA(const int i) {\n    bins[i] = 0;\n  });\n  // Parallelize over points\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()), KOKKOS_LAMBDA(const int i) {\n    const auto& point = points[i];\n    // Add the point to the corresponding bin.\n    if (point.x >= 0 && point.y >= 0)\n      bins[0]++;\n    else if (point.x < 0 && point.y >= 0)\n      bins[1]++;\n    else if (point.x >= 0 && point.y < 0)\n      bins[2]++;\n    else if (point.x < 0 && point.y < 0)\n      bins[3]++;\n  });\n}",
            "// Initialize bins to 0\n    Kokkos::parallel_for(\n        \"InitializeBins\",\n        Kokkos::RangePolicy<Kokkos::OpenMP>(0, bins.size()),\n        [&](const int &idx) {\n            bins[idx] = 0;\n        }\n    );\n\n    // Now count the points by updating bins\n    Kokkos::parallel_for(\n        \"CountPoints\",\n        Kokkos::RangePolicy<Kokkos::OpenMP>(0, points.size()),\n        [&](const int &idx) {\n            auto x = points[idx].x;\n            auto y = points[idx].y;\n\n            // Fill in quadrant code below\n        }\n    );\n\n    Kokkos::fence();\n}",
            "/*\n    * TODO\n    */\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA(const size_t i) {\n      // TODO: add code to count points in each quadrant\n   });\n   Kokkos::fence();\n}",
            "/* TODO: Fill in the body of this function */\n  Kokkos::parallel_for(\"quadrant_counter\", points.extent(0), KOKKOS_LAMBDA (const size_t& i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n          Kokkos::atomic_increment(&bins[0]);\n      } else if (points[i].x < 0 && points[i].y > 0) {\n          Kokkos::atomic_increment(&bins[1]);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n          Kokkos::atomic_increment(&bins[2]);\n      } else if (points[i].x > 0 && points[i].y < 0) {\n          Kokkos::atomic_increment(&bins[3]);\n      } else {\n          // do nothing\n      }\n  });\n  Kokkos::fence();\n}",
            "// Your code goes here\n}",
            "// Create a Kokkos parallel_for lambda to count the number of points in each quadrant.\n  // Invoke the lambda with Kokkos::parallel_for, specifying the execution policy.\n\n  // Check your work by printing the bins.\n  for(int i=0; i<4; i++) {\n    printf(\"%lu \", bins[i]);\n  }\n}",
            "// TODO: Count the number of cartesian points in each quadrant.\n   //       Store the counts in bins.\n   //       Use Kokkos to count in parallel.\n   //       Assume Kokkos has already been initialized.\n\n   //...\n\n}",
            "// Create an atomic counter for each quadrant\n   Kokkos::View<Kokkos::atomic_t<size_t>*> atomics(\"atomics\", 4);\n   // Zero the atomics\n   Kokkos::deep_copy(atomics, 0);\n\n   // Set the number of threads per team (Kokkos will divide the work up among these many threads)\n   // Try using a different number of threads here, for example: 128, 256, 512\n   constexpr int threadsPerTeam = 128;\n   // Create a team policy that will create one team with the given number of threads per team\n   Kokkos::TeamPolicy<Kokkos::LaunchBounds<threadsPerTeam>> teamPolicy(points.size());\n\n   // Create a parallel lambda to count the points in each quadrant.\n   // The parallel_for function will launch the given lambda in parallel for all the points\n   // in `points` (see https://kokkos.readthedocs.io/en/latest/parallel-for.html for more\n   // details on parallel_for)\n   Kokkos::parallel_for(\n      teamPolicy,\n      KOKKOS_LAMBDA(const Kokkos::TeamPolicy<Kokkos::LaunchBounds<threadsPerTeam>>::member_type& member) {\n         // Get the thread's index in the team\n         int threadIdx = member.team_rank();\n         // Get the index of the point in the array that this thread is responsible for\n         int pointIdx = member.league_rank() * threadsPerTeam + threadIdx;\n\n         // Get the x and y components of the point\n         const Point& point = points[pointIdx];\n         double x = point.x;\n         double y = point.y;\n\n         // Determine the quadrant of the point\n         int quadrant = 0;\n         if (x >= 0) {\n            quadrant += 1;\n            if (y >= 0)\n               quadrant += 2;\n         } else {\n            if (y < 0)\n               quadrant += 2;\n         }\n         // Atomically increment the number of points in this quadrant\n         Kokkos::atomic_fetch_add(&atomics(quadrant), 1);\n      }\n   );\n\n   // Copy the values in atomics to bins.\n   // The copy is asynchronous, i.e., it will start as soon as it's launched but doesn't wait for it to complete.\n   Kokkos::deep_copy(bins, atomics);\n\n   // Wait for the copy to complete\n   Kokkos::fence();\n}",
            "// Your code here\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::OpenMP>(points.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      if (points[i].x >= 0) {\n        if (points[i].y >= 0) {\n          ++bins[0];\n        } else {\n          ++bins[3];\n        }\n      } else {\n        if (points[i].y >= 0) {\n          ++bins[1];\n        } else {\n          ++bins[2];\n        }\n      }\n    }\n  );\n\n  Kokkos::fence();\n}",
            "using ExecutionSpace = typename Kokkos::DefaultExecutionSpace;\n  using PointHost = typename Kokkos::View<Point*, ExecutionSpace>;\n  using PointDevice = typename Kokkos::View<Point*, ExecutionSpace::memory_space>;\n  using BinDevice = typename Kokkos::View<size_t[4], ExecutionSpace::memory_space>;\n\n  const int n = points.extent(0);\n  const int blockSize = 1024;\n  const int numBlocks = (n + blockSize - 1) / blockSize;\n\n  // Allocate the inputs and outputs in Kokkos' memory space\n  PointHost points_host(Kokkos::view_alloc(Kokkos::WithoutInitializing, \"points\"), n);\n  BinDevice bins_device(Kokkos::view_alloc(Kokkos::WithoutInitializing, \"bins\"), 4);\n\n  // Copy the inputs to the host view\n  Kokkos::deep_copy(points_host, points);\n\n  // Run the kernel\n  Kokkos::parallel_for(\n    \"countQuadrants\",\n    Kokkos::MDRangePolicy<ExecutionSpace, Kokkos::Rank<2>>({0, 0}, {numBlocks, 1}, {blockSize, 1}),\n    KOKKOS_LAMBDA(const int&, const int&) {\n      for (int i = 0; i < blockSize; ++i) {\n        int index = blockIdx.x * blockDim.x + threadIdx.x;\n        if (index < n) {\n          int x_bin = points_host(index).x > 0? 1 : 0;\n          int y_bin = points_host(index).y > 0? 2 : 0;\n          if (points_host(index).x < 0) {\n            x_bin = 2;\n          } else if (points_host(index).x > 0) {\n            x_bin = 3;\n          }\n          if (points_host(index).y < 0) {\n            y_bin = 2;\n          } else if (points_host(index).y > 0) {\n            y_bin = 3;\n          }\n          Kokkos::atomic_increment(&bins_device(y_bin * 2 + x_bin));\n        }\n      }\n    });\n\n  // Copy the result back to the host\n  Kokkos::deep_copy(bins, bins_device);\n}",
            "/* Your solution goes here  */\n}",
            "// Use a parallel for loop to assign each quadrant index to each Point.\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0,points.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n    // If the x-coordinate of the point is positive...\n    if (points(i).x >= 0) {\n      // If the y-coordinate of the point is positive...\n      if (points(i).y >= 0) {\n        // It is in the first quadrant, so assign 0 to the quadrant index.\n        bins(0) += 1;\n      }\n      // If the y-coordinate of the point is negative...\n      else {\n        // It is in the third quadrant, so assign 2 to the quadrant index.\n        bins(2) += 1;\n      }\n    }\n    // If the x-coordinate of the point is negative...\n    else {\n      // If the y-coordinate of the point is positive...\n      if (points(i).y >= 0) {\n        // It is in the second quadrant, so assign 1 to the quadrant index.\n        bins(1) += 1;\n      }\n      // If the y-coordinate of the point is negative...\n      else {\n        // It is in the fourth quadrant, so assign 3 to the quadrant index.\n        bins(3) += 1;\n      }\n    }\n  });\n  Kokkos::fence();\n}",
            "// TODO: Implement me!\n}",
            "// Create a functor to count the quadrants.\n  // The functor is templated over the index type for the points and the bins.\n  // The functor is invoked over the range of points.\n\n  Kokkos::parallel_for(\n    \"Count Quadrants\",\n    Kokkos::RangePolicy<Kokkos::Cuda>(0, points.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      auto point = points[i];\n      if (point.x >= 0.0 && point.y >= 0.0) {\n        bins[0] += 1;\n      } else if (point.x < 0.0 && point.y >= 0.0) {\n        bins[1] += 1;\n      } else if (point.x >= 0.0 && point.y < 0.0) {\n        bins[2] += 1;\n      } else if (point.x < 0.0 && point.y < 0.0) {\n        bins[3] += 1;\n      }\n    }\n  );\n\n  // Kokkos::fence(); // ensure that all operations are completed before return\n}",
            "size_t n = points.extent(0);\n\n   Kokkos::parallel_for(n, KOKKOS_LAMBDA (size_t i) {\n      const Point p = points(i);\n      int q = 0;\n      if (p.x < 0) q += 1;\n      if (p.y < 0) q += 2;\n      // Update the bin count for the current point\n      Kokkos::atomic_increment(&bins[q]);\n   });\n}",
            "// Your code goes here\n}",
            "// Your code here\n\n}",
            "/* YOUR CODE HERE */\n}",
            "// Your code goes here\n   Kokkos::parallel_for(\"count quadrants\", points.extent(0), KOKKOS_LAMBDA (const size_t i) {\n     if (points(i).x>0 && points(i).y>0) bins(0)++;\n     else if (points(i).x<0 && points(i).y>0) bins(1)++;\n     else if (points(i).x<0 && points(i).y<0) bins(2)++;\n     else if (points(i).x>0 && points(i).y<0) bins(3)++;\n   });\n   Kokkos::fence();\n}",
            "Kokkos::RangePolicy<Kokkos::ExecutionPolicy<Kokkos::Serial>> range(0, points.extent(0));\n   Kokkos::parallel_for(range, KOKKOS_LAMBDA(const int i) {\n      if (points(i).x > 0 && points(i).y > 0) {\n         Kokkos::atomic_add(&bins(0), 1);\n      } else if (points(i).x <= 0 && points(i).y > 0) {\n         Kokkos::atomic_add(&bins(1), 1);\n      } else if (points(i).x > 0 && points(i).y <= 0) {\n         Kokkos::atomic_add(&bins(2), 1);\n      } else if (points(i).x <= 0 && points(i).y <= 0) {\n         Kokkos::atomic_add(&bins(3), 1);\n      }\n   });\n}",
            "// your code here\n}",
            "// TODO: implement this function\n  Kokkos::View<size_t[4]> tmp(\"tmp\");\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 4),\n                       KOKKOS_LAMBDA(int i) {\n                         tmp(i) = 0;\n                       });\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n                       KOKKOS_LAMBDA(int i) {\n                         const Point& pt = points(i);\n                         if (pt.x >= 0 && pt.y >= 0) {\n                           tmp(0)++;\n                         }\n                         else if (pt.x < 0 && pt.y >= 0) {\n                           tmp(1)++;\n                         }\n                         else if (pt.x < 0 && pt.y < 0) {\n                           tmp(2)++;\n                         }\n                         else {\n                           tmp(3)++;\n                         }\n                       });\n\n  Kokkos::deep_copy(bins, tmp);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n      [&](const int i) {\n         // your code goes here\n         // you can access elements in `points` using the `points(i)` syntax.\n         // you can access elements in `bins` using the `bins(0)`, `bins(1)`, `bins(2)`, `bins(3)` syntax.\n         // `i` will be a variable that ranges from 0 to points.extent(0)-1.\n\n         // Example:\n         // bins(0)++;\n      }\n   );\n}",
            "// Your code goes here\n}",
            "// insert your code here\n}",
            "Kokkos::parallel_for(\"CountQuadrants\", Kokkos::RangePolicy<Kokkos::Cuda>(0, points.size()), KOKKOS_LAMBDA(const size_t& i) {\n      int quadrant = 0;\n      if (points(i).x >= 0 && points(i).y >= 0)\n         quadrant = 1;\n      else if (points(i).x < 0 && points(i).y >= 0)\n         quadrant = 2;\n      else if (points(i).x < 0 && points(i).y < 0)\n         quadrant = 3;\n      else if (points(i).x >= 0 && points(i).y < 0)\n         quadrant = 4;\n      Kokkos::atomic_increment(&bins(quadrant));\n   });\n   Kokkos::fence();\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "}",
            "// add your code here\n}",
            "size_t n = points.size();\n   // TODO\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n       double x = points[i].x;\n       double y = points[i].y;\n       if (x > 0 && y > 0)\n           #pragma omp atomic\n           bins[0]++;\n       else if (x < 0 && y > 0)\n           #pragma omp atomic\n           bins[1]++;\n       else if (x < 0 && y < 0)\n           #pragma omp atomic\n           bins[2]++;\n       else\n           #pragma omp atomic\n           bins[3]++;\n   }\n}",
            "// TODO: replace this with your solution\n   size_t n = points.size();\n   std::array<size_t, 4> bins2;\n   size_t bins_index = 0;\n\n   #pragma omp parallel for private(bins_index)\n   for (size_t i = 0; i < n; i++) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            bins2[0]++;\n         } else {\n            bins2[1]++;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            bins2[2]++;\n         } else {\n            bins2[3]++;\n         }\n      }\n   }\n\n   for (size_t i = 0; i < bins.size(); i++) {\n      bins[i] = bins2[i];\n   }\n}",
            "size_t size = points.size();\n\n   std::array<size_t, 4> counts {0,0,0,0};\n\n   #pragma omp parallel\n   {\n      std::array<size_t, 4> thread_counts;\n      #pragma omp for\n      for (int i = 0; i < size; i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n         if (x >= 0) {\n            if (y >= 0) {\n               thread_counts[0]++;\n            } else {\n               thread_counts[1]++;\n            }\n         } else {\n            if (y >= 0) {\n               thread_counts[2]++;\n            } else {\n               thread_counts[3]++;\n            }\n         }\n      }\n\n      #pragma omp critical\n      for (int i = 0; i < 4; i++) {\n         counts[i] += thread_counts[i];\n      }\n   }\n\n   bins = counts;\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < points.size(); ++i) {\n        Point p = points[i];\n        if(p.x >= 0 && p.y >= 0) {\n            #pragma omp atomic update\n            bins[0]++;\n        }\n        else if(p.x < 0 && p.y >= 0) {\n            #pragma omp atomic update\n            bins[1]++;\n        }\n        else if(p.x < 0 && p.y < 0) {\n            #pragma omp atomic update\n            bins[2]++;\n        }\n        else {\n            #pragma omp atomic update\n            bins[3]++;\n        }\n    }\n}",
            "//std::cout << \"thread \" << std::this_thread::get_id() << \" \" << omp_get_thread_num() << \" start\" << std::endl;\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n   //std::cout << \"thread \" << std::this_thread::get_id() << \" \" << omp_get_thread_num() << \" end\" << std::endl;\n}",
            "// TODO\n}",
            "bins = {0, 0, 0, 0};\n  omp_set_num_threads(2);\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (auto const& pt : points)\n      bins[pt.x >= 0 && pt.y >= 0]++;\n  }\n}",
            "size_t n = points.size();\n   bins = {0, 0, 0, 0};\n\n#pragma omp parallel for\n   for(size_t i = 0; i < n; ++i) {\n      Point const& p = points[i];\n      if (p.x >= 0 && p.y >= 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y >= 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "// TODO: YOUR CODE GOES HERE\n    bins = {0, 0, 0, 0};\n\n    #pragma omp parallel for schedule(dynamic)\n    for(size_t i = 0; i < points.size(); i++)\n    {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if(x >= 0 && y >= 0)\n        {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if(x < 0 && y >= 0)\n        {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if(x < 0 && y < 0)\n        {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if(x >= 0 && y < 0)\n        {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement me\n}",
            "}",
            "/*\n    This section is for debugging.\n    It is not required for the autograder and should be removed.\n    */\n    std::cout << \"Points:\" << std::endl;\n    for (Point const& p : points) {\n        std::cout << \"  (\" << p.x << \", \" << p.y << \")\" << std::endl;\n    }\n\n    // Fill in your code here\n    size_t i = 0;\n    #pragma omp parallel for schedule(static,1)\n    for (auto it = points.begin(); it!= points.end(); it++)\n    {\n        Point p = *it;\n        if (p.x > 0 && p.y > 0) {\n            #pragma omp critical\n            {\n                bins[0]++;\n            }\n        }\n        if (p.x < 0 && p.y > 0) {\n            #pragma omp critical\n            {\n                bins[1]++;\n            }\n        }\n        if (p.x < 0 && p.y < 0) {\n            #pragma omp critical\n            {\n                bins[2]++;\n            }\n        }\n        if (p.x > 0 && p.y < 0) {\n            #pragma omp critical\n            {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      int x = points[i].x;\n      int y = points[i].y;\n      if (x > 0 && y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (x < 0 && y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else if (x > 0 && y < 0) {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 and points[i].y >= 0) {\n         #pragma omp atomic\n         bins[0]++;\n      }\n      if (points[i].x < 0 and points[i].y >= 0) {\n         #pragma omp atomic\n         bins[1]++;\n      }\n      if (points[i].x >= 0 and points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      }\n      if (points[i].x < 0 and points[i].y < 0) {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "int size = points.size();\n    #pragma omp parallel for\n    for (int i = 0; i < size; i++) {\n        int x = points[i].x;\n        int y = points[i].y;\n        int quadrant = 0;\n        if (x >= 0) {\n            if (y >= 0) {\n                quadrant = 1;\n            } else {\n                quadrant = 4;\n            }\n        } else {\n            if (y >= 0) {\n                quadrant = 2;\n            } else {\n                quadrant = 3;\n            }\n        }\n        bins[quadrant] += 1;\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i=0; i<points.size(); i++) {\n      size_t bin = 0;\n      if (points[i].x > 0)\n         bin++;\n      if (points[i].y > 0)\n         bin += 2;\n      bins[bin]++;\n   }\n}",
            "// count the number of points in each quadrant\n   // add code here\n}",
            "size_t bins_per_thread[4] = {};\n    size_t total_points = points.size();\n    double sum = 0;\n\n    #pragma omp parallel shared(points, total_points, sum)\n    {\n        size_t bins_per_thread_private[4] = {};\n\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < total_points; i++) {\n            Point point = points.at(i);\n            if (point.x < 0 && point.y < 0) {\n                bins_per_thread_private[0]++;\n            } else if (point.x >= 0 && point.y < 0) {\n                bins_per_thread_private[1]++;\n            } else if (point.x >= 0 && point.y >= 0) {\n                bins_per_thread_private[2]++;\n            } else if (point.x < 0 && point.y >= 0) {\n                bins_per_thread_private[3]++;\n            }\n        }\n\n        #pragma omp critical\n        for (size_t i = 0; i < 4; i++) {\n            bins_per_thread[i] += bins_per_thread_private[i];\n        }\n    }\n\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = bins_per_thread[i];\n    }\n}",
            "// TODO: Implement using OpenMP\n\n}",
            "// TODO:\n}",
            "// TODO: Your solution goes here\n}",
            "#pragma omp parallel for\nfor (size_t i = 0; i < points.size(); ++i)\n{\n    int quadrant = 0;\n    if (points[i].x > 0 && points[i].y > 0)\n        quadrant = 1;\n    else if (points[i].x < 0 && points[i].y > 0)\n        quadrant = 2;\n    else if (points[i].x < 0 && points[i].y < 0)\n        quadrant = 3;\n    else\n        quadrant = 4;\n    #pragma omp atomic\n    bins[quadrant]++;\n}\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        // add your code here\n    }\n}",
            "// Initialize the array to 0\n   for (auto &bin : bins) {\n      bin = 0;\n   }\n\n#pragma omp parallel for num_threads(4)\n   for (size_t i = 0; i < points.size(); i++) {\n\n      // Determine the quadrant\n      int quadrant = 1;\n      if (points[i].x < 0) {\n         quadrant += 2;\n      }\n      if (points[i].y < 0) {\n         quadrant += 1;\n      }\n\n      // Increment the counter for the correct quadrant\n      bins[quadrant-1]++;\n   }\n\n}",
            "size_t const nr_threads = omp_get_max_threads();\n   std::vector<size_t> thread_bins(nr_threads);\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      auto &p = points[i];\n      if (p.x >= 0 && p.y >= 0)\n         thread_bins[omp_get_thread_num()]++;\n      else if (p.x >= 0 && p.y < 0)\n         thread_bins[omp_get_thread_num()]++;\n      else if (p.x < 0 && p.y >= 0)\n         thread_bins[omp_get_thread_num()]++;\n      else\n         thread_bins[omp_get_thread_num()]++;\n   }\n\n   for (size_t i = 0; i < nr_threads; ++i)\n      bins[i] += thread_bins[i];\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        Point &p = points[i];\n        if (p.x >= 0 && p.y >= 0)\n            bins[0]++;\n        else if (p.x < 0 && p.y >= 0)\n            bins[1]++;\n        else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n        else if (p.x >= 0 && p.y < 0)\n            bins[3]++;\n    }\n}",
            "// Your code goes here\n   bins[0]=0;\n   bins[1]=0;\n   bins[2]=0;\n   bins[3]=0;\n\n   #pragma omp parallel for\n   for(size_t i=0;i<points.size();i++){\n       if(points[i].x>=0&&points[i].y>=0){\n           bins[0]+=1;\n       }\n       else if(points[i].x<=0&&points[i].y>=0){\n           bins[1]+=1;\n       }\n       else if(points[i].x>=0&&points[i].y<=0){\n           bins[2]+=1;\n       }\n       else if(points[i].x<=0&&points[i].y<=0){\n           bins[3]+=1;\n       }\n   }\n\n}",
            "#pragma omp parallel for\n   for(auto& p : points) {\n      int quadrant = 0;\n\n      if (p.x > 0 && p.y > 0) quadrant = 0;\n      else if (p.x < 0 && p.y > 0) quadrant = 1;\n      else if (p.x < 0 && p.y < 0) quadrant = 2;\n      else if (p.x > 0 && p.y < 0) quadrant = 3;\n\n      #pragma omp atomic\n      bins[quadrant]++;\n   }\n}",
            "#pragma omp parallel for\n   for(size_t i=0; i<points.size(); i++) {\n      if(points[i].x > 0 && points[i].y > 0)\n         bins[0]++;\n      else if(points[i].x < 0 && points[i].y > 0)\n         bins[1]++;\n      else if(points[i].x > 0 && points[i].y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "for (auto i = 0; i < 4; ++i) {\n        bins[i] = 0;\n    }\n\n    // Fill in the body of this function\n    #pragma omp parallel\n    {\n        std::array<size_t, 4> thread_bins{};\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); i++) {\n            // Fill in the body of this loop\n        }\n        #pragma omp critical\n        {\n            for (auto i = 0; i < 4; ++i) {\n                bins[i] += thread_bins[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0) {\n            if (points[i].y >= 0) {\n                #pragma omp critical\n                bins[0]++;\n            } else {\n                #pragma omp critical\n                bins[3]++;\n            }\n        } else {\n            if (points[i].y >= 0) {\n                #pragma omp critical\n                bins[1]++;\n            } else {\n                #pragma omp critical\n                bins[2]++;\n            }\n        }\n    }\n}",
            "// TODO: Your code here\n    int nthreads, tid;\n    int i, j, k, t;\n    size_t count[4] = {0, 0, 0, 0};\n\n    #pragma omp parallel private(i, j, tid, nthreads, t) shared(count)\n    {\n        #pragma omp for\n        for(i = 0; i < points.size(); i++) {\n            if(points[i].x >= 0 && points[i].y >= 0) {\n                count[0]++;\n            } else if(points[i].x < 0 && points[i].y >= 0) {\n                count[1]++;\n            } else if(points[i].x >= 0 && points[i].y < 0) {\n                count[2]++;\n            } else if(points[i].x < 0 && points[i].y < 0) {\n                count[3]++;\n            }\n        }\n        for(t = 0; t < 4; t++) {\n            #pragma omp atomic\n            bins[t] += count[t];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1] += 1;\n        }\n        else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// TODO: use OpenMP\n\n  // Initialize all bins to 0\n  for(int i=0; i<4; i++){\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for(int i=0; i<points.size(); i++){\n    if(points[i].x > 0 && points[i].y > 0) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if(points[i].x < 0 && points[i].y > 0) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if(points[i].x > 0 && points[i].y < 0) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if(points[i].x < 0 && points[i].y < 0) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "omp_set_num_threads(4);\n    #pragma omp parallel for\n    for(int i = 0; i < points.size(); ++i) {\n        int bin = 0;\n        if(points[i].x < 0) {\n            bin += 1;\n        }\n        if(points[i].y < 0) {\n            bin += 2;\n        }\n        bins[bin]++;\n    }\n}",
            "#pragma omp parallel for num_threads(4)\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         #pragma omp atomic\n         bins[0] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y >= 0) {\n         #pragma omp atomic\n         bins[1] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2] += 1;\n      }\n      else {\n         #pragma omp atomic\n         bins[3] += 1;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0] += 1;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1] += 1;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2] += 1;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3] += 1;\n        }\n    }\n}",
            "// Add your code here\n    // Use OpenMP to parallelize the loop.\n    // You can use omp_get_thread_num() to get the thread id of the current thread.\n    // You can use omp_get_num_threads() to get the total number of threads.\n    // Use omp_get_num_procs() to get the total number of available cores.\n\n    // Example of parallel for loop using OpenMP\n    //\n    // #pragma omp parallel for\n    // for (int i = 0; i < 100; i++) {\n    //     // Do something here\n    // }\n\n}",
            "// 4 threads, each loop a quarter of the points\n   #pragma omp parallel for num_threads(4)\n   for (int i = 0; i < 4; ++i) {\n      for (int j = 0; j < points.size(); ++j) {\n         int x = points[j].x;\n         int y = points[j].y;\n         if ((i == 0 && x >= 0 && y >= 0) ||\n             (i == 1 && x < 0 && y >= 0) ||\n             (i == 2 && x < 0 && y < 0) ||\n             (i == 3 && x >= 0 && y < 0)) {\n            bins[i] += 1;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (x < 0 && y >= 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (x >= 0 && y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else if (x < 0 && y < 0) {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: count the number of points in each quadrant.\n   //       The quadrant are:\n   //         0: bottom left,\n   //         1: top left,\n   //         2: bottom right,\n   //         3: top right.\n   //       Use the vector `points` to fill `bins` with the right values.\n   //       The `bins` array is already preallocated and initialized to zero.\n   //       You can use OpenMP to count in parallel.\n   //       Hint: use the modulus operator to determine in which quadrant a point resides.\n\n   #pragma omp parallel\n   {\n      #pragma omp for schedule(dynamic, 100)\n      for (size_t i = 0; i < points.size(); i++) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n         }\n         else if (points[i].x < 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n         }\n         else if (points[i].x >= 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n         }\n         else {\n            #pragma omp atomic\n            bins[3]++;\n         }\n      }\n   }\n}",
            "}",
            "for(size_t i = 0; i < 4; i++){\n     bins[i] = 0;\n   }\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      Point point = points[i];\n      if (point.x > 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[0] += 1;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         #pragma omp atomic\n         bins[1] += 1;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[2] += 1;\n      }\n      else if (point.x > 0 && point.y < 0) {\n         #pragma omp atomic\n         bins[3] += 1;\n      }\n   }\n}",
            "// add parallel for here.\n    for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        } else if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        } else if (point.x >= 0 && point.y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < points.size(); ++i) {\n        Point p = points[i];\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0] += 1;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1] += 1;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      size_t bin = 0;\n      if (points[i].x > 0 && points[i].y > 0) {\n         bin = 1;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bin = 2;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bin = 3;\n      }\n      #pragma omp atomic\n      bins[bin]++;\n   }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// your code here\n    int i = 0;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (auto point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      \n      Point p = points[i];\n      int quadrant = 0;\n      if (p.x < 0) {\n         quadrant += 1;\n      }\n      if (p.y < 0) {\n         quadrant += 2;\n      }\n      bins[quadrant]++;\n   }\n}",
            "// 4 threads\n  // #pragma omp parallel for schedule(dynamic, 1) num_threads(4)\n  // for (int i=0; i < points.size(); i++) {\n  //   if (points[i].x > 0 && points[i].y > 0) {\n  //     #pragma omp atomic\n  //     bins[0]++;\n  //   } else if (points[i].x < 0 && points[i].y > 0) {\n  //     #pragma omp atomic\n  //     bins[1]++;\n  //   } else if (points[i].x > 0 && points[i].y < 0) {\n  //     #pragma omp atomic\n  //     bins[2]++;\n  //   } else {\n  //     #pragma omp atomic\n  //     bins[3]++;\n  //   }\n  // }\n\n  #pragma omp parallel for schedule(dynamic, 1) num_threads(4)\n  for (int i=0; i < points.size(); i++) {\n    int bins_idx;\n    if (points[i].x > 0 && points[i].y > 0) {\n      bins_idx = 0;\n    } else if (points[i].x < 0 && points[i].y > 0) {\n      bins_idx = 1;\n    } else if (points[i].x > 0 && points[i].y < 0) {\n      bins_idx = 2;\n    } else {\n      bins_idx = 3;\n    }\n    #pragma omp atomic\n    bins[bins_idx]++;\n  }\n}",
            "// Your code here\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < points.size(); ++i) {\n        double x = points[i].x;\n        double y = points[i].y;\n        int bin = 0;\n        if(x > 0 && y > 0) {\n            bin = 0;\n        } else if(x < 0 && y > 0) {\n            bin = 1;\n        } else if(x < 0 && y < 0) {\n            bin = 2;\n        } else if(x > 0 && y < 0) {\n            bin = 3;\n        }\n        #pragma omp critical\n        bins[bin] += 1;\n    }\n}",
            "#pragma omp parallel for\n   for(size_t i=0; i<points.size(); i++) {\n      int quadrant = 0;\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0)\n         quadrant = 1;\n      else if (x < 0 && y > 0)\n         quadrant = 2;\n      else if (x < 0 && y < 0)\n         quadrant = 3;\n      bins[quadrant]++;\n   }\n}",
            "int n = points.size();\n\n   #pragma omp parallel for\n   for (int i = 0; i < n; i++) {\n      Point p = points[i];\n      int bin = 0;\n      if (p.x < 0 && p.y < 0) {\n         bin = 3;\n      } else if (p.x >= 0 && p.y < 0) {\n         bin = 2;\n      } else if (p.x < 0 && p.y >= 0) {\n         bin = 1;\n      } else if (p.x >= 0 && p.y >= 0) {\n         bin = 0;\n      }\n      #pragma omp atomic\n      bins[bin] += 1;\n   }\n}",
            "// Your code here!\n\n  //#pragma omp parallel for\n  for(int i=0; i < points.size(); i++)\n  {\n    if(points[i].x < 0 && points[i].y < 0) bins[0]++;\n    else if(points[i].x < 0 && points[i].y > 0) bins[1]++;\n    else if(points[i].x > 0 && points[i].y < 0) bins[2]++;\n    else if(points[i].x > 0 && points[i].y > 0) bins[3]++;\n  }\n}",
            "bins.fill(0);\n\n   // your code here\n   std::array<int, 2> x = {1, 2};\n   std::array<int, 2> y = {1, 2};\n   std::array<int, 4> counts = {0, 0, 0, 0};\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++)\n   {\n      int x_pos = (points[i].x >= 0);\n      int y_pos = (points[i].y >= 0);\n      counts[(x_pos * y_pos)]++;\n   }\n\n   bins = counts;\n}",
            "auto countQuadrant = [&bins](auto const& p) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    };\n\n    omp_set_num_threads(4);\n#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        countQuadrant(points[i]);\n    }\n}",
            "bins = {0, 0, 0, 0};\n   #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// count the number of points in each quadrant\n   // Use 4 threads to do the counting, one thread per quadrant\n\n   #pragma omp parallel num_threads(4)\n   {\n      int tid = omp_get_thread_num();\n      switch(tid)\n      {\n      case 0:\n         // count the number of points in quadrant 1\n         break;\n      case 1:\n         // count the number of points in quadrant 2\n         break;\n      case 2:\n         // count the number of points in quadrant 3\n         break;\n      case 3:\n         // count the number of points in quadrant 4\n         break;\n      }\n   }\n}",
            "int n = points.size();\n   #pragma omp parallel\n   {\n       // Create a private copy of `bins` for each thread.\n       #pragma omp for\n       for (int i = 0; i < n; ++i) {\n           if (points[i].x >= 0 && points[i].y >= 0) {\n               bins[0] += 1;\n           } else if (points[i].x < 0 && points[i].y >= 0) {\n               bins[1] += 1;\n           } else if (points[i].x < 0 && points[i].y < 0) {\n               bins[2] += 1;\n           } else if (points[i].x >= 0 && points[i].y < 0) {\n               bins[3] += 1;\n           }\n       }\n   }\n}",
            "// Your code goes here\n\n}",
            "#pragma omp parallel\n    {\n        // initialize the private bins\n        std::array<size_t, 4> bins_private;\n        bins_private.fill(0);\n\n        // iterate over all points in parallel\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); ++i) {\n            auto& p = points[i];\n\n            // count points depending on the quadrant\n            auto& bins_current = p.x >= 0? p.y >= 0? bins_private[0] : bins_private[1] : p.y >= 0? bins_private[2] : bins_private[3];\n            bins_current++;\n        }\n\n        #pragma omp critical\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] += bins_private[i];\n        }\n    }\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for(auto &point: points) {\n      if(point.x >= 0 && point.y >= 0)\n        #pragma omp atomic\n        bins[0]++;\n      else if(point.x < 0 && point.y >= 0)\n        #pragma omp atomic\n        bins[1]++;\n      else if(point.x < 0 && point.y < 0)\n        #pragma omp atomic\n        bins[2]++;\n      else if(point.x >= 0 && point.y < 0)\n        #pragma omp atomic\n        bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n   for(auto const& point : points) {\n      int quadrant = 1;\n      if(point.x < 0)\n         quadrant += 1;\n      if(point.y < 0)\n         quadrant += 2;\n      bins[quadrant]++;\n   }\n}",
            "// TODO: your code goes here\n\n}",
            "#pragma omp parallel\n   {\n      size_t tid = omp_get_thread_num();\n\n      #pragma omp for\n      for (size_t i = 0; i < points.size(); i++) {\n         Point p = points[i];\n         if (p.x >= 0 && p.y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n         }\n         else if (p.x < 0 && p.y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n         }\n         else if (p.x < 0 && p.y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n         }\n         else if (p.x >= 0 && p.y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for\n      for(size_t i = 0; i < points.size(); ++i) {\n         //...\n      }\n   }\n}",
            "// your code here\n}",
            "for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < points.size(); i++) {\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x > 0 && y > 0) {\n      #pragma omp critical\n      bins[0]++;\n    }\n    else if (x < 0 && y > 0) {\n      #pragma omp critical\n      bins[1]++;\n    }\n    else if (x < 0 && y < 0) {\n      #pragma omp critical\n      bins[2]++;\n    }\n    else {\n      #pragma omp critical\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for schedule(static)\n   for (auto &p : points) {\n       #pragma omp atomic\n       bins[p.x < 0? 0 : p.x == 0? 2 : 1] += p.y < 0? 0 : p.y == 0? 1 : 2;\n   }\n}",
            "// your code goes here\n    #pragma omp parallel for shared(points)\n    for (int i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x <= 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x <= 0 && points[i].y <= 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x > 0 && points[i].y <= 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: replace this code with your solution\n    int N = points.size();\n    #pragma omp parallel for num_threads(4)\n    for (int i = 0; i < N; i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel\n   {\n       #pragma omp single\n       {\n          #pragma omp task\n          {\n             bins[0] = 0;\n          }\n          #pragma omp task\n          {\n             bins[1] = 0;\n          }\n          #pragma omp task\n          {\n             bins[2] = 0;\n          }\n          #pragma omp task\n          {\n             bins[3] = 0;\n          }\n\n          #pragma omp taskwait\n          for (auto const& point : points) {\n             double x = point.x;\n             double y = point.y;\n             if (x >= 0 && y >= 0) {\n                #pragma omp atomic\n                bins[0]++;\n             } else if (x < 0 && y >= 0) {\n                #pragma omp atomic\n                bins[1]++;\n             } else if (x < 0 && y < 0) {\n                #pragma omp atomic\n                bins[2]++;\n             } else if (x >= 0 && y < 0) {\n                #pragma omp atomic\n                bins[3]++;\n             }\n          }\n       }\n   }\n}",
            "// your code here\n    std::array<size_t, 4> num{};\n    #pragma omp parallel\n    {\n        std::array<size_t, 4> num_t{};\n        #pragma omp for schedule(static)\n        for (int i = 0; i < points.size(); ++i) {\n            auto& point = points[i];\n            if (point.x >= 0 && point.y >= 0) {\n                num_t[0]++;\n            } else if (point.x < 0 && point.y >= 0) {\n                num_t[1]++;\n            } else if (point.x < 0 && point.y < 0) {\n                num_t[2]++;\n            } else if (point.x >= 0 && point.y < 0) {\n                num_t[3]++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            num[0] += num_t[0];\n            num[1] += num_t[1];\n            num[2] += num_t[2];\n            num[3] += num_t[3];\n        }\n    }\n\n    for (int i = 0; i < 4; ++i) {\n        bins[i] = num[i];\n    }\n}",
            "#pragma omp parallel for default(shared) firstprivate(points)\n   for (int i = 0; i < points.size(); i++) {\n       double x = points[i].x;\n       double y = points[i].y;\n       if (x >= 0.0 && y >= 0.0) {\n           #pragma omp atomic\n           bins[0]++;\n       }\n       else if (x < 0.0 && y >= 0.0) {\n           #pragma omp atomic\n           bins[1]++;\n       }\n       else if (x >= 0.0 && y < 0.0) {\n           #pragma omp atomic\n           bins[2]++;\n       }\n       else if (x < 0.0 && y < 0.0) {\n           #pragma omp atomic\n           bins[3]++;\n       }\n   }\n}",
            "#pragma omp parallel for\n   for (auto const &p: points) {\n       const double x = p.x, y = p.y;\n       if (x >= 0 && y >= 0) {\n           bins[0]++;\n       } else if (x < 0 && y >= 0) {\n           bins[1]++;\n       } else if (x < 0 && y < 0) {\n           bins[2]++;\n       } else if (x >= 0 && y < 0) {\n           bins[3]++;\n       }\n   }\n}",
            "bins = {0, 0, 0, 0};\n\n#pragma omp parallel for\n  for (size_t i=0; i<points.size(); i++) {\n    double x = points[i].x;\n    double y = points[i].y;\n    int quadrant = 0;\n\n    if (x > 0) {\n      if (y > 0) {\n        quadrant = 0;\n      } else if (y < 0) {\n        quadrant = 3;\n      }\n    } else {\n      if (y > 0) {\n        quadrant = 1;\n      } else if (y < 0) {\n        quadrant = 2;\n      }\n    }\n\n    omp_set_lock(&(locks[quadrant]));\n    bins[quadrant]++;\n    omp_unset_lock(&(locks[quadrant]));\n  }\n}",
            "#pragma omp parallel\n   {\n      #pragma omp single\n      {\n         for (auto p : points) {\n            if (p.x >= 0 && p.y >= 0) {\n               #pragma omp atomic\n               bins[0]++;\n            }\n            else if (p.x < 0 && p.y >= 0) {\n               #pragma omp atomic\n               bins[1]++;\n            }\n            else if (p.x >= 0 && p.y < 0) {\n               #pragma omp atomic\n               bins[2]++;\n            }\n            else if (p.x < 0 && p.y < 0) {\n               #pragma omp atomic\n               bins[3]++;\n            }\n         }\n      }\n   }\n}",
            "//TODO\n}",
            "// TODO: implement this function\n    #pragma omp parallel for\n    for(auto p : points) {\n        if(p.x >= 0) {\n            if(p.y >= 0)\n                #pragma omp atomic\n                bins[0]++;\n            else\n                #pragma omp atomic\n                bins[3]++;\n        } else {\n            if(p.y >= 0)\n                #pragma omp atomic\n                bins[1]++;\n            else\n                #pragma omp atomic\n                bins[2]++;\n        }\n    }\n}",
            "for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n\n    int N = points.size();\n    //#pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            bins[2]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// This is a parallel region, because we're using the OpenMP pragma.\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      Point const& p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         // Quadrant 1\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         // Quadrant 2\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         // Quadrant 3\n         ++bins[2];\n      } else {\n         // Quadrant 4\n         ++bins[3];\n      }\n   }\n}",
            "// Use omp_get_num_threads() to get the number of threads in this parallel section.\n   // Use omp_get_thread_num() to get the current thread number (0-indexed) in this parallel section.\n   // The thread number is used to find the correct bin to increment.\n   // Don't forget to initialize each bin to 0 before counting.\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      int quadrant = 0;\n      if (points[i].x < 0)\n         quadrant += 1;\n      if (points[i].y < 0)\n         quadrant += 2;\n      #pragma omp atomic\n      bins[quadrant]++;\n   }\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for\n      for(int i=0; i<points.size(); i++) {\n         if(points[i].x > 0 && points[i].y > 0)\n            bins[0]++;\n         else if(points[i].x < 0 && points[i].y > 0)\n            bins[1]++;\n         else if(points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n         else if(points[i].x > 0 && points[i].y < 0)\n            bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for default(shared)\n    for (int i=0;i<points.size();i++) {\n        if (points[i].x > 0 && points[i].y > 0) bins[0]++;\n        else if (points[i].x < 0 && points[i].y > 0) bins[1]++;\n        else if (points[i].x > 0 && points[i].y < 0) bins[2]++;\n        else if (points[i].x < 0 && points[i].y < 0) bins[3]++;\n    }\n}",
            "//...\n}",
            "}",
            "//...\n}",
            "auto size = points.size();\n  // Your code here\n  #pragma omp parallel for\n  for(int i = 0; i < size; i++)\n  {\n    if(points[i].x >= 0 && points[i].y >= 0)\n    {\n      #pragma omp atomic\n      bins[0]++;\n    }\n    else if(points[i].x < 0 && points[i].y >= 0)\n    {\n      #pragma omp atomic\n      bins[1]++;\n    }\n    else if(points[i].x >= 0 && points[i].y < 0)\n    {\n      #pragma omp atomic\n      bins[2]++;\n    }\n    else if(points[i].x < 0 && points[i].y < 0)\n    {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n}",
            "// Your code here.\n}",
            "#pragma omp parallel for num_threads(2)\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n      else if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n      else if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n      else if (points[i].x >= 0 && points[i].y < 0) bins[3]++;\n   }\n}",
            "// TODO\n    #pragma omp parallel for shared(bins)\n    for (size_t i = 0; i < points.size(); i++) {\n        Point const& p = points[i];\n        if (p.x >= 0 && p.y >= 0) {\n            #pragma omp atomic\n            bins[0] += 1;\n        } else if (p.x < 0 && p.y >= 0) {\n            #pragma omp atomic\n            bins[1] += 1;\n        } else if (p.x < 0 && p.y < 0) {\n            #pragma omp atomic\n            bins[2] += 1;\n        } else {\n            #pragma omp atomic\n            bins[3] += 1;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for num_threads(4)\n    for (auto i = 0; i < points.size(); ++i)\n    {\n        auto p = points[i];\n        int quadrant = (p.x < 0) + 2 * (p.y < 0);\n\n        #pragma omp atomic\n        ++bins[quadrant];\n    }\n}",
            "#pragma omp parallel for default(none) shared(points, bins)\n   for(unsigned long i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      int quad = ((x >= 0)? 1 : 0) + ((y >= 0)? 2 : 0);\n      bins[quad]++;\n   }\n}",
            "// Your code here\n\n}",
            "// TODO: add OpenMP parallel for loop to count points in each quadrant\n\n    // TODO: use bins[0] = count of points where x > 0 and y > 0\n    // TODO: use bins[1] = count of points where x < 0 and y > 0\n    // TODO: use bins[2] = count of points where x < 0 and y < 0\n    // TODO: use bins[3] = count of points where x > 0 and y < 0\n\n}",
            "// TODO: fill out this function\n   // Use OpenMP to parallelize the counting of the points.\n}",
            "}",
            "#pragma omp parallel for num_threads(4)\n    for(size_t i = 0; i < points.size(); ++i) {\n        Point const& p = points[i];\n        int quadrant = (p.x > 0) + 2 * (p.y > 0);\n        ++bins[quadrant];\n    }\n}",
            "// Your code here\n}",
            "// TODO: implement\n}",
            "// Write your code here\n   int nthreads, thread_num;\n\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   nthreads = omp_get_num_threads();\n\n   #pragma omp parallel private(thread_num)\n   {\n     thread_num = omp_get_thread_num();\n     bins[0] += points.size() / 4;\n   }\n\n   #pragma omp parallel private(thread_num)\n   {\n     thread_num = omp_get_thread_num();\n     bins[1] += points.size() / 4;\n   }\n\n   #pragma omp parallel private(thread_num)\n   {\n     thread_num = omp_get_thread_num();\n     bins[2] += points.size() / 4;\n   }\n\n   #pragma omp parallel private(thread_num)\n   {\n     thread_num = omp_get_thread_num();\n     bins[3] += points.size() / 4;\n   }\n\n   return;\n\n}",
            "// TODO: count the number of points in each quadrant.\n\n}",
            "for (size_t i=0; i<points.size(); i++) {\n      // Your code here\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if(points[i].x > 0 && points[i].y > 0)\n         #pragma omp atomic\n         bins[0]++;\n\n      if(points[i].x < 0 && points[i].y > 0)\n         #pragma omp atomic\n         bins[1]++;\n\n      if(points[i].x < 0 && points[i].y < 0)\n         #pragma omp atomic\n         bins[2]++;\n\n      if(points[i].x > 0 && points[i].y < 0)\n         #pragma omp atomic\n         bins[3]++;\n   }\n}",
            "// add your implementation here\n   int size = points.size();\n   int size_x = 2;\n   int size_y = 2;\n\n   // #pragma omp parallel for // <-- uncomment this line to use parallelism\n   for (int i = 0; i < size; i++) {\n      if (points[i].x < 0 && points[i].y < 0) {\n         bins[0] += 1;\n      }\n\n      if (points[i].x > 0 && points[i].y < 0) {\n         bins[1] += 1;\n      }\n\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[2] += 1;\n      }\n\n      if (points[i].x < 0 && points[i].y > 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "size_t n = points.size();\n\n   #pragma omp parallel for schedule(static) num_threads(4)\n   for(size_t i = 0; i < n; ++i)\n   {\n      if(points[i].x >= 0 && points[i].y >= 0)\n         ++bins[0];\n      else if(points[i].x < 0 && points[i].y >= 0)\n         ++bins[1];\n      else if(points[i].x >= 0 && points[i].y < 0)\n         ++bins[2];\n      else if(points[i].x < 0 && points[i].y < 0)\n         ++bins[3];\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      Point& point = points[i];\n\n      int x = point.x > 0? 1 : point.x < 0? 2 : 0;\n      int y = point.y > 0? 3 : point.y < 0? 4 : 0;\n      int quadrant = x + y;\n\n      bins[quadrant - 1] += 1;\n   }\n}",
            "size_t n = points.size();\n  bins.fill(0);\n  #pragma omp parallel for num_threads(4)\n  for (size_t i = 0; i < n; i++) {\n    int quadrant = (points[i].x > 0) + (points[i].y > 0) * 2;\n    bins[quadrant]++;\n  }\n}",
            "// YOUR CODE HERE\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        size_t quadrant = (points[i].x >= 0) + 2 * (points[i].y >= 0);\n        bins[quadrant]++;\n    }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < points.size(); ++i)\n    {\n        Point p = points[i];\n        if(p.x > 0 && p.y > 0)\n            ++bins[0];\n        else if(p.x < 0 && p.y > 0)\n            ++bins[1];\n        else if(p.x > 0 && p.y < 0)\n            ++bins[2];\n        else if(p.x < 0 && p.y < 0)\n            ++bins[3];\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "/* TODO */\n}",
            "//#pragma omp parallel for\n   for(auto i = 0u; i < points.size(); ++i) {\n       if(points[i].x > 0 && points[i].y > 0)\n           bins[0]++;\n       else if(points[i].x < 0 && points[i].y > 0)\n           bins[1]++;\n       else if(points[i].x > 0 && points[i].y < 0)\n           bins[2]++;\n       else if(points[i].x < 0 && points[i].y < 0)\n           bins[3]++;\n   }\n}",
            "// TODO: Implement this function\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         #pragma omp atomic\n         bins[0] += 1;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         #pragma omp atomic\n         bins[1] += 1;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2] += 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[3] += 1;\n      }\n   }\n}",
            "}",
            "// TODO: count points in quadrants using OpenMP\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for num_threads(4)\n  for (size_t i = 0; i < points.size(); ++i) {\n    const auto &p = points[i];\n\n    if (p.x >= 0 && p.y >= 0) bins[0]++;\n    else if (p.x < 0 && p.y >= 0) bins[1]++;\n    else if (p.x < 0 && p.y < 0) bins[2]++;\n    else if (p.x >= 0 && p.y < 0) bins[3]++;\n  }\n}",
            "// TODO: use omp_get_num_threads() to get the number of threads\n    int numberOfThreads = omp_get_num_threads();\n    // TODO: use omp_get_thread_num() to get the thread index\n    int threadIndex = omp_get_thread_num();\n    // TODO: use the above variables to count the points in the correct bin\n    for(size_t i = 0; i < points.size(); i++)\n    {\n        if(points[i].x > 0 && points[i].y > 0)\n        {\n            // TODO: increment the first bin\n            bins[0]++;\n        }\n        else if(points[i].x < 0 && points[i].y > 0)\n        {\n            // TODO: increment the second bin\n            bins[1]++;\n        }\n        else if(points[i].x < 0 && points[i].y < 0)\n        {\n            // TODO: increment the third bin\n            bins[2]++;\n        }\n        else if(points[i].x > 0 && points[i].y < 0)\n        {\n            // TODO: increment the fourth bin\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel\n   {\n      int i, j;\n      i = omp_get_thread_num();\n      j = omp_get_num_threads();\n\n      #pragma omp for schedule(static)\n      for (size_t i = 0; i < points.size(); ++i) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0]++;\n         }\n         else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1]++;\n         }\n         else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n         }\n         else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); ++i) {\n      const auto &p = points[i];\n      int x = p.x;\n      int y = p.y;\n      if (x < 0) {\n         if (y < 0) {\n            bins[0]++;\n         } else {\n            bins[3]++;\n         }\n      } else {\n         if (y < 0) {\n            bins[1]++;\n         } else {\n            bins[2]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        int q = 0;\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x < 0 && y < 0) {\n            q = 0;\n        }\n        else if (x > 0 && y > 0) {\n            q = 1;\n        }\n        else if (x > 0 && y < 0) {\n            q = 2;\n        }\n        else if (x < 0 && y > 0) {\n            q = 3;\n        }\n        bins[q] += 1;\n    }\n}",
            "// Set the counts to zero\n   for (size_t i = 0; i < bins.size(); i++)\n      bins[i] = 0;\n\n   // Count the points\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      // Calculate which quadrant each point is in\n      int q = 0;\n      if (points[i].x >= 0)\n         q += 1;\n      if (points[i].y >= 0)\n         q += 2;\n\n      // Increment the bin\n      bins[q]++;\n   }\n}",
            "//std::array<size_t, 4> bins;\n   //size_t bins[4];\n   bins.fill(0);\n\n   /*\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n       int quadrant = (points[i].x > 0)? 1 : 2;\n       quadrant += (points[i].y > 0)? 1 : 0;\n       bins[quadrant]++;\n   }\n   */\n\n   #pragma omp parallel for\n   for (auto const& point: points) {\n       int quadrant = (point.x > 0)? 1 : 2;\n       quadrant += (point.y > 0)? 1 : 0;\n       //std::cout << \"Quadrant: \" << quadrant << std::endl;\n       #pragma omp atomic\n       bins[quadrant]++;\n   }\n\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        auto p = points[i];\n        if (p.x >= 0 && p.y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (p.x < 0 && p.y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        } else {\n            // error\n        }\n    }\n}",
            "// YOUR CODE GOES HERE\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      // Point x\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      // Point i\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      // Point ii\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      // Point iii\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: Implement me!\n\n}",
            "// TODO: fill this in\n}",
            "bins = {0, 0, 0, 0};\n\n    // Your code here\n\n    // for (auto i = 0; i < points.size(); i++)\n    // {\n    //     if (points[i].x > 0 && points[i].y > 0)\n    //     {\n    //         bins[0]++;\n    //     }\n    //     else if (points[i].x < 0 && points[i].y > 0)\n    //     {\n    //         bins[1]++;\n    //     }\n    //     else if (points[i].x < 0 && points[i].y < 0)\n    //     {\n    //         bins[2]++;\n    //     }\n    //     else if (points[i].x > 0 && points[i].y < 0)\n    //     {\n    //         bins[3]++;\n    //     }\n    // }\n}",
            "//TODO\n}",
            "//std::cout << \"countQuadrants\" << std::endl;\n   // use OpenMP here to count the points in each quadrant in parallel\n   //std::cout << \"countQuadrants\" << std::endl;\n\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         #pragma omp atomic\n         bins[0]++;\n      }\n      else if (points[i].x < 0 && points[i].y >= 0) {\n         #pragma omp atomic\n         bins[1]++;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      }\n      else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n   for(auto p = points.begin(); p!= points.end(); ++p)\n   {\n      if((p->x) >= 0 && (p->y) >= 0) bins[0]++;\n      if((p->x) < 0 && (p->y) >= 0) bins[1]++;\n      if((p->x) < 0 && (p->y) < 0) bins[2]++;\n      if((p->x) >= 0 && (p->y) < 0) bins[3]++;\n   }\n}",
            "// Your code here.\n   // Hint: You should only have 4 parallel regions.\n\n}",
            "//...\n}",
            "//#pragma omp parallel for\n   // for (int i = 0; i < points.size(); i++)\n   // {\n   //     if (points[i].x >= 0 && points[i].y >= 0)\n   //     {\n   //         bins[0]++;\n   //     }\n   //     else if (points[i].x < 0 && points[i].y >= 0)\n   //     {\n   //         bins[1]++;\n   //     }\n   //     else if (points[i].x < 0 && points[i].y < 0)\n   //     {\n   //         bins[2]++;\n   //     }\n   //     else\n   //     {\n   //         bins[3]++;\n   //     }\n   // }\n\n\n   // for(Point point : points){\n   //     if(point.x >= 0 && point.y >= 0){\n   //         bins[0]++;\n   //     }else if(point.x < 0 && point.y >= 0){\n   //         bins[1]++;\n   //     }else if(point.x < 0 && point.y < 0){\n   //         bins[2]++;\n   //     }else{\n   //         bins[3]++;\n   //     }\n   // }\n\n   // for(int i = 0; i < points.size(); i++)\n   // {\n   //     if (points[i].x >= 0 && points[i].y >= 0)\n   //     {\n   //         bins[0]++;\n   //     }\n   //     else if (points[i].x < 0 && points[i].y >= 0)\n   //     {\n   //         bins[1]++;\n   //     }\n   //     else if (points[i].x < 0 && points[i].y < 0)\n   //     {\n   //         bins[2]++;\n   //     }\n   //     else\n   //     {\n   //         bins[3]++;\n   //     }\n   // }\n\n   // for(int i = 0; i < points.size(); i++)\n   // {\n   //     if(points[i].x >= 0 && points[i].y >= 0){\n   //         bins[0]++;\n   //     }else if(points[i].x < 0 && points[i].y >= 0){\n   //         bins[1]++;\n   //     }else if(points[i].x < 0 && points[i].y < 0){\n   //         bins[2]++;\n   //     }else{\n   //         bins[3]++;\n   //     }\n   // }\n\n   int num_threads = omp_get_num_threads();\n   std::cout << \"Number of threads is \" << num_threads << std::endl;\n\n   #pragma omp parallel for\n   for(int i = 0; i < points.size(); i++){\n        if(points[i].x >= 0 && points[i].y >= 0){\n            bins[0]++;\n        }else if(points[i].x < 0 && points[i].y >= 0){\n            bins[1]++;\n        }else if(points[i].x < 0 && points[i].y < 0){\n            bins[2]++;\n        }else{\n            bins[3]++;\n        }\n   }\n\n   std::cout << \"Number of threads is \" << omp_get_num_threads() << std::endl;\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for\n      for(int i = 0; i < points.size(); i++){\n         // if (points[i].x >= 0 && points[i].y >= 0) { bins[0]++; }\n         // else if (points[i].x < 0 && points[i].y >= 0) { bins[1]++; }\n         // else if (points[i].x < 0 && points[i].y < 0) { bins[2]++; }\n         // else if (points[i].x >= 0 && points[i].y < 0) { bins[3]++; }\n\n         int index = ((points[i].x >= 0) << 1) | (points[i].y >= 0);\n         bins[index]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        Point const& point = points[i];\n        if (point.x >= 0 && point.y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        Point point = points[i];\n        if (point.x > 0 && point.y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if (point.x <= 0 && point.y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if (point.x <= 0 && point.y <= 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "// Your code goes here\n    #pragma omp parallel\n    {\n        int nthreads = omp_get_num_threads();\n        int threadID = omp_get_thread_num();\n        #pragma omp for\n        for(size_t i=0;i<points.size();++i)\n        {\n            int quadrant = 0;\n            if(points[i].x < 0) quadrant |= 1;\n            if(points[i].y < 0) quadrant |= 2;\n            //#pragma omp critical\n            //++bins[quadrant];\n            #pragma omp atomic\n            bins[quadrant]++;\n        }\n    }\n}",
            "// TODO: implement me\n}",
            "bins.fill(0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      double const x = points[i].x;\n      double const y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         bins[0]++;\n      } else if (x < 0 && y >= 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else if (x >= 0 && y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "//...\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        if(points[i].x > 0 && points[i].y > 0) {\n            ++bins[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            ++bins[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            ++bins[2];\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "// Your code goes here!\n    for(int i=0; i<4; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n    for(auto& point : points) {\n        if(point.x > 0 && point.y > 0) {\n            ++bins[0];\n        } else if(point.x < 0 && point.y > 0) {\n            ++bins[1];\n        } else if(point.x > 0 && point.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n\n}",
            "// use OpenMP to count in parallel\n   // Use omp_get_num_threads() to get the total number of threads\n   // Use omp_get_thread_num() to get the number of the current thread\n   // Use omp_get_max_threads() to get the max number of threads (used in the last for loop)\n   // HINT: Use the #pragma omp simd directive to vectorize the innermost loop\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      Point const& p = points[i];\n      double const x = p.x;\n      double const y = p.y;\n      if (x >= 0) {\n         if (y >= 0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (y >= 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "// Initialize each element of bins to 0\n   std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      int x = points[i].x;\n      int y = points[i].y;\n\n      int bin = (x >= 0 and y >= 0)? 0 : (x >= 0 and y < 0)? 1 : (x < 0 and y >= 0)? 2 : 3;\n      bins[bin] += 1;\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      int quadrant;\n      if (x < 0 && y < 0) quadrant = 3;\n      else if (x >= 0 && y < 0) quadrant = 2;\n      else if (x >= 0 && y >= 0) quadrant = 1;\n      else quadrant = 0;\n      #pragma omp atomic\n      bins[quadrant]++;\n   }\n}",
            "#pragma omp parallel for num_threads(4)\n    for (size_t i = 0; i < points.size(); i++) {\n        Point p = points[i];\n        int quad = (p.x >= 0) + 2 * (p.y >= 0);\n        #pragma omp atomic\n        bins[quad]++;\n    }\n}",
            "// TODO: implement this function\n\n}",
            "// YOUR CODE HERE\n   //\n   // Note:\n   //\n   // You're allowed to add any code you need to implement this function, including\n   // creating additional helper functions.\n   //\n   // You may assume that the given vector contains at least one point\n\n}",
            "// use OpenMP here\n   // Hint: you can use parallel for loops with the following syntax:\n   // #pragma omp parallel for\n   // for (size_t i = 0; i < points.size(); ++i) {\n   //    ...\n   // }\n   //\n   // Hint: you can get the current thread id with omp_get_thread_num(),\n   // which returns an integer with the id of the thread, from 0 to the\n   // number of threads used minus one\n   //\n   // Hint: you can use critical sections to prevent race conditions with\n   // the following syntax:\n   // #pragma omp critical\n   //... // the part of the code which should only be executed by one thread at a time\n   //\n   // Hint: you can use the following code to force the use of only one thread:\n   // #pragma omp parallel num_threads(1)\n\n}",
            "// Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use omp_get_thread_num() to determine the thread ID.\n\n  // Use omp_get_num_threads() to determine the number of threads.\n  // Use",
            "// TODO: implement\n}",
            "for (int i = 0; i < 4; i++) bins[i] = 0;\n\n   // YOUR CODE HERE\n}",
            "#pragma omp parallel for\n  for(size_t i = 0; i < points.size(); ++i) {\n    Point p = points[i];\n    if(p.x >= 0 && p.y >= 0)\n      bins[0]++;\n    else if(p.x < 0 && p.y >= 0)\n      bins[1]++;\n    else if(p.x >= 0 && p.y < 0)\n      bins[2]++;\n    else if(p.x < 0 && p.y < 0)\n      bins[3]++;\n  }\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for\n      for (auto const& p : points) {\n         double q = (p.x >= 0.0) - (p.x < 0.0);\n         double r = (p.y >= 0.0) - (p.y < 0.0);\n         bins[q + 2*r] += 1;\n      }\n   }\n}",
            "int n = points.size();\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n    #pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        if(points[i].x >= 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if(points[i].x < 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if(points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if(points[i].x >= 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (auto& point : points) {\n      int index = 0;\n      if (point.x > 0 && point.y > 0) {\n         index = 0;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         index = 1;\n      }\n      else if (point.x > 0 && point.y < 0) {\n         index = 2;\n      }\n      else {\n         index = 3;\n      }\n      bins[index]++;\n   }\n}",
            "// TODO\n}",
            "// TODO: add parallel loop here\n\n}",
            "// Initialize the bins to 0.\n   bins[0] = 0; bins[1] = 0; bins[2] = 0; bins[3] = 0;\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n\n      Point& p = points[i];\n\n      // Check to see which quadrant p is in.\n      if (p.x >= 0.0 && p.y >= 0.0) {\n         bins[0]++;\n      }\n      else if (p.x < 0.0 && p.y >= 0.0) {\n         bins[1]++;\n      }\n      else if (p.x < 0.0 && p.y < 0.0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "#pragma omp parallel for\n    for(size_t i=0; i < points.size(); i++) {\n        if(points[i].x >= 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if(points[i].x < 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if(points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if(points[i].x >= 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "/* TODO: Complete this method */\n   bins.fill(0);\n   #pragma omp parallel for\n   for (auto i = 0u; i < points.size(); ++i)\n   {\n      if (points[i].x >= 0 && points[i].y >= 0)\n      {\n         #pragma omp atomic\n         bins[0]++;\n      }\n      else if (points[i].x < 0 && points[i].y >= 0)\n      {\n         #pragma omp atomic\n         bins[1]++;\n      }\n      else if (points[i].x < 0 && points[i].y < 0)\n      {\n         #pragma omp atomic\n         bins[2]++;\n      }\n      else if (points[i].x >= 0 && points[i].y < 0)\n      {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        Point p = points[i];\n        int quadrant = -1;\n        if (p.x < 0) {\n            if (p.y < 0) {\n                quadrant = 2;\n            } else {\n                quadrant = 3;\n            }\n        } else {\n            if (p.y < 0) {\n                quadrant = 1;\n            } else {\n                quadrant = 0;\n            }\n        }\n        ++bins[quadrant];\n    }\n}",
            "}",
            "std::array<size_t, 4> tBins = {0, 0, 0, 0};\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      int x = points[i].x;\n      int y = points[i].y;\n\n      if (x > 0 && y > 0) {\n         tBins[0]++;\n      }\n      else if (x < 0 && y > 0) {\n         tBins[1]++;\n      }\n      else if (x < 0 && y < 0) {\n         tBins[2]++;\n      }\n      else if (x > 0 && y < 0) {\n         tBins[3]++;\n      }\n\n   }\n\n   for (size_t i = 0; i < tBins.size(); i++) {\n      bins[i] = tBins[i];\n   }\n\n}",
            "size_t npoints = points.size();\n   #pragma omp parallel for\n   for(size_t i = 0; i < npoints; i++)\n   {\n      auto& pt = points[i];\n      if(pt.x >= 0)\n      {\n         if(pt.y >= 0) bins[0]++;\n         else          bins[1]++;\n      }\n      else\n      {\n         if(pt.y >= 0) bins[2]++;\n         else          bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < points.size(); ++i) {\n        const auto &p = points[i];\n\n        // Which quadrant the current point belongs to.\n        int quadrant = 0;\n        if (p.x < 0 && p.y < 0) {\n            quadrant = 1;\n        } else if (p.x >= 0 && p.y < 0) {\n            quadrant = 2;\n        } else if (p.x < 0 && p.y >= 0) {\n            quadrant = 3;\n        }\n\n        #pragma omp atomic\n        bins[quadrant]++;\n    }\n}",
            "// TODO: Implement this function\n   std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n       if (points[i].x >= 0 && points[i].y >= 0)\n           bins[0]++;\n       else if (points[i].x < 0 && points[i].y >= 0)\n           bins[1]++;\n       else if (points[i].x >= 0 && points[i].y < 0)\n           bins[2]++;\n       else\n           bins[3]++;\n   }\n}",
            "// #pragma omp parallel for num_threads(4)\n   // for(auto& p:points) {\n   //    auto x = p.x;\n   //    auto y = p.y;\n   //    if(x >= 0) {\n   //       if(y >= 0) {\n   //          ++bins[0];\n   //       } else {\n   //          ++bins[1];\n   //       }\n   //    } else {\n   //       if(y >= 0) {\n   //          ++bins[2];\n   //       } else {\n   //          ++bins[3];\n   //       }\n   //    }\n   // }\n}",
            "// Insert your implementation here\n}",
            "// Your code here\n  bins = {0,0,0,0};\n  #pragma omp parallel\n  {\n      #pragma omp for\n      for(auto it = points.cbegin(); it < points.cend(); ++it) {\n          if(it->x > 0 && it->y > 0) ++bins[0];\n          else if(it->x < 0 && it->y > 0) ++bins[1];\n          else if(it->x < 0 && it->y < 0) ++bins[2];\n          else if(it->x > 0 && it->y < 0) ++bins[3];\n      }\n  }\n}",
            "// TODO\n}",
            "size_t const npoints = points.size();\n    std::array<size_t, 4> bins_private;\n    #pragma omp parallel for\n    for (size_t i = 0; i < npoints; ++i) {\n        if (points[i].x >= 0) {\n            if (points[i].y >= 0) {\n                bins_private[0]++;\n            } else {\n                bins_private[2]++;\n            }\n        } else {\n            if (points[i].y >= 0) {\n                bins_private[1]++;\n            } else {\n                bins_private[3]++;\n            }\n        }\n    }\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = bins_private[i];\n    }\n}",
            "// TODO: implement this function using OpenMP\n}",
            "int n = points.size();\n    int nthreads = omp_get_num_threads();\n    int tid = omp_get_thread_num();\n    std::cout << \"thread id: \" << tid << \" num of threads: \" << nthreads << std::endl;\n\n    for (int i = 0; i < n; i++) {\n        Point p = points[i];\n        int x = (p.x > 0)? 0 : (p.x < 0)? 1 : 2;\n        int y = (p.y > 0)? 1 : (p.y < 0)? 2 : 3;\n        int bin = x + y * 2;\n        std::cout << \"bin: \" << bin << std::endl;\n    }\n\n}",
            "// add code here\n    #pragma omp parallel for\n    for(size_t i=0; i<points.size(); ++i)\n    {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        int index = (x > 0 && y > 0)? 0 :\n                    (x < 0 && y > 0)? 1 :\n                    (x < 0 && y < 0)? 2 :\n                                       3;\n\n        #pragma omp atomic\n        ++bins[index];\n    }\n}",
            "// TODO: fill in the code\n   int nthreads;\n#pragma omp parallel num_threads(4)\n{\n   nthreads = omp_get_num_threads();\n   int threadID = omp_get_thread_num();\n\n   #pragma omp for\nfor(int i = 0; i < points.size(); i++){\n    if(threadID == 0 && points[i].x > 0 && points[i].y > 0){\n        bins[0]++;\n    }\n    else if(threadID == 1 && points[i].x < 0 && points[i].y > 0){\n        bins[1]++;\n    }\n    else if(threadID == 2 && points[i].x < 0 && points[i].y < 0){\n        bins[2]++;\n    }\n    else if(threadID == 3 && points[i].x > 0 && points[i].y < 0){\n        bins[3]++;\n    }\n}\n}\n}",
            "for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        const Point &p = points[i];\n        size_t bin = 0;\n        if (p.x > 0) bin |= 1;\n        if (p.y > 0) bin |= 2;\n\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "#pragma omp parallel\n   {\n      int i, n;\n      #pragma omp for\n      for (i = 0; i < points.size(); i++) {\n         n = (points[i].x > 0) * 2 + (points[i].y > 0);\n         #pragma omp atomic\n         bins[n]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (auto const &p : points) {\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n         } else {\n            #pragma omp atomic\n            bins[1]++;\n         }\n      } else {\n         if (p.y >= 0) {\n            #pragma omp atomic\n            bins[2]++;\n         } else {\n            #pragma omp atomic\n            bins[3]++;\n         }\n      }\n   }\n}",
            "/* TODO */\n}",
            "bins = {0, 0, 0, 0};\n\n    // #pragma omp parallel for schedule(dynamic, 1)\n    // for (size_t i = 0; i < points.size(); ++i) {\n    //     Point p = points[i];\n    //     if (p.x >= 0 && p.y >= 0)\n    //         ++bins[0];\n    //     else if (p.x < 0 && p.y >= 0)\n    //         ++bins[1];\n    //     else if (p.x < 0 && p.y < 0)\n    //         ++bins[2];\n    //     else if (p.x >= 0 && p.y < 0)\n    //         ++bins[3];\n    // }\n\n    #pragma omp parallel for schedule(dynamic, 1)\n    for (size_t i = 0; i < points.size(); ++i) {\n        auto &p = points[i];\n        #pragma omp critical\n        {\n            if (p.x >= 0 && p.y >= 0)\n                ++bins[0];\n            else if (p.x < 0 && p.y >= 0)\n                ++bins[1];\n            else if (p.x < 0 && p.y < 0)\n                ++bins[2];\n            else if (p.x >= 0 && p.y < 0)\n                ++bins[3];\n        }\n    }\n}",
            "// TODO: Fill in code here.\n}",
            "// Count the number of points in each quadrant\n    // in parallel\n    // using OpenMP\n    // Use a critical section to ensure that\n    // the bins are not incremented by multiple threads\n    // at the same time\n    #pragma omp parallel\n    {\n    #pragma omp critical\n    {\n        for (auto point: points) {\n            if (point.x >= 0 && point.y >= 0)\n            {\n                bins[0]++;\n            }\n            else if (point.x < 0 && point.y >= 0)\n            {\n                bins[1]++;\n            }\n            else if (point.x < 0 && point.y < 0)\n            {\n                bins[2]++;\n            }\n            else if (point.x >= 0 && point.y < 0)\n            {\n                bins[3]++;\n            }\n        }\n    }\n    }\n}",
            "// TODO\n    // #pragma omp parallel for\n    for(int i=0; i<points.size(); i++)\n    {\n        if(points[i].x > 0 && points[i].y > 0)\n            bins[0]++;\n        else if(points[i].x < 0 && points[i].y > 0)\n            bins[1]++;\n        else if(points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n        else if(points[i].x > 0 && points[i].y < 0)\n            bins[3]++;\n    }\n}",
            "#pragma omp parallel for\n   for(size_t i=0; i<points.size(); ++i) {\n      Point p = points[i];\n      if(p.x >= 0 && p.y >= 0) {\n         #pragma omp atomic update\n         bins[0]++;\n      } else if(p.x < 0 && p.y >= 0) {\n         #pragma omp atomic update\n         bins[1]++;\n      } else if(p.x >= 0 && p.y < 0) {\n         #pragma omp atomic update\n         bins[2]++;\n      } else if(p.x < 0 && p.y < 0) {\n         #pragma omp atomic update\n         bins[3]++;\n      }\n   }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (auto const& p : points) {\n    int q = 0;\n    if (p.x > 0 && p.y > 0) {\n      q = 0;\n    }\n    else if (p.x < 0 && p.y > 0) {\n      q = 1;\n    }\n    else if (p.x < 0 && p.y < 0) {\n      q = 2;\n    }\n    else if (p.x > 0 && p.y < 0) {\n      q = 3;\n    }\n    else {\n      continue;\n    }\n\n    bins.at(q) += 1;\n  }\n}",
            "std::array<size_t, 4> tmp = {};\n   std::for_each(points.cbegin(), points.cend(), [&tmp] (Point const& p) {\n       auto const index = (p.x >= 0) + 2 * (p.y >= 0);\n       ++tmp[index];\n   });\n   bins = tmp;\n}",
            "for (auto const& pt : points) {\n      if (pt.x > 0.0 && pt.y > 0.0) {\n         ++bins[0];\n      } else if (pt.x < 0.0 && pt.y > 0.0) {\n         ++bins[1];\n      } else if (pt.x < 0.0 && pt.y < 0.0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      }\n      if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      }\n      if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      }\n      if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "int size = points.size();\n\n    for (int i = 0; i < size; i++) {\n        Point point = points[i];\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0] += 1;\n        }\n        else if (point.x < 0 && point.y >= 0) {\n            bins[1] += 1;\n        }\n        else if (point.x < 0 && point.y < 0) {\n            bins[2] += 1;\n        }\n        else if (point.x >= 0 && point.y < 0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n\n    for (Point const& point : points) {\n        int quadrant = -1;\n        if (point.x >= 0 && point.y >= 0) {\n            quadrant = 0;\n        }\n        else if (point.x <= 0 && point.y >= 0) {\n            quadrant = 1;\n        }\n        else if (point.x <= 0 && point.y <= 0) {\n            quadrant = 2;\n        }\n        else if (point.x >= 0 && point.y <= 0) {\n            quadrant = 3;\n        }\n        if (quadrant >= 0) {\n            ++bins.at(quadrant);\n        }\n    }\n\n}",
            "for (auto const& p : points) {\n    if (p.x >= 0 && p.y >= 0) {\n      ++bins[0];\n    } else if (p.x < 0 && p.y >= 0) {\n      ++bins[1];\n    } else if (p.x < 0 && p.y < 0) {\n      ++bins[2];\n    } else if (p.x >= 0 && p.y < 0) {\n      ++bins[3];\n    }\n  }\n}",
            "std::array<size_t, 4> bins = {0,0,0,0};\n   for(auto point : points) {\n      if(point.x < 0 && point.y < 0)\n         bins[0]++;\n      else if(point.x >= 0 && point.y < 0)\n         bins[1]++;\n      else if(point.x >= 0 && point.y >= 0)\n         bins[2]++;\n      else if(point.x < 0 && point.y >= 0)\n         bins[3]++;\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 and point.y >= 0)\n         bins[0]++;\n      else if (point.x < 0 and point.y >= 0)\n         bins[1]++;\n      else if (point.x >= 0 and point.y < 0)\n         bins[2]++;\n      else if (point.x < 0 and point.y < 0)\n         bins[3]++;\n   }\n}",
            "for (auto const& pt : points) {\n      if (pt.x > 0 && pt.y > 0)\n         ++bins[0];\n      else if (pt.x < 0 && pt.y > 0)\n         ++bins[1];\n      else if (pt.x < 0 && pt.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "// your code goes here\n}",
            "for(auto const& point: points) {\n      if (point.x < 0 and point.y > 0) {\n         bins.at(0)++;\n      } else if (point.x > 0 and point.y > 0) {\n         bins.at(1)++;\n      } else if (point.x < 0 and point.y < 0) {\n         bins.at(2)++;\n      } else if (point.x > 0 and point.y < 0) {\n         bins.at(3)++;\n      }\n   }\n}",
            "for (auto const& pt : points) {\n        if (pt.x >= 0.0 && pt.y >= 0.0) {\n            ++bins[0];\n        } else if (pt.x < 0.0 && pt.y >= 0.0) {\n            ++bins[1];\n        } else if (pt.x < 0.0 && pt.y < 0.0) {\n            ++bins[2];\n        } else if (pt.x >= 0.0 && pt.y < 0.0) {\n            ++bins[3];\n        }\n    }\n}",
            "for (Point const& p : points) {\n        if (p.x >= 0 && p.y >= 0)\n            ++bins[0];\n        else if (p.x < 0 && p.y >= 0)\n            ++bins[1];\n        else if (p.x < 0 && p.y < 0)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      }\n      else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      }\n      else if (points[i].x > 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (const auto& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        }\n        else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        }\n        else if (point.x > 0 && point.y < 0) {\n            bins[2]++;\n        }\n        else if (point.x < 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n  for (Point const& point : points) {\n    if (point.x > 0 && point.y > 0)\n      ++bins[0];\n    else if (point.x < 0 && point.y > 0)\n      ++bins[1];\n    else if (point.x < 0 && point.y < 0)\n      ++bins[2];\n    else if (point.x > 0 && point.y < 0)\n      ++bins[3];\n  }\n}",
            "// TODO: Calculate the number of points in each quadrant.\n   // 0 = lower-left, 1 = lower-right, 2 = upper-left, 3 = upper-right\n   bins = {0, 0, 0, 0};\n\n   for (auto const& point : points) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            ++bins[0];\n         }\n         else {\n            ++bins[1];\n         }\n      }\n      else {\n         if (point.y > 0) {\n            ++bins[2];\n         }\n         else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n   for (auto const& p : points)\n      bins[getQuadrant(p)]++;\n}",
            "for (Point const& p : points) {\n        auto const x = p.x;\n        auto const y = p.y;\n        auto const is_first_quadrant = (x >= 0 && y >= 0);\n        auto const is_second_quadrant = (x < 0 && y >= 0);\n        auto const is_third_quadrant = (x < 0 && y < 0);\n        auto const is_fourth_quadrant = (x >= 0 && y < 0);\n\n        if (is_first_quadrant) {\n            ++bins[0];\n        } else if (is_second_quadrant) {\n            ++bins[1];\n        } else if (is_third_quadrant) {\n            ++bins[2];\n        } else if (is_fourth_quadrant) {\n            ++bins[3];\n        }\n    }\n}",
            "for (auto& p : points) {\n      auto x = p.x;\n      auto y = p.y;\n      // Fill in this line\n   }\n}",
            "bins.fill(0);\n   for (auto const& p : points)\n      ++bins[(p.x > 0 && p.y > 0) | (p.x < 0 && p.y > 0) << 1 | (p.x > 0 && p.y < 0) << 2];\n}",
            "for(const Point& point : points) {\n      if (point.x < 0 && point.y < 0) bins[0]++;\n      else if (point.x > 0 && point.y < 0) bins[1]++;\n      else if (point.x > 0 && point.y > 0) bins[2]++;\n      else if (point.x < 0 && point.y > 0) bins[3]++;\n   }\n}",
            "// Please complete this method\n}",
            "for (Point const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// Fill in the code here\n    // Loop over the points in the vector\n    // Count the number of points in each quadrant\n    // Store the counts in the array bins\n    for (const Point& p: points) {\n        if (p.x > 0 && p.y > 0)\n            bins[0] += 1;\n        else if (p.x < 0 && p.y > 0)\n            bins[1] += 1;\n        else if (p.x < 0 && p.y < 0)\n            bins[2] += 1;\n        else if (p.x > 0 && p.y < 0)\n            bins[3] += 1;\n    }\n}",
            "size_t i = 0;\n\n    for (auto const& x: points) {\n        if (x.x >= 0 && x.y >= 0) {\n            bins[0]++;\n        } else if (x.x <= 0 && x.y >= 0) {\n            bins[1]++;\n        } else if (x.x <= 0 && x.y <= 0) {\n            bins[2]++;\n        } else if (x.x >= 0 && x.y <= 0) {\n            bins[3]++;\n        }\n        i++;\n    }\n}",
            "bins.fill(0);\n\n    for (Point const& point : points) {\n        int x = point.x, y = point.y;\n        if (x > 0 && y > 0)\n            bins[0]++;\n        else if (x < 0 && y > 0)\n            bins[1]++;\n        else if (x < 0 && y < 0)\n            bins[2]++;\n        else if (x > 0 && y < 0)\n            bins[3]++;\n    }\n}",
            "for (const auto& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "size_t i = 0;\n   for (Point p : points)\n   {\n      if (p.x > 0 && p.y > 0) bins[i]++;\n      else if (p.x < 0 && p.y > 0) bins[i+1]++;\n      else if (p.x < 0 && p.y < 0) bins[i+2]++;\n      else if (p.x > 0 && p.y < 0) bins[i+3]++;\n      i++;\n   }\n}",
            "for(auto it = points.begin(); it!= points.end(); ++it) {\n      if(it->x > 0) {\n         if(it->y > 0) {\n            bins[0]++;\n         } else {\n            bins[2]++;\n         }\n      } else {\n         if(it->y > 0) {\n            bins[1]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "size_t ctr = 0;\n    for (auto p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n        ctr++;\n    }\n}",
            "for (Point const& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y > 0) {\n            bins[1]++;\n        } else if (point.x > 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n    for(auto p : points){\n        if(p.x > 0 && p.y > 0) bins[0]++;\n        else if(p.x < 0 && p.y > 0) bins[1]++;\n        else if(p.x < 0 && p.y < 0) bins[2]++;\n        else if(p.x > 0 && p.y < 0) bins[3]++;\n    }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else if (point.x > 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto &p: points) {\n        int quadrant = (p.x > 0 && p.y > 0)? 1 :\n                       (p.x < 0 && p.y > 0)? 2 :\n                       (p.x > 0 && p.y < 0)? 3 :\n                       4;\n\n        bins[quadrant - 1] += 1;\n    }\n}",
            "// write your code here\n\n    // This is a sample solution:\n    for (auto const& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            ++bins[0];\n        }\n        else if (p.x < 0 && p.y > 0) {\n            ++bins[1];\n        }\n        else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "// IMPLEMENT THIS\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (auto point : points) {\n    if (point.x > 0 && point.y > 0) {\n      bins[0]++;\n    } else if (point.x < 0 && point.y > 0) {\n      bins[1]++;\n    } else if (point.x < 0 && point.y < 0) {\n      bins[2]++;\n    } else if (point.x > 0 && point.y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "// Implement this function...\n}",
            "// TODO: Write your solution here\n   for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "//...\n    for(int i=0;i<4;i++){\n        bins[i]=0;\n    }\n\n    for(int i=0;i<points.size();i++){\n        if(points[i].x>0 && points[i].y>0){\n            bins[0]+=1;\n        }\n        if(points[i].x<0 && points[i].y>0){\n            bins[1]+=1;\n        }\n        if(points[i].x<0 && points[i].y<0){\n            bins[2]+=1;\n        }\n        if(points[i].x>0 && points[i].y<0){\n            bins[3]+=1;\n        }\n    }\n}",
            "for(auto it = points.begin(); it!= points.end(); ++it) {\n      if(it->x > 0 && it->y > 0) bins[0]++;\n      else if(it->x < 0 && it->y > 0) bins[1]++;\n      else if(it->x < 0 && it->y < 0) bins[2]++;\n      else if(it->x > 0 && it->y < 0) bins[3]++;\n   }\n}",
            "// write your solution here\n}",
            "for (auto const& p : points) {\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (p.y >= 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x < 0 && point.y < 0) bins[0]++;\n      else if (point.x >= 0 && point.y < 0) bins[1]++;\n      else if (point.x >= 0 && point.y >= 0) bins[2]++;\n      else if (point.x < 0 && point.y >= 0) bins[3]++;\n   }\n}",
            "// your code here\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      auto const x = p.x;\n      auto const y = p.y;\n      auto const q = (x > 0? 1 : 0) + (y > 0? 2 : 0);\n      bins[q]++;\n   }\n}",
            "for (auto& pt : points) {\n      auto x = pt.x, y = pt.y;\n      if (x >= 0 && y >= 0) {\n         bins[0]++;\n      } else if (x < 0 && y >= 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else if (x >= 0 && y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n      size_t bin = 0;\n      if (p.x < 0) {\n         bin += 1;\n         if (p.y < 0) {\n            bin += 1;\n         }\n      } else if (p.y > 0) {\n         bin += 2;\n      }\n      bins[bin]++;\n   }\n}",
            "std::array<size_t, 4> quadrants { {0, 0, 0, 0} };\n\n   for (auto const& point: points) {\n      auto quadrant = 0;\n\n      if (point.x >= 0) {\n         quadrant |= 1;\n      }\n\n      if (point.y >= 0) {\n         quadrant |= 2;\n      }\n\n      quadrants[quadrant]++;\n   }\n\n   bins = quadrants;\n}",
            "for(auto p : points){\n      if(p.x > 0 && p.y > 0){\n         bins[0]++;\n      }else if(p.x < 0 && p.y > 0){\n         bins[1]++;\n      }else if(p.x > 0 && p.y < 0){\n         bins[2]++;\n      }else if(p.x < 0 && p.y < 0){\n         bins[3]++;\n      }\n   }\n}",
            "bins = std::array<size_t, 4>{};\n\n    for (Point const &point : points) {\n        int x = point.x;\n        int y = point.y;\n\n        if (x > 0 && y > 0) {\n            ++bins[0];\n        } else if (x < 0 && y > 0) {\n            ++bins[1];\n        } else if (x < 0 && y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "for (auto p : points) {\n    if (p.x >= 0 && p.y >= 0) {\n      bins[0]++;\n    } else if (p.x < 0 && p.y >= 0) {\n      bins[1]++;\n    } else if (p.x >= 0 && p.y < 0) {\n      bins[2]++;\n    } else if (p.x < 0 && p.y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "for(Point const& p : points) {\n       if(p.x >= 0 && p.y >= 0) { bins[0]++; }\n       else if(p.x < 0 && p.y >= 0) { bins[1]++; }\n       else if(p.x < 0 && p.y < 0) { bins[2]++; }\n       else { bins[3]++; }\n   }\n}",
            "// code here\n   for(int i = 0; i < points.size(); i++){\n      if(points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      }else if(points[i].x < 0 && points[i].y > 0){\n         bins[1]++;\n      }else if(points[i].x < 0 && points[i].y < 0){\n         bins[2]++;\n      }else if(points[i].x > 0 && points[i].y < 0){\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const &point : points) {\n        if (point.x > 0 && point.y > 0)\n            bins[0]++;\n        else if (point.x < 0 && point.y > 0)\n            bins[1]++;\n        else if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "// The bins are indexed as:\n    //      [0] : 0: upper-left quadrant\n    //      [1] : 1: upper-right quadrant\n    //      [2] : 2: lower-left quadrant\n    //      [3] : 3: lower-right quadrant\n\n    //...\n}",
            "// Your code here\n\n}",
            "for (auto &p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else if (p.x > 0 && p.y < 0)\n         ++bins[3];\n   }\n}",
            "for (Point const& p : points) {\n      double x = p.x;\n      double y = p.y;\n      if (x > 0 && y > 0)\n         ++bins[0];\n      else if (x < 0 && y > 0)\n         ++bins[1];\n      else if (x < 0 && y < 0)\n         ++bins[2];\n      else if (x > 0 && y < 0)\n         ++bins[3];\n   }\n}",
            "for (Point const& p : points) {\n      int quad = 0;\n      if (p.x < 0)\n         quad += 1;\n      if (p.y < 0)\n         quad += 2;\n      bins[quad]++;\n   }\n}",
            "for (auto const& point: points) {\n        double x = point.x;\n        double y = point.y;\n        if (x < 0 && y < 0) {\n            bins[0]++;\n        } else if (x >= 0 && y < 0) {\n            bins[1]++;\n        } else if (x < 0 && y >= 0) {\n            bins[2]++;\n        } else if (x >= 0 && y >= 0) {\n            bins[3]++;\n        } else {\n            throw std::domain_error(\"Unreachable\");\n        }\n    }\n}",
            "for(Point& point: points) {\n\n        if(point.x >= 0 && point.y >= 0) bins[0]++;\n        else if(point.x < 0 && point.y >= 0) bins[1]++;\n        else if(point.x < 0 && point.y < 0) bins[2]++;\n        else bins[3]++;\n    }\n\n}",
            "for (Point const& point : points) {\n        if (point.x > 0 && point.y > 0)\n            bins[0]++;\n        else if (point.x < 0 && point.y > 0)\n            bins[1]++;\n        else if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        else if (point.x > 0 && point.y < 0)\n            bins[3]++;\n    }\n}",
            "for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0) {\n            if (points[i].y >= 0) {\n                bins[0]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n        else {\n            if (points[i].y >= 0) {\n                bins[1]++;\n            }\n            else {\n                bins[2]++;\n            }\n        }\n    }\n\n}",
            "for(Point point: points) {\n       if(point.x > 0) {\n           if(point.y > 0) {\n               bins[0]++;\n           } else {\n               bins[3]++;\n           }\n       } else {\n           if(point.y > 0) {\n               bins[1]++;\n           } else {\n               bins[2]++;\n           }\n       }\n   }\n}",
            "for (Point const& point : points) {\n      if (point.x >= 0 && point.y >= 0)\n         ++bins[0];\n      else if (point.x < 0 && point.y >= 0)\n         ++bins[1];\n      else if (point.x < 0 && point.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "/*\n     * your code goes here\n     */\n}",
            "for (auto& p : points) {\n      int idx = (p.y >= 0) * 2 + (p.x >= 0);\n      bins[idx] += 1;\n   }\n}",
            "for (auto p : points) {\n      int quadrant = 0;\n\n      if (p.x < 0 && p.y > 0) quadrant = 0;\n      if (p.x >= 0 && p.y > 0) quadrant = 1;\n      if (p.x >= 0 && p.y < 0) quadrant = 2;\n      if (p.x < 0 && p.y < 0) quadrant = 3;\n\n      bins[quadrant]++;\n   }\n}",
            "for (Point const& p : points) {\n      if (p.x > 0) {\n         if (p.y > 0) bins[0]++;\n         else if (p.y < 0) bins[3]++;\n      } else if (p.x < 0) {\n         if (p.y > 0) bins[1]++;\n         else if (p.y < 0) bins[2]++;\n      }\n   }\n}",
            "bins.fill(0);\n\n   for (auto const& point : points) {\n      auto const x = point.x;\n      auto const y = point.y;\n      auto const quadrant = [x, y]() -> size_t {\n         if (x < 0) {\n            if (y < 0)\n               return 0;\n            else\n               return 1;\n         } else {\n            if (y < 0)\n               return 2;\n            else\n               return 3;\n         }\n      }();\n      ++bins.at(quadrant);\n   }\n}",
            "// TODO\n}",
            "int i = 0;\n    for (auto const& p: points) {\n        int b = 0;\n        if (p.x < 0) b += 1;\n        if (p.y < 0) b += 2;\n        bins[b]++;\n    }\n}",
            "for (auto const& point: points) {\n        size_t idx = 0;\n        if (point.x > 0)\n            idx += 1;\n        if (point.y > 0)\n            idx += 2;\n        ++bins[idx];\n    }\n}",
            "std::array<size_t, 4> tmp {0, 0, 0, 0};\n    for (auto const& p: points) {\n        if (p.x > 0 && p.y > 0) tmp[0]++;\n        else if (p.x < 0 && p.y > 0) tmp[1]++;\n        else if (p.x < 0 && p.y < 0) tmp[2]++;\n        else if (p.x > 0 && p.y < 0) tmp[3]++;\n    }\n    bins = tmp;\n}",
            "std::for_each(points.begin(), points.end(), [&bins](Point point) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[3]++;\n      }\n   });\n}",
            "bins = std::array<size_t, 4>{};\n\n   // TODO\n}",
            "for (Point const& point : points) {\n      int quadrant = 0;\n      if (point.x >= 0 && point.y >= 0) quadrant = 1;\n      if (point.x < 0 && point.y < 0) quadrant = 2;\n      if (point.x <= 0 && point.y >= 0) quadrant = 3;\n      if (point.x > 0 && point.y < 0) quadrant = 4;\n      bins[quadrant-1] += 1;\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      }\n      else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      }\n      else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// Implement this method\n}",
            "for(const auto &p: points){\n        if (p.x < 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x > 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for(Point const& p : points) {\n      if(p.x >= 0 && p.y >= 0)\n         ++bins[0];\n      else if(p.x < 0 && p.y >= 0)\n         ++bins[1];\n      else if(p.x < 0 && p.y < 0)\n         ++bins[2];\n      else if(p.x >= 0 && p.y < 0)\n         ++bins[3];\n   }\n}",
            "size_t n = points.size();\n   for (size_t i = 0; i < n; ++i) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x <= 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x <= 0 && p.y <= 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// Your code here\n}",
            "for (auto point : points) {\n    if (point.x >= 0) {\n      if (point.y >= 0) {\n        bins[0]++;\n      }\n      else {\n        bins[2]++;\n      }\n    }\n    else {\n      if (point.y >= 0) {\n        bins[1]++;\n      }\n      else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "bins.fill(0);\n\n    for (auto const& point : points) {\n        // Determine the quadrant of the point.\n        // Hint: if x is positive and y is positive, then the point is in quadrant 1.\n        // Hint: if x is negative and y is positive, then the point is in quadrant 2.\n        // Hint: if x is negative and y is negative, then the point is in quadrant 3.\n        // Hint: if x is positive and y is negative, then the point is in quadrant 4.\n        // Hint: if x is zero and y is positive, then the point is in quadrant 1.\n        // Hint: if x is zero and y is negative, then the point is in quadrant 2.\n        // Hint: if x is negative and y is zero, then the point is in quadrant 3.\n        // Hint: if x is positive and y is zero, then the point is in quadrant 4.\n        //\n        // Hint: you can use `>`, `<`, `==` and `&&` operators to implement this.\n        //\n        // Hint: you can use `if` and `else` to select which bin to increment.\n        //\n        // Hint: you can use `std::array` to store the four bins.\n        //\n        // Hint: You can use `bins[index]++` to increment the value of the bin at position `index`.\n        //\n        // Hint: you can use `bins.at(index)` to access the bin at position `index`.\n\n    }\n}",
            "bins.fill(0);\n  for(const auto &p: points){\n    if (p.x > 0 && p.y > 0) {\n      bins[0]++;\n    } else if (p.x < 0 && p.y > 0) {\n      bins[1]++;\n    } else if (p.x < 0 && p.y < 0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "int size = points.size();\n\n    for(int i = 0; i < size; i++){\n        if (points[i].x >= 0 && points[i].y >= 0){\n            bins[0]++;\n        }\n        else if(points[i].x < 0 && points[i].y >= 0){\n            bins[1]++;\n        }\n        else if(points[i].x < 0 && points[i].y < 0){\n            bins[2]++;\n        }\n        else if(points[i].x >= 0 && points[i].y < 0){\n            bins[3]++;\n        }\n    }\n}",
            "for (int i = 0; i < points.size(); i++)\n  {\n    if (points[i].x >= 0 && points[i].y >= 0)\n    {\n      bins[0]++;\n    }\n    if (points[i].x <= 0 && points[i].y >= 0)\n    {\n      bins[1]++;\n    }\n    if (points[i].x <= 0 && points[i].y <= 0)\n    {\n      bins[2]++;\n    }\n    if (points[i].x >= 0 && points[i].y <= 0)\n    {\n      bins[3]++;\n    }\n  }\n}",
            "for (auto const& p : points) {\n      if (p.x >= 0) {\n         bins[0] += 1;\n      } else {\n         bins[2] += 1;\n      }\n      if (p.y >= 0) {\n         bins[1] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (auto &p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// write your code here\n}",
            "for(auto i = points.cbegin(); i!= points.cend(); i++) {\n      if(i->x >= 0 && i->y >= 0) bins[0]++;\n      else if(i->x < 0 && i->y >= 0) bins[1]++;\n      else if(i->x < 0 && i->y < 0) bins[2]++;\n      else if(i->x >= 0 && i->y < 0) bins[3]++;\n   }\n}",
            "for (auto it : points) {\n        if (it.x >= 0 && it.y >= 0)\n            bins[0]++;\n        else if (it.x <= 0 && it.y >= 0)\n            bins[1]++;\n        else if (it.x <= 0 && it.y <= 0)\n            bins[2]++;\n        else if (it.x >= 0 && it.y <= 0)\n            bins[3]++;\n    }\n}",
            "for(const Point &p: points) {\n      if(p.x >= 0) {\n         if(p.y >= 0)\n            bins[0] += 1;\n         else\n            bins[1] += 1;\n      }\n      else {\n         if(p.y >= 0)\n            bins[2] += 1;\n         else\n            bins[3] += 1;\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n    for (auto const& point : points) {\n        int quadrant;\n        if (point.x > 0 && point.y > 0) {\n            quadrant = 0;\n        } else if (point.x < 0 && point.y > 0) {\n            quadrant = 1;\n        } else if (point.x < 0 && point.y < 0) {\n            quadrant = 2;\n        } else if (point.x > 0 && point.y < 0) {\n            quadrant = 3;\n        }\n        bins[quadrant]++;\n    }\n}",
            "// your code here\n}",
            "size_t c1 = 0, c2 = 0, c3 = 0, c4 = 0;\n  for (auto it = points.begin(); it!= points.end(); ++it) {\n    if (it->x < 0 && it->y > 0) {\n      ++c1;\n    } else if (it->x < 0 && it->y < 0) {\n      ++c2;\n    } else if (it->x > 0 && it->y < 0) {\n      ++c3;\n    } else if (it->x > 0 && it->y > 0) {\n      ++c4;\n    }\n  }\n  bins[0] = c1;\n  bins[1] = c2;\n  bins[2] = c3;\n  bins[3] = c4;\n}",
            "for (Point point : points) {\n      int x = point.x;\n      int y = point.y;\n      if (x >= 0 && y >= 0) {\n         bins[0]++;\n      } else if (x < 0 && y >= 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else if (x >= 0 && y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "/* TODO: */\n\n}",
            "for(const auto &p: points) {\n      if(p.x >= 0 && p.y >= 0) bins[0]++;\n      if(p.x < 0 && p.y >= 0) bins[1]++;\n      if(p.x >= 0 && p.y < 0) bins[2]++;\n      if(p.x < 0 && p.y < 0) bins[3]++;\n   }\n}",
            "for(Point p : points) {\n        if(p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if(p.x <= 0 && p.y >= 0) {\n            bins[1]++;\n        } else if(p.x <= 0 && p.y <= 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    for (auto& p : points) {\n        if (p.x >= 0 && p.y >= 0)\n            bins[0]++;\n        else if (p.x < 0 && p.y >= 0)\n            bins[1]++;\n        else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n        else if (p.x >= 0 && p.y < 0)\n            bins[3]++;\n    }\n}",
            "for (auto p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& p : points) {\n        if (p.x > 0 && p.y > 0)\n            bins[0]++;\n        if (p.x > 0 && p.y < 0)\n            bins[1]++;\n        if (p.x < 0 && p.y > 0)\n            bins[2]++;\n        if (p.x < 0 && p.y < 0)\n            bins[3]++;\n    }\n}",
            "// write your code here\n}",
            "bins.fill(0);\n\n    for (auto& point : points) {\n        if (point.x < 0 && point.y > 0)\n            ++bins[0];\n        else if (point.x < 0 && point.y < 0)\n            ++bins[1];\n        else if (point.x > 0 && point.y > 0)\n            ++bins[2];\n        else if (point.x > 0 && point.y < 0)\n            ++bins[3];\n    }\n}",
            "int q=1;\n    for(Point p : points){\n        if(p.x > 0 && p.y > 0)\n            bins.at(0)++;\n        if(p.x < 0 && p.y > 0)\n            bins.at(1)++;\n        if(p.x < 0 && p.y < 0)\n            bins.at(2)++;\n        if(p.x > 0 && p.y < 0)\n            bins.at(3)++;\n    }\n}",
            "for(auto const& p: points) {\n      if(p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if(p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if(p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if(p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: fill the bins array\n\n   // The bins array will be indexed as follows:\n   // 0 is for the first quadrant, 1 is for the second, etc.\n   // Therefore, bins[0] will be the number of points in the first quadrant.\n\n   // Hint: You can use the `std::make_pair()` function to initialize a `std::pair` object.\n   // This is useful to store two values in a single object.\n}",
            "for (auto const& p: points) {\n    if (p.x > 0) {\n      if (p.y > 0) bins[0]++;\n      else bins[1]++;\n    } else {\n      if (p.y > 0) bins[2]++;\n      else bins[3]++;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   for (auto const& p : points) {\n      size_t quadrant = 0;\n\n      if (p.x >= 0 && p.y >= 0) {\n         quadrant = 1;\n      } else if (p.x < 0 && p.y >= 0) {\n         quadrant = 2;\n      } else if (p.x >= 0 && p.y < 0) {\n         quadrant = 3;\n      } else if (p.x < 0 && p.y < 0) {\n         quadrant = 4;\n      }\n\n      bins[quadrant - 1]++;\n   }\n}",
            "for (const auto& point : points) {\n      size_t q = 0;\n      if (point.x < 0) {\n         if (point.y >= 0) {\n            q = 1;\n         } else {\n            q = 2;\n         }\n      } else {\n         if (point.y >= 0) {\n            q = 0;\n         } else {\n            q = 3;\n         }\n      }\n      bins[q]++;\n   }\n}",
            "// TODO: implement this function\n    // HINT:  Use the quadrant formula (see above) to fill the bins\n\n    bins = {0, 0, 0, 0};\n\n    for (Point const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0)\n            ++bins[0];\n        else if (p.x < 0 && p.y >= 0)\n            ++bins[1];\n        else if (p.x < 0 && p.y < 0)\n            ++bins[2];\n        else if (p.x >= 0 && p.y < 0)\n            ++bins[3];\n    }\n}",
            "for(auto point : points){\n      if(point.x > 0 && point.y > 0) {\n         bins[0] += 1;\n      } else if(point.x <= 0 && point.y > 0) {\n         bins[1] += 1;\n      } else if(point.x <= 0 && point.y <= 0) {\n         bins[2] += 1;\n      } else if(point.x > 0 && point.y <= 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "/* Your solution goes here  */\n}",
            "bins.fill(0);\n  for (auto &p : points) {\n    if (p.x > 0 && p.y > 0) {\n      bins[0]++;\n    } else if (p.x < 0 && p.y > 0) {\n      bins[1]++;\n    } else if (p.x < 0 && p.y < 0) {\n      bins[2]++;\n    } else if (p.x > 0 && p.y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "for (size_t i = 0; i < points.size(); i++) {\n        int x = points[i].x;\n        int y = points[i].y;\n\n        if (x < 0 && y > 0) {\n            bins[0]++;\n        }\n        if (x > 0 && y > 0) {\n            bins[1]++;\n        }\n        if (x > 0 && y < 0) {\n            bins[2]++;\n        }\n        if (x < 0 && y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "for (const auto &point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else if (point.x >= 0 && point.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement this function\n   //...\n}",
            "// Implement this function\n}",
            "for (auto &point : points) {\n      if (point.x > 0.0 && point.y > 0.0)\n         ++bins[0];\n      else if (point.x <= 0.0 && point.y > 0.0)\n         ++bins[1];\n      else if (point.x < 0.0 && point.y <= 0.0)\n         ++bins[2];\n      else if (point.x >= 0.0 && point.y < 0.0)\n         ++bins[3];\n   }\n}",
            "for (auto const& point : points) {\n        auto const x_sign = (point.x >= 0)? 0 : 1;\n        auto const y_sign = (point.y >= 0)? 0 : 1;\n        auto const quadrant = y_sign | (x_sign << 1);\n        bins[quadrant]++;\n    }\n}",
            "size_t pointsInQuadrant[4] = {};\n\n    for (const auto &p : points) {\n        if (p.x < 0 && p.y < 0) {\n            pointsInQuadrant[0]++;\n        }\n        else if (p.x > 0 && p.y > 0) {\n            pointsInQuadrant[1]++;\n        }\n        else if (p.x < 0 && p.y > 0) {\n            pointsInQuadrant[2]++;\n        }\n        else if (p.x > 0 && p.y < 0) {\n            pointsInQuadrant[3]++;\n        }\n    }\n\n    for (size_t i = 0; i < 4; i++) {\n        bins[i] = pointsInQuadrant[i];\n    }\n}",
            "for (Point& p : points) {\n    // Count each point in quadrant 0, 1, 2 or 3.\n  }\n}",
            "for(auto it = points.begin(); it!= points.end(); it++) {\n      if (it->x >= 0 && it->y >= 0) {\n         bins[0]++;\n      } else if (it->x < 0 && it->y >= 0) {\n         bins[1]++;\n      } else if (it->x < 0 && it->y < 0) {\n         bins[2]++;\n      } else if (it->x >= 0 && it->y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "size_t n = points.size();\n   for(size_t i = 0; i < n; i++) {\n      if(points[i].x >= 0 && points[i].y >= 0) {\n         bins[0]++;\n      } else if(points[i].x < 0 && points[i].y >= 0) {\n         bins[1]++;\n      } else if(points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point: points) {\n      auto signX = std::signbit(point.x)? -1 : 1;\n      auto signY = std::signbit(point.y)? -1 : 1;\n      bins[signX * signY + 1] += 1;\n   }\n}",
            "for (auto point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "/* Your code goes here */\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else if (point.x >= 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "for(const Point& point: points) {\n        bins[determineQuadrant(point)]++;\n    }\n}",
            "for(auto const& point : points)\n   {\n       if(point.x >= 0 && point.y >= 0)\n       {\n           bins[0]++;\n       }\n       else if(point.x <= 0 && point.y >= 0)\n       {\n           bins[1]++;\n       }\n       else if(point.x <= 0 && point.y <= 0)\n       {\n           bins[2]++;\n       }\n       else if(point.x >= 0 && point.y <= 0)\n       {\n           bins[3]++;\n       }\n   }\n}",
            "bins = { 0, 0, 0, 0 };\n\n    for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        }\n        if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        }\n        if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        }\n        if (point.x >= 0 && point.y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "size_t a = 0, b = 0, c = 0, d = 0;\n    for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) a++;\n        if (point.x < 0 && point.y >= 0) b++;\n        if (point.x < 0 && point.y < 0) c++;\n        if (point.x >= 0 && point.y < 0) d++;\n    }\n    bins = { a, b, c, d };\n}",
            "//...\n}",
            "for (Point p : points) {\n        bins[quadrant(p)]++;\n    }\n}",
            "size_t const QUADRANTS_COUNT = 4;\n   for(auto const& point : points) {\n      // Determine quadrant\n      if (point.x > 0 && point.y > 0) {\n         bins[0] += 1;\n      } else if (point.x <= 0 && point.y > 0) {\n         bins[1] += 1;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2] += 1;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3] += 1;\n      } else {\n         throw std::invalid_argument(\"Invalid point (\" + std::to_string(point.x) + \",\" + std::to_string(point.y) + \")\");\n      }\n   }\n}",
            "for(int i=0; i<points.size(); i++){\n        if(points[i].x > 0 && points[i].y > 0){\n            bins[0]++;\n        }\n        else if(points[i].x < 0 && points[i].y > 0){\n            bins[1]++;\n        }\n        else if(points[i].x < 0 && points[i].y < 0){\n            bins[2]++;\n        }\n        else if(points[i].x > 0 && points[i].y < 0){\n            bins[3]++;\n        }\n        else{\n            bins[0]++;\n        }\n    }\n}",
            "//TODO: Implement this\n}",
            "for (auto const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto const& pt : points) {\n        int idx;\n        if (pt.x > 0) {\n            if (pt.y > 0)\n                idx = 0;\n            else\n                idx = 1;\n        } else {\n            if (pt.y > 0)\n                idx = 2;\n            else\n                idx = 3;\n        }\n        bins[idx]++;\n    }\n}",
            "bins.fill(0);\n\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0)\n         ++bins[0];\n      else if (points[i].x < 0 && points[i].y >= 0)\n         ++bins[1];\n      else if (points[i].x < 0 && points[i].y < 0)\n         ++bins[2];\n      else if (points[i].x >= 0 && points[i].y < 0)\n         ++bins[3];\n   }\n}",
            "bins.fill(0);\n\n  for (const Point& point: points) {\n    if (point.x >= 0 && point.y >= 0) {\n      bins[0]++;\n    } else if (point.x < 0 && point.y >= 0) {\n      bins[1]++;\n    } else if (point.x < 0 && point.y < 0) {\n      bins[2]++;\n    } else if (point.x >= 0 && point.y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "// your code goes here\n}",
            "for (auto const& p: points) {\n        int quadrant = (p.x > 0) + 2*(p.y > 0);\n        bins[quadrant]++;\n    }\n}",
            "//...\n}",
            "std::array<size_t, 4> counts;\n   std::array<std::string, 4> quadrants = {\"q1\", \"q2\", \"q3\", \"q4\"};\n\n   for (int i = 0; i < quadrants.size(); i++) {\n      counts[i] = 0;\n   }\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         counts[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         counts[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         counts[2]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         counts[3]++;\n      }\n   }\n\n   std::cout << \"Quadrants: \" << std::endl;\n   std::cout << \"q1: \" << counts[0] << std::endl;\n   std::cout << \"q2: \" << counts[1] << std::endl;\n   std::cout << \"q3: \" << counts[2] << std::endl;\n   std::cout << \"q4: \" << counts[3] << std::endl;\n   bins = counts;\n}",
            "// This should be a simple loop.\n   // Note that the quadrants are counted clockwise starting with (0, 0) at the top-left.\n   // Use the standard trigonometric functions to compute the quadrant.\n   // Use the bins vector to store the counts in.\n   // You can assume the points vector is not empty, and that x and y are always greater than or equal to zero.\n}",
            "}",
            "for (auto const& point : points) {\n    int x = point.x > 0? 0 : 1;\n    int y = point.y > 0? 2 : 3;\n    bins[x + y]++;\n  }\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n   for (auto const& p: points) {\n      if (p.x > 0 && p.y > 0) bins[0]++;\n      else if (p.x < 0 && p.y > 0) bins[1]++;\n      else if (p.x < 0 && p.y < 0) bins[2]++;\n      else if (p.x > 0 && p.y < 0) bins[3]++;\n   }\n}",
            "bins.fill(0);\n    for (auto const& pt : points) {\n        if (pt.x > 0) {\n            if (pt.y > 0) {\n                ++bins[0];\n            } else {\n                ++bins[1];\n            }\n        } else {\n            if (pt.y > 0) {\n                ++bins[2];\n            } else {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "for(auto const& p : points) {\n        bins[getQuadrant(p)]++;\n    }\n}",
            "for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for(auto p : points) {\n      if(p.x > 0 && p.y > 0)\n         bins[0]++;\n      else if(p.x < 0 && p.y > 0)\n         bins[1]++;\n      else if(p.x < 0 && p.y < 0)\n         bins[2]++;\n      else if(p.x > 0 && p.y < 0)\n         bins[3]++;\n   }\n}",
            "// TODO: Implement the function\n\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (size_t i = 0; i < points.size(); ++i) {\n      int q = 0;\n      if (points[i].x < 0 && points[i].y < 0) {\n         q = 1;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         q = 2;\n      } else if (points[i].x > 0 && points[i].y > 0) {\n         q = 3;\n      }\n\n      bins[q] = bins[q] + 1;\n   }\n}",
            "for (auto const& p : points) {\n        size_t const q = (p.x > 0) + 2 * (p.y > 0);\n        bins[q] += 1;\n    }\n}",
            "for (auto const& p : points) {\n        auto q = p.x >= 0? (p.y >= 0? 1 : 2) : (p.y >= 0? 3 : 4);\n        bins[q - 1]++;\n    }\n\n}",
            "for (auto& point : points) {\n        size_t const quadrant = (point.x >= 0) + (point.y >= 0) * 2;\n        ++bins[quadrant];\n    }\n}",
            "// TODO: Add your solution here\n}",
            "// TODO: Count the number of cartesian points in each quadrant.\n    //       Store the counts in `bins`.\n}",
            "size_t i = 0;\n    for (Point p : points) {\n        if (p.x >= 0 && p.y >= 0) bins[i]++;\n        else if (p.x <= 0 && p.y >= 0) bins[i + 1]++;\n        else if (p.x <= 0 && p.y <= 0) bins[i + 2]++;\n        else if (p.x >= 0 && p.y <= 0) bins[i + 3]++;\n    }\n}",
            "for(size_t i = 0; i < points.size(); ++i) {\n      if(points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      }\n      else if(points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      }\n      else if(points[i].x > 0 && points[i].y < 0) {\n         bins[2]++;\n      }\n      else if(points[i].x < 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n    for (auto const &p: points) {\n        size_t quadrant = p.x > 0 && p.y > 0? 0 :\n                          p.x > 0 && p.y < 0? 1 :\n                          p.x < 0 && p.y < 0? 2 : 3;\n        ++bins[quadrant];\n    }\n}",
            "//...\n}",
            "// TODO: Add implementation here\n\n}",
            "for (auto &p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x > 0 && p.y < 0) {\n         ++bins[2];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "for (const auto &point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "for (const auto& point : points) {\n        int quadrant = 0;\n\n        // The quadrant for the point is determined by the sign of the\n        // cartesian coordinates, not the direction of the point.\n        if (point.x < 0) {\n            quadrant |= 1;\n        }\n\n        if (point.y < 0) {\n            quadrant |= 2;\n        }\n\n        ++bins[quadrant];\n    }\n}",
            "for (auto const& point : points) {\n      bins[0] += (point.x >= 0) && (point.y >= 0);\n      bins[1] += (point.x < 0) && (point.y >= 0);\n      bins[2] += (point.x < 0) && (point.y < 0);\n      bins[3] += (point.x >= 0) && (point.y < 0);\n   }\n}",
            "// Add your code here\n\n   for(auto &elem: points){\n       if (elem.x < 0 && elem.y > 0){\n           bins[0]++;\n       } else if (elem.x > 0 && elem.y > 0){\n           bins[1]++;\n       } else if (elem.x < 0 && elem.y < 0){\n           bins[2]++;\n       } else {\n           bins[3]++;\n       }\n   }\n}",
            "// TODO: Implement\n}",
            "for (auto const& point : points) {\n      int x_bin = point.x > 0? 0 : (point.x < 0? 1 : 2);\n      int y_bin = point.y > 0? 0 : (point.y < 0? 1 : 2);\n      int bin = x_bin + y_bin*2;\n      ++bins[bin];\n   }\n}",
            "for(const auto & p : points) {\n        if(p.x < 0) {\n            if(p.y < 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if(p.y < 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "}",
            "//...\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (const auto& pt : points) {\n      int q = 0;\n      if (pt.x >= 0 && pt.y >= 0) {\n         q = 1;\n      } else if (pt.x >= 0 && pt.y < 0) {\n         q = 2;\n      } else if (pt.x < 0 && pt.y >= 0) {\n         q = 3;\n      }\n      bins[q]++;\n   }\n}",
            "}",
            "// Your code goes here\n}",
            "bins.fill(0);\n   for (const auto& p : points) {\n      ++bins[(p.x >= 0) + (p.y >= 0) * 2];\n   }\n}",
            "for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         bins[0]++;\n      } else if (x < 0 && y >= 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (size_t i = 0; i < points.size(); ++i) {\n    if (points[i].x > 0) {\n      if (points[i].y > 0)\n        bins[0]++;\n      else if (points[i].y < 0)\n        bins[2]++;\n    } else if (points[i].x < 0) {\n      if (points[i].y > 0)\n        bins[1]++;\n      else if (points[i].y < 0)\n        bins[3]++;\n    }\n  }\n}",
            "for (auto const& point : points)\n    {\n        if (point.x >= 0 && point.y >= 0) bins[0]++;\n        else if (point.x <= 0 && point.y >= 0) bins[1]++;\n        else if (point.x >= 0 && point.y <= 0) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "for (Point const& point : points) {\n        size_t i = 0;\n        if (point.x > 0 && point.y > 0) {\n            i = 0;\n        } else if (point.x < 0 && point.y > 0) {\n            i = 1;\n        } else if (point.x < 0 && point.y < 0) {\n            i = 2;\n        } else if (point.x > 0 && point.y < 0) {\n            i = 3;\n        }\n        bins.at(i)++;\n    }\n}",
            "// TODO: Implement this\n}",
            "bins.fill(0);\n    for (auto const &point : points) {\n        auto &bin = bins[std::make_tuple(point.x >= 0, point.y >= 0)];\n        bin++;\n    }\n}",
            "for (auto i = points.begin(); i!= points.end(); ++i) {\n        double x = i->x;\n        double y = i->y;\n        if (x < 0 && y < 0) bins[0]++;\n        else if (x < 0 && y > 0) bins[1]++;\n        else if (x > 0 && y > 0) bins[2]++;\n        else if (x > 0 && y < 0) bins[3]++;\n    }\n\n}",
            "bins = std::array<size_t, 4>{};\n   for (auto const &point : points) {\n      if (point.x >= 0 && point.y >= 0)\n         ++bins[0];\n      else if (point.x < 0 && point.y >= 0)\n         ++bins[1];\n      else if (point.x < 0 && point.y < 0)\n         ++bins[2];\n      else if (point.x >= 0 && point.y < 0)\n         ++bins[3];\n   }\n}",
            "// Fill the bins array with 0s\n    bins.fill(0);\n    for (auto p : points) {\n        int bin = 0;\n        if (p.x > 0 && p.y > 0) {\n            bin = 1;\n        }\n        else if (p.x < 0 && p.y > 0) {\n            bin = 2;\n        }\n        else if (p.x < 0 && p.y < 0) {\n            bin = 3;\n        }\n        bins[bin]++;\n    }\n}",
            "for (auto point : points) {\n        if (point.x < 0.0 && point.y < 0.0) {\n            bins[0]++;\n        } else if (point.x >= 0.0 && point.y < 0.0) {\n            bins[1]++;\n        } else if (point.x < 0.0 && point.y >= 0.0) {\n            bins[2]++;\n        } else if (point.x >= 0.0 && point.y >= 0.0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Add your code here\n}",
            "bins.fill(0);\n    for (auto const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        } else if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        } else if (point.x >= 0 && point.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "for (auto const &p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "size_t count[4] = { 0, 0, 0, 0 };\n    for(const auto &point : points) {\n        size_t x = (point.x > 0) + (point.x <= 0);\n        size_t y = (point.y > 0) + (point.y <= 0);\n        size_t quadrant = 2*x + y;\n        count[quadrant]++;\n    }\n    bins = count;\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= N) return;\n\n   int q = (points[i].x > 0) - (points[i].y > 0) + 2 * (points[i].x > 0 && points[i].y > 0);\n   atomicAdd(bins + q, 1);\n}",
            "const size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N) {\n      const auto &p = points[index];\n      const size_t x = (p.x > 0)? 0 : (p.x < 0)? 1 : 2;\n      const size_t y = (p.y > 0)? 0 : (p.y < 0)? 1 : 2;\n      atomicAdd(&bins[x + y * 3], 1);\n   }\n}",
            "// count the number of points in each quadrant\n    // each thread works on one point\n    size_t i = blockDim.x*blockIdx.x + threadIdx.x;\n    if(i < N) {\n        double x = points[i].x;\n        double y = points[i].y;\n        size_t bin;\n        if(x >= 0.0) {\n            bin = y >= 0.0? 0 : 2;\n        } else {\n            bin = y >= 0.0? 1 : 3;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) {\n        return;\n    }\n\n    const double x = points[idx].x;\n    const double y = points[idx].y;\n    if (x > 0 && y > 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (x < 0 && y > 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (x < 0 && y < 0) {\n        atomicAdd(&bins[2], 1);\n    } else if (x > 0 && y < 0) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n        if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n        if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n        if (points[i].x >= 0 && points[i].y < 0) bins[3]++;\n    }\n}",
            "size_t i = threadIdx.x;\n\n  if(i < N) {\n    size_t bin = 0;\n    if(points[i].x > 0) {\n      if(points[i].y > 0) {\n        bin = 1;\n      } else {\n        bin = 2;\n      }\n    } else {\n      if(points[i].y > 0) {\n        bin = 3;\n      } else {\n        bin = 0;\n      }\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Get a thread id\n   size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n\n   // Get the point\n   const Point& p = points[tid];\n   // Count the point in the corresponding bin\n   if (p.x > 0 && p.y > 0) {\n      atomicAdd(bins+0, 1);\n   } else if (p.x < 0 && p.y > 0) {\n      atomicAdd(bins+1, 1);\n   } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(bins+2, 1);\n   } else if (p.x > 0 && p.y < 0) {\n      atomicAdd(bins+3, 1);\n   }\n}",
            "// get the index of the thread\n   size_t i = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n   if (i >= N) return;\n   Point p = points[i];\n\n   // which quadrant is the point in?\n   int q = -1;\n   if (p.x >= 0 && p.y >= 0) {\n      q = 0;\n   } else if (p.x < 0 && p.y >= 0) {\n      q = 1;\n   } else if (p.x < 0 && p.y < 0) {\n      q = 2;\n   } else if (p.x >= 0 && p.y < 0) {\n      q = 3;\n   }\n   // atomic increment the bin for this quadrant\n   if (q >= 0) {\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "for (int i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x > 0.0 && points[i].y > 0.0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[i].x <= 0.0 && points[i].y > 0.0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[i].x > 0.0 && points[i].y <= 0.0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[i].x <= 0.0 && points[i].y <= 0.0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (i < N) {\n      size_t x = (points[i].x < 0.0)? 0 : 1;\n      size_t y = (points[i].y < 0.0)? 0 : 2;\n      atomicAdd(&bins[x + y], 1);\n   }\n}",
            "size_t idx = threadIdx.x;\n    while(idx < N) {\n        if (points[idx].x > 0 && points[idx].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[idx].x < 0 && points[idx].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[idx].x < 0 && points[idx].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n        idx += blockDim.x;\n    }\n}",
            "size_t gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n   if (gid >= N) return;\n\n   Point p = points[gid];\n\n   size_t bin_idx = (p.x < 0) + 2*(p.y < 0);\n   atomicAdd(&bins[bin_idx], 1);\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   int quadrant = 0;\n   if (points[i].x > 0) quadrant += 1;\n   if (points[i].y > 0) quadrant += 2;\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "// TODO\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (idx >= N) {\n      return;\n   }\n\n   if (points[idx].x >= 0 && points[idx].y >= 0) {\n      atomicAdd(bins + 0, 1);\n   } else if (points[idx].x < 0 && points[idx].y >= 0) {\n      atomicAdd(bins + 1, 1);\n   } else if (points[idx].x < 0 && points[idx].y < 0) {\n      atomicAdd(bins + 2, 1);\n   } else {\n      atomicAdd(bins + 3, 1);\n   }\n}",
            "int i = threadIdx.x;\n   // each thread handles one point\n   if (i < N) {\n      int quadrant;\n      if (points[i].x > 0 && points[i].y > 0) {\n         quadrant = 0;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         quadrant = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         quadrant = 2;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         quadrant = 3;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = 0;\n      if (points[i].x > 0.0) q += 1;\n      if (points[i].y > 0.0) q += 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// TODO\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if (index < N) {\n      Point p = points[index];\n\n      // Determine the quadrant using the sign of x and y\n      int q = (p.x >= 0) * 2 + (p.y >= 0);\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// Your code here\n   // For bonus points: \n   // 1) Use shared memory to reduce contention\n   // 2) Use a reduction to aggregate the data in fewer steps\n   int index = (int) (blockIdx.x * blockDim.x + threadIdx.x);\n\n   if (index < N) {\n      if (points[index].x > 0 && points[index].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[index].x < 0 && points[index].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[index].x > 0 && points[index].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[index].x < 0 && points[index].y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Write code here\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid >= N) {\n     return;\n   }\n   size_t bin;\n   double x = points[tid].x;\n   double y = points[tid].y;\n   if (x < 0) {\n      if (y < 0) {\n         bin = 2;\n      } else {\n         bin = 3;\n      }\n   } else {\n      if (y < 0) {\n         bin = 1;\n      } else {\n         bin = 0;\n      }\n   }\n   atomicAdd(bins + bin, 1);\n}",
            "// Your code goes here\n}",
            "// Count how many points are in each quadrant.\n  // Each thread is given a point.\n\n  __shared__ size_t quadrants[4];\n\n  size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (index < N) {\n    if (points[index].x > 0 && points[index].y > 0) {\n      atomicAdd(&quadrants[0], 1);\n    } else if (points[index].x < 0 && points[index].y > 0) {\n      atomicAdd(&quadrants[1], 1);\n    } else if (points[index].x > 0 && points[index].y < 0) {\n      atomicAdd(&quadrants[2], 1);\n    } else if (points[index].x < 0 && points[index].y < 0) {\n      atomicAdd(&quadrants[3], 1);\n    }\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = quadrants[i];\n    }\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (idx < N) {\n      double x = points[idx].x;\n      double y = points[idx].y;\n      size_t bin = 0;\n\n      if (x < 0) {\n         if (y < 0) {\n            bin = 2;\n         } else {\n            bin = 3;\n         }\n      } else {\n         if (y < 0) {\n            bin = 1;\n         }\n      }\n\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if (tid < N) {\n      int quadrant = 0;\n\n      if (points[tid].x > 0 && points[tid].y > 0) {\n         quadrant = 1;\n      } else if (points[tid].x < 0 && points[tid].y > 0) {\n         quadrant = 2;\n      } else if (points[tid].x < 0 && points[tid].y < 0) {\n         quadrant = 3;\n      }\n\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO\n  int i = threadIdx.x;\n  if (i<N) {\n    Point p = points[i];\n    int quadrant = ((p.x > 0) << 1) | ((p.y > 0) << 0);\n    atomicAdd(&(bins[quadrant]), 1);\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx >= N) return;\n  double x = points[idx].x;\n  double y = points[idx].y;\n  // add code to calculate the quadrant for this point\n}",
            "int idx = hipBlockIdx_x*hipBlockDim_x + hipThreadIdx_x;\n\n    if (idx >= N) return;\n\n    double x = points[idx].x;\n    double y = points[idx].y;\n\n    size_t q = 0;\n\n    if (x >= 0 && y >= 0) q = 0;\n    if (x < 0 && y >= 0) q = 1;\n    if (x < 0 && y < 0) q = 2;\n    if (x >= 0 && y < 0) q = 3;\n\n    atomicAdd(&bins[q], 1);\n}",
            "// TODO\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n    else if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n    else if (points[i].x >= 0 && points[i].y < 0) bins[2]++;\n    else if (points[i].x < 0 && points[i].y < 0) bins[3]++;\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        int q = (points[i].x > 0)? (points[i].y > 0? 0 : 3) : (points[i].y > 0? 1 : 2);\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  // For each point, count the number of points in the respective quadrant\n  if (tid < N) {\n    if (points[tid].x > 0.0 && points[tid].y > 0.0)\n      atomicAdd(&bins[0], 1);\n    else if (points[tid].x <= 0.0 && points[tid].y > 0.0)\n      atomicAdd(&bins[1], 1);\n    else if (points[tid].x <= 0.0 && points[tid].y <= 0.0)\n      atomicAdd(&bins[2], 1);\n    else if (points[tid].x > 0.0 && points[tid].y <= 0.0)\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO\n}",
            "// A thread's work is 1 point\n   const size_t index = threadIdx.x;\n   if (index >= N) return;\n   const auto p = points[index];\n   const int q = (p.x > 0) + 2 * (p.y > 0);\n   atomicAdd(&bins[q], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        Point p = points[i];\n        size_t q = (p.x > 0) + (p.y > 0) * 2;\n        atomicAdd(bins + q, 1);\n    }\n}",
            "const size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (thread_id >= N)\n      return;\n\n   const Point &p = points[thread_id];\n\n   if (p.x >= 0 && p.y >= 0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0 && p.y >= 0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0 && p.y < 0)\n      atomicAdd(&bins[2], 1);\n   else if (p.x >= 0 && p.y < 0)\n      atomicAdd(&bins[3], 1);\n}",
            "/* Compute the global thread index. */\n   const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   /* Check if the thread index is valid. */\n   if (idx < N) {\n\n      /* Compute the quadrant index. */\n      const int i = (points[idx].x >= 0.0)? (points[idx].y >= 0.0? 0 : 1) : (points[idx].y >= 0.0? 2 : 3);\n\n      /* Atomically increment the quadrant counter. */\n      atomicAdd(&bins[i], 1);\n   }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        int bin = (points[idx].x > 0)? (points[idx].y > 0? 0 : 3) : (points[idx].y > 0? 1 : 2);\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "size_t index = threadIdx.x + blockDim.x * blockIdx.x;\n\n   if (index >= N) return;\n\n   int q;\n   double x = points[index].x;\n   double y = points[index].y;\n\n   if (x >= 0 && y >= 0) {\n      q = 0;\n   } else if (x < 0 && y >= 0) {\n      q = 1;\n   } else if (x < 0 && y < 0) {\n      q = 2;\n   } else if (x >= 0 && y < 0) {\n      q = 3;\n   } else {\n      // Can't happen, but compiler doesn't know that.\n      q = 0;\n   }\n\n   atomicAdd(&bins[q], 1);\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (idx < N) {\n      int q;\n      if (points[idx].x >= 0) {\n         q = points[idx].y >= 0? 1 : 2;\n      } else {\n         q = points[idx].y >= 0? 0 : 3;\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// This thread is responsible for processing elements\n   // from the Nth element to the (N + blockDim.x)th element\n   // of the points array\n   // where N is a multiple of blockDim.x\n   for (auto i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x) {\n      auto x = points[i].x;\n      auto y = points[i].y;\n\n      if (x > 0) {\n         if (y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// Find the quadrant of this point.\n   // The range of x values is [0, 10) and the range of y values is [0, 10).\n   const int q = (x >= 5) + (y >= 5) * 2;\n\n   // Increment the count in the corresponding bin.\n   atomicAdd(&bins[q], 1);\n}",
            "// 1. Initialize bins to zero\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // 2. Determine which quadrant each point belongs to\n    //    If x < 0 and y < 0, it belongs in the 1st quadrant\n    //    If x < 0 and y >= 0, it belongs in the 2nd quadrant\n    //    If x >= 0 and y < 0, it belongs in the 3rd quadrant\n    //    If x >= 0 and y >= 0, it belongs in the 4th quadrant\n\n    // 3. Increment the appropriate bin\n\n    // 4. Increment the appropriate bin\n\n    // 5. Increment the appropriate bin\n\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N) return;\n\n    Point p = points[tid];\n    size_t bin;\n    if (p.x >= 0 && p.y >= 0) {\n        bin = 0;\n    } else if (p.x < 0 && p.y >= 0) {\n        bin = 1;\n    } else if (p.x < 0 && p.y < 0) {\n        bin = 2;\n    } else {\n        bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "const size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (id < N) {\n      int quadrant = 0;\n      if (points[id].x < 0) quadrant |= 1;\n      if (points[id].y < 0) quadrant |= 2;\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "size_t i = threadIdx.x;\n  while(i < N){\n    if(points[i].x >= 0) {\n      if(points[i].y >= 0)\n        atomicAdd(&bins[0], 1);\n      else\n        atomicAdd(&bins[1], 1);\n    }\n    else {\n      if(points[i].y >= 0)\n        atomicAdd(&bins[2], 1);\n      else\n        atomicAdd(&bins[3], 1);\n    }\n    i += blockDim.x;\n  }\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = gridDim.x * blockDim.x;\n\n  for (size_t i = gid; i < N; i += stride) {\n    size_t bin = 0;\n\n    if (points[i].x >= 0.0) {\n      if (points[i].y >= 0.0) {\n        bin = 0;\n      } else {\n        bin = 1;\n      }\n    } else {\n      if (points[i].y >= 0.0) {\n        bin = 2;\n      } else {\n        bin = 3;\n      }\n    }\n\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (index >= N)\n      return;\n\n   // determine the quadrant for this point\n   int q;\n   if (points[index].x >= 0 && points[index].y >= 0)\n      q = 0;\n   else if (points[index].x < 0 && points[index].y >= 0)\n      q = 1;\n   else if (points[index].x < 0 && points[index].y < 0)\n      q = 2;\n   else if (points[index].x >= 0 && points[index].y < 0)\n      q = 3;\n\n   // atomically increment the count for this quadrant\n   atomicAdd(&(bins[q]), 1);\n}",
            "const int id = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (id >= N)\n      return;\n\n   const int x = points[id].x;\n   const int y = points[id].y;\n   // TODO: Update bins in accordance with quadrant the point is in\n}",
            "/* Store counts in bins */\n   //...\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    Point p = points[idx];\n    int q = (p.x > 0)? (p.y > 0? 1 : 4) : (p.y > 0? 2 : 3);\n    atomicAdd(bins + q, 1);\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if (tid < N) {\n\n      if (points[tid].x >= 0 && points[tid].y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[tid].x < 0 && points[tid].y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[tid].x < 0 && points[tid].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Count the number of points in the quadrant of the current thread.\n   // Then, add the count to the corresponding bin.\n   // Use atomicAdd to avoid race conditions.\n   int bin = (threadIdx.x / (blockDim.x / 4)) % 4;\n   int count = 0;\n\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         count++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         count++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         count++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         count++;\n      }\n   }\n\n   atomicAdd(&(bins[bin]), count);\n}",
            "//...\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n  Point p = points[tid];\n  if (p.x > 0 && p.y > 0) {\n    atomicAdd(bins + 0, 1);\n  } else if (p.x < 0 && p.y > 0) {\n    atomicAdd(bins + 1, 1);\n  } else if (p.x < 0 && p.y < 0) {\n    atomicAdd(bins + 2, 1);\n  } else if (p.x > 0 && p.y < 0) {\n    atomicAdd(bins + 3, 1);\n  }\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      const Point &p = points[i];\n      const bool in1 = p.x >= 0 && p.y >= 0;\n      const bool in2 = p.x < 0 && p.y >= 0;\n      const bool in3 = p.x >= 0 && p.y < 0;\n      const bool in4 = p.x < 0 && p.y < 0;\n\n      if (in1)\n         atomicAdd(&bins[0], 1);\n      else if (in2)\n         atomicAdd(&bins[1], 1);\n      else if (in3)\n         atomicAdd(&bins[2], 1);\n      else if (in4)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// TODO: fill this in\n}",
            "size_t idx = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (idx < N) {\n      Point p = points[idx];\n      if (p.x > 0) {\n         if (p.y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (p.y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  // Add a check if the index i is inside the range N and the value at points[i].y is not infinity.\n  // If true, add the appropriate point to the appropriate bin in the output array bins.\n  if (i >= N || points[i].y == INFINITY) {\n    return;\n  }\n  if (points[i].x < 0 && points[i].y < 0) {\n    atomicAdd(&bins[0], 1);\n  } else if (points[i].x < 0 && points[i].y >= 0) {\n    atomicAdd(&bins[1], 1);\n  } else if (points[i].x >= 0 && points[i].y < 0) {\n    atomicAdd(&bins[2], 1);\n  } else if (points[i].x >= 0 && points[i].y >= 0) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "const size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (thread_id < N) {\n        Point p = points[thread_id];\n        if (p.x >= 0) {\n            if (p.y >= 0)\n                atomicAdd(&(bins[0]), 1);\n            else\n                atomicAdd(&(bins[2]), 1);\n        }\n        else {\n            if (p.y >= 0)\n                atomicAdd(&(bins[1]), 1);\n            else\n                atomicAdd(&(bins[3]), 1);\n        }\n    }\n}",
            "// TODO: count the number of points in each quadrant and store in bins\n}",
            "// TODO\n}",
            "// Write your kernel code here...\n}",
            "const int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n   if (tid < N) {\n      Point point = points[tid];\n      int bin = 0;\n\n      if (point.x >= 0) {\n         bin |= 1 << 0;\n         if (point.y >= 0) {\n            bin |= 1 << 1;\n         }\n      } else {\n         bin |= 1 << 2;\n         if (point.y >= 0) {\n            bin |= 1 << 3;\n         }\n      }\n\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         atomicAdd(bins, 0, 1);\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         atomicAdd(bins, 1, 1);\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         atomicAdd(bins, 2, 1);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         atomicAdd(bins, 3, 1);\n      }\n   }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    int q = 0;\n    if (points[i].x < 0) q += 1;\n    if (points[i].y < 0) q += 2;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "int i = threadIdx.x;\n   if (i < N) {\n      int q = (points[i].x >= 0)? 0 : 1;\n      q += (points[i].y >= 0)? 0 : 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// Write your code here\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid >= N) return;\n   Point p = points[tid];\n   int q;\n   if (p.x > 0 && p.y > 0) q = 0;\n   else if (p.x < 0 && p.y > 0) q = 1;\n   else if (p.x < 0 && p.y < 0) q = 2;\n   else q = 3;\n   atomicAdd(&bins[q], 1);\n}",
            "int bin_index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (bin_index >= N) return;\n\n   Point p = points[bin_index];\n   int bin = 0;\n   if (p.x < 0) bin += 1;\n   if (p.y < 0) bin += 2;\n\n   atomicAdd(&bins[bin], 1);\n}",
            "//...\n}",
            "// Compute the number of points in each quadrant.\n    size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i >= N) return;\n\n    int q = (points[i].x > 0 && points[i].y > 0)? 0 :\n            (points[i].x < 0 && points[i].y > 0)? 1 :\n            (points[i].x < 0 && points[i].y < 0)? 2 : 3;\n    atomicAdd(&bins[q], 1);\n}",
            "size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n   if (id >= N) return;\n   if (points[id].x > 0 && points[id].y > 0) atomicAdd(&bins[0], 1);\n   if (points[id].x < 0 && points[id].y > 0) atomicAdd(&bins[1], 1);\n   if (points[id].x < 0 && points[id].y < 0) atomicAdd(&bins[2], 1);\n   if (points[id].x > 0 && points[id].y < 0) atomicAdd(&bins[3], 1);\n}",
            "// TODO: implement me!\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N)\n    return;\n\n  bins[2 * (points[tid].x < 0) + (points[tid].y < 0)] += 1;\n}",
            "size_t globalThreadIndex = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (globalThreadIndex >= N) return;\n   size_t myBin = 0;\n   const Point &p = points[globalThreadIndex];\n   if (p.x > 0) {\n      myBin += (p.y > 0)? 1 : 2;\n   } else {\n      myBin += (p.y > 0)? 3 : 0;\n   }\n   atomicAdd(&bins[myBin], 1);\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    Point p = points[i];\n\n    if (p.x > 0 && p.y > 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (p.x < 0 && p.y > 0) {\n      atomicAdd(&bins[1], 1);\n    } else if (p.x > 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// Determine which quadrant this thread should handle.\n    const size_t offset = (blockIdx.x * blockDim.x) + threadIdx.x;\n    if (offset < N) {\n        const Point& p = points[offset];\n        int quadrant = 0;\n        if (p.x >= 0) {\n            quadrant += 1;\n            if (p.y >= 0) {\n                quadrant += 1;\n            }\n        }\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // out of range?\n    if (index >= N) {\n        return;\n    }\n\n    // count points in quadrants 1-4.\n    int quad = 0;\n\n    if (points[index].x >= 0) {\n        if (points[index].y >= 0) {\n            quad = 1;\n        } else {\n            quad = 2;\n        }\n    } else {\n        if (points[index].y >= 0) {\n            quad = 3;\n        } else {\n            quad = 4;\n        }\n    }\n\n    atomicAdd(&bins[quad], 1);\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) return;\n   Point point = points[idx];\n   size_t bin;\n   if (point.x < 0) {\n      if (point.y < 0) bin = 3;\n      else bin = 2;\n   } else {\n      if (point.y < 0) bin = 0;\n      else bin = 1;\n   }\n   atomicAdd(&bins[bin], 1);\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) { return; }\n   Point p = points[idx];\n   if (p.x > 0 && p.y > 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x < 0 && p.y > 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x > 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   size_t b = (points[i].x > 0 && points[i].y > 0)? 0 :\n              (points[i].x < 0 && points[i].y > 0)? 1 :\n              (points[i].x < 0 && points[i].y < 0)? 2 : 3;\n\n   atomicAdd(&bins[b], 1);\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        int quad = 0;\n        if (points[i].x < 0) {\n            quad += 1;\n        }\n        if (points[i].y < 0) {\n            quad += 2;\n        }\n        atomicAdd(&bins[quad], 1);\n    }\n}",
            "const size_t index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (index < N) {\n      const Point p = points[index];\n      if (p.x >= 0 && p.y >= 0) bins[0]++;\n      else if (p.x < 0 && p.y >= 0) bins[1]++;\n      else if (p.x < 0 && p.y < 0) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "size_t gid = blockIdx.x*blockDim.x + threadIdx.x;\n   size_t i = gid;\n\n   if (i < N) {\n      int quadrant = 0;\n\n      if (points[i].x < 0) {\n         quadrant += 1;\n      }\n      if (points[i].y < 0) {\n         quadrant += 2;\n      }\n\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    if (points[i].x > 0) {\n      if (points[i].y > 0) {\n        atomicAdd(&bins[0], 1);\n      } else {\n        atomicAdd(&bins[1], 1);\n      }\n    } else {\n      if (points[i].y > 0) {\n        atomicAdd(&bins[2], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    }\n  }\n}",
            "size_t thread_id = threadIdx.x + blockIdx.x*blockDim.x;\n    if (thread_id >= N) return;\n\n    const Point &p = points[thread_id];\n    int q = -1;\n    if (p.x >= 0) {\n        if (p.y >= 0) q = 0;\n        else q = 1;\n    } else {\n        if (p.y >= 0) q = 2;\n        else q = 3;\n    }\n\n    atomicAdd(bins + q, 1);\n}",
            "int bin = -1;\n   if (threadIdx.x < N) {\n      Point p = points[threadIdx.x];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            bin = 0;\n         } else {\n            bin = 1;\n         }\n      } else {\n         if (p.y >= 0) {\n            bin = 3;\n         } else {\n            bin = 2;\n         }\n      }\n   }\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t t = threadIdx.x;\n    if (t >= N)\n        return;\n\n    size_t bin;\n    if (points[t].x >= 0) {\n        bin = (points[t].y >= 0)? 0 : 1;\n    } else {\n        bin = (points[t].y >= 0)? 2 : 3;\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "// Define a shared memory array to hold points in the current block\n  // Each thread is responsible for a point\n  extern __shared__ Point block[];\n\n  // Load the current block into the shared memory\n  size_t blockIdx = blockIdx.x;\n  size_t i = blockIdx * blockDim.x + threadIdx.x;\n  block[threadIdx.x] = points[i];\n\n  // Wait for all threads in this block to finish loading data\n  __syncthreads();\n\n  // Iterate over all points in this block\n  for (size_t i=0; i < blockDim.x; ++i) {\n    Point& point = block[i];\n    int quadrant;\n    if (point.x < 0) {\n      if (point.y < 0) {\n        quadrant = 0;\n      } else {\n        quadrant = 1;\n      }\n    } else {\n      if (point.y < 0) {\n        quadrant = 2;\n      } else {\n        quadrant = 3;\n      }\n    }\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "size_t index = blockIdx.x*blockDim.x + threadIdx.x;\n   if (index < N) {\n      if (points[index].x >= 0 && points[index].y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (points[index].x < 0 && points[index].y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (points[index].x < 0 && points[index].y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        if (points[idx].x > 0 && points[idx].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[idx].x < 0 && points[idx].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[idx].x < 0 && points[idx].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (points[idx].x > 0 && points[idx].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n\n  // Your code here...\n\n}",
            "// Your code here.\n}",
            "int id = blockDim.x * blockIdx.x + threadIdx.x;\n   if (id >= N) return;\n\n   size_t bin = 0;\n\n   if (points[id].x >= 0) {\n      if (points[id].y >= 0) {\n         bin = 0;\n      } else {\n         bin = 3;\n      }\n   } else {\n      if (points[id].y >= 0) {\n         bin = 1;\n      } else {\n         bin = 2;\n      }\n   }\n\n   atomicAdd(&bins[bin], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) {\n        return;\n    }\n    Point p = points[tid];\n    if (p.x < 0 && p.y < 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (p.x >= 0 && p.y >= 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (p.x < 0 && p.y >= 0) {\n        atomicAdd(&bins[2], 1);\n    } else if (p.x >= 0 && p.y < 0) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      int q = 0;\n      if (points[i].x > 0 && points[i].y > 0)\n         q = 0;\n      else if (points[i].x < 0 && points[i].y > 0)\n         q = 1;\n      else if (points[i].x < 0 && points[i].y < 0)\n         q = 2;\n      else\n         q = 3;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// find the index of this thread in the array\n   int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // make sure thread doesn't go past the number of points\n   if (i >= N) return;\n\n   // find the quadrant of the point at index i\n   int q;\n   if (points[i].x < 0 && points[i].y < 0) {\n      q = 0;\n   } else if (points[i].x >= 0 && points[i].y < 0) {\n      q = 1;\n   } else if (points[i].x >= 0 && points[i].y >= 0) {\n      q = 2;\n   } else {\n      q = 3;\n   }\n\n   // increment the count of points in this quadrant\n   atomicAdd(&bins[q], 1);\n}",
            "for (size_t i=threadIdx.x; i<N; i+=blockDim.x) {\n      size_t q = points[i].x>=0 && points[i].y>=0? 0 : 0;\n         q += points[i].x<0 && points[i].y>=0? 1 : 0;\n         q += points[i].x<0 && points[i].y<0? 2 : 0;\n         q += points[i].x>=0 && points[i].y<0? 3 : 0;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) return;\n   int q = (points[idx].x > 0) + (points[idx].y > 0) * 2;\n   atomicAdd(bins + q, 1);\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (i < N) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (p.x >= 0 && p.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n}",
            "// Get the cartesian quadrant of each point in global memory.\n    // Store the results in shared memory.\n\n    // Get the cartesian quadrant of each point in shared memory.\n    // Store the results in global memory.\n\n    // Increment the appropriate bin by the number of points in shared memory.\n    // Use atomics or reduce.\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    size_t q = (points[idx].x < 0? 1 : 0) + (points[idx].y < 0? 2 : 0);\n\n    atomicAdd(&bins[q], 1);\n}",
            "auto i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int q = points[i].x > 0? 0 : 2;\n      if (points[i].y > 0)\n         q += 1;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// TODO: Implement\n  // For now, use a for loop instead of a parallel for loop\n}",
            "const size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (idx < N) {\n        Point point = points[idx];\n        if (point.x >= 0 && point.y >= 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (point.x < 0 && point.y >= 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (point.x >= 0 && point.y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        else if (point.x < 0 && point.y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Fill this in to count the number of points in each quadrant.\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      int xquad = (x >= 0) - (x < 0);\n      int yquad = (y >= 0) - (y < 0);\n      atomicAdd(&bins[xquad + 2*yquad], 1);\n   }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index >= N)\n      return;\n\n   Point p = points[index];\n\n   int q;\n\n   if (p.x > 0 && p.y > 0) {\n      q = 0;\n   } else if (p.x < 0 && p.y > 0) {\n      q = 1;\n   } else if (p.x < 0 && p.y < 0) {\n      q = 2;\n   } else {\n      q = 3;\n   }\n\n   atomicAdd(&bins[q], 1);\n}",
            "auto id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (id >= N) {\n      return;\n   }\n\n   auto x = points[id].x;\n   auto y = points[id].y;\n\n   if (x > 0 && y > 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (x <= 0 && y > 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (x <= 0 && y <= 0) {\n      atomicAdd(&bins[2], 1);\n   } else {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n   if (i < N) {\n      int quadrant = (points[i].x >= 0) + (points[i].y >= 0) * 2;\n      atomicAdd(&(bins[quadrant]), 1);\n   }\n}",
            "// TODO: implement this function\n\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  size_t q = 0;\n  if (points[idx].x > 0) q += 1;\n  if (points[idx].y > 0) q += 2;\n  atomicAdd(&bins[q], 1);\n}",
            "// count the number of points that fall in the current quadrant\n  int count = 0;\n  for (size_t i = 0; i < N; i++) {\n    const Point p = points[i];\n    if (p.x > 0 && p.y > 0) {\n      count++;\n    }\n  }\n\n  // use a reduction to sum the number of points in each quadrant\n  // each thread adds its count to the thread before it\n  // then the thread before it adds its count to the thread before it\n  // etc.\n  // the first thread in the block will have the sum\n  // so thread 0 will have the count for all 4 quadrants\n\n  // the following does the same as `if (threadIdx.x % 2 == 0) {... }`\n  // but it uses fewer threads, and is therefore faster\n  if (threadIdx.x & 1) {\n    count += bins[threadIdx.x - 1];\n    bins[threadIdx.x - 1] = count;\n  }\n\n  // this if statement could be removed\n  // it just prevents the first thread from writing the sum to bins[3]\n  if (threadIdx.x!= blockDim.x - 1) {\n    __syncthreads(); // wait for all threads to reach this point\n  }\n}",
            "int idx = threadIdx.x;\n   if (idx >= N)\n      return;\n   Point p = points[idx];\n   if (p.x >= 0.0 && p.y >= 0.0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0.0 && p.y >= 0.0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0.0 && p.y < 0.0)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "/* Add your code here */\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   // each thread checks an element\n   if(i < N) {\n      // compute the quadrant\n      int quadrant = (points[i].x < 0) + (points[i].y < 0) * 2;\n      // update the bin count\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO: count the number of points in each quadrant, using bins to store the result\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        Point p = points[i];\n\n        // 0: {x > 0, y > 0}\n        if (p.x > 0 && p.y > 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        // 1: {x < 0, y > 0}\n        else if (p.x < 0 && p.y > 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        // 2: {x < 0, y < 0}\n        else if (p.x < 0 && p.y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        // 3: {x > 0, y < 0}\n        else if (p.x > 0 && p.y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// Each thread counts a single point.\n   int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid >= N)\n      return;\n\n   size_t bin = 0;\n   if (points[tid].x >= 0) {\n      if (points[tid].y >= 0)\n         bin = 0;\n      else\n         bin = 1;\n   } else {\n      if (points[tid].y >= 0)\n         bin = 2;\n      else\n         bin = 3;\n   }\n\n   // Add to the appropriate bin in a thread safe manner.\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    const Point point = points[i];\n    if (point.x >= 0) {\n      if (point.y >= 0) {\n        atomicAdd(bins + 0, 1);\n      } else {\n        atomicAdd(bins + 1, 1);\n      }\n    } else {\n      if (point.y >= 0) {\n        atomicAdd(bins + 2, 1);\n      } else {\n        atomicAdd(bins + 3, 1);\n      }\n    }\n  }\n}",
            "// TODO: fill in the kernel.\n   int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N)\n      return;\n\n   if (points[idx].x > 0 && points[idx].y > 0) {\n      atomicAdd(&bins[0], 1);\n   }\n   else if (points[idx].x <= 0 && points[idx].y > 0) {\n      atomicAdd(&bins[1], 1);\n   }\n   else if (points[idx].x <= 0 && points[idx].y <= 0) {\n      atomicAdd(&bins[2], 1);\n   }\n   else if (points[idx].x > 0 && points[idx].y <= 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    if (points[tid].x > 0 && points[tid].y > 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (points[tid].x <= 0 && points[tid].y > 0) {\n      atomicAdd(&bins[1], 1);\n    } else if (points[tid].x <= 0 && points[tid].y <= 0) {\n      atomicAdd(&bins[2], 1);\n    } else if (points[tid].x > 0 && points[tid].y <= 0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) bins[0]++;\n      else if (x < 0 && y >= 0) bins[1]++;\n      else if (x >= 0 && y < 0) bins[2]++;\n      else if (x < 0 && y < 0) bins[3]++;\n   }\n}",
            "// TODO: replace this with the number of threads launched\n    unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    // TODO: write a loop that sets the correct quadrant for each point\n    for (int i = 0; i < N; i++) {\n        Point p = points[i];\n        int x = p.x;\n        int y = p.y;\n        int quadrant = 0;\n        if (x > 0 && y > 0) {\n            quadrant = 0;\n        }\n        else if (x < 0 && y > 0) {\n            quadrant = 1;\n        }\n        else if (x > 0 && y < 0) {\n            quadrant = 2;\n        }\n        else if (x < 0 && y < 0) {\n            quadrant = 3;\n        }\n\n        atomicAdd(bins + quadrant, 1);\n    }\n}",
            "size_t tid = hipBlockIdx_x*hipBlockDim_x+hipThreadIdx_x;\n    if (tid >= N) return;\n    bins[0] += (points[tid].x >= 0 && points[tid].y >= 0? 1 : 0);\n    bins[1] += (points[tid].x < 0 && points[tid].y >= 0? 1 : 0);\n    bins[2] += (points[tid].x < 0 && points[tid].y < 0? 1 : 0);\n    bins[3] += (points[tid].x >= 0 && points[tid].y < 0? 1 : 0);\n}",
            "// TODO: Fill the code\n   int tid = blockDim.x*blockIdx.x + threadIdx.x;\n   if(tid < N) {\n      int qu = 0;\n      if(points[tid].x < 0) qu += 1;\n      if(points[tid].y < 0) qu += 2;\n      atomicAdd(&bins[qu], 1);\n   }\n}",
            "size_t index = blockIdx.x*blockDim.x+threadIdx.x;\n   if (index >= N) return;\n\n   size_t q = 0;\n   if (points[index].x > 0) q += 1;\n   if (points[index].y > 0) q += 2;\n   atomicAdd(&bins[q], 1);\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n   int stride = blockDim.x * gridDim.x;\n   for (; tid < N; tid += stride) {\n      int quadrant = (points[tid].x < 0)? 1 : 0;\n      quadrant |= (points[tid].y < 0)? 2 : 0;\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO\n\n}",
            "// your code here\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid >= N) return;\n\n   int q = (points[tid].x > 0? 1 : 0) + (points[tid].y > 0? 2 : 0);\n   atomicAdd(&bins[q], 1);\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) return;\n   double x = points[idx].x;\n   double y = points[idx].y;\n   if (x >= 0 && y >= 0) {\n      atomicAdd(&(bins[0]), 1);\n   } else if (x < 0 && y >= 0) {\n      atomicAdd(&(bins[1]), 1);\n   } else if (x < 0 && y < 0) {\n      atomicAdd(&(bins[2]), 1);\n   } else if (x >= 0 && y < 0) {\n      atomicAdd(&(bins[3]), 1);\n   }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) return;\n\n   Point p = points[idx];\n   size_t bin = 0;\n   if (p.x > 0 && p.y > 0) {\n      bin = 0;\n   } else if (p.x < 0 && p.y > 0) {\n      bin = 1;\n   } else if (p.x > 0 && p.y < 0) {\n      bin = 2;\n   } else if (p.x < 0 && p.y < 0) {\n      bin = 3;\n   }\n\n   atomicAdd(&bins[bin], 1);\n}",
            "}",
            "// determine the thread number\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // check whether the index is within the range\n  if (idx < N) {\n    const Point& point = points[idx];\n    if (point.x > 0 && point.y > 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (point.x <= 0 && point.y > 0) {\n      atomicAdd(&bins[1], 1);\n    } else if (point.x > 0 && point.y <= 0) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "//...\n}",
            "int myBin = blockIdx.x * blockDim.x + threadIdx.x;\n   for (size_t i = 0; i < N; ++i) {\n      if (myBin < 4) {\n         if (points[i].x > 0 && points[i].y > 0)\n            atomicAdd(&bins[0], 1);\n         else if (points[i].x < 0 && points[i].y > 0)\n            atomicAdd(&bins[1], 1);\n         else if (points[i].x < 0 && points[i].y < 0)\n            atomicAdd(&bins[2], 1);\n         else if (points[i].x > 0 && points[i].y < 0)\n            atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "__shared__ int local_bin[4];\n    __shared__ int local_sum;\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    int lane_id = threadIdx.x % 32;\n    int warp_id = threadIdx.x / 32;\n    int num_warps = blockDim.x / 32;\n\n    if (id < N) {\n        int index = 0;\n        if (points[id].x < 0 && points[id].y < 0) {\n            index = 0;\n        } else if (points[id].x >= 0 && points[id].y < 0) {\n            index = 1;\n        } else if (points[id].x < 0 && points[id].y >= 0) {\n            index = 2;\n        } else if (points[id].x >= 0 && points[id].y >= 0) {\n            index = 3;\n        }\n        atomicAdd(local_bin+index, 1);\n    }\n\n    int warp_sum = 0;\n    for (int i = 0; i < 4; i++) {\n        warp_sum += local_bin[i];\n    }\n\n    atomicAdd(local_sum, warp_sum);\n\n    __syncthreads();\n    if (threadIdx.x == 0) {\n        atomicAdd(bins, local_sum);\n    }\n}",
            "// your code here\n}",
            "// TODO: count the number of points in each quadrant.\n    //       each quadrant is a set of points where x >= 0 and y >= 0.\n    //       use the Point struct's public fields\n    //       count is stored in `bins`\n\n    // HINT: you can use atomicAdd(address, val) to increment the bins\n\n    // your code goes here\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int q = (points[i].x > 0) + (points[i].y > 0) * 2;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "int myBin = 0;\n   if (threadIdx.x < N) {\n      int pointX = points[threadIdx.x].x;\n      int pointY = points[threadIdx.x].y;\n      if (pointX >= 0) {\n         if (pointY >= 0) myBin = 0;\n         else myBin = 1;\n      } else {\n         if (pointY >= 0) myBin = 2;\n         else myBin = 3;\n      }\n      atomicAdd(&bins[myBin], 1);\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (idx >= N) {\n      return;\n   }\n\n   const auto& p = points[idx];\n\n   if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x >= 0 && p.y < 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x < 0 && p.y >= 0) {\n      atomicAdd(&bins[2], 1);\n   } else {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "__shared__ size_t s_bins[4];\n\n    const size_t threadIdx = threadIdx.x;\n    if (threadIdx < 4) {\n        s_bins[threadIdx] = 0;\n    }\n    __syncthreads();\n\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx; i < N; i += blockDim.x * gridDim.x) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            atomicAdd(&s_bins[0], 1);\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            atomicAdd(&s_bins[1], 1);\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            atomicAdd(&s_bins[2], 1);\n        } else {\n            atomicAdd(&s_bins[3], 1);\n        }\n    }\n    __syncthreads();\n\n    for (size_t i = threadIdx; i < 4; i += blockDim.x) {\n        atomicAdd(&bins[i], s_bins[i]);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // If the element of the list is not in range, we do not access it\n   if (i >= N) {\n      return;\n   }\n\n   if (points[i].x >= 0 && points[i].y >= 0) {\n      atomicAdd(&bins[0], 1);\n   }\n   else if (points[i].x < 0 && points[i].y >= 0) {\n      atomicAdd(&bins[1], 1);\n   }\n   else if (points[i].x < 0 && points[i].y < 0) {\n      atomicAdd(&bins[2], 1);\n   }\n   else if (points[i].x >= 0 && points[i].y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "int index = threadIdx.x;\n   if (index < N) {\n      if (points[index].x >= 0) {\n         if (points[index].y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[index].y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (thread_id < N) {\n        auto &p = points[thread_id];\n        // 1st quadrant: x, y > 0\n        if (p.x > 0 && p.y > 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        // 2nd quadrant: x < 0, y > 0\n        else if (p.x < 0 && p.y > 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        // 3rd quadrant: x < 0, y < 0\n        else if (p.x < 0 && p.y < 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        // 4th quadrant: x > 0, y < 0\n        else if (p.x > 0 && p.y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      // Use floor to determine quadrant index\n      int quadrant = (points[idx].x < 0.0? 1 : 0) + (points[idx].y < 0.0? 2 : 0);\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n  int q = 0;\n  if (points[idx].x >= 0) q += 1;\n  if (points[idx].y >= 0) q += 2;\n  atomicAdd(&bins[q], 1);\n}",
            "// TODO: Fill in code to count the number of points in each quadrant.\n   const Point &p = points[blockIdx.x];\n   if (p.x > 0 && p.y > 0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0 && p.y > 0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0 && p.y < 0)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "size_t threadId = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (threadId < N) {\n     if (points[threadId].x >= 0 && points[threadId].y >= 0) {\n        atomicAdd(&bins[0], 1);\n     }\n     else if (points[threadId].x < 0 && points[threadId].y >= 0) {\n        atomicAdd(&bins[1], 1);\n     }\n     else if (points[threadId].x >= 0 && points[threadId].y < 0) {\n        atomicAdd(&bins[2], 1);\n     }\n     else if (points[threadId].x < 0 && points[threadId].y < 0) {\n        atomicAdd(&bins[3], 1);\n     }\n   }\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      if (points[tid].x >= 0.0 && points[tid].y >= 0.0) {\n         atomicAdd(&bins[0], 1);\n      }\n      else if (points[tid].x < 0.0 && points[tid].y >= 0.0) {\n         atomicAdd(&bins[1], 1);\n      }\n      else if (points[tid].x < 0.0 && points[tid].y < 0.0) {\n         atomicAdd(&bins[2], 1);\n      }\n      else if (points[tid].x >= 0.0 && points[tid].y < 0.0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (tid < N) {\n      bins[0] += (points[tid].x > 0 && points[tid].y > 0);\n      bins[1] += (points[tid].x < 0 && points[tid].y > 0);\n      bins[2] += (points[tid].x < 0 && points[tid].y < 0);\n      bins[3] += (points[tid].x > 0 && points[tid].y < 0);\n   }\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    size_t bin = 0;\n    if (points[idx].x < 0) bin += 1;\n    if (points[idx].y < 0) bin += 2;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n   if (index < N) {\n      if (points[index].x > 0 && points[index].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[index].x < 0 && points[index].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[index].x < 0 && points[index].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[index].x > 0 && points[index].y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Get our global thread ID\n    int globalId = blockDim.x * blockIdx.x + threadIdx.x;\n    // Make sure we do not go out of bounds\n    if (globalId < N)\n    {\n        // Get our point\n        const Point &p = points[globalId];\n        // Count the number of points in each quadrant\n        if (p.x > 0 && p.y > 0)\n        {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (p.x < 0 && p.y > 0)\n        {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (p.x < 0 && p.y < 0)\n        {\n            atomicAdd(&bins[2], 1);\n        }\n        else if (p.x > 0 && p.y < 0)\n        {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x*blockDim.x;\n  if (i < N) {\n    int q = (points[i].x > 0 && points[i].y > 0)? 0 :\n            (points[i].x > 0 && points[i].y < 0)? 1 :\n            (points[i].x < 0 && points[i].y > 0)? 2 : 3;\n    atomicAdd(&(bins[q]), 1);\n  }\n}",
            "size_t thread = hipThreadIdx_x;\n  size_t stride = hipBlockDim_x;\n\n  if (thread < N) {\n    Point p = points[thread];\n    if (p.x > 0 && p.y > 0)\n      atomicAdd(&bins[0], 1);\n    else if (p.x < 0 && p.y > 0)\n      atomicAdd(&bins[1], 1);\n    else if (p.x < 0 && p.y < 0)\n      atomicAdd(&bins[2], 1);\n    else if (p.x > 0 && p.y < 0)\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "const size_t tid = threadIdx.x;\n   const size_t stride = blockDim.x;\n   for (size_t i = tid; i < N; i += stride) {\n      // determine which quadrant the point is in\n      size_t q = (points[i].x > 0) + 2 * (points[i].y > 0);\n\n      // Atomically increment the bin counter for the quadrant.\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "//...\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int q = 0;\n        if (points[i].x >= 0) {\n            q += 1;\n            if (points[i].y >= 0) {\n                q += 2;\n            }\n        }\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      Point p = points[idx];\n      size_t b = (p.x > 0)? (p.y > 0? 1 : 2) : (p.y > 0? 3 : 4);\n      atomicAdd(&bins[b], 1);\n   }\n}",
            "// TODO: count the number of points in each quadrant using parallel_for and a reduction variable.\n   //       use 4 blocks of 128 threads\n\n}",
            "int tid = threadIdx.x;\n   if (tid >= N) return;\n   bins[whichQuadrant(points[tid].x, points[tid].y)] += 1;\n}",
            "size_t global_id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (global_id >= N) {\n      return;\n   }\n\n   //...\n}",
            "// get the current thread index\n  size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  // check if we are still in the range of points\n  if (tid < N) {\n    const Point &p = points[tid];\n    size_t bin = 0;\n    if (p.x >= 0) {\n      bin += (p.y >= 0)? 0 : 1;\n    } else {\n      bin += (p.y >= 0)? 2 : 3;\n    }\n\n    // atomically increment the bin count\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "const int bid = blockIdx.x;\n   const int tid = threadIdx.x;\n\n   // Each thread handles a point in the array\n   if (bid * blockDim.x + tid < N) {\n      //...\n   }\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N)\n        return;\n    // TODO: fill bins\n}",
            "int bin = 0;\n   if (points[blockIdx.x].x > 0) {\n      if (points[blockIdx.x].y > 0)\n         bin = 1;\n   }\n   else if (points[blockIdx.x].x < 0) {\n      if (points[blockIdx.x].y > 0)\n         bin = 2;\n      else if (points[blockIdx.x].y < 0)\n         bin = 3;\n   }\n   atomicAdd(&bins[bin], 1);\n}",
            "/* Your code here */\n\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (id >= N)\n    return;\n\n  double x = points[id].x;\n  double y = points[id].y;\n  double quadrant = (x >= 0.0 && y >= 0.0)? 0.0 : (x < 0.0 && y >= 0.0)? 1.0 : (x < 0.0 && y < 0.0)? 2.0 : (x >= 0.0 && y < 0.0)? 3.0 : 0.0;\n  atomicAdd(&bins[int(quadrant)], 1);\n}",
            "// The thread ID, the corresponding point, and the bin it belongs to.\n   size_t tid = hipThreadIdx_x;\n   const Point &pt = points[tid];\n   const int bin = pt.x >= 0 && pt.y >= 0? 0 :\n                   pt.x < 0  && pt.y >= 0? 1 :\n                   pt.x < 0  && pt.y <  0? 2 :\n                   3;\n   // Atomically increment the bin for this point.\n   atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      const Point& pt = points[i];\n      int q = 0;\n      if (pt.x > 0 && pt.y > 0) {\n         q = 0;\n      } else if (pt.x < 0 && pt.y > 0) {\n         q = 1;\n      } else if (pt.x < 0 && pt.y < 0) {\n         q = 2;\n      } else {\n         q = 3;\n      }\n      atomicAdd(bins + q, 1);\n   }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i >= N) return;\n\n   int bin = (points[i].x > 0) + 2*(points[i].y > 0);\n   atomicAdd(&bins[bin], 1);\n}",
            "int myBin = -1;\n   // TODO: calculate myBin\n   atomicAdd(&bins[myBin], 1);\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i >= N) return;\n   Point p = points[i];\n   size_t q = p.x > 0? 0 : (p.y > 0? 1 : (p.x < 0? 2 : 3));\n   atomicAdd(&bins[q], 1);\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = gridDim.x * blockDim.x;\n\n    for (size_t i = index; i < N; i += stride) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x >= 0.0) {\n            if (y >= 0.0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        } else {\n            if (y >= 0.0) {\n                atomicAdd(&bins[1], 1);\n            } else {\n                atomicAdd(&bins[2], 1);\n            }\n        }\n    }\n}",
            "// each thread works on one point\n    Point p = points[threadIdx.x];\n\n    // Compute quadrant number using the sign of `p.x` and `p.y`\n    size_t quadrant = ((p.x < 0)? 1 : 0) | ((p.y < 0)? 2 : 0);\n\n    // Update bin count\n    atomicAdd(&bins[quadrant], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n      if (points[tid].x >= 0 && points[tid].y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (points[tid].x < 0 && points[tid].y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (points[tid].x < 0 && points[tid].y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (points[tid].x >= 0 && points[tid].y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "__shared__ int s_bins[4];\n  size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  if (i < N) {\n    int q = 0;\n    if (points[i].x > 0) {\n      if (points[i].y > 0) q = 1;\n      else q = 2;\n    } else {\n      if (points[i].y > 0) q = 0;\n      else q = 3;\n    }\n    atomicAdd(&s_bins[q], 1);\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 4; ++i) atomicAdd(&bins[i], s_bins[i]);\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (i < N) {\n      size_t bin;\n      if (points[i].x < 0) {\n         if (points[i].y < 0)\n            bin = 0;\n         else\n            bin = 3;\n      } else {\n         if (points[i].y < 0)\n            bin = 1;\n         else\n            bin = 2;\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO\n}",
            "unsigned int idx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (idx < N) {\n      if (points[idx].x > 0) {\n         if (points[idx].y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (points[idx].y > 0) {\n            atomicAdd(&bins[1], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n   if (i >= N) return;\n\n   Point p = points[i];\n   int x = p.x;\n   int y = p.y;\n\n   if (x > 0 && y > 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (x < 0 && y > 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (x < 0 && y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (x > 0 && y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "}",
            "// use `tid` to access the current thread id\n   size_t tid = blockDim.x*blockIdx.x + threadIdx.x;\n   if (tid >= N)\n      return;\n\n   int q = 0;\n   if (points[tid].x >= 0 && points[tid].y >= 0)\n      q = 1;\n   else if (points[tid].x < 0 && points[tid].y >= 0)\n      q = 2;\n   else if (points[tid].x < 0 && points[tid].y < 0)\n      q = 3;\n\n   atomicAdd(&bins[q], 1);\n}",
            "// TODO: Implement this\n    size_t my_index = blockIdx.x * blockDim.x + threadIdx.x;\n    if(my_index < N) {\n      if(points[my_index].x < 0) {\n        if(points[my_index].y < 0) {\n          atomicAdd(bins, 0, 1);\n        }\n        else {\n          atomicAdd(bins, 2, 1);\n        }\n      }\n      else {\n        if(points[my_index].y < 0) {\n          atomicAdd(bins, 1, 1);\n        }\n        else {\n          atomicAdd(bins, 3, 1);\n        }\n      }\n    }\n}",
            "// TODO\n}",
            "int threadID = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadID < N) {\n        Point p = points[threadID];\n        if (p.x > 0 && p.y > 0)\n            atomicAdd(&bins[0], 1);\n        else if (p.x < 0 && p.y > 0)\n            atomicAdd(&bins[1], 1);\n        else if (p.x < 0 && p.y < 0)\n            atomicAdd(&bins[2], 1);\n        else if (p.x > 0 && p.y < 0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        const Point& p = points[i];\n        const int q = (p.x > 0) + (p.y > 0) * 2;\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x > 0 && p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (p.x < 0 && p.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (p.x > 0 && p.y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) return;\n   Point point = points[idx];\n   if (point.x >= 0 && point.y >= 0) bins[0]++;\n   if (point.x < 0 && point.y >= 0) bins[1]++;\n   if (point.x < 0 && point.y < 0) bins[2]++;\n   if (point.x >= 0 && point.y < 0) bins[3]++;\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "auto const rank = 0;\n   auto const nranks = 1;\n   auto const size = points.size();\n   auto const offset = rank * size / nranks;\n   auto const local_size = size / nranks;\n   auto const local_points = points.data() + offset;\n\n   std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n   for (size_t i = 0; i < local_size; ++i) {\n      auto const p = local_points[i];\n      if (p.x < 0) {\n         if (p.y < 0) {\n            local_bins[3]++;\n         } else {\n            local_bins[2]++;\n         }\n      } else {\n         if (p.y < 0) {\n            local_bins[1]++;\n         } else {\n            local_bins[0]++;\n         }\n      }\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n}",
            "int rank;\n   int nranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n   // Do not modify the code in this function!\n   if (rank == 0) {\n      // Count points in each quadrant in serial. Store the results in bins.\n      // Rank 0 has a complete copy of points.\n      for (auto &point : points) {\n         if (point.x >= 0) {\n            if (point.y >= 0) {\n               bins[0]++;\n            } else {\n               bins[2]++;\n            }\n         } else {\n            if (point.y >= 0) {\n               bins[1]++;\n            } else {\n               bins[3]++;\n            }\n         }\n      }\n   } else {\n      // Count the number of points that belong to this rank\n      // and send the result to rank 0\n      size_t myPoints = 0;\n      for (auto &point : points) {\n         if (point.x >= 0) {\n            if (point.y >= 0) {\n               myPoints++;\n            }\n         } else {\n            if (point.y >= 0) {\n               myPoints++;\n            }\n         }\n      }\n      MPI_Send(&myPoints, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n   if (rank == 0) {\n      // Receive counts from other ranks and add them to bins.\n      // Rank 0 has a complete copy of points.\n      for (int i = 1; i < nranks; i++) {\n         int count;\n         MPI_Recv(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         bins[0] += count;\n      }\n   }\n}",
            "int nbproc, id;\n  MPI_Comm_size(MPI_COMM_WORLD, &nbproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &id);\n\n  int nbpoints = points.size();\n  int nbpoints_total = nbpoints * nbproc;\n  int nbpoints_p = nbpoints / nbproc;\n  int nbpoints_r = nbpoints - nbproc * nbpoints_p;\n\n  int offset = nbpoints_p * id + (id >= nbproc - nbpoints_r? nbproc - nbpoints_r : 0);\n  int nbpoints_p_id = nbpoints_p + (id >= nbproc - nbpoints_r? 1 : 0);\n\n  bins = std::array<size_t, 4>{0, 0, 0, 0};\n\n  for (int i = 0; i < nbpoints_p_id; i++) {\n    if (points[offset + i].x > 0 && points[offset + i].y > 0)\n      bins[0]++;\n    else if (points[offset + i].x < 0 && points[offset + i].y > 0)\n      bins[1]++;\n    else if (points[offset + i].x < 0 && points[offset + i].y < 0)\n      bins[2]++;\n    else if (points[offset + i].x > 0 && points[offset + i].y < 0)\n      bins[3]++;\n  }\n\n  // The other ranks will have to send their data\n  if (id!= 0) {\n    for (int i = 0; i < 4; i++)\n      MPI_Send(&(bins[i]), 1, MPI_UNSIGNED_LONG, 0, i, MPI_COMM_WORLD);\n  }\n\n  // Rank 0 has to receive all the data from other ranks\n  if (id == 0) {\n    for (int i = 1; i < nbproc; i++)\n      for (int j = 0; j < 4; j++)\n        MPI_Recv(&(bins[j]), 1, MPI_UNSIGNED_LONG, i, j, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n}",
            "// Your code here!\n   int rnk;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rnk);\n   if(rnk == 0) {\n      for(size_t i = 0; i < bins.size(); i++)\n      {\n         bins[i] = 0;\n      }\n   }\n   for(auto p: points) {\n      if(p.x >= 0 && p.y >= 0)\n         bins[0]++;\n      if(p.x >= 0 && p.y < 0)\n         bins[1]++;\n      if(p.x < 0 && p.y >= 0)\n         bins[2]++;\n      if(p.x < 0 && p.y < 0)\n         bins[3]++;\n   }\n   MPI_Reduce(&bins, bins.data(), bins.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement me!\n}",
            "int size, rank;\n\n   // get size of MPI world\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   // get rank of this process\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // divide work\n   int numPointsPerProc = points.size() / size;\n   int numPointsRemain = points.size() % size;\n\n   // allocate a buffer to store points\n   std::vector<Point> procPoints(numPointsPerProc);\n\n   // copy points to processors\n   int numPointsToCopy = rank == size - 1? numPointsPerProc + numPointsRemain : numPointsPerProc;\n   std::copy_n(points.begin() + rank * numPointsPerProc, numPointsToCopy, procPoints.begin());\n\n   // count points in each quadrant\n   std::array<size_t, 4> procBins{};\n   for (Point const& p : procPoints) {\n      if (p.x >= 0 && p.y >= 0) procBins[0]++;\n      if (p.x < 0 && p.y >= 0) procBins[1]++;\n      if (p.x < 0 && p.y < 0) procBins[2]++;\n      if (p.x >= 0 && p.y < 0) procBins[3]++;\n   }\n\n   // combine results from all processors\n   if (rank == 0) {\n      // copy results from processors 1 to size-1 to bins\n      for (int i = 1; i < size; i++) {\n         MPI_Status status;\n         MPI_Recv(procBins.data(), procBins.size(), MPI_UNSIGNED_LONG_LONG, i, 12345, MPI_COMM_WORLD, &status);\n         for (int j = 0; j < 4; j++) bins[j] += procBins[j];\n      }\n   } else {\n      // send results to rank 0\n      MPI_Send(procBins.data(), procBins.size(), MPI_UNSIGNED_LONG_LONG, 0, 12345, MPI_COMM_WORLD);\n   }\n\n   // clear bins on all processors\n   if (rank == 0) std::fill(bins.begin(), bins.end(), 0);\n}",
            "// TODO\n}",
            "const size_t n = points.size();\n   const size_t n_proc = 4; // Assume we know how many processes we have\n   const size_t n_per_proc = n / n_proc;\n\n   // First gather data from all processes\n   std::vector<size_t> counts(n_proc * 4);\n   MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &counts[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   // Add up all the results\n   if (0 == MPI::COMM_WORLD.Get_rank()) {\n      for (size_t i = 0; i < n_proc; ++i) {\n         size_t offset = i * 4;\n         for (size_t j = 0; j < 4; ++j) {\n            bins[j] += counts[offset + j];\n         }\n      }\n   }\n}",
            "// YOUR CODE HERE\n\n}",
            "// Your code here!\n}",
            "int mpi_rank;\n    int mpi_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n\n    // TODO: implement\n}",
            "int const size = points.size();\n    int const rank = MPI::COMM_WORLD.Get_rank();\n    int const root = 0;\n    int const maxQuad = 4;\n\n    std::array<size_t, 4> local_bins = {{ 0, 0, 0, 0 }};\n\n    if (rank == root) {\n        for (auto const& pt : points) {\n            local_bins[std::max(0, std::min(3, static_cast<int>(pt.x >= 0? (pt.y >= 0? 0 : 3) : (pt.y >= 0? 1 : 2))))]++;\n        }\n    }\n\n    MPI::COMM_WORLD.Reduce(&local_bins, &bins, maxQuad, MPI_UNSIGNED_LONG, MPI_SUM, root);\n}",
            "if (points.empty()) return;\n\n    // Count quadrants in one thread\n    if (points.size() < 2) {\n        if (points[0].x >= 0 && points[0].y >= 0) bins[0]++;\n        if (points[0].x < 0 && points[0].y >= 0) bins[1]++;\n        if (points[0].x >= 0 && points[0].y < 0) bins[2]++;\n        if (points[0].x < 0 && points[0].y < 0) bins[3]++;\n        return;\n    }\n\n    int n, m;\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n    MPI_Comm_rank(MPI_COMM_WORLD, &m);\n\n    // Every thread should have the same number of points\n    size_t pointsPerThread = points.size() / n;\n    std::vector<Point> threadPoints(pointsPerThread);\n    for (size_t i = 0; i < pointsPerThread; ++i) {\n        threadPoints[i] = points[i + pointsPerThread * m];\n    }\n\n    // Count quadrants in one thread\n    if (pointsPerThread < 2) {\n        if (threadPoints[0].x >= 0 && threadPoints[0].y >= 0) bins[0]++;\n        if (threadPoints[0].x < 0 && threadPoints[0].y >= 0) bins[1]++;\n        if (threadPoints[0].x >= 0 && threadPoints[0].y < 0) bins[2]++;\n        if (threadPoints[0].x < 0 && threadPoints[0].y < 0) bins[3]++;\n        return;\n    }\n\n    // Send each thread's first point to thread 0\n    Point point;\n    if (m!= 0) {\n        MPI_Send(&threadPoints[0], 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Thread 0 receives all points and counts the quadrants\n    if (m == 0) {\n        for (int i = 1; i < n; ++i) {\n            MPI_Recv(&point, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            if (point.x >= 0 && point.y >= 0) bins[0]++;\n            if (point.x < 0 && point.y >= 0) bins[1]++;\n            if (point.x >= 0 && point.y < 0) bins[2]++;\n            if (point.x < 0 && point.y < 0) bins[3]++;\n        }\n    }\n\n    // Thread 0 sends result back to all other threads\n    if (m == 0) {\n        for (int i = 1; i < n; ++i) {\n            MPI_Send(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n\n   // TODO: your code here\n}",
            "// your code goes here\n}",
            "int myrank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  int bins_per_process = 4 / nprocs;\n  int remainder = 4 % nprocs;\n  int left_over = 0;\n  for (int i = 0; i < nprocs; ++i) {\n    int num_bins = (i < remainder)? bins_per_process + 1 : bins_per_process;\n    if (myrank == i) {\n      for (Point& p : points) {\n        if (p.x > 0.0 && p.y > 0.0) {\n          bins[0] += 1;\n        } else if (p.x < 0.0 && p.y > 0.0) {\n          bins[1] += 1;\n        } else if (p.x < 0.0 && p.y < 0.0) {\n          bins[2] += 1;\n        } else {\n          bins[3] += 1;\n        }\n      }\n    }\n  }\n\n  if (myrank == 0) {\n    for (int i = 1; i < nprocs; ++i) {\n      int recv_counts[4];\n      MPI_Recv(recv_counts, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[0] += recv_counts[0];\n      bins[1] += recv_counts[1];\n      bins[2] += recv_counts[2];\n      bins[3] += recv_counts[3];\n    }\n  } else {\n    int send_counts[4] = {0, 0, 0, 0};\n    for (Point& p : points) {\n      if (p.x > 0.0 && p.y > 0.0) {\n        send_counts[0] += 1;\n      } else if (p.x < 0.0 && p.y > 0.0) {\n        send_counts[1] += 1;\n      } else if (p.x < 0.0 && p.y < 0.0) {\n        send_counts[2] += 1;\n      } else {\n        send_counts[3] += 1;\n      }\n    }\n    MPI_Send(send_counts, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Implement\n}",
            "// TODO\n}",
            "int numRanks, myRank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n   // TODO: Implement\n}",
            "// YOUR CODE HERE\n}",
            "// Your code here\n}",
            "if (points.size() == 0) {\n      return;\n   }\n\n   // TODO\n   bins = {0,0,0,0};\n\n   int numtasks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<Point> points_copy;\n   if (rank == 0) {\n      points_copy = points;\n   }\n\n   int points_per_rank = points.size() / numtasks;\n   int remainder = points.size() % numtasks;\n   int my_points_per_rank;\n   if (rank == 0) {\n      my_points_per_rank = points_per_rank + remainder;\n   }\n   else {\n      my_points_per_rank = points_per_rank;\n   }\n\n   std::vector<Point> my_points(my_points_per_rank);\n   MPI_Scatter(&points_copy[0], my_points_per_rank, sizeof(Point), &my_points[0], my_points_per_rank, sizeof(Point), 0, MPI_COMM_WORLD);\n\n   std::array<size_t, 4> my_bins{0,0,0,0};\n\n   for (Point p : my_points) {\n      if (p.x > 0 && p.y > 0) {\n         my_bins[0]++;\n      }\n      else if (p.x < 0 && p.y > 0) {\n         my_bins[1]++;\n      }\n      else if (p.x > 0 && p.y < 0) {\n         my_bins[2]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         my_bins[3]++;\n      }\n   }\n\n   MPI_Reduce(&my_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int nproc;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   int myrank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n   /* your code here */\n}",
            "}",
            "// TODO:\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // YOUR CODE HERE\n   // Assume that bins has been initialized with zeros\n   // Assume that points is stored on all ranks\n\n   // Example:\n   // bins = [0, 0, 0, 0]\n   // points = [{-1, 1}, {-1, -1}, {1, 1}, {1, -1}]\n   // bins = [1, 1, 1, 1]\n}",
            "// TODO\n}",
            "int comm_size, comm_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n   int q1, q2, q3, q4;\n   q1 = q2 = q3 = q4 = 0;\n   for (auto p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         q1++;\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         q2++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         q3++;\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         q4++;\n      }\n   }\n   std::array<int, 4> counts;\n   counts[0] = q1;\n   counts[1] = q2;\n   counts[2] = q3;\n   counts[3] = q4;\n   MPI_Reduce(&counts, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n}",
            "/* TODO: Add your code here\n    */\n\n}",
            "// TODO\n}",
            "bins = {0, 0, 0, 0};\n\n   // This is where your code goes!\n}",
            "constexpr int numQuads = 4;\n  MPI_Comm comm;\n  int rank;\n  int size;\n  MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  // create equal-sized chunks\n  int per_chunk = points.size() / size;\n  int remainder = points.size() % size;\n  int chunk_size = (rank == 0)? per_chunk + remainder : per_chunk;\n  std::vector<Point> my_points(chunk_size);\n  MPI_Scatter(&points[0], chunk_size, MPI_DOUBLE, &my_points[0], chunk_size, MPI_DOUBLE, 0, comm);\n\n  // count the points in this chunk\n  std::array<size_t, 4> my_bins{0, 0, 0, 0};\n  for (size_t i = 0; i < chunk_size; i++) {\n    if (my_points[i].x >= 0 && my_points[i].y >= 0)\n      my_bins[0]++;\n    else if (my_points[i].x < 0 && my_points[i].y >= 0)\n      my_bins[1]++;\n    else if (my_points[i].x >= 0 && my_points[i].y < 0)\n      my_bins[2]++;\n    else if (my_points[i].x < 0 && my_points[i].y < 0)\n      my_bins[3]++;\n  }\n\n  MPI_Reduce(my_bins.data(), bins.data(), numQuads, MPI_INT, MPI_SUM, 0, comm);\n  MPI_Comm_free(&comm);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   auto pointsPerRank = points.size() / size;\n   auto pointsInThisRank = points.size() - (pointsPerRank * (size - 1));\n   std::array<size_t, 4> counts;\n   if (rank == 0) {\n      for (int i = 0; i < pointsPerRank; i++) {\n         auto const& point = points[i];\n         int bin = (point.x >= 0 && point.y >= 0)? 0 : (point.x < 0 && point.y >= 0)? 1\n            : (point.x < 0 && point.y < 0)? 2 : 3;\n         counts[bin]++;\n      }\n   } else {\n      for (int i = 0; i < pointsInThisRank; i++) {\n         auto const& point = points[i];\n         int bin = (point.x >= 0 && point.y >= 0)? 0 : (point.x < 0 && point.y >= 0)? 1\n            : (point.x < 0 && point.y < 0)? 2 : 3;\n         counts[bin]++;\n      }\n   }\n   MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "}",
            "//TODO\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int my_rank;\n   MPI_Comm_rank(comm, &my_rank);\n   int num_processes;\n   MPI_Comm_size(comm, &num_processes);\n\n   std::vector<int> quadrant_counts(4);\n\n   for (int i = 0; i < num_processes; i++) {\n      for (auto point : points) {\n         int x_in_q = point.x >= 0? 1 : 0;\n         int y_in_q = point.y >= 0? 2 : 0;\n         quadrant_counts[x_in_q + y_in_q]++;\n      }\n      if (my_rank == 0) {\n         std::cout << quadrant_counts[0] << \" \" << quadrant_counts[1] << \" \" << quadrant_counts[2] << \" \" << quadrant_counts[3] << std::endl;\n      }\n      MPI_Barrier(MPI_COMM_WORLD);\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n   if (my_rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = quadrant_counts[i];\n      }\n   }\n}",
            "// TODO: Write code to count the number of points in each quadrant.\n    // TODO: Use MPI to distribute points across ranks.\n    // TODO: Use MPI to collect the result in bins on rank 0.\n    int my_rank;\n    int comm_size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    // Compute the number of points per rank\n    int pointsPerRank = points.size() / comm_size;\n    int remainingPoints = points.size() % comm_size;\n\n    // Compute the start and end index for this rank\n    int start = my_rank * pointsPerRank + (my_rank < remainingPoints? my_rank : remainingPoints);\n    int end = (my_rank + 1) * pointsPerRank + (my_rank + 1 < remainingPoints? my_rank + 1 : remainingPoints);\n\n    // Compute the number of points in this rank\n    int numPoints = end - start;\n\n    // Store the number of points in each quadrant\n    std::array<size_t, 4> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n\n    // Count the points in each quadrant\n    for (int i = start; i < end; i++) {\n        int q = 0;\n        if (points[i].x > 0)\n            q++;\n        if (points[i].y > 0)\n            q++;\n        localBins[q]++;\n    }\n\n    // TODO: Use MPI to collect the result in bins on rank 0\n    // TODO: Use MPI_Gather\n    if (my_rank == 0) {\n        std::array<size_t, 4> globalBins;\n        MPI_Gather(localBins.data(), 4, MPI_LONG, globalBins.data(), 4, MPI_LONG, 0, MPI_COMM_WORLD);\n        bins = globalBins;\n    } else {\n        MPI_Gather(localBins.data(), 4, MPI_LONG, nullptr, 4, MPI_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int n, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &n);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (rank == 0) {\n      for (auto const& point : points) {\n         if (point.x > 0 && point.y > 0) {\n            ++bins[0];\n         } else if (point.x < 0 && point.y > 0) {\n            ++bins[1];\n         } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n         } else if (point.x > 0 && point.y < 0) {\n            ++bins[3];\n         } else {\n            std::cout << \"Unrecognized point: \" << point.x << \", \" << point.y << '\\n';\n            exit(1);\n         }\n      }\n   }\n}",
            "// TODO: Fill in your code here\n}",
            "// Your code here\n\n}",
            "int nproc;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int const npts = points.size();\n   int const npts_per_proc = npts / nproc;\n   int const npts_rem = npts % nproc;\n   int const start_point = rank * npts_per_proc + std::min(rank, npts_rem);\n   int const end_point = (rank + 1) * npts_per_proc + std::min(rank + 1, npts_rem);\n   int const npts_local = end_point - start_point;\n\n   std::vector<int> counts;\n   counts.reserve(4);\n\n   for (int i = start_point; i < end_point; i++) {\n      Point const& p = points[i];\n      int const bin = (p.x >= 0) + (p.y >= 0) * 2;\n      counts[bin]++;\n   }\n\n   int *buffer;\n   MPI_Alloc_mem(npts_local * sizeof(int), MPI_INFO_NULL, &buffer);\n\n   MPI_Gather(&counts[0], 4, MPI_INT, buffer, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         int sum = 0;\n         for (int j = 0; j < nproc; j++) {\n            sum += buffer[j * 4 + i];\n         }\n         bins[i] = sum;\n      }\n   }\n\n   MPI_Free_mem(buffer);\n}",
            "// TODO\n}",
            "// TODO:\n\n}",
            "// your code goes here\n}",
            "}",
            "std::vector<size_t> counts = {0,0,0,0}; // create a vector of 4 elements, all set to 0\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0)\n    {\n        for (Point p: points)\n        {\n            if ((p.x > 0) && (p.y > 0))\n            {\n                counts[0]++;\n            }\n            else if ((p.x < 0) && (p.y > 0))\n            {\n                counts[1]++;\n            }\n            else if ((p.x < 0) && (p.y < 0))\n            {\n                counts[2]++;\n            }\n            else\n            {\n                counts[3]++;\n            }\n        }\n    }\n    MPI_Gather(&counts, 4, MPI_SIZE_T, &bins, 4, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n\n    if (rank == 0)\n    {\n        for (auto bin: bins)\n        {\n            std::cout << bin << \" \";\n        }\n    }\n\n}",
            "// TODO\n}",
            "// TODO: Your code here.\n   int size, rank;\n   int tag1 = 0, tag2 = 1, tag3 = 2, tag4 = 3;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      for (int i = 1; i < size; i++) {\n         MPI_Send(&points.at(0), points.size(), MPI_DOUBLE, i, tag1, MPI_COMM_WORLD);\n      }\n   }\n\n   if (rank!= 0) {\n      std::vector<double> temp;\n      MPI_Status status;\n      MPI_Recv(&temp.at(0), points.size(), MPI_DOUBLE, 0, tag1, MPI_COMM_WORLD, &status);\n\n      for (int i = 0; i < temp.size(); i++) {\n         if (temp.at(i) < 0 && temp.at(i + 1) >= 0) {\n            bins.at(0) += 1;\n         } else if (temp.at(i) < 0 && temp.at(i + 1) < 0) {\n            bins.at(1) += 1;\n         } else if (temp.at(i) >= 0 && temp.at(i + 1) < 0) {\n            bins.at(2) += 1;\n         } else if (temp.at(i) >= 0 && temp.at(i + 1) >= 0) {\n            bins.at(3) += 1;\n         }\n      }\n\n      MPI_Send(&bins.at(0), 4, MPI_INT, 0, tag2, MPI_COMM_WORLD);\n   }\n\n   if (rank == 0) {\n      for (int i = 1; i < size; i++) {\n         MPI_Recv(&bins.at(0), 4, MPI_INT, i, tag2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   }\n}",
            "// TODO: Implement\n   int num_proc, rank;\n   int size = points.size();\n   MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if(rank == 0)\n   {\n    std::array<size_t, 4> tBins;\n    for(size_t i = 0; i < 4; ++i)\n      tBins[i] = 0;\n\n    for(size_t i = 0; i < size; ++i)\n    {\n      if(points[i].x > 0 && points[i].y > 0)\n        tBins[0] += 1;\n      else if(points[i].x < 0 && points[i].y > 0)\n        tBins[1] += 1;\n      else if(points[i].x < 0 && points[i].y < 0)\n        tBins[2] += 1;\n      else\n        tBins[3] += 1;\n    }\n\n    for(size_t i = 1; i < num_proc; ++i)\n    {\n      MPI_Recv(bins.data(), 4, MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(size_t j = 0; j < 4; ++j)\n        tBins[j] += bins[j];\n    }\n    bins = tBins;\n  }\n  else\n  {\n    for(size_t i = 0; i < size; ++i)\n    {\n      if(points[i].x > 0 && points[i].y > 0)\n        bins[0] += 1;\n      else if(points[i].x < 0 && points[i].y > 0)\n        bins[1] += 1;\n      else if(points[i].x < 0 && points[i].y < 0)\n        bins[2] += 1;\n      else\n        bins[3] += 1;\n    }\n    MPI_Send(bins.data(), 4, MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "size_t np = points.size();\n\n   // Use a temporary variable to count the number of points\n   // in each quadrant.\n   // The elements of `counts` correspond to the quadrants in the following order:\n   //   3 | 0\n   //   -----\n   //   2 | 1\n   std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n   // Count the number of points in each quadrant and\n   // put the counts into `counts`.\n\n   // TODO: Replace this statement with your own code\n   std::fill(bins.begin(), bins.end(), 0);\n}",
            "// TODO\n}",
            "// Your code goes here\n}",
            "// TODO: Implement this\n   MPI_Bcast(&bins, bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // You fill in this function\n\n}",
            "}",
            "// your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: YOUR CODE HERE\n  const int rank = 0;\n  const int size = 0;\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for (auto it = points.cbegin(); it!= points.cend(); it++) {\n    if (it->x > 0) {\n      if (it->y > 0) {\n        bins[0] += 1;\n      } else {\n        bins[1] += 1;\n      }\n    } else {\n      if (it->y > 0) {\n        bins[2] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    }\n  }\n  if (rank == 0) {\n    for (auto it = bins.cbegin(); it!= bins.cend(); it++) {\n      printf(\"%zu \", *it);\n    }\n  }\n}",
            "// Implement me\n\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Fill in this function\n\n}",
            "int nproc, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // your code here\n\n}",
            "// TODO\n}",
            "/* YOUR CODE HERE */\n  int numprocs;\n  int rank;\n  int left = 0;\n  int right = 0;\n  int up = 0;\n  int down = 0;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n  int left_count = 0;\n  int right_count = 0;\n  int up_count = 0;\n  int down_count = 0;\n\n  if(rank == 0) {\n    for(int i = 0; i < points.size(); i++) {\n      if(points[i].x < 0 && points[i].y < 0) {\n        left++;\n      }\n      if(points[i].x > 0 && points[i].y < 0) {\n        right++;\n      }\n      if(points[i].x > 0 && points[i].y > 0) {\n        up++;\n      }\n      if(points[i].x < 0 && points[i].y > 0) {\n        down++;\n      }\n    }\n  }\n\n  MPI_Send(&left, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n  MPI_Send(&right, 1, MPI_INT, 2, 0, MPI_COMM_WORLD);\n  MPI_Send(&up, 1, MPI_INT, 3, 0, MPI_COMM_WORLD);\n  MPI_Send(&down, 1, MPI_INT, 4, 0, MPI_COMM_WORLD);\n\n  if(rank == 1) {\n    MPI_Recv(&left_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  if(rank == 2) {\n    MPI_Recv(&right_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  if(rank == 3) {\n    MPI_Recv(&up_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n  if(rank == 4) {\n    MPI_Recv(&down_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n\n  if(rank == 0) {\n    left = left + left_count;\n    right = right + right_count;\n    up = up + up_count;\n    down = down + down_count;\n\n    bins[0] = left;\n    bins[1] = right;\n    bins[2] = up;\n    bins[3] = down;\n  }\n}",
            "// TODO: your code here\n    std::array<int, 4> bins_all;\n    bins_all.fill(0);\n\n    MPI_Reduce(&bins_all[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code goes here\n}",
            "// TODO: your code goes here\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n  auto num_ranks = omp_get_num_threads();\n  auto rank = omp_get_thread_num();\n  auto local_points = std::vector<Point>();\n  auto local_bins = std::array<size_t, 4>();\n  if(rank == 0){\n    auto begin = points.begin() + rank;\n    auto end = points.begin() + rank + points.size() / num_ranks;\n    local_points = std::vector<Point>(begin, end);\n    for(auto& p : local_points){\n      local_bins[getQuadrant(p)]++;\n    }\n    MPI_Scatter(local_bins.data(), 4, MPI_SIZE_T, bins.data(), 4, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n  } else {\n    auto begin = points.begin() + rank * points.size() / num_ranks;\n    auto end = points.begin() + (rank + 1) * points.size() / num_ranks;\n    local_points = std::vector<Point>(begin, end);\n    for(auto& p : local_points){\n      local_bins[getQuadrant(p)]++;\n    }\n    MPI_Scatter(local_bins.data(), 4, MPI_SIZE_T, bins.data(), 4, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n\n    // 1. create 4 MPI groups\n    // 2. send points to the correct group\n    // 3. count points in each group\n    // 4. combine counts of each group\n\n}",
            "size_t n = points.size();\n   int num_ranks, rank_id;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n\n   if (rank_id == 0) {\n       size_t n_per_rank = n / num_ranks;\n       std::vector<size_t> counts(num_ranks);\n\n       for (int i = 0; i < num_ranks; i++) {\n           size_t offset = n_per_rank * i;\n           for (int j = 0; j < n_per_rank; j++) {\n               double x = points[offset + j].x;\n               double y = points[offset + j].y;\n\n               int quadrant = 0;\n               if (x < 0) quadrant |= 1;\n               if (y < 0) quadrant |= 2;\n               bins[quadrant]++;\n           }\n           counts[i] = bins[quadrant];\n       }\n   } else {\n       std::vector<size_t> counts(num_ranks);\n\n       size_t n_per_rank = n / num_ranks;\n       size_t offset = n_per_rank * rank_id;\n       for (int i = 0; i < n_per_rank; i++) {\n           double x = points[offset + i].x;\n           double y = points[offset + i].y;\n\n           int quadrant = 0;\n           if (x < 0) quadrant |= 1;\n           if (y < 0) quadrant |= 2;\n           bins[quadrant]++;\n       }\n       counts[rank_id] = bins[quadrant];\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   if (rank_id == 0) {\n       for (int i = 0; i < num_ranks; i++) {\n           MPI_Reduce(&counts[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, i, MPI_COMM_WORLD);\n       }\n   } else {\n       MPI_Reduce(&counts[rank_id], &bins[rank_id], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: fill this in!\n\n}",
            "int size;\n   int rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   //...\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO\n\n   // Example:\n   // bins[0] = 3\n   // bins[1] = 1\n   // bins[2] = 0\n   // bins[3] = 2\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "size_t myCounts[4];\n   MPI_Allreduce(myCounts, bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// Your code goes here\n}",
            "// Your code goes here.\n}",
            "// TODO\n}",
            "}",
            "// TODO: Your code goes here\n\n}",
            "MPI_Status status;\n    bins.fill(0);\n    if (points.size() > 0)\n    {\n        const int size = points.size();\n        const int n = size / 4;\n        const int rest = size % 4;\n        MPI_Send(&size, 1, MPI_INT, 1, 0, MPI_COMM_WORLD);\n        MPI_Send(&points[0], size, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD);\n    }\n    if (points.size() == 0)\n    {\n        int size;\n        MPI_Recv(&size, 1, MPI_INT, 1, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&size, size, MPI_DOUBLE, 1, 0, MPI_COMM_WORLD, &status);\n    }\n    if (points.size() > 0)\n    {\n        const int size = points.size();\n        const int n = size / 4;\n        const int rest = size % 4;\n        MPI_Send(&size, 1, MPI_INT, 2, 0, MPI_COMM_WORLD);\n        MPI_Send(&points[0], size, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD);\n    }\n    if (points.size() == 0)\n    {\n        int size;\n        MPI_Recv(&size, 1, MPI_INT, 2, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&size, size, MPI_DOUBLE, 2, 0, MPI_COMM_WORLD, &status);\n    }\n    if (points.size() > 0)\n    {\n        const int size = points.size();\n        const int n = size / 4;\n        const int rest = size % 4;\n        MPI_Send(&size, 1, MPI_INT, 3, 0, MPI_COMM_WORLD);\n        MPI_Send(&points[0], size, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD);\n    }\n    if (points.size() == 0)\n    {\n        int size;\n        MPI_Recv(&size, 1, MPI_INT, 3, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&size, size, MPI_DOUBLE, 3, 0, MPI_COMM_WORLD, &status);\n    }\n    if (points.size() > 0)\n    {\n        const int size = points.size();\n        const int n = size / 4;\n        const int rest = size % 4;\n        MPI_Send(&size, 1, MPI_INT, 4, 0, MPI_COMM_WORLD);\n        MPI_Send(&points[0], size, MPI_DOUBLE, 4, 0, MPI_COMM_WORLD);\n    }\n    if (points.size() == 0)\n    {\n        int size;\n        MPI_Recv(&size, 1, MPI_INT, 4, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&size, size, MPI_DOUBLE, 4, 0, MPI_COMM_WORLD, &status);\n    }\n}",
            "// Your code here\n}",
            "if (points.size() == 0) return;\n    std::array<int, 4> counts;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        std::array<std::vector<Point>, 4> parts;\n        for (int i = 0; i < size; i++) {\n            MPI_Recv(&counts, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++) {\n                parts[j].insert(parts[j].end(), count[j], points.begin() + j * points.size() / 4);\n            }\n        }\n    }\n    else {\n        counts[0] = std::count_if(points.begin(), points.end(), [](auto p) { return p.x > 0 && p.y > 0; });\n        counts[1] = std::count_if(points.begin(), points.end(), [](auto p) { return p.x < 0 && p.y > 0; });\n        counts[2] = std::count_if(points.begin(), points.end(), [](auto p) { return p.x > 0 && p.y < 0; });\n        counts[3] = std::count_if(points.begin(), points.end(), [](auto p) { return p.x < 0 && p.y < 0; });\n        MPI_Send(&counts, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// Your code goes here!\n}",
            "MPI_Status status;\n   bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n   for (auto const& p : points) {\n      int bin;\n      if (p.x >= 0 && p.y >= 0) {\n         bin = 0;\n      } else if (p.x < 0 && p.y >= 0) {\n         bin = 1;\n      } else if (p.x >= 0 && p.y < 0) {\n         bin = 2;\n      } else {\n         bin = 3;\n      }\n\n      int rank;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n      if (rank == 0) {\n         //rank 0 receives from all other ranks\n         for (int i = 1; i < 4; i++) {\n            int size;\n            MPI_Recv(&size, 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            bins[bin] += size;\n         }\n      } else {\n         // all other ranks send to rank 0\n         MPI_Send(&p.x, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      }\n   }\n}",
            "// TODO: implement me\n}",
            "// TODO\n\n}",
            "const int myrank = mpi::rank(mpi::const_comm_ref(MPI_COMM_WORLD));\n   const int size = mpi::size(mpi::const_comm_ref(MPI_COMM_WORLD));\n\n   // TODO: use `points` and `bins` to count how many points are in each quadrant.\n   //       Store the result in `bins`.\n   //       Assume that the `points` vector is complete on every MPI rank.\n   //       Use `mpi::reduce()` to do the aggregation in parallel.\n\n   // TODO: use `mpi::gatherv()` to collect `bins` into a vector of `bins` on rank 0.\n}",
            "if(points.empty()) return;\n\n   MPI_Datatype p2d;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &p2d);\n   MPI_Type_commit(&p2d);\n\n   const auto N = points.size();\n   MPI_Reduce(MPI_IN_PLACE, &N, 1, MPI_UNSIGNED, MPI_MAX, 0, MPI_COMM_WORLD);\n   if(MPI_Comm_rank(MPI_COMM_WORLD) == 0) {\n      auto b = bins.begin();\n      for(auto i = 0; i < N; i++, b++)\n         *b = 0;\n   }\n\n   const auto r = MPI_Comm_rank(MPI_COMM_WORLD);\n   MPI_Reduce(&points[0], &points[0], N, p2d, MPI_SUM, 0, MPI_COMM_WORLD);\n   if(r == 0) {\n      auto i = 0;\n      auto b = bins.begin();\n      for(; i < N; i++, b++)\n         *b = (i % 2 == 0)? b->x > 0 : b->y > 0;\n   }\n}",
            "// TODO\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   const int n = points.size();\n   int np = n / size;\n   int rp = n % size;\n   int start = rank * np + std::min(rank, rp);\n   int end = (rank + 1) * np + std::min(rank + 1, rp);\n\n   auto localBins = std::array<size_t, 4>{};\n   for (size_t i = start; i < end; ++i)\n   {\n      Point const &p = points[i];\n      int q = 0;\n      if (p.x >= 0)\n      {\n         q |= 1;\n      }\n      if (p.y >= 0)\n      {\n         q |= 2;\n      }\n      ++localBins[q];\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int num_procs, my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    // TODO: complete the implementation\n\n    // bins contains the counts of points in each quadrant\n    // Example: if bins = [1,2,3,4], then there is 1 point in the first quadrant, 2 in the second, 3 in the third, and 4 in the fourth\n}",
            "// Your solution goes here\n   MPI_Bcast(bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "// TODO\n  size_t size = points.size();\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank = 0;\n  int numprocs = 0;\n\n  MPI_Comm_size(comm, &numprocs);\n  MPI_Comm_rank(comm, &rank);\n  MPI_Request *req = new MPI_Request[numprocs-1];\n\n  if (rank == 0) {\n    bins = {0, 0, 0, 0};\n    for (int i=1; i<numprocs; i++) {\n      MPI_Isend(&size, 1, MPI_INT, i, i, comm, &req[i-1]);\n      MPI_Isend(&points[0], size, MPI_DOUBLE, i, i, comm, &req[i-1]);\n    }\n  } else {\n    int rsize;\n    MPI_Status status;\n    std::vector<Point> rpoints(size);\n    MPI_Recv(&rsize, 1, MPI_INT, 0, rank, comm, &status);\n    MPI_Recv(&rpoints[0], rsize, MPI_DOUBLE, 0, rank, comm, &status);\n  }\n\n  if (rank == 0) {\n    for (int i=1; i<numprocs; i++) {\n      MPI_Waitall(numprocs-1, req, MPI_STATUSES_IGNORE);\n      std::vector<Point> rpoints(size);\n      MPI_Recv(&rsize, 1, MPI_INT, i, i, comm, &status);\n      MPI_Recv(&rpoints[0], rsize, MPI_DOUBLE, i, i, comm, &status);\n\n      for (int j=0; j<rsize; j++) {\n        if (rpoints[j].x > 0 && rpoints[j].y > 0) {\n          bins[0]++;\n        } else if (rpoints[j].x < 0 && rpoints[j].y > 0) {\n          bins[1]++;\n        } else if (rpoints[j].x > 0 && rpoints[j].y < 0) {\n          bins[2]++;\n        } else if (rpoints[j].x < 0 && rpoints[j].y < 0) {\n          bins[3]++;\n        }\n      }\n    }\n  }\n\n  delete[] req;\n}",
            "if (bins.size()!= 4) {\n      std::cerr << \"Expecting array of size 4\" << std::endl;\n      return;\n   }\n\n   // TODO\n\n   return;\n}",
            "// TODO: Write code for MPI to count in parallel.\n}",
            "// Your code here.\n}",
            "// TODO\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   int myRank, numRanks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   int count = points.size();\n   int localCount = count / numRanks;\n   int remainder = count % numRanks;\n   int localStart = myRank * localCount;\n   if (myRank == numRanks - 1) {\n      localCount += remainder;\n   }\n   else if (myRank < remainder) {\n      localCount += 1;\n      localStart += myRank;\n   }\n   else {\n      localStart += remainder;\n   }\n\n   for (int i = localStart; i < localStart + localCount; ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[1]++;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n\n   // gather all counts to rank 0\n   std::array<size_t, 4> tempBins;\n   if (myRank == 0) {\n      for (int i = 1; i < numRanks; ++i) {\n         MPI_Recv(&tempBins, 4, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         bins[0] += tempBins[0];\n         bins[1] += tempBins[1];\n         bins[2] += tempBins[2];\n         bins[3] += tempBins[3];\n      }\n   } else {\n      MPI_Send(&bins, 4, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n   }\n}",
            "size_t n = points.size();\n   std::vector<size_t> n_local(n, 0);\n\n   int myrank, numprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n   auto points_begin = points.begin() + n / numprocs * myrank;\n   auto points_end = points.begin() + n / numprocs * (myrank + 1);\n\n   for (auto it = points_begin; it!= points_end; it++) {\n      if (it->x >= 0 && it->y >= 0) {\n         n_local[0]++;\n      } else if (it->x < 0 && it->y >= 0) {\n         n_local[1]++;\n      } else if (it->x >= 0 && it->y < 0) {\n         n_local[2]++;\n      } else if (it->x < 0 && it->y < 0) {\n         n_local[3]++;\n      }\n   }\n\n   MPI_Gather(&n_local, 4, MPI_UNSIGNED_LONG_LONG, &bins, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "// TODO: Replace this line with your code\n    std::fill(bins.begin(), bins.end(), 0);\n}",
            "// TODO: your code here\n}",
            "}",
            "size_t const n_points = points.size();\n\n   int num_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   if (num_ranks > 1) {\n      // TODO\n   }\n   else {\n      bins.fill(0);\n      for (size_t i = 0; i < n_points; ++i) {\n         int const q = (points[i].x >= 0)? (points[i].y >= 0? 0 : 1) : (points[i].y >= 0? 2 : 3);\n         ++bins[q];\n      }\n   }\n}",
            "}",
            "/*\n   The algorithm is:\n\n   split the points into 4 groups (1 for each quadrant) using an MPI_Comm_split\n   on each group, count the number of points\n   use an MPI_Reduce to combine the counts\n   */\n   int numRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   //... rest of function...\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<size_t> bins_local;\n   for (int i = 0; i < 4; i++) {\n      bins_local.push_back(0);\n   }\n   int x_min, x_max, y_min, y_max;\n\n   if (rank == 0) {\n      x_min = points[0].x;\n      y_min = points[0].y;\n      for (int i = 1; i < points.size(); i++) {\n         if (points[i].x < x_min) {\n            x_min = points[i].x;\n         }\n         if (points[i].y < y_min) {\n            y_min = points[i].y;\n         }\n      }\n      x_max = points[0].x;\n      y_max = points[0].y;\n      for (int i = 1; i < points.size(); i++) {\n         if (points[i].x > x_max) {\n            x_max = points[i].x;\n         }\n         if (points[i].y > y_max) {\n            y_max = points[i].y;\n         }\n      }\n   }\n\n   // Send information about min and max\n   if (rank == 0) {\n      MPI_Bcast(&x_min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&x_max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&y_min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&y_max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n   else {\n      MPI_Bcast(&x_min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&x_max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&y_min, 1, MPI_INT, 0, MPI_COMM_WORLD);\n      MPI_Bcast(&y_max, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n\n   // Divide the points into each rank's own part\n   for (auto p : points) {\n      int index = 0;\n      if (p.x >= 0 && p.y >= 0) {\n         index = 1;\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         index = 2;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         index = 3;\n      }\n      else {\n         index = 0;\n      }\n      bins_local[index]++;\n   }\n\n   // Sum the counts from all ranks\n   MPI_Reduce(bins_local.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   return;\n}",
            "// TODO\n}",
            "MPI_Status status;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: your code here\n}",
            "//TODO: Replace this with your implementation.\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<size_t> my_bins(4, 0);\n    for(auto const& p : points) {\n        int bin = 0;\n        if (p.x < 0 && p.y < 0) {\n            bin = 3;\n        }\n        else if (p.x < 0) {\n            bin = 2;\n        }\n        else if (p.y < 0) {\n            bin = 1;\n        }\n\n        my_bins[bin]++;\n    }\n\n    std::vector<size_t> all_bins(4*world_size, 0);\n    MPI_Gather(my_bins.data(), 4, MPI_UNSIGNED_LONG,\n        all_bins.data(), 4, MPI_UNSIGNED_LONG,\n        0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        for(int i = 0; i < 4*world_size; i+=4) {\n            bins[0] += all_bins[i];\n            bins[1] += all_bins[i+1];\n            bins[2] += all_bins[i+2];\n            bins[3] += all_bins[i+3];\n        }\n    }\n}",
            "}",
            "}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 4> bins_local;\n    bins_local.fill(0);\n\n    // Do something\n\n    // Add results\n    MPI_Reduce(bins_local.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Add your code here\n   MPI_Init(NULL, NULL);\n   int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   // Calculate the points per rank\n   auto per_rank = points.size() / world_size;\n   auto remainder = points.size() % world_size;\n   // Check if the rank has a remainder\n   int has_remainder = (world_rank < remainder);\n   // Calculate how many points the rank has\n   int count = per_rank + has_remainder;\n\n   // Allocate points for this rank\n   std::vector<Point> local_points(count);\n\n   // Copy points from global to local\n   MPI_Scatter(points.data(), count, MPI_DOUBLE, local_points.data(), count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   // Count points in the quadrant\n   size_t count_1 = 0;\n   size_t count_2 = 0;\n   size_t count_3 = 0;\n   size_t count_4 = 0;\n\n   for (int i = 0; i < count; i++) {\n      Point point = local_points[i];\n      if (point.x >= 0 && point.y >= 0) {\n         count_1 += 1;\n      } else if (point.x < 0 && point.y >= 0) {\n         count_2 += 1;\n      } else if (point.x >= 0 && point.y < 0) {\n         count_3 += 1;\n      } else {\n         count_4 += 1;\n      }\n   }\n\n   // Combine all counts into one\n   int total_count[4] = { 0 };\n   MPI_Reduce(count_1, total_count, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Assign counts to the bins\n   bins[0] = total_count[0];\n   bins[1] = total_count[1];\n   bins[2] = total_count[2];\n   bins[3] = total_count[3];\n\n   MPI_Finalize();\n}",
            "// Implement this function.\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // each process takes a slice of the points and processes it\n    size_t pts_per_proc = points.size() / num_procs;\n    size_t start_index = rank * pts_per_proc;\n    size_t end_index = start_index + pts_per_proc;\n\n    // initialize array\n    std::array<size_t, 4> bins_local = { 0, 0, 0, 0 };\n\n    for(size_t i = start_index; i < end_index; ++i) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            ++bins_local[0];\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            ++bins_local[1];\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            ++bins_local[2];\n        } else {\n            ++bins_local[3];\n        }\n    }\n\n    // collect results\n    if (rank == 0) {\n        for(int i = 1; i < num_procs; ++i) {\n            int buf[4];\n            MPI_Recv(buf, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            bins_local[0] += buf[0];\n            bins_local[1] += buf[1];\n            bins_local[2] += buf[2];\n            bins_local[3] += buf[3];\n        }\n    } else {\n        int buf[4] = { bins_local[0], bins_local[1], bins_local[2], bins_local[3] };\n        MPI_Send(buf, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        bins = bins_local;\n    }\n}",
            "MPI_Status status;\n    int N = points.size();\n    // This is the number of quadrants\n    int R = 4;\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    // Partition the points among the ranks\n    int N_per_process = N/nprocs;\n    std::vector<Point> my_points(points.begin() + rank * N_per_process,\n                                 points.begin() + (rank+1) * N_per_process);\n\n    std::array<size_t, R> local_bins;\n    local_bins.fill(0);\n    for(Point point : my_points) {\n        if(point.x > 0) {\n            if(point.y > 0)\n                local_bins[0] += 1;\n            else\n                local_bins[1] += 1;\n        } else {\n            if(point.y > 0)\n                local_bins[2] += 1;\n            else\n                local_bins[3] += 1;\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), R, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size;\n   int rank;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<int> locBins(4);\n\n   MPI_Datatype typePoint;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &typePoint);\n   MPI_Type_commit(&typePoint);\n\n   int numberOfPoints = points.size();\n\n   MPI_Bcast(&numberOfPoints, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   MPI_Bcast(&points[0], numberOfPoints, typePoint, 0, MPI_COMM_WORLD);\n\n   MPI_Type_free(&typePoint);\n\n   int sizeOfBin = numberOfPoints / size;\n   int remainder = numberOfPoints % size;\n\n   for (int i = rank * sizeOfBin; i < (rank + 1) * sizeOfBin; i++)\n   {\n      if (points[i].x > 0 && points[i].y > 0)\n      {\n         locBins[0]++;\n      }\n      else if (points[i].x < 0 && points[i].y > 0)\n      {\n         locBins[1]++;\n      }\n      else if (points[i].x > 0 && points[i].y < 0)\n      {\n         locBins[2]++;\n      }\n      else\n      {\n         locBins[3]++;\n      }\n   }\n\n   if (rank == 0)\n   {\n      for (int i = 0; i < remainder; i++)\n      {\n         if (points[i].x > 0 && points[i].y > 0)\n         {\n            locBins[0]++;\n         }\n         else if (points[i].x < 0 && points[i].y > 0)\n         {\n            locBins[1]++;\n         }\n         else if (points[i].x > 0 && points[i].y < 0)\n         {\n            locBins[2]++;\n         }\n         else\n         {\n            locBins[3]++;\n         }\n      }\n   }\n\n   MPI_Gather(&locBins[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int myRank, nRanks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n   int myBin[4];\n\n   MPI_Reduce(&myBin, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "if (points.empty()) {\n      return;\n   }\n\n   int rank, nproc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n   // TODO: Implement\n}",
            "// TODO: Your code goes here\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "int rank, p;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    std::array<size_t, 4> local_bins;\n    local_bins.fill(0);\n    for(int i=0; i<points.size(); i++){\n        if (points[i].x < 0 && points[i].y < 0)\n            local_bins[0]++;\n        if (points[i].x >= 0 && points[i].y < 0)\n            local_bins[1]++;\n        if (points[i].x < 0 && points[i].y >= 0)\n            local_bins[2]++;\n        if (points[i].x >= 0 && points[i].y >= 0)\n            local_bins[3]++;\n    }\n\n    std::array<size_t, 4> global_bins;\n    global_bins.fill(0);\n\n    MPI_Reduce(&local_bins, &global_bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0)\n        bins = global_bins;\n}",
            "// Fill in your code here\n}",
            "// Your code here\n}",
            "// your code goes here\n}",
            "auto countInQuadrant = [](Point const& p) -> int {\n      if (p.x > 0 && p.y > 0) {\n         return 0;\n      }\n      else if (p.x < 0 && p.y > 0) {\n         return 1;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         return 2;\n      }\n      else {\n         return 3;\n      }\n   };\n\n   auto numPoints = points.size();\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO\n\n   if (rank == 0) {\n      for (int i = 1; i < size; ++i) {\n         int localCounts[4];\n         MPI_Recv(localCounts, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j = 0; j < 4; ++j) {\n            bins[j] += localCounts[j];\n         }\n      }\n   }\n   else {\n      int localCounts[4] = {0};\n      for (int i = 0; i < numPoints; ++i) {\n         auto q = countInQuadrant(points[i]);\n         localCounts[q] += 1;\n      }\n      MPI_Send(localCounts, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// your code here\n}",
            "size_t num = points.size();\n    std::array<size_t, 4> counts;\n    counts.fill(0);\n    for(auto &p : points) {\n        if(p.x >= 0 && p.y >= 0) {\n            counts[0]++;\n        }\n        if(p.x < 0 && p.y >= 0) {\n            counts[1]++;\n        }\n        if(p.x < 0 && p.y < 0) {\n            counts[2]++;\n        }\n        if(p.x >= 0 && p.y < 0) {\n            counts[3]++;\n        }\n    }\n    if(bins.size()!= counts.size()) {\n        throw std::runtime_error(\"counts.size()!= bins.size()\");\n    }\n    std::vector<size_t> send_counts(counts.size());\n    for(size_t i = 0; i < counts.size(); i++) {\n        send_counts[i] = counts[i];\n    }\n    std::vector<size_t> offsets(send_counts.size());\n    size_t offset = 0;\n    for(size_t i = 0; i < send_counts.size(); i++) {\n        offsets[i] = offset;\n        offset += send_counts[i];\n    }\n    std::vector<size_t> recv_counts(send_counts.size());\n    MPI_Gather(&send_counts[0], send_counts.size(), MPI_INT, &recv_counts[0], send_counts.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    if(MPI_COMM_WORLD->rank == 0) {\n        for(size_t i = 1; i < recv_counts.size(); i++) {\n            recv_counts[0] += recv_counts[i];\n        }\n        if(recv_counts[0]!= num) {\n            throw std::runtime_error(\"recv_counts[0]!= num\");\n        }\n        for(size_t i = 1; i < offsets.size(); i++) {\n            offsets[0] += offsets[i];\n        }\n        bins.fill(0);\n        for(size_t i = 0; i < send_counts.size(); i++) {\n            for(size_t j = 0; j < send_counts[i]; j++) {\n                if(points[j + offsets[i]].x >= 0 && points[j + offsets[i]].y >= 0) {\n                    bins[0]++;\n                }\n                if(points[j + offsets[i]].x < 0 && points[j + offsets[i]].y >= 0) {\n                    bins[1]++;\n                }\n                if(points[j + offsets[i]].x < 0 && points[j + offsets[i]].y < 0) {\n                    bins[2]++;\n                }\n                if(points[j + offsets[i]].x >= 0 && points[j + offsets[i]].y < 0) {\n                    bins[3]++;\n                }\n            }\n        }\n    }\n}",
            "const int size = points.size();\n\n    // TODO: Initialize bins with zero\n    for(int i=0; i<4; ++i){\n        bins[i] = 0;\n    }\n\n    // TODO: Count the number of points in each quadrant\n    //  - Note that the bins are 0-indexed\n    //  - Use the MPI reduction functions `MPI_Reduce` or `MPI_Allreduce`\n    //  - `MPI_Reduce` is called on each rank, `MPI_Allreduce` is called on rank 0.\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int size, rank;\n   MPI_Comm_size(comm, &size);\n   MPI_Comm_rank(comm, &rank);\n\n   // YOUR CODE HERE\n}",
            "// 1. Initialize the bins to 0.\n   // 2. Divide the input data into 4 parts based on the quadrant.\n   // 3. Count the number of points in each quadrant.\n   // 4. Assemble the counts on rank 0.\n\n\n   int size;\n   int rank;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   MPI_Datatype MPI_Point;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &MPI_Point);\n   MPI_Type_commit(&MPI_Point);\n\n   MPI_Bcast(&size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   int sub_size = points.size() / size;\n   int sub_rank = rank;\n\n   std::vector<Point> local_data(sub_size);\n   int count = 0;\n\n   if(sub_size * sub_rank < points.size()) {\n      for(int i = sub_size * sub_rank; i < sub_size * (sub_rank + 1); i++) {\n         if(i < points.size()) {\n            local_data[count] = points[i];\n            count++;\n         }\n      }\n   }\n\n   int local_counts[4] = {0};\n   for(int i = 0; i < count; i++) {\n      if(local_data[i].x >= 0 && local_data[i].y >= 0) local_counts[0]++;\n      if(local_data[i].x < 0 && local_data[i].y >= 0) local_counts[1]++;\n      if(local_data[i].x < 0 && local_data[i].y < 0) local_counts[2]++;\n      if(local_data[i].x >= 0 && local_data[i].y < 0) local_counts[3]++;\n   }\n\n   int global_counts[4] = {0};\n   MPI_Gather(&local_counts, 4, MPI_INT, global_counts, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if(rank == 0) {\n      for(int i = 0; i < 4; i++) {\n         bins[i] = global_counts[i];\n      }\n   }\n\n   MPI_Type_free(&MPI_Point);\n}",
            "auto countQuadrant = [](Point p) {\n      return 4*(p.x < 0) + 2*(p.y < 0) + (p.x < 0 && p.y < 0);\n   };\n\n   bins = {0, 0, 0, 0};\n   for (auto& p : points) {\n      ++bins[countQuadrant(p)];\n   }\n}",
            "const int size = points.size();\n   const int rank = 0;\n   const int numRanks = 1;\n   MPI_Comm comm = MPI_COMM_WORLD;\n   MPI_Status status;\n\n   // TODO: Implement\n   MPI_Scatter(&size,1,MPI_INT,&size,1,MPI_INT,0,comm);\n   std::vector<Point> myPoints(size/numRanks);\n   MPI_Scatterv(&points[0],&size,MPI_INT,&myPoints[0],&size,MPI_INT,0,comm);\n   //\n   for (int i=0;i<size/numRanks;i++)\n   {\n    if (myPoints[i].x>=0 && myPoints[i].y>=0)\n     {\n       bins[0]++;\n     }\n    else if(myPoints[i].x<0 && myPoints[i].y>=0)\n     {\n       bins[1]++;\n     }\n    else if(myPoints[i].x<0 && myPoints[i].y<0)\n     {\n       bins[2]++;\n     }\n    else if(myPoints[i].x>=0 && myPoints[i].y<0)\n     {\n       bins[3]++;\n     }\n   }\n   //\n   MPI_Gather(&bins[0],1,MPI_INT,&bins[0],1,MPI_INT,0,comm);\n}",
            "// Your code here\n}",
            "// TODO\n  int size;\n  int rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = points.size();\n\n  std::vector<Point> local_points(points.begin() + rank * n / size, points.begin() + (rank + 1) * n / size);\n\n  std::array<size_t, 4> local_bins;\n  local_bins.fill(0);\n\n  for (Point point : local_points) {\n    if (point.x >= 0 && point.y >= 0) {\n      local_bins[0]++;\n    } else if (point.x >= 0 && point.y < 0) {\n      local_bins[1]++;\n    } else if (point.x < 0 && point.y >= 0) {\n      local_bins[2]++;\n    } else if (point.x < 0 && point.y < 0) {\n      local_bins[3]++;\n    }\n  }\n\n  std::array<size_t, 4> global_bins;\n  global_bins.fill(0);\n\n  MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "// TODO\n}",
            "int rank;\n   int num_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: Implement this function.\n   // Count the number of cartesian points in each quadrant.\n   // Store the counts in bins.\n   // Use MPI to count in parallel.\n\n   // You can use the `countPoints` function to count the points in a single quadrant.\n   // You may assume that there are 4 quadrants (0, 1, 2, 3), but no need to use these\n   // values.\n   // Note that the first rank has indices 0..num_points-1, the second 0..num_points/2-1,\n   // etc.\n   // For example, if there are 5 ranks, the first rank will have indices 0, 1, 2, 3, 4,\n   // the second rank will have indices 0, 1, 2, 3, the third rank will have indices 0, 1,\n   // 2, the last rank will have index 0.\n   // For the last rank you can assume the number of points is even.\n}",
            "// TODO: write code here\n  \n  // MPI_Barrier(MPI_COMM_WORLD);\n  // // Counts of points in each quadrant\n  // std::array<size_t, 4> bins = {0, 0, 0, 0};\n\n  // int rank, size;\n  // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // size_t chunk = points.size() / size;\n  // size_t start = rank * chunk;\n  // size_t end = (rank + 1) * chunk;\n  // end = end > points.size()? points.size() : end;\n\n  // for (size_t i = start; i < end; i++) {\n  //   Point const& p = points[i];\n  //   if (p.x > 0 && p.y > 0)\n  //     bins[0]++;\n  //   else if (p.x < 0 && p.y > 0)\n  //     bins[1]++;\n  //   else if (p.x < 0 && p.y < 0)\n  //     bins[2]++;\n  //   else if (p.x > 0 && p.y < 0)\n  //     bins[3]++;\n  // }\n\n  // std::array<size_t, 4> recv_counts;\n  // MPI_Gather(bins.data(), 4, MPI_LONG, recv_counts.data(), 4, MPI_LONG, 0, MPI_COMM_WORLD);\n  // if (rank == 0)\n  //   for (int i = 1; i < size; i++)\n  //     for (int j = 0; j < 4; j++)\n  //       bins[j] += recv_counts[i * 4 + j];\n}",
            "int numtasks, rank, i;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n   int max = points.size();\n   std::vector<int> points_per_process(numtasks, max / numtasks);\n   std::vector<int> leftover(numtasks, 0);\n\n   if (max % numtasks!= 0) {\n      for (int i = 0; i < max % numtasks; i++) {\n         leftover[i] = 1;\n      }\n   }\n\n   for (int i = 0; i < numtasks; i++) {\n      points_per_process[i] += leftover[i];\n   }\n\n   int start = 0;\n   int stop = points_per_process[rank];\n   std::vector<Point> points_per_process_rank;\n\n   for (i = 0; i < points.size(); i++) {\n      if (i >= start && i < stop) {\n         points_per_process_rank.push_back(points[i]);\n      }\n   }\n\n   int local_bins[4] = { 0 };\n\n   for (auto const& point : points_per_process_rank) {\n      if (point.x >= 0 && point.y >= 0) {\n         local_bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         local_bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         local_bins[2]++;\n      } else {\n         local_bins[3]++;\n      }\n   }\n\n   std::vector<int> local_bins_vec(local_bins, local_bins + 4);\n\n   MPI_Gather(local_bins_vec.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n\n   // the results are stored on rank 0, in the array bins\n\n}",
            "// TODO: your code here\n   size_t size = points.size();\n   //std::cout << size;\n   int myrank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   std::vector<Point> p1, p2, p3, p4;\n   for (int i=0; i<size; i++){\n      if (points[i].x > 0 && points[i].y > 0){\n         p1.push_back(points[i]);\n      }\n      if (points[i].x < 0 && points[i].y > 0){\n         p2.push_back(points[i]);\n      }\n      if (points[i].x > 0 && points[i].y < 0){\n         p3.push_back(points[i]);\n      }\n      if (points[i].x < 0 && points[i].y < 0){\n         p4.push_back(points[i]);\n      }\n   }\n   if (myrank == 0){\n      bins[0] = p1.size();\n      bins[1] = p2.size();\n      bins[2] = p3.size();\n      bins[3] = p4.size();\n   }\n   if (myrank == 1){\n      bins[0] = p1.size();\n      bins[1] = p2.size();\n      bins[2] = p3.size();\n      bins[3] = p4.size();\n   }\n   if (myrank == 2){\n      bins[0] = p1.size();\n      bins[1] = p2.size();\n      bins[2] = p3.size();\n      bins[3] = p4.size();\n   }\n   if (myrank == 3){\n      bins[0] = p1.size();\n      bins[1] = p2.size();\n      bins[2] = p3.size();\n      bins[3] = p4.size();\n   }\n   //bins[0] = p1.size();\n   //bins[1] = p2.size();\n   //bins[2] = p3.size();\n   //bins[3] = p4.size();\n   //std::cout << bins[0];\n}",
            "}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    bins = {0, 0, 0, 0};\n  }\n\n  std::array<size_t, 4> local_bins{};\n  for (const auto &p : points) {\n    if (p.x < 0 && p.y < 0) {\n      local_bins[0]++;\n    } else if (p.x >= 0 && p.y >= 0) {\n      local_bins[1]++;\n    } else if (p.x < 0 && p.y >= 0) {\n      local_bins[2]++;\n    } else if (p.x >= 0 && p.y < 0) {\n      local_bins[3]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n    //int i;\n    //for (i = 0; i < 4; ++i) {\n    //    bins[i] = 0;\n    //}\n    //\n    //for (Point const& point : points) {\n    //    int num = 0;\n    //    if (point.x > 0 && point.y > 0) {\n    //        num = 0;\n    //    } else if (point.x < 0 && point.y > 0) {\n    //        num = 1;\n    //    } else if (point.x < 0 && point.y < 0) {\n    //        num = 2;\n    //    } else if (point.x > 0 && point.y < 0) {\n    //        num = 3;\n    //    }\n    //    bins[num] += 1;\n    //}\n\n\n    // Your code here\n    int size;\n    int rank;\n    int bins_[4] = {0, 0, 0, 0};\n    int bins_count = 0;\n    std::array<size_t, 4> bins;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num;\n\n    if (rank == 0) {\n        for (Point const& point : points) {\n            if (point.x > 0 && point.y > 0) {\n                num = 0;\n            } else if (point.x < 0 && point.y > 0) {\n                num = 1;\n            } else if (point.x < 0 && point.y < 0) {\n                num = 2;\n            } else if (point.x > 0 && point.y < 0) {\n                num = 3;\n            }\n            bins[num] += 1;\n            bins_count += 1;\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Gather(bins_, 4, MPI_INT, bins_.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int nprocs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<size_t> localBins(4, 0);\n   if (rank == 0) {\n      for (int i = 1; i < nprocs; i++) {\n         MPI_Send(points.data(), points.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      }\n   } else {\n      MPI_Recv(localBins.data(), 4, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   // count points on rank 0\n   for (auto const &p: points) {\n      if (p.x >= 0 && p.y >= 0) {\n         localBins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         localBins[1]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         localBins[2]++;\n      } else {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // your code here\n\n   int sendcount = 0;\n   if (rank == 0) {\n       int* counts = new int[size];\n       int sum = 0;\n       for (size_t i = 0; i < points.size(); i++) {\n           if (points[i].x > 0 && points[i].y > 0) {\n               sendcount++;\n               sum++;\n           }\n           else if (points[i].x < 0 && points[i].y > 0) {\n               sendcount++;\n               sum++;\n           }\n           else if (points[i].x > 0 && points[i].y < 0) {\n               sendcount++;\n               sum++;\n           }\n           else if (points[i].x < 0 && points[i].y < 0) {\n               sendcount++;\n               sum++;\n           }\n       }\n       counts[0] = sum;\n       MPI_Gather(&sendcount, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n       int start = 0;\n       for (size_t i = 0; i < size; i++) {\n           bins[i] = counts[i];\n           start += counts[i];\n       }\n\n       delete[] counts;\n   }\n   else {\n       int* counts = new int[size];\n       int sum = 0;\n       for (size_t i = 0; i < points.size(); i++) {\n           if (points[i].x > 0 && points[i].y > 0) {\n               sum++;\n           }\n           else if (points[i].x < 0 && points[i].y > 0) {\n               sum++;\n           }\n           else if (points[i].x > 0 && points[i].y < 0) {\n               sum++;\n           }\n           else if (points[i].x < 0 && points[i].y < 0) {\n               sum++;\n           }\n       }\n       counts[rank] = sum;\n       MPI_Gather(&sum, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count, start;\n    count = points.size()/size;\n    start = rank*count;\n\n    if (rank == 0) {\n        bins = {0,0,0,0};\n    }\n\n    std::array<size_t, 4> counts;\n    counts = {0, 0, 0, 0};\n    for (int i = start; i < start+count; i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            counts[0] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            counts[1] += 1;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            counts[2] += 1;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            counts[3] += 1;\n        }\n    }\n\n    MPI_Reduce(&counts, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Add the required MPI code here.\n   // You will need to create a vector of 4 numbers in every rank.\n   // Then, each rank will count the number of points in each quadrant\n   // and store the result in the vector\n\n   MPI_Init(NULL,NULL);\n   int size, rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int numPerRank = (int)points.size()/size;\n\n   std::vector<int> countsPerQuadrant(4);\n   for(int i = 0; i < numPerRank; i++){\n      int x = points[rank*numPerRank + i].x;\n      int y = points[rank*numPerRank + i].y;\n\n      if(x > 0 && y > 0){\n         countsPerQuadrant[0]++;\n      }\n      else if(x < 0 && y > 0){\n         countsPerQuadrant[1]++;\n      }\n      else if(x > 0 && y < 0){\n         countsPerQuadrant[2]++;\n      }\n      else if(x < 0 && y < 0){\n         countsPerQuadrant[3]++;\n      }\n\n   }\n\n   MPI_Gather(countsPerQuadrant.data(),4,MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if(rank == 0){\n      for(int i = 0; i < 4; i++){\n         std::cout<< bins[i] <<'';\n      }\n   }\n\n}",
            "int nproc;\n   int rank;\n\n   // Initialize MPI\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int npoints_per_proc = points.size() / nproc;\n\n   std::vector<Point> points_per_proc(npoints_per_proc);\n\n   MPI_Scatter(&points[0], npoints_per_proc, MPI_DOUBLE, &points_per_proc[0], npoints_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   std::array<size_t, 4> bins_per_proc;\n\n   for (int i = 0; i < points_per_proc.size(); i++) {\n      if (points_per_proc[i].x > 0) {\n         if (points_per_proc[i].y > 0) {\n            bins_per_proc[0] += 1;\n         }\n         else {\n            bins_per_proc[1] += 1;\n         }\n      }\n      else {\n         if (points_per_proc[i].y > 0) {\n            bins_per_proc[2] += 1;\n         }\n         else {\n            bins_per_proc[3] += 1;\n         }\n      }\n   }\n\n   MPI_Gather(&bins_per_proc[0], 4, MPI_SIZE_T, &bins[0], 4, MPI_SIZE_T, 0, MPI_COMM_WORLD);\n}",
            "int rank, p;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &p);\n   if (rank == 0) {\n      bins.fill(0);\n   }\n   // TODO\n}",
            "// TODO\n}",
            "if(points.empty()) {\n      return;\n   }\n   auto pointsPerRank = points.size() / MPI::COMM_WORLD.size();\n   auto reminder = points.size() % MPI::COMM_WORLD.size();\n   auto rank = MPI::COMM_WORLD.rank();\n   auto size = MPI::COMM_WORLD.size();\n   std::vector<Point> myPoints;\n   // Assign points to ranks\n   if(rank == 0) {\n      myPoints.assign(points.begin(), points.begin() + pointsPerRank + reminder);\n   } else {\n      myPoints.assign(points.begin() + rank * pointsPerRank, points.begin() + rank * pointsPerRank + pointsPerRank);\n   }\n\n   std::array<size_t, 4> myBins = {0, 0, 0, 0};\n   for(auto p : myPoints) {\n      auto q = (p.x < 0) + (p.y < 0);\n      myBins[q]++;\n   }\n   MPI::COMM_WORLD.Allreduce(myBins.data(), bins.data(), bins.size(), MPI::UNSIGNED_LONG_LONG);\n}",
            "if (points.empty()) {\n    return;\n  }\n\n  const auto count = points.size();\n  std::array<size_t, 4> counts = { 0, 0, 0, 0 };\n  auto const& [num_ranks, my_rank] = mpi::initialized();\n  auto const chunk = (count + num_ranks - 1) / num_ranks;\n  auto const offset = chunk * my_rank;\n  auto const limit = std::min(offset + chunk, count);\n  for (auto i = offset; i < limit; ++i) {\n    auto const& [x, y] = points[i];\n    if (x >= 0.0 && y >= 0.0) {\n      ++counts[0];\n    } else if (x < 0.0 && y >= 0.0) {\n      ++counts[1];\n    } else if (x < 0.0 && y < 0.0) {\n      ++counts[2];\n    } else {\n      ++counts[3];\n    }\n  }\n  if (my_rank == 0) {\n    std::fill(std::begin(bins), std::end(bins), 0);\n  }\n  mpi::reduce(bins, counts, std::plus<size_t>{}, 0);\n}",
            "// YOUR CODE GOES HERE\n\n}",
            "}",
            "// TODO\n}",
            "// TODO: your code here\n   MPI_Bcast(bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "auto const& comm = MPI_COMM_WORLD;\n   auto const& rank = MPI_Comm_rank(comm);\n   auto const& nRanks = MPI_Comm_size(comm);\n\n   bins.fill(0);\n   auto nLocalPoints = points.size();\n   for (size_t i = 0; i < nLocalPoints; i++) {\n      auto const& point = points[i];\n      // TODO: count in which quadrant the point belongs and add 1 to the respective bin\n   }\n\n   for (int i = 1; i < nRanks; i++) {\n      // TODO: gather the bins of other ranks\n   }\n}",
            "// TODO: implement\n\n}",
            "int world_size;\n    int my_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    // Compute the number of points to send to each process\n    std::vector<size_t> num_points_per_proc(world_size, 0);\n    for (size_t i = 0; i < points.size(); ++i) {\n        int proc = i % world_size;\n        ++num_points_per_proc[proc];\n    }\n\n    std::vector<int> num_points_per_proc_int(num_points_per_proc.begin(), num_points_per_proc.end());\n    std::vector<int> recvcounts(num_points_per_proc_int.size());\n    std::vector<int> displs(num_points_per_proc_int.size());\n\n    // Compute the displacements\n    int count = 0;\n    for (size_t i = 0; i < recvcounts.size(); ++i) {\n        recvcounts[i] = num_points_per_proc_int[i];\n        displs[i] = count;\n        count += num_points_per_proc[i];\n    }\n\n    // Perform the MPI calls\n    std::vector<int> counts(bins.size());\n    MPI_Scatterv(num_points_per_proc_int.data(), recvcounts.data(), displs.data(), MPI_INT, counts.data(), counts.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::vector<Point> points_on_proc;\n    for (size_t i = 0; i < counts.size(); ++i) {\n        for (int j = 0; j < counts[i]; ++j) {\n            points_on_proc.push_back(points[displs[i] + j]);\n        }\n    }\n\n    // Compute the quadrant bins\n    std::array<size_t, 4> counts_per_bin = {0, 0, 0, 0};\n    for (size_t i = 0; i < points_on_proc.size(); ++i) {\n        int quad = 0;\n        if (points_on_proc[i].x > 0 && points_on_proc[i].y > 0) {\n            quad = 1;\n        } else if (points_on_proc[i].x < 0 && points_on_proc[i].y > 0) {\n            quad = 2;\n        } else if (points_on_proc[i].x > 0 && points_on_proc[i].y < 0) {\n            quad = 3;\n        }\n        ++counts_per_bin[quad];\n    }\n\n    // Gather the counts\n    MPI_Gatherv(counts_per_bin.data(), counts_per_bin.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), counts.data(), displs.data(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "// your code goes here\n}",
            "// TODO\n}",
            "size_t bins_sizes[4] = {0, 0, 0, 0};\n   int number_of_processes;\n   MPI_Comm_size(MPI_COMM_WORLD, &number_of_processes);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<size_t> local_bins(4, 0);\n   for (auto& p: points) {\n      int index = p.x < 0? p.y < 0? 0 : 1 : p.y < 0? 2 : 3;\n      if (rank == 0) bins_sizes[index]++;\n      local_bins[index]++;\n   }\n   // Send the values from all processes to rank 0\n   int* bins_sizes_array = new int[4];\n   MPI_Gather(local_bins.data(), 4, MPI_INT, bins_sizes_array, 4, MPI_INT, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) bins[i] = bins_sizes[i];\n   }\n   delete[] bins_sizes_array;\n}",
            "}",
            "//\n   // Your code here\n   //\n}",
            "// Your code goes here\n   //\n\n}",
            "// TODO: add your code here\n}",
            "const int rank = 0;\n    const int size = 0;\n    int num_points;\n\n    if (rank == 0) {\n        num_points = points.size();\n        MPI_Bcast(&num_points, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        MPI_Bcast(points.data(), num_points, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n    else {\n        MPI_Bcast(&num_points, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        points.resize(num_points);\n        MPI_Bcast(points.data(), num_points, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n\n    for (size_t i = 0; i < points.size(); i++) {\n        int ix = (points[i].x < 0)? 0 : (points[i].x > 0)? 1 : 2;\n        int iy = (points[i].y < 0)? 0 : (points[i].y > 0)? 2 : 1;\n        int bin = iy + ix * 2;\n        MPI_Allreduce(&bin, &bins[bin], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < bins.size(); i++) {\n            printf(\"bin %d has %lu points\\n\", i, bins[i]);\n        }\n    }\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n   MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: count the number of cartesian points in each quadrant\n   bins.fill(0);\n\n   int size, rank;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // calculate number of points for each rank\n   size_t points_per_rank = points.size() / size;\n   size_t points_left = points.size() - (points_per_rank * size);\n   size_t start_index = rank * points_per_rank;\n   size_t end_index = start_index + points_per_rank;\n\n   // check if there are any leftover points\n   if (rank < points_left) {\n      end_index++;\n   } else if (rank >= points_left) {\n      start_index += points_left;\n   }\n\n   // compute local counts\n   std::array<size_t, 4> local_bins;\n   local_bins.fill(0);\n\n   for (size_t i = start_index; i < end_index; i++) {\n      if (points[i].x < 0 && points[i].y < 0) {\n         local_bins[0]++;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         local_bins[1]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         local_bins[2]++;\n      } else if (points[i].x >= 0 && points[i].y >= 0) {\n         local_bins[3]++;\n      }\n   }\n\n   // combine counts\n   if (rank == 0) {\n      for (int i = 1; i < size; i++) {\n         size_t temp[4];\n         MPI_Recv(temp, 4, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n         for (int j = 0; j < 4; j++) {\n            bins[j] += temp[j];\n         }\n      }\n   } else {\n      MPI_Send(&local_bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO: Your code here\n}",
            "}",
            "// Your code here\n\n\n}",
            "bins.fill(0);\n   for (Point p: points) {\n       int index = 0;\n       if (p.x >= 0 && p.y >= 0)\n           index = 0;\n       else if (p.x < 0 && p.y >= 0)\n           index = 1;\n       else if (p.x < 0 && p.y < 0)\n           index = 2;\n       else\n           index = 3;\n       ++bins[index];\n   }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Calculate the number of points to send to each rank\n    int nPoints = points.size();\n    int pointsPerRank = nPoints / size;\n    int pointsLeft = nPoints % size;\n\n    // Send points to each rank\n    std::vector<Point> myPoints;\n    MPI_Scatter(points.data(), pointsPerRank, MPI_INT, myPoints.data(), pointsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        MPI_Send(points.data() + pointsPerRank * size, pointsLeft, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Count the points in each quadrant\n    std::array<size_t, 4> myBins;\n    for (int i = 0; i < myPoints.size(); ++i) {\n        if (myPoints[i].x >= 0 && myPoints[i].y >= 0) {\n            myBins[0]++;\n        } else if (myPoints[i].x < 0 && myPoints[i].y >= 0) {\n            myBins[1]++;\n        } else if (myPoints[i].x >= 0 && myPoints[i].y < 0) {\n            myBins[2]++;\n        } else if (myPoints[i].x < 0 && myPoints[i].y < 0) {\n            myBins[3]++;\n        }\n    }\n\n    // Reduce the results\n    std::array<size_t, 4> allBins;\n    MPI_Reduce(myBins.data(), allBins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Write results\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i) {\n            std::cout << allBins[i] <<'';\n        }\n        std::cout << '\\n';\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "__shared__ size_t shared_x[4];\n    __shared__ size_t shared_y[4];\n\n    size_t idx = threadIdx.x;\n    size_t idy = threadIdx.y;\n\n    // first element of shared memory\n    shared_x[0] = 0;\n    shared_y[0] = 0;\n    __syncthreads();\n\n    // first element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // second element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // second element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // third element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // third element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // fourth element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // fourth element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // third element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // third element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // second element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // second element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // first element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // first element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // first element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // first element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0] + 1);\n    atomicAdd(&shared_y[0], shared_y[0] + 1);\n    __syncthreads();\n\n    // first element of shared memory\n    atomicAdd(&shared_x[0], shared_x[0",
            "// Find this thread's index.\n    int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index >= N) { return; }\n\n    // Do some computation.\n    if (points[index].x > 0 && points[index].y > 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (points[index].x <= 0 && points[index].y > 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (points[index].x <= 0 && points[index].y <= 0) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO: fill this in\n    int myBin = 0;\n    int myIndex = blockIdx.x*blockDim.x + threadIdx.x;\n    if(myIndex < N) {\n        if (points[myIndex].x >= 0.0 && points[myIndex].y >= 0.0)\n            myBin = 0;\n        else if (points[myIndex].x < 0.0 && points[myIndex].y >= 0.0)\n            myBin = 1;\n        else if (points[myIndex].x < 0.0 && points[myIndex].y < 0.0)\n            myBin = 2;\n        else\n            myBin = 3;\n        atomicAdd(&bins[myBin], 1);\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (index < N) {\n        auto point = points[index];\n        bins[1 + ((point.x >= 0) * 2 + (point.y >= 0))]++;\n    }\n}",
            "// TODO: implement\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N)\n    return;\n\n  // TODO\n\n  // Add your code here\n\n}",
            "// your code goes here\n\n}",
            "// TODO\n}",
            "}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n\n  const Point& p = points[idx];\n  size_t q;\n  if (p.x >= 0 && p.y >= 0) q = 0;\n  else if (p.x < 0 && p.y >= 0) q = 1;\n  else if (p.x < 0 && p.y < 0) q = 2;\n  else q = 3;\n\n  atomicAdd(&bins[q], 1);\n}",
            "const size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      int bin = 0;\n      if (points[i].x > 0) bin += 1;\n      if (points[i].y > 0) bin += 2;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO: your code here\n   int i = threadIdx.x + blockIdx.x * blockDim.x;\n   double x = points[i].x;\n   double y = points[i].y;\n   if (x >= 0 && y >= 0) {\n     atomicAdd(bins, 0, 1);\n   }\n   if (x < 0 && y >= 0) {\n     atomicAdd(bins, 1, 1);\n   }\n   if (x < 0 && y < 0) {\n     atomicAdd(bins, 2, 1);\n   }\n   if (x >= 0 && y < 0) {\n     atomicAdd(bins, 3, 1);\n   }\n}",
            "// 1. Determine which quadrant this thread belongs to.\n    //    For this example we'll use 0, 1, 2, 3\n    // 2. Determine what array index you should increment using the thread's ID\n    // 3. Increment the corresponding array index by 1\n}",
            "//...\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n   unsigned int n = (blockDim.x * gridDim.x);\n   unsigned int index = i;\n   size_t bin = 0;\n\n   while (index < N) {\n      Point p = points[index];\n      if (p.x >= 0 && p.y >= 0) {\n         bin = 0;\n      } else if (p.x < 0 && p.y >= 0) {\n         bin = 1;\n      } else if (p.x < 0 && p.y < 0) {\n         bin = 2;\n      } else if (p.x >= 0 && p.y < 0) {\n         bin = 3;\n      }\n\n      atomicAdd(&bins[bin], 1);\n\n      index += n;\n   }\n}",
            "// TODO\n}",
            "int index = blockIdx.x*blockDim.x + threadIdx.x;\n   int count = 0;\n\n   if (index < N) {\n     if (points[index].x > 0 && points[index].y > 0) count = 1;\n     else if (points[index].x < 0 && points[index].y > 0) count = 2;\n     else if (points[index].x < 0 && points[index].y < 0) count = 3;\n     else if (points[index].x > 0 && points[index].y < 0) count = 4;\n\n     atomicAdd(&bins[count], 1);\n   }\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if (thread_id < N) {\n      int quadrant = (points[thread_id].x > 0? 0 : 1) | (points[thread_id].y > 0? 0 : 2);\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "size_t i = blockDim.x*blockIdx.x + threadIdx.x;\n   size_t bin[4];\n   bin[0] = bin[1] = bin[2] = bin[3] = 0;\n\n   if (i < N) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bin[0] = 1;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bin[1] = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bin[2] = 1;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bin[3] = 1;\n      }\n   }\n\n   atomicAdd(&bins[0], bin[0]);\n   atomicAdd(&bins[1], bin[1]);\n   atomicAdd(&bins[2], bin[2]);\n   atomicAdd(&bins[3], bin[3]);\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      int bin = 0;\n      if (points[idx].x >= 0.0) {\n         if (points[idx].y >= 0.0) {\n            bin = 1;\n         } else {\n            bin = 2;\n         }\n      } else {\n         if (points[idx].y >= 0.0) {\n            bin = 3;\n         } else {\n            bin = 4;\n         }\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   int nb = blockDim.x;\n\n   if (tid < N && bid == 0) {\n      int q = 0;\n      if (points[tid].x < 0.0 && points[tid].y < 0.0) {\n         q = 0;\n      } else if (points[tid].x > 0.0 && points[tid].y < 0.0) {\n         q = 1;\n      } else if (points[tid].x < 0.0 && points[tid].y > 0.0) {\n         q = 2;\n      } else if (points[tid].x > 0.0 && points[tid].y > 0.0) {\n         q = 3;\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) return;\n\n   double x = points[idx].x;\n   double y = points[idx].y;\n   int q = 0;\n   if (x >= 0.0) q += 1;\n   if (y >= 0.0) q += 2;\n   atomicAdd(&bins[q], 1);\n}",
            "//...\n}",
            "}",
            "// Set each thread to write its own bin index\n   int bin = threadIdx.x;\n   // TODO: Count the number of points in each bin.\n\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t xBin = (points[i].x > 0)? 0 : (points[i].x < 0)? 1 : 2;\n        size_t yBin = (points[i].y > 0)? 0 : (points[i].y < 0)? 1 : 2;\n        atomicAdd(&bins[yBin*2 + xBin], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n   if(tid < N) {\n     if(points[tid].x > 0 && points[tid].y > 0) bins[0] += 1;\n     if(points[tid].x < 0 && points[tid].y > 0) bins[1] += 1;\n     if(points[tid].x < 0 && points[tid].y < 0) bins[2] += 1;\n     if(points[tid].x > 0 && points[tid].y < 0) bins[3] += 1;\n   }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if(tid < N) {\n        if (points[tid].x > 0) {\n            if (points[tid].y > 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[1], 1);\n            }\n        } else {\n            if (points[tid].y > 0) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N)\n      return;\n\n   // Your code here\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      int quadrant = 0;\n      if (points[idx].x > 0) {\n         if (points[idx].y > 0) {\n            quadrant = 1;\n         } else {\n            quadrant = 4;\n         }\n      } else {\n         if (points[idx].y > 0) {\n            quadrant = 2;\n         } else {\n            quadrant = 3;\n         }\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// Set the value of each element in bins to 0\n   __syncthreads();\n\n   // Determine which quadrant each point is in and add 1 to the corresponding element in bins.\n   // Use atomicAdd() to ensure that each increment is thread-safe.\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      if (points[idx].x < 0 && points[idx].y < 0) {\n         atomicAdd(&bins[0], 1);\n      }\n      if (points[idx].x >= 0 && points[idx].y < 0) {\n         atomicAdd(&bins[1], 1);\n      }\n      if (points[idx].x < 0 && points[idx].y >= 0) {\n         atomicAdd(&bins[2], 1);\n      }\n      if (points[idx].x >= 0 && points[idx].y >= 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: Your code here\n}",
            "// Use `i` to access individual points\n   const Point p = points[i];\n\n   // Use `blockIdx` and `threadIdx` to compute the quadrant.\n   // `bins[q]` should be incremented with `atomicAdd` to make this concurrent\n   // Your implementation here\n}",
            "int tid = blockIdx.x*blockDim.x + threadIdx.x;\n  if (tid < N) {\n    Point p = points[tid];\n    if (p.x > 0 && p.y > 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (p.x < 0 && p.y > 0) {\n      atomicAdd(&bins[1], 1);\n    } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n    } else if (p.x > 0 && p.y < 0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int tid = blockIdx.x*blockDim.x + threadIdx.x;\n   if (tid < N) {\n      if (points[tid].x > 0) {\n         if (points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else if (points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else if (points[tid].x < 0) {\n         if (points[tid].y > 0) {\n            atomicAdd(&bins[3], 1);\n         } else if (points[tid].y < 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      } else {\n         atomicAdd(&bins[1], 1);\n      }\n   }\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        Point p = points[idx];\n        if (p.x >= 0 && p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (p.x < 0 && p.y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (p.x < 0 && p.y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (p.x >= 0 && p.y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N)\n    return;\n\n  Point p = points[idx];\n  if (p.x >= 0 && p.y >= 0)\n    atomicAdd(&bins[0], 1);\n  else if (p.x <= 0 && p.y >= 0)\n    atomicAdd(&bins[1], 1);\n  else if (p.x <= 0 && p.y <= 0)\n    atomicAdd(&bins[2], 1);\n  else if (p.x >= 0 && p.y <= 0)\n    atomicAdd(&bins[3], 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= N) return;\n   Point p = points[i];\n   if (p.x >= 0 && p.y >= 0)\n       atomicAdd(&bins[0], 1);\n   else if (p.x < 0 && p.y >= 0)\n       atomicAdd(&bins[1], 1);\n   else if (p.x < 0 && p.y < 0)\n       atomicAdd(&bins[2], 1);\n   else if (p.x >= 0 && p.y < 0)\n       atomicAdd(&bins[3], 1);\n}",
            "// TODO: Your code here\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id >= N) return;\n\n    size_t x = points[id].x;\n    size_t y = points[id].y;\n\n    if (x > 0 && y > 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (x < 0 && y > 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (x > 0 && y < 0) {\n        atomicAdd(&bins[2], 1);\n    } else if (x < 0 && y < 0) {\n        atomicAdd(&bins[3], 1);\n    }\n\n}",
            "}",
            "for (size_t i=blockIdx.x*blockDim.x+threadIdx.x; i<N; i+=blockDim.x*gridDim.x) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (points[i].x <= 0 && points[i].y >= 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (points[i].x >= 0 && points[i].y <= 0) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n   if(tid >= N)\n      return;\n\n   if(points[tid].x >= 0 && points[tid].y >= 0)\n      atomicAdd(&bins[0], 1);\n   else if(points[tid].x < 0 && points[tid].y >= 0)\n      atomicAdd(&bins[1], 1);\n   else if(points[tid].x < 0 && points[tid].y < 0)\n      atomicAdd(&bins[2], 1);\n   else if(points[tid].x >= 0 && points[tid].y < 0)\n      atomicAdd(&bins[3], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      int quad = 0;\n      if (points[idx].x >= 0) {\n         if (points[idx].y >= 0) quad = 1;\n         else quad = 2;\n      } else {\n         if (points[idx].y >= 0) quad = 4;\n         else quad = 3;\n      }\n      atomicAdd(&bins[quad], 1);\n   }\n}",
            "// TODO: Your code here\n}",
            "// write your code here\n}",
            "/* Each thread works on a different point.\n      Use an if-else-if-else ladder to determine in which quadrant a point lies\n      and increment the corresponding counter. */\n}",
            "__shared__ size_t counts[4];\n   counts[threadIdx.x] = 0;\n   __syncthreads();\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n      size_t bin = 0;\n      if (points[i].x > 0) {\n         bin += 1;\n      }\n      if (points[i].y > 0) {\n         bin += 2;\n      }\n      atomicAdd(&counts[bin], 1);\n   }\n   for (int i = 1; i < 4; i <<= 1) {\n      __syncthreads();\n      if (threadIdx.x % (i << 1) == 0) {\n         atomicAdd(&counts[threadIdx.x / i], counts[threadIdx.x + i]);\n      }\n   }\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 4; i++) {\n         atomicAdd(&bins[i], counts[i]);\n      }\n   }\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // Ignore the last thread to avoid out-of-bounds access\n   if (threadId >= N) return;\n\n   // Add 1 to the correct bin (0 to 3)\n   size_t binId = (points[threadId].x >= 0)? (points[threadId].y >= 0? 0 : 2) : (points[threadId].y >= 0? 1 : 3);\n   atomicAdd(&bins[binId], 1);\n\n}",
            "size_t i = threadIdx.x;\n   while (i < N) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n      i += blockDim.x;\n   }\n}",
            "int tid = threadIdx.x;\n   if (tid < N) {\n      if (points[tid].x > 0 && points[tid].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[tid].x < 0 && points[tid].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[tid].x < 0 && points[tid].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[tid].x > 0 && points[tid].y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Initialize bins to zero.\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   // Fill in the bins with the correct counts.\n   //...\n}",
            "// Initialize all the bins to zero\n   for (int i = 0; i < 4; i++)\n      bins[i] = 0;\n\n   // Count the number of points in the cartesian quadrant they fall in\n   // You can use the sign of x and y as an offset into the array\n   // 0 is in the first bin\n   // 1 is in the second bin\n   // 2 is in the third bin\n   // 3 is in the fourth bin\n   for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0)\n            atomicAdd(&bins[0], 1);\n         else\n            atomicAdd(&bins[1], 1);\n      } else {\n         if (points[i].y >= 0)\n            atomicAdd(&bins[2], 1);\n         else\n            atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "/*\n     * The task is to write the kernel\n     * - For each point p in `points`, place it in the appropriate quadrant\n     * - Store the counts in `bins`\n     */\n    int index = threadIdx.x;\n    if (index < N)\n    {\n        float x = points[index].x;\n        float y = points[index].y;\n        if (x >= 0 && y >= 0)\n        {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (x <= 0 && y >= 0)\n        {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (x <= 0 && y <= 0)\n        {\n            atomicAdd(&bins[2], 1);\n        }\n        else\n        {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "__shared__ size_t bin[4];\n\n  // Initialize shared memory\n  bin[0] = bin[1] = bin[2] = bin[3] = 0;\n  __syncthreads();\n\n  int threadIdx = threadIdx.x;\n  if (threadIdx < N) {\n    Point p = points[threadIdx];\n    int bin = (p.x >= 0 && p.y >= 0)? 0 : (p.x < 0 && p.y >= 0)? 1 : (p.x < 0 && p.y < 0)? 2 : 3;\n    atomicAdd(&(bin[bin]), 1);\n  }\n\n  __syncthreads();\n  bins[0] = bin[0];\n  bins[1] = bin[1];\n  bins[2] = bin[2];\n  bins[3] = bin[3];\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n\n    // TODO: Implement\n}",
            "// TODO\n\n}",
            "// your code here\n\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n        else if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n        else if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "// Fill in this function!\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx >= N) return;\n    int i = 0;\n    if (points[idx].x >= 0 && points[idx].y >= 0)\n        i = 0;\n    if (points[idx].x < 0 && points[idx].y >= 0)\n        i = 1;\n    if (points[idx].x < 0 && points[idx].y < 0)\n        i = 2;\n    if (points[idx].x >= 0 && points[idx].y < 0)\n        i = 3;\n    atomicAdd(&bins[i], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        Point p = points[tid];\n        if (p.x > 0 && p.y > 0)\n            atomicAdd(&bins[0], 1);\n        else if (p.x < 0 && p.y > 0)\n            atomicAdd(&bins[1], 1);\n        else if (p.x < 0 && p.y < 0)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// Use the thread id to determine the point to count.\n   // Assume there are N points.\n   const size_t id = threadIdx.x;\n   const Point &p = points[id];\n\n   // Determine the quadrant of p, and increment the corresponding bin.\n   if (p.x >= 0 && p.y >= 0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0 && p.y >= 0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0 && p.y < 0)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        bins[quadrant(points[tid].x, points[tid].y)]++;\n    }\n}",
            "// Your code here\n}",
            "__shared__ size_t s[4];\n    size_t tid = threadIdx.x;\n    s[tid] = 0;\n    __syncthreads();\n\n    if(tid < 4) {\n        for(int i = 0; i < N; i++) {\n            if(tid == 0 && points[i].x > 0 && points[i].y > 0) s[0]++;\n            if(tid == 1 && points[i].x < 0 && points[i].y > 0) s[1]++;\n            if(tid == 2 && points[i].x > 0 && points[i].y < 0) s[2]++;\n            if(tid == 3 && points[i].x < 0 && points[i].y < 0) s[3]++;\n        }\n    }\n    __syncthreads();\n    if(tid < 4) bins[tid] = s[tid];\n}",
            "// Implement this function\n}",
            "}",
            "// TODO\n   // replace this with the real kernel code\n   bins[0] = 0;\n   bins[1] = 1;\n   bins[2] = 2;\n   bins[3] = 3;\n\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n        if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n        if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n        if (points[i].x >= 0 && points[i].y < 0) bins[3]++;\n    }\n}",
            "__shared__ size_t bin_cache[4];\n   size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t bin_index = 0;\n   if (index < N) {\n      if (points[index].x > 0) {\n         bin_index += 1;\n      }\n      if (points[index].y > 0) {\n         bin_index += 2;\n      }\n      atomicAdd(&bin_cache[bin_index], 1);\n   }\n   __syncthreads();\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 4; i++) {\n         atomicAdd(&bins[i], bin_cache[i]);\n      }\n   }\n}",
            "__shared__ size_t cache[THREADS_PER_BLOCK];\n   int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   cache[tid] = 0;\n   for(size_t i=bid*blockDim.x+tid; i<N; i+=blockDim.x*gridDim.x) {\n      Point p = points[i];\n      int bin = (p.x >= 0 && p.y >= 0) + (p.x < 0 && p.y >= 0) + (p.x < 0 && p.y < 0) + (p.x >= 0 && p.y < 0);\n      atomicAdd(&cache[bin], 1);\n   }\n   __syncthreads();\n   if(tid == 0) {\n      for(int i=0; i<4; i++) atomicAdd(bins+i, cache[i]);\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx >= N) return;\n\n    Point p = points[idx];\n\n    if (p.x > 0 && p.y > 0) bins[0]++;\n    else if (p.x <= 0 && p.y > 0) bins[1]++;\n    else if (p.x <= 0 && p.y <= 0) bins[2]++;\n    else if (p.x > 0 && p.y <= 0) bins[3]++;\n}",
            "// Fill in the kernel.\n    int id = threadIdx.x;\n    // \u5982\u679cid\u6bd4\u6570\u7ec4\u7684\u957f\u5ea6\u8fd8\u8981\u5c0f\uff0c\u5219\u4e0d\u8fdb\u884c\u64cd\u4f5c\n    if(id < N) {\n        int x = points[id].x;\n        int y = points[id].y;\n        if(x >= 0 && y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if(x < 0 && y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if(x < 0 && y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if(x >= 0 && y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t index = threadIdx.x;\n   while (index < N) {\n\n      double x = points[index].x;\n      double y = points[index].y;\n      size_t bin;\n      if (x > 0) {\n         if (y > 0) {\n            bin = 0;\n         }\n         else {\n            bin = 3;\n         }\n      }\n      else {\n         if (y > 0) {\n            bin = 1;\n         }\n         else {\n            bin = 2;\n         }\n      }\n      atomicAdd(&bins[bin], 1);\n\n      index += gridDim.x;\n   }\n}",
            "const size_t idx = blockDim.x*blockIdx.x + threadIdx.x;\n   if (idx < N) {\n      int q = (points[idx].x >= 0.0) + (points[idx].y >= 0.0) * 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= N) return;\n\n   size_t quadrant = (points[i].x >= 0.0) << 1 | (points[i].y >= 0.0);\n   atomicAdd(&bins[quadrant], 1);\n}",
            "// Get thread id\n   int id = threadIdx.x + blockIdx.x * blockDim.x;\n\n   if (id < N) {\n      if (points[id].x >= 0 && points[id].y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[id].x < 0 && points[id].y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[id].x < 0 && points[id].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (points[id].x >= 0 && points[id].y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// get thread id\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n    // count\n    if (i < N) {\n        int quadrant = 0;\n        if (points[i].x >= 0) {\n            quadrant += 1;\n        }\n        if (points[i].y >= 0) {\n            quadrant += 2;\n        }\n        atomicAdd(bins + quadrant, 1);\n    }\n}",
            "// Each thread is responsible for counting a single point in each quadrant.\n  // In case of a tie, multiple threads may try to increment the same bin at the same time.\n  // If that happens, one of them will win the race and the result will be the correct number.\n  // This is okay as long as the number of points in each quadrant is relatively small.\n\n  // Find the point the thread is responsible for.\n  // Use an atomicAdd so that if multiple threads try to count the same point, only one of them succeeds.\n  int index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index >= N)\n    return;\n\n  Point p = points[index];\n  if (p.x > 0) {\n    if (p.y > 0)\n      atomicAdd(&bins[0], 1);\n    else\n      atomicAdd(&bins[1], 1);\n  } else {\n    if (p.y > 0)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = threadIdx.x;\n    if (i < N) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// your code here\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n       int quadrant = 0;\n       if (points[tid].x > 0 && points[tid].y > 0) {\n           quadrant = 1;\n       } else if (points[tid].x < 0 && points[tid].y > 0) {\n           quadrant = 2;\n       } else if (points[tid].x < 0 && points[tid].y < 0) {\n           quadrant = 3;\n       }\n       atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// Count the number of points in each quadrant (1, 2, 3, 4)\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (points[i].x >= 0.0) {\n      if (points[i].y >= 0.0) {\n        atomicAdd(&bins[1], 1);\n      } else {\n        atomicAdd(&bins[4], 1);\n      }\n    } else {\n      if (points[i].y >= 0.0) {\n        atomicAdd(&bins[2], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    }\n  }\n}",
            "// Implement this function\n}",
            "int thread_id = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + thread_id;\n\n    if(i < N) {\n        int bin_id = 0;\n\n        if(points[i].x < 0) {\n            bin_id += 1;\n        }\n        if(points[i].y < 0) {\n            bin_id += 2;\n        }\n\n        atomicAdd(&(bins[bin_id]), 1);\n    }\n}",
            "__shared__ size_t shared_mem[4];\n    size_t idx = threadIdx.x;\n    shared_mem[0] = 0;\n    shared_mem[1] = 0;\n    shared_mem[2] = 0;\n    shared_mem[3] = 0;\n    __syncthreads();\n    if (idx < N) {\n        if (points[idx].x >= 0 && points[idx].y >= 0) {\n            atomicAdd(&shared_mem[0], 1);\n        }\n        else if (points[idx].x < 0 && points[idx].y >= 0) {\n            atomicAdd(&shared_mem[1], 1);\n        }\n        else if (points[idx].x >= 0 && points[idx].y < 0) {\n            atomicAdd(&shared_mem[2], 1);\n        }\n        else {\n            atomicAdd(&shared_mem[3], 1);\n        }\n    }\n    __syncthreads();\n    atomicAdd(&bins[0], shared_mem[0]);\n    atomicAdd(&bins[1], shared_mem[1]);\n    atomicAdd(&bins[2], shared_mem[2]);\n    atomicAdd(&bins[3], shared_mem[3]);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        Point p = points[i];\n        if (p.x >= 0) {\n            if (p.y >= 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        } else {\n            if (p.y >= 0) {\n                atomicAdd(&bins[1], 1);\n            } else {\n                atomicAdd(&bins[2], 1);\n            }\n        }\n    }\n}",
            "}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (i < N) {\n      int q;\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         q = 0;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         q = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         q = 2;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         q = 3;\n      }\n\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "//...\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (tid < N) {\n        int i = floor(points[tid].x / abs(points[tid].x));\n        int j = floor(points[tid].y / abs(points[tid].y));\n        atomicAdd(&bins[i+2*j], 1);\n    }\n}",
            "//...\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx >= N) return;\n\n   Point point = points[idx];\n   int q = 0;\n   if (point.x > 0 && point.y > 0) q = 1;\n   else if (point.x < 0 && point.y > 0) q = 2;\n   else if (point.x < 0 && point.y < 0) q = 3;\n   atomicAdd(&bins[q], 1);\n}",
            "}",
            "// TODO: implement\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx >= N) {\n        return;\n    }\n    Point p = points[idx];\n    unsigned int quadrant = (p.x >= 0)? (p.y >= 0? 0 : 1) : (p.y >= 0? 2 : 3);\n    atomicAdd(&bins[quadrant], 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      int quadrant = 0;\n      if (points[idx].x > 0 && points[idx].y > 0) quadrant = 0;\n      else if (points[idx].x < 0 && points[idx].y > 0) quadrant = 1;\n      else if (points[idx].x < 0 && points[idx].y < 0) quadrant = 2;\n      else quadrant = 3;\n      atomicAdd(bins + quadrant, 1);\n   }\n}",
            "__shared__ size_t xNeg[256];\n   __shared__ size_t xPos[256];\n   __shared__ size_t yNeg[256];\n   __shared__ size_t yPos[256];\n\n   // Find the index of this thread in the global array\n   size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // Each thread takes a point, and counts it in the correct quadrant\n   if (i < N) {\n      if (points[i].x < 0)\n         xNeg[threadIdx.x]++;\n      else\n         xPos[threadIdx.x]++;\n      if (points[i].y < 0)\n         yNeg[threadIdx.x]++;\n      else\n         yPos[threadIdx.x]++;\n   }\n\n   // Wait for all threads to finish\n   __syncthreads();\n\n   // Now we sum up the contributions from each thread.\n   // The reduction is done in the first 256 threads (index < 256).\n\n   // First, add up the elements in xNeg\n   size_t sum = 0;\n   for (size_t j = 0; j < blockDim.x; j++)\n      sum += xNeg[j];\n   xNeg[threadIdx.x] = sum;\n   __syncthreads();\n\n   // Then add up the elements in xPos\n   sum = 0;\n   for (size_t j = 0; j < blockDim.x; j++)\n      sum += xPos[j];\n   xPos[threadIdx.x] = sum;\n   __syncthreads();\n\n   // Now, add up the elements in yNeg\n   sum = 0;\n   for (size_t j = 0; j < blockDim.x; j++)\n      sum += yNeg[j];\n   yNeg[threadIdx.x] = sum;\n   __syncthreads();\n\n   // Finally, add up the elements in yPos\n   sum = 0;\n   for (size_t j = 0; j < blockDim.x; j++)\n      sum += yPos[j];\n   yPos[threadIdx.x] = sum;\n   __syncthreads();\n\n   // At this point, all of the threads in the same block have\n   // the correct counts for their respective quadrants\n\n   // We can now just set the values in `bins`\n   if (threadIdx.x == 0) {\n      bins[0] = xNeg[threadIdx.x];\n      bins[1] = yNeg[threadIdx.x];\n      bins[2] = xPos[threadIdx.x];\n      bins[3] = yPos[threadIdx.x];\n   }\n}",
            "__shared__ size_t cache[256];\n   for (size_t i = 0; i < 4; i++) {\n      cache[threadIdx.x + blockDim.x * i] = 0;\n   }\n   __syncthreads();\n\n   for (size_t i = 0; i < N; i++) {\n      if ((points[i].x >= 0 && points[i].y >= 0) || (points[i].x < 0 && points[i].y < 0)) {\n         atomicAdd(&cache[threadIdx.x + blockDim.x * 0], 1);\n      }\n\n      if ((points[i].x < 0 && points[i].y >= 0) || (points[i].x >= 0 && points[i].y < 0)) {\n         atomicAdd(&cache[threadIdx.x + blockDim.x * 1], 1);\n      }\n\n      if ((points[i].x >= 0 && points[i].y < 0) || (points[i].x < 0 && points[i].y >= 0)) {\n         atomicAdd(&cache[threadIdx.x + blockDim.x * 2], 1);\n      }\n\n      if ((points[i].x < 0 && points[i].y < 0) || (points[i].x >= 0 && points[i].y >= 0)) {\n         atomicAdd(&cache[threadIdx.x + blockDim.x * 3], 1);\n      }\n   }\n   __syncthreads();\n   for (size_t i = 0; i < 4; i++) {\n      atomicAdd(&bins[i], cache[threadIdx.x + blockDim.x * i]);\n   }\n}",
            "int index = threadIdx.x;\n   if (index < N) {\n      if (points[index].x >= 0 && points[index].y >= 0) bins[0]++;\n      else if (points[index].x < 0 && points[index].y >= 0) bins[1]++;\n      else if (points[index].x < 0 && points[index].y < 0) bins[2]++;\n      else if (points[index].x >= 0 && points[index].y < 0) bins[3]++;\n   }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        size_t bin = (points[i].x > 0 && points[i].y > 0) || (points[i].x < 0 && points[i].y < 0)? 0 :\n                     (points[i].x == 0 && points[i].y > 0) || (points[i].x < 0 && points[i].y == 0)? 1 :\n                     (points[i].x == 0 && points[i].y < 0) || (points[i].x > 0 && points[i].y == 0)? 2 : 3;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// Get the x and y coordinates from the input array\n    double x = points[blockIdx.x].x;\n    double y = points[blockIdx.x].y;\n\n    // Choose the bin based on quadrant\n    if (x > 0 && y > 0) {\n        atomicAdd(&bins[0], 1);\n    }\n    else if (x < 0 && y > 0) {\n        atomicAdd(&bins[1], 1);\n    }\n    else if (x < 0 && y < 0) {\n        atomicAdd(&bins[2], 1);\n    }\n    else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n\n   int start = bid * N + tid;\n\n   if (start < N) {\n      Point p = points[start];\n      int index = 0;\n      if (p.x > 0 && p.y > 0)\n         index = 0;\n      else if (p.x < 0 && p.y > 0)\n         index = 1;\n      else if (p.x < 0 && p.y < 0)\n         index = 2;\n      else if (p.x > 0 && p.y < 0)\n         index = 3;\n      atomicAdd(&(bins[index]), 1);\n   }\n}",
            "__shared__ size_t bin[4];\n\n   for (size_t i = 0; i < 4; i++) {\n      bin[i] = 0;\n   }\n\n   size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      double x = points[idx].x;\n      double y = points[idx].y;\n\n      if (x >= 0 && y >= 0) {\n         atomicAdd(&bin[0], 1);\n      } else if (x < 0 && y >= 0) {\n         atomicAdd(&bin[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bin[2], 1);\n      } else if (x >= 0 && y < 0) {\n         atomicAdd(&bin[3], 1);\n      }\n   }\n\n   __syncthreads();\n\n   for (size_t i = 0; i < 4; i++) {\n      atomicAdd(&bins[i], bin[i]);\n   }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if(i<N)\n  {\n    if(points[i].x>0 && points[i].y>0)\n    {\n      atomicAdd(&bins[0],1);\n    }\n    else if(points[i].x<0 && points[i].y>0)\n    {\n      atomicAdd(&bins[1],1);\n    }\n    else if(points[i].x>0 && points[i].y<0)\n    {\n      atomicAdd(&bins[2],1);\n    }\n    else if(points[i].x<0 && points[i].y<0)\n    {\n      atomicAdd(&bins[3],1);\n    }\n  }\n}",
            "int index = threadIdx.x;\n    if (index < N)\n    {\n        // Determine the quadrant\n        size_t quadrant = (points[index].x < 0) + 2 * (points[index].y < 0);\n\n        // Increment the counter for the corresponding quadrant\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "// Your code here\n}",
            "// Find the index of the thread that this block contains.\n   size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // If i is outside the range of the array, exit the function.\n   if (i >= N) {\n      return;\n   }\n\n   size_t x = (size_t)floor(points[i].x);\n   size_t y = (size_t)floor(points[i].y);\n\n   if (x == 0 && y == 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (x < 0 && y > 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (x < 0 && y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (x > 0 && y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "const size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index >= N) return;\n\n   size_t bin = 0;\n   if (points[index].x >= 0 && points[index].y >= 0) bin = 0;\n   if (points[index].x < 0 && points[index].y >= 0) bin = 1;\n   if (points[index].x < 0 && points[index].y < 0) bin = 2;\n   if (points[index].x >= 0 && points[index].y < 0) bin = 3;\n\n   atomicAdd(&bins[bin], 1);\n}",
            "auto tid = threadIdx.x;\n   auto gid = blockIdx.x;\n   if (gid < N) {\n      auto p = points[gid];\n      // TODO\n   }\n}",
            "/*\n       The indices for the quadrant counts are as follows:\n\n       0 1\n       2 3\n\n       For example, bins[2] is the count of points in quadrant 2,\n       which is the third quadrant.\n    */\n\n    /* TODO: count the points in each quadrant in parallel. */\n}",
            "int tid = threadIdx.x;\n    if (tid < N) {\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (points[tid].x > 0 && points[tid].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadId < N) {\n        Point point = points[threadId];\n        if (point.x > 0 && point.y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (point.x < 0 && point.y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (point.x > 0 && point.y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if (point.x < 0 && point.y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int num_threads = gridDim.x * blockDim.x;\n    while (idx < N) {\n        if (points[idx].x > 0 && points[idx].y > 0)\n            atomicAdd(&bins[0], 1);\n        if (points[idx].x < 0 && points[idx].y > 0)\n            atomicAdd(&bins[1], 1);\n        if (points[idx].x < 0 && points[idx].y < 0)\n            atomicAdd(&bins[2], 1);\n        if (points[idx].x > 0 && points[idx].y < 0)\n            atomicAdd(&bins[3], 1);\n        idx += num_threads;\n    }\n}",
            "// your code here\n}",
            "// get the index of this thread\n    size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // check if this thread should work\n    if (index < N) {\n\n        // get the point at the current index\n        const Point &point = points[index];\n\n        // count the point into a quadrant based on its (x, y) position\n        size_t quadrant = (point.x > 0 && point.y > 0) + (point.x < 0 && point.y > 0) * 2 + (point.x < 0 && point.y < 0) * 4 + (point.x > 0 && point.y < 0) * 8;\n\n        // add the point to the appropriate quadrant's counter\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "// Compute this thread's index into the array\n   int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (threadIdx < N) {\n      // Compute the quadrant in which the point is located\n      int iq = 0;\n      if (points[threadIdx].x > 0)\n         iq += 1;\n      if (points[threadIdx].y > 0)\n         iq += 2;\n      // Increment the appropriate bin\n      atomicAdd(&bins[iq], 1);\n   }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index >= N) return;\n\n    size_t bin = 0;\n    if (points[index].x >= 0 && points[index].y >= 0)\n        bin = 0;\n    else if (points[index].x < 0 && points[index].y >= 0)\n        bin = 1;\n    else if (points[index].x < 0 && points[index].y < 0)\n        bin = 2;\n    else if (points[index].x >= 0 && points[index].y < 0)\n        bin = 3;\n    atomicAdd(&bins[bin], 1);\n}",
            "//...\n}",
            "int threadIndex = threadIdx.x + blockIdx.x * blockDim.x;\n\n   // Count the points in each quadrant\n   if (threadIndex < N) {\n      Point point = points[threadIndex];\n\n      if (point.x > 0 && point.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (point.x < 0 && point.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (point.x < 0 && point.y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (point.x > 0 && point.y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (id >= N) return;\n\n   size_t bin = 0;\n\n   if (points[id].x > 0 && points[id].y > 0)\n      bin = 0;\n   else if (points[id].x < 0 && points[id].y > 0)\n      bin = 1;\n   else if (points[id].x < 0 && points[id].y < 0)\n      bin = 2;\n   else if (points[id].x > 0 && points[id].y < 0)\n      bin = 3;\n\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    if (points[tid].x >= 0 && points[tid].y >= 0)\n      atomicAdd(&bins[0], 1);\n    else if (points[tid].x < 0 && points[tid].y >= 0)\n      atomicAdd(&bins[1], 1);\n    else if (points[tid].x < 0 && points[tid].y < 0)\n      atomicAdd(&bins[2], 1);\n    else if (points[tid].x >= 0 && points[tid].y < 0)\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: fill this in.\n   // TODO: use an atomic operation to avoid race conditions when incrementing the counts.\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        if(points[i].x > 0 && points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if(points[i].x < 0 && points[i].y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if(points[i].x < 0 && points[i].y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else if(points[i].x > 0 && points[i].y < 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "/* Your code here */\n}",
            "// Count the number of points in each quadrant\n    // Hint: use a for-loop\n    // Hint: use atomic operations to increment bins\n    //...\n}",
            "// You can use the function \"threadIdx.x\" to find your thread number.\n  // For example, if N=6 and threadIdx.x=0, your thread number is 0.\n  // If N=6 and threadIdx.x=1, your thread number is 1.\n  // If N=6 and threadIdx.x=2, your thread number is 2.\n  // etc.\n  //\n  // You can use the function \"blockIdx.x\" to find the block number.\n  // For example, if the kernel is launched with 32 threads, you will have 1 block with 32 threads.\n  // If you launch 2 blocks with 32 threads each, the first block number will be 0 and the second will be 1.\n  //\n  // You can use the function \"blockDim.x\" to find the total number of threads. In the example above, this would return 32.\n  //\n  // Note that the threads are numbered sequentially from 0 to N-1.\n  //\n  // Note that thread number \"0\" will access points[0] and thread number \"N-1\" will access points[N-1].\n  //\n  // Note that you can use \"if\" statements to check if a thread number is in a certain quadrant.\n  // For example:\n  // if (threadIdx.x < N / 4) {... }\n  // if (threadIdx.x < N / 2) {... }\n  // if (threadIdx.x < 3 * N / 4) {... }\n  //\n  // Note that if you use an \"if\" statement, each thread must execute the same instructions.\n  // If thread number 0 in your kernel executes the first branch, then all other threads\n  // in your kernel must also execute the first branch.\n  // If thread number 0 in your kernel executes the second branch, then all other threads\n  // in your kernel must also execute the second branch.\n  //\n  // This is how you can do \"if\" statements in CUDA:\n  // if (condition) {\n  //   // Execute this code if the condition is true.\n  // }\n  // else {\n  //   // Execute this code if the condition is false.\n  // }\n  //\n  // For this exercise, you will check the quadrant of a single thread.\n  // You can do this by comparing threadIdx.x with N/4, N/2, and 3*N/4.\n  //\n  // Note that you will need to use \"atomicAdd(&bins[i], 1)\" to increment bins[i].\n  //\n  // You can find the atomicAdd function on CUDA's website:\n  // http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions\n  //\n  // Note that you will need to use \"__syncthreads()\" to wait for all threads in your block to finish their\n  // execution before you can return.\n  //\n  // You can find the syncthreads function on CUDA's website:\n  // http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#synchronization-functions\n}",
            "/* You need to compute the index of the current thread */\n   unsigned int idx = threadIdx.x;\n\n   /* You need to compute the quadrant of the current point */\n   /* For example:\n      * Quadrant 1: x >= 0 and y >= 0\n      * Quadrant 2: x <  0 and y >= 0\n      * Quadrant 3: x <  0 and y <  0\n      * Quadrant 4: x >= 0 and y <  0\n    */\n   unsigned int quadrant = (points[idx].x >= 0) * (points[idx].y >= 0);\n\n   /* Each thread atomically increments the count in the corresponding quadrant */\n   atomicAdd(&bins[quadrant], 1);\n}",
            "// TODO: Implement this kernel\n}",
            "// TODO: Implement\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      int x = (points[tid].x >= 0)? 0 : 1;\n      int y = (points[tid].y >= 0)? 0 : 1;\n      atomicAdd(&bins[x + y * 2], 1);\n   }\n}",
            "const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid >= N) return;\n\n    // Use the `bins` vector to store the number of points in each quadrant.\n    // Use atomic addition (e.g., `atomicAdd(bins + 0, 1)`) to increment the count for a specific quadrant.\n    // You can use `points[tid].x` and `points[tid].y` to access the x and y coordinates of the point.\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if (i >= N)\n        return;\n\n    const Point &p = points[i];\n\n    if (p.x > 0 && p.y > 0)\n        atomicAdd(&bins[0], 1);\n    else if (p.x < 0 && p.y > 0)\n        atomicAdd(&bins[1], 1);\n    else if (p.x > 0 && p.y < 0)\n        atomicAdd(&bins[2], 1);\n    else if (p.x < 0 && p.y < 0)\n        atomicAdd(&bins[3], 1);\n}",
            "unsigned idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    if (points[idx].x >= 0 && points[idx].y >= 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (points[idx].x < 0 && points[idx].y >= 0) {\n      atomicAdd(&bins[1], 1);\n    } else if (points[idx].x >= 0 && points[idx].y < 0) {\n      atomicAdd(&bins[2], 1);\n    } else if (points[idx].x < 0 && points[idx].y < 0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "__shared__ size_t shmem[4];\n\n   // Each thread is responsible for one point.\n   const Point &p = points[blockIdx.x * blockDim.x + threadIdx.x];\n   size_t quadrant = 0;\n\n   // Check for each quadrant.\n   if (p.x >= 0 && p.y >= 0) quadrant = 0;\n   else if (p.x < 0 && p.y >= 0) quadrant = 1;\n   else if (p.x < 0 && p.y < 0) quadrant = 2;\n   else if (p.x >= 0 && p.y < 0) quadrant = 3;\n\n   atomicAdd(&shmem[quadrant], 1);\n   __syncthreads();\n   bins[quadrant] = shmem[quadrant];\n}",
            "size_t i = threadIdx.x;\n   if (i < N) {\n      size_t x = (points[i].x >= 0);\n      size_t y = (points[i].y >= 0);\n      atomicAdd(&bins[x + 2*y], 1);\n   }\n}",
            "// YOUR CODE HERE\n\n}",
            "int i = threadIdx.x;\n    if (i >= N)\n        return;\n    Point p = points[i];\n    if (p.x >= 0 && p.y >= 0)\n        atomicAdd(&bins[0], 1);\n    else if (p.x < 0 && p.y >= 0)\n        atomicAdd(&bins[1], 1);\n    else if (p.x < 0 && p.y < 0)\n        atomicAdd(&bins[2], 1);\n    else\n        atomicAdd(&bins[3], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        bins[0] = (points[tid].x >= 0 && points[tid].y >= 0? bins[0] + 1 : bins[0]);\n        bins[1] = (points[tid].x < 0 && points[tid].y >= 0? bins[1] + 1 : bins[1]);\n        bins[2] = (points[tid].x < 0 && points[tid].y < 0? bins[2] + 1 : bins[2]);\n        bins[3] = (points[tid].x >= 0 && points[tid].y < 0? bins[3] + 1 : bins[3]);\n    }\n}",
            "// YOUR CODE HERE\n}",
            "}",
            "for (int i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        int quadrant = 0;\n        if (points[i].x < 0) {\n            quadrant += 1;\n        }\n        if (points[i].y < 0) {\n            quadrant += 2;\n        }\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "__shared__ size_t localBins[4];\n\n   if(threadIdx.x == 0) {\n      localBins[0] = 0;\n      localBins[1] = 0;\n      localBins[2] = 0;\n      localBins[3] = 0;\n   }\n\n   __syncthreads();\n\n   const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if(i < N) {\n      Point point = points[i];\n      if(point.x >= 0 && point.y >= 0) {\n         localBins[0] += 1;\n      } else if(point.x < 0 && point.y >= 0) {\n         localBins[1] += 1;\n      } else if(point.x >= 0 && point.y < 0) {\n         localBins[2] += 1;\n      } else {\n         localBins[3] += 1;\n      }\n   }\n\n   __syncthreads();\n\n   if(threadIdx.x == 0) {\n      atomicAdd(&bins[0], localBins[0]);\n      atomicAdd(&bins[1], localBins[1]);\n      atomicAdd(&bins[2], localBins[2]);\n      atomicAdd(&bins[3], localBins[3]);\n   }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index >= N) {\n      return;\n   }\n   Point p = points[index];\n   int x = p.x;\n   int y = p.y;\n   if (x < 0 && y < 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (x > 0 && y < 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (x < 0 && y > 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (x > 0 && y > 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    if (points[i].x > 0 && points[i].y > 0) {\n        atomicAdd(&bins[0], 1);\n    }\n    else if (points[i].x <= 0 && points[i].y > 0) {\n        atomicAdd(&bins[1], 1);\n    }\n    else if (points[i].x <= 0 && points[i].y <= 0) {\n        atomicAdd(&bins[2], 1);\n    }\n    else if (points[i].x > 0 && points[i].y <= 0) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    if (points[tid].x >= 0 && points[tid].y >= 0)\n      atomicAdd(bins + 0, 1);\n    else if (points[tid].x < 0 && points[tid].y >= 0)\n      atomicAdd(bins + 1, 1);\n    else if (points[tid].x < 0 && points[tid].y < 0)\n      atomicAdd(bins + 2, 1);\n    else if (points[tid].x >= 0 && points[tid].y < 0)\n      atomicAdd(bins + 3, 1);\n  }\n}",
            "__shared__ size_t smem[256];\n\n   int idx = threadIdx.x + blockDim.x * blockIdx.x;\n   if (idx >= N) return;\n\n   size_t binIdx = (points[idx].x >= 0) + (points[idx].y >= 0) * 2;\n   atomicAdd(&smem[binIdx], 1);\n\n   __syncthreads();\n   if (threadIdx.x < 4) {\n      size_t sum = 0;\n      for (size_t i = 0; i < 4; ++i) {\n         sum += smem[i];\n         smem[i] = sum;\n      }\n   }\n   __syncthreads();\n\n   atomicAdd(&bins[binIdx], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // The kernel does not have to process all the points.\n   if (idx < N) {\n      int quad = 0;\n      if (points[idx].x > 0) {\n         if (points[idx].y > 0) {\n            quad = 0;\n         } else if (points[idx].y < 0) {\n            quad = 1;\n         }\n      } else if (points[idx].x < 0) {\n         if (points[idx].y > 0) {\n            quad = 2;\n         } else if (points[idx].y < 0) {\n            quad = 3;\n         }\n      }\n\n      atomicAdd(&bins[quad], 1);\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   Point p = points[i];\n   int q = p.x > 0? p.y > 0? 0 : 3 : p.y > 0? 1 : 2;\n   atomicAdd(&bins[q], 1);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (idx < N) {\n      int x = points[idx].x;\n      int y = points[idx].y;\n\n      size_t bin = 0;\n\n      if (x >= 0 && y >= 0) {\n         bin = 0;\n      } else if (x < 0 && y >= 0) {\n         bin = 1;\n      } else if (x < 0 && y < 0) {\n         bin = 2;\n      } else if (x >= 0 && y < 0) {\n         bin = 3;\n      }\n\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    // Check if within range\n    if(idx < N) {\n        if(points[idx].x >= 0 && points[idx].y >= 0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if(points[idx].x <= 0 && points[idx].y >= 0) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if(points[idx].x >= 0 && points[idx].y <= 0) {\n            atomicAdd(&bins[2], 1);\n        }\n        else if(points[idx].x <= 0 && points[idx].y <= 0) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t index = threadIdx.x;\n  size_t bin;\n  if (index < N)\n  {\n    if (points[index].x >= 0.0 && points[index].y >= 0.0)\n    {\n      bin = 0;\n    }\n    else if (points[index].x < 0.0 && points[index].y >= 0.0)\n    {\n      bin = 1;\n    }\n    else if (points[index].x < 0.0 && points[index].y < 0.0)\n    {\n      bin = 2;\n    }\n    else\n    {\n      bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    int quadrant;\n    if (i < N) {\n        if (points[i].x >= 0) {\n            if (points[i].y >= 0) {\n                quadrant = 0;\n            } else {\n                quadrant = 1;\n            }\n        } else {\n            if (points[i].y >= 0) {\n                quadrant = 2;\n            } else {\n                quadrant = 3;\n            }\n        }\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        size_t i = 0;\n        double x = points[tid].x;\n        double y = points[tid].y;\n        if (x >= 0) {\n            if (y >= 0) i += 1;\n            if (y < 0) i += 2;\n        } else {\n            if (y >= 0) i += 3;\n            if (y < 0) i += 0;\n        }\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "size_t idx = threadIdx.x;\n    //...\n}",
            "/* TODO: fill this in. */\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if(tid < N) {\n    int quadrant = 0;\n    if(points[tid].x > 0) {\n      quadrant += 1;\n    }\n    if(points[tid].y > 0) {\n      quadrant += 2;\n    }\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      int index = 0;\n      if (points[i].x > 0 && points[i].y > 0) {\n         index = 0;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         index = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         index = 2;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         index = 3;\n      }\n      atomicAdd(&(bins[index]), 1);\n   }\n}",
            "int index = blockIdx.x*blockDim.x + threadIdx.x;\n    if (index < N)\n    {\n        int x = floor(points[index].x);\n        int y = floor(points[index].y);\n        atomicAdd(&bins[0 + (x >= 0 && y >= 0)], 1);\n        atomicAdd(&bins[1 + (x < 0 && y >= 0)], 1);\n        atomicAdd(&bins[2 + (x < 0 && y < 0)], 1);\n        atomicAdd(&bins[3 + (x >= 0 && y < 0)], 1);\n    }\n}",
            "// TODO: Your code here\n}",
            "auto idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) {\n       return;\n   }\n   auto p = points[idx];\n   auto q = 0;\n   if (p.x > 0 && p.y > 0) {\n       q = 0;\n   } else if (p.x < 0 && p.y > 0) {\n       q = 1;\n   } else if (p.x < 0 && p.y < 0) {\n       q = 2;\n   } else {\n       q = 3;\n   }\n   atomicAdd(&bins[q], 1);\n}",
            "size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (thread_id < N) {\n      size_t quadrant = 0;\n      if (points[thread_id].x >= 0) {\n         if (points[thread_id].y >= 0) {\n            quadrant = 0;\n         } else {\n            quadrant = 1;\n         }\n      } else {\n         if (points[thread_id].y >= 0) {\n            quadrant = 2;\n         } else {\n            quadrant = 3;\n         }\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) return;\n\n   Point point = points[idx];\n\n   int quadrant = 0;\n   if (point.x >= 0 && point.y >= 0) quadrant = 0;\n   else if (point.x < 0 && point.y >= 0) quadrant = 1;\n   else if (point.x < 0 && point.y < 0) quadrant = 2;\n   else if (point.x >= 0 && point.y < 0) quadrant = 3;\n   else assert(false);\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "/*... */\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    double x = points[index].x;\n    double y = points[index].y;\n    size_t quadrant;\n    if (x >= 0 && y >= 0) {\n      quadrant = 0;\n    } else if (x < 0 && y >= 0) {\n      quadrant = 1;\n    } else if (x < 0 && y < 0) {\n      quadrant = 2;\n    } else {\n      quadrant = 3;\n    }\n    atomicAdd(bins + quadrant, 1);\n  }\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n}",
            "size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (thread_id < N) {\n        if (points[thread_id].x > 0 && points[thread_id].y > 0)\n            atomicAdd(&bins[0], 1);\n        else if (points[thread_id].x < 0 && points[thread_id].y > 0)\n            atomicAdd(&bins[1], 1);\n        else if (points[thread_id].x < 0 && points[thread_id].y < 0)\n            atomicAdd(&bins[2], 1);\n        else if (points[thread_id].x > 0 && points[thread_id].y < 0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "__shared__ size_t counts[4];\n\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  counts[0] = 0;\n  counts[1] = 0;\n  counts[2] = 0;\n  counts[3] = 0;\n  if (i < N) {\n    // count points based on their quadrant\n    if (points[i].x > 0) {\n      if (points[i].y > 0) {\n        counts[0] = counts[0] + 1;\n      }\n      else {\n        counts[3] = counts[3] + 1;\n      }\n    }\n    else {\n      if (points[i].y > 0) {\n        counts[1] = counts[1] + 1;\n      }\n      else {\n        counts[2] = counts[2] + 1;\n      }\n    }\n  }\n  __syncthreads();\n  // sum the counts\n  atomicAdd(&bins[0], counts[0]);\n  atomicAdd(&bins[1], counts[1]);\n  atomicAdd(&bins[2], counts[2]);\n  atomicAdd(&bins[3], counts[3]);\n  return;\n}",
            "// your code here\n}",
            "// Each thread processes one element.\n    // Fill in the body of this function.\n}",
            "// TODO\n}",
            "// your code here\n\n}",
            "// The code you write here.\n   size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if(idx < N) {\n      if(points[idx].x > 0 && points[idx].y > 0) {\n         atomicAdd(&bins[0], 1);\n      }\n      else if(points[idx].x < 0 && points[idx].y > 0) {\n         atomicAdd(&bins[1], 1);\n      }\n      else if(points[idx].x > 0 && points[idx].y < 0) {\n         atomicAdd(&bins[2], 1);\n      }\n      else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      int x = (int)points[tid].x;\n      int y = (int)points[tid].y;\n\n      if (x >= 0) {\n         if (y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n      int q = 0;\n      Point p = points[tid];\n      if (p.x >= 0 && p.y >= 0) {\n         q = 0;\n      }\n      if (p.x < 0 && p.y >= 0) {\n         q = 1;\n      }\n      if (p.x >= 0 && p.y < 0) {\n         q = 2;\n      }\n      if (p.x < 0 && p.y < 0) {\n         q = 3;\n      }\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i >= N) return;\n\n    int quadrant;\n\n    if (points[i].x >= 0) {\n        if (points[i].y >= 0) quadrant = 0;\n        else quadrant = 1;\n    } else {\n        if (points[i].y >= 0) quadrant = 2;\n        else quadrant = 3;\n    }\n\n    atomicAdd(&bins[quadrant], 1);\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n\n   // TODO\n\n   __syncthreads();\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i < N) {\n    if (points[i].x >= 0 && points[i].y >= 0) bins[0] += 1;\n    else if (points[i].x < 0 && points[i].y >= 0) bins[1] += 1;\n    else if (points[i].x < 0 && points[i].y < 0) bins[2] += 1;\n    else if (points[i].x >= 0 && points[i].y < 0) bins[3] += 1;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int x = points[i].x;\n        int y = points[i].y;\n        int quadrant;\n        if (x >= 0 && y >= 0) quadrant = 0;\n        else if (x < 0 && y >= 0) quadrant = 1;\n        else if (x < 0 && y < 0) quadrant = 2;\n        else quadrant = 3;\n\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "int index = threadIdx.x;\n   // TODO: add code here\n   __syncthreads();\n}",
            "// TODO: Your code here\n}",
            "// Replace this code\n   // ---------------------------------------------------------------------------------\n   size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (i >= N) return;\n\n   size_t x = 0, y = 1;\n   if (points[i].x < 0) x = 1;\n   if (points[i].y < 0) y = 2;\n\n   atomicAdd(&bins[x + y*2], 1);\n   // ---------------------------------------------------------------------------------\n}",
            "// TODO\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      int index = 0;\n      if (points[tid].x >= 0) {\n         if (points[tid].y >= 0) {\n            index = 0;\n         } else {\n            index = 1;\n         }\n      } else {\n         if (points[tid].y >= 0) {\n            index = 2;\n         } else {\n            index = 3;\n         }\n      }\n      atomicAdd(&bins[index], 1);\n   }\n}",
            "unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if(i<N) {\n      Point p = points[i];\n      if (p.x >= 0.0 && p.y >= 0.0) bins[0]++;\n      else if (p.x < 0.0 && p.y >= 0.0) bins[1]++;\n      else if (p.x < 0.0 && p.y < 0.0) bins[2]++;\n      else bins[3]++;\n   }\n}",
            "// your code here\n}",
            "unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (tid < N) {\n    if (points[tid].x >= 0) {\n      if (points[tid].y >= 0) {\n        atomicAdd(&bins[0], 1);\n      } else {\n        atomicAdd(&bins[1], 1);\n      }\n    } else {\n      if (points[tid].y >= 0) {\n        atomicAdd(&bins[2], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    }\n  }\n}",
            "// This is a GPU kernel, so you must use a CUDA-compatible type for `i`\n   // CUDA-compatible types: int, long, long long, unsigned int, unsigned long, and unsigned long long\n   // https://docs.nvidia.com/cuda/cuda-c-programming-guide/#built-in-data-types\n   int i =...;\n\n   // Store each point's quadrant in `quadrant`.\n   // You must use a CUDA-compatible type for `quadrant`.\n   int quadrant =...;\n\n   // Increment the appropriate element of `bins`.\n   // Remember that `bins` is indexed by quadrant!\n   // `bins` is not the correct type. See the question on this slide for details.\n   bins[quadrant] =...;\n}",
            "}",
            "__shared__ size_t counts[4];\n\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int nb = gridDim.x;\n  int tb = blockDim.x;\n  int gt = bid*tb+tid;\n\n  // Compute which quadrant this thread is responsible for.\n  // Assume the input is not very large.\n  int quadrant = (gt < N)? 0 :\n    (gt < N*2)? 1 :\n    (gt < N*3)? 2 : 3;\n\n  // Loop over points to compute the count for each quadrant\n  // Use warp-level reduction to compute the count\n  // If you have no idea how to do warp-level reduction, please study the warp-level reduction example\n  for (size_t i = gt; i < N; i += nb*tb) {\n    // Compute which quadrant this point is in\n    int q = 0;\n    if (points[i].x < 0) q += 1;\n    if (points[i].y < 0) q += 2;\n    // Add one to the count for this quadrant\n    atomicAdd(&counts[q], 1);\n  }\n\n  // Reduce the values stored in counts to obtain the final result\n  // The code for this is in the warp-level reduction example\n  // Note: atomicAdd only works on int, unsigned int, unsigned long long int, or float, double\n  // You need to change the types in the shared memory\n  for (unsigned int s = tb >> 1; s > 0; s >>= 1) {\n    if (tid < s) {\n      for (int i = 0; i < 4; ++i)\n        counts[i] += counts[i + s];\n    }\n    __syncthreads();\n  }\n  // Write the results to the output\n  if (tid == 0)\n    for (int i = 0; i < 4; ++i)\n      bins[i] = counts[i];\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        Point p = points[i];\n        int b = 0;\n        if (p.x > 0) b += 1;\n        if (p.y > 0) b += 2;\n        atomicAdd(&bins[b], 1);\n    }\n}",
            "int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (threadIdx >= N) return;\n\n   // TODO: Use a conditional statement to assign to bins[q] if the point is in that quadrant.\n   // Remember to use atomicAdd on bins[q].\n\n\n   // TODO: Use the atomicAdd function to update bins[q].\n}",
            "//...\n}",
            "int idx = threadIdx.x;\n    int bin = (idx >= N)? -1 : (int) (points[idx].x >= 0.0? (points[idx].y >= 0.0? 0 : 3) : (points[idx].y >= 0.0? 1 : 2));\n    atomicAdd(bins + bin, 1);\n}",
            "}",
            "//  TODO: compute number of points in each quadrant\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "bins.fill(0);\n   if (points.empty()) return;\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      const auto &p = points[i];\n      int q = 0;\n      if (p.x < 0) q += 1;\n      if (p.y < 0) q += 2;\n      bins[q]++;\n   }\n}",
            "// TODO\n}",
            "if (points.empty())\n      return;\n\n   // TODO: Your solution goes here\n}",
            "if (bins.size()!= 4) throw std::invalid_argument(\"bins has to be size 4\");\n   const int rank = omp_get_thread_num();\n   if (rank == 0) {\n       std::fill(bins.begin(), bins.end(), 0);\n   }\n   omp_barrier_wait();\n   const int num_threads = omp_get_num_threads();\n\n   // This is where you implement your parallel algorithm\n   // You have to use MPI for communication, OpenMP for parallelization.\n\n   omp_barrier_wait();\n   if (rank == 0) {\n       // Reduce all bins to rank 0\n       std::vector<int> sendBins(bins.size() * num_threads, 0);\n       std::vector<int> recvBins(bins.size(), 0);\n       for (int i = 0; i < num_threads; i++) {\n           MPI_Recv(sendBins.data() + i * bins.size(), bins.size(), MPI_INT, i, 100, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       }\n       for (int i = 0; i < bins.size(); i++) {\n           for (int j = 0; j < num_threads; j++) {\n               recvBins[i] += sendBins[i + j * bins.size()];\n           }\n       }\n       for (int i = 0; i < bins.size(); i++) {\n           bins[i] += recvBins[i];\n       }\n   }\n   if (rank!= 0) {\n       std::vector<int> sendBins(bins.size(), 0);\n       for (int i = 0; i < points.size(); i++) {\n           if (points[i].x > 0 && points[i].y > 0) sendBins[0]++;\n           else if (points[i].x < 0 && points[i].y > 0) sendBins[1]++;\n           else if (points[i].x > 0 && points[i].y < 0) sendBins[2]++;\n           else if (points[i].x < 0 && points[i].y < 0) sendBins[3]++;\n       }\n       MPI_Send(sendBins.data(), bins.size(), MPI_INT, 0, 100, MPI_COMM_WORLD);\n   }\n}",
            "auto pointsPerThread = points.size() / omp_get_num_threads();\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      auto &pt = points[i];\n      if (pt.x >= 0 && pt.y >= 0) {\n         #pragma omp atomic\n         bins[0] += 1;\n      }\n      if (pt.x < 0 && pt.y >= 0) {\n         #pragma omp atomic\n         bins[1] += 1;\n      }\n      if (pt.x >= 0 && pt.y < 0) {\n         #pragma omp atomic\n         bins[2] += 1;\n      }\n      if (pt.x < 0 && pt.y < 0) {\n         #pragma omp atomic\n         bins[3] += 1;\n      }\n   }\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (nproc == 1) {\n    size_t n = points.size();\n    std::array<size_t, 4> bins_l = {0,0,0,0};\n    std::vector<Point> points_l = points;\n    // count in serial\n    for (size_t i=0; i<n; i++) {\n      double x = points_l[i].x;\n      double y = points_l[i].y;\n      if (x >= 0) {\n        if (y >= 0)\n          bins_l[0]++;\n        else\n          bins_l[1]++;\n      }\n      else {\n        if (y >= 0)\n          bins_l[2]++;\n        else\n          bins_l[3]++;\n      }\n    }\n    if (rank == 0) bins = bins_l;\n  }\n  else {\n    // count in parallel\n  }\n}",
            "const int num_threads = omp_get_max_threads();\n   int const num_points = points.size();\n\n   #pragma omp parallel num_threads(num_threads)\n   {\n      // Each thread should have its own copy of bins\n      std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n      #pragma omp for\n      for (int i = 0; i < num_points; ++i) {\n         Point const& p = points[i];\n         if (p.x > 0) {\n            if (p.y > 0) {\n               // Quadrant I\n               ++localBins[0];\n            }\n            else {\n               // Quadrant II\n               ++localBins[1];\n            }\n         }\n         else {\n            if (p.y > 0) {\n               // Quadrant III\n               ++localBins[2];\n            }\n            else {\n               // Quadrant IV\n               ++localBins[3];\n            }\n         }\n      }\n\n      // Merge localBins into bins\n      #pragma omp critical\n      {\n         for (int i = 0; i < 4; ++i) {\n            bins[i] += localBins[i];\n         }\n      }\n   }\n}",
            "/* Your solution goes here */\n}",
            "int nproc, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    // Create local data\n    std::vector<Point> local_points(points.size() / nproc);\n\n    // Send data to other processors\n    MPI_Scatter(&points[0], points.size() / nproc, MPI_DOUBLE, &local_points[0], local_points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Now, each processor has a different copy of points.\n    // Count on each processor\n    //...\n\n    // Collect counts to rank 0\n    MPI_Gather(&local_points[0], local_points.size(), MPI_DOUBLE, &bins[0], local_points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "// TO DO: implement parallel counting\n\n   // Use MPI_Reduce to merge counts from all ranks into bins.\n   // Use MPI_Comm_rank to determine the rank number\n   // Use MPI_Comm_size to determine the total number of ranks\n   // Use MPI_Type_vector to define a custom MPI data type (see MPI documentation)\n   // Use MPI_Reduce to combine the counts into bins on rank 0\n\n}",
            "// Count the number of points in each quadrant.\n   // Hint:\n   //     - OpenMP can be used to parallelize the loop over the points.\n   //     - OpenMP is thread safe.\n   //     - MPI can be used to parallelize the loop over the ranks.\n   //     - Make sure to synchronize all MPI ranks after the loop.\n\n   int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   const int local_points_count = points.size() / size;\n   const int remainder = points.size() % size;\n   const int local_start = std::min(rank, remainder) * (local_points_count + 1) + std::max(0, rank - remainder) * local_points_count;\n   const int local_end = rank < remainder? local_start + local_points_count + 1 : local_start + local_points_count;\n   #pragma omp parallel for\n   for (int i = local_start; i < local_end; i++) {\n      int local_count = 0;\n      if (points[i].x > 0 && points[i].y > 0) {\n         local_count = 1;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         local_count = 2;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         local_count = 3;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         local_count = 4;\n      }\n      bins[local_count]++;\n   }\n   if (rank == 0) {\n     int total_count = 0;\n     MPI_Reduce(&bins, &total_count, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     bins = { total_count, total_count, total_count, total_count };\n   } else {\n     MPI_Reduce(&bins, NULL, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins[0]);\n    MPI_Comm_rank(MPI_COMM_WORLD, &bins[1]);\n    int *pointsInRank = (int *)malloc(sizeof(int) * (points.size() / bins[0]));\n    int totalPoints;\n    int *sendcounts = (int *)malloc(sizeof(int) * (bins[0]));\n    int *displs = (int *)malloc(sizeof(int) * (bins[0]));\n    for (int i = 0; i < bins[0]; ++i) {\n        sendcounts[i] = points.size() / bins[0];\n        displs[i] = sendcounts[i] * i;\n    }\n    sendcounts[bins[1]] = points.size() % bins[0];\n\n    MPI_Scatterv(points.data(), sendcounts, displs, MPI_DOUBLE, pointsInRank, sendcounts[bins[1]], MPI_DOUBLE, 0,\n        MPI_COMM_WORLD);\n\n    std::array<size_t, 4> binsInRank = {0, 0, 0, 0};\n    for (int i = 0; i < (int)points.size() / bins[0]; ++i) {\n        if (pointsInRank[i].x > 0 && pointsInRank[i].y > 0)\n            binsInRank[0]++;\n        if (pointsInRank[i].x < 0 && pointsInRank[i].y > 0)\n            binsInRank[1]++;\n        if (pointsInRank[i].x < 0 && pointsInRank[i].y < 0)\n            binsInRank[2]++;\n        if (pointsInRank[i].x > 0 && pointsInRank[i].y < 0)\n            binsInRank[3]++;\n    }\n\n    int *temp = (int *)malloc(sizeof(int) * (bins[0]));\n    for (int i = 0; i < 4; ++i)\n        temp[i] = binsInRank[i];\n\n    MPI_Reduce(temp, &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Finalize();\n}",
            "size_t n = points.size();\n   std::array<size_t, 4> counts = {0, 0, 0, 0};\n   int myrank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n   if (myrank == 0) {\n      for (size_t i = 0; i < n; ++i) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            ++counts[0];\n         } else if (points[i].x < 0 && points[i].y >= 0) {\n            ++counts[1];\n         } else if (points[i].x >= 0 && points[i].y < 0) {\n            ++counts[2];\n         } else {\n            ++counts[3];\n         }\n      }\n   }\n   else {\n      int myrank, nprocs;\n      MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n      MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n      for (int i = myrank; i < n; i += nprocs) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            ++counts[0];\n         } else if (points[i].x < 0 && points[i].y >= 0) {\n            ++counts[1];\n         } else if (points[i].x >= 0 && points[i].y < 0) {\n            ++counts[2];\n         } else {\n            ++counts[3];\n         }\n      }\n   }\n   MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: insert code here\n   int num_threads;\n   #pragma omp parallel\n   {\n      num_threads = omp_get_num_threads();\n   }\n   int num_procs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (rank == 0)\n   {\n      std::vector<std::vector<Point>> image(num_procs, std::vector<Point>(points.size() / num_procs));\n      for (int i = 0; i < points.size(); i++)\n      {\n         image[i % num_procs].push_back(points[i]);\n      }\n   }\n   else\n   {\n      std::vector<std::vector<Point>> image(1, std::vector<Point>(points.size() / num_procs));\n      for (int i = 0; i < points.size(); i++)\n      {\n         image[0].push_back(points[i]);\n      }\n   }\n   std::vector<std::vector<Point>> image_reduced(1, std::vector<Point>(points.size() / num_procs));\n   for (int i = 0; i < image[0].size(); i++)\n   {\n      image_reduced[0].push_back(image[0][i]);\n   }\n   MPI_Reduce(&image, &image_reduced, 1, MPI_POINT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0)\n   {\n      std::vector<size_t> bins_temp(4, 0);\n      for (int i = 0; i < image_reduced[0].size(); i++)\n      {\n         int count = 0;\n         if (image_reduced[0][i].x >= 0 && image_reduced[0][i].y >= 0)\n         {\n            count = 0;\n         }\n         else if (image_reduced[0][i].x <= 0 && image_reduced[0][i].y >= 0)\n         {\n            count = 1;\n         }\n         else if (image_reduced[0][i].x <= 0 && image_reduced[0][i].y <= 0)\n         {\n            count = 2;\n         }\n         else if (image_reduced[0][i].x >= 0 && image_reduced[0][i].y <= 0)\n         {\n            count = 3;\n         }\n         bins_temp[count] += 1;\n      }\n      bins[0] = bins_temp[0];\n      bins[1] = bins_temp[1];\n      bins[2] = bins_temp[2];\n      bins[3] = bins_temp[3];\n   }\n}",
            "int size, rank, i;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      #pragma omp parallel for private(i)\n      for (i = 0; i < points.size(); i++) {\n         bins[0] += points[i].x >= 0 && points[i].y >= 0;\n         bins[1] += points[i].x < 0 && points[i].y >= 0;\n         bins[2] += points[i].x >= 0 && points[i].y < 0;\n         bins[3] += points[i].x < 0 && points[i].y < 0;\n      }\n   } else {\n      #pragma omp parallel for private(i)\n      for (i = 0; i < points.size(); i++) {\n         if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n         else if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n         else if (points[i].x >= 0 && points[i].y < 0) bins[2]++;\n         else bins[3]++;\n      }\n   }\n   MPI_Reduce(&bins, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: insert your code here\n}",
            "// TODO\n}",
            "bins = {0, 0, 0, 0};\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  const int n = points.size();\n  const int start = rank*n/size;\n  const int end = (rank + 1)*n/size;\n\n  #pragma omp parallel for\n  for (int i = start; i < end; ++i) {\n    const Point &p = points[i];\n    if (p.x >= 0 && p.y >= 0) {\n      #pragma omp atomic\n      ++bins[0];\n    } else if (p.x < 0 && p.y >= 0) {\n      #pragma omp atomic\n      ++bins[1];\n    } else if (p.x < 0 && p.y < 0) {\n      #pragma omp atomic\n      ++bins[2];\n    } else if (p.x >= 0 && p.y < 0) {\n      #pragma omp atomic\n      ++bins[3];\n    } else {\n      throw std::logic_error(\"This should never happen\");\n    }\n  }\n\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "if (bins.size()!= 4) {\n      throw std::runtime_error(\"bins.size() must be 4\");\n   }\n   // YOUR CODE HERE\n}",
            "// TODO: your code here\n}",
            "}",
            "#pragma omp parallel\n    {\n        int nthreads = omp_get_num_threads();\n        int rank = omp_get_thread_num();\n        int total = omp_get_num_threads();\n        int local_n = points.size()/total;\n        int local_start = rank * local_n;\n        int local_end = local_start + local_n;\n\n        int up_q1 = 0;\n        int up_q2 = 0;\n        int up_q3 = 0;\n        int up_q4 = 0;\n        int down_q1 = 0;\n        int down_q2 = 0;\n        int down_q3 = 0;\n        int down_q4 = 0;\n\n        #pragma omp for\n        for (int i = local_start; i < local_end; i++) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                up_q1++;\n            }\n            else if (points[i].x < 0 && points[i].y > 0) {\n                up_q2++;\n            }\n            else if (points[i].x < 0 && points[i].y < 0) {\n                up_q3++;\n            }\n            else if (points[i].x > 0 && points[i].y < 0) {\n                up_q4++;\n            }\n        }\n\n        #pragma omp for\n        for (int i = local_start; i < local_end; i++) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                down_q1++;\n            }\n            else if (points[i].x < 0 && points[i].y > 0) {\n                down_q2++;\n            }\n            else if (points[i].x < 0 && points[i].y < 0) {\n                down_q3++;\n            }\n            else if (points[i].x > 0 && points[i].y < 0) {\n                down_q4++;\n            }\n        }\n\n        MPI_Reduce(&up_q1, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&up_q2, &bins[1], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&up_q3, &bins[2], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&up_q4, &bins[3], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&down_q1, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&down_q2, &bins[1], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&down_q3, &bins[2], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&down_q4, &bins[3], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "// TODO\n\n}",
            "// your code here\n}",
            "// TODO\n}",
            "//TODO\n}",
            "// Your code here\n}",
            "size_t N = points.size();\n   size_t Nt = 1; // The number of threads we should use\n   size_t Np = 1; // The number of points each thread should process\n   size_t Nt_per_node = 1; // The number of threads per node\n   size_t Np_per_node = 1; // The number of points per node\n\n   bins.fill(0);\n\n   #pragma omp parallel\n   {\n      // Determine the number of threads we will use.\n      size_t id = omp_get_thread_num();\n      size_t Np_node = (N + Nt_per_node - 1) / Nt_per_node;\n      size_t Np_my_node = std::min(Np_node, N - id * Np_node);\n      Nt = (Np_my_node + Np - 1) / Np;\n      Np = (Np_my_node + Nt - 1) / Nt;\n      Np_per_node = (Np_my_node + Nt_per_node - 1) / Nt_per_node;\n      // Np_per_node = (Np_node + Nt_per_node - 1) / Nt_per_node;\n\n      // Find the offset into the points vector for this thread.\n      size_t offset = id * Np_my_node;\n\n      // Count the points.\n      for (size_t i = 0; i < Np; ++i) {\n         // Determine the quadrant for this point.\n         double x = points[offset].x;\n         double y = points[offset].y;\n         size_t q = (x > 0) + 2 * (y > 0);\n         // Add 1 to the bin for this quadrant.\n         #pragma omp atomic\n         bins[q] += 1;\n         // Go to the next point.\n         offset += Nt * Np_per_node;\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n   #pragma omp parallel for\n   for (size_t i=0; i < points.size(); ++i) {\n      const Point& p = points[i];\n      int q;\n      if (p.x >= 0 && p.y >= 0) {\n         q = 0;\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         q = 1;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         q = 2;\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         q = 3;\n      }\n      #pragma omp atomic\n      ++bins[q];\n   }\n}",
            "int numproc, rank;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   size_t num_points_per_rank = points.size()/numproc;\n\n   // TODO: fill the bins vector with the count of points in each quadrant\n   // Each rank should have a complete copy of points and should fill bins\n   // Hint: bins[0] = count of points in top right quadrant (x>0, y>0)\n   // Hint: bins[1] = count of points in bottom right quadrant (x>0, y<=0)\n   // Hint: bins[2] = count of points in bottom left quadrant (x<=0, y<=0)\n   // Hint: bins[3] = count of points in top left quadrant (x<=0, y>0)\n\n\n\n\n   // TODO: Use MPI to exchange the counts between ranks and fill in bins\n   // Hint: use `MPI_Recv` and `MPI_Send`\n   // Hint: use `MPI_Alltoall` if you want to avoid explicit send/recv\n\n\n\n\n}",
            "}",
            "// Replace this code with a correct implementation\n   int nprocs, myrank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   const int N = points.size();\n\n   if (nprocs == 1) {\n\n      for (size_t i = 0; i < N; i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n\n         if (x >= 0 && y >= 0) {\n            bins[0]++;\n         } else if (x < 0 && y >= 0) {\n            bins[1]++;\n         } else if (x < 0 && y < 0) {\n            bins[2]++;\n         } else if (x >= 0 && y < 0) {\n            bins[3]++;\n         }\n      }\n\n   } else {\n\n      int i, n_per_proc = N/nprocs;\n      std::vector<Point> my_points;\n\n      for (i = myrank*n_per_proc; i < (myrank+1)*n_per_proc; i++) {\n         my_points.push_back(points[i]);\n      }\n\n      if (myrank == nprocs - 1) {\n         for (i = (myrank+1)*n_per_proc; i < N; i++) {\n            my_points.push_back(points[i]);\n         }\n      }\n\n      // Count the points in my_points\n      std::array<size_t, 4> my_bins = { 0, 0, 0, 0 };\n      for (size_t i = 0; i < my_points.size(); i++) {\n         double x = my_points[i].x;\n         double y = my_points[i].y;\n\n         if (x >= 0 && y >= 0) {\n            my_bins[0]++;\n         } else if (x < 0 && y >= 0) {\n            my_bins[1]++;\n         } else if (x < 0 && y < 0) {\n            my_bins[2]++;\n         } else if (x >= 0 && y < 0) {\n            my_bins[3]++;\n         }\n      }\n\n      // Gather all the bins to rank 0\n      MPI_Gather(&my_bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   }\n}",
            "// TODO: Implement this function\n}",
            "int size, rank, flag;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    //int numberOfThreads = 4;\n    int numberOfThreads;\n    MPI_Status status;\n    int flag_send = 0;\n    int count = 0;\n    int i = 0;\n    int count_x;\n    int count_y;\n    int bins_x[4];\n    int bins_y[4];\n    int count_x_total;\n    int count_y_total;\n    int bins_x_total[4];\n    int bins_y_total[4];\n\n    if (rank == 0)\n    {\n        for (i = 0; i < 4; i++)\n        {\n            bins[i] = 0;\n        }\n\n        int threadCount = 0;\n        if (flag == 0)\n        {\n            //int threadCount = 0;\n            //while (threadCount < numberOfThreads)\n            //{\n                int threadCount = 0;\n                //int flag_rec = 0;\n                flag = 1;\n                MPI_Send(&points, points.size(), MPI_DOUBLE, rank + 1, 1, MPI_COMM_WORLD);\n            //}\n        }\n        //else if (flag == 1)\n        //{\n            flag = 0;\n            //int threadCount = 0;\n            //while (threadCount < numberOfThreads)\n            //{\n                //int threadCount = 0;\n                //int flag_rec = 0;\n                MPI_Recv(&points, points.size(), MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD, &status);\n            //}\n        //}\n        //while (rank > 0)\n        //{\n            MPI_Recv(&points, points.size(), MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD, &status);\n            flag = 0;\n        //}\n    }\n\n    if (rank!= 0)\n    {\n        for (i = 0; i < 4; i++)\n        {\n            bins[i] = 0;\n        }\n\n        //while (rank > 1)\n        //{\n            if (flag == 1)\n            {\n                int threadCount = 0;\n                while (threadCount < numberOfThreads)\n                {\n                    omp_set_num_threads(numberOfThreads);\n\n                    //#pragma omp parallel for schedule(dynamic, 1)\n                    //#pragma omp parallel for\n                    //#pragma omp for\n                    #pragma omp parallel\n                    for (i = 0; i < points.size(); i++)\n                    {\n                        if (points[i].x < 0)\n                        {\n                            if (points[i].y < 0)\n                            {\n                                bins[0] += 1;\n                            }\n                            else\n                            {\n                                bins[1] += 1;\n                            }\n                        }\n                        else\n                        {\n                            if (points[i].y < 0)\n                            {\n                                bins[2] += 1;\n                            }\n                            else\n                            {\n                                bins[3] += 1;\n                            }\n                        }\n                    }\n                    threadCount++;\n                }\n                flag = 0;\n            }\n            else if (flag == 0)\n            {\n                //int threadCount = 0;\n                MPI_Send(&points, points.size(), MPI_DOUBLE, rank - 1, 1, MPI_COMM_WORLD);\n                flag = 1;\n            }\n        //}\n        //while (rank < size - 1)\n        //{\n            MPI_Recv(&points, points.size(), MPI_DOUBLE, rank + 1, 1, MPI_COMM_WORLD, &status);\n            flag",
            "// TODO: parallelize and vectorize this loop\n\n    for (auto const& p : points) {\n        int x = p.x > 0? 0 : p.x < 0? 1 : 2;\n        int y = p.y > 0? 0 : p.y < 0? 2 : 1;\n        bins[x + 2*y]++;\n    }\n}",
            "int rank = 0, size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: Implement this function\n}",
            "// TODO\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for(auto point : points){\n         int index = -1;\n         if(point.x > 0){\n            index = point.y > 0? 0 : 1;\n         } else {\n            index = point.y > 0? 2 : 3;\n         }\n         #pragma omp atomic\n         ++bins[index];\n      }\n   }\n}",
            "// TODO\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for\n      for (size_t i = 0; i < points.size(); ++i) {\n         if (points[i].x < 0) {\n            if (points[i].y < 0) {\n               #pragma omp atomic\n               bins[0]++;\n            } else {\n               #pragma omp atomic\n               bins[1]++;\n            }\n         } else if (points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n         } else {\n            #pragma omp atomic\n            bins[3]++;\n         }\n      }\n   }\n}",
            "int size = points.size();\n   int nprocs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // allocate and initialize bins\n   bins.fill(0);\n\n   if (rank == 0) {\n      // master process does all the work\n      // count points in each quadrant\n      #pragma omp parallel for\n      for (int i = 0; i < size; i++) {\n         int quadrant = 0;\n         if (points[i].x < 0) quadrant += 1;\n         if (points[i].y < 0) quadrant += 2;\n         bins[quadrant]++;\n      }\n   }\n   else {\n      // slave processes simply send the count to the master process\n      MPI_Send(&size, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n   // gather the results\n   MPI_Gather(&size, 1, MPI_INT, &size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    std::vector<Point> local_points;\n    if (rank == 0) {\n        local_points = points;\n    }\n    MPI_Bcast(&local_points[0], points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int points_per_thread = local_points.size() / nproc;\n    int points_per_thread_extra = local_points.size() % nproc;\n    int points_offset = rank * (points_per_thread + points_per_thread_extra);\n    int points_count = points_per_thread + points_per_thread_extra;\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 4> thread_bins{0, 0, 0, 0};\n        #pragma omp for\n        for (int i = 0; i < points_count; ++i) {\n            int x_bin, y_bin;\n            if (local_points[i+points_offset].x >= 0.0 && local_points[i+points_offset].y >= 0.0) {\n                x_bin = 0;\n                y_bin = 0;\n            } else if (local_points[i+points_offset].x < 0.0 && local_points[i+points_offset].y >= 0.0) {\n                x_bin = 1;\n                y_bin = 0;\n            } else if (local_points[i+points_offset].x >= 0.0 && local_points[i+points_offset].y < 0.0) {\n                x_bin = 0;\n                y_bin = 1;\n            } else {\n                x_bin = 1;\n                y_bin = 1;\n            }\n            ++thread_bins[x_bin * 2 + y_bin];\n        }\n        #pragma omp critical\n        for (int i = 0; i < 4; ++i) {\n            bins[i] += thread_bins[i];\n        }\n    }\n\n    MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank = 0;\n    int num_proc = 0;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n    int count = points.size() / num_proc;\n    int remainder = points.size() % num_proc;\n\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    std::vector<Point> points_private;\n    for (int i = 0; i < remainder; i++) {\n        points_private.push_back(points[i]);\n    }\n\n    for (int i = remainder; i < points.size(); i++) {\n        points_private.push_back(points[i + (num_proc - remainder - 1) * count]);\n    }\n\n    int num_threads = omp_get_max_threads();\n\n#pragma omp parallel\n    {\n        int local_counts[4] = {0, 0, 0, 0};\n\n        #pragma omp for schedule(static)\n        for (int i = 0; i < points_private.size(); i++) {\n            if (points_private[i].x >= 0 && points_private[i].y >= 0) local_counts[0]++;\n            if (points_private[i].x < 0 && points_private[i].y >= 0) local_counts[1]++;\n            if (points_private[i].x < 0 && points_private[i].y < 0) local_counts[2]++;\n            if (points_private[i].x >= 0 && points_private[i].y < 0) local_counts[3]++;\n        }\n\n        #pragma omp critical\n        {\n            for (int i = 0; i < 4; i++) {\n                bins[i] += local_counts[i];\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// TODO: Your code here\n    int size;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int npoints = points.size();\n    int npointsPerRank = npoints / size;\n    int npointsRemaining = npoints % size;\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&points[i * npointsPerRank], npointsPerRank, MPI_DOUBLE, i, 1, MPI_COMM_WORLD);\n            if (i < npointsRemaining) {\n                MPI_Send(&points[(i * npointsPerRank) + npointsPerRank], 1, MPI_DOUBLE, i, 1, MPI_COMM_WORLD);\n            }\n        }\n\n        for (int i = 0; i < npointsPerRank; i++) {\n            bins[rankBin(points[i].x, points[i].y)]++;\n        }\n    } else {\n        std::vector<Point> pointsRank(npointsPerRank + (rank < npointsRemaining? 1 : 0));\n        MPI_Recv(&pointsRank[0], npointsPerRank + (rank < npointsRemaining? 1 : 0), MPI_DOUBLE, 0, 1, MPI_COMM_WORLD,\n                MPI_STATUS_IGNORE);\n\n        for (int i = 0; i < pointsRank.size(); i++) {\n            bins[rankBin(pointsRank[i].x, pointsRank[i].y)]++;\n        }\n    }\n\n    if (rank!= 0) {\n        MPI_Send(&bins[0], 4, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            std::array<size_t, 4> binsRank;\n            MPI_Recv(&binsRank[0], 4, MPI_DOUBLE, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < 4; j++) {\n                bins[j] += binsRank[j];\n            }\n        }\n    }\n}",
            "bins.fill(0);\n\n   // TODO: Fill in the code here to compute the bins using MPI and OpenMP\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (rank == 0) {\n      // The master node (rank 0)\n\n      std::vector<size_t> bins_temp;\n      bins_temp.resize(4);\n\n      // distribute the work to the other nodes\n      for (int i = 1; i < size; i++) {\n         int start_i = i * points.size() / size;\n         int end_i = (i + 1) * points.size() / size;\n         MPI_Send(&points[start_i], end_i - start_i, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      }\n\n      // do the work on the master node\n      int start_i = 0;\n      int end_i = points.size() / size;\n      for (int i = 0; i < end_i; i++) {\n         if (points[i].x > 0) {\n            if (points[i].y > 0)\n               bins_temp[0]++;\n            else\n               bins_temp[1]++;\n         }\n         else {\n            if (points[i].y > 0)\n               bins_temp[2]++;\n            else\n               bins_temp[3]++;\n         }\n      }\n\n      // receive the results from the other nodes\n      for (int i = 1; i < size; i++) {\n         MPI_Status status;\n         MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n         int count;\n         MPI_Get_count(&status, MPI_DOUBLE, &count);\n         MPI_Recv(&bins_temp[0], count, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n\n      // aggregate the results\n      for (size_t i = 0; i < 4; i++) {\n         for (int j = 0; j < size; j++) {\n            bins[i] += bins_temp[i];\n         }\n      }\n   }\n   else {\n      // The other nodes\n      int start_i = rank * points.size() / size;\n      int end_i = (rank + 1) * points.size() / size;\n      for (int i = start_i; i < end_i; i++) {\n         if (points[i].x > 0) {\n            if (points[i].y > 0)\n               bins[0]++;\n            else\n               bins[1]++;\n         }\n         else {\n            if (points[i].y > 0)\n               bins[2]++;\n            else\n               bins[3]++;\n         }\n      }\n      MPI_Send(bins, 4, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n   }\n\n   MPI_Finalize();\n}",
            "MPI_Status status;\n    int rank, size;\n    int bin[4] = {0, 0, 0, 0};\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int length = points.size() / size;\n    std::vector<Point> subPoints;\n    subPoints = std::vector<Point>(points.begin() + rank * length, points.begin() + (rank + 1) * length);\n\n    #pragma omp parallel for\n    for (int i = 0; i < subPoints.size(); i++) {\n        double x = subPoints[i].x;\n        double y = subPoints[i].y;\n        if (x > 0 && y > 0) {\n            bin[0]++;\n        } else if (x < 0 && y > 0) {\n            bin[1]++;\n        } else if (x > 0 && y < 0) {\n            bin[2]++;\n        } else {\n            bin[3]++;\n        }\n    }\n\n    MPI_Reduce(&bin, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank = 0;\n   int size = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      bins.fill(0);\n   }\n\n   std::vector<Point> local_points(points.size()/size);\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); ++i) {\n       int x = static_cast<int>(points[i].x);\n       int y = static_cast<int>(points[i].y);\n\n       if (rank == 0) {\n           if (x > 0 && y > 0) {\n               bins[0]++;\n           }\n           else if (x < 0 && y > 0) {\n               bins[1]++;\n           }\n           else if (x < 0 && y < 0) {\n               bins[2]++;\n           }\n           else if (x > 0 && y < 0) {\n               bins[3]++;\n           }\n       }\n\n       // fill local_points\n       local_points[i] = points[i];\n   }\n\n   std::array<size_t, 4> local_bins;\n   local_bins.fill(0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < local_points.size(); ++i) {\n       int x = static_cast<int>(local_points[i].x);\n       int y = static_cast<int>(local_points[i].y);\n\n       if (rank == 0) {\n           if (x > 0 && y > 0) {\n               local_bins[0]++;\n           }\n           else if (x < 0 && y > 0) {\n               local_bins[1]++;\n           }\n           else if (x < 0 && y < 0) {\n               local_bins[2]++;\n           }\n           else if (x > 0 && y < 0) {\n               local_bins[3]++;\n           }\n       }\n   }\n\n   std::array<size_t, 4> global_bins;\n   MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n       bins = global_bins;\n   }\n}",
            "int numproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  std::vector<Point> local_points;\n  // Divide the points among the processes\n  int size = points.size() / numproc;\n  if (rank == numproc - 1) {\n    local_points.resize(size + points.size() % numproc);\n  } else {\n    local_points.resize(size);\n  }\n  MPI_Scatter(points.data(), size, MPI_DOUBLE_INT, local_points.data(), size, MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < local_points.size(); i++) {\n    Point p = local_points[i];\n    if (p.x >= 0 && p.y >= 0) {\n      #pragma omp atomic\n      local_bins[0]++;\n    }\n    if (p.x < 0 && p.y >= 0) {\n      #pragma omp atomic\n      local_bins[1]++;\n    }\n    if (p.x < 0 && p.y < 0) {\n      #pragma omp atomic\n      local_bins[2]++;\n    }\n    if (p.x >= 0 && p.y < 0) {\n      #pragma omp atomic\n      local_bins[3]++;\n    }\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "if (MPI::COMM_WORLD.Get_rank() == 0) {\n      for (auto &p : points) {\n         int x = (p.x > 0) - (p.x < 0);\n         int y = (p.y > 0) - (p.y < 0);\n         bins[2*x+y]++;\n      }\n   }\n   else {\n      //TODO\n   }\n}",
            "// TODO\n}",
            "// Your code goes here\n   // You may use any MPI or OpenMP commands\n   int npes, rank, tid;\n   MPI_Comm_size(MPI_COMM_WORLD, &npes);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Part 1: Make a new list of points. Remove any points that are out of bounds\n   std::vector<Point> new_points;\n   for (auto& p: points) {\n      if ((p.x >= 0) && (p.y >= 0)) {\n         new_points.push_back(p);\n      }\n   }\n\n   // Part 2: Divide the points into chunks\n   int chunk_size = (int)new_points.size()/npes;\n   std::vector<Point> my_points;\n   if (rank == 0) {\n      for (int i=0; i<chunk_size; i++) {\n         my_points.push_back(new_points[i]);\n      }\n   } else {\n      for (int i=chunk_size*rank; i<chunk_size*(rank+1); i++) {\n         my_points.push_back(new_points[i]);\n      }\n   }\n\n   // Part 3: Count the number of points in each chunk\n   int chunk_quad_count[4];\n   chunk_quad_count[0] = 0;\n   chunk_quad_count[1] = 0;\n   chunk_quad_count[2] = 0;\n   chunk_quad_count[3] = 0;\n   #pragma omp parallel for\n   for (int i=0; i<(int)my_points.size(); i++) {\n      auto p = my_points[i];\n      if ((p.x >= 0) && (p.y >= 0)) {\n         chunk_quad_count[0]++;\n      } else if ((p.x < 0) && (p.y >= 0)) {\n         chunk_quad_count[1]++;\n      } else if ((p.x >= 0) && (p.y < 0)) {\n         chunk_quad_count[2]++;\n      } else {\n         chunk_quad_count[3]++;\n      }\n   }\n\n   // Part 4: Add up the counts of each chunk\n   if (rank == 0) {\n      for (int i=1; i<npes; i++) {\n         MPI_Recv(&chunk_quad_count, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         bins[0] += chunk_quad_count[0];\n         bins[1] += chunk_quad_count[1];\n         bins[2] += chunk_quad_count[2];\n         bins[3] += chunk_quad_count[3];\n      }\n   } else {\n      MPI_Send(&chunk_quad_count, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Count the number of points in each quadrant on rank 0, and sum the counts to find the total number of points\n   size_t count = 0;\n   if (rank == 0) {\n      for (auto const& p : points) {\n         if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n         }\n         else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n         }\n         else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n         }\n         else if (p.x >= 0 && p.y < 0) {\n            bins[3]++;\n         }\n         count++;\n      }\n   }\n\n   // Sum up the counts from each rank\n   int count_send = count;\n   int count_recv;\n   MPI_Reduce(&count_send, &count_recv, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Distribute the input points to each rank\n   int num_points = count / size;\n   int start_idx = rank * num_points;\n   int end_idx = (rank + 1) * num_points;\n   if (rank == (size - 1)) {\n      end_idx = count;\n   }\n\n   // Count the number of points in each quadrant on each rank\n   if (rank == 0) {\n      for (size_t i = 0; i < bins.size(); i++) {\n         bins[i] = 0;\n      }\n   }\n   for (int i = start_idx; i < end_idx; i++) {\n      auto const& p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n\n   // Sum the counts from each rank\n   MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n}",
            "if (bins.size()!= 4)\n   {\n      // ERROR: bins must have length 4\n      throw \"ERROR\";\n   }\n\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   auto start = rank * points.size() / size;\n   auto end = (rank + 1) * points.size() / size;\n\n   if (rank == 0)\n   {\n      // ERROR: bins must have length 4\n      throw \"ERROR\";\n   }\n\n   for (int i = start; i < end; i++)\n   {\n      if (points[i].x > 0 && points[i].y > 0)\n      {\n         bins[0]++;\n      }\n      else if (points[i].x < 0 && points[i].y > 0)\n      {\n         bins[1]++;\n      }\n      else if (points[i].x > 0 && points[i].y < 0)\n      {\n         bins[2]++;\n      }\n      else if (points[i].x < 0 && points[i].y < 0)\n      {\n         bins[3]++;\n      }\n   }\n\n   if (rank == 0)\n   {\n      for (int i = 1; i < size; i++)\n      {\n         int temp_bins[4];\n         MPI_Recv(temp_bins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j = 0; j < 4; j++)\n         {\n            bins[j] += temp_bins[j];\n         }\n      }\n   }\n   else\n   {\n      int temp_bins[4];\n      for (int i = 0; i < 4; i++)\n      {\n         temp_bins[i] = bins[i];\n      }\n      MPI_Send(temp_bins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "/* Your solution goes here  */\n}",
            "int mpi_size, mpi_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   const int root = 0;\n   const int width = points.size();\n   const int size = width/mpi_size;\n   const int r = width%mpi_size;\n   std::vector<int> local(size + (mpi_rank < r? 1 : 0));\n\n   if (mpi_rank == 0)\n   {\n      for (int i=1; i<mpi_size; ++i)\n         MPI_Send(points.data()+i*size, size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      for (int i=0; i<mpi_size; ++i)\n      {\n         MPI_Status status;\n         MPI_Recv(local.data(), size + (i < r? 1 : 0), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n      }\n      for (int i=0; i<width; ++i)\n      {\n         if (points[i].x >= 0.0 && points[i].y >= 0.0)\n            bins[0]++;\n         else if (points[i].x < 0.0 && points[i].y >= 0.0)\n            bins[1]++;\n         else if (points[i].x < 0.0 && points[i].y < 0.0)\n            bins[2]++;\n         else if (points[i].x >= 0.0 && points[i].y < 0.0)\n            bins[3]++;\n      }\n   }\n   else\n   {\n      MPI_Status status;\n      MPI_Recv(local.data(), size + (mpi_rank < r? 1 : 0), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n      int count[4] = {0, 0, 0, 0};\n      #pragma omp parallel for\n      for (int i=0; i<local.size(); ++i)\n      {\n         if (local[i].x >= 0.0 && local[i].y >= 0.0)\n            count[0]++;\n         else if (local[i].x < 0.0 && local[i].y >= 0.0)\n            count[1]++;\n         else if (local[i].x < 0.0 && local[i].y < 0.0)\n            count[2]++;\n         else if (local[i].x >= 0.0 && local[i].y < 0.0)\n            count[3]++;\n      }\n      MPI_Gather(count, 4, MPI_INT, bins.data(), 4, MPI_INT, root, MPI_COMM_WORLD);\n   }\n}",
            "int myrank;\n  int procs;\n  int source;\n  int dest;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  // Number of points per rank\n  int points_per_rank = points.size() / procs;\n\n  // Number of points left over\n  int leftover = points.size() % procs;\n\n  // Calculate where the data goes\n  std::vector<size_t> bins_per_rank(procs);\n  for(int i = 0; i < procs; ++i) {\n    bins_per_rank[i] = points_per_rank;\n\n    // Add one to any ranks with leftover points\n    if(i < leftover) {\n      bins_per_rank[i]++;\n    }\n  }\n\n  // Calculate where my data starts\n  int data_start = 0;\n  for(int i = 0; i < myrank; ++i) {\n    data_start += bins_per_rank[i];\n  }\n\n  // Get a pointer to my data\n  Point* my_points = &points[data_start];\n\n  // Count the number of points in each quadrant\n  std::array<size_t, 4> bins_local;\n  std::fill(bins_local.begin(), bins_local.end(), 0);\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for(int i = 0; i < bins_per_rank[myrank]; ++i) {\n      if(my_points[i].x >= 0) {\n        if(my_points[i].y >= 0) {\n          bins_local[0]++;\n        } else {\n          bins_local[1]++;\n        }\n      } else {\n        if(my_points[i].y >= 0) {\n          bins_local[2]++;\n        } else {\n          bins_local[3]++;\n        }\n      }\n    }\n  }\n\n  // Gather all counts into bins on rank 0\n  MPI_Reduce(bins_local.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// count the points in the first quadrant, second quadrant, third quadrant, and fourth quadrant\n   #pragma omp parallel for reduction(+:bins[0]:bins[1]:bins[2]:bins[3])\n   for (auto const& point : points) {\n      int index = 0;\n      if (point.x >= 0 && point.y >= 0) {\n         index = 0;\n      } else if (point.x >= 0 && point.y < 0) {\n         index = 1;\n      } else if (point.x < 0 && point.y >= 0) {\n         index = 2;\n      } else if (point.x < 0 && point.y < 0) {\n         index = 3;\n      }\n      ++bins[index];\n   }\n}",
            "// The size of each rank\n    std::size_t size = points.size();\n    // The rank number\n    int rank = 0;\n    // The number of ranks\n    int p = 0;\n\n    // Getting the rank\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // Getting the number of ranks\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    // Calculate the start and end index of points of current rank\n    std::size_t begin = rank*size/p;\n    std::size_t end = (rank+1)*size/p;\n\n    // Count in parallel\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    #pragma omp parallel for\n    for (std::size_t i = begin; i < end; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n\n    // Combine results from all ranks\n    MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Output bins in rank 0\n    if (rank == 0) {\n        for (auto v : bins) {\n            std::cout << v << \" \";\n        }\n        std::cout << \"\\n\";\n    }\n\n}",
            "}",
            "// TODO: Implement this\n   bins = {0, 0, 0, 0};\n\n#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0] += 1;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1] += 1;\n      } else if (p.x >= 0 && p.y < 0) {\n         bins[2] += 1;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int n, i, k;\n   if (rank == 0)\n   {\n       bins = std::array<size_t, 4>{{0,0,0,0}};\n       n = points.size();\n   }\n   MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   if (rank!= 0)\n   {\n       points.resize(n);\n   }\n   MPI_Bcast(points.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   if (rank == 0)\n   {\n       // for (i = 0; i < n; i++) {\n       //    k = points[i].x >= 0? points[i].y >= 0? 0 : 1 : points[i].y >= 0? 2 : 3;\n       //    bins[k]++;\n       // }\n       k = 0;\n       int rank, size;\n       MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n       MPI_Comm_size(MPI_COMM_WORLD, &size);\n       int n, i, k;\n       double t;\n       double *points = new double[n];\n       double *points_sub = new double[n / size];\n       double *bins = new double[4];\n       MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n       MPI_Bcast(points, n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n       for (i = rank; i < n; i += size) {\n           t = points[i];\n           if (t >= 0) {\n               if (t >= 0) {\n                   bins[0]++;\n               } else {\n                   bins[1]++;\n               }\n           } else {\n               if (t >= 0) {\n                   bins[2]++;\n               } else {\n                   bins[3]++;\n               }\n           }\n       }\n\n       MPI_Reduce(bins, points_sub, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n       MPI_Bcast(points_sub, 4, MPI_INT, 0, MPI_COMM_WORLD);\n       bins[0] = points_sub[0];\n       bins[1] = points_sub[1];\n       bins[2] = points_sub[2];\n       bins[3] = points_sub[3];\n   } else {\n       int rank, size;\n       MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n       MPI_Comm_size(MPI_COMM_WORLD, &size);\n       int n, i, k;\n       double *bins = new double[4];\n       double t;\n       double *points = new double[n];\n       double *points_sub = new double[n / size];\n       MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n       MPI_Bcast(points, n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n       for (i = rank; i < n; i += size) {\n           t = points[i];\n           if (t >= 0) {\n               if (t >= 0) {\n                   bins[0]++;\n               } else {\n                   bins[1]++;\n               }\n           } else {\n               if (t >= 0) {\n                   bins[2]++;\n               } else {\n                   bins[3]++;\n               }\n           }\n       }\n\n       MPI_Reduce(bins, points_sub, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WOR",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   const int numThreads = 4;\n   // TODO: Use MPI to distribute the workload.\n   // TODO: Use OpenMP to parallelize within each rank.\n\n   if (rank == 0) {\n      // TODO: Reduce the count results on rank 0\n   }\n}",
            "int nprocs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   //... add your code here...\n   if (rank == 0) {\n      for (size_t i = 0; i < points.size(); ++i) {\n         Point p = points[i];\n         int count = 0;\n         if (p.x < 0) {\n            count += 1;\n         }\n         if (p.y < 0) {\n            count += 2;\n         }\n         bins[count]++;\n      }\n   }\n}",
            "bins = {0,0,0,0};\n    std::vector<size_t> local_bins;\n    local_bins = {0,0,0,0};\n\n    size_t points_per_rank = points.size() / omp_get_num_threads();\n    size_t remainder = points.size() % omp_get_num_threads();\n    size_t offset = 0;\n    size_t local_start = 0;\n\n    // MPI_Barrier(MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < points.size(); i++) {\n            // std::cout << \"Point \" << i << \" is at thread \" << omp_get_thread_num() << std::endl;\n            if (i == offset) {\n                local_start = 0;\n                offset += points_per_rank;\n                if (remainder) {\n                    offset++;\n                    remainder--;\n                }\n            }\n            if (i >= local_start && i < offset) {\n                if (points[i].x > 0 && points[i].y > 0) {\n                    local_bins[0] += 1;\n                } else if (points[i].x < 0 && points[i].y > 0) {\n                    local_bins[1] += 1;\n                } else if (points[i].x > 0 && points[i].y < 0) {\n                    local_bins[2] += 1;\n                } else if (points[i].x < 0 && points[i].y < 0) {\n                    local_bins[3] += 1;\n                }\n            }\n        }\n    }\n\n    // std::cout << \"Rank \" << rank << \": \" << local_bins[0] << \" \" << local_bins[1] << \" \" << local_bins[2] << \" \" << local_bins[3] << std::endl;\n\n    MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "if (points.empty()) {\n      // what goes here?\n   }\n\n   // what goes here?\n\n   // This is wrong, but illustrates the general idea.\n   // MPI_Scatter(&bins, 4, MPI_UNSIGNED_LONG_LONG, &bins, 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   // What goes here?\n\n   // This is wrong, but illustrates the general idea.\n   // MPI_Reduce(MPI_IN_PLACE, &bins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n    bins = std::array<size_t, 4>{0,0,0,0};\n    // Count total points\n    size_t num_points = points.size();\n    // Number of points per rank\n    size_t num_points_per_rank = num_points / omp_get_num_threads();\n    // Vector to store the counts per rank\n    std::array<size_t, 4> local_bins = std::array<size_t, 4>{0,0,0,0};\n#pragma omp parallel\n    {\n    // Calculate number of points per rank\n    size_t start = omp_get_thread_num() * num_points_per_rank;\n    // Calculate last element of local vector\n    size_t end = start + num_points_per_rank;\n    if (omp_get_thread_num() == omp_get_num_threads() - 1)\n        end = num_points;\n    // Count points\n    for (size_t i = start; i < end; i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            local_bins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            local_bins[1]++;\n        } else if (points[i].x > 0 && points[i].y < 0) {\n            local_bins[2]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            local_bins[3]++;\n        }\n    }\n    }\n    // Reduce the local vectors to a single vector\n    for (size_t i = 0; i < 4; i++) {\n        MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t num_points = points.size();\n   size_t local_num_points = num_points / omp_get_num_threads();\n\n   // TODO: add OpenMP pragmas to parallelize over threads\n   // TODO: use MPI_Reduce to combine bins into bins on rank 0\n}",
            "//\n    // Your code here\n    //\n}",
            "if (bins.size()!= 4) {\n        throw std::invalid_argument(\"invalid argument\");\n    }\n\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    // The number of points in the current rank.\n    int local_count = points.size();\n\n    // The number of points in the entire rank.\n    int all_count = 0;\n\n    // Compute the counts using OpenMP.\n#pragma omp parallel for default(none) shared(points) reduction(+:all_count)\n    for (size_t i = 0; i < points.size(); i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            all_count++;\n        }\n    }\n\n    // Reduce the results of the OpenMP calculations using MPI.\n    MPI_Reduce(&all_count, &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n    int my_rank, p;\n    MPI_Comm_rank(comm, &my_rank);\n    MPI_Comm_size(comm, &p);\n\n    int local_size = points.size() / p;\n    int start_index = my_rank * local_size;\n    int end_index = start_index + local_size;\n\n    // Count points on each rank\n    size_t count = 0;\n    for (size_t i = start_index; i < end_index; i++) {\n        auto point = points[i];\n        if (point.x >= 0 && point.y >= 0)\n            count++;\n        else if (point.x < 0 && point.y < 0)\n            count++;\n        else if (point.x >= 0 && point.y < 0)\n            count++;\n        else if (point.x < 0 && point.y >= 0)\n            count++;\n    }\n\n    // Gather counts to rank 0\n    size_t *counts = nullptr;\n    if (my_rank == 0) {\n        counts = new size_t[p];\n    }\n    MPI_Gather(&count, 1, MPI_UNSIGNED_LONG, counts, 1, MPI_UNSIGNED_LONG, 0, comm);\n\n    // Sum counts on rank 0 and store in bins\n    if (my_rank == 0) {\n        bins[0] = counts[0];\n        bins[1] = counts[1];\n        bins[2] = counts[2];\n        bins[3] = counts[3];\n    }\n\n    // Clean up\n    if (my_rank == 0) {\n        delete[] counts;\n    }\n}",
            "const int n_procs = omp_get_num_procs();\n   const int rank = omp_get_thread_num();\n\n   std::vector<std::array<size_t, 4>> local_bins(n_procs);\n#pragma omp parallel for\n   for(auto it = points.begin(); it!= points.end(); ++it) {\n      int quadrant = 0;\n      if (it->x < 0.0 && it->y < 0.0) {\n         quadrant = 3;\n      } else if (it->x < 0.0) {\n         quadrant = 2;\n      } else if (it->y < 0.0) {\n         quadrant = 1;\n      } else {\n         quadrant = 0;\n      }\n      ++local_bins[rank][quadrant];\n   }\n\n   if (rank == 0) {\n      for (int i = 1; i < n_procs; ++i) {\n         for (int j = 0; j < 4; ++j) {\n            bins[j] += local_bins[i][j];\n         }\n      }\n   }\n}",
            "size_t n = points.size();\n   int mpi_size, mpi_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   std::vector<size_t> local_bins(4, 0);\n   int num_threads = omp_get_num_threads();\n   std::vector<size_t> local_thread_bins(4, 0);\n\n   #pragma omp parallel shared(points, local_bins, local_thread_bins) num_threads(num_threads)\n   {\n      size_t start = mpi_rank * n / mpi_size;\n      size_t end = (mpi_rank + 1) * n / mpi_size;\n      for(size_t i = start; i < end; ++i){\n         Point p = points.at(i);\n         if (p.x >= 0 && p.y >= 0){\n            #pragma omp atomic\n            local_thread_bins.at(0)++;\n         }\n         if (p.x < 0 && p.y >= 0){\n            #pragma omp atomic\n            local_thread_bins.at(1)++;\n         }\n         if (p.x < 0 && p.y < 0){\n            #pragma omp atomic\n            local_thread_bins.at(2)++;\n         }\n         if (p.x >= 0 && p.y < 0){\n            #pragma omp atomic\n            local_thread_bins.at(3)++;\n         }\n      }\n\n      #pragma omp for nowait\n      for(size_t i = 0; i < 4; ++i){\n         #pragma omp atomic\n         local_bins.at(i) += local_thread_bins.at(i);\n      }\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// use 2d blocks to partition the points\n   const int ndims = 2;\n   int nprocs, my_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   int mx = nprocs;\n   int my = nprocs;\n   // compute nblocks\n   int nblocks = mx * my;\n   // compute m,n\n   int m = points.size() / nblocks + 1;\n   int n = points.size() / nblocks + 1;\n   // compute block size\n   int block_size = m * n;\n   std::vector<Point> local_points;\n   for (int i = my_rank * block_size; i < my_rank * block_size + block_size; i++) {\n      if (i < points.size()) {\n         local_points.push_back(points[i]);\n      }\n   }\n\n   // calculate the number of quadrants per process\n   int n_quadrants = nblocks / 4;\n\n   std::array<size_t, 4> local_bins{};\n   if (my_rank == 0) {\n      local_bins[0] = n_quadrants;\n   } else if (my_rank == 1) {\n      local_bins[1] = n_quadrants;\n   } else if (my_rank == 2) {\n      local_bins[2] = n_quadrants;\n   } else if (my_rank == 3) {\n      local_bins[3] = n_quadrants;\n   }\n\n   #pragma omp parallel for num_threads(4)\n   for (int i = 0; i < local_bins.size(); i++) {\n      int index = local_bins[i];\n      std::array<size_t, 4> counts{};\n      for (int j = 0; j < local_points.size(); j++) {\n         int x = (local_points[j].x >= 0.0);\n         int y = (local_points[j].y >= 0.0);\n         counts[x + y * 2]++;\n      }\n      if (my_rank == 0) {\n         bins[index] += counts[0];\n      } else if (my_rank == 1) {\n         bins[index] += counts[1];\n      } else if (my_rank == 2) {\n         bins[index] += counts[2];\n      } else if (my_rank == 3) {\n         bins[index] += counts[3];\n      }\n   }\n}",
            "int nranks, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::array<size_t, 4> localBins;\n   for (int i = 0; i < 4; i++) {\n      localBins[i] = 0;\n   }\n\n   #pragma omp parallel for num_threads(4)\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         localBins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         localBins[1]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         localBins[2]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         localBins[3]++;\n      }\n   }\n\n   MPI_Reduce(&localBins, &bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Use MPI and OpenMP to parallelize this loop.\n\n    // Every rank has a complete copy of `points`\n    // The results of each rank need to be collected in `bins`.\n\n    // NOTE: The results on rank 0 need to be overwritten!\n    // NOTE: You can assume the array `bins` to be zero initialized\n\n    // Example:\n    // Input:\n    // {{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}}\n    // Output:\n    // bins = {3, 1, 0, 2}\n\n    if (rank == 0) {\n        for (auto &p : points) {\n            if (p.x > 0) {\n                if (p.y > 0) {\n                    bins[0]++;\n                } else {\n                    bins[3]++;\n                }\n            } else {\n                if (p.y > 0) {\n                    bins[1]++;\n                } else {\n                    bins[2]++;\n                }\n            }\n        }\n    }\n\n}",
            "// TODO: Implement\n}",
            "}",
            "//...\n}",
            "// fill in here\n  MPI_Status status;\n  MPI_Request req;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpisize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &mpirank);\n\n  int mpisize, mpirank;\n\n  double *mydata;\n  double *recvdata;\n  mydata = new double[points.size()];\n  recvdata = new double[points.size()];\n\n  int i;\n  for(int i=0; i<points.size(); i++){\n    mydata[i] = points[i].x*points[i].y;\n  }\n\n  MPI_Bcast(&mydata[0], points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&mydata[0], &recvdata[0], points.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  for(int i=0; i<points.size(); i++){\n    if (recvdata[i] > 0) {\n      bins[0]++;\n    } else if (recvdata[i] < 0) {\n      bins[1]++;\n    } else if (recvdata[i] == 0) {\n      bins[2]++;\n    } else if (recvdata[i] < 0) {\n      bins[3]++;\n    }\n  }\n\n  if (mpirank == 0) {\n    for(int i=0; i<points.size(); i++){\n      if (recvdata[i] > 0) {\n        bins[0]++;\n      } else if (recvdata[i] < 0) {\n        bins[1]++;\n      } else if (recvdata[i] == 0) {\n        bins[2]++;\n      } else if (recvdata[i] < 0) {\n        bins[3]++;\n      }\n    }\n  }\n\n  delete[] mydata;\n  delete[] recvdata;\n}",
            "// your implementation goes here\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    int rank;\n    int size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<int> pointsPerProc = std::vector<int>(size, points.size() / size);\n    std::vector<int> pointsPerProc2 = std::vector<int>(size, points.size() / size);\n\n    for (int i = 0; i < size - 1; i++) {\n        pointsPerProc2[i] = pointsPerProc[i];\n    }\n    pointsPerProc2[size - 1] = points.size() - (pointsPerProc.size() - 1) * (points.size() / size);\n\n    int sum = 0;\n    for (int i = 0; i < pointsPerProc.size(); i++) {\n        sum += pointsPerProc[i];\n    }\n\n    int sum2 = 0;\n    for (int i = 0; i < pointsPerProc2.size(); i++) {\n        sum2 += pointsPerProc2[i];\n    }\n\n    MPI_Bcast(pointsPerProc2.data(), size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    std::vector<Point> localPoints = std::vector<Point>(pointsPerProc2[rank]);\n\n    int offset = 0;\n    for (int i = 0; i < rank; i++) {\n        offset += pointsPerProc2[i];\n    }\n\n    for (int i = 0; i < pointsPerProc2[rank]; i++) {\n        localPoints[i] = points[i + offset];\n    }\n\n    int n_threads = omp_get_max_threads();\n    omp_set_num_threads(n_threads);\n\n    #pragma omp parallel\n    {\n        int thread_id = omp_get_thread_num();\n        int n_threads = omp_get_num_threads();\n\n        int localBin[4] = {0};\n\n        #pragma omp for\n        for (int i = 0; i < localPoints.size(); i++) {\n            if (localPoints[i].x >= 0 && localPoints[i].y >= 0) {\n                localBin[0]++;\n            } else if (localPoints[i].x < 0 && localPoints[i].y >= 0) {\n                localBin[1]++;\n            } else if (localPoints[i].x < 0 && localPoints[i].y < 0) {\n                localBin[2]++;\n            } else if (localPoints[i].x >= 0 && localPoints[i].y < 0) {\n                localBin[3]++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            bins[0] += localBin[0];\n            bins[1] += localBin[1];\n            bins[2] += localBin[2];\n            bins[3] += localBin[3];\n        }\n    }\n\n    std::vector<int> results = std::vector<int>(4);\n\n    MPI_Reduce(bins.data(), results.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < results.size(); i++) {\n            bins[i] = results[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); i++) {\n      #pragma omp critical\n      if(points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if(points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      } else if(points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n   int nbproc = 0, rank = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &nbproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Add your code here.\n   const size_t n = points.size();\n   int n_local = (n + nbproc - 1) / nbproc;\n   int start = n_local * rank;\n   int end = std::min(n_local * (rank + 1), n);\n   int local_bin[4] = {0};\n\n   if (rank == 0) {\n      for (size_t i = 0; i < n; i++) {\n         int bin = 0;\n         if (points[i].x > 0) {\n            if (points[i].y > 0) {\n               bin = 1;\n            } else {\n               bin = 4;\n            }\n         } else if (points[i].x < 0) {\n            if (points[i].y > 0) {\n               bin = 2;\n            } else {\n               bin = 3;\n            }\n         }\n         bins[bin]++;\n      }\n   }\n   if (rank == 0) {\n      for (size_t i = start; i < end; i++) {\n         int bin = 0;\n         if (points[i].x > 0) {\n            if (points[i].y > 0) {\n               bin = 1;\n            } else {\n               bin = 4;\n            }\n         } else if (points[i].x < 0) {\n            if (points[i].y > 0) {\n               bin = 2;\n            } else {\n               bin = 3;\n            }\n         }\n         local_bin[bin]++;\n      }\n   }\n\n   for (int r = 1; r < nbproc; r++) {\n      MPI_Status status;\n      int local_bin[4];\n      MPI_Recv(local_bin, 4, MPI_INT, r, 1, MPI_COMM_WORLD, &status);\n      for (int i = 0; i < 4; i++) {\n         bins[i] += local_bin[i];\n      }\n   }\n   if (rank == 0) {\n      for (size_t i = start; i < end; i++) {\n         int bin = 0;\n         if (points[i].x > 0) {\n            if (points[i].y > 0) {\n               bin = 1;\n            } else {\n               bin = 4;\n            }\n         } else if (points[i].x < 0) {\n            if (points[i].y > 0) {\n               bin = 2;\n            } else {\n               bin = 3;\n            }\n         }\n         bins[bin]++;\n      }\n   }\n\n   for (int r = 1; r < nbproc; r++) {\n      MPI_Status status;\n      int local_bin[4];\n      MPI_Recv(local_bin, 4, MPI_INT, r, 1, MPI_COMM_WORLD, &status);\n      for (int i = 0; i < 4; i++) {\n         bins[i] += local_bin[i];\n      }\n   }\n}",
            "// Part 1: Fill this in\n\n}",
            "// YOUR CODE HERE\n}",
            "// your code goes here\n}",
            "// TODO: Your code goes here\n\n}",
            "// your code here\n}",
            "#pragma omp parallel for schedule(static) num_threads(4)\n  for(int i = 0; i < points.size(); i++){\n  double x = points[i].x;\n  double y = points[i].y;\n  // Counts how many points are in quadrants\n  if(x >= 0 && y >= 0){\n  #pragma omp critical\n  {\n    bins[0]++;\n  }\n  }\n  else if(x < 0 && y >= 0){\n  #pragma omp critical\n  {\n    bins[1]++;\n  }\n  }\n  else if(x < 0 && y < 0){\n  #pragma omp critical\n  {\n    bins[2]++;\n  }\n  }\n  else if(x >= 0 && y < 0){\n  #pragma omp critical\n  {\n    bins[3]++;\n  }\n  }\n  }\n\n}",
            "// TODO\n}",
            "int nProcs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nProcs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   const int master = 0;\n   int size = points.size();\n   int chunkSize = size / nProcs;\n   int remainder = size % nProcs;\n\n   int myStart;\n   int myEnd;\n\n   if (rank == master) {\n      myStart = 0;\n      myEnd = size;\n   } else {\n      myStart = chunkSize * rank + remainder * (rank - 1);\n      myEnd = myStart + chunkSize;\n   }\n\n   int chunkSizeLocal = myEnd - myStart;\n\n   std::vector<Point> localPoints(chunkSizeLocal);\n\n   for (int i = myStart; i < myEnd; i++) {\n      localPoints[i - myStart] = points[i];\n   }\n\n   for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n   }\n\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for (int i = 0; i < chunkSizeLocal; i++) {\n         if (localPoints[i].x >= 0 && localPoints[i].y >= 0) {\n            bins[0]++;\n         } else if (localPoints[i].x < 0 && localPoints[i].y >= 0) {\n            bins[1]++;\n         } else if (localPoints[i].x < 0 && localPoints[i].y < 0) {\n            bins[2]++;\n         } else if (localPoints[i].x >= 0 && localPoints[i].y < 0) {\n            bins[3]++;\n         }\n      }\n   }\n\n   if (rank == master) {\n      for (int i = 1; i < nProcs; i++) {\n         int temp[4];\n         MPI_Recv(&temp, 4, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n         for (int j = 0; j < 4; j++) {\n            bins[j] += temp[j];\n         }\n      }\n   } else {\n      MPI_Send(&bins, 4, MPI_INT, master, 1, MPI_COMM_WORLD);\n   }\n}",
            "// Your code here\n\n}",
            "// TODO\n}",
            "int rank, nprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n   std::vector<std::vector<Point>> points_split(nprocs);\n   if (rank == 0) {\n       for (size_t i = 0; i < nprocs; ++i) {\n           points_split[i].reserve(points.size() / nprocs);\n       }\n   }\n\n   // Split points in rank 0.\n   if (rank == 0) {\n       for (size_t i = 0; i < points.size(); ++i) {\n           points_split[i % nprocs].push_back(points[i]);\n       }\n   }\n\n   // Broadcast all points to other ranks.\n   for (int i = 0; i < nprocs; ++i) {\n       MPI_Bcast(points_split[i].data(), points_split[i].size(), MPI_DOUBLE, i, MPI_COMM_WORLD);\n   }\n\n   std::array<size_t, 4> bins_split;\n   #pragma omp parallel for num_threads(4)\n   for (int i = 0; i < nprocs; ++i) {\n       for (size_t j = 0; j < points_split[i].size(); ++j) {\n           double x = points_split[i][j].x;\n           double y = points_split[i][j].y;\n           if (x >= 0 && y >= 0) {\n               ++bins_split[0];\n           }\n           else if (x < 0 && y >= 0) {\n               ++bins_split[1];\n           }\n           else if (x >= 0 && y < 0) {\n               ++bins_split[2];\n           }\n           else if (x < 0 && y < 0) {\n               ++bins_split[3];\n           }\n       }\n   }\n\n   // Gather results from all ranks.\n   MPI_Reduce(bins_split.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int numRanks;\n   int rank;\n   int rankSize;\n   int rankOffset;\n   int binsPerRank;\n\n   // TODO: Your code here\n   // Count the number of points in each quadrant.\n   // Store the counts in bins.\n   // Every rank has a complete copy of image. The result is stored in bins on rank 0.\n\n   // Set the number of ranks\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n   // Set the rank of the process\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // Set the size of the local rank\n   MPI_Get_processor_name(rank, &rankSize);\n\n   // Set the offset for each rank\n   rankOffset = rank * points.size() / numRanks;\n   // Set the number of points for each rank\n   rankSize = points.size() / numRanks;\n\n   // Set the size of the bins\n   binsPerRank = bins.size() / numRanks;\n\n   // Count the points in each quadrant\n   if (rank == 0) {\n      for (int i = 0; i < rankSize; ++i) {\n         if (points[i].x < 0 && points[i].y < 0) {\n            bins[0]++;\n         }\n         else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[1]++;\n         }\n         else if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[2]++;\n         }\n         else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[3]++;\n         }\n      }\n   }\n\n   // TODO: Your code here\n\n   // Gather the counts from each rank\n   MPI_Gather(&bins, binsPerRank, MPI_INT, &bins, binsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        int quadrant = 0;\n        if (points[i].y >= 0) quadrant += 1;\n        if (points[i].x >= 0) quadrant += 2;\n        ++bins[quadrant];\n    }\n}",
            "// TODO: your code here\n\n}",
            "// TODO:\n}",
            "MPI_Status status;\n\n   int rank = 0;\n   int size = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: Implement this function!\n   std::array<size_t, 4> localBins = {};\n   if (rank == 0) {\n      for (int i = 0; i < points.size(); i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n\n         if (x >= 0 && y >= 0)\n            localBins[0]++;\n         else if (x < 0 && y >= 0)\n            localBins[1]++;\n         else if (x < 0 && y < 0)\n            localBins[2]++;\n         else\n            localBins[3]++;\n      }\n   }\n\n   for (int i = 1; i < size; i++) {\n      //TODO: Receive and merge the bins\n   }\n\n   if (rank == 0)\n      bins = localBins;\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n\n   std::array<size_t, 4> localBins;\n   for (auto i = 0; i < 4; i++) localBins[i] = 0;\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      auto& p = points[i];\n      int q = (p.x >= 0) + 2*((p.y >= 0));\n      localBins[q]++;\n   }\n\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int P = omp_get_num_threads();\n   const int Q = omp_get_num_procs();\n   const int N = points.size();\n   const int NP = (N+P-1)/P;\n   const int start = NP*omp_get_thread_num();\n   const int end = std::min(start+NP, N);\n   for (int i = start; i < end; i++) {\n      int bin = 0;\n      if (points[i].x >= 0) {\n         bin += 2;\n      }\n      if (points[i].y >= 0) {\n         bin += 1;\n      }\n      // Count using atomic add\n      #pragma omp atomic\n      bins[bin]++;\n   }\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = points.size();\n\n    std::array<size_t, 4> local_bins;\n    local_bins.fill(0);\n    std::vector<int> local_points;\n    for (int i = 0; i < n; i++) {\n        int q = 0;\n        if (points[i].x > 0)\n            q += 1;\n        if (points[i].y > 0)\n            q += 2;\n        local_points.push_back(q);\n    }\n\n    int local_n = local_points.size();\n    int local_begin = (rank * local_n) / size;\n    int local_end = ((rank + 1) * local_n) / size;\n\n    for (int i = local_begin; i < local_end; i++) {\n        local_bins[local_points[i]]++;\n    }\n\n    MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        printf(\"local_bins:\");\n        for (int i = 0; i < 4; i++) {\n            printf(\"%d \", local_bins[i]);\n        }\n        printf(\"\\n\");\n        printf(\"bins:\");\n        for (int i = 0; i < 4; i++) {\n            printf(\"%d \", bins[i]);\n        }\n        printf(\"\\n\");\n    }\n}",
            "#pragma omp parallel for schedule(dynamic)\n    for(int i = 0; i < points.size(); i++)\n    {\n        int rank = 0;\n        int size = 0;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n        if (points[i].x > 0 && points[i].y > 0) bins[0] += 1;\n        else if (points[i].x < 0 && points[i].y > 0) bins[1] += 1;\n        else if (points[i].x > 0 && points[i].y < 0) bins[2] += 1;\n        else if (points[i].x < 0 && points[i].y < 0) bins[3] += 1;\n    }\n}",
            "}",
            "int rank;\n   int size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<size_t> bin_counts(4, 0);\n\n   // Partition\n   int num_points = points.size();\n   int num_per_rank = num_points / size;\n\n   int my_offset = num_per_rank * rank;\n   int my_end = my_offset + num_per_rank;\n   if (rank == size - 1) {\n      my_end = num_points;\n   }\n\n   // Compute\n   for (int i = my_offset; i < my_end; i++) {\n      if (points[i].x > 0 && points[i].y > 0)\n         bin_counts[0]++;\n      else if (points[i].x < 0 && points[i].y > 0)\n         bin_counts[1]++;\n      else if (points[i].x > 0 && points[i].y < 0)\n         bin_counts[2]++;\n      else if (points[i].x < 0 && points[i].y < 0)\n         bin_counts[3]++;\n   }\n\n   // Combine\n   std::array<size_t, 4> final_counts{0, 0, 0, 0};\n   MPI_Reduce(bin_counts.data(), final_counts.data(), 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      bins = final_counts;\n   }\n}",
            "int my_rank, number_of_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &number_of_procs);\n    std::vector<size_t> local_bins(bins.size());\n    if (my_rank == 0)\n    {\n        std::vector<Point> local_points(points);\n        #pragma omp parallel for\n        for (int i = 0; i < local_points.size(); ++i) {\n            if (local_points[i].x < 0 && local_points[i].y < 0)\n                local_bins[0]++;\n            else if (local_points[i].x < 0 && local_points[i].y > 0)\n                local_bins[1]++;\n            else if (local_points[i].x > 0 && local_points[i].y < 0)\n                local_bins[2]++;\n            else if (local_points[i].x > 0 && local_points[i].y > 0)\n                local_bins[3]++;\n        }\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else\n    {\n        #pragma omp parallel for\n        for (int i = 0; i < points.size(); ++i) {\n            if (points[i].x < 0 && points[i].y < 0)\n                local_bins[0]++;\n            else if (points[i].x < 0 && points[i].y > 0)\n                local_bins[1]++;\n            else if (points[i].x > 0 && points[i].y < 0)\n                local_bins[2]++;\n            else if (points[i].x > 0 && points[i].y > 0)\n                local_bins[3]++;\n        }\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank;\n  int size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n\n  // TODO: Count the number of points in each quadrant\n  // You may assume that the input is in Cartesian coordinates.\n\n  // TODO: Store the result in the bins vector.\n  //       You may assume that the input is in Cartesian coordinates.\n\n}",
            "if (omp_get_num_threads() == 1) {\n       std::cerr << \"Please use OpenMP threads\" << std::endl;\n       exit(1);\n   }\n\n   std::vector<size_t> partial_results(omp_get_max_threads());\n\n   #pragma omp parallel num_threads(omp_get_max_threads())\n   {\n     int tid = omp_get_thread_num();\n     partial_results[tid] = 0;\n     for (auto& point : points) {\n       int x_quadrant = point.x >= 0? 0 : 1;\n       int y_quadrant = point.y >= 0? 0 : 1;\n       if (x_quadrant == 0 && y_quadrant == 0) {\n         #pragma omp atomic\n         partial_results[tid]++;\n       } else if (x_quadrant == 1 && y_quadrant == 1) {\n         #pragma omp atomic\n         partial_results[tid]++;\n       } else if (x_quadrant == 0 && y_quadrant == 1) {\n         #pragma omp atomic\n         partial_results[tid]++;\n       } else if (x_quadrant == 1 && y_quadrant == 0) {\n         #pragma omp atomic\n         partial_results[tid]++;\n       }\n     }\n   }\n\n   #pragma omp parallel for\n   for (int i = 0; i < omp_get_num_threads(); i++)\n   {\n     if (i == 0) {\n       for (int j = 0; j < omp_get_num_threads(); j++) {\n         bins[j] = partial_results[j];\n       }\n     } else {\n       for (int j = 0; j < omp_get_num_threads(); j++) {\n         bins[j] += partial_results[j];\n       }\n     }\n   }\n\n   if (omp_get_num_threads() > 1)\n   {\n     int num_ranks;\n     MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n     if (num_ranks == 1) {\n       return;\n     }\n\n     MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "int rank, size, n = points.size();\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   #pragma omp parallel num_threads(size)\n   #pragma omp sections\n   {\n      #pragma omp section\n      {\n         // rank 0 thread 0, counts the points in quadrants 0 and 1\n         int i = 0, j = 0;\n         #pragma omp for\n         for (i = 0; i < n; i++) {\n            if (points[i].x > 0 && points[i].y > 0) {\n               j++;\n            }\n         }\n         bins[0] = j;\n      }\n      #pragma omp section\n      {\n         // rank 0 thread 1, counts the points in quadrants 2 and 3\n         int i = 0, j = 0;\n         #pragma omp for\n         for (i = 0; i < n; i++) {\n            if (points[i].x < 0 && points[i].y < 0) {\n               j++;\n            }\n         }\n         bins[1] = j;\n      }\n      #pragma omp section\n      {\n         // rank 0 thread 2, counts the points in quadrants 0 and 2\n         int i = 0, j = 0;\n         #pragma omp for\n         for (i = 0; i < n; i++) {\n            if (points[i].x > 0 && points[i].y < 0) {\n               j++;\n            }\n         }\n         bins[2] = j;\n      }\n      #pragma omp section\n      {\n         // rank 0 thread 3, counts the points in quadrants 1 and 3\n         int i = 0, j = 0;\n         #pragma omp for\n         for (i = 0; i < n; i++) {\n            if (points[i].x < 0 && points[i].y > 0) {\n               j++;\n            }\n         }\n         bins[3] = j;\n      }\n   }\n}",
            "}",
            "// YOUR CODE HERE\n}",
            "int size, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int N = points.size();\n    int k = N / size;\n    int start = rank * k;\n    int end = rank == size - 1? N : (rank + 1) * k;\n    std::array<size_t, 4> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    std::vector<Point> local_points;\n    for (int i = start; i < end; ++i) {\n        local_points.push_back(points[i]);\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < local_points.size(); ++i) {\n        Point p = local_points[i];\n        if (p.x < 0 && p.y < 0) {\n            local_bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            local_bins[1]++;\n        } else if (p.x >= 0 && p.y < 0) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n    std::vector<size_t> recv_buff(4);\n    MPI_Reduce(&local_bins, &recv_buff, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = recv_buff;\n    }\n}",
            "// TODO: your code here\n}",
            "// TODO\n}",
            "int size;\n   int rank;\n   int count;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int count_per_rank;\n   int starting_point;\n\n   if (rank == 0) {\n      count_per_rank = points.size() / size;\n      starting_point = 0;\n   } else {\n      MPI_Recv(&count_per_rank, 1, MPI_INT, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      starting_point = rank * count_per_rank;\n   }\n\n   int ending_point = starting_point + count_per_rank;\n\n   std::array<size_t, 4> counts;\n   counts[0] = 0;\n   counts[1] = 0;\n   counts[2] = 0;\n   counts[3] = 0;\n\n#pragma omp parallel for\n   for (int i = starting_point; i < ending_point; i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         counts[0]++;\n      }\n      if (points[i].x < 0 && points[i].y >= 0) {\n         counts[1]++;\n      }\n      if (points[i].x >= 0 && points[i].y < 0) {\n         counts[2]++;\n      }\n      if (points[i].x < 0 && points[i].y < 0) {\n         counts[3]++;\n      }\n   }\n\n   if (rank == size - 1) {\n      MPI_Send(&counts, 4, MPI_INT, rank - 1, 0, MPI_COMM_WORLD);\n   } else {\n      MPI_Send(&counts, 4, MPI_INT, rank - 1, 0, MPI_COMM_WORLD);\n      MPI_Recv(&counts, 4, MPI_INT, rank + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   if (rank == 0) {\n      for (int i = 1; i < size; i++) {\n         MPI_Recv(&counts, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j = 0; j < 4; j++) {\n            bins[j] += counts[j];\n         }\n      }\n   }\n}",
            "// TODO\n\n}",
            "}",
            "const size_t size = points.size();\n    const size_t offset = size / omp_get_num_threads();\n    const size_t my_offset = omp_get_thread_num() * offset;\n    std::array<size_t, 4> bin_local = {0, 0, 0, 0};\n    for (size_t i = my_offset; i < my_offset + offset; ++i) {\n        const Point& point = points[i];\n        if (point.x >= 0 && point.y >= 0) {\n            ++bin_local[0];\n        } else if (point.x < 0 && point.y >= 0) {\n            ++bin_local[1];\n        } else if (point.x >= 0 && point.y < 0) {\n            ++bin_local[2];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bin_local[3];\n        }\n    }\n    const int rank = 0;\n    const int num_ranks = 1;\n    MPI_Reduce(bin_local.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n}",
            "if (points.empty()) {\n      return;\n   }\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int pointsPerRank = (points.size() / size) + 1;\n   int startIndex = rank * pointsPerRank;\n   int endIndex = startIndex + pointsPerRank;\n\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for (int i = startIndex; i < endIndex; i++) {\n         Point currentPoint = points[i];\n         if (currentPoint.x > 0 && currentPoint.y > 0) {\n            #pragma omp atomic\n            bins[0]++;\n         }\n         else if (currentPoint.x < 0 && currentPoint.y > 0) {\n            #pragma omp atomic\n            bins[1]++;\n         }\n         else if (currentPoint.x < 0 && currentPoint.y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n         }\n         else {\n            #pragma omp atomic\n            bins[3]++;\n         }\n      }\n   }\n\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// This is where you will need to parallelize\n    for(int i = 0; i < points.size(); i++) {\n        if(points[i].x < 0 && points[i].y < 0) {\n            bins[0]++;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[1]++;\n        } else if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n    int size = omp_get_num_threads();\n    int rank = omp_get_thread_num();\n    int *bins_local = new int[4];\n    bins_local[0] = 0;\n    bins_local[1] = 0;\n    bins_local[2] = 0;\n    bins_local[3] = 0;\n    for (int i = 0; i < points.size(); i++) {\n        if(points[i].x < 0 && points[i].y < 0) {\n            bins_local[0]++;\n        } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins_local[1]++;\n        } else if (points[i].x >= 0 && points[i].y >= 0) {\n            bins_local[2]++;\n        } else {\n            bins_local[3]++;\n        }\n    }\n\n    MPI_Reduce(bins_local, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<std::array<size_t, 4>> bins_all(size);\n   //bins_all[0] = bins;\n   if (rank == 0) {\n       for (size_t i = 0; i < points.size(); i++) {\n           if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n           else if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n           else if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n           else if (points[i].x >= 0 && points[i].y < 0) bins[3]++;\n       }\n       for (size_t i = 1; i < size; i++) {\n           MPI_Recv(bins_all[i].data(), 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       }\n\n       for (int i = 0; i < 4; i++) {\n           for (size_t j = 1; j < size; j++) {\n               bins[i] += bins_all[j][i];\n           }\n       }\n   }\n   else {\n       #pragma omp parallel for\n       for (size_t i = 0; i < points.size(); i++) {\n           if (points[i].x >= 0 && points[i].y >= 0) bins[0]++;\n           else if (points[i].x < 0 && points[i].y >= 0) bins[1]++;\n           else if (points[i].x < 0 && points[i].y < 0) bins[2]++;\n           else if (points[i].x >= 0 && points[i].y < 0) bins[3]++;\n       }\n       MPI_Send(bins.data(), 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n\n}",
            "// TODO: use MPI and OpenMP to count the points\n\n}",
            "const int size = points.size();\n  const int rank = omp_get_num_threads();\n  // TODO: add your code here\n  std::vector<std::vector<Point>> tempPoints(omp_get_num_threads());\n  for(int i = 0; i < size; i++) {\n    tempPoints[omp_get_thread_num()].push_back(points[i]);\n  }\n\n  std::vector<size_t> tempBins(4);\n\n  for(int i = 0; i < omp_get_num_threads(); i++) {\n    for(int j = 0; j < tempPoints[i].size(); j++) {\n      int quadrant = 0;\n      if(tempPoints[i][j].x < 0) quadrant++;\n      if(tempPoints[i][j].y < 0) quadrant += 2;\n      tempBins[quadrant]++;\n    }\n  }\n\n  MPI_Reduce(&tempBins, &bins, 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO: implement\n}",
            "// TODO: Your code here\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  if (points.size() == 0)\n    return;\n  #pragma omp parallel for\n  for (int i = 0; i < points.size(); i++) {\n    if (points[i].x >= 0 && points[i].y >= 0)\n      bins[0]++;\n    else if (points[i].x < 0 && points[i].y >= 0)\n      bins[1]++;\n    else if (points[i].x < 0 && points[i].y < 0)\n      bins[2]++;\n    else if (points[i].x >= 0 && points[i].y < 0)\n      bins[3]++;\n  }\n}",
            "// TODO\n}",
            "// TODO\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < points.size(); ++i) {\n   \t\tint quadrant;\n\n      if(points[i].x > 0 && points[i].y > 0){\n      \tquadrant = 0;\n      } else if (points[i].x < 0 && points[i].y > 0){\n      \tquadrant = 1;\n      } else if (points[i].x > 0 && points[i].y < 0){\n      \tquadrant = 2;\n      } else {\n      \tquadrant = 3;\n      }\n\n      #pragma omp atomic\n      bins[quadrant] += 1;\n   }\n}",
            "MPI_Status status;\n   const int size = MPI_Size();\n   const int rank = MPI_Rank();\n   const size_t n = points.size();\n   const size_t numBins = bins.size();\n   const size_t chunkSize = n / size;\n   const size_t rem = n % size;\n\n   if (rank == 0) {\n      for (size_t i = 0; i < numBins; ++i) {\n         bins[i] = 0;\n      }\n   }\n\n   std::vector<size_t> chunks(size, chunkSize);\n   if (rank == 0) {\n      for (size_t i = 0; i < rem; ++i) {\n         chunks[i] += 1;\n      }\n   }\n   std::vector<size_t> disps(size);\n   if (rank == 0) {\n      disps[0] = 0;\n      for (size_t i = 1; i < size; ++i) {\n         disps[i] = disps[i - 1] + chunks[i - 1];\n      }\n   }\n   MPI_Scatterv(points.data(), chunks.data(), disps.data(), MPI_POINT,\n                points.data(), chunkSize, MPI_POINT, 0, MPI_COMM_WORLD);\n\n   size_t localSum[4] = {0, 0, 0, 0};\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for (size_t i = 0; i < chunkSize; ++i) {\n         int x = (points[i].x >= 0? 1 : 0) + (points[i].y >= 0? 2 : 0);\n         #pragma omp atomic\n         localSum[x] += 1;\n      }\n   }\n   MPI_Gatherv(localSum, numBins, MPI_SIZE_T, bins.data(), chunks.data(), disps.data(),\n               MPI_SIZE_T, 0, MPI_COMM_WORLD);\n}",
            "MPI_Status status;\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int number_of_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &number_of_ranks);\n    int my_points_count = points.size() / number_of_ranks;\n    int remaining_points = points.size() % number_of_ranks;\n    if (my_rank < remaining_points)\n        my_points_count++;\n    std::vector<Point> my_points(my_points_count);\n\n    MPI_Scatter(points.data(), my_points_count, MPI_DOUBLE, my_points.data(), my_points_count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int my_points_count_int = my_points.size();\n\n    std::array<size_t, 4> my_bins;\n    my_bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < my_points_count_int; i++) {\n        Point p = my_points[i];\n        if (p.x >= 0 && p.y >= 0)\n            my_bins[0]++;\n        else if (p.x < 0 && p.y >= 0)\n            my_bins[1]++;\n        else if (p.x >= 0 && p.y < 0)\n            my_bins[2]++;\n        else\n            my_bins[3]++;\n    }\n\n    MPI_Reduce(my_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// Your code goes here\n}",
            "// Your code here\n}",
            "// 1. Create a copy of points. You do not need to do anything with this data.\n  std::vector<Point> points_copy;\n\n  // 2. Calculate the number of points in each quadrant.\n  // You can use the `count` method of `std::vector`.\n\n  // 3. Use the OpenMP directive to divide up the work\n\n  // 4. Use MPI to communicate the results\n\n}",
            "// TODO\n}",
            "// TODO: Fill in the blanks to get this function to compile and run correctly.\n\n   // The number of cartesian quadrants is 4.\n   // We will have to loop over this many times.\n   for (int i = 0; i < __; i++) {\n      // The index of `bins` corresponding to the current quadrant is `i`.\n      size_t &bin = bins[i];\n      // Loop over `points` to count the number of cartesian points in quadrant `i`.\n      for (Point const& point: __) {\n         // The cartesian quadrant is computed using the following formula.\n         int quadrant = ((__)? 1 : 0) + ((__)? 2 : 0);\n\n         // Check if we're in the right quadrant.\n         if (quadrant == i) {\n            // We are. Increment the bin counter.\n            bin++;\n         }\n      }\n   }\n}",
            "int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   if (world_size == 1)\n   {\n      std::fill(bins.begin(), bins.end(), 0);\n   }\n   else\n   {\n      std::vector<Point> local_points;\n      std::vector<std::array<size_t, 4>> local_bins(world_size);\n      int n = points.size();\n      int p = n / world_size;\n      int r = n % world_size;\n      if (world_rank < r)\n      {\n         local_points.insert(local_points.end(), points.begin() + world_rank * (p + 1), points.begin() + (world_rank + 1) * (p + 1));\n      }\n      else\n      {\n         local_points.insert(local_points.end(), points.begin() + (world_rank * p + r) * p, points.begin() + (world_rank + 1) * p);\n      }\n\n      std::fill(local_bins[world_rank].begin(), local_bins[world_rank].end(), 0);\n      for (auto point : local_points)\n      {\n         if (point.x >= 0 && point.y >= 0)\n         {\n            local_bins[world_rank][0]++;\n         }\n         else if (point.x < 0 && point.y >= 0)\n         {\n            local_bins[world_rank][1]++;\n         }\n         else if (point.x < 0 && point.y < 0)\n         {\n            local_bins[world_rank][2]++;\n         }\n         else if (point.x >= 0 && point.y < 0)\n         {\n            local_bins[world_rank][3]++;\n         }\n      }\n\n      MPI_Gather(local_bins.data(), 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "//...\n}",
            "// TODO: Implement\n\n}",
            "/* YOUR CODE HERE */\n   std::vector<int> bins_mpi(4);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Status status;\n   int n_workers = omp_get_num_threads();\n\n   for(int i=0; i<n_workers; i++) {\n      MPI_Recv(&bins_mpi[0], 4, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n   }\n   for(int i=0; i<4; i++) {\n      bins[i] = bins_mpi[i];\n   }\n}",
            "const int size = points.size();\n   const int chunk_size = size / (omp_get_num_threads());\n   const int num_threads = omp_get_num_threads();\n\n   int start, end;\n\n   #pragma omp parallel private(start, end)\n   {\n      int thread_id = omp_get_thread_num();\n      start = thread_id * chunk_size;\n      end = thread_id!= (num_threads - 1)? (thread_id + 1) * chunk_size : size;\n\n      #pragma omp for\n      for(int i = start; i < end; i++){\n         int quadrant = (points[i].x > 0 && points[i].y > 0)? 1 : 0;\n         quadrant += (points[i].x > 0 && points[i].y < 0)? 2 : 0;\n         quadrant += (points[i].x < 0 && points[i].y > 0)? 3 : 0;\n         quadrant += (points[i].x < 0 && points[i].y < 0)? 4 : 0;\n         bins[quadrant]++;\n      }\n   }\n}",
            "}",
            "// TODO: implement\n}",
            "int num_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   if (num_ranks == 1) {\n      for (auto& p : points) {\n         auto q = (p.x > 0) + 2 * (p.y > 0);\n         ++bins[q];\n      }\n      return;\n   }\n\n   int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   std::vector<Point> my_points;\n   MPI_Scatter(points.data(), points.size() / num_ranks,\n               MPI_CHAR, my_points.data(), my_points.size(),\n               MPI_CHAR, 0, MPI_COMM_WORLD);\n\n   std::array<size_t, 4> my_bins = {0, 0, 0, 0};\n   // TODO: Count the number of points in each quadrant in `my_points` and store the result in `my_bins`.\n\n   // TODO: Reduce my_bins to bins using MPI_Reduce\n}",
            "// TODO: your implementation here\n}",
            "//TODO: Your code here\n\n}",
            "std::vector<size_t> vect;\n\n  // Complete this function\n\n}",
            "int numRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // We'll use this to keep track of how many points each rank has\n   // Assume for now that we'll have roughly the same number of points on each rank\n   std::vector<int> counts(numRanks, (int) points.size() / numRanks);\n   if (rank < points.size() % numRanks)\n      counts[rank]++;\n\n   // Create the partial bins on each rank\n   std::vector<std::array<size_t, 4>> partialBins(numRanks, {0, 0, 0, 0});\n\n   // Count the points\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const &point = points[i];\n      int partialIndex = point.x >= 0? (point.y >= 0? 0 : 1) : (point.y >= 0? 2 : 3);\n      partialBins[rank][partialIndex]++;\n   }\n\n   // Combine the partial bins from each rank\n   MPI_Reduce(partialBins.data(), bins.data(), bins.size(), MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/*\n      Your code goes here.\n   */\n}",
            "int rank;\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int const chunk = points.size() / size;\n    int const left = (chunk * rank);\n    int const right = left + chunk;\n\n    std::array<size_t, 4> local_bins{};\n\n    for(size_t i = left; i < right; i++) {\n        Point const& point = points[i];\n\n        if(point.x >= 0 && point.y >= 0) {\n            ++local_bins[0];\n        } else if(point.x < 0 && point.y >= 0) {\n            ++local_bins[1];\n        } else if(point.x >= 0 && point.y < 0) {\n            ++local_bins[2];\n        } else if(point.x < 0 && point.y < 0) {\n            ++local_bins[3];\n        }\n    }\n\n    std::array<size_t, 4> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        bins = global_bins;\n    }\n}",
            "const int size = points.size();\n   const int rank = omp_get_thread_num();\n   const int numthreads = omp_get_num_threads();\n\n   std::vector<std::array<size_t, 4>> counts(numthreads);\n   std::vector<std::vector<Point>> pointsPerThread(numthreads);\n\n   int numPerThread = size / numthreads;\n   int remainder = size % numthreads;\n   int offset = rank * numPerThread;\n   if (rank == 0) {\n      offset = 0;\n   }\n\n   if (rank == 0) {\n      counts[rank][0] = 0;\n      counts[rank][1] = 0;\n      counts[rank][2] = 0;\n      counts[rank][3] = 0;\n   } else {\n      counts[rank][0] = 0;\n      counts[rank][1] = 0;\n      counts[rank][2] = 0;\n      counts[rank][3] = 0;\n   }\n\n   if (rank == 0) {\n      for (int i = 1; i < numthreads; i++) {\n         counts[i][0] = 0;\n         counts[i][1] = 0;\n         counts[i][2] = 0;\n         counts[i][3] = 0;\n      }\n   }\n\n   if (rank == 0) {\n      for (int i = 0; i < numPerThread; i++) {\n         pointsPerThread[rank].push_back(points[i]);\n      }\n   } else {\n      for (int i = 0; i < numPerThread; i++) {\n         pointsPerThread[rank].push_back(points[i + remainder]);\n      }\n   }\n\n   for (int i = 0; i < pointsPerThread[rank].size(); i++) {\n      if (pointsPerThread[rank][i].x >= 0 && pointsPerThread[rank][i].y >= 0) {\n         counts[rank][0]++;\n      } else if (pointsPerThread[rank][i].x < 0 && pointsPerThread[rank][i].y >= 0) {\n         counts[rank][1]++;\n      } else if (pointsPerThread[rank][i].x >= 0 && pointsPerThread[rank][i].y < 0) {\n         counts[rank][2]++;\n      } else if (pointsPerThread[rank][i].x < 0 && pointsPerThread[rank][i].y < 0) {\n         counts[rank][3]++;\n      }\n   }\n\n   for (int i = 0; i < numthreads; i++) {\n      if (i == 0) {\n         for (int j = 0; j < counts[0].size(); j++) {\n            counts[i][j] += counts[j][0];\n         }\n      } else {\n         counts[i] = counts[0];\n      }\n   }\n\n   if (rank == 0) {\n      for (int i = 0; i < numthreads; i++) {\n         for (int j = 0; j < counts[i].size(); j++) {\n            bins[j] += counts[i][j];\n         }\n      }\n   }\n\n   // Counting in parallel\n   // int numPerThread = size / numthreads;\n   // int remainder = size % numthreads;\n   // int offset = rank * numPerThread;\n\n   // // Count quadrants per thread\n   // for (int i = 0; i < numPerThread; i++) {\n   //    if (points[i + offset].x >= 0 && points[i + offset].y >= 0) {\n   //       counts[0][rank]++;\n   //    } else if (points[i + offset].x < 0 && points[i + offset].y >= 0) {\n   //       counts[1][rank]++;\n   //    } else if (points[i + offset].x >= 0 && points[i + offset].y < 0) {\n   //       counts[2][rank]++;\n   //    } else if (points[i + offset].x < 0 && points[i + offset].y < 0) {\n   //",
            "// TODO: Implement\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = points.size();\n    std::vector<int> count(4, 0);\n\n    if (rank == 0) {\n        for (int i = 0; i < n; ++i) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                ++count[0];\n            } else if (points[i].x < 0 && points[i].y > 0) {\n                ++count[1];\n            } else if (points[i].x > 0 && points[i].y < 0) {\n                ++count[2];\n            } else if (points[i].x < 0 && points[i].y < 0) {\n                ++count[3];\n            }\n        }\n\n        bins[0] = count[0];\n        bins[1] = count[1];\n        bins[2] = count[2];\n        bins[3] = count[3];\n\n    } else {\n\n        int subsize = n / size;\n        int subremainder = n % size;\n\n        if (rank < subremainder) {\n            subsize += 1;\n        }\n\n        if (rank == 0) {\n            subsize += subremainder;\n        }\n\n        int start = rank * subsize;\n\n        if (rank < subremainder) {\n            start += rank;\n        } else {\n            start += subremainder;\n        }\n\n        for (int i = start; i < start + subsize; ++i) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                ++count[0];\n            } else if (points[i].x < 0 && points[i].y > 0) {\n                ++count[1];\n            } else if (points[i].x > 0 && points[i].y < 0) {\n                ++count[2];\n            } else if (points[i].x < 0 && points[i].y < 0) {\n                ++count[3];\n            }\n        }\n\n        std::vector<int> count_buf(4);\n        MPI_Gather(&count[0], 4, MPI_INT, &count_buf[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            bins[0] = count_buf[0];\n            bins[1] = count_buf[1];\n            bins[2] = count_buf[2];\n            bins[3] = count_buf[3];\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (auto &p : points) {\n      #pragma omp critical\n      bins[p.y < 0? 0 : (p.x < 0? 1 : 2)]++;\n   }\n}",
            "}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    int rank = 0;\n    int size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int chunk_size = (int)(points.size() / size);\n\n    int my_start = rank * chunk_size;\n    int my_end = (rank == size - 1)? points.size() : (rank + 1) * chunk_size;\n    std::vector<Point> my_points;\n    for (int i = my_start; i < my_end; i++) {\n        my_points.push_back(points[i]);\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < my_points.size(); i++) {\n        int x_quadrant = (my_points[i].x >= 0)? 1 : 0;\n        int y_quadrant = (my_points[i].y >= 0)? 1 : 0;\n        bins[x_quadrant + y_quadrant * 2]++;\n    }\n\n    MPI_Reduce(&bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code goes here\n}",
            "int myRank, rankCount;\n    int left, right, top, bottom;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &rankCount);\n    std::vector<Point> localPoints;\n    int numPoints = points.size();\n    int numPerRank = numPoints / rankCount;\n    int leftover = numPoints % rankCount;\n    if (myRank == 0) {\n        // Master rank\n        bottom = 0;\n        left = 0;\n        right = 0;\n        top = 0;\n        for (int i = 0; i < rankCount; i++) {\n            MPI_Recv(&localPoints, numPerRank, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (int j = 0; j < numPerRank; j++) {\n                if (localPoints[j].x < 0 && localPoints[j].y < 0) {\n                    bottom++;\n                }\n                else if (localPoints[j].x > 0 && localPoints[j].y < 0) {\n                    top++;\n                }\n                else if (localPoints[j].x > 0 && localPoints[j].y > 0) {\n                    right++;\n                }\n                else if (localPoints[j].x < 0 && localPoints[j].y > 0) {\n                    left++;\n                }\n            }\n        }\n        bins = {left, right, top, bottom};\n    } else {\n        // Slave rank\n        if (myRank < leftover) {\n            // Send numPerRank + 1\n            localPoints.resize(numPerRank + 1);\n            MPI_Send(&points[myRank * (numPerRank + 1)], numPerRank + 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        } else {\n            // Send numPerRank\n            localPoints.resize(numPerRank);\n            MPI_Send(&points[myRank * numPerRank + leftover], numPerRank, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n        }\n    }\n\n}",
            "int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   // Your code here\n}",
            "auto npoints = points.size();\n   auto nrank = omp_get_num_threads();\n   std::vector<size_t> bins_rank(4);\n   for(int i = 0; i < 4; i++) {\n      bins_rank[i] = 0;\n   }\n   #pragma omp parallel for default(none) firstprivate(npoints, nrank) shared(bins_rank, points)\n   for(int i = 0; i < npoints; i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if(x >= 0 && y >= 0) {\n         bins_rank[0]++;\n      } else if(x < 0 && y >= 0) {\n         bins_rank[1]++;\n      } else if(x < 0 && y < 0) {\n         bins_rank[2]++;\n      } else if(x >= 0 && y < 0) {\n         bins_rank[3]++;\n      }\n   }\n   for(int i = 0; i < 4; i++) {\n      MPI_Reduce(&bins_rank[i], &bins[i], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   const int kNumBins = 4;\n   const size_t points_per_rank = points.size() / size;\n   std::vector<Point> points_in_rank(points.begin() + points_per_rank * rank, points.begin() + points_per_rank * (rank + 1));\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points_in_rank.size(); ++i) {\n      auto &p = points_in_rank[i];\n      int x_quadrant = p.x > 0? 0 : 1;\n      int y_quadrant = p.y > 0? 0 : 1;\n      bins[x_quadrant + 2*y_quadrant]++;\n   }\n\n   if (rank == 0) {\n      for (int i = 1; i < size; ++i) {\n         MPI_Recv(bins.data(), kNumBins, MPI_UNSIGNED_LONG_LONG, i, MPI_ANY_TAG, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   } else {\n      MPI_Send(bins.data(), kNumBins, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "int numberOfProcesses, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numberOfProcesses);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<int> localBins;\n    localBins.assign(4, 0);\n\n    if (rank == 0) {\n        localBins = bins;\n    }\n\n    int n;\n    if (rank == 0) {\n        n = points.size();\n    }\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = rank; i < n; i += numberOfProcesses) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            localBins[0]++;\n        } else if (points[i].x < 0 && points[i].y > 0) {\n            localBins[1]++;\n        } else if (points[i].x < 0 && points[i].y < 0) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    int recv[4];\n    int send[4];\n    send[0] = localBins[0];\n    send[1] = localBins[1];\n    send[2] = localBins[2];\n    send[3] = localBins[3];\n\n    MPI_Reduce(send, recv, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins[0] = recv[0];\n        bins[1] = recv[1];\n        bins[2] = recv[2];\n        bins[3] = recv[3];\n    }\n}",
            "// TODO: Your code here\n\n}",
            "size_t num_points = points.size();\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: Create a local copy of `bins` for each rank.\n   std::array<size_t, 4> local_bins;\n\n   // TODO: Use MPI_Scatter to scatter a portion of the points\n   //       to each rank.\n   std::vector<Point> local_points;\n   MPI_Scatter(points.data(), num_points, MPI_DOUBLE, local_points.data(), num_points, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   // TODO: Count the number of points in each quadrant using\n   //       a parallel for loop.\n   #pragma omp parallel for\n   for (size_t i = 0; i < num_points; i++) {\n     // TODO: Compute the quadrant of each point and update the\n     //       local bins.\n   }\n\n   // TODO: Use MPI_Gather to gather the local bins on rank 0.\n   std::array<size_t, 4> global_bins;\n   MPI_Gather(local_bins.data(), 4, MPI_INT, global_bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // TODO: Rank 0 should have the final `bins` count.\n   if (rank == 0) {\n     bins = global_bins;\n   }\n}",
            "// TODO\n}",
            "if (bins.size()!= 4) {\n        throw std::invalid_argument(\"Expected bins array of size 4\");\n    }\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = (int)points.size();\n    int nthr = omp_get_max_threads();\n    int localn = n / size;\n    int localn_t = localn / nthr;\n\n    for (int i = 0; i < nthr; ++i) {\n        for (int j = 0; j < localn_t; ++j) {\n            int pos = rank * nthr * localn_t + i * localn_t + j;\n            if (pos < n) {\n                if (points[pos].x >= 0 && points[pos].y >= 0) {\n                    #pragma omp atomic\n                    bins[0] += 1;\n                } else if (points[pos].x < 0 && points[pos].y >= 0) {\n                    #pragma omp atomic\n                    bins[1] += 1;\n                } else if (points[pos].x >= 0 && points[pos].y < 0) {\n                    #pragma omp atomic\n                    bins[2] += 1;\n                } else if (points[pos].x < 0 && points[pos].y < 0) {\n                    #pragma omp atomic\n                    bins[3] += 1;\n                }\n            }\n        }\n    }\n\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    bins = { 0, 0, 0, 0 };\n    if (rank == 0) {\n        int start = 0, end = points.size();\n        for (int r = 1; r < size; r++) {\n            int mid = (start + end) / 2;\n            MPI_Send(&start, 1, MPI_INT, r, 0, MPI_COMM_WORLD);\n            MPI_Send(&mid, 1, MPI_INT, r, 0, MPI_COMM_WORLD);\n            start = mid;\n        }\n    }\n\n    int start, end;\n    MPI_Recv(&start, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    MPI_Recv(&end, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    #pragma omp parallel for\n    for (int i = start; i < end; i++) {\n        if (points[i].x >= 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y >= 0) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else if (points[i].x >= 0 && points[i].y < 0) {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n    if (rank!= 0) {\n        MPI_Send(&start, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(&end, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Implement me!\n}",
            "// TODO\n  // The implementation should be parallelized using MPI and OpenMP\n  // The results should be written in the vector `bins`\n  //\n  // For the input above the following result should be expected\n  // (0, 0): 3\n  // (0, 1): 1\n  // (1, 0): 0\n  // (1, 1): 2\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int pointsPerRank = points.size() / size;\n    int remainder = points.size() % size;\n\n    std::vector<size_t> localBins(4, 0);\n    std::vector<int> localRanks(4, 0);\n    std::vector<std::vector<Point>> localPoints(4);\n\n    int localPointsSize;\n    if (rank < remainder) {\n        localPointsSize = pointsPerRank + 1;\n        for (int i = 0; i < 4; i++) {\n            localRanks[i] = (rank * 4 + i) % size;\n            localPoints[i].reserve(localPointsSize);\n        }\n    } else {\n        localPointsSize = pointsPerRank;\n        for (int i = 0; i < 4; i++) {\n            localRanks[i] = (rank * 4 + i) % size;\n            localPoints[i].reserve(localPointsSize);\n        }\n    }\n\n    int offset = rank * pointsPerRank + std::min(rank, remainder);\n    for (int i = 0; i < localPointsSize; i++) {\n        int pos = points[i + offset].x >= 0? 0 : 2;\n        if (points[i + offset].y >= 0)\n            pos++;\n        localPoints[pos].push_back(points[i + offset]);\n    }\n\n    for (int i = 0; i < 4; i++) {\n        MPI_Send(&localPoints[i].size(), 1, MPI_INT, localRanks[i], 0, MPI_COMM_WORLD);\n        if (localPoints[i].size() > 0) {\n            MPI_Send(&localPoints[i][0], localPoints[i].size(), MPI_DOUBLE, localRanks[i], 1, MPI_COMM_WORLD);\n        }\n    }\n\n    int totalPoints = points.size();\n    for (int i = 0; i < 4; i++) {\n        if (rank == localRanks[i]) {\n            continue;\n        }\n        MPI_Status status;\n        int pointsSize;\n        MPI_Recv(&pointsSize, 1, MPI_INT, localRanks[i], 0, MPI_COMM_WORLD, &status);\n        std::vector<Point> recvPoints;\n        recvPoints.reserve(pointsSize);\n        MPI_Recv(&recvPoints[0], pointsSize, MPI_DOUBLE, localRanks[i], 1, MPI_COMM_WORLD, &status);\n        int recvPointsSize = recvPoints.size();\n#pragma omp parallel for reduction(+ : localBins)\n        for (int j = 0; j < recvPointsSize; j++) {\n            int pos = recvPoints[j].x >= 0? 0 : 2;\n            if (recvPoints[j].y >= 0)\n                pos++;\n            localBins[pos]++;\n        }\n    }\n\n    for (int i = 0; i < 4; i++) {\n        if (rank == localRanks[i]) {\n            for (int j = 0; j < localPoints[i].size(); j++) {\n                int pos = localPoints[i][j].x >= 0? 0 : 2;\n                if (localPoints[i][j].y >= 0)\n                    pos++;\n                localBins[pos]++;\n            }\n            MPI_Reduce(&localBins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n            break;\n        }\n    }\n}",
            "// Your code here.\n    int num_procs, proc_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n\n    size_t n = points.size();\n    int k = n/num_procs;\n    int n_l = 0;\n    int n_r = 0;\n    int n_u = 0;\n    int n_d = 0;\n    std::vector<Point> l_points;\n    std::vector<Point> r_points;\n    std::vector<Point> u_points;\n    std::vector<Point> d_points;\n    if (proc_id == 0){\n        for (int i = 1; i < num_procs; i++){\n            MPI_Send(&n, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n        for (int i = 0; i < k; i++){\n            if (points[i].x < 0 && points[i].y > 0){\n                n_l++;\n            }\n            else if (points[i].x > 0 && points[i].y > 0){\n                n_r++;\n            }\n            else if (points[i].x > 0 && points[i].y < 0){\n                n_u++;\n            }\n            else if (points[i].x < 0 && points[i].y < 0){\n                n_d++;\n            }\n        }\n        l_points.resize(n_l);\n        r_points.resize(n_r);\n        u_points.resize(n_u);\n        d_points.resize(n_d);\n        for (int i = 0; i < k; i++){\n            if (points[i].x < 0 && points[i].y > 0){\n                l_points[n_l-1] = points[i];\n                n_l--;\n            }\n            else if (points[i].x > 0 && points[i].y > 0){\n                r_points[n_r-1] = points[i];\n                n_r--;\n            }\n            else if (points[i].x > 0 && points[i].y < 0){\n                u_points[n_u-1] = points[i];\n                n_u--;\n            }\n            else if (points[i].x < 0 && points[i].y < 0){\n                d_points[n_d-1] = points[i];\n                n_d--;\n            }\n        }\n        for (int i = 1; i < num_procs; i++){\n            MPI_Send(&l_points[0], n_l, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&r_points[0], n_r, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&u_points[0], n_u, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&d_points[0], n_d, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n        MPI_Recv(&l_points[0], n_l, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(&r_points[0], n_r, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(&u_points[0], n_u, MPI_DOUBLE, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(&d_points[0], n_d, MPI_DOUBLE, MPI_ANY",
            "int N_threads = 1;\n    int N_procs = 1;\n    int N_each_procs = points.size();\n    int my_rank = 0;\n\n    bins = {0, 0, 0, 0};\n    MPI_Comm_size(MPI_COMM_WORLD, &N_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    if (my_rank == 0) {\n        N_each_procs = points.size() / N_procs;\n        N_threads = omp_get_max_threads();\n    }\n\n    MPI_Bcast(&N_threads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&N_each_procs, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    MPI_Bcast(&N_procs, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (my_rank!= 0) {\n        points = std::vector<Point>(N_each_procs);\n    }\n\n    // Broadcast the points to all the processes\n    MPI_Bcast(&points[0], N_each_procs, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    omp_set_num_threads(N_threads);\n\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static, 1)\n        for (size_t i = 0; i < N_each_procs; i++) {\n            int thread_id = omp_get_thread_num();\n            double x = points[i].x;\n            double y = points[i].y;\n\n            if (x >= 0 && y >= 0) {\n                bins[0]++;\n            } else if (x < 0 && y >= 0) {\n                bins[1]++;\n            } else if (x >= 0 && y < 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n\n    MPI_Reduce(MPI_IN_PLACE, &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO\n}",
            "int comm_size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<size_t> local_bins(bins.size(), 0);\n\n   // Parallelize the for loop with OpenMP\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      // Determine the quadrant this point belongs to\n      int x = (points[i].x > 0);\n      int y = (points[i].y > 0);\n      int q = x + y*2;\n      // Add one to the correct bin\n      #pragma omp atomic\n      local_bins[q]++;\n   }\n\n   // Aggregate the counts on rank 0\n   MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// fill the array with zeros\n    std::fill(bins.begin(), bins.end(), 0);\n\n    if (points.size() < 2)\n        return;\n\n    // TODO: complete this function\n}",
            "}",
            "// your code goes here\n}",
            "int psize = points.size();\n   double pi = 3.14;\n   size_t local_bins[4] = {0};\n   int rank = omp_get_thread_num();\n   double start = 0;\n   double end = 0;\n   int num_threads = omp_get_num_threads();\n   int num_ranks = omp_get_num_procs();\n   if(rank == 0) {\n      for(int i = 0; i < num_ranks; i++) {\n         start = (2*pi)/num_threads * i;\n         end = (2*pi)/num_threads * (i+1);\n         if(i == num_ranks-1) {\n            end = 2*pi;\n         }\n         #pragma omp parallel num_threads(num_threads)\n         {\n            int thread_rank = omp_get_thread_num();\n            for(int j = 0; j < psize; j++) {\n               if(points[j].x > 0 && points[j].y > 0) {\n                  if((points[j].x >= start && points[j].x < end)) {\n                     local_bins[0]++;\n                  }\n               }\n               else if(points[j].x < 0 && points[j].y > 0) {\n                  if((points[j].x >= start && points[j].x < end)) {\n                     local_bins[1]++;\n                  }\n               }\n               else if(points[j].x > 0 && points[j].y < 0) {\n                  if((points[j].x >= start && points[j].x < end)) {\n                     local_bins[2]++;\n                  }\n               }\n               else if(points[j].x < 0 && points[j].y < 0) {\n                  if((points[j].x >= start && points[j].x < end)) {\n                     local_bins[3]++;\n                  }\n               }\n            }\n         }\n      }\n   }\n   else {\n      #pragma omp parallel num_threads(num_threads)\n      {\n         int thread_rank = omp_get_thread_num();\n         start = (2*pi)/num_threads * thread_rank;\n         end = (2*pi)/num_threads * (thread_rank+1);\n         if(thread_rank == num_threads-1) {\n            end = 2*pi;\n         }\n         for(int i = 0; i < psize; i++) {\n            if(points[i].x > 0 && points[i].y > 0) {\n               if((points[i].x >= start && points[i].x < end)) {\n                  local_bins[0]++;\n               }\n            }\n            else if(points[i].x < 0 && points[i].y > 0) {\n               if((points[i].x >= start && points[i].x < end)) {\n                  local_bins[1]++;\n               }\n            }\n            else if(points[i].x > 0 && points[i].y < 0) {\n               if((points[i].x >= start && points[i].x < end)) {\n                  local_bins[2]++;\n               }\n            }\n            else if(points[i].x < 0 && points[i].y < 0) {\n               if((points[i].x >= start && points[i].x < end)) {\n                  local_bins[3]++;\n               }\n            }\n         }\n      }\n   }\n\n   if(rank == 0) {\n      for(int i = 0; i < 4; i++) {\n         bins[i] = 0;\n         for(int j = 0; j < num_ranks; j++) {\n            MPI_Status status;\n            int count;\n            MPI_Recv(&count, 1, MPI_INT, j, i, MPI_COMM_WORLD, &status);\n            bins[i] += count;\n         }\n      }\n   }\n   else {\n      for(int i = 0",
            "// TODO\n    // Use the Point.x and Point.y members to bin the data.\n    // Do not create copies of the data.\n    // Use MPI and OpenMP to count in parallel.\n    // Assume that the number of MPI processes is equal to the number of OpenMP threads.\n    // Rank 0 will have the final result stored in bins.\n}",
            "int numprocs, myrank;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n   if (myrank == 0) {\n      for (size_t i = 0; i < 4; i++) {\n         bins[i] = 0;\n      }\n   }\n\n   MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   // Calculate my chunk size\n   size_t chunk_size = points.size() / numprocs;\n   size_t my_start = myrank * chunk_size;\n\n   if (myrank!= numprocs - 1) {\n      my_start += chunk_size;\n   }\n\n   #pragma omp parallel\n   {\n      int threadID = omp_get_thread_num();\n\n      #pragma omp for\n      for (size_t i = my_start; i < my_start + chunk_size; i++) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] += 1;\n         } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1] += 1;\n         } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n         } else if (points[i].x >= 0 && points[i].y < 0) {\n            bins[3] += 1;\n         }\n      }\n   }\n\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO: Fill this in\n}",
            "// 0: bottom left\n   // 1: top left\n   // 2: bottom right\n   // 3: top right\n   int worldSize, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO:\n   // 1. Compute the number of points on each processor.\n   // 2. Assign points to each processor\n   // 3. Use MPI_Bcast to communicate the bins.\n   // 4. Use MPI_Reduce to sum all bins\n   // 5. Use MPI_Bcast to communicate the bins to each processor\n   // 6. Use MPI_Barrier to wait for all processes to finish\n   // 7. Use OpenMP to count points in parallel\n\n}",
            "// TODO:\n   // Your code goes here.\n   size_t size = points.size();\n   bins = {0};\n   // 4 threads\n   #pragma omp parallel num_threads(4)\n   {\n     // 1 thread\n     #pragma omp for\n     for (int i = 0; i < size; ++i) {\n       double x = points[i].x;\n       double y = points[i].y;\n       int whichQuadrant = (x > 0? 1 : 0) + (y > 0? 2 : 0);\n       #pragma omp atomic\n       ++bins[whichQuadrant];\n     }\n   }\n}",
            "int npes;\n   MPI_Comm_size(MPI_COMM_WORLD, &npes);\n   int mype;\n   MPI_Comm_rank(MPI_COMM_WORLD, &mype);\n   const int N = points.size();\n   const int Np = N / npes; //number of points per process\n   const int r = N % npes; //number of points to be added to bigger processes\n   int Np_rank = Np + (r > mype); //number of points per process for rank mype\n   std::array<size_t, 4> bins_rank;\n   for (int i = 0; i < 4; i++)\n      bins_rank[i] = 0;\n   #pragma omp parallel\n   {\n      const int tid = omp_get_thread_num();\n      const int num_threads = omp_get_num_threads();\n      const int Np_threads = Np_rank / num_threads; //number of points per thread\n      const int r_threads = Np_rank % num_threads; //number of points to be added to bigger threads\n      const int Np_rank_tid = Np_threads + (r_threads > tid); //number of points per thread for rank mype\n      #pragma omp for\n      for (int i = 0; i < Np_rank_tid; i++) {\n         const Point &p = points[mype * Np_rank + tid * Np_threads + i];\n         if (p.x >= 0 && p.y >= 0)\n            bins_rank[0]++;\n         else if (p.x < 0 && p.y >= 0)\n            bins_rank[1]++;\n         else if (p.x < 0 && p.y < 0)\n            bins_rank[2]++;\n         else\n            bins_rank[3]++;\n      }\n   }\n   MPI_Reduce(&bins_rank, &bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t npoints = points.size();\n  const size_t nthreads = omp_get_max_threads();\n  size_t npoints_per_thread = npoints / nthreads;\n  size_t npoints_leftover = npoints % nthreads;\n  std::vector<std::array<size_t, 4>> bins_per_thread(nthreads);\n  #pragma omp parallel\n  {\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int thread_id = omp_get_thread_num();\n    size_t npoints_thread = npoints_per_thread + (thread_id < npoints_leftover? 1 : 0);\n    std::vector<Point>::const_iterator first = points.begin() + thread_id * npoints_per_thread + (thread_id < npoints_leftover? thread_id : npoints_leftover);\n    std::vector<Point>::const_iterator last = first + npoints_thread;\n\n    for (auto p = first; p < last; ++p) {\n      auto &bin = bins_per_thread[thread_id];\n      bin[0] += p->x > 0 && p->y > 0;\n      bin[1] += p->x < 0 && p->y > 0;\n      bin[2] += p->x > 0 && p->y < 0;\n      bin[3] += p->x < 0 && p->y < 0;\n    }\n\n    if (rank == 0) {\n      for (auto &bin : bins_per_thread) {\n        bins[0] += bin[0];\n        bins[1] += bin[1];\n        bins[2] += bin[2];\n        bins[3] += bin[3];\n      }\n    }\n  }\n}",
            "}",
            "#pragma omp parallel for shared(points)\n    for (int i = 0; i < points.size(); ++i) {\n        Point p = points[i];\n        int quadrant = 0;\n        if (p.x < 0) quadrant += 1;\n        if (p.y < 0) quadrant += 2;\n        #pragma omp atomic\n        bins[quadrant]++;\n    }\n}"
        ]
    }
]