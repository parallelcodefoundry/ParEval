[
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(Kokkos::View<const int*> const& image, Kokkos::View<size_t[256]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: allocate temporary image\n    // TODO: write the parallel loop\n}",
            "Kokkos::parallel_reduce(\"Pixel Count\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n     KOKKOS_LAMBDA(const int i, size_t& sum) {\n       sum += image(i);\n     },\n     Kokkos::Sum<size_t>(bins)\n   );\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n                         [&](int i){\n                             for (int j = 0; j < image.extent(1); j++) {\n                                 auto image_ij = image(i, j);\n                                 atomic_fetch_add(&(bins(image_ij, 0)), 1);\n                             }\n                         });\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t>(0, 256),\n                         KOKKOS_LAMBDA(const int& i) {\n                           bins(i) = 0;\n                         });\n\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.size()),\n                         KOKKOS_LAMBDA(const int& i) {\n                           bins(image(i)) += 1;\n                         });\n }",
            "/* TODO: Complete this function */\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0,256), KOKKOS_LAMBDA (int i) {\n     bins(i) = 0;\n   });\n   Kokkos::parallel_for(Kokkos::RangePolicy<>(0,image.extent(0)), KOKKOS_LAMBDA (int i) {\n     bins(image(i))++;\n   });\n }",
            "// TODO: Implement this function\n    size_t n;\n    Kokkos::parallel_reduce(\"Pixel counts\", image.extent(0), KOKKOS_LAMBDA(const int i, size_t& sum) {\n        sum += 1;\n    }, n);\n    std::cout << \"The number of pixels in the image is: \" << n << \"\\n\";\n    Kokkos::parallel_for(\"Compute histogram\", image.extent(0), KOKKOS_LAMBDA(const int i) {\n        bins(image(i)) += 1;\n    });\n}",
            "// TODO: Complete this function\n }",
            "// TODO: Implement this function.\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP, size_t>(0, image.extent(0)), [=] (size_t i) {\n     for (size_t j = 0; j < image.extent(1); ++j) {\n       Kokkos::atomic_fetch_add(&bins[image(i, j)], 1);\n     }\n   });\n   Kokkos::fence();\n }",
            "// TODO(you): implement me\n }",
            "Kokkos::parallel_for(\"compute_counts\", image.extent(0),\n    KOKKOS_LAMBDA(const int i) {\n        const int pixel = image(i);\n        Kokkos::atomic_fetch_add(&(bins[pixel]), 1);\n    });\n}",
            "Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy{100, 100};\n   Kokkos::parallel_for(\"Kokkos pixel count\", policy, KOKKOS_LAMBDA (const int& i) {\n     for (int j = 0; j < 256; j++) {\n       bins(j) += image(i) == j;\n     }\n   });\n }",
            "// TODO: implement this function\n   // hint: look at the \"counting_pixels.cpp\" reference implementation\n }",
            "// TODO\n }",
            "// TODO\n    // Your code here\n    //\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA (const size_t &i) {\n        size_t bin;\n        bin = image(i);\n        bins(bin)++;\n    });\n}",
            "// Your code here\n\n }",
            "//TODO: write code\n}",
            "Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(image.extent(0), Kokkos::AUTO);\n  Kokkos::parallel_for(\"pixelCounts\", policy, [=](const Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>::member_type& team_member) {\n    auto my_image = image(team_member.league_rank());\n    auto my_bins = bins(team_member.league_rank());\n\n    Kokkos::parallel_for(Kokkos::TeamThreadRange(team_member, 256), [&] (int i) {\n      my_bins[i] = 0;\n    });\n\n    Kokkos::parallel_for(Kokkos::TeamThreadRange(team_member, my_image.extent(0)), [&] (int j) {\n      my_bins[my_image(j)]++;\n    });\n  });\n}",
            "}",
            "// Create a parallel for loop.\n     // Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const size_t i) {\n     //     if (0 <= image(i) && image(i) < 256) {\n     //         ++bins(image(i));\n     //     }\n     // });\n\n     // Create a parallel for loop.\n     Kokkos::parallel_for(image.size(), KOKKOS_LAMBDA(const size_t i) {\n         // Create a local variable to store the grayscale intensity.\n         int gray;\n         // Check to see if the image intensity is between 0 and 255.\n         if (0 <= image(i) && image(i) < 256) {\n             // Get the grayscale intensity.\n             gray = image(i);\n             // Increment the counter for this grayscale intensity.\n             // Access the counter using the grayscale intensity.\n             ++bins(gray);\n         }\n     });\n }",
            "/* Your code goes here */\n }",
            "// TODO: Implement this function.\n }",
            "/* Insert your code here */\n }",
            "// Your code goes here\n }",
            "// Your code goes here:\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Dynamic>(0, image.extent(0)), [=] (int i) {\n     int val = image(i);\n     if (val < 256) {\n       Kokkos::atomic_fetch_add(&(bins(val)), 1);\n     }\n   });\n }",
            "// TODO: Implement this function\n }",
            "/* TODO: your code here */\n}",
            "}",
            "// TODO: Implement this function\n   // You may assume bins is already initialized to 0\n   // you may not assume the image is contiguous or that it is one dimensional\n   // you may not assume the image contains grayscale values between 0 and 255\n   // you may not assume that image.size() is a multiple of 256\n   // you may not assume that the input is a Kokkos::View\n }",
            "// TODO: Write code here\n   Kokkos::parallel_for(\n       Kokkos::RangePolicy<Kokkos::OpenMP, int>(0, image.extent(0)),\n       KOKKOS_LAMBDA(const int i) {\n         for (int j = 0; j < 256; j++) {\n           if (image(i) == j) {\n             bins[j] += 1;\n           }\n         }\n       });\n }",
            "// TODO\n  // you may assume 256 grayscale levels, so bins is 256 elements\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,256), KOKKOS_LAMBDA(const int i){\n    size_t count = 0;\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,image.extent(0)), KOKKOS_LAMBDA(const int j){\n      if(image(j) == i)\n        ++count;\n    });\n    bins(i) = count;\n  });\n}",
            "// TODO: fill in this function\n}",
            "Kokkos::parallel_for(\n         Kokkos::RangePolicy<Kokkos::Serial>(0, 256), [=](int k) {\n             size_t count = 0;\n             for(int i = 0; i < image.extent(0); i++) {\n                 if(image(i) == k) count++;\n             }\n             bins(k) = count;\n         });\n }",
            "//TODO: Fill this in\n}",
            "// TODO: your code here\n}",
            "Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA(const int i) {\n     // Count the number of pixels with each value\n     for (int j = 0; j < 256; ++j) {\n       bins(j) += (image(i) == j);\n     }\n   });\n }",
            "// TODO\n}",
            "// TODO: Your code goes here.\n}",
            "// Your code here\n}",
            "// TODO: Implement this function\n   // Do not change the function signature\n   auto pixel = Kokkos::TeamPolicy<>::TeamMember();\n   auto const& team = pixel.team();\n   auto const& myRank = pixel.league_rank();\n   auto const& myColor = pixel.team_rank();\n   auto const& myVal = image[pixel.league_rank()];\n   //for(auto const& myVal: image) {\n     if(myRank == 0) {\n       std::cout << \"Image is: \" << std::endl;\n     }\n     std::cout << myVal << \", \";\n   //}\n   //std::cout << std::endl;\n   Kokkos::parallel_for(\"count_pixels\", 256, KOKKOS_LAMBDA(const int i) {\n     bins(i) = 0;\n   });\n   Kokkos::parallel_for(\"count_pixels\", image.size(), KOKKOS_LAMBDA(const int i) {\n     if(myColor == myVal) {\n       Kokkos::atomic_fetch_add(&bins(myVal), 1);\n     }\n   });\n }",
            "Kokkos::parallel_for(\n        \"pixelCounts\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n        KOKKOS_LAMBDA(int i) {\n            for (int j = 0; j < image.extent(1); j++) {\n                // insert your code here\n                int gray_intens = image(i,j);\n                bins(gray_intens)++;\n            }\n        });\n}",
            "// TODO: Complete this function.\n     // Hint: Use the Kokkos parallel_for to parallelize.\n }",
            "auto count = Kokkos::View<size_t>(\"count\", 1);\n    Kokkos::parallel_for(\"pixelCounts\", Kokkos::RangePolicy<Kokkos::Serial>(0, image.extent(0)), KOKKOS_LAMBDA(const int i) {\n      // TODO: replace with your code\n      //auto const& c = image(i);\n      //auto& b = bins(c);\n      //Kokkos::atomic_fetch_add(&b, 1);\n      //Kokkos::atomic_fetch_add(&count(0), 1);\n    });\n    Kokkos::fence();\n    auto const n = count();\n    if (n > 0) {\n      Kokkos::parallel_for(\"scale\", Kokkos::RangePolicy<Kokkos::Serial>(0, bins.extent(0)), KOKKOS_LAMBDA(const int i) {\n        // TODO: replace with your code\n        //auto& b = bins(i);\n        //b /= n;\n      });\n    }\n }",
            "Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n                           [&image, &bins](size_t row, size_t &sum) {\n                             for (size_t col = 0; col < image.extent(1); col++) {\n                               sum += image(row, col);\n                             }\n                           },\n                           bins);\n }",
            "// TODO implement this function\n     // TODO initialize `bins` to all zeros\n }",
            "/* TODO: Your code goes here */\n }",
            "// TODO: count the number of pixels in each grayscale intensity\n\n}",
            "// YOUR CODE HERE\n\n  // Example to create a view\n  //\n  // Kokkos::View<size_t[256]> bins_h(\"bins_h\");\n  // Kokkos::deep_copy(bins_h, bins);\n  //\n  // for(int i=0; i<256; i++) {\n  //   printf(\"%d, %lu\\n\", i, bins_h(i));\n  // }\n\n}",
            "Kokkos::parallel_for(256, KOKKOS_LAMBDA (int bin) {\n        bins[bin] = 0;\n    });\n\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA (size_t i) {\n        bins(image(i)) += 1;\n    });\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256),\n    KOKKOS_LAMBDA(const int i) {\n      bins[i] = 0;\n      for(int j=0; j < 256; ++j) {\n        if(image[j] == i) ++bins[i];\n      }\n    }\n  );\n}",
            "// Compute the range for each thread\n   size_t start = 0;\n   size_t end = image.extent(0);\n\n   size_t pixels_per_thread = end / Kokkos::TeamPolicy<>::team_size();\n\n   // Iterate over each thread\n   Kokkos::parallel_for(\"Pixel Counts\", Kokkos::TeamPolicy<>({0, 0, end}),\n    [&] (Kokkos::TeamPolicy<>::member_type team) {\n      // For each pixel in the range\n      for (size_t i = start + team.league_rank() * pixels_per_thread; i < end; i += team.league_size() * pixels_per_thread) {\n        // Increment the corresponding bin\n        ++bins(image(i));\n      }\n    });\n }",
            "Kokkos::parallel_for(\n        \"pixelCounts\",\n        image.size(),\n        KOKKOS_LAMBDA(int pixel) {\n            bins(image(pixel))++;\n        });\n}",
            "// TODO\n }",
            "// YOUR CODE HERE\n}",
            "// TODO: fill this in\n }",
            "// YOUR CODE HERE\n   throw std::runtime_error(\"pixelCounts not implemented\");\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, image.size()), [&image, &bins](const size_t i) {\n     size_t idx = image(i);\n     Kokkos::atomic_fetch_add(&(bins(idx)), 1);\n   });\n }",
            "// TODO: implement this function\n}",
            "// YOUR CODE HERE\n     // Initialize the histogram.\n     // YOUR CODE HERE\n\n     // YOUR CODE HERE\n     // For each pixel, increment the histogram.\n     // YOUR CODE HERE\n\n     // YOUR CODE HERE\n     // Check your answer\n     // YOUR CODE HERE\n }",
            "// Kokkos has a built-in parallel_for that takes lambdas\n     Kokkos::parallel_for(256, KOKKOS_LAMBDA(const int i) {\n         // initialize each element of bin to 0\n         bins(i) = 0;\n         // count the number of pixels with intensity i\n         for(size_t j=0; j<image.extent(0); j++) {\n             if (image(j) == i) {\n                 bins(i)++;\n             }\n         }\n     });\n }",
            "// TODO: your code here\n     size_t n = image.extent(0);\n     auto team_policy = Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>(n);\n     auto pixels = team_policy.team_size();\n     auto image_device = Kokkos::create_mirror(image);\n     Kokkos::deep_copy(image_device, image);\n     auto hist_device = Kokkos::create_mirror(bins);\n\n     Kokkos::parallel_for(\n         \"pixel_counts\",\n         team_policy,\n         KOKKOS_LAMBDA(const Kokkos::TeamThreadRange& range) {\n             size_t i = range.league_rank();\n             size_t k = range.team_rank();\n             for (size_t p = range.begin(); p < range.end(); ++p) {\n                 size_t l = image_device(p);\n                 Kokkos::atomic_fetch_add(&hist_device(l, k), 1);\n             }\n         });\n\n     Kokkos::deep_copy(bins, hist_device);\n }",
            "// TODO: compute histogram\n }",
            "}",
            "// TODO: implement this function\n    // this function should assume image is 1-dimensional and bins is a 2-dimensional array\n    // the size of the 2-dimensional array should be [256][1]\n    // use Kokkos to count the number of pixels for each grayscale intensity value\n    // use parallel_for or parallel_reduce for this task\n    // this function does not need to return anything\n    Kokkos::parallel_for(\"pixelCounts\", 1, [&] (int i) {\n      int val = image[i];\n      Kokkos::atomic_fetch_add(&(bins[val][0]), 1);\n    });\n}",
            "Kokkos::View<size_t[256]> counts(\"counts\", 256);\n\tfor (int i = 0; i < 256; i++) {\n\t\tcounts(i) = 0;\n\t}\n\tint count = 0;\n\tfor (int i = 0; i < 200; i++) {\n\t\tcount = counts(image(i));\n\t\tcounts(image(i)) = count + 1;\n\t}\n\tfor (int i = 0; i < 256; i++) {\n\t\tbins(i) = counts(i);\n\t}\n}",
            "// TODO\n    size_t *bins_data = bins.data();\n    Kokkos::parallel_for(\"pixelcounts\", Kokkos::RangePolicy<Kokkos::HostSpace, size_t>(0, image.extent(0)), [=](const size_t i) {\n        int pixel = image(i);\n        if (pixel < 0) {\n          // Do nothing.\n        } else if (pixel > 255) {\n          // Do nothing.\n        } else {\n          // Increment the count of that pixel.\n        }\n    });\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: write your code here\n}",
            "Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, image.extent(0));\n     Kokkos::parallel_for(policy, [&](const int i) {\n         for(int j = 0; j < image.extent(1); j++){\n             for(int k = 0; k < image.extent(2); k++){\n                 bins(image(i, j, k)) += 1;\n             }\n         }\n     });\n }",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n\n }",
            "// Fill in this function\n }",
            "Kokkos::parallel_for(\n         \"pixelCounts\", image.extent(0),\n         KOKKOS_LAMBDA(const size_t &i) {\n             int value = image(i);\n             if (value >= 0 && value < 256) {\n                 Kokkos::atomic_fetch_add(&bins(value), 1);\n             }\n         });\n     Kokkos::fence();\n }",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here.\n }",
            "// TODO\n}",
            "const int n = 256;\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA (const int i) {\n        bins(i) = 0;\n    });\n\n    Kokkos::parallel_for(Kokkos::TeamThreadRange(Kokkos::TeamThreadRange(n)), KOKKOS_LAMBDA (const int i) {\n        for(size_t j=0; j<image.size(); j++){\n            if(image(j) == i){\n                Kokkos::atomic_fetch_add(&bins(i), 1);\n            }\n        }\n    });\n}",
            "// TODO: fill in the code here\n}",
            "// TODO: implement this function\n }",
            "// TODO\n}",
            "// Your code here\n }",
            "// Fill bins with zeroes\n    Kokkos::deep_copy(bins, 0);\n\n    // YOUR CODE HERE\n    throw std::runtime_error(\"ERROR: pixelCounts has not been implemented.\");\n}",
            "// Your code here.\n  // Hint: https://github.com/kokkos/kokkos/wiki/Tutorial_How_To_Count_Each_Color_in_an_Image\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::HostSpace>(0, 256), KOKKOS_LAMBDA(const int& gray) {\n         bins(gray) = 0;\n     });\n     Kokkos::fence();\n     \n     Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), KOKKOS_LAMBDA(const int& i) {\n         Kokkos::atomic_fetch_add(&bins(image(i)), 1);\n     });\n     Kokkos::fence();\n }",
            "// TODO: Implement this function.\n    // Don't forget to initialize the array with zeros.\n    // You may find the following functions helpful:\n    //   Kokkos::parallel_for\n    //   Kokkos::single\n    //   Kokkos::atomic_fetch_add\n    //   Kokkos::atomic_fetch_sub\n    //   Kokkos::atomic_fetch_max\n    //   Kokkos::atomic_fetch_min\n\n    Kokkos::parallel_for(\"Kokkos::pixelCounts\", image.extent(0), KOKKOS_LAMBDA(size_t i) {\n      int pixel = image(i);\n      Kokkos::atomic_fetch_add(&bins(pixel), 1);\n    });\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, image.extent(0)), KOKKOS_LAMBDA(const int i) {\n         const int intensity = image(i);\n         bins(intensity)++;\n     });\n }",
            "// TODO: write your solution here\n   // Do not modify the above line, you can modify everything below\n   Kokkos::parallel_for(256, KOKKOS_LAMBDA (const int i){\n    bins(i) = 0;\n   });\n   Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA (const int i){\n    bins(image(i))++;\n   });\n }",
            "//TODO: implement this function\n  }",
            "//TODO: Implement this function\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256), KOKKOS_LAMBDA (const int i) {\n    int count = 0;\n    for (int j = 0; j < 256; ++j) {\n      if (image(j) == i) {\n        count++;\n      }\n    }\n    bins(i) = count;\n  });\n}",
            "size_t block_size = 1024;\n   size_t numBlocks = image.extent(0) / block_size;\n\n   auto img_view = Kokkos::subview(image, 0, Kokkos::ALL());\n   Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(numBlocks, block_size);\n\n   Kokkos::parallel_for(\"counting pixels\", policy,\n                         KOKKOS_LAMBDA(const typename Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>::member_type &member) {\n   size_t count = 0;\n   int i = member.league_rank() * block_size + member.local_id();\n   if (i >= image.extent(0)) return;\n   for(size_t j = 0; j < block_size; ++j) {\n     if(img_view(i) == img_view(i+j)) {\n       count++;\n     }\n   }\n   Kokkos::atomic_fetch_add(&bins[img_view(i)], count);\n   });\n }",
            "}",
            "// Your code here...\n}",
            "Kokkos::TeamPolicy policy(image.extent(0), 128);\n   Kokkos::parallel_for(\"pixel_counts\", policy, KOKKOS_LAMBDA (const Kokkos::TeamMember &team) {\n       Kokkos::parallel_for(Kokkos::TeamThreadRange(team, 0, image.extent(1)), [&] (const int i) {\n           int bin = image(team.league_rank(), i) % 256;\n           Kokkos::atomic_fetch_add(&bins(bin), 1);\n       });\n   });\n }",
            "/* TODO: Replace this comment with the assignment. */\n\n}",
            "// TODO\n  // Hint: Use Kokkos parallel_for to iterate through each pixel\n  //       Use Kokkos atomic_fetch_add for each pixel to increment its bin counter\n\n}",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(const int i) {\n     size_t counter = 0;\n     for (size_t j = 0; j < image.extent(0); ++j) {\n       if (image(j) == image(i)) ++counter;\n     }\n     bins(image(i)) = counter;\n   });\n }",
            "// TODO\n\n}",
            "auto policy = Kokkos::RangePolicy<decltype(Kokkos::TeamThreadRange(Kokkos::TeamThreadRange(Kokkos::ThreadVectorRange(0, 0))))>(Kokkos::TeamThreadRange(Kokkos::TeamThreadRange(Kokkos::ThreadVectorRange(0, 0))));\n    \n    Kokkos::parallel_for(policy, 0, 0, 0, [&](const int &i, const int &j, const int &k){\n        if (image(i,j,k) < 0)\n            Kokkos::abort(\"pixelCounts: image value is negative\");\n        if (image(i,j,k) > 255)\n            Kokkos::abort(\"pixelCounts: image value is too large\");\n    });\n    Kokkos::parallel_for(\"Pixel Counts\", image.extent(0), image.extent(1), image.extent(2), [&](const int &i, const int &j, const int &k){\n        Kokkos::atomic_fetch_add(&bins(image(i,j,k)), 1);\n    });\n}",
            "// Replace this comment with code.\n  // You might need to create Kokkos views for image and bins.\n}",
            "// Compute the number of pixels in each bin.\n   // Your code goes here.\n   // Hint: https://github.com/LLNL/lbann/blob/develop/src/layers/data_type/categorical.cpp\n\n   // Copy the output from bins to host to print the result.\n   // Your code goes here.\n   // Hint: https://github.com/LLNL/lbann/blob/develop/src/layers/data_type/categorical.cpp\n }",
            "// TODO: implement me!\n}",
            "Kokkos::parallel_for(256, KOKKOS_LAMBDA(const int& i) {\n        bins(i) = 0;\n        for (int j = 0; j < image.extent(0); j++) {\n            if (image(j) == i)\n                bins(i)++;\n        }\n    });\n}",
            "// TODO\n }",
            "// TODO: Implement me!\n    int tmp;\n    int count = 0;\n\n    for (int i = 0; i < 256; i++) {\n        bins(i) = 0;\n    }\n\n    for (int i = 0; i < image.extent(0); i++) {\n        tmp = image(i);\n        count++;\n        bins(tmp) = count;\n    }\n}",
            "}",
            "// TODO: Your code goes here\n\n  //for (size_t i = 0; i < image.extent(0); i++) {\n  //  for (size_t j = 0; j < image.extent(1); j++) {\n  //    bins(image(i, j))++;\n  //  }\n  //}\n}",
            "// TODO\n   // Use Kokkos to count the number of pixels in each intensity bin.\n   // Store the number of pixels in each bin in the `bins` vector.\n   // Hint: This is a reduction operation.\n }",
            "Kokkos::parallel_for(\"Counts\", image.extent(0), KOKKOS_LAMBDA(int i) {\n      int val = image(i);\n      bins(val) += 1;\n    });\n  }",
            "// TODO: Your code goes here\n }",
            "// YOUR CODE HERE\n }",
            "int N = image.extent(0);\n    for (int i = 0; i < N; ++i) {\n        // do something\n    }\n}",
            "}",
            "// TODO: Kokkos code goes here\n}",
            "// TODO: implement this function\n\n    Kokkos::View<int[256]> counts(\"counts\", 256);\n    Kokkos::parallel_for(counts.extent(0), KOKKOS_LAMBDA (int i) {\n        counts(i) = 0;\n    });\n    Kokkos::fence();\n\n    Kokkos::parallel_for(image.extent(0), KOKKOS_LAMBDA (int i) {\n        counts(image(i)) += 1;\n    });\n    Kokkos::fence();\n\n    Kokkos::parallel_for(counts.extent(0), KOKKOS_LAMBDA (int i) {\n        bins(i) = counts(i);\n    });\n    Kokkos::fence();\n}",
            "}",
            "// TODO\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t>(0, image.extent(0)), [=](const size_t i) {\n     size_t gray = image(i);\n     bins(gray)++;\n   });\n }",
            "// TODO\n   // Implement the Kokkos version of pixelCounts\n   // Your code should be able to use the same inputs and outputs\n   // as the CPU version.\n   // However, you will need to use Kokkos constructs to\n   // implement this.\n   // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), [&image, &bins](const int& i) {\n   //   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256), [&image, &bins, &i](const int& j) {\n   //     size_t bin = 0;\n   //     Kokkos::parallel_scan(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), [&image, &bin, &j](const int& k) {\n   //       bin += image(k) == j? 1 : 0;\n   //     }, Kokkos::Sum<size_t>(bin));\n   //     bins(j) = bin;\n   //   });\n   // });\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), KOKKOS_LAMBDA(int i) {\n     Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256), KOKKOS_LAMBDA(int j) {\n       size_t bin = 0;\n       Kokkos::parallel_scan(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), [&image, &bin, &j](int k) {\n         bin += image(k) == j? 1 : 0;\n       }, Kokkos::Sum<size_t>(bin));\n       bins(j) = bin;\n     });\n   });\n }",
            "}",
            "// TODO(you): implement this function\n }",
            "// YOUR CODE HERE\n}",
            "const size_t N = image.size();\n    const int* imageData = image.data();\n\n    Kokkos::parallel_for(N, KOKKOS_LAMBDA(const size_t& i) {\n        bins(imageData[i])++;\n    });\n    Kokkos::fence();\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// Your code here...\n }",
            "// TODO: insert code here.\n}",
            "// TODO: Implement this function.\n }",
            "/* TODO */\n}",
            "// TODO: implement this method\n  // hint:\n  //   - use Kokkos::parallel_reduce to call countInGrayLevel_kernel\n  //   - it's not necessary to store the intermediate results in a temporary array\n  //     the result can be stored in bins\n}",
            "// TODO: Implement this function.\n }",
            "// TODO: Your code goes here.\n\n    // TODO: The following line is the solution for the exercise.\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, image.extent(0)), KOKKOS_LAMBDA(const int &i) {\n        bins[image(i)]++;\n    });\n}",
            "Kokkos::parallel_for(\n       \"pixelCounts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n       KOKKOS_LAMBDA(int pixel) {\n         bins(image(pixel)) += 1;\n       });\n }",
            "// TODO: Implement this method.\n    int nthreads = Kokkos::TeamPolicy<>::team_size();\n    int nteams = Kokkos::TeamPolicy<>::team_size();\n    //Kokkos::TeamPolicy<> policy(image.extent(0), Kokkos::AUTO);\n    //Kokkos::TeamPolicy<> policy(image.extent(0), Kokkos::AUTO);\n    Kokkos::TeamPolicy<> policy(image.extent(0), nthreads, nteams);\n\n    Kokkos::parallel_for(policy, KOKKOS_LAMBDA (const Kokkos::TeamThreadRange& r) {\n      const int row = r.league_rank();\n      for (int col = 0; col < image.extent(1); ++col) {\n        const int i = row*image.extent(1) + col;\n        const int bin = image(i);\n        Kokkos::atomic_fetch_add(&bins(bin), 1);\n      }\n    });\n }",
            "// TODO\n }",
            "// TODO: implement this\n   // You'll need to add the Kokkos parallel_for command to this method\n   Kokkos::parallel_for(256, KOKKOS_LAMBDA(const int i) {\n       bins(i) = 0;\n       for(size_t j = 0; j < image.size(); j++) {\n           if(image(j) == i) {\n               bins(i)++;\n           }\n       }\n   });\n }",
            "const size_t N = image.extent(0);\n   // TODO: Finish this function.\n   // Hint: You may want to use Kokkos::parallel_reduce.\n   // Hint: You may also want to use Kokkos::parallel_for.\n }",
            "// TODO: Your code here\n }",
            "}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,image.extent(0));\n  \n  //Kokkos::parallel_for(policy, KOKKOS_LAMBDA (const int i) {\n  //  int current_pixel = image(i);\n  //  bins[current_pixel]++;\n  //});\n  //\n  //Kokkos::parallel_for(policy, KOKKOS_LAMBDA (const int i) {\n  //  Kokkos::atomic_fetch_add(&bins[0], bins[i]);\n  //  Kokkos::atomic_fetch_add(&bins[1], bins[i+1]);\n  // ...\n  //  Kokkos::atomic_fetch_add(&bins[255], bins[i+255]);\n  //});\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO: YOUR CODE HERE\n    // The vector `image` contains grayscale values.\n    // To get the number of pixels with a given grayscale value,\n    //   add up all the elements of the `image` vector that have that value.\n    // Store the result in the element of `bins` with the corresponding\n    //   grayscale value.\n    //\n    // HINTS:\n    // - You'll need to do the counting for each value of `image`.\n    // - You can use the Kokkos parallel_reduce function to do this,\n    //   and to do it in parallel.\n    // - There are a couple of ways to do this, but you can start with\n    //   a nested parallel_for loop, where each loop iterates over\n    //   all the values of `image`.\n    // - Your code will likely look like this:\n    //\n    //   Kokkos::parallel_for(\"...\",\n    //      KOKKOS_LAMBDA(const int& pixel) {\n    //          // TODO: Count pixels with each grayscale value.\n    //      });\n    //\n    // - You'll need to use parallel_reduce to combine the results\n    //   of the nested parallel_for loops into the corresponding\n    //   bins of `bins`.\n    //\n    // - To combine the results of the nested parallel_for loops,\n    //   you'll need to add a lambda to the parallel_reduce that\n    //   increments a value in the corresponding bin of `bins`.\n    //\n    // - You can use a Kokkos::View to get the bin of `bins`\n    //   corresponding to a given value of `image`.\n    //\n    // - The Kokkos parallel_for and parallel_reduce functions can\n    //   take a lambda with an arbitrary number of arguments,\n    //   but they'll only accept up to 10 lambda arguments. If you\n    //   have more than 10 arguments, you can use an object that\n    //   contains multiple data members (e.g. a struct) as one of\n    //   the arguments.\n    //\n    // - There are a few ways to do this, but you can initialize\n    //   the struct outside of the parallel_for, and then assign\n    //   the values in the lambda to the struct member variables\n    //   inside the parallel_for.\n    //\n    // - You can test this with the parallel_for and parallel_reduce\n    //   functions in the test code, and for this reason we have\n    //   provided this test code. You should make sure this code\n    //   works before you try to run it on your laptop.\n    //\n\n    // TODO: YOUR CODE HERE\n    // Example:\n    //\n    // size_t[256] bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,",
            "size_t numPixels = image.extent(0);\n   // TODO: Write your code here.\n}",
            "// TODO: Fill in the correct Kokkos reduction functor.\n  Kokkos::MDRangePolicy<Kokkos::Rank<2>> policy({0,0},{2,image.extent(1)},{1,1});\n  Kokkos::parallel_for(\"pixelCounts\", policy, KOKKOS_LAMBDA (const int& i, const int& j){\n    int intensity = image(i,j);\n    Kokkos::atomic_fetch_add(&(bins(intensity)),1);\n  });\n}",
            "Kokkos::parallel_for(\"pixel_count\", image.extent(0), KOKKOS_LAMBDA (const int i) {\n         const int grayscale_value = image(i);\n         bins(grayscale_value) += 1;\n     });\n     Kokkos::fence();\n }",
            "// TODO: complete this function\n   int numThreads = 256;\n   Kokkos::RangePolicy<Kokkos::Serial> policy(0, numThreads);\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA (int i){\n     bins(image(i))++;\n   });\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256),\n      KOKKOS_LAMBDA (const int i) {\n        bins(i) = 0;\n        for (int j = 0; j < image.extent(0); j++) {\n          if (image(j) == i) bins(i)++;\n        }\n      });\n  }",
            "// TODO\n}",
            "/* Your code here */\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256), [&](int i){\n     bins(i) = 0;\n   });\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), [&](int i){\n     bins(image(i)) += 1;\n   });\n }",
            "// TODO: implement this function using Kokkos\n }",
            "// Your code goes here\n }",
            "// TODO: your code here\n\n}",
            "// TODO: Implement the function\n}",
            "// TODO\n }",
            "// YOUR CODE HERE\n}",
            "// Your code here.\n }",
            "// Kokkos provides a vector type, `Kokkos::View`, which wraps a contiguous chunk of memory.\n   // The first template parameter is the type of data in the vector.\n   // The second template parameter is the execution space to use when running this code.\n   // You can use different execution spaces by defining different macros.\n   // The third template parameter is the memory space to use for the underlying data.\n   // By default, the memory space for the vector is the same as the execution space.\n   // You can also make the vector use a different memory space by explicitly setting the third template parameter.\n   //\n   // Kokkos is a header-only library. The header file only needs to be included once.\n   // You don't need to link the library or include other source files.\n   //\n   // A vector is initialized with a size and a pointer to a block of memory.\n   // There is also a constructor that takes a `Kokkos::HostSpace` execution space and a C array.\n   //\n   // You can use a Kokkos vector as a C array using the `data()` method.\n   //\n   // The code here is in a function so that you can use the `KOKKOS_FUNCTION` macro to annotate that the function is Kokkos-parallelized.\n   //\n   // Kokkos provides parallel_for that takes three template parameters:\n   //   * Execution space\n   //   * Function object that will be run in parallel\n   //   * View of data to iterate over\n   // The function object is a lambda, or a function object created with `Kokkos::KOKKOS_LAMBDA`.\n   // Lambda expressions are written using C++11 features.\n   //\n   // For more information, see:\n   //   http://kokkos.github.io/\n   //   https://github.com/kokkos/kokkos/wiki\n   //   http://on-demand.gputechconf.com/gtc/2014/presentations/S4430-C++11-Features-in-Kokkos.pdf\n   Kokkos::View<const int*> image_c(image.data(), image.size());\n   Kokkos::parallel_for(\"CountPixels\", image.size(), KOKKOS_FUNCTION(const int i, const int n, size_t bins[256], const int val, {\n     bins[val] += 1;\n   }), image_c, bins);\n }",
            "// Fill the vector with zeros.\n   // TODO: Kokkos doesn't have a zero-initialization constructor for Views.\n   //       One workaround is to copy an array of zeros, but that doesn't look\n   //       as nice. Maybe there's a better way?\n   Kokkos::deep_copy(bins, 0);\n   \n   // The Kokkos parallel_for loop.\n   // See https://github.com/kokkos/kokkos/wiki/FAQ#is-kokkos-for-loop-the-same-as-c11-for-loop\n   // for a discussion about Kokkos parallel_for vs. C11 for loops.\n   // The loop iterates over the image (i) and increments the corresponding\n   // element in `bins` (j).\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n     KOKKOS_LAMBDA(int i) {\n       for (int j = 0; j < image.extent(1); ++j) {\n         // Kokkos has an atomic add function:\n         Kokkos::atomic_fetch_add(&bins(image(i, j)), 1);\n       }\n     }\n   );\n   \n   // TODO: Kokkos doesn't have a synchronization function.\n   //       A workaround is to copy the data back to the host.\n   //       For this to work we need to copy the entire array to a single\n   //       rank-1 array.\n   auto hbins = Kokkos::create_mirror_view(bins);\n   Kokkos::deep_copy(hbins, bins);\n   \n   // Print out the values\n   std::cout << \"Pixel counts:\" << std::endl;\n   for (int i = 0; i < 256; i++) {\n     std::cout << i << \": \" << hbins(i) << std::endl;\n   }\n }",
            "}",
            "// TODO: Kokkos code goes here\n  // For now, just copy this code:\n  size_t zero = 0;\n  Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace,size_t>(0,256),\n  [=](const size_t i, size_t &update) {\n    if (image[i] > 255 || image[i] < 0) return;\n    update = update + 1;\n    }, zero);\n  bins[0] = zero;\n}",
            "// TODO: Implement this function.\n }",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n    KOKKOS_LAMBDA(const int& i) {\n      int c = image[i];\n      Kokkos::atomic_fetch_add(&bins(c), 1);\n  });\n}",
            "// TODO: Your code here\n   // hint: this will be similar to the reduction example in lecture 4\n   // hint: look at Kokkos::parallel_for to parallelize this code\n   // hint: look at Kokkos::atomic_fetch_add to modify bins in parallel\n}",
            "auto exec_space = Kokkos::DefaultExecutionSpace();\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256),\n    KOKKOS_LAMBDA(int i) {\n    bins(i) = 0;\n  });\n  Kokkos::fence();\n\n  auto policy = Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>(image.size(), 1024);\n\n  Kokkos::parallel_for(policy, \n    KOKKOS_LAMBDA (const Kokkos::TeamMember &team) {\n    size_t local_count = 0;\n    for (int i = team.league_rank() * team.team_size(); i < (team.league_rank()+1) * team.team_size(); ++i) {\n      if (image(i) == i) {\n        ++local_count;\n      }\n    }\n    team.team_barrier();\n\n    Kokkos::parallel_reduce(Kokkos::TeamThreadRange(team, 256),\n      [&] (int i, int &global_count) {\n      global_count += (i == team.team_rank())? local_count : 0;\n    }, Kokkos::Sum<int>(global_count));\n  });\n\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(\"pixelCounts\", 0, image.size(), KOKKOS_LAMBDA (const int& i) {\n         ++bins[image(i)];\n     });\n }",
            "// YOUR CODE HERE\n  // loop over the image\n  // for each pixel:\n  //   increment the bin associated with the pixel's value\n  // hint: look at the kokkos_examples/example_histogram.cpp and kokkos_examples/example_histogram_cuda.cpp\n  // hint: to copy the contents of a view to a Kokkos::RangePolicy, use the `copy` method\n}",
            "// TODO: YOUR CODE HERE\n }",
            "// TODO: Fill in the rest of the function.\n}",
            "//TODO:\n }",
            "// TODO: write this function\n\n }",
            "Kokkos::View<size_t*> sums(\"sums\", 256);\n   Kokkos::deep_copy(sums, size_t(0));\n   Kokkos::parallel_for(\"Pixel Counts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 256), KOKKOS_LAMBDA(const int i) {\n     sums(i) = Kokkos::parallel_sum(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), KOKKOS_LAMBDA(const int j) {\n       return image(j) == i? 1 : 0;\n     });\n   });\n   Kokkos::deep_copy(bins, sums);\n }",
            "// TODO: YOUR CODE HERE\n  }",
            "// TODO: implement this function\n }",
            "// TODO\n }",
            "Kokkos::parallel_for(\"pixelCounts\", image.extent(0), KOKKOS_LAMBDA(int i) {\n        int const c = image(i);\n        ++bins[c];\n    });\n    Kokkos::fence();\n}",
            "// TODO: fill this in\n }",
            "// TODO: implement the function.\n   // Hints: \n   //   - consider parallel_for and reductions\n   //   - parallel_for can be called from parallel_for as long as the iteration space is not modified\n   //   - you can use Kokkos::single() to update a single element\n   //   - use Kokkos::atomic_fetch_add() and Kokkos::atomic_fetch_sub()\n }",
            "// YOUR CODE HERE\n   \n }",
            "// Write your code here\n }",
            "// Your code goes here\n}",
            "}",
            "Kokkos::View<size_t[256]> counters(\"counters\", 256);\n  Kokkos::View<size_t> scratch(\"scratch\", 256);\n  counters();\n  scratch();\n  Kokkos::parallel_for(\"reset\", 1, KOKKOS_LAMBDA(const int&){\n    for (int i = 0; i < 256; ++i) {\n      counters(i) = 0;\n    }\n  });\n  Kokkos::parallel_for(\"count\", image.extent(0), KOKKOS_LAMBDA(const int& i){\n    counters(image(i))++;\n  });\n  Kokkos::parallel_for(\"accumulate\", 256, KOKKOS_LAMBDA(const int& i){\n    Kokkos::atomic_fetch_add(&scratch(i), counters(i));\n  });\n  Kokkos::parallel_for(\"copy\", 256, KOKKOS_LAMBDA(const int& i){\n    bins(i) = scratch(i);\n  });\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO: implement the function\n }",
            "}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,image.size()),\n        KOKKOS_LAMBDA (const int i) {\n            size_t pixel = image(i);\n            Kokkos::atomic_fetch_add(&(bins(pixel)), 1);\n        });\n}",
            "// TODO:\n  // implement this function.\n\n}",
            "// YOUR CODE HERE\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)),\n    KOKKOS_LAMBDA(const int i){\n        int x = image(i);\n        ++bins(x);\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, image.extent(0)), KOKKOS_LAMBDA (int i) {\n        for (int j=0; j<image.extent(0); j++) {\n            bins(image(i)) += 1;\n        }\n    });\n}",
            "// YOUR CODE HERE\n   // Hint: 256 is the maximum grayscale value + 1.\n   // Hint: try to count in parallel using a Kokkos parallel_for\n }",
            "// TODO: fill in the implementation.\n   // Hint: Use Kokkos parallel_reduce.\n   // Hint: Use Kokkos parallel_for for the inner loop.\n   // Hint: You can use Kokkos parallel_for to sum the values in bins.\n   // Hint: Use Kokkos::atomic_fetch_add to update bins.\n }",
            "// TODO: your code here\n  // Use a parallel_for to perform the counting.\n  // See https://github.com/kokkos/kokkos/wiki/tutorial_simple_vector_add\n }",
            "// TODO: complete the function\n   // Hint: use parallel_for to count the pixels in parallel\n }",
            "// Your code goes here.\n }",
            "/* YOUR CODE HERE */\n }",
            "Kokkos::View<size_t[256], Kokkos::LayoutLeft, Kokkos::CudaUVMSpace> bins_h(\"counts\", 256);\n\n   Kokkos::deep_copy(bins_h, 0);\n\n   // TODO\n\n }",
            "size_t local_bins[256] = {0};\n    for(size_t i = 0; i < image.extent(0); i++){\n      local_bins[image(i)] += 1;\n    }\n    for(size_t i = 0; i < 256; i++){\n      bins(i) = local_bins[i];\n    }\n}",
            "/* Your code here */\n }"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "std::fill(bins.begin(), bins.end(), 0);\n \t#pragma omp parallel for\n \tfor (size_t i = 0; i < image.size(); i++) {\n \t\tbins[image[i]]++;\n \t}\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: compute the histogram of the input image\n    int width = image.size();\n    int height = 1;\n    std::array<size_t, 256> hist;\n    hist.fill(0);\n    \n    #pragma omp parallel for\n    for (int i = 0; i < width; i++){\n        hist[image[i]]++;\n    }\n    \n    // TODO: fill `bins` with counts of pixels with grayscale intensities\n    bins = hist;\n}",
            "// TODO: Your code here\n\n }",
            "for (size_t i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n}",
            "// TODO: Your code goes here\n }",
            "for (int i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n}",
            "#pragma omp parallel for\n for (int i = 0; i < image.size(); ++i) {\n   bins[image[i]]++;\n }\n}",
            "int nthreads = omp_get_max_threads();\n\t#pragma omp parallel num_threads(nthreads)\n\t{\n\t\t#pragma omp for schedule(static)\n\t\tfor (int i = 0; i < image.size(); i++) {\n\t\t\tbins[image[i]]++;\n\t\t}\n\t}\n}",
            "// your code here\n}",
            "// TODO: Your code here\n\n  size_t nbins = 256;\n  bins = std::array<size_t, 256>(nbins);\n  size_t num_threads = 8;\n#pragma omp parallel for num_threads(num_threads)\n  for(int i=0;i<image.size();i++)\n    bins[image[i]]++;\n}",
            "// 1. For each pixel intensity, count the number of pixels in the image.\n  //    Store the result in `bins`.\n\n  // 2. Use OpenMP to count the number of pixels in parallel.\n  //    OpenMP parallelization is a good idea even if you only have a\n  //    single thread.  This will make the code more scalable.\n\n  // You can use the `omp parallel for` directive for this.\n\n  // 3. The above code is a sequential implementation of the parallel\n  //    algorithm. The code below will execute in parallel.\n\n  // Hint: for the first pixel (x=0), all the pixels in the image have\n  // the same intensity (y=0). Thus, the first thread will write a result\n  // to the bins[0] bin. However, there are 4 threads, so the result will\n  // be added 4 times, which is wrong. To solve this, we need to use the\n  // atomic directive on the bins[x] bin.\n  // Hint: OpenMP has a `reduction` clause for this.\n\n  // 4. Verify that your implementation is correct.\n\n\n}",
            "// TODO: YOUR CODE HERE\n    int num_threads = omp_get_max_threads();\n    std::vector<int> image_thr(image.size());\n    for(int i=0; i<num_threads; i++)\n        image_thr.push_back(image[i]);\n    int num_pixels = image.size();\n    #pragma omp parallel for schedule(static, 1024)\n    for(int i=0; i<num_pixels; i++)\n        bins[image[i]]++;\n }",
            "// initialize histogram bins to 0\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // parallel region\n  #pragma omp parallel \n  {\n    // each thread needs its own private version of bins\n    std::array<size_t, 256> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n\n    // loop over pixels\n    #pragma omp for schedule(dynamic)\n    for (size_t i = 0; i < image.size(); ++i) {\n      // increment the bin at index image[i]\n      localBins[image[i]] += 1;\n    }\n\n    // synchronize (required for reduction)\n    #pragma omp critical\n    {\n      // add the local histogram bins to the global bins\n      for (size_t i = 0; i < localBins.size(); ++i) {\n        bins[i] += localBins[i];\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < image.size(); i++) {\n        size_t grayscale = image[i];\n        bins[grayscale]++;\n    }\n\n}",
            "int nthreads = 0;\n#pragma omp parallel\n  {\n#pragma omp single\n    nthreads = omp_get_num_threads();\n  }\n  size_t const n = image.size();\n  size_t const chunkSize = n / nthreads;\n  std::vector<size_t> counts(nthreads);\n\n#pragma omp parallel num_threads(nthreads)\n  {\n    int tid = omp_get_thread_num();\n    size_t start = tid * chunkSize;\n    size_t end = (tid == nthreads - 1)? n : (tid + 1) * chunkSize;\n    std::fill(counts.begin(), counts.end(), 0);\n    for (size_t i = start; i < end; ++i) {\n      ++counts[image[i]];\n    }\n  }\n\n  for (size_t i = 0; i < nthreads; ++i) {\n    bins[i] += counts[i];\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "bins = std::array<size_t, 256>();\n    for (int pixel : image) {\n        ++bins[pixel];\n    }\n}",
            "omp_set_num_threads(8);\n    #pragma omp parallel for\n    for(unsigned int i=0; i<image.size(); ++i) {\n        unsigned int index = image[i];\n        ++bins[index];\n    }\n}",
            "/* Your code here */\n }",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++)\n        bins[image[i]]++;\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "size_t n = image.size();\n    // YOUR CODE HERE\n    int th = 16;\n    omp_set_num_threads(th);\n    #pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        bins[image[i]]++;\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n}",
            "// Add your code here.\n }",
            "// Your code here!\n}",
            "size_t i;\n\n   /* Put your code here */\n   #pragma omp parallel for\n   for(i=0;i<image.size();i++)\n   {\n       bins[image[i]]++;\n   }\n}",
            "int threads = omp_get_max_threads();\n    // YOUR CODE HERE\n    int max = 0;\n    #pragma omp parallel for num_threads(threads)\n    for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n      if (max < image[i]) {\n        max = image[i];\n      }\n    }\n    int sum = 0;\n    for (int i = 0; i <= max; i++) {\n      sum += bins[i];\n      bins[i] = sum;\n    }\n}",
            "// TODO: Your code goes here.\n}",
            "#pragma omp parallel for\n   for(int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: write code here\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel\n{\n\n#pragma omp for\nfor (size_t i = 0; i < image.size(); ++i) {\n    //std::cout << omp_get_thread_num() << \"  \" << omp_get_num_threads() << std::endl;\n    ++bins[image[i]];\n}\n\n}\n }",
            "/* YOUR CODE HERE */\n   for(size_t i = 0; i < image.size(); i++) bins[image[i]]++;\n }",
            "#pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < image.size(); ++i) {\n      ++bins[image[i]];\n    }\n }",
            "// Your code goes here...\n}",
            "// YOUR CODE HERE\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < image.size(); ++i) {\n            ++bins[image[i]];\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "// YOUR CODE HERE\n   #pragma omp parallel for\n   for(int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// omp_set_num_threads(1);\n  // #pragma omp parallel for\n  // for (int i = 0; i < image.size(); i++) {\n  //   bins[image[i]]++;\n  // }\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n    }\n  }\n\n}",
            "for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// Your code here\n}",
            "int threads = omp_get_max_threads();\n  std::vector<std::array<size_t, 256>> threadBins(threads, std::array<size_t, 256>());\n  int pixels = image.size();\n  int pixelsPerThread = pixels/threads;\n  int index = 0;\n  // for each thread, set it's bins array to 0\n  #pragma omp parallel for\n  for (int i = 0; i < threads; i++) {\n    for (int j = 0; j < 256; j++) {\n      threadBins[i][j] = 0;\n    }\n  }\n  // for each thread, increment the bins array for each pixel\n  #pragma omp parallel for\n  for (int i = 0; i < threads; i++) {\n    for (int j = 0; j < pixelsPerThread; j++) {\n      threadBins[i][image[index]]++;\n      index++;\n    }\n  }\n  // merge all the bins arrays\n  #pragma omp parallel for\n  for (int i = 0; i < 256; i++) {\n    for (int j = 0; j < threads; j++) {\n      bins[i] += threadBins[j][i];\n    }\n  }\n}",
            "// your code here\n }",
            "#pragma omp parallel for\n    for (int i=0; i < image.size(); ++i) {\n      ++bins[image[i]];\n    }\n}",
            "// TODO: your code here\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]] += 1;\n   }\n }",
            "size_t N = image.size();\n    bins.fill(0);\n#pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n      bins[image[i]]++;\n    }\n }",
            "omp_set_num_threads(4);\n    #pragma omp parallel\n    {\n        int thread_num = omp_get_thread_num();\n        std::vector<int> local_bins(bins.size());\n        memset(local_bins.data(), 0, sizeof(int) * bins.size());\n        for (size_t i = 0; i < image.size(); i++) {\n            local_bins[image[i]]++;\n        }\n        for (size_t i = 0; i < local_bins.size(); i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "omp_set_num_threads(4);\n    for (auto i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: fill out this function\n    bins.fill(0);\n    size_t len = image.size();\n#pragma omp parallel for\n    for(size_t i=0;i<len;i++){\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for schedule(static)\n   for(size_t i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: Implement OpenMP version\n}",
            "size_t i, n = image.size();\n  bins.fill(0);\n  #pragma omp parallel for\n  for (i = 0; i < n; i++) {\n    bins[image[i]]++;\n  }\n  /* Your code here */\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// YOUR CODE HERE\n }",
            "// YOUR CODE HERE\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n   \n   #pragma omp parallel\n   {\n     std::array<size_t, 256> localBins;\n     std::fill(localBins.begin(), localBins.end(), 0);\n     \n     //TODO: Update code to count the number of pixels in image with each grayscale intensity\n     //Hint: Use std::transform to iterate over the pixels in the image\n     \n     #pragma omp critical\n     {\n       for(int i = 0; i < 256; i++) {\n         bins[i] += localBins[i];\n       }\n     }\n   }\n }",
            "// TODO: fill in this function\n}",
            "#pragma omp parallel for\n   for (auto i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "//   int thread_count = omp_get_max_threads();\n//   std::cout << \"Using \" << thread_count << \" threads.\" << std::endl;\n\n  // parallel for:\n#pragma omp parallel for\n  for (int i = 0; i < image.size(); ++i) {\n    // bins[image[i]]++; // sequential\n\n#pragma omp atomic\n    bins[image[i]]++; // atomic\n  }\n}",
            "// TODO: your code here\n   size_t size = image.size();\n   std::vector<size_t> temp(256, 0);\n   omp_set_num_threads(10);\n   #pragma omp parallel for\n   for (size_t i = 0; i < size; i++) {\n       temp[image[i]]++;\n   }\n   for (size_t i = 0; i < 256; i++) {\n       bins[i] = temp[i];\n   }\n }",
            "// your code here\n    \n#pragma omp parallel num_threads(10) default(none)\n    {\n        int tid = omp_get_thread_num();\n        int img_size = image.size();\n        int start = (img_size/10)*tid;\n        int end = (img_size/10)*(tid+1);\n        int sum = 0;\n        for(int i = start; i < end; i++)\n            sum += image[i];\n        bins[tid] = sum;\n    }\n    \n    // TODO: Add your code here\n    // We used #pragma omp to run the for loop in parallel\n    // We created 10 threads, each thread will process 1/10 of the image\n    // Sum up the pixel counts in each thread and store the results in bins\n}",
            "// Your code here\n\n    // for (auto i = 0; i < image.size(); i++) {\n    //     bins[image[i]]++;\n    // }\n\n    // omp parallel\n    // {\n    //     int myID = omp_get_thread_num();\n    //     std::cout << \"Hello World\" << myID << std::endl;\n    // }\n\n    // #pragma omp parallel for\n    // for (auto i = 0; i < image.size(); i++) {\n    //     bins[image[i]]++;\n    // }\n\n    // #pragma omp parallel for\n    // for (auto i = 0; i < image.size(); i++) {\n    //     omp_set_num_threads(4);\n    //     bins[image[i]]++;\n    // }\n\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "// TODO: Your code here\n  int size = image.size();\n  int n = 256;\n  int m = 512;\n  //int threads = 5;\n  int block = 1;\n  int nthreads;\n  int *data = new int[n*m];\n  int *sum = new int[m];\n  for (int i = 0; i < size; i++){\n    data[image[i]*m + (i%m)]++;\n  }\n  #pragma omp parallel num_threads(block) private(nthreads)\n  {\n    nthreads = omp_get_num_threads();\n    int start = omp_get_thread_num()*m/nthreads;\n    int end = (omp_get_thread_num()+1)*m/nthreads;\n    for (int i = 0; i < m; i++){\n      sum[i] = 0;\n    }\n    for (int i = start; i < end; i++){\n      for (int j = 0; j < n; j++){\n        sum[i] += data[j*m + i];\n      }\n    }\n  }\n  for (int i = 0; i < 256; i++){\n    bins[i] = sum[i];\n  }\n  delete[] data;\n  delete[] sum;\n}",
            "//...\n }",
            "for (int i = 0; i < 256; ++i) bins[i] = 0;\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) ++bins[image[i]];\n }",
            "#pragma omp parallel for\n     for (int i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < image.size(); ++i) {\n\t\t++bins[image[i]];\n\t}\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// TODO: implement this function\n\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "// Add your code here.\n   for(int i = 0; i < 256; i++){\n       bins[i] = 0;\n   }\n   for(int i = 0; i < image.size(); i++){\n       bins[image[i]] += 1;\n   }\n}",
            "// TODO: Your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// Replace the following line with your solution.\n   // #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for schedule(dynamic)\n    for (auto& val: image) {\n        bins[val] += 1;\n    }\n}",
            "// TODO: implement pixelCounts\n   // Hint: if you have a vector or array of 256 ints, then you can\n   // use the function \"omp parallel for\" to run the following code\n   // for each element in parallel.\n\n}",
            "//omp_set_num_threads(4);\n     //#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); ++i) {\n         bins[image[i]]++;\n     }\n }",
            "// TODO: YOUR CODE HERE\n }",
            "// TODO: Your code here\n  int num_threads = 1;\n#pragma omp parallel\n  {\n#pragma omp single\n    num_threads = omp_get_num_threads();\n  }\n  auto thread_id = 0;\n#pragma omp parallel private(thread_id)\n  {\n#pragma omp single\n    thread_id = omp_get_thread_num();\n  }\n  auto pixels_per_thread = image.size() / num_threads;\n  auto start = thread_id * pixels_per_thread;\n  auto end = (thread_id + 1) * pixels_per_thread;\n  for (auto i = start; i < end; ++i) {\n    bins[image[i]]++;\n  }\n}",
            "size_t const num_pixels = image.size();\n     #pragma omp parallel for\n     for (size_t i = 0; i < num_pixels; ++i) {\n         bins[image[i]] += 1;\n     }\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n        for (size_t j = 0; j < image.size(); j++) {\n            if (image[j] == i) bins[i] += 1;\n        }\n    }\n}",
            "// fill the bins with zeroes\n  std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    // increment bin corresponding to intensity\n    bins[image[i]]++;\n  }\n}",
            "for(size_t i=0; i<image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n   size_t n = image.size();\n\n   size_t numThreads = 0;\n#pragma omp parallel \n   {\n#pragma omp single\n      {\n#pragma omp task\n         {\n            numThreads = omp_get_num_threads();\n         }\n      }\n   }\n\n   size_t stride = n/numThreads;\n   std::vector<size_t> count(256, 0);\n   std::vector<size_t> local(256, 0);\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < n; i++) {\n      local[image[i]]++;\n   }\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < 256; i++) {\n      count[i] = local[i];\n   }\n\n   #pragma omp parallel for\n   for (size_t i = 1; i < 256; i++) {\n      count[i] += count[i-1];\n   }\n\n   bins = count;\n}",
            "// TODO: YOUR CODE HERE\n }",
            "#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n}",
            "// Add your code here.\n  // We will use the OpenMP library to parallelize the computation.\n  // Please refer to the course handout and/or slides for more details.\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    int bin;\n    size_t local_count = 0;\n    for (auto &pixel : image) {\n      bin = pixel;\n      bins[bin]++;\n      local_count++;\n    }\n  }\n}",
            "#pragma omp parallel for\n   for (int i=0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "int sum = 0;\n    int i, j;\n    #pragma omp parallel for reduction(+:sum) private(i, j)\n    for (i=0; i<image.size(); i++) {\n        sum = sum + image[i];\n    }\n    for (i=0; i<bins.size(); i++) {\n        bins[i] = sum;\n    }\n}",
            "// YOUR CODE HERE\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// Initialize bins array\n  std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n  // Calculate the number of pixels of each grayscale intensity\n  for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// write code here\n\n    // #pragma omp parallel for\n    // for (int i = 0; i < image.size(); ++i) {\n    //     int pixel = image[i];\n    //     bins[pixel]++;\n    // }\n}",
            "// TODO\n}",
            "#pragma omp parallel for\n\tfor(int i=0; i<image.size(); i++) {\n\t\t#pragma omp atomic\n\t\tbins[image[i]]++;\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n    }\n  }",
            "size_t size = image.size();\n   std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel for\n   for (int i = 0; i < size; ++i) {\n     bins[image[i]]++;\n   }\n }",
            "//TODO: Your code here\n}",
            "int num_threads = 4;\n    omp_set_num_threads(num_threads);\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < image.size(); i++) {\n            bins[image[i]]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n  for (int i = 0; i < image.size(); i++)\n    bins[image[i]]++;\n}",
            "#pragma omp parallel for\n     for (size_t i = 0; i < image.size(); ++i) {\n         bins[image[i]]++;\n     }\n }",
            "for(size_t i=0; i<image.size(); ++i) {\n        bins[image[i]] += 1;\n    }\n}",
            "// TODO\n }",
            "int n = image.size();\n   int i;\n   int num_threads = omp_get_max_threads();\n   #pragma omp parallel num_threads(num_threads)\n   {\n       int id = omp_get_thread_num();\n       int n_i = n/num_threads;\n       std::array<size_t, 256> local_bins;\n       memset(local_bins.data(), 0, sizeof(size_t)*256);\n       for(i=0; i<n_i; i++) {\n           local_bins[image[i]]++;\n       }\n       #pragma omp barrier\n       #pragma omp single\n       {\n           for(i=0; i<num_threads; i++) {\n               for(int j=0; j<256; j++) {\n                   bins[j] += local_bins[j];\n               }\n           }\n       }\n   }\n}",
            "bins = std::array<size_t, 256>();\n\n    #pragma omp parallel for\n    for (int i = 0; i < image.size(); i++){\n        bins[image[i]] += 1;\n    }\n\n}",
            "int num_threads = omp_get_max_threads();\n  int chunk_size = image.size() / num_threads;\n\n  #pragma omp parallel for schedule(static, chunk_size) reduction(+:bins)\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]] += 1;\n  }\n\n}",
            "bins = std::array<size_t, 256> {};\n    #pragma omp parallel for\n    for(auto const& p : image) {\n        bins[p]++;\n    }\n}",
            "/* Put your code here */\n   bins[0] = 0;\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "#pragma omp parallel for schedule(static)\n  for (int i = 0; i < image.size(); i++) {\n    int bin = image[i];\n    bins[bin]++;\n  }\n}",
            "// TODO\n    // 1. count number of pixels in each intensity\n    // 2. update bins array\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n }",
            "//...\n\n }",
            "#pragma omp parallel for schedule(static)\n    for (auto i = 0u; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "const int numPixels = image.size();\n\n   // Your code goes here.\n   bins.fill(0);\n\n   for (int i = 0; i < numPixels; i++) {\n     bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (auto i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "// Count the number of pixels in the image with each grayscale intensity.\n   // The vector `image` is a grayscale image with values 0-255.\n   // Store the results in `bins`.\n   // Use OpenMP to count in parallel.\n\n   // TODO: YOUR CODE HERE\n }",
            "#pragma omp parallel for\n   for(size_t i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n   }\n }",
            "// TODO: your code here\n    int m = image.size();\n    bins.fill(0);\n    #pragma omp parallel for\n    for (int i = 0; i < m; i++)\n        bins[image[i]]++;\n}",
            "// fill in code...\n }",
            "size_t n = image.size();\n\tint const M = 256;\n\tstd::fill(bins.begin(), bins.end(), 0);\n\t#pragma omp parallel for\n\tfor(size_t i=0;i<n;i++)\n\t\tbins[image[i]]++;\n }",
            "const int num_bins = 256;\n   #pragma omp parallel for\n   for (int i = 0; i < num_bins; i++)\n     bins[i] = 0;\n\n   int img_length = image.size();\n\n   #pragma omp parallel for\n   for (int i = 0; i < img_length; i++)\n     bins[image[i]]++;\n }",
            "#pragma omp parallel for schedule(static)\n\tfor (int i = 0; i < image.size(); ++i) {\n\t\t++bins[image[i]];\n\t}\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// Your code here.\n\n}",
            "bins = {0}; // Initialize `bins` array to zero.\n     #pragma omp parallel for // use OpenMP to parallelize this loop\n     for(size_t i=0; i<image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for(size_t i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n}",
            "// Add your code here...\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// Your code goes here\n }",
            "bins.fill(0);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < (int)image.size(); ++i) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "// Your code goes here\n   // omp_set_num_threads(omp_get_max_threads());\n   std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n   for(int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n}",
            "#pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// YOUR CODE HERE\n}",
            "for (size_t i=0; i < image.size(); ++i) {\n     // TODO: Add your code here.\n     ++bins[image[i]];\n   }\n }",
            "for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]]++;\n    }\n}",
            "// Your code here.\n    for(int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Your code here.\n  bins.fill(0);\n  omp_set_num_threads(8);\n  #pragma omp parallel for\n  for(int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// Add your code here\n    std::fill(bins.begin(), bins.end(), 0);\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "//TODO: Fill this in!\n\n    //Hint: you can parallelize this loop.\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i)\n    {\n        bins[image[i]] += 1;\n    }\n\n}",
            "// Your code here\n }",
            "// TODO: Your code here\n    int size = image.size();\n    int bin = 0;\n    int max = 0;\n    #pragma omp parallel for reduction(max:max)\n    for(int i=0; i<size; i++){\n        bin = image[i];\n        #pragma omp atomic update\n        bins[bin]++;\n        if(bins[bin]>max){\n            max = bins[bin];\n        }\n    }\n    printf(\"Maximum = %d\\n\", max);\n}",
            "// Your code here\n }",
            "#pragma omp parallel for\n  for (int pixel : image) {\n    bins[pixel]++;\n  }\n}",
            "// YOUR CODE HERE\n\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// write your solution here\n  // bins is a 256-element array\n  // hint: use a counter\n#pragma omp parallel for\nfor(int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   // TODO: replace this loop with an OpenMP parallel for loop.\n   #pragma omp parallel for\n   for (size_t i = 0; i < image.size(); ++i) {\n     bins[image[i]]++;\n   }\n }",
            "size_t i;\n    // your code here\n    //std::vector<int>::const_iterator it = image.begin();\n    //bins[image[0]]++;\n    //bins[image[1]]++;\n    //bins[image[2]]++;\n    //bins[image[3]]++;\n    //bins[image[4]]++;\n    //bins[image[5]]++;\n    //bins[image[6]]++;\n    //bins[image[7]]++;\n    //bins[image[8]]++;\n    //bins[image[9]]++;\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n    //for (i = 0; i < image.size(); i++){\n    //    bins[image[i]]++;\n    //}\n}",
            "// Your code here\n   // Hint: bins[i] = count(image, image + image.size(), i)\n   for(size_t i = 0; i < bins.size(); i++){\n      bins[i] = 0;\n   }\n   for(int i = 0; i < image.size(); i++){\n      bins[image[i]]++;\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// Your code here\n\t#pragma omp parallel for\n\tfor(int i=0;i<image.size();i++){\n\t\tbins[image[i]]++;\n\t}\n}",
            "// You may need this for the outer loop\n  // omp_set_dynamic(0);\n  // omp_set_num_threads(NUM_THREADS);\n  // Your code here\n  size_t num_pixels = image.size();\n  bins.fill(0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < num_pixels; i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: Your code here\n\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "#pragma omp parallel for schedule(static, 1)\n   for(size_t i=0; i < image.size(); i++){\n     int val = image[i];\n     bins[val]++;\n   }\n }",
            "// TODO: your code here\n  omp_set_num_threads(8);\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); i++)\n    bins[image[i]]++;\n  return;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n\n    // TODO: replace this with the correct code from the exercise.\n    // Hint: You may want to use a parallel reduction here\n    // #pragma omp parallel for reduction(+:bins[i])\n    // for (size_t i = 0; i < image.size(); i++) {\n    //     bins[image[i]]++;\n    // }\n}",
            "bins.fill(0);\n    #pragma omp parallel\n    {\n        size_t tid = omp_get_thread_num();\n        for (size_t i = tid; i < image.size(); i += 4) {\n            bins[image[i]]++;\n        }\n    }\n}",
            "/* TODO: Fill in this function. */\n   bins.fill(0);\n\n#pragma omp parallel for reduction(+:bins)\n   for(auto i = 0; i < image.size(); ++i){\n      bins[image[i]] += 1;\n   }\n\n\n}",
            "bins.fill(0);\n \t#pragma omp parallel for\n \tfor (size_t i = 0; i < image.size(); ++i) {\n \t\t++bins[image[i]];\n \t}\n }",
            "//TODO: Write your code here\n}",
            "size_t i = 0;\n#pragma omp parallel for\n     for (i = 0; i < image.size(); ++i) {\n         bins[image[i]] += 1;\n     }\n }",
            "// YOUR CODE HERE\n   return;\n}",
            "// Your code here\n\n   // TODO: your code here\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   \n   // Your code here.\n }",
            "// YOUR CODE HERE\n  size_t nPixels = image.size();\n  std::array<size_t, 256> counts = {0};\n  for (auto x : image) {\n    counts[x]++;\n  }\n  bins = counts;\n}",
            "#pragma omp parallel for\n   for (int i=0; i<256; i++)\n     bins[i] = 0;\n\n   for (int i=0; i<(int)image.size(); i++)\n     bins[image[i]]++;\n\n   #pragma omp parallel for\n   for (int i=0; i<256; i++)\n     bins[i] = (int)bins[i];\n }",
            "#  pragma omp parallel for\n   for (int pixel : image) {\n      bins[pixel] += 1;\n   }\n}",
            "// TODO: Implement this function\n}",
            "bins.fill(0);\n    int num_threads = 4;\n    #pragma omp parallel for num_threads(num_threads) schedule(static)\n    for (auto pixel = 0; pixel < image.size(); ++pixel) {\n        bins[image[pixel]]++;\n    }\n}",
            "// Your code here...\n}",
            "// YOUR CODE HERE\n    // Use omp_set_num_threads(nthreads) to set the number of threads.\n    // YOUR CODE HERE\n}",
            "/* YOUR CODE HERE */\n    #pragma omp parallel for\n    for (size_t i=0; i<image.size(); i++){\n        bins[image[i]] += 1;\n    }\n}",
            "for (int const& value : image) {\n     bins[value] += 1;\n   }\n}",
            "// YOUR CODE HERE\n    // TODO: OpenMP parallel for\n    // TODO: OpenMP atomic\n}",
            "/* TODO: YOUR CODE HERE */\n}",
            "// TODO\n}",
            "// TODO: Compute the counts in parallel using OpenMP\n    // Hint: #pragma omp parallel for reduction(+:bins[i])\n    // Hint: use the length of the image array as the size of the\n    //       loop bounds.\n\n\tint i, j;\n\tbins.fill(0);\n\n\tomp_set_num_threads(omp_get_max_threads());\n\t#pragma omp parallel for\n\tfor (i = 0; i < image.size(); i++) {\n\t\tbins[image[i]] += 1;\n\t}\n}",
            "bins.fill(0);\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "bins.fill(0);\n  #pragma omp parallel for \n  for (size_t i = 0; i < image.size(); i++)\n    bins[image[i]] += 1;\n}",
            "int num_threads = 0;\n#pragma omp parallel\n     {\n#pragma omp single\n         num_threads = omp_get_num_threads();\n     }\n\n     std::vector<int> pixels_per_thread(num_threads);\n\n     // Count how many pixels in the image fall in each bin\n     // NOTE: This is O(N) if there are `num_threads` threads\n     for (auto val : image) {\n         pixels_per_thread[val]++;\n     }\n\n     // Sum all the pixels from each thread\n     for (int i = 1; i < num_threads; i++) {\n         pixels_per_thread[0] += pixels_per_thread[i];\n     }\n\n     // Store the results\n     for (int i = 1; i < 256; i++) {\n         bins[i] = pixels_per_thread[i];\n     }\n }",
            "#pragma omp parallel for reduction(+:bins[:])\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// Your code here...\n}",
            "#pragma omp parallel for schedule(dynamic)\n \tfor (int i = 0; i < image.size(); i++) {\n \t\tbins[image[i]]++;\n \t}\n }",
            "// Your code goes here...\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// your code here\n\tbins.fill(0);\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp for schedule(static)\n\t\tfor (size_t i = 0; i < image.size(); i++) {\n\t\t\tbins[image[i]]++;\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n   for (int i=0; i<image.size(); ++i) {\n     bins[image[i]] += 1;\n   }\n }",
            "#pragma omp parallel for\n    for(int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for(int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "/* TODO */\n}",
            "int N = image.size();\n    #pragma omp parallel for\n    for(int i = 0; i < N; ++i) {\n        bins[image[i]]++;\n    }\n}",
            "size_t i;\n    #pragma omp parallel for\n    for (i = 0; i < image.size(); i++) {\n        #pragma omp atomic\n        bins[image[i]]++;\n    }\n}",
            "size_t threads = std::thread::hardware_concurrency();\n#pragma omp parallel num_threads(threads)\n   {\n     //std::cout << \"Hello \" << omp_get_thread_num() << std::endl;\n     std::array<size_t, 256> partialBins = {};\n     std::fill(partialBins.begin(), partialBins.end(), 0);\n\n     int from = omp_get_thread_num();\n     int to = omp_get_num_threads();\n     for (int i = from; i < image.size(); i += to) {\n       partialBins[image[i]] += 1;\n     }\n#pragma omp critical\n     {\n       for (size_t i = 0; i < partialBins.size(); i++) {\n         bins[i] += partialBins[i];\n       }\n     }\n   }\n }"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (auto p : image)\n     ++bins[p];\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "for (size_t i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "for(int i=0; i<image.size(); i++) {\n \t\tbins[image[i]]++;\n \t}\n }",
            "// Fill in your solution here\n   bins[image[0]]++;\n   for (int i = 1; i < image.size(); ++i)\n     ++bins[image[i]];\n }",
            "for (auto i : image) {\n     ++bins[i];\n   }\n }",
            "// Write your code here...\n }",
            "for (int i : image) {\n     bins[i]++;\n   }\n }",
            "bins.fill(0);\n   for (int i : image) {\n     ++bins[i];\n   }\n }",
            "for(auto const& pixel: image) {\n    bins[pixel]++;\n  }\n}",
            "// your code goes here\n }",
            "int i = 0;\n   // YOUR CODE HERE\n\n   for (i = 0; i < image.size(); i++)\n     bins[image[i]]++;\n}",
            "for (auto intensity : image) {\n         bins[intensity]++;\n     }\n }",
            "int c = 0;\n  for (auto& i : image) {\n    bins[c] += 1;\n    c++;\n  }\n}",
            "// TODO: Fill in this method\n}",
            "for(auto const &pix : image) {\n \t\tbins[pix]++;\n \t}\n }",
            "for(auto const &pixel: image) {\n    ++bins[pixel];\n  }\n}",
            "for (size_t x = 0; x < image.size(); x++) {\n        bins[image[x]]++;\n    }\n}",
            "bins = std::array<size_t, 256>{};\n  for (auto i : image) {\n    bins[i]++;\n  }\n}",
            "// Insert your code here.\n}",
            "for (auto const& pixel : image)\n\t\t++bins[pixel];\n}",
            "// TODO: Your code here.\n}",
            "for (int i = 0; i < 256; i++) {\n        for (int j = 0; j < 256; j++) {\n            if (image[j] == i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "int pixel_count = 0;\n   for (auto value : image) {\n     pixel_count++;\n     bins[value]++;\n   }\n }",
            "for (auto p: image) ++bins[p];\n }",
            "/* YOUR CODE HERE */\n    /*\n    for (int value: image) {\n        ++bins[value];\n    }\n    */\n}",
            "for(auto const& x : image) {\n        bins[x] += 1;\n    }\n}",
            "for (int v : image)\n     ++bins[v];\n }",
            "for (auto pixel : image) {\n    ++bins[pixel];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: implement this function.\n }",
            "for (size_t i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "// TODO:\n   int c = 0;\n   size_t size = image.size();\n   for(int i = 0; i < size; i++){\n       bins[image[i]]++;\n   }\n}",
            "// TODO: implement this function\n   // store the result in the `bins` vector\n   for(size_t i=0; i<image.size(); ++i){\n     ++bins[image[i]];\n   }\n}",
            "// Initialize the bins array to 0\n     for (int i=0; i<256; i++) {\n         bins[i] = 0;\n     }\n     \n     for (int i=0; i<image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "// TODO: Fill this in!\n}",
            "for (auto x : image) {\n        ++bins[x];\n    }\n}",
            "for(const auto& i: image) {\n     bins[i] += 1;\n   }\n }",
            "bins.fill(0);\n   // TODO: fill this in.\n   for(auto i = 0; i < image.size(); i++) {\n       bins[image[i]]++;\n   }\n }",
            "bins.fill(0);\n    for (int const p : image) {\n        bins[p]++;\n    }\n}",
            "int n = image.size();\n   for (int i = 0; i < n; ++i) {\n     ++bins[image[i]];\n   }\n }",
            "// TODO\n }",
            "for (auto intensity : image) {\n    bins[intensity] += 1;\n  }\n}",
            "for (int pixel : image) {\n    ++bins[pixel];\n  }\n}",
            "for (auto &&intensity : image) {\n     bins[intensity]++;\n   }\n }",
            "bins.fill(0);\n\tfor (int intensity : image) {\n\t\tbins[intensity]++;\n\t}\n}",
            "// TODO(student): implement this function\n   throw std::logic_error(\"Not implemented yet\");\n }",
            "// TODO: Implement\n    for (int i : image) {\n        ++bins[i];\n    }\n}",
            "for (auto pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "std::array<size_t, 256> bins2;\n\tstd::fill(bins2.begin(), bins2.end(), 0);\n\tfor (const int x : image) {\n\t\tbins2[x] += 1;\n\t}\n\tbins = bins2;\n}",
            "// initialize vector to zero\n   std::fill(bins.begin(), bins.end(), 0);\n\n   // iterate over elements of the vector\n   for (auto i: image) {\n     // increase the value of the element by 1\n     bins[i]++;\n   }\n }",
            "// Initialize the bins array to all zeroes.\n   for (size_t i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n   \n   // Count the number of pixels in the image with each grayscale intensity.\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n}",
            "int size = image.size();\n\tfor (int i = 0; i < size; i++) {\n\t\tint pixel = image[i];\n\t\tbins[pixel]++;\n\t}\n}",
            "// TODO\n }",
            "for (auto const& p : image) {\n     ++bins[p];\n   }\n }",
            "std::array<size_t, 256> bins2 = {};\n    for(int i=0; i<image.size(); i++){\n        bins2[image[i]]++;\n    }\n    bins = bins2;\n}",
            "for (int i : image) {\n    ++bins[i];\n  }\n}",
            "for (auto pixel: image) {\n     bins[pixel]++;\n   }\n }",
            "// YOUR CODE HERE\n   // Please do not remove this line.\n   return;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   for (auto pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "// YOUR CODE HERE\n  }",
            "for (int i = 0; i < image.size(); ++i) {\n      bins[image[i]]++;\n   }\n}",
            "for (const auto& value : image) {\n     bins[value]++;\n   }\n }",
            "// YOUR CODE HERE\n }",
            "// TODO: Your code here.\n  std::array<size_t, 256> count;\n  for (int i : image) {\n    count[i]++;\n  }\n  bins = count;\n }",
            "size_t size = image.size();\n   for (size_t i = 0; i < size; i++) {\n     bins[image[i]]++;\n   }\n\n }",
            "// TODO: implement\n    // Hint: you can use a simple loop over the pixels of the image.\n    // The function `int grayLevel(int r, int g, int b)` from lab2 could be useful.\n    for (auto const& i : image) {\n        bins[grayLevel(i)]++;\n    }\n}",
            "for (auto pix : image) {\n         bins[pix]++;\n     }\n }",
            "//TODO: implement this method\n   // Hint: Use a for loop over the elements of image.\n   //       You'll need to increment `bins[image[i]]` by one.\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]] += 1;\n   }\n}",
            "for (auto const& element : image) {\n     bins[element]++;\n   }\n}",
            "/* TODO */\n    for (int i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n    return;\n}",
            "//...\n }",
            "std::array<size_t, 256> histogram = {0};\n  for (auto const& value : image) {\n    histogram[value]++;\n  }\n  bins = histogram;\n}",
            "for (auto i : image) {\n     bins[i]++;\n   }\n }",
            "int max = 0;\n \tint min = 255;\n \tfor (int i : image) {\n \t\tif (i > max)\n \t\t\tmax = i;\n \t\tif (i < min)\n \t\t\tmin = i;\n \t}\n\n \tfor (int i = min; i <= max; i++) {\n \t\tbins[i]++;\n \t}\n }",
            "// TODO\n }",
            "// YOUR CODE HERE\n   bins[image[0]]++;\n   for (int i = 1; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (auto pixel:image) {\n     ++bins[pixel];\n   }\n }",
            "// Your code here\n   for (const auto& i : image) {\n      ++bins[i];\n   }\n }",
            "int const width = 256; // we have 256 grayscale values\n\t// Initialize the bins array to 0\n\tstd::fill(bins.begin(), bins.end(), 0);\n\t\n\t// loop over each intensity value (0-255) and increment\n\t// the corresponding bin.\n\tfor (int intensity = 0; intensity < width; intensity++) {\n\t\tbins[intensity] = std::count(image.begin(), image.end(), intensity);\n\t}\n }",
            "for (auto i : image) {\n    ++bins[i];\n  }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "bins = std::array<size_t, 256>();\n\n\tfor (const auto& val : image)\n\t\t++bins[val];\n}",
            "// TODO: Implement this function\n    for (int x = 0; x < image.size(); x++) {\n        bins[image[x]]++;\n    }\n}",
            "for (auto const &pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "// YOUR CODE HERE\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (auto const& pixel: image) {\n     bins[pixel]++;\n   }\n }",
            "// TODO(student): implement\n   bins = std::array<size_t, 256>();\n   for (int i = 0; i < image.size(); i++) {\n     bins[image[i]] += 1;\n   }\n }",
            "for(int i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n\n   for(int i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto pixel : image) {\n        ++bins[pixel];\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "// Fill in the histogram data with the appropriate number of pixels\n\t// with each intensity.\n\tfor (auto const& i: image)\n\t{\n\t\tbins.at(i)++;\n\t}\n}",
            "for (auto pixel : image)\n        bins[pixel]++;\n}",
            "// YOUR CODE HERE\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (auto p : image) {\n     bins[p]++;\n   }\n }",
            "// TODO: Your code goes here\n}",
            "for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "// TODO: Your code here.\n }",
            "for(auto value: image){\n     bins[value]++;\n   }\n }",
            "int count;\n     for (size_t i = 0; i < image.size(); i++) {\n         count = image[i];\n         bins[count]++;\n     }\n }",
            "for (int val: image) {\n        ++bins[val];\n    }\n}",
            "// TODO: Your code here.\n    for (auto &a : image) {\n        bins[a]++;\n    }\n}",
            "for (size_t pixel : image) {\n    bins[pixel]++;\n  }\n}",
            "for (auto c : image) {\n     ++bins[c];\n   }\n }",
            "for (auto const& e : image) {\n     bins[e]++;\n   }\n }",
            "// TODO: fill this in\n}",
            "for(int i = 0; i < 256; i++) {\n     bins[i] = std::count(image.begin(), image.end(), i);\n   }\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "for (auto it : image) {\n        bins[it]++;\n    }\n}",
            "// TODO: replace this with your own code\n}",
            "// TODO: Your code goes here.\n \t//for (const auto &v : image) {\n \t\t//bins[v] = 1;\n //\t}\n\tstd::transform(image.begin(), image.end(), bins.begin(), 1, std::plus<size_t>());\n}",
            "// Initialize bins.\n  for (auto& bin : bins) {\n    bin = 0;\n  }\n\n  // Your code here.\n}",
            "for(auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "// YOUR CODE HERE\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n         bins[image[i]]++;\n     }\n }",
            "// Implement this method.\n }",
            "for (auto pixel : image) {\n     bins[pixel] += 1;\n }\n}",
            "for (auto x : image) {\n    bins[x]++;\n  }\n}",
            "// TODO:\n     for(int i = 0; i < image.size(); i++) {\n         ++bins[image[i]];\n     }\n }",
            "for(int p : image) {\n        bins[p]++;\n    }\n}",
            "for (int i : image) {\n     bins[i]++;\n   }\n }",
            "for (auto pixel: image) {\n    bins[pixel] += 1;\n  }\n}",
            "// Your code here\n  bins[image[0]]++;\n  for(size_t i=1; i<image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// Your code goes here!\n\n   // To check your code, run `bin/test_bin counts`.\n }",
            "for (auto p : image)\n    bins[p]++;\n}",
            "// Implement this function\n\t// 1. count how many pixels are in each gray level\n\t// 2. store the results in `bins`\n\t\n\tfor (int i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "// TODO: Your code here\n }",
            "// TODO: Implement this function\n    size_t length = image.size();\n    size_t count = 0;\n    int pixel = 0;\n    for (size_t i = 0; i < length; i++) {\n        pixel = image[i];\n        count = bins[pixel];\n        count++;\n        bins[pixel] = count;\n    }\n}",
            "// Your code here...\n   for (auto &&x : image) {\n     bins[x] += 1;\n   }\n }",
            "bins.fill(0);\n    for (size_t i=0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// Your code here.\n}",
            "// Your code goes here...\n}",
            "for (auto i : image) {\n     ++bins[i];\n   }\n }",
            "// write your code here\n  int j = 0;\n  for (auto i = 0; i < image.size(); i++) {\n    bins[image[i]] += 1;\n    j = image[i];\n  }\n  return;\n}",
            "for (auto i = 0; i < image.size(); i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "for (int i = 0; i < image.size(); i++) {\n    ++bins[image[i]];\n  }\n}",
            "for (auto& count : bins) {\n     count = 0;\n   }\n   for (auto& pixel : image) {\n     bins[pixel] = bins[pixel] + 1;\n   }\n }",
            "// Your code here\n    int n = image.size();\n    bins.fill(0);\n    for (int i=0; i<n; ++i){\n        ++bins[image[i]];\n    }\n}",
            "for (auto const& i : image) {\n    bins[i]++;\n  }\n}",
            "// Implement the function\n    size_t size = image.size();\n    for(size_t i = 0; i < size; i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "for(auto c : image) {\n         bins[c] += 1;\n     }\n }",
            "// Add your code here\n   // bins[i] stores the number of pixels that have grayscale intensity i.\n   // Hint:\n   // - use for-loop.\n   for (int i = 0; i < 256; i++) {\n     int count = 0;\n     for (size_t j = 0; j < image.size(); j++) {\n       if (image[j] == i) {\n         count++;\n       }\n     }\n     bins[i] = count;\n   }\n }",
            "// Initialize the bins array with zeros.\n    for (size_t i = 0; i < 256; i++) {\n        bins[i] = 0;\n    }\n\n    // Fill in the array with the counts.\n    for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (size_t i = 0; i < image.size(); ++i) {\n    bins[image[i]] += 1;\n  }\n}",
            "for(size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Your code here...\n}",
            "for(auto i = 0; i < 256; i++){\n     bins[i] = 0;\n   }\n   for(auto i = 0; i < image.size(); i++){\n     bins[image[i]] += 1;\n   }\n }",
            "for (auto &bin : bins) bin = 0;\n   for (auto const& pixel : image) bins[pixel]++;\n }",
            "bins.fill(0);\n   for (auto val : image) {\n     bins[val]++;\n   }\n }",
            "// TODO: Implement this function!\n   int i;\n   for (i = 0; i < image.size(); ++i)\n   {\n       ++bins[image[i]];\n   }\n}",
            "for (int i : image) {\n        bins[i]++;\n    }\n}",
            "bins.fill(0);\n\n   for (auto pixel : image) {\n      ++bins[pixel];\n   }\n}",
            "std::for_each(image.begin(), image.end(), [&bins](int x) { bins[x]++; });\n}",
            "bins.fill(0);\n    for (int val : image)\n        bins[val]++;\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   for (size_t i = 0; i < image.size(); ++i) {\n     ++bins[image[i]];\n   }\n }",
            "for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "// TODO\n }",
            "for (auto c : image) {\n    bins[c] += 1;\n  }\n}",
            "// *** YOUR CODE HERE ***\n}",
            "for (size_t i = 0; i < image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "for (auto &i : image) {\n\t\tbins[i] += 1;\n\t}\n}",
            "// Your code goes here.\n }",
            "bins.fill(0);\n   for (int const& val : image) ++bins[val];\n }",
            "for (auto pixel : image) {\n    bins[pixel]++;\n  }\n}",
            "for (int i : image) {\n     bins[i]++;\n   }\n }",
            "// Write your code here.\n }",
            "for (int i : image) {\n     bins[i] += 1;\n   }\n }",
            "// for each intensity, add the number of pixels in the vector\n   for (auto p : image) {\n     ++bins[p];\n   }\n }",
            "for(auto pixel : image) {\n         bins[pixel]++;\n     }\n }",
            "// TO DO...\n }",
            "for (int i : image) {\n    bins[i]++;\n  }\n }",
            "for (int i=0; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   // Your code here\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (auto p: image) {\n     bins[p] += 1;\n   }\n }",
            "// your code here\n}",
            "// TODO: YOUR CODE HERE\n }",
            "int num = image.size();\n  for (int i = 0; i < num; i++) {\n    bins[image[i]]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for(int pix : image) {\n     bins[pix]++;\n   }\n }",
            "bins.fill(0);\n\n    for(auto pixel : image) {\n        bins[pixel]++;\n    }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// Write your code here.\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n }",
            "for (int x: image) {\n     ++bins[x];\n   }\n }",
            "// Your code here\n }",
            "// YOUR CODE HERE\n}",
            "for (auto pixel : image) {\n     bins[pixel]++;\n   }\n }",
            "//...\n }",
            "for (auto intensity : image) {\n        ++bins[intensity];\n    }\n}",
            "// TODO: Implement this function.\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (int i = 0; i < image.size(); ++i) {\n     bins.at(image.at(i))++;\n   }\n }",
            "for(auto x:image)\n        bins[x]++;\n}",
            "// YOUR CODE HERE\n    int value = 0;\n\n    for(auto i: image)\n        ++bins[i];\n\n}",
            "for (auto const& p: image) {\n        ++bins[p];\n    }\n}",
            "// Your code here\n }",
            "// TODO: Implement this function\n   for (int i = 0; i < image.size(); i++) {\n      bins[image[i]]++;\n   }\n}",
            "for (auto x : image) {\n     ++bins[x];\n   }\n }",
            "size_t const size = image.size();\n   bins.fill(0);\n   for(size_t i=0; i<size; i++) {\n     ++bins[image[i]];\n   }\n }",
            "bins = std::array<size_t, 256>{};\n\n   for (auto const& i : image) {\n     bins[i]++;\n   }\n }",
            "bins.fill(0);\n    for (int v : image) {\n        ++bins[v];\n    }\n}",
            "// Fill `bins` array with zeros\n    for (auto &bin : bins) {\n        bin = 0;\n    }\n    // Count pixels for each intensity\n    for (auto const& intensity : image) {\n        bins[intensity]++;\n    }\n}",
            "bins.fill(0);\n   \n   for (auto pixel : image) {\n     ++bins[pixel];\n   }\n }"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "#ifdef AMD_HIP\n    if (threadIdx.x == 0) {\n        int idx = blockIdx.x * blockDim.x + threadIdx.x;\n        if (idx < N) {\n            bins[image[idx]]++;\n        }\n    }\n#endif\n}",
            "size_t binId = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t localSum = 0;\n    while (binId < 256) {\n        localSum += __popc(image[binId * N] | (image[binId * N + 1] << 8));\n        binId += blockDim.x * gridDim.x;\n    }\n    atomicAdd(&bins[threadIdx.x], localSum);\n}",
            "// YOUR CODE HERE\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    int bin = 0;\n    if (tid < N)\n        bin = image[tid];\n    __syncthreads();\n    \n    atomicAdd(&bins[bin], 1);\n}",
            "int threadID = blockDim.x*blockIdx.x + threadIdx.x;\n    if (threadID < N) {\n        int intensity = image[threadID];\n        atomicAdd(&bins[intensity], 1);\n    }\n}",
            "int x = blockDim.x*blockIdx.x + threadIdx.x;\n  int stride = blockDim.x*gridDim.x;\n\n  int gray;\n  for (size_t i = x; i < N; i += stride) {\n    gray = image[i];\n    atomicAdd(&bins[gray], 1);\n  }\n}",
            "// TODO\n}",
            "int myId = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (myId < N) {\n        atomicAdd(&bins[image[myId]], 1);\n    }\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n\n}",
            "// TODO: Fill in the kernel code.\n  // HINT: bins is a host-side array. Use atomicAdd(bins + pixelIntensity, 1) to add to pixel intensity count in a thread-safe way.\n  for(int i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n    atomicAdd(bins + image[i], 1);\n  }\n}",
            "// TODO: your code here\n  const unsigned int id = threadIdx.x + blockDim.x * blockIdx.x;\n  if (id >= N) return;\n  const int pixel = image[id];\n  atomicAdd(&bins[pixel], 1);\n}",
            "// TODO: Your code here\n}",
            "size_t tidx = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t tidy = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t tidx_global = blockDim.x * gridDim.x * blockIdx.x + threadIdx.x;\n  size_t tidy_global = blockDim.y * gridDim.y * blockIdx.y + threadIdx.y;\n\n  if (tidx < N) {\n    // Convert to grayscale\n    uint8_t gray = image[tidx];\n    // Increment the count for this pixel\n    atomicAdd(&bins[gray], 1);\n  }\n}",
            "// TODO: YOUR CODE HERE\n    __shared__ size_t sh_bins[256];\n    if (threadIdx.x < 256) {\n        sh_bins[threadIdx.x] = 0;\n    }\n    __syncthreads();\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        atomicAdd(&sh_bins[image[i]], 1);\n    }\n    __syncthreads();\n    if (threadIdx.x < 256) {\n        atomicAdd(&bins[threadIdx.x], sh_bins[threadIdx.x]);\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here\n  __syncthreads();\n}",
            "// TODO: Fill in the kernel code to count the number of pixels\n  // with each intensity in the image.\n  // Remember to call `hipDeviceSynchronize` after `hipLaunchKernelGGL`.\n\n  // The HIP kernel API takes a void function pointer, so we can use a __device__ function.\n  // This will only be run on the GPU.\n  __device__ void countIntensity(const int *image, size_t N, size_t bins[256]) {\n      // TODO: Implement the function.\n  }\n\n  // We need to call the function on the device.\n  countIntensity(image, N, bins);\n  hipDeviceSynchronize();\n}",
            "size_t local_id = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t global_id = blockDim.x * gridDim.x + threadIdx.x;\n  size_t block_start = global_id * 256;\n  size_t block_end = (global_id+1) * 256;\n  size_t i = 0;\n  \n  if(local_id < 256 && block_start < N) {\n    bins[local_id] = 0;\n    for(i=block_start; i < block_end && i < N; ++i)\n      ++bins[local_id];\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  int idx = tid * 8;\n  if (idx < N) {\n    int value = image[idx];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 1];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 2];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 3];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 4];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 5];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 6];\n    atomicAdd(&bins[value], 1);\n    value = image[idx + 7];\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "int tid = hipThreadIdx_x;\n  int i = tid;\n  int bin = 0;\n  \n  while (i < N) {\n    bin = image[i];\n    atomicAdd(&bins[bin], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "const size_t block_size = 512;\n  __shared__ size_t partial_sums[block_size];\n  const int thread_id = threadIdx.x + threadIdx.y*block_size;\n\n  // Each block sums a subset of the bins.\n  // This is the only __syncthreads() before the reduction.\n  __syncthreads();\n  size_t sum = 0;\n  for (int i = thread_id; i < N; i += block_size*gridDim.x) {\n    sum += image[i];\n  }\n  partial_sums[thread_id] = sum;\n  __syncthreads();\n\n  // Add the partial sums.\n  // This is the only __syncthreads() after the reduction.\n  for (int stride = block_size/2; stride > 0; stride /= 2) {\n    if (thread_id < stride) {\n      partial_sums[thread_id] += partial_sums[thread_id + stride];\n    }\n    __syncthreads();\n  }\n\n  // Store the result in the global histogram.\n  if (thread_id == 0) {\n    bins[blockIdx.x] = partial_sums[0];\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "// YOUR CODE HERE\n\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "/* TODO: Your code goes here */\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t thx = hipBlockIdx_x * hipBlockDim_x + tid;\n   int v = image[thx];\n   if (thx < N) {\n      atomicAdd(&bins[v], 1);\n   }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    while (index < N) {\n        atomicAdd(&(bins[image[index]]), 1);\n        index += stride;\n    }\n}",
            "// TODO\n}",
            "__shared__ size_t s_bins[256];\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    s_bins[threadIdx.x] = 0;\n    while (i < N) {\n        s_bins[image[i]] += 1;\n        i += blockDim.x * gridDim.x;\n    }\n    __syncthreads();\n    atomicAdd(bins + threadIdx.x, s_bins[threadIdx.x]);\n}",
            "// 1. Your code goes here\n\t// Hints:\n\t// 1. You may want to use hipThreadIdx_x to get the id of a thread in a block\n\t// 2. The value in the image can be calculated with hipBlockIdx_x * N + hipThreadIdx_x\n\t// 3. The grayscale intensity of the current pixel is image[hipBlockIdx_x * N + hipThreadIdx_x]\n\t// 4. You may want to use atomicAdd to increment the number of pixels for a grayscale intensity.\n\t//    Remember that the bin is bin[image[hipBlockIdx_x * N + hipThreadIdx_x]].\n\t// 5. You may want to use __syncthreads() after each block to synchronize all threads in a block.\n\t// 6. You may want to use a for-loop in this kernel.\n\t// 7. You may want to use hipThreadIdx_x < N to check if the thread id is less than N.\n\t\n\t// for(int i=hipThreadIdx_x;i<N;i+=hipBlockDim_x)\n\t// {\n\t// \tatomicAdd(&bins[image[hipBlockIdx_x * N + hipThreadIdx_x]], 1);\n\t// }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  int i = tid / 32;\n  int j = tid % 32;\n\n  if (i >= N) return;\n  if (j >= 256) return;\n\n  int histIdx = image[i];\n  atomicAdd(&bins[histIdx * 32 + j], 1);\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(&bins[image[i]], 1);\n\t}\n}",
            "// TODO\n}",
            "// TODO: Your implementation here.\n}",
            "int id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\tif (id < N) {\n\t\tint intensity = image[id];\n\t\tatomicAdd(&(bins[intensity]), 1);\n\t}\n}",
            "// TODO: your code goes here\n}",
            "int gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if(gid < N) {\n      ++bins[image[gid]];\n   }\n}",
            "// TODO: Your code here\n    // Your code should be able to count the number of pixels in image with each grayscale intensity.\n    // Each thread will process one grayscale intensity value.\n    // The vector image is a grayscale image with values 0-255.\n    // The vector bins is filled with the results of the kernel.\n}",
            "// TODO: add your code here\n}",
            "// Insert your code here.\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    int pixel = image[idx];\n    atomicAdd(&bins[pixel], 1);\n  }\n}",
            "// TODO: Fill in\n}",
            "// Your code goes here\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t i = tid / 256;\n  if (tid < N) {\n    bins[image[i]] += 1;\n  }\n}",
            "int globalThreadId = blockIdx.x*blockDim.x + threadIdx.x;\n  int localThreadId = threadIdx.x;\n\n  __shared__ int localCount[256];\n\n  for (int i=0; i<256; i++) {\n    localCount[i] = 0;\n  }\n\n  // Your code here\n\n  __syncthreads();\n\n  for (int i=0; i<256; i++) {\n    atomicAdd(&bins[i], localCount[i]);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  size_t bin = image[tid];\n  if (tid < N)\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO: Your code here\n  return;\n}",
            "// YOUR CODE HERE\n  // Launch the kernel with at least N threads\n}",
            "// Your code here\n  \n}",
            "// TODO: replace with your own code\n  // The kernel is launched with at least N threads\n  // TODO: add a kernel\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n\tint bin = 255;\n\tif (i < N) {\n\t\tbin = image[i];\n\t\tatomicAdd(&bins[bin], 1);\n\t}\n\t__syncthreads();\n}",
            "// Insert your implementation here.\n}",
            "int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadIdx < N) {\n    atomicAdd(&bins[image[threadIdx]], 1);\n  }\n}",
            "unsigned int ix = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  unsigned int iy = hipBlockIdx_y * hipBlockDim_y + hipThreadIdx_y;\n\n  if (ix < N && iy < N) {\n    const unsigned int pixel = image[iy*N + ix];\n    atomicAdd(&bins[pixel], 1);\n  }\n}",
            "const int tid = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n    const int blk = hipBlockDim_x * hipBlockIdx_x;\n\n    __shared__ size_t s_bins[256];\n\n    for (int i = 0; i < 256; i++) s_bins[i] = 0;\n\n    for (int idx = tid; idx < N; idx += hipBlockDim_x * hipGridDim_x) {\n        s_bins[image[idx]]++;\n    }\n\n    __syncthreads();\n\n    for (int idx = blk; idx < 256; idx += hipBlockDim_x * hipGridDim_x) {\n        atomicAdd(&bins[idx], s_bins[idx]);\n    }\n}",
            "/* TODO: Your code goes here */\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int bin_idx = image[tid];\n  __atomicAdd(&bins[bin_idx], 1);\n}",
            "__shared__ size_t s_bins[256];\n  int tid = threadIdx.x;\n  int i = blockIdx.x;\n  if (tid == 0) {\n    s_bins[0] = 0;\n  }\n  __syncthreads();\n  if (i < N) {\n    atomicAdd(&s_bins[image[i]], 1);\n  }\n  __syncthreads();\n  if (tid == 0) {\n    for (int i = 1; i < 256; i++) {\n      atomicAdd(&bins[i], s_bins[i]);\n    }\n  }\n}",
            "// TODO: your code goes here\n}",
            "// TODO: Fill in code to count pixels of each grayscale intensity\n}",
            "// YOUR CODE HERE\n  // TODO(lab): Your code goes here\n}",
            "int index = threadIdx.x;\n\tsize_t[256] bins = 0;\n\t// TODO: Implement the kernel.\n\t// The kernels must be launched with at least N threads.\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n    //TODO\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    int intensity = image[tid];\n    atomicAdd(&bins[intensity], 1);\n  }\n}",
            "unsigned int i = blockIdx.x*blockDim.x+threadIdx.x;\n   if(i < N)\n      atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO: Complete the kernel function for counting the pixels in each grayscale intensity\n    // Hint:\n    // - The grayscale intensity is given by `image[tid]`\n    // - The pixel count for each intensity is given by `bins[image[tid]]`\n    // - If `tid` is greater than `N`, the pixel count for grayscale intensity 255 is `bins[255]`.\n    // - This kernel function is launched with at least N threads.\n    // - Do NOT use an atomic operation to increment the count for a grayscale intensity.\n\n    // TODO: Define the grayscale intensity of the first thread in the block\n    int gray = 0;\n    // TODO: Compute the grayscale intensity of the first thread in the block\n    // Hint:\n    // - The grayscale intensity is given by `image[tid]`\n\n    // TODO: Loop over all threads in the block\n    // Hint:\n    // - The `tid` of the first thread is 0\n    // - The `tid` of the last thread is `blockDim.x - 1`\n    // - `tid` is the global ID of the thread in the block\n    for (int i = 0; i < N; i++) {\n        // TODO: Increment the pixel count for the grayscale intensity of the thread\n        // Hint:\n        // - `tid` is the global ID of the thread in the block\n        // - `gray` is the grayscale intensity of the thread\n        atomicAdd(&bins[gray], 1);\n    }\n}",
            "// TODO\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N)\n        atomicAdd(&bins[image[i]], 1);\n}",
            "// Your code goes here\n}",
            "unsigned int idx = threadIdx.x;\n  unsigned int val = image[idx];\n  if (idx < N) {\n    atomicAdd(&bins[val], 1);\n  }\n}",
            "size_t ix = threadIdx.x + blockIdx.x * blockDim.x;\n  if (ix >= N) return;\n  atomicAdd(&bins[image[ix]], 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\tint my_hist[256];\n\tfor (int i = 0; i < 256; i++) {\n\t\tmy_hist[i] = 0;\n\t}\n\tif (tid < N) {\n\t\tatomicAdd(&my_hist[image[tid]], 1);\n\t}\n\t__syncthreads();\n\n\tif (threadIdx.x == 0) {\n\t\tfor (int i = 0; i < 256; i++) {\n\t\t\tatomicAdd(&bins[i], my_hist[i]);\n\t\t}\n\t}\n}",
            "// YOUR CODE GOES HERE\n}",
            "unsigned int x = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int y = blockIdx.y * blockDim.y + threadIdx.y;\n  if (x >= N || y >= N) {\n    return;\n  }\n  unsigned int idx = y * N + x;\n  atomicAdd(&(bins[image[idx]]), 1);\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if(tid < N) {\n    int i = image[tid];\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "int gray = blockDim.x * blockIdx.x + threadIdx.x;\n  int local = 0;\n\n  for (int idx = blockIdx.x; idx < N; idx += gridDim.x) {\n    if (gray == image[idx]) {\n      ++local;\n    }\n  }\n\n  atomicAdd(&bins[gray], local);\n}",
            "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (gid < N) {\n        unsigned char intensity = image[gid];\n        atomicAdd(&bins[intensity], 1);\n    }\n}",
            "// Compute the global thread index\n\tint globalId = blockDim.x * blockIdx.x + threadIdx.x;\n\tif(globalId < N) {\n\t\t// Compute the local (thread) index for the block\n\t\tint localId = threadIdx.x;\n\t\t// Increment the bin for the grayscale intensity at this position\n\t\tbins[image[globalId]] = bins[image[globalId]] + 1;\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "unsigned int blockDim = 256;\n  size_t globalId = blockDim*blockIdx.x + threadIdx.x;\n  if (globalId >= N) return;\n  atomicAdd(&bins[image[globalId]], 1);\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n   if(i < N) {\n      atomicAdd(&bins[image[i]], 1);\n   }\n}",
            "size_t bin = (unsigned char)image[blockIdx.x];\n    atomicAdd(&bins[bin], 1);\n}",
            "int x = threadIdx.x + blockIdx.x * blockDim.x;\n  if (x < N) {\n    int p = image[x];\n    atomicAdd(&bins[p], 1);\n  }\n}",
            "int threadId = threadIdx.x + blockDim.x * blockIdx.x;\n  int localSum = 0;\n  if (threadId < N)\n    localSum += image[threadId];\n  __syncthreads();\n  __shared__ int shmem[256];\n  shmem[threadIdx.x] = localSum;\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[image[threadId]], shmem[threadIdx.x]);\n  }\n}",
            "// Your code goes here\n}",
            "int tid = hipThreadIdx_x;\n    size_t *myBins = &bins[tid];\n    size_t c;\n    \n    // YOUR CODE HERE\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "int block_size = 256;\n  int block_id = blockIdx.x;\n  int thread_id = threadIdx.x;\n  int block_offset = thread_id + block_id*block_size;\n\n  for(size_t i = block_offset; i < N; i+=block_size) {\n    int value = image[i];\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "int intensity = image[blockIdx.x * blockDim.x + threadIdx.x];\n  atomicAdd(&bins[intensity], 1);\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) bins[image[i]]++;\n}",
            "// YOUR CODE HERE\n  // You can use 'hipThreadIdx_x' and 'hipBlockIdx_x'\n  // to get a unique thread and block ID, respectively.\n  // Use 'hipBlockDim_x' to get the number of threads\n  // in each block.\n  // You can use 'hipGridDim_x' to get the number of blocks\n  // in the grid.\n  // To count the number of pixels with intensity X, you can do the following:\n  // atomicAdd(&bins[X], 1);\n  \n  int start = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  int stride = hipGridDim_x * hipBlockDim_x;\n  for (int i = start; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n}",
            "int x = blockDim.x * blockIdx.x + threadIdx.x;\n    int y = blockDim.y * blockIdx.y + threadIdx.y;\n\n    // TODO: Your code goes here\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "}",
            "const int tid = threadIdx.x;\n    const int gid = threadIdx.y;\n    const int tid_x = blockIdx.x * blockDim.x + tid;\n    const int tid_y = blockIdx.y * blockDim.y + gid;\n    if (tid_x >= N || tid_y >= N) return;\n    atomicAdd(&bins[image[tid_y * N + tid_x]], 1);\n}",
            "// TODO: Implement pixel counts kernel in AMD HIP.\n  // The kernel is launched with at least N threads.\n  \n  /*\n     TODO:\n     - Implement your kernel here.\n     - We suggest to use a for-loop with the following parameters:\n       - `i` is the current pixel index\n       - `j` is the current grayscale intensity\n     - Store in `bins` the number of pixels with intensity `j`\n   */\n  \n}",
            "// TODO\n}",
            "__shared__ size_t s_bins[256];\n  const size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  const size_t blockSize = blockDim.x * gridDim.x;\n  const size_t binId = threadId / (256 / 2);\n  const size_t offset = threadId % (256 / 2);\n\n  // Each thread calculates one bin.\n  // There is a barrier at the end of this loop to ensure all threads have\n  // finished writing to the bins.\n  // Threads are blocked until all threads have finished writing.\n  size_t localCount = 0;\n  while (threadId < N) {\n    if (image[threadId] == binId) {\n      localCount++;\n    }\n    threadId += blockSize;\n  }\n  __syncthreads();\n\n  // Each thread adds to the count of the bin it calculates\n  // Each thread also adds to the global count.\n  if (threadIdx.x < 256 / 2) {\n    atomicAdd(&(bins[binId * 2 + offset]), localCount);\n  }\n  __syncthreads();\n\n  // Each thread writes one bin to shared memory\n  if (threadIdx.x < 256 / 2) {\n    s_bins[threadIdx.x * 2 + 0] = bins[threadIdx.x * 2 + 0];\n    s_bins[threadIdx.x * 2 + 1] = bins[threadIdx.x * 2 + 1];\n  }\n  __syncthreads();\n\n  // Each thread adds to the global count.\n  // Each thread also adds to the count of the bin it calculates.\n  if (threadIdx.x < 256 / 2) {\n    atomicAdd(&(bins[threadIdx.x * 2 + 0]), s_bins[threadIdx.x * 2 + 0]);\n    atomicAdd(&(bins[threadIdx.x * 2 + 1]), s_bins[threadIdx.x * 2 + 1]);\n  }\n}",
            "// TODO\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        atomicAdd(bins + image[i], 1);\n    }\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO: implement\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "size_t tid = hipThreadIdx_x;\n  // TODO 1: your code goes here\n}",
            "__shared__ int localBins[256];\n    if (threadIdx.x < 256) localBins[threadIdx.x] = 0;\n    __syncthreads();\n\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        atomicAdd(&localBins[image[i]], 1);\n    }\n    __syncthreads();\n\n    if (threadIdx.x < 256) atomicAdd(&bins[threadIdx.x], localBins[threadIdx.x]);\n}",
            "const int tid = hipThreadIdx_x;\n    const int bin = image[tid];\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO: implement the kernel\n}",
            "}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "// TODO: Your code here\n}",
            "int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  int thread_stride = blockDim.x * gridDim.x;\n  for (size_t i = thread_id; i < N; i += thread_stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    int count = 0;\n    while (gid < N) {\n        atomicAdd(&bins[image[gid]], 1);\n        gid += blockDim.x * gridDim.x;\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int nThreads = blockDim.x;\n    int blocksize = 256;\n    int blockStart = blocksize * bid;\n    int blockEnd = min(blockStart + blocksize, N);\n    int localCount[256] = {0};\n    for (int idx = blockStart + tid; idx < blockEnd; idx += nThreads) {\n        localCount[image[idx]]++;\n    }\n    __syncthreads();\n    for (int idx = tid; idx < 256; idx += nThreads) {\n        atomicAdd(&bins[idx], localCount[idx]);\n    }\n}",
            "int bin = 0;\n   int threadId = blockDim.x*blockIdx.x+threadIdx.x;\n   int stride = blockDim.x*gridDim.x;\n   // TODO: Implement the kernel\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// Your code goes here\n}",
            "// Get pixel index\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Count the number of pixels in image with each grayscale intensity.\n  // Note: for simplicity we do not check for valid image index.\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n    if (x < N)\n        atomicAdd(&bins[image[x]], 1);\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  __shared__ size_t sdata[256];\n\n  // Each thread block is responsible for counting pixels with a certain\n  // grayscale intensity.\n\n  // Initialize to zero.\n  sdata[hipThreadIdx_x] = 0;\n\n  // Each thread processes a single pixel.\n  if (tid < N) {\n    sdata[image[tid]]++;\n  }\n\n  // The total number of pixels is the sum of all the counts.\n  __syncthreads();\n  if (hipThreadIdx_x == 0) {\n    for (int i = 0; i < 256; i++) {\n      atomicAdd(&(bins[i]), sdata[i]);\n    }\n  }\n}",
            "int tid = hipThreadIdx_x;\n    int gid = hipBlockIdx_x * hipBlockDim_x + tid;\n\n    if (gid < N) {\n        atomicAdd(bins + image[gid], 1);\n    }\n}",
            "size_t blockId = hipBlockIdx_x + hipBlockIdx_y * hipGridDim_x;\n    size_t threadId = hipThreadIdx_x;\n    int x = blockId * hipBlockDim_x + threadId;\n    if (x < N) {\n        // bins[image[x]]++;\n        atomicAdd(&bins[image[x]], 1);\n    }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i >= N) return;\n  atomicAdd(&bins[image[i]], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x+threadIdx.x;\n  if (i<N) {\n    bins[image[i]] += 1;\n  }\n}",
            "size_t i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (i >= N)\n        return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "// TODO: add code\n}",
            "__shared__ size_t count_per_thread[256];\n  int tid = threadIdx.x;\n  int gray = image[tid];\n  atomicAdd(&count_per_thread[gray], 1);\n  __syncthreads();\n  atomicAdd(&bins[gray], count_per_thread[tid]);\n}",
            "int gid = blockDim.x * blockIdx.x + threadIdx.x;\n  int nThreads = blockDim.x * gridDim.x;\n  for (int tid = gid; tid < N; tid += nThreads) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int myid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (myid >= N) {\n    return;\n  }\n  int pixel = image[myid];\n  atomicAdd(&bins[pixel], 1);\n}",
            "int i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    __shared__ size_t sbins[256];\n    if (tid < N) {\n        size_t bin = image[tid];\n        atomicAdd(&sbins[bin], 1);\n    }\n    __syncthreads();\n    if (hipThreadIdx_x < 256)\n        atomicAdd(&bins[hipThreadIdx_x], sbins[hipThreadIdx_x]);\n}",
            "__shared__ size_t sbins[256];\n  for(int t=threadIdx.x; t<256; t+=blockDim.x)\n    sbins[t] = 0;\n  __syncthreads();\n\n  for(int i=blockIdx.x*blockDim.x+threadIdx.x; i<N; i+=blockDim.x*gridDim.x)\n    atomicAdd(&sbins[image[i]], 1);\n  __syncthreads();\n\n  for(int t=threadIdx.x; t<256; t+=blockDim.x)\n    atomicAdd(&bins[t], sbins[t]);\n}",
            "__shared__ size_t smem[256];\n  size_t block_sum = 0;\n  const size_t tid = threadIdx.x;\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    const size_t bin = image[i];\n    atomicAdd(&smem[bin], 1);\n    block_sum += smem[bin];\n  }\n  // smem[tid] now contains the sum of the first tid+1 elements of image\n  __syncthreads();\n  // reduce thread block sums to one thread\n  for (size_t stride = blockDim.x / 2; stride > 0; stride /= 2) {\n    if (tid < stride) smem[tid] += smem[tid + stride];\n    __syncthreads();\n  }\n  if (tid == 0) atomicAdd(&bins[blockIdx.x], block_sum);\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "const int tid = hipThreadIdx_x;\n  const int block = blockIdx.x;\n  const int blockSize = blockDim.x;\n\n  int i = tid + block * blockSize;\n\n  __shared__ int smem[256];\n\n  if (i < N) {\n    const int bin = image[i];\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Your code here\n}",
            "// YOUR CODE HERE\n    unsigned int idx = blockIdx.x*blockDim.x + threadIdx.x;\n    if(idx >= N) return;\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "int gid = hipThreadIdx_x;\n   if (gid < N) {\n      bins[image[gid]] += 1;\n   }\n}",
            "size_t idx = blockIdx.x*blockDim.x+threadIdx.x;\n  size_t bin = image[idx]/256;\n  atomicAdd(&bins[bin], 1);\n}",
            "__shared__ int tmp[256];\n  for (int tid = threadIdx.x; tid < 256; tid += blockDim.x) {\n    tmp[tid] = 0;\n  }\n  __syncthreads();\n  for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    atomicAdd(&tmp[image[i]], 1);\n  }\n  __syncthreads();\n  for (int tid = threadIdx.x; tid < 256; tid += blockDim.x) {\n    atomicAdd(&bins[tid], tmp[tid]);\n  }\n}",
            "int tid = hipThreadIdx_x;\n\n  // Your code here.\n  // Hints:\n  // - The global thread ID is `hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x`.\n  // - You can use atomicAdd to increment the count for a given grayscale value.\n  // - You can count the number of threads in the block by calling hipBlockDim_x().\n  // - You can load a value from global memory using the \"ldg\" prefix,\n  //   which is a load which has a higher latency, but is safe to execute from\n  //   any thread. For example,\n  //     int i = ldg(image + tid);\n  //   This reads a value from memory at `image + tid` and returns it.\n  // - You can use the `tid` variable to index into `bins`, but if you do that,\n  //   you might want to use atomicAdd to increment the count.\n  // - For example, if you have 8 threads in a block, and you want to store\n  //   the 4-th element in `bins`, you can do so with:\n  //     atomicAdd(&(bins[tid * 8]), 1);\n\n  int i = tid;\n\n  atomicAdd(&(bins[image[i]]), 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tint stride = blockDim.x * gridDim.x;\n\n\tfor (int i = idx; i < N; i += stride) {\n\t\tatomicAdd(&bins[image[i]], 1);\n\t}\n}",
            "const int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  const int stride = blockDim.x * gridDim.x;\n  for (int i = idx; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t x = hipThreadIdx_x;\n\n    for (size_t i = x; i < N; i += hipBlockDim_x) {\n        size_t bin = image[i];\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO:\n    // 1. compute the bin number using the intensity value\n    // 2. increment the bin number using atomicAdd\n}",
            "// Your code here\n  // To launch: hipLaunchKernelGGL(pixelCounts, dim3(bins.size()), dim3(threads_per_block), 0, 0, image, N, bins)\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t sum = 0;\n\n  if (i < N) {\n    atomicAdd(bins + image[i], 1);\n  }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x*blockDim.x;\n  while (i < N) {\n    int index = image[i];\n    atomicAdd(&(bins[index]), 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "/* TODO */\n}",
            "// TODO\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        bins[image[idx]] += 1;\n    }\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n   size_t Nperthread = N / blockDim.x;\n   size_t threadIdx_x = threadIdx.x;\n   size_t threadIdx_y = threadIdx.y;\n   size_t threadIdx_z = threadIdx.z;\n   size_t threadIdx_w = threadIdx.w;\n   size_t blockIdx_x = blockIdx.x;\n   size_t blockIdx_y = blockIdx.y;\n   size_t blockIdx_z = blockIdx.z;\n   size_t blockIdx_w = blockIdx.w;\n   size_t blockDim_x = blockDim.x;\n   size_t blockDim_y = blockDim.y;\n   size_t blockDim_z = blockDim.z;\n   size_t blockDim_w = blockDim.w;\n   size_t gridDim_x = gridDim.x;\n   size_t gridDim_y = gridDim.y;\n   size_t gridDim_z = gridDim.z;\n   size_t gridDim_w = gridDim.w;\n   size_t blockId = blockIdx.x + blockIdx.y*gridDim.x + blockIdx.z*gridDim.x*gridDim.y + blockIdx.w*gridDim.x*gridDim.y*gridDim.z;\n   size_t gridId = gridIdx.x + gridIdx.y*gridDim.x + gridIdx.z*gridDim.x*gridDim.y + gridIdx.w*gridDim.x*gridDim.y*gridDim.z;\n\n   for (int i=tid; i<Nperthread; i+=blockDim.x) {\n      size_t value = image[i];\n      atomicAdd(&bins[value], 1);\n   }\n}",
            "int sum = 0;\n    int sum_old = 0;\n    //int threadId = hipBlockDim_x*hipBlockIdx_x + hipThreadIdx_x;\n    int blockId = hipBlockIdx_x;\n    //int numBlocks = (N + hipBlockDim_x - 1) / hipBlockDim_x;\n    //int blockId = threadId / hipBlockDim_x;\n    if (blockId < N) {\n        int pixel = image[blockId];\n        sum_old = atomicAdd(&bins[pixel], 1);\n    }\n    __syncthreads();\n    if (blockId < N) {\n        //while (threadId < N) {\n            int pixel = image[blockId];\n            sum = atomicAdd(&bins[pixel], 1);\n            //threadId += numBlocks;\n        //}\n        atomicAdd(&bins[pixel], sum_old);\n    }\n    __syncthreads();\n    if (blockId < N) {\n        //while (threadId < N) {\n            int pixel = image[blockId];\n            sum = atomicAdd(&bins[pixel], 1);\n            //threadId += numBlocks;\n        //}\n        atomicAdd(&bins[pixel], sum_old);\n    }\n}",
            "__shared__ size_t sbins[256];\n   size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) sbins[image[idx]]++;\n   __syncthreads();\n   if (threadIdx.x == 0) for (int i=0; i<256; i++) bins[i] += sbins[i];\n}",
            "// TODO: Launch the pixelCounts kernel\n    // TODO: Use atomicAdd to add the result to bins\n    // TODO: Use AMD HIP to launch the kernel\n}",
            "int index = threadIdx.x;\n  int stride = blockDim.x;\n  for (int i = blockIdx.x; i < N; i += gridDim.x) {\n    int val = image[i];\n    atomicAdd(&bins[val], 1);\n  }\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "// Each thread counts one pixel.\n  int tid = threadIdx.x;\n  // Each block counts `blockDim.x` pixels.\n  int bid = blockIdx.x;\n  // First compute `bid * blockDim.x + tid` pixel.\n  int pixel = bid * blockDim.x + tid;\n  // If it's out of bounds, return.\n  if (pixel >= N) return;\n  // Next, compute the grayscale intensity value for this pixel.\n  int intensity = image[pixel];\n  // Finally, increment the count for this intensity value.\n  atomicAdd(&bins[intensity], 1);\n}",
            "// TODO\n}",
            "size_t local_bins[256];\n    memset(local_bins, 0, 256 * sizeof(size_t));\n    for (size_t i = 0; i < N; i++) {\n        local_bins[image[i]]++;\n    }\n\n    __syncthreads();\n    for (size_t i = threadIdx.x; i < 256; i += blockDim.x) {\n        atomicAdd(&bins[i], local_bins[i]);\n    }\n}",
            "/* YOUR CODE HERE */\n    unsigned int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    unsigned int blockSize = hipBlockDim_x * hipGridDim_x;\n    unsigned int gridSize = N / blockSize + (N % blockSize!= 0? 1 : 0);\n    unsigned int gridId = hipBlockIdx_x;\n    unsigned int threadId = hipThreadIdx_x;\n\n    while(tid < N) {\n        bins[image[tid]]++;\n        tid += blockSize * gridSize;\n    }\n}",
            "// TODO: Your code here\n  // Use threadIdx.x, blockDim.x, blockIdx.x, and gridDim.x.\n  // See https://en.wikipedia.org/wiki/CUDA#Programming_model\n  int grayscale = image[blockIdx.x * blockDim.x + threadIdx.x];\n  atomicAdd(&bins[grayscale], 1);\n}",
            "// Each thread is responsible for 1 element of the image\n  for (int i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: your code here\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t blocksize = hipBlockDim_x;\n  __shared__ size_t sbins[256];\n  if (tid == 0) {\n    for (size_t i = 0; i < 256; i++) {\n      sbins[i] = 0;\n    }\n  }\n  __syncthreads();\n  for (size_t i = hipBlockIdx_x * blocksize + tid; i < N; i += blocksize * hipGridDim_x) {\n    sbins[image[i]]++;\n  }\n  __syncthreads();\n  for (size_t i = tid; i < 256; i += blocksize) {\n    atomicAdd(&bins[i], sbins[i]);\n  }\n}",
            "// TODO: implement this function\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "const int tx = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tx < N) {\n    const int v = image[tx];\n    atomicAdd(&bins[v], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "// TODO: Implement pixelCounts\n}",
            "int idx = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  if (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = tid; i < 256; i += stride) {\n    bins[i] = 0;\n  }\n  __syncthreads();\n  for (size_t i = tid; i < N; i += stride) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "// YOUR CODE HERE\n  // Your code should call hipThreadIdx_x(), hipBlockDim_x() and hipBlockIdx_x() to get the\n  // thread ID and block ID.  You can use these to compute the row and column ID of this thread.\n  // The values of thread ID and block ID are in the range from 0 to max block ID - 1 and\n  // from 0 to max thread ID - 1, respectively.\n  // Your code should also use hipMemcpy() and hipMemcpyAsync() to copy data between the\n  // image and the output array.\n}",
            "unsigned int t = hipThreadIdx_x;\n    unsigned int i = hipBlockIdx_x * hipBlockDim_x + t;\n    unsigned int intensity;\n    while (i < N) {\n        intensity = image[i];\n        atomicAdd(&(bins[intensity]), 1);\n        i += hipBlockDim_x * hipGridDim_x;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n      bins[image[tid]]++;\n    }\n}",
            "// your code goes here\n    __syncthreads();\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    int gray = image[tid];\n    atomicAdd(&bins[gray], 1);\n  }\n}",
            "// TODO: Your code here\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "// TODO: Write your kernel here\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t bin_id = image[tid];\n\n  // count number of pixels in bin bin_id\n  atomicAdd(&bins[bin_id], 1);\n}",
            "__shared__ unsigned int count[256];\n\n    int idx = threadIdx.x;\n    count[idx] = 0;\n    for (int i = blockIdx.x; i < N; i += gridDim.x) {\n        atomicAdd(&count[image[i]], 1);\n    }\n    for (int i = blockDim.x/2; i > 0; i /= 2) {\n        __syncthreads();\n        if (idx < i) {\n            atomicAdd(&count[idx], count[idx + i]);\n        }\n    }\n    if (idx == 0) {\n        for (int i = 0; i < 256; i++) {\n            atomicAdd(&bins[i], count[i]);\n        }\n    }\n}",
            "const int thread = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n    if (thread < N) {\n        ++bins[image[thread]];\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: implement this function\n }",
            "// Fill this in\n}",
            "/* TODO: your code goes here! */\n}",
            "// TODO\n}",
            "const size_t rank = 0;\n   const size_t size = 1;\n   const size_t width = image.size();\n   const size_t height = 1;\n   const size_t imageSize = width * height;\n   int numProcessors;\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcessors);\n   // get the local number of pixels in the image\n   size_t localNumPixels = 0;\n   if(rank == 0){\n     localNumPixels = imageSize / numProcessors;\n   }\n   size_t globalNumPixels;\n   MPI_Allreduce(&localNumPixels, &globalNumPixels, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n   // get the local histogram\n   std::array<size_t, 256> localHistogram;\n   std::fill(localHistogram.begin(), localHistogram.end(), 0);\n   if(rank == 0){\n     for(size_t i = 0; i < globalNumPixels; ++i){\n       localHistogram[image[i]] += 1;\n     }\n   }\n   // send the local histogram to rank 0\n   std::array<size_t, 256> globalHistogram;\n   MPI_Gather(localHistogram.data(), 256, MPI_UNSIGNED_LONG, globalHistogram.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   if(rank == 0){\n     bins = globalHistogram;\n   }\n }",
            "// TODO: your code here\n  size_t len = image.size();\n  int rank, size;\n  int count = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int *counts, *displs, i, total;\n  std::fill(bins.begin(), bins.end(), 0);\n  int loc_counts[256];\n  MPI_Allgather(&len, 1, MPI_INT, loc_counts, 1, MPI_INT, MPI_COMM_WORLD);\n  total = 0;\n  for(i=0;i<256;i++) {\n    total += loc_counts[i];\n  }\n  counts = new int[size];\n  displs = new int[size];\n  for(i=0;i<size;i++) {\n    counts[i] = loc_counts[i];\n  }\n  displs[0] = 0;\n  for(i=1;i<size;i++) {\n    displs[i] = displs[i-1] + counts[i-1];\n  }\n  int *pixels = new int[len];\n  MPI_Allgatherv(&image[0], len, MPI_INT, pixels, counts, displs, MPI_INT, MPI_COMM_WORLD);\n  for(i=0;i<len;i++) {\n    bins[pixels[i]]++;\n  }\n  if(rank == 0) {\n    for(i=0;i<256;i++) {\n      bins[i] = 0;\n      for(int j=0;j<size;j++) {\n        bins[i] += loc_counts[j];\n      }\n    }\n  }\n  delete [] pixels;\n  delete [] counts;\n  delete [] displs;\n}",
            "// Your code here\n  // return 0;\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int height = image.size();\n   int width = image[0];\n\n   // Compute total number of pixels in the image.\n   int pixel_count = height * width;\n\n   // Compute the number of pixels in each rank.\n   int pixels_per_rank = pixel_count / size;\n   int extra_pixels = pixel_count % size;\n\n   // Each rank will get one additional pixel if it has more than or equal to 1.\n   if (rank < extra_pixels) {\n     pixels_per_rank++;\n   }\n\n   // Each rank gets a vector of `pixels_per_rank` pixels.\n   std::vector<int> pixels(pixels_per_rank);\n\n   // Each rank gets a vector of `pixels_per_rank` pixels.\n   if (rank < extra_pixels) {\n     pixels_per_rank++;\n   }\n\n   // Copy pixels from the beginning of the image to each rank.\n   MPI_Scatter(&image[0], pixels_per_rank, MPI_INT, &pixels[0], pixels_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // Each rank will count the number of pixels in the sub-vector\n   // of `pixels` that correspond to each grayscale value.\n   std::array<size_t, 256> sub_bins = {0};\n   for (auto &pixel : pixels) {\n     sub_bins[pixel]++;\n   }\n\n   // Each rank will send the sub-counts to rank 0.\n   MPI_Reduce(&sub_bins, &bins, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n }",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  int pixels = image.size();\n  \n  int totalPixels;\n  MPI_Allreduce(&pixels, &totalPixels, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  // If there are no pixels, we don't need to do anything\n  if (totalPixels == 0)\n    return;\n  \n  // Calculate pixels per rank\n  int pixelsPerRank = totalPixels / size;\n  int remainder = totalPixels % size;\n\n  // Calculate start and end indexes for this rank\n  int start = 0;\n  if (rank > 0)\n    start = pixelsPerRank * rank;\n  \n  int end = pixelsPerRank + remainder;\n  if (rank == size - 1)\n    end = totalPixels;\n  \n  // Calculate the histogram of the pixels for this rank\n  std::array<size_t, 256> localBins;\n  std::fill(localBins.begin(), localBins.end(), 0);\n  for (int i = start; i < end; ++i) {\n    ++localBins[image[i]];\n  }\n\n  // Send the result to rank 0\n  MPI_Reduce(localBins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// implement this function with MPI\n   // You may need to add code for initializing MPI.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int start = 0;\n  int end = image.size();\n  int imageSize = end - start;\n\n  int subImageSize = (imageSize + size - 1) / size;\n  int totalImageSize = size * subImageSize;\n\n  std::vector<int> subImage(subImageSize);\n  std::array<size_t, 256> subBins;\n\n  int imageRank = rank * (imageSize / size);\n  int imageEnd = imageRank + subImageSize;\n  for (int i = imageRank; i < imageEnd; ++i) {\n    subImage[i - imageRank] = image[i];\n  }\n  for (int i = imageEnd; i < imageSize; ++i) {\n    subImage[i - imageRank] = 0;\n  }\n  MPI_Reduce(subImage.data(), subBins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  for (int i = 0; i < 256; ++i) {\n    bins[i] += subBins[i];\n  }\n}",
            "// TO DO: complete this function\n}",
            "// TODO\n}",
            "/* TODO: Your code here */\n}",
            "// Your code here\n\n    int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int count = image.size() / size;\n    int residual = image.size() % size;\n    int start = rank * count;\n    int end = start + count;\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    if (rank < residual) {\n        end++;\n    }\n\n    for (int i = start; i < end; i++) {\n        bins[image[i]]++;\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int localCount = 0;\n    int imageCount = image.size();\n    for (int i = 0; i < imageCount; i++) {\n        localCount++;\n    }\n    std::vector<int> localBins(256);\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            localBins[i] = 0;\n        }\n    }\n    //std::vector<int> tempVector(size);\n    int localImageCounter = 0;\n    //localCount = tempVector[rank] = localImageCounter;\n    for (int i = 0; i < imageCount; i++) {\n        int localGrayscale = image[i];\n        localBins[localGrayscale]++;\n        localImageCounter++;\n        if (localImageCounter >= localCount) {\n            localCount = tempVector[rank] = 0;\n            for (int j = 0; j < 256; j++) {\n                localBins[j] = 0;\n            }\n        }\n    }\n    MPI_Reduce(tempVector.data(), localBins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = localBins;\n}",
            "// TODO: Your code goes here\n}",
            "// Your code here.\n}",
            "// your code here\n}",
            "/* Fill in your solution here. */\n    int myrank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    size_t numRows = image.size() / nprocs;\n\n    if(myrank == 0){\n        size_t start = 0;\n        for(int i = 1; i < nprocs; i++){\n            size_t end = start + numRows;\n            std::vector<int> sub(image.begin() + start, image.begin() + end);\n            std::array<size_t, 256> subbins;\n            pixelCounts(sub, subbins);\n            for(size_t j = 0; j < 256; j++){\n                bins[j] += subbins[j];\n            }\n            start += numRows;\n        }\n    } else {\n        std::vector<int> sub(image.begin() + numRows * myrank, image.begin() + numRows * (myrank + 1));\n        std::array<size_t, 256> subbins;\n        pixelCounts(sub, subbins);\n        MPI_Send(subbins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n    \n    if(myrank == 0){\n        for(int i = 1; i < nprocs; i++){\n            std::array<size_t, 256> subbins;\n            MPI_Recv(subbins.data(), 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(size_t j = 0; j < 256; j++){\n                bins[j] += subbins[j];\n            }\n        }\n    }\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint n = image.size();\n\tint count = n / size;\n\tint remain = n - (size - 1) * count;\n\tint start = rank * count;\n\tint end = (rank < remain)? (start + count) : (start + remain);\n\tstd::vector<int> myimage(image.begin() + start, image.begin() + end);\n\n\tint local_count = 0;\n\tint local_bins[256] = {0};\n\tfor (int pixel : myimage) {\n\t\tlocal_count++;\n\t\tlocal_bins[pixel] += 1;\n\t}\n\t//std::cout << \"Rank \" << rank << \" has \" << local_count << \" local counts\\n\";\n\n\t// Sum the local counts\n\tMPI_Reduce(&local_count, &(bins[0]), 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//std::cout << \"Rank \" << rank << \" has \" << bins[0] << \" total counts\\n\";\n\n\t// Sum the local bins\n\tMPI_Reduce(local_bins, &(bins[0]), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//std::cout << \"Rank \" << rank << \" has \" << bins[255] << \" total bins\\n\";\n}",
            "// TODO\n }",
            "int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  // TODO: Count in parallel using MPI_Scatter and MPI_Gather.\n  // Each process should process a chunk of the data.\n}",
            "// TODO: your code here\n}",
            "// Your code here.\n    // Initialize bins to 0\n    for(int i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n    // count each pixel and store them in bins\n    for(int i = 0; i < image.size(); i++)\n    {\n        bins[image[i]]++;\n    }\n}",
            "// TODO: Your code here\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  std::vector<int> image_local;\n  if (world_rank == 0) {\n    image_local = image;\n  } else {\n    image_local.resize(image.size());\n  }\n  std::array<size_t, 256> bins_local = {0};\n  MPI_Scatter(&image_local[0], image_local.size(), MPI_INT, &bins_local[0], image_local.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  for (size_t i = 0; i < image_local.size(); ++i) {\n    ++bins_local[image_local[i]];\n  }\n  MPI_Reduce(&bins_local[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// 1. Count the number of pixels with each intensity in the image\n   // 2. Store the result in `bins`\n   // 3. Use MPI to distribute the work\n   // 4. When you have finished, compare your result with the result of the\n   //    serial algorithm, and verify that they are the same.\n   // 5. Repeat the experiment for a number of iterations. 10000 is a good number to try.\n   // 6. The serial and parallel algorithms should produce the same result.\n }",
            "// Initialize the vector bins to hold the results\n    bins.fill(0);\n\n    // Get the number of processes\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Get the rank of the process\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Split the array up according to the rank of the process\n    int splitSize = image.size()/size;\n    int offset = rank * splitSize;\n\n    // Create the sub-vector for the process\n    std::vector<int> subImage(image.begin()+offset, image.begin()+offset+splitSize);\n\n    // Initialize the local count of each grayscale intensity\n    std::array<size_t, 256> localCount;\n    localCount.fill(0);\n\n    // Count the number of pixels in the sub-image\n    for(auto const& pixel: subImage) {\n        localCount[pixel] += 1;\n    }\n\n    // Send the result to rank 0\n    if(rank!= 0) {\n        MPI_Send(localCount.data(), 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    // Get the result of rank 0\n    else {\n        for(int p = 0; p < size-1; p++) {\n            std::array<size_t, 256> result;\n            MPI_Recv(result.data(), 256, MPI_UNSIGNED_LONG, p, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n            for(int c = 0; c < 256; c++) {\n                bins[c] += result[c];\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n     int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     int image_size = image.size();\n     int part = image_size / size;\n     int start = rank * part;\n     int end = (rank+1) * part;\n     int local_result[256] = {0};\n     for(int i = start; i < end; i++){\n         local_result[image[i]]++;\n     }\n     MPI_Reduce(local_result, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// 0. Set up the MPI environment\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // 1. Partition the image evenly\n  // TODO:\n  int num_pixels = image.size() / world_size;\n  int extra_pixels = image.size() % world_size;\n  std::vector<int> image_split;\n  image_split = image;\n\n  // 2. Count the number of pixels of each grayscale intensity\n  // TODO:\n\n  // 3. Combine the result with MPI\n  // TODO:\n}",
            "}",
            "int rank, num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if(rank == 0) {\n    bins.fill(0);\n  }\n\n  int n = image.size();\n  int chunk_size = n / num_ranks;\n\n  std::vector<int> local_image(chunk_size);\n  MPI_Scatter(image.data(), chunk_size, MPI_INT, local_image.data(), chunk_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int local_bins[256];\n  local_bins[0] = 0;\n  for(int i = 0; i < chunk_size; ++i) {\n    ++local_bins[local_image[i]];\n  }\n\n  MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Status status;\n \tint rank, size;\n \tMPI_Comm_size(MPI_COMM_WORLD, &size);\n \tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n \tint imageSize = image.size();\n \tint local_bins[256] = {};\n \tint local_counts[256] = {};\n \tint start = (imageSize / size) * rank;\n \tint end = (imageSize / size) * (rank + 1);\n \tfor (int i = start; i < end; i++) {\n \t\tlocal_counts[image[i]]++;\n \t}\n \tMPI_Reduce(&local_counts, &local_bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n \tif (rank == 0) {\n \t\tfor (int i = 0; i < 256; i++) {\n \t\t\tbins[i] = local_bins[i];\n \t\t}\n \t}\n }",
            "// get the number of processes and the rank\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // get the number of pixels\n    int n = image.size();\n\n    // calculate the chunk size and the starting pixel\n    int chunk_size = n / world_size;\n    int start = chunk_size * world_rank;\n\n    // get the count for each pixel and store the result in bins\n    for(int i = 0; i < chunk_size; ++i) {\n        bins[image[start+i]]++;\n    }\n\n    // send the bins to the master process\n    if(world_rank!= 0) {\n        MPI_Send(&bins, 256, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    } else {\n        for(int i = 1; i < world_size; ++i) {\n            MPI_Status status;\n            MPI_Recv(&bins, 256, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n}",
            "// TODO: Replace this with your own code\n\n  // Each rank needs to calculate the histogram for its pixels.\n\n  // TODO: Replace this with your own code\n}",
            "// TODO: Implement this function.\n}",
            "// YOUR CODE HERE\n }",
            "// TODO: Implement the function\n}",
            "std::vector<int> localBins(256, 0);\n\n  for(size_t i=0; i<image.size(); i++){\n    localBins[image[i]]++;\n  }\n\n  MPI_Reduce(localBins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n    //...\n    //...\n}",
            "// TODO: Your code here.\n}",
            "// TODO: Your code here.\n }",
            "size_t size = image.size();\n    size_t world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    std::vector<size_t> local_bins(256, 0);\n    if (world_size == 1) {\n        local_bins = std::vector<size_t>(bins);\n        return;\n    }\n    size_t start = world_rank * size / world_size;\n    size_t end = (world_rank + 1) * size / world_size;\n    size_t local_size = end - start;\n    for (size_t i = start; i < end; ++i) {\n        local_bins[image[i]]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (world_rank == 0) {\n        for (size_t i = 1; i < world_size; ++i) {\n            for (size_t j = 0; j < 256; ++j) {\n                bins[j] += bins[j + 256 * i];\n            }\n        }\n    }\n}",
            "int rank, size;\n \tMPI_Comm_size(MPI_COMM_WORLD, &size);\n \tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n \t// if (rank == 0) {\n \t// \tstd::cout << \"rank \" << rank << \" size \" << size << std::endl;\n \t// \tstd::cout << \"image \" << std::endl;\n \t// \tfor (auto& e : image) {\n \t// \t\tstd::cout << e << \", \";\n \t// \t}\n \t// \tstd::cout << std::endl;\n \t// }\n \t// int len = image.size();\n \t// std::cout << \"len \" << len << std::endl;\n \tif (rank == 0) {\n \t\t// std::cout << \"rank \" << rank << std::endl;\n \t\tstd::fill(bins.begin(), bins.end(), 0);\n \t}\n \t// std::cout << \"rank \" << rank << std::endl;\n \t// auto len = image.size();\n \t// std::cout << \"rank \" << rank << \" len \" << len << std::endl;\n \t// auto image_r = image.data();\n \t// std::cout << \"rank \" << rank << \" image_r \" << image_r << std::endl;\n \tint len_local = image.size() / size;\n \tint rest = image.size() % size;\n \tint start = 0;\n \tif (rank < rest) {\n \t\tstart = rank * (len_local + 1);\n \t\tlen_local += 1;\n \t}\n \telse {\n \t\tstart = rank * len_local + rest;\n \t}\n \t// std::cout << \"rank \" << rank << \" start \" << start << \" len_local \" << len_local << std::endl;\n \tauto image_local = image.data() + start;\n \t// std::cout << \"rank \" << rank << \" image_local \" << image_local << std::endl;\n \t// auto bins_local = bins.data() + rank;\n \t// std::cout << \"rank \" << rank << \" bins_local \" << bins_local << std::endl;\n \t// auto bins_local = bins.data();\n \t// std::cout << \"rank \" << rank << \" bins_local \" << bins_local << std::endl;\n \t// bins.assign(bins.size(), 0);\n \tstd::fill(bins.begin(), bins.end(), 0);\n \t// std::cout << \"rank \" << rank << \" len_local \" << len_local << std::endl;\n \t// auto len_local = image.size() / size;\n \t// auto rest = image.size() % size;\n \t// if (rank < rest) {\n \t// \tlen_local += 1;\n \t// }\n \t// // std::cout << \"rank \" << rank << \" len_local \" << len_local << std::endl;\n \t// auto image_local = image.data() + rank * len_local;\n \t// auto bins_local = bins.data() + rank;\n \tfor (int i = 0; i < len_local; i++) {\n \t\tbins[image_local[i]] += 1;\n \t}\n \t// std::cout << \"rank \" << rank << \" bins_local \" << bins_local << std::endl;\n \t// MPI_Barrier(MPI_COMM_WORLD);\n \t// if (rank == 0) {\n \t// \tstd::cout << \"rank \" << rank << \" bins \" << std::endl;\n \t// \tfor (auto& e : bins) {\n \t// \t\tstd::cout << e << \", \";\n \t// \t}\n \t// \tstd::cout << std::endl;\n \t// }\n \tMPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n \t// MPI_Gather(bins_local, bins.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n \t// std::cout << \"rank \" << rank << \" bins \" << std::endl;\n \t// for (auto& e : bins) {\n \t// \tstd::cout << e << \", \";\n \t// }\n \t//",
            "MPI_Datatype MPI_Int = getMPIType(image[0]);\n   MPI_Comm world;\n   int worldRank, worldSize;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n   int const N = image.size();\n   int const n = N / worldSize;\n\n   if (worldRank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] = 0;\n     }\n   }\n\n   int *imageRank = new int[n];\n   MPI_Scatter(image.data(), n, MPI_Int, imageRank, n, MPI_Int, 0, MPI_COMM_WORLD);\n\n   int *counts = new int[256];\n\n   for (int i = 0; i < 256; i++) {\n     counts[i] = 0;\n   }\n\n   for (int i = 0; i < n; i++) {\n     counts[imageRank[i]]++;\n   }\n\n   MPI_Reduce(counts, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (worldRank == 0) {\n     int sum = 0;\n     for (int i = 0; i < 256; i++) {\n       sum += bins[i];\n       bins[i] = sum;\n     }\n   }\n\n   MPI_Comm_free(&world);\n   delete [] counts;\n   delete [] imageRank;\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t sizePerRank = image.size() / size;\n    // Rank 0 will receive the results, so only allocate memory if rank is 0\n    if(rank == 0) {\n        bins = std::array<size_t, 256>{};\n    }\n    // For each rank, count the pixels in the portion of image assigned to that rank\n    std::array<size_t, 256> counts = std::array<size_t, 256>{};\n    for(size_t i = sizePerRank * rank; i < sizePerRank * (rank + 1); ++i) {\n        ++counts[image[i]];\n    }\n    // Sum the counts from all the ranks\n    std::array<size_t, 256> sendcounts = counts;\n    std::array<size_t, 256> recvcounts = counts;\n    std::array<int, 256> sdispls;\n    std::array<int, 256> rdispls;\n    MPI_Scatter(&sendcounts, 256, MPI_UNSIGNED_LONG_LONG, &recvcounts, 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    for(int i = 1; i < size; ++i) {\n        sdispls[i] = sdispls[i - 1] + sendcounts[i - 1];\n        rdispls[i] = rdispls[i - 1] + recvcounts[i - 1];\n    }\n    if(rank!= 0) {\n        MPI_Scatterv(&counts, &sendcounts, &sdispls, MPI_INT, &recvcounts, 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n    MPI_Gatherv(&recvcounts, 256, MPI_UNSIGNED_LONG_LONG, &bins, &recvcounts, &rdispls, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    // TODO: your code here\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int left, right;\n    MPI_Request request[2];\n    if (rank!= 0) {\n        left = rank - 1;\n    }\n    else {\n        left = size - 1;\n    }\n    if (rank!= (size - 1)) {\n        right = rank + 1;\n    }\n    else {\n        right = 0;\n    }\n    int count = 0;\n    int recv_count = 0;\n    int send_count = 0;\n    int tag1 = 0;\n    int tag2 = 1;\n    int tag = 1;\n    int recv_flag;\n    int send_flag;\n    int send_count_1, send_count_2, recv_count_1, recv_count_2;\n    if (rank == 0) {\n        send_count_1 = image.size() / 2;\n        send_count_2 = image.size() - image.size() / 2;\n        MPI_Send(&send_count_1, 1, MPI_INT, left, tag1, MPI_COMM_WORLD);\n        MPI_Send(&send_count_2, 1, MPI_INT, right, tag2, MPI_COMM_WORLD);\n    }\n    if (rank == (size - 1)) {\n        recv_count_1 = image.size() / 2;\n        recv_count_2 = image.size() - image.size() / 2;\n        MPI_Recv(&recv_count_1, 1, MPI_INT, left, tag1, MPI_COMM_WORLD, &recv_flag);\n        MPI_Recv(&recv_count_2, 1, MPI_INT, right, tag2, MPI_COMM_WORLD, &recv_flag);\n    }\n    else if (rank == 0) {\n        recv_count_1 = image.size() / 2;\n        MPI_Recv(&recv_count_1, 1, MPI_INT, left, tag1, MPI_COMM_WORLD, &recv_flag);\n        MPI_Send(&recv_count_1, 1, MPI_INT, right, tag2, MPI_COMM_WORLD);\n    }\n    else if (rank == (size - 1)) {\n        send_count_1 = image.size() / 2;\n        MPI_Recv(&recv_count_1, 1, MPI_INT, left, tag1, MPI_COMM_WORLD, &recv_flag);\n        MPI_Send(&send_count_1, 1, MPI_INT, right, tag2, MPI_COMM_WORLD);\n    }\n    else {\n        send_count_1 = image.size() / 2;\n        send_count_2 = image.size() - image.size() / 2;\n        MPI_Send(&send_count_1, 1, MPI_INT, left, tag1, MPI_COMM_WORLD);\n        MPI_Send(&send_count_2, 1, MPI_INT, right, tag2, MPI_COMM_WORLD);\n        MPI_Recv(&recv_count_1, 1, MPI_INT, left, tag1, MPI_COMM_WORLD, &recv_flag);\n        MPI_Recv(&recv_count_2, 1, MPI_INT, right, tag2, MPI_COMM_WORLD, &recv_flag);\n    }\n    if (rank == (size - 1)) {\n        recv_count = recv_count_1 + recv_count_2;\n    }\n    else {\n        recv_count = recv_count_1;\n    }\n    if (rank == 0) {\n        send_count = send_count_1;\n    }\n    else {\n        send_count = send_count_2;\n    }\n    MPI_Irecv(bins.data(), 256, MPI_INT, left, tag, MPI_COMM_WORLD, &request[0]);\n    MPI_Isend(bins.data(), 256, MPI_INT, right, tag, MPI_COMM_WORLD, &request[1]);\n    if (rank == 0) {\n        count = std::accumulate(image.begin() + image.size() / 2, image.end(), 0);\n    }\n    else {\n        count = std::accumulate(image.begin(), image.begin() + image.size() / 2, 0);\n    }\n    bins[image[rank * (image",
            "/*\n    int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t num_pixels = image.size() / num_procs;\n    size_t start = rank * num_pixels;\n    size_t end = (rank + 1) * num_pixels;\n    if (rank == num_procs - 1) end = image.size();\n\n    // calculate histogram\n    std::array<size_t, 256> histogram;\n    for (size_t i = start; i < end; ++i) {\n      histogram[image[i]]++;\n    }\n\n    // merge results\n    MPI_Reduce(\n      MPI_IN_PLACE,\n      histogram.data(),\n      256,\n      MPI_UNSIGNED_LONG,\n      MPI_SUM,\n      0,\n      MPI_COMM_WORLD\n    );\n    MPI_Bcast(\n      bins.data(),\n      256,\n      MPI_UNSIGNED_LONG,\n      0,\n      MPI_COMM_WORLD\n    );\n  */\n}",
            "int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  // TODO: count in parallel\n}",
            "// Your code here\n}",
            "/* TODO */\n}",
            "// TODO: Implement this function.\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int number_pixels_per_rank = image.size() / size;\n   int remainder = image.size() % size;\n\n   std::vector<int> pixels_per_rank(number_pixels_per_rank + (rank < remainder));\n   for (int i = 0; i < pixels_per_rank.size(); i++) {\n     pixels_per_rank[i] = image[i + number_pixels_per_rank * rank];\n   }\n\n   int number_bins;\n   MPI_Allreduce(&pixels_per_rank, &bins, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n }",
            "std::vector<size_t> local_bins(256, 0);\n  for (auto pixel : image) {\n    local_bins[pixel]++;\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* YOUR CODE HERE */\n   int nProc, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nProc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int chunk_size = (image.size()) / nProc;\n   int start = rank * chunk_size;\n   int end = start + chunk_size;\n   std::array<size_t, 256> local_bins{};\n   for (int i = start; i < end; i++) {\n     int pixel = image[i];\n     local_bins[pixel]++;\n   }\n   MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: Implement this function\n    // You will need to use MPI_Reduce, MPI_Bcast, MPI_Gather, MPI_Gatherv, and MPI_Allgatherv\n\n    // TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> local_bins(256, 0);\n  size_t offset = rank * image.size() / size;\n  size_t size_per_rank = (rank + 1) * image.size() / size - offset;\n\n  for (size_t i = 0; i < image.size(); i++) {\n    local_bins[image[i]]++;\n  }\n\n  // sum up local bins\n  std::vector<size_t> local_sum(local_bins.size());\n  MPI_Allreduce(local_bins.data(), local_sum.data(), local_bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // put into global bins\n  for (size_t i = 0; i < local_sum.size(); i++) {\n    bins[i] += local_sum[i];\n  }\n}",
            "// 1. Send the data to the other processors\n  MPI_Datatype data_type;\n  MPI_Type_contiguous(sizeof(int), MPI_BYTE, &data_type);\n  MPI_Type_commit(&data_type);\n\n  size_t my_count = image.size();\n\n  if (my_count % 2!= 0) {\n    my_count++;\n  }\n\n  int partner = 1;\n  MPI_Send(&my_count, 1, MPI_INT, partner, 0, MPI_COMM_WORLD);\n\n  MPI_Send(&image[0], my_count, data_type, partner, 0, MPI_COMM_WORLD);\n\n  // 2. Receive data from the other processors\n  std::vector<int> result(256);\n\n  for (auto &i : result) {\n    i = 0;\n  }\n\n  int partner_size;\n  MPI_Recv(&partner_size, 1, MPI_INT, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  if (partner_size % 2!= 0) {\n    partner_size++;\n  }\n\n  std::vector<int> partner_result(partner_size);\n\n  MPI_Recv(&partner_result[0], partner_size, data_type, partner, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  // 3. Count in parallel\n  for (size_t i = 0; i < partner_result.size(); i+=2) {\n    result[partner_result[i]] += partner_result[i + 1];\n  }\n\n  // 4. Send data back to processor 0\n  MPI_Send(&result[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\n  // 5. Get result back to processor 0 and store it in bins\n  MPI_Recv(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int image_size = image.size();\n    int chunk_size = (image_size + size - 1) / size;\n\n    std::vector<int> local_image;\n    if (rank == 0) {\n        local_image.assign(image.begin(), image.begin() + chunk_size);\n    } else {\n        local_image.assign(image.begin() + rank * chunk_size, image.begin() + (rank + 1) * chunk_size);\n    }\n\n    std::array<size_t, 256> local_bins = {0};\n\n    for (auto const& p : local_image) {\n        local_bins[p]++;\n    }\n\n    std::vector<size_t> all_bins;\n    MPI_Gather(&local_bins, 256, MPI_UNSIGNED_LONG, &all_bins, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins.assign(all_bins.begin(), all_bins.begin() + 256);\n    }\n}",
            "/* your code here */\n }",
            "// MPI_Init(nullptr, nullptr);\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // get number of elements\n   int count = image.size();\n\n   // number of pixels per process\n   int num_pixels_per_process = count / size;\n\n   // remainder, number of pixels left over\n   int remainder = count % size;\n\n   // offsets for each process\n   int my_offset = num_pixels_per_process * rank + std::min(rank, remainder);\n\n   // pixels for this process\n   std::vector<int> my_pixels;\n   my_pixels.insert(my_pixels.end(), image.begin() + my_offset, image.begin() + my_offset + num_pixels_per_process);\n\n   // count number of pixels\n   int my_count = 0;\n   for (int i = 0; i < my_pixels.size(); i++) {\n     my_count++;\n   }\n\n   // gather results\n   std::vector<int> counts(size);\n   MPI_Allgather(&my_count, 1, MPI_INT, counts.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n   // add up the pixel counts\n   bins[0] = counts[0];\n   for (int i = 1; i < 256; i++) {\n     bins[i] = counts[i] + bins[i - 1];\n   }\n\n   // MPI_Finalize();\n }",
            "// Your code here\n}",
            "// TODO: implement this\n   int world_rank;\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   size_t const kNumBins = 256;\n   std::array<size_t, kNumBins> local_bins{};\n   std::vector<size_t> local_image{};\n   int count_local_image;\n   if (world_rank == 0) {\n     local_image.insert(local_image.end(), image.begin(), image.end());\n   }\n   MPI_Bcast(&count_local_image, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   local_image.resize(count_local_image);\n   MPI_Scatter(local_image.data(), count_local_image, MPI_INT, local_bins.data(), count_local_image, MPI_INT, 0, MPI_COMM_WORLD);\n   for (size_t i = 0; i < local_image.size(); ++i) {\n     ++local_bins[local_image[i]];\n   }\n   MPI_Gather(local_bins.data(), kNumBins, MPI_UNSIGNED_LONG_LONG, bins.data(), kNumBins, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n }",
            "// *** INSERT YOUR CODE HERE ***\n}",
            "// Your code here\n }",
            "// TODO: implement\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = image.size();\n    int num_of_pixels = n / size;\n    std::vector<int> rank_image(num_of_pixels);\n\n    int offset = rank * num_of_pixels;\n    int total = 0;\n    for (int i = 0; i < num_of_pixels; i++) {\n        rank_image[i] = image[offset + i];\n    }\n\n    std::array<size_t, 256> rank_bins;\n    std::fill(rank_bins.begin(), rank_bins.end(), 0);\n\n    for (int i = 0; i < num_of_pixels; i++) {\n        rank_bins[rank_image[i]] += 1;\n    }\n    MPI_Reduce(rank_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n}",
            "// TODO: Your code here.\n    int rank, size;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<int> myImage = std::vector<int>(image.begin() + size * rank, image.begin() + size * (rank + 1));\n\n    size_t binsSum;\n    binsSum = std::accumulate(myImage.begin(), myImage.end(), 0);\n    bins[myImage[0]] = binsSum;\n\n    std::vector<int> localBins = std::vector<int>(bins.begin(), bins.end());\n\n    MPI_Reduce(localBins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::vector<int> local_image = image;\n   std::vector<size_t> local_bins(bins.begin(), bins.end());\n   MPI_Datatype MPI_TYPE_INT = MPI_INT;\n\n   // 1. Split image vector into local vectors.\n   // 2. Count number of pixels in each image vector.\n   // 3. Send counts to rank 0.\n   // 4. Reduce results on rank 0.\n\n }",
            "// TODO: Implement\n    // bins[i] is the number of pixels with grayscale value i\n    bins[image[0]] = 1;\n    for(size_t i=1; i<image.size(); i++) {\n        bins[image[i]]++;\n    }\n}",
            "std::vector<int> local_image(image);\n     // TODO\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::array<size_t, 256> temp;\n   std::fill(temp.begin(), temp.end(), 0);\n\n   int count = image.size() / size;\n   int rest = image.size() % size;\n\n   std::vector<int> image_copy = image;\n\n   for (int i = 0; i < rest; i++) {\n     MPI_Send(&image_copy[i * count + i], count, MPI_INT, i, rank, MPI_COMM_WORLD);\n   }\n\n   for (int i = 0; i < count; i++) {\n     MPI_Send(&image_copy[i * count + i + rest], count, MPI_INT, count + i, rank, MPI_COMM_WORLD);\n   }\n\n   for (int i = 0; i < 256; i++) {\n     for (int j = 0; j < size; j++) {\n       int size = 0;\n       MPI_Status status;\n       MPI_Probe(j, rank, MPI_COMM_WORLD, &status);\n       MPI_Get_count(&status, MPI_INT, &size);\n       if (size > 0) {\n         MPI_Recv(&temp[i], size, MPI_INT, j, rank, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       }\n     }\n   }\n\n   if (rank == 0) {\n     for (int i = 0; i < 256; i++) {\n       bins[i] = temp[i];\n     }\n   }\n }",
            "// TODO: implement\n}",
            "// Your code goes here...\n }",
            "// TODO: implement\n   // The following code works but has not been optimized for speed.\n   // Do not change this code.\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   if(image.size() % size!= 0) throw std::runtime_error(\"image.size() % size!= 0\");\n   int nPixels = image.size() / size;\n   std::vector<int> imageRank(nPixels);\n   if(rank == 0)\n   {\n     imageRank.assign(image.begin(), image.end());\n   }\n   MPI_Scatter(imageRank.data(), nPixels, MPI_INT, image.data(), nPixels, MPI_INT, 0, MPI_COMM_WORLD);\n   for(int i = 0; i < nPixels; i++) {\n     bins[image[i]]++;\n   }\n   MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "// TODO: Replace this code with your solution\n   // MPI init code\n   int rank, world_size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   // End MPI init code\n\n   // Each process will compute bins[i]\n   int bin = 0;\n   for (auto i : image) {\n     bins[i]++;\n   }\n\n   // Collect all of the counts\n   std::vector<size_t> counts(256, 0);\n   MPI_Reduce(bins.data(), counts.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Copy from counts to bins\n   if (rank == 0) {\n     for (size_t i = 0; i < counts.size(); i++) {\n       bins[i] = counts[i];\n     }\n   }\n }",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int length = image.size();\n    int chunk = length / size;\n\n    int lower = chunk * rank;\n    int upper = chunk * (rank + 1);\n\n    if (rank == size - 1) {\n        upper = length;\n    }\n\n    std::vector<int> image_sub(upper - lower);\n\n    std::copy(image.begin() + lower, image.begin() + upper, image_sub.begin());\n\n    std::array<size_t, 256> bins_sub;\n    std::fill(bins_sub.begin(), bins_sub.end(), 0);\n\n    int pixels_sub = std::accumulate(image_sub.begin(), image_sub.end(), 0);\n\n    // Each rank does a local count\n    for (auto pixel: image_sub) {\n        bins_sub[pixel]++;\n    }\n\n    MPI_Reduce(bins_sub.data(), bins.data(), bins_sub.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // All ranks have a complete copy of image, so the final sum is the same as the subtotal\n        bins[0] = pixels_sub;\n    }\n}",
            "// TODO: implement this\n  int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  int total_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &total_rank);\n  size_t n_image = image.size();\n  size_t n_bins = bins.size();\n  int n_per_process = n_image / total_rank;\n\n  // split the image to every process\n  std::vector<int> image_local(n_per_process, 0);\n  std::vector<int> temp_local(n_per_process, 0);\n  for (size_t i = 0; i < n_per_process; i++) {\n    image_local[i] = image[i + n_per_process * my_rank];\n  }\n  // count the number of pixels in every bin\n  for (int i = 0; i < n_per_process; i++) {\n    temp_local[i] = image_local[i] + 1;\n  }\n\n  // get the sum of temp_local\n  size_t local_total = std::accumulate(temp_local.begin(), temp_local.end(), 0);\n  // get the global total\n  size_t global_total;\n  MPI_Allreduce(&local_total, &global_total, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n  // get the local offset\n  size_t local_offset = std::accumulate(temp_local.begin(), temp_local.begin() + my_rank, 0);\n  // get the global offset\n  size_t global_offset;\n  MPI_Allreduce(&local_offset, &global_offset, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // calculate the bins\n  for (size_t i = 0; i < n_per_process; i++) {\n    bins[image_local[i] + global_offset] += temp_local[i];\n  }\n}",
            "//... your code here\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int size = image.size();\n    int part = size / world_size;\n    std::vector<int> local_image;\n    if (world_rank == 0) {\n        for (int i = 0; i < world_size; i++) {\n            if (i == 0)\n                local_image = std::vector<int>(image.begin(), image.begin() + part);\n            else if (i == world_size - 1)\n                local_image = std::vector<int>(image.begin() + part * i, image.end());\n            else\n                local_image = std::vector<int>(image.begin() + part * i, image.begin() + part * (i + 1));\n            int k = 0;\n            for (int j = 0; j < local_image.size(); j++) {\n                bins[local_image[j]] += 1;\n                k += 1;\n            }\n        }\n    } else {\n        local_image = std::vector<int>(image.begin() + part * world_rank, image.begin() + part * (world_rank + 1));\n        int k = 0;\n        for (int j = 0; j < local_image.size(); j++) {\n            bins[local_image[j]] += 1;\n            k += 1;\n        }\n    }\n}",
            "// TODO\n}",
            "// write your solution here\n }",
            "int n_processes;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &n_processes);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (n_processes > image.size())\n        n_processes = image.size();\n\n    // TODO: Your code here.\n}",
            "std::array<size_t, 256> bins_local;\n  for (size_t i = 0; i < image.size(); i++) {\n    bins_local[image[i]]++;\n  }\n  MPI_Reduce(bins_local.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t total = image.size();\n\tsize_t n = total/2;\n\tint rank, nprocs;\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint *counts = new int[nprocs];\n\tint *disps = new int[nprocs];\n\tint *scounts = new int[nprocs];\n\tint *sdisps = new int[nprocs];\n\tint *dcounts = new int[nprocs];\n\tint *ddisps = new int[nprocs];\n\tint *recvcounts = new int[nprocs];\n\tint *recvdisps = new int[nprocs];\n\tint *recvs = new int[nprocs];\n\tint *srecvs = new int[nprocs];\n\tint *srecvcounts = new int[nprocs];\n\tint *srecvdisps = new int[nprocs];\n\n\tif(rank==0){\n\t\t// std::cout << \"total number of pixels: \" << total << std::endl;\n\t\t// std::cout << \"size of image: \" << image.size() << std::endl;\n\t\t// std::cout << \"number of procs: \" << nprocs << std::endl;\n\t\t// std::cout << \"n: \" << n << std::endl;\n\t\t// std::cout << \"size of counts: \" << sizeof(counts)/sizeof(*counts) << std::endl;\n\t\t// std::cout << \"size of disps: \" << sizeof(disps)/sizeof(*disps) << std::endl;\n\t\t// std::cout << \"size of scounts: \" << sizeof(scounts)/sizeof(*scounts) << std::endl;\n\t\t// std::cout << \"size of sdisps: \" << sizeof(sdisps)/sizeof(*sdisps) << std::endl;\n\t\t// std::cout << \"size of dcounts: \" << sizeof(dcounts)/sizeof(*dcounts) << std::endl;\n\t\t// std::cout << \"size of ddisps: \" << sizeof(ddisps)/sizeof(*ddisps) << std::endl;\n\t\t// std::cout << \"size of recvcounts: \" << sizeof(recvcounts)/sizeof(*recvcounts) << std::endl;\n\t\t// std::cout << \"size of recvdisps: \" << sizeof(recvdisps)/sizeof(*recvdisps) << std::endl;\n\t\t// std::cout << \"size of recvs: \" << sizeof(recvs)/sizeof(*recvs) << std::endl;\n\t\t// std::cout << \"size of srecvs: \" << sizeof(srecvs)/sizeof(*srecvs) << std::endl;\n\t\t// std::cout << \"size of srecvcounts: \" << sizeof(srecvcounts)/sizeof(*srecvcounts) << std::endl;\n\t\t// std::cout << \"size of srecvdisps: \" << sizeof(srecvdisps)/sizeof(*srecvdisps) << std::endl;\n\t}\n\n\tif(rank==0){\n\t\tscounts[0] = n/nprocs;\n\t\tsdisps[0] = 0;\n\t\tfor(int i=1;i<nprocs;i++){\n\t\t\tscounts[i] = scounts[i-1]+(n/nprocs);\n\t\t\tsdisps[i] = sdisps[i-1]+(n/nprocs);\n\t\t}\n\t}else{\n\t\tscounts[rank] = n/(nprocs-rank);\n\t\tsdisps[rank] = sdisps[rank-1]+(n/(nprocs-rank));\n\t}\n\n\t// std::cout << \"rank: \" << rank << std::endl;\n\t// std::cout << \"scounts: \" << scounts[rank] << std::endl;\n\t// std::cout << \"sdisps: \" << sdisps[rank] << std::endl;\n\n\t// std::cout << \"scounts[0]: \" << scounts[0] << std::endl;\n\t// std::cout << \"scounts[1]: \" << scounts[1] << std::endl;\n\t// std::cout << \"scounts[2]: \" << scounts[2] << std::endl;\n\t// std::cout << \"scounts[3]: \" << scounts[3] << std::endl;\n\t// std::cout << \"scounts[4]: \"",
            "int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   std::vector<size_t> local_bins(256);\n   for (size_t i = 0; i < image.size(); ++i) {\n     ++local_bins[image[i]];\n   }\n\n   std::vector<size_t> total_bins(256);\n   MPI_Reduce(local_bins.data(), total_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (world_rank == 0) {\n     for (size_t i = 0; i < 256; ++i) {\n       bins[i] = total_bins[i];\n     }\n   }\n }",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    //int world_size = 2;\n    //int world_rank = 0;\n    std::array<std::array<size_t, 256>, 2> local_bins;\n\n    // initialize local_bins\n    for (size_t i = 0; i < 256; i++) {\n        local_bins[0][i] = 0;\n        local_bins[1][i] = 0;\n    }\n\n    for (size_t i = 0; i < image.size(); i++) {\n        local_bins[world_rank][image[i]]++;\n    }\n\n    // reduce\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // if 0, print result\nif (world_rank == 0) {\n    for (size_t i = 0; i < 256; i++) {\n        std::cout << bins[i] << \" \";\n    }\n    std::cout << std::endl;\n}\n}",
            "// TODO: implement\n  // use MPI_Gatherv\n\n}",
            "}",
            "int size, rank;\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     int n = image.size();\n     int nLocal = (n + size - 1) / size;\n     int start = rank * nLocal;\n     int end = std::min(n, (rank + 1) * nLocal);\n     bins = std::array<size_t, 256>({});\n     for (int i = start; i < end; ++i) {\n         bins[image[i]] += 1;\n     }\n }",
            "// TODO: your code goes here\n }",
            "/*\n   // Count local pixels and store in local bins\n   std::array<size_t, 256> bins{};\n   for (size_t i = 0; i < image.size(); i++) {\n     bins[image[i]]++;\n   }\n\n   // Sum values in bins across all ranks\n   std::vector<size_t> sums(bins.size());\n   MPI_Reduce(\n     bins.data(),\n     sums.data(),\n     bins.size(),\n     MPI_UNSIGNED_LONG_LONG,\n     MPI_SUM,\n     0,\n     MPI_COMM_WORLD);\n   */\n\n}",
            "// Fill in code here. \n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   MPI_Datatype dtype;\n   MPI_Type_contiguous(256, MPI_INT, &dtype);\n   MPI_Type_commit(&dtype);\n\n   std::vector<int> image_local = image;\n   size_t n = image.size();\n\n   std::vector<int> bins_local(256, 0);\n\n   MPI_Scatter(&image_local[0], 1, dtype, &bins_local[0], 1, dtype, 0, MPI_COMM_WORLD);\n\n   int pixels = image.size() / size;\n\n   MPI_Reduce(&bins_local[0], &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   MPI_Barrier(MPI_COMM_WORLD);\n }",
            "// TODO: implement this function\n    return;\n}",
            "#pragma omp parallel num_threads(NUM_THREADS)\n\t {\n\t\t std::array<size_t, 256> localBins;\n\t\t for (size_t i = 0; i < 256; i++) {\n\t\t\t localBins[i] = 0;\n\t\t }\n\t\t #pragma omp for \n\t\t for (int i = 0; i < image.size(); i++) {\n\t\t\t localBins[image[i]]++;\n\t\t }\n\t\t #pragma omp critical\n\t\t {\n\t\t\t for (size_t i = 0; i < 256; i++) {\n\t\t\t\t bins[i] += localBins[i];\n\t\t\t }\n\t\t }\n\t }\n }",
            "// TODO: implement here\n}",
            "// TODO: Fill this in\n\t// TODO: Use MPI to split the image amongst all ranks, count all pixels with every grayscale intensity\n\t// TODO: Store the results in bins on rank 0\n\t// TODO: Use MPI to sum across all ranks\n\tstd::array<int, 256> localBins;\n\tfor (auto i = 0; i < image.size(); i++)\n\t\tlocalBins[image[i]]++;\n\tMPI_Reduce(localBins.data(), bins.data(), localBins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    // Create a new vector with `size` elements, each element is 0\n    std::vector<size_t> local_bins(256, 0);\n\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            local_bins[i] = std::count(image.begin(), image.end(), i);\n        }\n    }\n\n    MPI_Scatter(local_bins.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function.\n\n    // TODO: your code goes here.\n}",
            "// TODO: write your code here\n    \n    return;\n}",
            "int rank, n;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n    int pixelsPerRank = image.size()/n;\n\n    if(rank == 0) {\n        std::array<int, 256> tempBins = {0};\n        for(int i=0; i<n; i++) {\n            MPI_Scatter(image.data() + i*pixelsPerRank, pixelsPerRank, MPI_INT, tempBins.data(), pixelsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n\n            for(auto const& [key, value] : tempBins) {\n                bins[key] += value;\n            }\n        }\n    } else {\n        MPI_Scatter(image.data(), pixelsPerRank, MPI_INT, bins.data(), pixelsPerRank, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n }",
            "const int image_size = image.size();\n    const int rank = 0;\n    const int nprocs = 1;\n    \n    std::vector<int> local_image(image_size);\n    int local_sum = 0;\n    \n    if (rank == 0) {\n        for (int proc = 0; proc < nprocs; proc++) {\n            // Receive counts\n            MPI_Recv(local_image.data(), image_size, MPI_INT, proc, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            // Compute local sum\n            for (size_t i = 0; i < image_size; i++) {\n                local_sum += local_image[i];\n            }\n        }\n    } else {\n        // Send data\n        MPI_Send(image.data(), image_size, MPI_INT, rank, 0, MPI_COMM_WORLD);\n    }\n\n    // Get the local sum\n    MPI_Reduce(&local_sum, bins.data(), 256, MPI_INT, MPI_SUM, rank, MPI_COMM_WORLD);\n    MPI_Bcast(bins.data(), 256, MPI_INT, rank, MPI_COMM_WORLD);\n}",
            "// TODO\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   std::vector<int> counts(256, 0);\n   int len = image.size();\n   if (rank == 0) {\n   \tfor (int i = 0; i < len; ++i) {\n      counts[image[i]]++;\n    }\n   \tMPI_Scatter(counts.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n   } else {\n   \tMPI_Scatter(counts.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n}",
            "// get the rank and the number of processes\n  int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  \n  // get the length of the image\n  size_t length = image.size();\n  \n  // get the start and end indices\n  size_t start_index = rank * (length / num_procs);\n  size_t end_index = (rank + 1) * (length / num_procs);\n\n  // get the local histogram\n  std::array<size_t, 256> local_bins;\n  for (int i = 0; i < 256; i++)\n    local_bins[i] = 0;\n\n  for (int i = start_index; i < end_index; i++)\n    local_bins[image[i]]++;\n  \n  // sum the histograms on rank 0\n  if (rank == 0) {\n    std::array<size_t, 256> global_bins;\n    for (int i = 0; i < 256; i++)\n      global_bins[i] = 0;\n\n    MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = global_bins;\n  } else {\n    MPI_Reduce(local_bins.data(), nullptr, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "bins = std::array<size_t, 256>();\n    size_t numPixels = image.size();\n\n    for (size_t i = 0; i < numPixels; i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "// TODO: implement this function\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   const size_t n = image.size();\n   int start = n*rank/size;\n   int end = n*(rank+1)/size;\n   std::array<size_t, 256> localBins;\n   for (size_t i=start; i<end; ++i) {\n     ++localBins[image[i]];\n   }\n   MPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: implement me\n   MPI_Status status;\n   int local_bin[256];\n   for (int i = 0; i < 256; i++) local_bin[i] = 0;\n   for (size_t i = 0; i < image.size(); i++) local_bin[image[i]]++;\n   MPI_Reduce(local_bin, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    size_t height = image.size();\n    int chunk = height / size;\n    int start = rank * chunk;\n    int end = (rank == size - 1)? height : (rank + 1) * chunk;\n    std::vector<int> chunk_image(image.begin() + start, image.begin() + end);\n    std::for_each(chunk_image.begin(), chunk_image.end(), [&bins](int gray_value) { bins[gray_value] += 1; });\n    std::vector<size_t> sum(256);\n    MPI_Reduce(bins.data(), sum.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = sum;\n    }\n}",
            "// TODO: Your code here.\n  // Your code should be a single MPI_Gather call, so that each rank has the result.\n  // You should not need to modify this function.\n\n  // TODO: Your code here.\n  // Your code should be a single MPI_Gather call, so that each rank has the result.\n  // You should not need to modify this function.\n  MPI_Gather(\n    image.data(),\n    image.size(),\n    MPI_INT,\n    bins.data(),\n    image.size(),\n    MPI_INT,\n    0,\n    MPI_COMM_WORLD\n  );\n}",
            "// Your code goes here...\n    bins.fill(0);\n\n    MPI_Comm comm = MPI_COMM_WORLD;\n    MPI_Datatype image_type;\n    MPI_Type_contiguous(256, MPI_INT, &image_type);\n    MPI_Type_commit(&image_type);\n    int rank, size;\n    MPI_Comm_size(comm, &size);\n    MPI_Comm_rank(comm, &rank);\n    int n = image.size();\n    int block_size = n / size;\n    int remainder = n % size;\n    int begin = block_size * rank;\n    if (rank < remainder) {\n        begin += rank;\n    } else {\n        begin += remainder;\n    }\n\n    std::vector<int> local_image(block_size);\n    MPI_Scatter(image.data(), block_size, image_type, local_image.data(), block_size, image_type, 0, comm);\n\n    for (int i = 0; i < block_size; ++i) {\n        ++bins[local_image[i]];\n    }\n\n    std::vector<std::array<size_t, 256>> local_bins(1);\n    MPI_Gather(bins.data(), 256, MPI_INT, local_bins.data(), 256, MPI_INT, 0, comm);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            for (int j = 0; j < 256; ++j) {\n                local_bins[0][j] += local_bins[i][j];\n            }\n        }\n        bins = local_bins[0];\n    }\n}",
            "// Number of ranks\n    int ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n    // Get rank\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Get the number of pixels in the image\n    size_t pixels = image.size();\n\n    // Calculate the number of pixels that each rank will calculate\n    size_t pixels_per_rank = pixels / ranks;\n\n    // If rank = 0, then the first rank will process the remaining pixels\n    size_t extra = pixels - pixels_per_rank * ranks;\n\n    // If rank = 0, the number of pixels for this rank will be\n    // `pixels_per_rank + extra`\n    // If rank > 0, the number of pixels for this rank will be\n    // `pixels_per_rank`\n\n    // The pixel count for this rank is stored in the `bin` array\n    std::array<size_t, 256> bin = {0};\n\n    // If rank == 0, then this rank will calculate the pixel counts for the\n    // first `extra` pixels.\n    for (size_t i = 0; i < extra; i++) {\n        bin[image[i]] += 1;\n    }\n\n    // If rank > 0, then this rank will calculate the pixel counts for the\n    // remaining `pixels_per_rank` pixels.\n    for (size_t i = extra; i < extra + pixels_per_rank; i++) {\n        bin[image[i]] += 1;\n    }\n\n    // Reduce the `bin` array to a total count on rank 0\n    MPI_Reduce(bin.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int rank = 0;\n   const int size = 0;\n   \n   // TODO: implement this function\n }",
            "// TODO: compute the histogram by dividing image into equal segments and count the number of pixels in each\n   size_t num_pixels = image.size();\n   int rank;\n   int size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int div = num_pixels / size;\n   int remain = num_pixels % size;\n   std::vector<int> segment(image.begin() + rank * (div + remain), image.begin() + (rank + 1) * (div + remain));\n   std::array<size_t, 256> result = std::array<size_t, 256>();\n   for (int i = 0; i < segment.size(); i++) {\n     result[segment[i]]++;\n   }\n   MPI_Reduce(result.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO\n}",
            "/* TODO: Implement this function */\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // std::cout << \"Hello World from rank \" << rank << std::endl;\n\n   if (rank == 0) {\n     int sendcounts[size];\n     int recvcounts[size];\n     int recvdispl[size];\n     int senddispl[size];\n     int counts;\n     int displ;\n\n     int n = image.size();\n     // std::cout << \"size of vector image : \" << n << std::endl;\n\n     // Counting\n     counts = 1;\n     for (int i = 0; i < size; i++) {\n       sendcounts[i] = counts;\n       counts += n;\n     }\n\n     // Calculate displacement\n     displ = 0;\n     for (int i = 0; i < size; i++) {\n       recvcounts[i] = 0;\n       senddispl[i] = displ;\n       recvdispl[i] = displ;\n       for (int j = 0; j < i; j++) {\n         recvcounts[i] += sendcounts[j];\n         recvdispl[i] += recvcounts[j];\n       }\n       displ += sendcounts[i];\n     }\n\n     // Sendrecv\n     int *counts_out = new int[n];\n     int *counts_in = new int[n];\n     int *displ_out = new int[n];\n     int *displ_in = new int[n];\n     MPI_Gatherv(sendcounts, size, MPI_INT, counts_out, sendcounts, senddispl, MPI_INT, 0, MPI_COMM_WORLD);\n     MPI_Gatherv(recvcounts, size, MPI_INT, counts_in, recvcounts, recvdispl, MPI_INT, 0, MPI_COMM_WORLD);\n     MPI_Gatherv(senddispl, size, MPI_INT, displ_out, sendcounts, senddispl, MPI_INT, 0, MPI_COMM_WORLD);\n     MPI_Gatherv(recvdispl, size, MPI_INT, displ_in, recvcounts, recvdispl, MPI_INT, 0, MPI_COMM_WORLD);\n     int n_out = counts_out[0];\n     int n_in = counts_in[0];\n     delete[] counts_out;\n     delete[] counts_in;\n     delete[] displ_out;\n     delete[] displ_in;\n\n     // std::cout << \"size of vector image : \" << n << std::endl;\n     // std::cout << \"size of vector output : \" << n_out << std::endl;\n     // std::cout << \"size of vector input : \" << n_in << std::endl;\n\n     int *image_out = new int[n_out];\n     int *image_in = new int[n_in];\n     // std::cout << \"Rank \" << rank << \" : \" << displ_out[rank] << \" : \" << recvdispl[rank] << std::endl;\n     // std::cout << \"Rank \" << rank << \" : \" << displ_in[rank] << \" : \" << recvdispl[rank] << std::endl;\n\n     MPI_Gatherv(image.data(), n, MPI_INT, image_out, counts_out, displ_out, MPI_INT, 0, MPI_COMM_WORLD);\n     MPI_Gatherv(image.data(), n, MPI_INT, image_in, counts_in, displ_in, MPI_INT, 0, MPI_COMM_WORLD);\n\n     // Counting\n     for (int i = 0; i < 256; i++) {\n       bins[i] = 0;\n     }\n\n     for (int i = 0; i < n_out; i++) {\n       bins[image_out[i]] += 1;\n     }\n\n     // delete[] image_out;\n     // delete[] image_in;\n   } else {\n     MPI_Gatherv(image.data(), image.size(), MPI_INT, nullptr, nullptr, nullptr, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO\n}",
            "/* Your solution goes here */\n}",
            "size_t size = image.size();\n    std::vector<int> localImage(image.begin(), image.end());\n    MPI_Scatter(localImage.data(), image.size(), MPI_INT, localImage.data(), image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    //count\n    for (size_t i = 0; i < size; i++) {\n        int pixel = localImage[i];\n        bins[pixel]++;\n    }\n}",
            "// TODO: your code here\n}",
            "// TODO: Your code here!\n }",
            "/* Your code here. */\n  }",
            "int rank, nproc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n   // Compute local histogram\n   // TODO: YOUR CODE HERE\n\n   // Gather results\n   // TODO: YOUR CODE HERE\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Counts local grayscale values\n    std::array<size_t, 256> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n    for (auto i = 0; i < image.size(); i++) {\n        localBins[image[i]]++;\n    }\n\n    // Send counts to root\n    std::vector<size_t> counts(localBins.begin(), localBins.end());\n    if (rank == 0) {\n        std::vector<size_t> allCounts(size * counts.size());\n        MPI_Gather(counts.data(), counts.size(), MPI_UNSIGNED_LONG,\n            allCounts.data(), counts.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n        for (auto i = 0; i < 256; i++) {\n            bins[i] = 0;\n            for (auto j = 0; j < size; j++) {\n                bins[i] += allCounts[i * size + j];\n            }\n        }\n    }\n    else {\n        MPI_Gather(counts.data(), counts.size(), MPI_UNSIGNED_LONG,\n            nullptr, counts.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "MPI_Datatype intType;\n    MPI_Type_contiguous(256, MPI_UNSIGNED_INT, &intType);\n    MPI_Type_commit(&intType);\n\n    // TODO: implement pixelCounts.\n    // The counts vector has been initialized for you.\n    // The results should be stored in the vector bins.\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Status status;\n    int num_pixels = image.size();\n\n    // MPI_Scatterv - Gatherv\n    int recvcounts[size];\n    int displs[size];\n    if (rank == 0) {\n        for (size_t i = 0; i < size; ++i) {\n            recvcounts[i] = (num_pixels + size - 1) / size;\n            displs[i] = i * (num_pixels + size - 1) / size;\n        }\n        recvcounts[size - 1] += num_pixels - (size - 1) * (num_pixels + size - 1) / size;\n    }\n\n    int* sendcounts = nullptr;\n    int* senddispls = nullptr;\n    if (rank!= 0) {\n        sendcounts = new int[size];\n        senddispls = new int[size];\n        for (size_t i = 0; i < size; ++i) {\n            sendcounts[i] = recvcounts[i];\n            senddispls[i] = displs[i];\n        }\n    }\n\n    // std::vector<int> sendbuf;\n    // if (rank == 0) {\n    //     sendbuf = image;\n    // }\n\n    std::vector<int> recvbuf(recvcounts[rank]);\n    MPI_Scatterv(image.data(), sendcounts, senddispls, intType,\n                 recvbuf.data(), recvcounts[rank], intType, 0, MPI_COMM_WORLD);\n\n    // std::vector<int> recvbuf(recvcounts[rank]);\n    // MPI_Scatter(image.data(), num_pixels, intType,\n    //             recvbuf.data(), recvcounts[rank], intType, 0, MPI_COMM_WORLD);\n\n    std::vector<int> sendbuf(recvcounts[rank]);\n    if (rank!= 0) {\n        for (size_t i = 0; i < recvcounts[rank]; ++i) {\n            sendbuf[i] = 1;\n        }\n    }\n\n    // std::vector<int> recvbuf(recvcounts[rank]);\n    // MPI_Scatter(image.data(), image.size(), intType,\n    //             recvbuf.data(), recvcounts[rank], intType, 0, MPI_COMM_WORLD);\n\n    // std::vector<int> recvbuf;\n    // MPI_Scatter(image.data(), image.size(), intType,\n    //             recvbuf.data(), recvcounts[rank], intType, 0, MPI_COMM_WORLD);\n\n    // std::vector<int> recvbuf(recvcounts[rank]);\n    // MPI_Scatterv(image.data(), recvcounts, displs, intType,\n    //              recvbuf.data(), recvcounts[rank], intType, 0, MPI_COMM_WORLD);\n\n    std::vector<int> recvcounts_vector(recvcounts, recvcounts + size);\n    std::vector<int> displs_vector(displs, displs + size);\n\n    MPI_Gatherv(sendbuf.data(), recvcounts[rank], intType,\n                recvbuf.data(), recvcounts_vector.data(), displs_vector.data(),\n                intType, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < recvcounts[rank]; ++i) {\n            ++bins[recvbuf[i]];\n        }\n    }\n\n    MPI_Type_free(&intType);\n}",
            "/* TODO: Your code here */\n    MPI_Datatype my_type;\n    MPI_Type_contiguous(sizeof(int), MPI_INT, &my_type);\n    MPI_Type_commit(&my_type);\n\n    MPI_Win win;\n    MPI_Win_create(bins.data(), 256 * sizeof(int), sizeof(int), MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n    MPI_Win_fence(0, win);\n    MPI_Accumulate(image.data(), image.size(), MPI_INT, 0, 0, image.size(), MPI_INT, MPI_SUM, win);\n    MPI_Win_fence(0, win);\n    MPI_Win_free(&win);\n\n    MPI_Type_free(&my_type);\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int pixels = image.size();\n  int stride = pixels / size;\n  std::vector<int> send(stride);\n  std::vector<int> recv(stride);\n  int counter = 0;\n  for (int i = 0; i < size; ++i) {\n    std::copy(image.begin() + (i * stride), image.begin() + ((i + 1) * stride), send.begin());\n    MPI_Scatter(send.data(), stride, MPI_INT, recv.data(), stride, MPI_INT, 0, MPI_COMM_WORLD);\n    for (auto pixel : recv)\n      ++bins[pixel];\n    if (rank == 0) {\n      for (int j = 0; j < stride; ++j) {\n        counter += recv[j];\n      }\n      MPI_Gather(&counter, 1, MPI_INT, recv.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n    MPI_Bcast(recv.data(), stride, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(recv.data(), stride, MPI_INT, send.data(), stride, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(send.data(), stride, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n      for (int k = 0; k < size; ++k) {\n        bins[k] += send[k];\n      }\n    }\n  }\n}",
            "std::array<int,256> localbins;\n\t//fill in array with zeros\n\tfor(int i =0; i < 256; i++){\n\t\tlocalbins[i] = 0;\n\t}\n\n\tfor (int x = 0; x < image.size(); x++){\n\t\tlocalbins[image[x]]++;\n\t}\n\n\tbins = localbins;\n}",
            "std::array<size_t, 256> bins_local{};\n    // YOUR CODE HERE\n    // std::array<size_t, 256> bins_local{};\n    // MPI_Init(NULL, NULL);\n    // int world_size, world_rank;\n    // MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    // MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    // int n = image.size();\n    // int chunk = n / world_size;\n    // int first = world_rank * chunk;\n    // int last = (world_rank + 1) * chunk - 1;\n    // if (world_rank == world_size - 1)\n    //     last = n - 1;\n    // for (size_t i = first; i <= last; i++)\n    //     bins_local[image[i]]++;\n    // std::array<size_t, 256> bins_global{};\n    // MPI_Reduce(bins_local.data(), bins_global.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    // bins = bins_global;\n    // MPI_Finalize();\n    // for (size_t i = 0; i < bins.size(); i++)\n    //     std::cout << i << \" : \" << bins[i] << std::endl;\n    // int count = 0;\n    // for (size_t i = 0; i < n; i++)\n    // {\n    //     int gray_value = image[i];\n    //     bins_local[gray_value]++;\n    //     count++;\n    //     if (count == chunk)\n    //     {\n    //         std::array<size_t, 256> bins_global{};\n    //         MPI_Reduce(bins_local.data(), bins_global.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    //         bins = bins_global;\n    //         count = 0;\n    //     }\n    // }\n    std::fill(bins_local.begin(), bins_local.end(), 0);\n    for (size_t i = 0; i < image.size(); i++)\n    {\n        int gray_value = image[i];\n        bins_local[gray_value]++;\n    }\n    std::array<size_t, 256> bins_global{};\n    MPI_Reduce(bins_local.data(), bins_global.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = bins_global;\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t//each process gets the chunk\n\tint local_bins[256];\n\tstd::fill(local_bins, local_bins+256, 0);\n\n\tfor (int i = rank; i < image.size(); i += size) {\n\t\tlocal_bins[image[i]]++;\n\t}\n\t//now each process has its own local counts\n\t//send the result to rank 0\n\tMPI_Gather(local_bins, 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\t//now rank 0 has all counts\n\t\t//use a local histogram\n\t\tstd::array<size_t, 256> local_hist;\n\t\tstd::fill(local_hist.begin(), local_hist.end(), 0);\n\t\t//now we need to count to total\n\t\tfor (size_t i = 0; i < image.size(); i++) {\n\t\t\tlocal_hist[image[i]]++;\n\t\t}\n\t\t//now we add the local hist to the global hist\n\t\tfor (size_t i = 0; i < 256; i++) {\n\t\t\tbins[i] += local_hist[i];\n\t\t}\n\t}\n }",
            "// YOUR CODE HERE\n \n}",
            "const int num_ranks = 2;\n    const int num_pixels = image.size();\n    // get rank\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    // get rank counts\n    int count_pixels = num_pixels / num_ranks;\n    // last rank gets the remainder\n    if (my_rank == num_ranks - 1) {\n        count_pixels += num_pixels % num_ranks;\n    }\n    // create a local image for this rank\n    std::vector<int> local_image;\n    // local_image.reserve(count_pixels);\n    for (int i = 0; i < count_pixels; i++) {\n        local_image.push_back(image[i]);\n    }\n    // count pixels\n    std::array<size_t, 256> local_bins;\n    for (int i = 0; i < local_image.size(); i++) {\n        local_bins[local_image[i]] += 1;\n    }\n    // reduce bins\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n    int rank;\n    int nproc;\n    int i = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    std::array<size_t, 256> local_bins;\n    for(i = 0; i < 256; i++)\n    {\n        local_bins[i] = 0;\n    }\n    int width = image.size() / nproc;\n    std::vector<int> local_image(image.begin() + rank * width, image.begin() + (rank + 1) * width);\n    for(auto &val: local_image)\n    {\n        local_bins[val] += 1;\n    }\n    std::vector<size_t> sendcounts(nproc);\n    std::vector<size_t> displs(nproc);\n    for(int i = 0; i < nproc; i++)\n    {\n        sendcounts[i] = 256;\n        displs[i] = i * 256;\n    }\n    MPI_Gatherv(&local_bins[0], 256, MPI_UNSIGNED_LONG, &bins[0], &sendcounts[0], &displs[0], MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n }",
            "// YOUR CODE HERE\n}",
            "int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n     std::array<int, 256> counts;\n     // Initialize counts\n     if (rank == 0) {\n         for (int i = 0; i < 256; ++i) {\n             counts[i] = 0;\n         }\n     }\n\n     // For each pixel in image, update count\n     for (int i = 0; i < image.size(); ++i) {\n         int pixel = image[i];\n         MPI_Send(&pixel, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n     }\n\n     // For each rank, update count based on pixels that it has\n     MPI_Status status;\n     int pixel;\n     for (int i = 0; i < image.size(); ++i) {\n         MPI_Recv(&pixel, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n         counts[pixel] += 1;\n     }\n\n     // Add up all the counts on each rank\n     for (int i = 1; i < size; ++i) {\n         std::vector<int> temp(256);\n         MPI_Recv(&temp[0], 256, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n         for (int j = 0; j < 256; ++j) {\n             counts[j] += temp[j];\n         }\n     }\n\n     // Send the counts to rank 0 to store in bins\n     std::vector<int> temp(256);\n     if (rank == 0) {\n         for (int i = 0; i < 256; ++i) {\n             temp[i] = counts[i];\n         }\n     }\n     MPI_Send(&temp[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\n     // Copy bins from rank 0\n     MPI_Status status2;\n     MPI_Recv(&bins[0], 256, MPI_INT, 0, 0, MPI_COMM_WORLD, &status2);\n }",
            "// TODO: implement this function\n\n   int world_size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Get the number of pixels in the image\n   int size = image.size();\n   int delta = size / world_size;\n\n   // Create new arrays to hold the local image and the local histogram\n   std::vector<int> local_image(delta);\n   std::array<size_t, 256> local_bins;\n\n   // Copy the data to the local image and histogram\n   std::copy_n(image.begin(), delta, local_image.begin());\n\n   // Get the number of pixels in the local image\n   int local_size = delta;\n\n   // For each intensity in the local image\n   for(int i = 0; i < local_size; i++) {\n     \n     // Increase the number of pixels by one\n     local_bins[local_image[i]]++;\n   }\n\n   // Gather the local histogram on rank 0\n   MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   if(rank == 0) {\n     for(size_t i = 1; i < world_size; i++) {\n       for(size_t j = 0; j < 256; j++) {\n         bins[j] += bins[j+256];\n       }\n     }\n   }\n}",
            "//TODO: Fill in code\n  \n  // Initialize the histogram of pixel counts to all zeros\n  for (size_t i = 0; i < 256; i++)\n    bins[i] = 0;\n  \n  // Loop over all pixels in image\n  for (size_t i = 0; i < image.size(); i++)\n    bins[image[i]]++;\n}",
            "// TODO: your code goes here\n  \n}",
            "// TODO: implement this function\n  \n  // get the size of the image\n  int N = image.size();\n  \n  // number of processes\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  // get the rank\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // split image into N/p chunks\n  std::vector<int> imageChunk(N/size);\n  MPI_Scatter(image.data(), N/size, MPI_INT, imageChunk.data(), N/size, MPI_INT, 0, MPI_COMM_WORLD);\n  \n  // count the number of pixels for each grayscale intensity\n  for (int i=0; i<N/size; i++){\n      bins[imageChunk[i]]++;\n  }\n  \n  // combine the results and store in bins on rank 0\n  MPI_Reduce(bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype pixel_type;\n   MPI_Type_contiguous(1, MPI_INT, &pixel_type);\n   MPI_Type_commit(&pixel_type);\n\n   MPI_Aint lb, extent;\n   MPI_Type_get_extent(pixel_type, &lb, &extent);\n\n   MPI_Win win;\n   MPI_Win_create(bins.data(), 256*sizeof(size_t), sizeof(size_t), MPI_INFO_NULL, MPI_COMM_WORLD, &win);\n\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<int> my_image = image;\n   int start_image = image.size()/2;\n   int end_image = image.size();\n\n   int start = rank*start_image;\n   int end = (rank+1)*start_image;\n\n   std::vector<int> my_pixel_counts(256, 0);\n   std::vector<int> pixel_counts(256, 0);\n   MPI_Win_fence(0, win);\n\n   MPI_Accumulate(my_image.data() + start, end-start, MPI_INT, 0, start, end-start, MPI_INT, MPI_SUM, win);\n   MPI_Win_fence(0, win);\n\n   MPI_Win_fence(0, win);\n   MPI_Accumulate(my_pixel_counts.data(), 256, MPI_INT, 0, 0, 256, MPI_INT, MPI_SUM, win);\n   MPI_Win_fence(0, win);\n\n   MPI_Win_free(&win);\n   MPI_Type_free(&pixel_type);\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n   if (rank == 0) {\n     bins = my_pixel_counts;\n   }\n }",
            "// TODO: implement this function\n   // TODO: implement this function\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        // rank 0\n        bins.fill(0);\n        // each rank calculates its own histogram\n        for (int p = 0; p < image.size(); ++p) {\n            bins[image[p]]++;\n        }\n        // now each rank sends its histogram to rank 0\n        for (int i = 1; i < size; ++i) {\n            MPI_Send(&bins[0], 256, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        // rank > 0\n        MPI_Status status;\n        MPI_Recv(&bins[0], 256, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n    }\n}",
            "// TODO\n }",
            "// Add your code here\n   int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int n = image.size();\n   int part_size = n / world_size;\n   int start = part_size * world_rank;\n   int end = part_size * (world_rank + 1);\n   if (world_rank == world_size - 1) end = n;\n\n   std::vector<int> part;\n   part.assign(image.begin() + start, image.begin() + end);\n   std::array<size_t, 256> part_bins;\n   part_bins.fill(0);\n\n   std::for_each(part.begin(), part.end(), [&](int i) {\n     part_bins[i] += 1;\n   });\n\n   MPI_Reduce(part_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// Your code here.\n  // The MPI library provides MPI_Reduce.\n  // You need to write a function that uses it.\n  //\n  // MPI_Reduce(const void *sendbuf, void *recvbuf, int count, \n  //            MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: Your code goes here\n  size_t numprocs, myid;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n  \n  int i, size = image.size();\n  std::vector<int> localBins(256, 0);\n  if (myid == 0) {\n    for (i = 0; i < size; i++) {\n      localBins[image[i]]++;\n    }\n  }\n  MPI_Gather(&localBins[0], 256, MPI_INT, &bins[0], 256, MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // YOUR CODE HERE\n  std::vector<size_t> result = counting(image, rank);\n\n  MPI_Gather(&result[0], result.size(), MPI_INT, &bins[0], result.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::vector<size_t> temp(256);\n    MPI_Reduce(&bins[0], &temp[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = temp;\n  }\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n  int block_size = image.size() / size;\n  if (rank == 0) {\n    std::vector<int> local_bins(256);\n    for (int i = 1; i < size; ++i) {\n      MPI_Send(image.data() + i * block_size, block_size, MPI_INT, i, 0, comm);\n    }\n    for (int i = 0; i < block_size; ++i) {\n      ++local_bins[image[i]];\n    }\n    MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, comm);\n  } else {\n    MPI_Status status;\n    MPI_Recv(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 0, comm, &status);\n  }\n}",
            "// Your code goes here!\n }",
            "MPI_Datatype imageType;\n  int counts[2] = {image.size(), 256};\n  int disp[2] = {0, 0};\n  MPI_Type_create_struct(2, counts, disp, MPI_INT, &imageType);\n  MPI_Type_commit(&imageType);\n\n  int nRanks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  MPI_Bcast(image.data(), image.size(), imageType, 0, MPI_COMM_WORLD);\n\n  std::vector<size_t> local_bins(256, 0);\n\n  // Count pixels in local image\n  for (int val : image) {\n    local_bins[val]++;\n  }\n\n  if (rank == 0) {\n    // Collect local results into `bins`\n    std::vector<size_t> local_bins_all(256 * nRanks, 0);\n    MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, local_bins_all.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < 256; i++) {\n      for (int j = 0; j < nRanks; j++) {\n        bins[i] += local_bins_all[i + 256 * j];\n      }\n    }\n  } else {\n    MPI_Gather(local_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, nullptr, 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Type_free(&imageType);\n}",
            "// TODO\n }",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (rank == 0) {\n     std::array<size_t, 256> localBins{};\n     for (int i = 0; i < size; ++i) {\n       for (size_t j = 0; j < image.size(); ++j) {\n         ++localBins[image[j]];\n       }\n     }\n\n     MPI_Gather(&localBins, 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   } else {\n     MPI_Gather(image.data(), image.size(), MPI_UNSIGNED_CHAR, nullptr, 0, MPI_UNSIGNED_CHAR, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO\n   int rank, numprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   std::vector<int> local_counts(256,0);\n   if (rank == 0) {\n     local_counts = std::vector<int>(image.begin(), image.end());\n     MPI_Scatter(local_counts.data(), image.size(), MPI_INT, bins.data(), image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n   } else {\n     MPI_Scatter(local_counts.data(), image.size(), MPI_INT, bins.data(), image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n   }\n }",
            "// TODO: Your code here\n  MPI_Comm comm;\n  MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n  int size, rank;\n  MPI_Comm_size(comm, &size);\n  MPI_Comm_rank(comm, &rank);\n  int localBins[256] = {0};\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      MPI_Send(&image[i * image.size() / size], image.size() / size, MPI_INT, i, 0, comm);\n    }\n  } else {\n    MPI_Recv(&image[rank * image.size() / size], image.size() / size, MPI_INT, 0, 0, comm, MPI_STATUS_IGNORE);\n    for (int i = 0; i < image.size() / size; i++) {\n      localBins[image[rank * image.size() / size + i]]++;\n    }\n  }\n  MPI_Gather(localBins, 256, MPI_INT, bins.data(), 256, MPI_INT, 0, comm);\n  MPI_Comm_free(&comm);\n}",
            "size_t size = image.size();\n   int rank, n_procs;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int chunk = size / n_procs;\n\n   if (rank == 0) {\n     bins.fill(0);\n     for (int i = 0; i < size; ++i) {\n       ++bins[image[i]];\n     }\n   }\n\n   std::vector<int> local_pixels(chunk);\n   if (rank < size % n_procs) {\n     int last_elem = (rank + 1) * chunk;\n     for (int i = rank * chunk; i < last_elem; ++i) {\n       local_pixels[i - rank * chunk] = image[i];\n     }\n   }\n   else {\n     for (int i = rank * chunk; i < size; ++i) {\n       local_pixels[i - rank * chunk] = image[i];\n     }\n   }\n\n   std::array<size_t, 256> local_bins;\n   local_bins.fill(0);\n\n   for (int i = 0; i < chunk; ++i) {\n     ++local_bins[local_pixels[i]];\n   }\n\n   MPI_Reduce(&local_bins[0], &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: your code here\n }",
            "size_t image_size = image.size();\n    int my_rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    size_t local_bins[256];\n    memset(local_bins, 0, 256*sizeof(size_t));\n    int prev = 0, cur = 1, next = 2;\n    if (my_rank == 0) {\n        prev = 255;\n        cur = 0;\n        next = 1;\n    }\n    size_t start = my_rank * image_size / num_ranks;\n    size_t end = (my_rank + 1) * image_size / num_ranks;\n    for (size_t i = start; i < end; i++) {\n        local_bins[image[i]]++;\n    }\n    // Sum up all local bins\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    // Add the count of the first and last element to the global bin counts\n    if (my_rank == 0) {\n        bins[image[0]]++;\n        bins[image[image_size-1]]++;\n    }\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: Your code here. You may need to call MPI_Bcast() to broadcast \n   // the image to all processes, since different processes may not have the\n   // same rank. The `image` vector should be resized for every process\n   // to avoid overwritting.\n\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    int width = image.size();\n    int height = image[0];\n    int chunk = height / size;\n    int rest = height % size;\n\n    int start = rank * chunk + std::min(rank, rest);\n    int end = (rank + 1) * chunk + std::min(rank + 1, rest);\n\n    // Get the image chunk\n    std::vector<int> local_image(width * chunk);\n    std::copy(image.begin() + start * width, image.begin() + end * width, local_image.begin());\n\n    // Count the grayscale intensities\n    size_t *local_bins = new size_t[256];\n    local_bins[0] = local_image.size();\n    for (auto i : local_image) {\n        ++local_bins[i];\n    }\n\n    // Send the results back to rank 0\n    int *sendcounts = new int[size], *displs = new int[size];\n\n    sendcounts[0] = 256;\n    for (int i = 1; i < size; ++i) {\n        sendcounts[i] = 0;\n    }\n\n    MPI_Scatter(local_bins, sendcounts[rank], MPI_UNSIGNED_LONG_LONG,\n                bins.data(), sendcounts[rank], MPI_UNSIGNED_LONG_LONG,\n                0, MPI_COMM_WORLD);\n\n    MPI_Gather(bins.data(), sendcounts[rank], MPI_UNSIGNED_LONG_LONG,\n               local_bins, sendcounts[rank], MPI_UNSIGNED_LONG_LONG,\n               0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            displs[i] = displs[i - 1] + sendcounts[i - 1];\n        }\n    }\n\n    MPI_Scatterv(local_bins, sendcounts, displs, MPI_UNSIGNED_LONG_LONG,\n                bins.data(), sendcounts[rank], MPI_UNSIGNED_LONG_LONG,\n                0, MPI_COMM_WORLD);\n\n    MPI_Gatherv(bins.data(), sendcounts[rank], MPI_UNSIGNED_LONG_LONG,\n               local_bins, sendcounts, displs, MPI_UNSIGNED_LONG_LONG,\n               0, MPI_COMM_WORLD);\n\n    delete[] sendcounts;\n    delete[] displs;\n\n    if (rank == 0) {\n        delete[] local_bins;\n    }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    size_t local_bins[256];\n    size_t total_bins[256];\n    memset(local_bins, 0, 256*sizeof(size_t));\n    for (size_t i = 0; i < image.size(); ++i)\n        ++local_bins[image[i]];\n    MPI_Allreduce(local_bins, total_bins, 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    if (world_rank == 0)\n        std::copy(total_bins, total_bins + 256, bins.begin());\n}",
            "// TODO: implement this method\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, size;\n  MPI_Comm_rank(comm, &rank);\n  MPI_Comm_size(comm, &size);\n  int sum = 0;\n  std::array<size_t, 256> locBins = {};\n  std::array<int, 256> locImage = {};\n  if (rank == 0) {\n    locImage.fill(0);\n  }\n  MPI_Scatter(image.data(), 256, MPI_INT, locImage.data(), 256, MPI_INT, 0, comm);\n  for (int i = 0; i < 256; i++) {\n    locBins[locImage[i]]++;\n  }\n  MPI_Reduce(locBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n}",
            "// TODO: your code goes here\n    \n    return;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Your code here\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int n = image.size();\n  int block_size = n/world_size;\n\n  // divide the work\n  int first = block_size*world_rank;\n  int last = first + block_size;\n\n  int rank_bins[256] = {0};\n  for (size_t i = first; i < last; i++) {\n    rank_bins[image[i]]++;\n  }\n  // sum all the results\n  MPI_Reduce(rank_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "auto image_size = image.size();\n   // TODO: implement this function.\n   //\n   // To get the number of processes and the rank of the current process,\n   // use MPI_Comm_size and MPI_Comm_rank.\n   //\n   // To get the number of pixels in a vector, use std::vector::size().\n   //\n   // To get the pixel value at position i, use image[i].\n   //\n   // To count the number of elements in an array, use std::array::size().\n   //\n   // To increment the value of an array at position i, use ++bins[i].\n   //\n   // To get the value of an array at position i, use bins[i].\n   //\n   // To print the values of bins, use\n   //    for (auto bin : bins) {\n   //      std::cout << bin << \" \";\n   //    }\n   //    std::cout << std::endl;\n   //\n   // To print the value of a rank, use\n   //    std::cout << \"Rank \" << rank << \": \";\n }",
            "// TODO: Your code here\n  size_t n = image.size();\n  std::vector<size_t> local_bins(256);\n  for (auto i = 0; i < n; i++) {\n    local_bins[image[i]]++;\n  }\n  // std::copy(local_bins.begin(), local_bins.end(), bins.begin());\n  // bins = local_bins;\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Barrier(MPI_COMM_WORLD);\n    int step = image.size()/size;\n    int start = step * rank;\n    int end = rank == size - 1? image.size() : step * (rank + 1);\n    std::vector<int> local(image.begin() + start, image.begin() + end);\n    int count = countPixelsByIntensity(local);\n    MPI_Reduce(&count, &(bins[0]), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n }",
            "// TODO: your code here\n   // get process rank and number of processes\n   int proc_rank, proc_count;\n   MPI_Comm_rank(MPI_COMM_WORLD, &proc_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &proc_count);\n\n   // divide the grayscale values in the image equally amongst processes\n   int values_per_process = image.size() / proc_count;\n   int start = (proc_rank * values_per_process);\n   int end = (proc_rank + 1) * values_per_process;\n   if (proc_rank == proc_count - 1) end = image.size();\n\n   // count the pixels for each grayscale value\n   for (int i = start; i < end; i++) {\n      bins[image[i]]++;\n   }\n\n   // gather all the results from each process\n   MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int mySum = 0;\n    if (rank == 0) {\n        for (int i = 0; i < image.size(); ++i) {\n            ++bins[image[i]];\n        }\n    }\n\n    MPI_Scatter(bins.data(), 256, MPI_INT, &mySum, 256, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(&mySum, 1, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype imageType;\n    MPI_Type_contiguous(256, MPI_INT, &imageType);\n    MPI_Type_commit(&imageType);\n\n    int world_size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int local_image_size = image.size() / world_size;\n\n    int local_image[local_image_size];\n    for (size_t i = 0; i < local_image_size; i++) {\n        local_image[i] = image[local_image_size*rank + i];\n    }\n\n    MPI_Scatter(local_image, local_image_size, imageType, bins.data(), local_image_size, imageType, 0, MPI_COMM_WORLD);\n\n    // std::cout << \"rank\" << rank << \" \" << bins << std::endl;\n\n    // MPI_Gather(bins.data(), local_image_size, imageType, bins.data(), local_image_size, imageType, 0, MPI_COMM_WORLD);\n\n    MPI_Type_free(&imageType);\n}",
            "// TODO: implement this function\n}",
            "int rank, n;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n);\n\n  if(rank == 0) {\n    bins = std::array<size_t, 256>();\n  }\n\n  // TODO: implement this function\n  // Your code should be similar to what is done in the serial implementation.\n  // The only difference is that you need to split the work of counting pixels\n  // across the ranks.\n}",
            "// Your code goes here!\n}",
            "// 1. initialize the result vector with zeros, the image length\n    bins.fill(0);\n    \n    // 2. set up the MPI data type for the image pixels\n    MPI_Datatype pixelType;\n    MPI_Type_contiguous(image.size(), MPI_INT, &pixelType);\n    MPI_Type_commit(&pixelType);\n    \n    // 3. count the image pixels\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        // rank 0 will receive the result\n        MPI_Reduce(MPI_IN_PLACE, &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        // all other ranks will send their image\n        MPI_Reduce(&image[0], &bins[0], image.size(), pixelType, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    \n    // 4. clean up\n    MPI_Type_free(&pixelType);\n}",
            "int rank, num_procs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   int n = image.size();\n   size_t image_size = n * sizeof(int);\n   int num_pixels = n / num_procs;\n\n   // send_counts and displs will be filled in by scatterv\n   std::vector<int> send_counts(num_procs, 0);\n   std::vector<int> displs(num_procs, 0);\n\n   for (int i = 0; i < num_procs; ++i) {\n      if (i == rank) {\n         for (int j = 0; j < num_pixels; ++j) {\n             send_counts[i] += image[j];\n         }\n      }\n      MPI_Bcast(&send_counts[i], 1, MPI_INT, i, MPI_COMM_WORLD);\n   }\n\n   MPI_Gather(&send_counts[0], num_procs, MPI_INT, &displs[0], num_procs, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // find the total number of pixels for each rank\n   if (rank == 0) {\n      for (int i = 1; i < num_procs; ++i) {\n         displs[i] += displs[i-1];\n      }\n   }\n   int total_pixels = displs[num_procs - 1] + send_counts[num_procs - 1];\n\n   // scatterv the counts to all procs\n   int *send_pixels = nullptr;\n   if (rank == 0) {\n      send_pixels = (int *) malloc(total_pixels * sizeof(int));\n      for (int i = 0; i < num_procs; ++i) {\n         for (int j = 0; j < send_counts[i]; ++j) {\n            send_pixels[j + displs[i]] = image[j + displs[i]];\n         }\n      }\n   }\n   std::vector<int> recv_counts(num_procs, 0);\n   std::vector<int> recv_displs(num_procs, 0);\n   MPI_Scatterv(&send_pixels, &send_counts[0], &displs[0], MPI_INT,\n                &recv_counts[0], num_procs, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // find the total number of pixels for each rank\n   if (rank == 0) {\n      for (int i = 1; i < num_procs; ++i) {\n         recv_displs[i] += recv_displs[i-1];\n      }\n   }\n\n   // allocate memory for the pixels\n   recv_pixels = (int *) malloc(recv_counts[rank] * sizeof(int));\n\n   // now we can get the pixels in rank order\n   MPI_Scatterv(&send_pixels, &send_counts[0], &displs[0], MPI_INT,\n                &recv_pixels[0], recv_counts[rank], MPI_INT, 0, MPI_COMM_WORLD);\n\n   // clean up\n   if (rank == 0) {\n      free(send_pixels);\n   }\n\n   // count the pixels in each rank\n   if (rank == 0) {\n      for (int i = 0; i < num_procs; ++i) {\n         for (int j = 0; j < recv_counts[i]; ++j) {\n            ++bins[recv_pixels[j]];\n         }\n      }\n   } else {\n      for (int i = 0; i < recv_counts[rank]; ++i) {\n         ++bins[recv_pixels[i]];\n      }\n   }\n\n   MPI_Gatherv(bins.data(), 256, MPI_UNSIGNED_LONG_LONG,\n               NULL, NULL, NULL, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "//...\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    \n    int *local_bins = new int[256];\n    int n = image.size();\n    \n    // split n elements to each rank\n    int step = n / world_size;\n    int start = world_rank * step;\n    int end = (world_rank + 1) * step;\n    \n    for(int i = start; i < end; i++) {\n        local_bins[image[i]]++;\n    }\n    \n    // each rank send its bins to rank 0\n    MPI_Gather(local_bins, 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    // rank 0 sum up the bins from all ranks\n    if(world_rank == 0) {\n        for(int i = 1; i < world_size; i++) {\n            for(int j = 0; j < 256; j++) {\n                bins[j] += bins[j];\n            }\n        }\n    }\n    \n    // cleanup\n    delete[] local_bins;\n}",
            "int rank, size;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   size_t pixelsPerRank = image.size() / size;\n   int firstPixel = pixelsPerRank * rank;\n   int lastPixel = firstPixel + pixelsPerRank;\n\n   // Send each rank's pixels to the master rank\n   std::vector<int> pixels(image.begin() + firstPixel, image.begin() + lastPixel);\n   int master = 0;\n   MPI_Scatter(pixels.data(), pixelsPerRank, MPI_INT, &bins[0], pixelsPerRank, MPI_INT, master, MPI_COMM_WORLD);\n\n   // Count the pixels\n   for (int pixel : pixels) {\n     bins[pixel]++;\n   }\n\n   // Combine the counts from the ranks\n   MPI_Reduce(MPI_IN_PLACE, &bins[0], 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, master, MPI_COMM_WORLD);\n }",
            "int mpi_size, mpi_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n   int length = image.size() / mpi_size;\n\n   for(int i = 0; i < 256; i++) {\n     bins[i] = 0;\n   }\n\n   // Count how many pixels have each grayscale value\n   // TODO: Complete this function\n   // Hint: MPI_Gather()\n}",
            "// Your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size == 1) {\n        return;\n    }\n\n    int n = image.size();\n    int p = n / size;\n    if (rank < n % size) {\n        ++p;\n    }\n\n    std::vector<int> local_image(p);\n    std::copy(image.begin() + n/size * rank, image.begin() + n/size * (rank+1), local_image.begin());\n    if (rank < n % size) {\n        local_image.push_back(image[n/size * rank + n % size]);\n    }\n\n    std::array<size_t, 256> local_bins{};\n    for (int i = 0; i < p; i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    std::vector<int> recv_counts(size, 256);\n    std::vector<int> displs(size, 0);\n    MPI_Gather(&local_bins, 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "int world_rank, world_size;\n\n  // MPI setup\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // Partition the work\n  int n = image.size();\n  std::vector<int> slice;\n  int slice_size = n / world_size;\n  if (world_rank < n % world_size) {\n    slice_size++;\n  }\n  slice.resize(slice_size);\n  MPI_Scatter(&image[0], slice_size, MPI_INT, &slice[0], slice_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Do the work\n  std::array<size_t, 256> localBins;\n  std::fill(localBins.begin(), localBins.end(), 0);\n  for (size_t i = 0; i < slice_size; i++) {\n    localBins[slice[i]]++;\n  }\n\n  // Gather the results\n  MPI_Gather(&localBins[0], 256, MPI_UNSIGNED_LONG_LONG, &bins[0], 256, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Count the number of pixels in image with each grayscale intensity and store the results in bins.\n    int color, i, n, size, rank;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    n = image.size();\n\n    for (i = 0; i < 256; i++)\n    {\n        bins[i] = 0;\n    }\n\n    int sum;\n\n    for (int j = rank; j < n; j += size)\n    {\n        color = image[j];\n        bins[color]++;\n    }\n\n    MPI_Reduce(&bins, &sum, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0)\n    {\n        for (i = 0; i < 256; i++)\n        {\n            bins[i] = sum[i];\n        }\n    }\n\n    return;\n}",
            "int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n     std::vector<int> image_local(image);\n\n     int local_bins[256] = {0};\n\n     // if rank == 0, then image_local will be the entire image.\n     // if rank > 0, then image_local will be the portion of the image assigned to that rank.\n     MPI_Scatter(image_local.data(), image_local.size(), MPI_INT, local_bins, 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n     for (int pixel : local_bins) {\n         bins[pixel]++;\n     }\n\n     // if rank == 0, then bins will be the global result.\n     // if rank > 0, then bins will be a local copy of the global result.\n     MPI_Gather(local_bins, 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "//TODO: Your code here.\n  int world_size;\n  int world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  int image_size = image.size();\n  if(world_rank == 0)\n  {\n    std::vector<int> sub_image(image_size/world_size);\n    int offset = 0;\n    for(int i=1; i<world_size; i++)\n    {\n      MPI_Send(image.data()+offset, image_size/world_size, MPI_INT, i, 0, MPI_COMM_WORLD);\n      offset += image_size/world_size;\n    }\n    MPI_Send(image.data()+offset, image_size - offset, MPI_INT, world_size, 0, MPI_COMM_WORLD);\n    for(int i=1; i<world_size; i++)\n    {\n      MPI_Status status;\n      MPI_Recv(sub_image.data(), image_size/world_size, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n      for(int j=0; j<image_size/world_size; j++)\n      {\n        bins[sub_image[j]]++;\n      }\n    }\n    MPI_Recv(sub_image.data(), image_size - offset, MPI_INT, world_size, 0, MPI_COMM_WORLD, &status);\n    for(int j=0; j<image_size - offset; j++)\n    {\n      bins[sub_image[j]]++;\n    }\n  }\n  else\n  {\n    MPI_Status status;\n    std::vector<int> sub_image(image_size/world_size);\n    MPI_Recv(sub_image.data(), image_size/world_size, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n    MPI_Send(sub_image.data(), image_size/world_size, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code here.\n}",
            "// 1. \u8ba1\u7b97\u5404\u50cf\u7d20\u503c\u51fa\u73b0\u7684\u6b21\u6570\n    // 2. \u7edf\u8ba1\u5404\u50cf\u7d20\u503c\u51fa\u73b0\u7684\u6b21\u6570\uff0c\u4ece\u800c\u5f97\u5230bins\n\n }",
            "// You fill in here.\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int number_pixels = image.size();\n   int pixels_per_process = number_pixels / size;\n\n   // The first and last process calculate the first and last image index\n   int start = rank * pixels_per_process;\n   int end = start + pixels_per_process;\n   if (rank == size - 1) {\n     end = number_pixels;\n   }\n\n   // Compute the histogram\n   std::array<size_t, 256> local_bins;\n   for (int i = start; i < end; i++) {\n     local_bins[image[i]]++;\n   }\n\n   // Merge the results\n   MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO\n    int world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int i, start, end;\n    size_t local_bins[256];\n    memset(local_bins, 0, 256*sizeof(size_t));\n    int size = image.size();\n    int quotient = size / world_size;\n    int remainder = size % world_size;\n\n    if(world_rank == 0) {\n        for(i = 0; i < quotient; i++) {\n            start = quotient * world_rank + i;\n            end = quotient * (world_rank+1) + i;\n            local_bins[image[start]] += 1;\n            local_bins[image[end]] += 1;\n        }\n    } else if(world_rank == world_size-1) {\n        for(i = 0; i < remainder; i++) {\n            start = quotient * world_rank + i + quotient*remainder;\n            end = quotient * (world_rank+1) + i + quotient*remainder;\n            local_bins[image[start]] += 1;\n            local_bins[image[end]] += 1;\n        }\n    } else {\n        for(i = 0; i < quotient; i++) {\n            start = quotient * world_rank + i;\n            end = quotient * (world_rank+1) + i;\n            local_bins[image[start]] += 1;\n            local_bins[image[end]] += 1;\n        }\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // MPI_Finalize();\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunk_size = image.size() / world_size;\n    std::vector<int> local_image;\n\n    if (rank == 0) {\n        for (int i = 0; i < image.size() % world_size; i++) {\n            local_image.push_back(image[i]);\n        }\n    }\n\n    if (rank < image.size() % world_size) {\n        local_image.push_back(image[rank]);\n    }\n\n    int local_bins[256] = { 0 };\n\n    for (size_t i = 0; i < local_image.size(); i++) {\n        local_bins[local_image[i]]++;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 1; i < world_size; i++) {\n            for (int j = 0; j < 256; j++) {\n                bins[j] += bins[i*256 + j];\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Fill in this function\n  // Hint: remember the idea behind the prefix sum algorithm\n  // to count each grayscale intensity in parallel\n  size_t size = image.size();\n  // int rank;\n  // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // int size;\n  // MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int recvcounts = size / size;\n  int displs = 0;\n  int source = 0;\n  MPI_Scatter(image.data(), recvcounts, MPI_INT, bins.data(), recvcounts, MPI_INT, source, MPI_COMM_WORLD);\n  // int recvcounts[size];\n  // int displs[size];\n  // for (int i = 0; i < size; i++)\n  // {\n  //   recvcounts[i] = size / size;\n  //   displs[i] = i * recvcounts[i];\n  // }\n  // MPI_Scatterv(image.data(), recvcounts, displs, MPI_INT, bins.data(), recvcounts, MPI_INT, source, MPI_COMM_WORLD);\n\n  MPI_Reduce(bins.data(), bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function\n    int numprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int subsize = image.size() / numprocs;\n    int substart = subsize * rank;\n    int substop = substart + subsize;\n    std::vector<int> subvec;\n    if (rank == 0) {\n        subvec = image;\n        for (int i = 1; i < numprocs; i++) {\n            MPI_Send(subvec.data(), subsize, MPI_INT, i, 0, MPI_COMM_WORLD);\n            MPI_Send(image.data() + substop, image.size() - substop, MPI_INT, i, 1, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(subvec.data(), subsize, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        MPI_Recv(subvec.data() + substop, image.size() - substop, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i = 0; i < subsize; i++) {\n        bins[subvec[i]]++;\n    }\n    if (rank!= 0) {\n        MPI_Send(bins.data(), 256, MPI_UNSIGNED_LONG, 0, 2, MPI_COMM_WORLD);\n    } else {\n        std::array<size_t, 256> temp;\n        MPI_Status status;\n        for (int i = 1; i < numprocs; i++) {\n            MPI_Recv(temp.data(), 256, MPI_UNSIGNED_LONG, i, 2, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 256; j++) {\n                bins[j] += temp[j];\n            }\n        }\n    }\n}",
            "size_t imageSize = image.size();\n   int rank = 0;\n   int worldSize = 0;\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n   if(worldSize > imageSize){\n       std::cout << \"Number of processes cannot be larger than the size of the image\" << std::endl;\n       return;\n   }\n\n   int imageStart = rank*imageSize/worldSize;\n   int imageEnd = (rank+1)*imageSize/worldSize;\n\n   std::array<size_t, 256> localBins;\n   localBins.fill(0);\n\n   for(int i = imageStart; i < imageEnd; ++i){\n       ++localBins[image[i]];\n   }\n\n   std::array<size_t, 256> tempBins;\n\n   MPI_Reduce(localBins.data(), tempBins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if(rank == 0){\n       for(int i = 0; i < 256; ++i){\n           bins[i] = tempBins[i];\n       }\n   }\n }",
            "// TODO: YOUR CODE HERE\n    int imageLength = image.size();\n    int localCount[256];\n    int resultCount[256];\n    //Initialize the bins array\n    for(size_t i=0;i<bins.size();i++){\n        bins[i]=0;\n    }\n    //Initialize the localCount array\n    for(size_t i=0;i<256;i++){\n        localCount[i]=0;\n    }\n    for(int i=0;i<imageLength;i++){\n        localCount[image[i]]++;\n    }\n    MPI_Allreduce(localCount, resultCount, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    for(size_t i=0;i<bins.size();i++){\n        bins[i]=resultCount[i];\n    }\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int block_size = image.size() / size;\n   int remainder = image.size() % size;\n   if (rank == 0) {\n     for (int i = 1; i < size; i++) {\n       MPI_Send(image.data() + i*block_size, block_size, MPI_INT, i, 0, MPI_COMM_WORLD);\n     }\n   }\n   if (rank < remainder) {\n     block_size += 1;\n   }\n   std::vector<int> block_local(block_size, 0);\n   MPI_Status status;\n   MPI_Recv(block_local.data(), block_size, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n   std::for_each(block_local.begin(), block_local.end(), [&](int value) {\n     bins[value] += 1;\n   });\n   MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: implement this\n}",
            "// TODO: implement this function\n    int size, rank, world_size, world_rank, send_data, recv_data;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    std::vector<int> bin_data(world_size);\n\n    // Counting the grayscale intensity of the image\n    int gray;\n    for (int i = 0; i < image.size(); i++)\n    {\n        gray = image.at(i);\n        bin_data.at(gray)++;\n    }\n\n    // Gathering the number of pixels in every grayscale intensity\n    MPI_Gather(&bin_data, 1, MPI_INT, &bins, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Storing the total number of pixels in every grayscale intensity\n    if (world_rank == 0)\n    {\n        bins[0] = 0;\n        for (int i = 1; i < 256; i++)\n        {\n            bins[i] += bins[i-1];\n        }\n    }\n}",
            "// TODO: your code here\n }",
            "bins.fill(0);\n    // YOUR CODE HERE\n}",
            "// Your code here.\n }"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n__global__ void pixelCounts(const int *image, size_t N, size_t[256] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "...\n}",
            "// TODO: your code here\n}",
            "// YOUR CODE HERE\n   //...\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    int thread_sum = 0;\n    __shared__ int shared_sum[256];\n\n    if (idx < N) {\n        thread_sum = 1;\n        for (int i = 0; i < 256; i++) {\n            if (image[idx] == i) {\n                atomicAdd(&shared_sum[i], 1);\n            }\n        }\n    }\n\n    __syncthreads();\n\n    for (int i = 1; i < 256; i *= 2) {\n        if (idx % (i * 2) == 0) {\n            atomicAdd(&shared_sum[idx / (i * 2)], shared_sum[idx / (i * 2) + i]);\n        }\n        __syncthreads();\n    }\n\n    if (idx < 256) {\n        atomicAdd(&bins[idx], shared_sum[idx]);\n    }\n}",
            "/*... */\n}",
            "// YOUR CODE HERE\n}",
            "__shared__ size_t sbins[256];\n\n   // Each thread counts the number of pixels in image\n   // with each grayscale intensity.\n   // To do this, thread i counts the number of pixels\n   // in image with intensity i, so each thread\n   // will do the work of a single pixel.\n   // The first thread in the block will do the work\n   // of the first pixel in the image, the second thread\n   // will do the work of the second pixel, and so on.\n   // A single thread will do work on a single pixel,\n   // so a single thread is used per pixel.\n\n   // Threads are ordered by block.\n   // Each block has N threads.\n   // Each thread will be given a unique pixel index.\n   // The loop below loops over the pixels\n   // in the image, in increasing order.\n\n   int index = threadIdx.x + blockIdx.x*blockDim.x;\n   if (index >= N) return;\n\n   // Each thread will count pixels\n   // in image with each grayscale intensity.\n   // So the first thread will count\n   // the pixels in image with intensity 2,\n   // the second thread will count\n   // the pixels in image with intensity 116,\n   // and so on.\n   // This thread will do the work of pixel index\n   // `index` in the image.\n   int i = image[index];\n   atomicAdd(&sbins[i], 1);\n}",
            "int intensity = image[blockIdx.x * N];\n  atomicAdd(&bins[intensity], 1);\n}",
            "}",
            "__shared__ size_t sharedBins[256]; // 256 values\n\n  // Fill shared bins with zeroes.\n  if (threadIdx.x < 256) {\n    sharedBins[threadIdx.x] = 0;\n  }\n\n  // Synchronize.\n  __syncthreads();\n\n  // Do the computation.\n  if (threadIdx.x < N) {\n    sharedBins[image[threadIdx.x]]++;\n  }\n\n  // Synchronize.\n  __syncthreads();\n\n  // Write results back to global memory.\n  if (threadIdx.x < 256) {\n    atomicAdd(&bins[threadIdx.x], sharedBins[threadIdx.x]);\n  }\n}",
            "// TODO: Replace with your own code\n  // NOTE: The kernel is launched with at least N threads.\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n  int i = 0;\n  while (index < N) {\n    ++bins[image[index]];\n    index += blockDim.x * gridDim.x;\n  }\n}",
            "// TODO: Your code here.\n\t// Note: Please do not remove the below line.\n\tassert(N > 0);\n\n\tint idx = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (idx >= N)\n\t\treturn;\n\n\tint intensity = image[idx];\n\tatomicAdd(&(bins[intensity]), 1);\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: Fill in this function\n   int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   int bdim = blockDim.x;\n   int i = bid * bdim + tid;\n   if (i >= N) return;\n   bins[image[i]] += 1;\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    bins[image[idx]]++;\n  }\n}",
            "// TODO: write your code here\n}",
            "int id = threadIdx.x + blockDim.x * blockIdx.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "// TODO: implement this function\n    // Hint: You can use atomicAdd()\n    int start = blockIdx.x * blockDim.x + threadIdx.x;\n    int end = min(start+blockDim.x, N);\n    int value = 0;\n    for (int i = start; i < end; i++) {\n        value = image[i];\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "// TODO: implement\n    int count;\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n    while (index < N) {\n        int value = image[index];\n        atomicAdd(&bins[value], 1);\n        index += blockDim.x * gridDim.x;\n    }\n}",
            "// TODO: implement this method\n\n\t//threadId = threadIdx.x + blockIdx.x*blockDim.x;\n\n\t//__syncthreads();\n\n}",
            "// Your code here.\n}",
            "__shared__ int counters[MAX_THREADS_PER_BLOCK];\n  int tid = threadIdx.x;\n  int gid = threadIdx.x + blockIdx.x * blockDim.x;\n  int step = blockDim.x * gridDim.x;\n  int my_bin;\n  int my_count;\n\n  // For every pixel in the image\n  for (int i = gid; i < N; i += step) {\n    my_bin = image[i] / 256;\n    atomicAdd(&counters[my_bin], 1);\n  }\n\n  // Sum the number of pixels for each grayscale intensity\n  for (int bin = tid; bin < 256; bin += MAX_THREADS_PER_BLOCK) {\n    atomicAdd(&bins[bin], counters[bin]);\n  }\n}",
            "// TODO:\n}",
            "// TODO: YOUR CODE HERE\n    __shared__ size_t imageCount[256];\n\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int blockSize = blockDim.x;\n\n    if (bid >= N) return;\n\n    int start = tid + bid * blockSize;\n    int end = start + blockSize;\n\n    for (int i = start; i < end; i++) {\n        if (image[i] >= 0 && image[i] < 256)\n            atomicAdd(&imageCount[image[i]], 1);\n    }\n    __syncthreads();\n\n    if (tid < 256)\n        atomicAdd(&bins[tid], imageCount[tid]);\n}",
            "const int id = blockDim.x * blockIdx.x + threadIdx.x;\n  if (id < N) {\n    atomicAdd(&bins[image[id]], 1);\n  }\n}",
            "/* TODO: Your code here */\n}",
            "int grayscale = 0;\n\tint idx = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (idx < N) {\n\t\tgrayscale = image[idx];\n\t\tatomicAdd(&bins[grayscale], 1);\n\t}\n}",
            "__shared__ int s_bins[256];\n    int t = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    // initialize bins to 0\n    if (t == 0) {\n        for (int i = 0; i < 256; i++) {\n            s_bins[i] = 0;\n        }\n    }\n    __syncthreads();\n\n    // count pixels in each intensity\n    for (; i < N; i += stride) {\n        atomicAdd(&s_bins[image[i]], 1);\n    }\n    __syncthreads();\n\n    // add up bins from all threads\n    if (t == 0) {\n        for (int i = 1; i < 256; i++) {\n            bins[i] = s_bins[i] + bins[i];\n        }\n    }\n}",
            "// TODO: Fill this in\n}",
            "// TODO: Your code goes here\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "// TODO: Your code goes here\n}",
            "// Your code here\n  int thid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  int idx = bid*blockDim.x + thid;\n\n  if(idx < N) {\n    bins[image[idx]] += 1;\n  }\n}",
            "//TODO\n\t// Use blockIdx.x and blockDim.x to compute the id of a thread\n\t// within the block and then use threadIdx.x to compute the id\n\t// within the block.\n\t//\n\t// Store the number of pixels at the grayscale value of the current thread.\n\t//\n\t// The kernel must not write beyond `bins`.\n\t//\n\t// You will probably need to synchronize threads within the block.\n\t//\n\t// NOTE: threadIdx.x and blockIdx.x start at 0 and blockDim.x is the size of a block,\n\t// not the number of blocks.\n\t//\n\t// You should use atomicAdd() to increment the counters in `bins`.\n\t//\n\t// Use __syncthreads() to sync threads within the block.\n\t//\n\t// You should use __syncthreads() to sync threads within the block.\n\n\tif (threadIdx.x < 256) {\n\t\tbins[threadIdx.x] = 0;\n\t}\n\t__syncthreads();\n\tif (threadIdx.x < N) {\n\t\tatomicAdd(&bins[image[threadIdx.x]], 1);\n\t}\n\t__syncthreads();\n}",
            "// YOUR CODE HERE\n  int thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n  if (thread_id < N){\n    bins[image[thread_id]]++;\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  \n  if (tid < N) {\n    atomicAdd(&bins[image[tid]], 1);\n  }\n}",
            "int myId = threadIdx.x + blockIdx.x * blockDim.x;\n\tint myBin;\n\tif (myId < N) {\n\t\tmyBin = image[myId];\n\t\tatomicAdd(&bins[myBin], 1);\n\t}\n}",
            "size_t bin = threadIdx.x;\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // TODO\n}",
            "//...\n}",
            "// TODO: fill in this kernel function\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int pixel = 0;\n\n  if(i<N){\n    pixel = image[i];\n    atomicAdd(&bins[pixel], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "__shared__ int tempBins[256];\n  unsigned int id = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (id < N) {\n    atomicAdd(&tempBins[image[id]], 1);\n  }\n\n  __syncthreads();\n  atomicAdd(&bins[threadIdx.x], tempBins[threadIdx.x]);\n}",
            "}",
            "int t_idx = threadIdx.x;\n    int block_idx = blockIdx.x;\n    int idx = t_idx + block_idx * blockDim.x;\n    int x, r, g, b;\n\n    // Loop over each pixel in image\n    for (int i = idx; i < N; i += blockDim.x * gridDim.x) {\n        r = image[i];\n        g = image[i + N];\n        b = image[i + 2 * N];\n        x = (r + g + b) / 3;\n        bins[x] += 1;\n    }\n}",
            "int bin = image[blockIdx.x];\n   atomicAdd(&bins[bin], 1);\n}",
            "// Your code goes here\n}",
            "// TODO: fill in this function\n    int i=blockIdx.x*blockDim.x+threadIdx.x;\n    while(i<N)\n    {\n        bins[image[i]]++;\n        i+=blockDim.x*gridDim.x;\n    }\n    \n}",
            "int binIndex = threadIdx.x + blockIdx.x * blockDim.x;\n  if (binIndex < 256) {\n    for (int i = 0; i < N; i++) {\n      if (image[i] == binIndex) {\n        atomicAdd(bins + binIndex, 1);\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// Each thread computes one output element. Your job is to compute\n  // a histogram of the 256 grayscale values in this image.\n  //\n  // In parallel, your histogram should be updated in a thread-safe manner.\n  //\n  // You can use the following atomic operations to update the histogram:\n  // atomicAdd(int *address, int val);\n  // atomicExch(int *address, int val);\n  // atomicMin(int *address, int val);\n  // atomicMax(int *address, int val);\n\n  int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "// YOUR CODE GOES HERE\n}",
            "// Replace this code with your own code\n  // You will likely need to copy the pixel values in to shared memory\n  // and then sum the values in shared memory\n\n  // You may wish to launch more threads than pixels\n  // 1 block per intensity\n  // The intensity is the blockIdx.x, and the pixel is the threadIdx.x\n  // Each thread should iterate over the pixels for the intensity\n  // It will need to load its pixels from global memory\n  // Store the sum of those pixels in the bin for the intensity\n  \n  // Be sure to synchronize when you're done counting pixels in each intensity\n  \n  // Be sure to check for invalid image sizes\n\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        ++bins[image[idx]];\n    }\n}",
            "// YOUR CODE HERE\n}",
            "/* Your solution goes here */\n  int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if(index < N){\n      atomicAdd(&bins[image[index]],1);\n  }\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx >= N) return;\n\n  // TODO: modify this to count in parallel\n}",
            "unsigned int x = threadIdx.x + blockIdx.x * blockDim.x;\n    unsigned int y = threadIdx.y + blockIdx.y * blockDim.y;\n    if (x < N && y < N) {\n        bins[image[x + N * y]]++;\n    }\n}",
            "// TODO\n}",
            "// TODO: implement this function\n  // do not change this function signature!\n  extern __shared__ size_t s_bins[];\n  if(threadIdx.x<256)\n  {\n    s_bins[threadIdx.x]=0;\n  }\n  __syncthreads();\n  \n  for (size_t i = threadIdx.x; i < N; i+=blockDim.x)\n  {\n    atomicAdd(&(s_bins[image[i]]),1);\n  }\n  __syncthreads();\n  for (size_t i = threadIdx.x; i < 256; i+=blockDim.x)\n  {\n    atomicAdd(&(bins[i]),s_bins[i]);\n  }\n  \n}",
            "// TODO: Your code here\n}",
            "// TODO: Implement pixelCounts kernel\n}",
            "// TODO: Implement the pixel counting kernel.\n    // Hint: you can use __syncthreads() to help parallelize\n    // your code.\n}",
            "for (size_t n = blockIdx.x * blockDim.x + threadIdx.x; n < N; n += blockDim.x * gridDim.x) {\n        atomicAdd(&bins[image[n]], 1);\n    }\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id < N) {\n    atomicAdd(bins + image[id], 1);\n  }\n}",
            "// YOUR CODE HERE\n  size_t n = blockDim.x * blockIdx.x + threadIdx.x;\n  // 1. thread\n  // 2. block\n  // 3. warp\n  // 4. sm\n  // 5. global\n  if (n < N) {\n    size_t value = image[n];\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "//TODO: implement kernel\n   __syncthreads();\n}",
            "// TODO\n    int i = threadIdx.x + blockIdx.x * blockDim.x;\n    int count = 0;\n    if (i < N) {\n        count = image[i];\n        atomicAdd(bins + image[i], 1);\n    }\n    __syncthreads();\n}",
            "// YOUR CODE HERE\n   // You can use atomicAdd to add to a counter.\n   // You can also use the threadIdx.x to get the thread number.\n   // For more information, see the CUDA programming guide.\n   // https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\n   // \n   // Note that you don't need to implement this function\n   // The following code is just for you to help debug\n   \n   // if (threadIdx.x == 0) {\n   //   for (int i = 0; i < 256; i++) {\n   //     atomicAdd(&bins[i], 0);\n   //   }\n   // }\n   // __syncthreads();\n   // if (threadIdx.x == 0) {\n   //   for (int i = 0; i < 256; i++) {\n   //     printf(\"bins[%d] = %d\\n\", i, bins[i]);\n   //   }\n   // }\n}",
            "// TODO: Implement this function.\n  __shared__ size_t temp[256];\n  int tx = threadIdx.x;\n  int bx = blockIdx.x;\n\n  int threadSum = 0;\n  int threadVal = 0;\n  for (int i = bx * 256 + tx; i < N; i += 256 * gridDim.x) {\n    threadVal = image[i];\n    atomicAdd(&temp[threadVal], 1);\n  }\n\n  __syncthreads();\n\n  if (tx < 256) {\n    atomicAdd(&bins[tx], temp[tx]);\n  }\n}",
            "// YOUR CODE GOES HERE\n}",
            "__shared__ size_t binsShared[256];\n\n    // blockDim.x is guaranteed to be a power of 2\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int bx = blockIdx.x;\n    int by = blockIdx.y;\n    int bdx = blockDim.x;\n\n    int x = tx + bx * bdx;\n\n    // Load each thread block with a grayscale intensity.\n    // The value stored in image[x,y] is not the grayscale intensity\n    // but the index of the pixel.\n    size_t grayscale = image[x];\n\n    // Synchronize threads in block so that all threads have loaded their data.\n    __syncthreads();\n\n    // Each thread adds its contribution to grayscale.\n    atomicAdd(&binsShared[grayscale], 1);\n\n    // Wait for all threads in block to finish.\n    __syncthreads();\n\n    // Store counts for all 256 grayscale levels in this block.\n    if (tx == 0 && ty == 0) {\n        atomicAdd(&bins[grayscale], binsShared[grayscale]);\n    }\n}",
            "//TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx < N) bins[image[idx]] += 1;\n}",
            "// TODO: replace this with your implementation\n    int t = threadIdx.x;\n    int i = blockIdx.x*blockDim.x+threadIdx.x;\n    if(i>=N) return;\n    atomicAdd(&bins[image[i]], 1);\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int bin;\n    if(idx < N) {\n        bin = image[idx];\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Your code goes here\n  // Note that the following code is inefficient because it has a high \n  // memory footprint (256*size_t) but is just a simple example.\n  // Ideally, we would want to have the counter incremented in each thread\n  // and then have each thread read its own value in a second pass.\n\n  // Each thread is responsible for a value between 0 and 255, inclusive.\n  const int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  const int num_threads = blockDim.x * gridDim.x;\n  const int value = thread_id;\n  \n  if (thread_id >= 256) {\n    return;\n  }\n\n  for (size_t i = 0; i < N; i++) {\n    int current_pixel = image[i];\n    if (current_pixel == value) {\n      atomicAdd(&bins[current_pixel], 1);\n    }\n  }\n}",
            "// TODO: Fill in your code here\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// Add your code here\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) {\n    return;\n  }\n  bins[image[idx]]++;\n}",
            "unsigned int idx = blockIdx.x*blockDim.x+threadIdx.x;\n    if (idx >= N) {\n        return;\n    }\n    atomicAdd(&bins[image[idx]], 1);\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    while (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n        i += blockDim.x * gridDim.x;\n    }\n}",
            "int i = threadIdx.x;\n    __shared__ size_t counter[256];\n    for (int start = blockIdx.x * blockDim.x + i; start < N; start += blockDim.x * gridDim.x) {\n        int intensity = image[start];\n        atomicAdd(&counter[intensity], 1);\n    }\n    __syncthreads();\n    if (i == 0) {\n        for (int j = 1; j < 256; j++) {\n            atomicAdd(&counter[j], counter[j-1]);\n        }\n    }\n    __syncthreads();\n    for (int start = blockIdx.x * blockDim.x + i; start < N; start += blockDim.x * gridDim.x) {\n        int intensity = image[start];\n        atomicAdd(&bins[intensity], 1);\n    }\n}",
            "// TODO: Fill in.\n}",
            "// TODO\n}",
            "}",
            "// YOUR CODE HERE\n}",
            "/* Your code here */\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if(tid < N) {\n    const int v = image[tid];\n    atomicAdd(&bins[v], 1);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "size_t x = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t y = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t n = blockIdx.z * blockDim.z + threadIdx.z;\n\n  if (x < N && y < N) {\n    atomicAdd(&bins[image[y*N+x]], 1);\n  }\n}",
            "// Fill this in with your code.\n\n}",
            "size_t index = threadIdx.x;\n    size_t stride = blockDim.x;\n    size_t bin = 0;\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += stride * gridDim.x) {\n        bin = image[i];\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n        bins[i] = 0;\n    }\n    __syncthreads();\n    \n    int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "// TODO: implement this\n   // Hint: see http://docs.nvidia.com/cuda/cuda-c-programming-guide/#programming-model\n   int tid = threadIdx.x;\n   if (tid >= 256) return;\n   \n   size_t count = 0;\n   for (size_t i = 0; i < N; i++) {\n       if (image[i] == tid) count++;\n   }\n   atomicAdd(&bins[tid], count);\n}",
            "int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  int thread_count = blockDim.x * gridDim.x;\n\n  for (int i = thread_id; i < N; i += thread_count) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if(idx < N) {\n        atomicAdd(&bins[image[idx]], 1);\n    }\n}",
            "// TODO: Implement me!\n}",
            "// TODO: implement pixelCounts here\n    __shared__ int temp[256];\n    int thread_id = threadIdx.x;\n    int block_id = blockIdx.x;\n    int offset = block_id * N;\n    int count = 0;\n    int localCount = 0;\n    for(int i=thread_id; i<N; i+=blockDim.x){\n        if(image[i+offset] == i){\n            localCount++;\n        }\n    }\n    temp[thread_id] = localCount;\n    __syncthreads();\n    for(int s=1; s<blockDim.x; s*=2){\n        if(thread_id % (2*s) == 0){\n            count += temp[thread_id+s];\n        }\n        __syncthreads();\n    }\n    if(thread_id == 0){\n        bins[block_id] = count;\n    }\n}",
            "int my_id = blockIdx.x*blockDim.x + threadIdx.x;\n  int num_threads = gridDim.x * blockDim.x;\n\n  for (int i = my_id; i < N; i += num_threads) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "}",
            "}",
            "// TODO: Your code goes here\n}",
            "// TODO: Fill in code here.\n}",
            "/* TODO */\n}",
            "}",
            "/* TODO */\n    int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (thread_id < N) {\n        atomicAdd(&bins[image[thread_id]], 1);\n    }\n}",
            "// TODO: Your code here.\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    atomicAdd(&(bins[image[tid]]), 1);\n  }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tatomicAdd(&bins[image[tid]], 1);\n\t}\n}",
            "/* TODO */\n}",
            "// Each thread is responsible for 1 pixel.\n    // Each thread has one 32-bit int of shared memory.\n    // To calculate the thread block size, we use the number of active\n    // threads in a thread block as the thread block size.\n    __shared__ int temp[32];\n    int tid = threadIdx.x;\n    int lane = tid & 31;\n    int wid = tid >> 5;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int pixel = 0;\n    int grayscale = 0;\n    int count = 0;\n    if (i < N) {\n        pixel = image[i];\n        grayscale = pixel & 255;\n        atomicAdd(&bins[grayscale], 1);\n    }\n    __syncthreads();\n    if (wid == 0) {\n        // Each thread block is responsible for 256 elements of the `bins` array.\n        // Threads with the same ID in a thread block share the same `bins` array element.\n        // The first element of `bins` is in element 0 of the `bins` array.\n        // Each thread block sums up 32 elements in the `bins` array.\n        // First the first element is summed up, then the second element is summed up,\n        // and so on.\n        // The result is stored in the `temp` array.\n        // After the first 32 elements have been summed up, the next 32 elements are summed up,\n        // and so on.\n        // The result is stored in the `bins` array.\n        temp[lane] = bins[lane];\n        temp[lane + 32] = bins[lane + 32];\n        temp[lane + 64] = bins[lane + 64];\n        temp[lane + 96] = bins[lane + 96];\n        temp[lane + 128] = bins[lane + 128];\n        temp[lane + 160] = bins[lane + 160];\n        temp[lane + 192] = bins[lane + 192];\n        temp[lane + 224] = bins[lane + 224];\n        temp[lane + 256] = bins[lane + 256];\n        __syncthreads();\n        temp[lane] += temp[lane + 32];\n        temp[lane + 16] += temp[lane + 48];\n        temp[lane + 8] += temp[lane + 40];\n        temp[lane + 24] += temp[lane + 56];\n        temp[lane + 4] += temp[lane + 36];\n        temp[lane + 20] += temp[lane + 52];\n        temp[lane + 12] += temp[lane + 44];\n        temp[lane + 28] += temp[lane + 60];\n        temp[lane + 2] += temp[lane + 34];\n        temp[lane + 18] += temp[lane + 50];\n        temp[lane + 10] += temp[lane + 42];\n        temp[lane + 26] += temp[lane + 58];\n        temp[lane + 6] += temp[lane + 38];\n        temp[lane + 22] += temp[lane + 54];\n        temp[lane + 14] += temp[lane + 46];\n        temp[lane + 30] += temp[lane + 62];\n        temp[lane + 1] += temp[lane + 33];\n        temp[lane + 17] += temp[lane + 49];\n        temp[lane + 9] += temp[lane + 41];\n        temp[lane + 25] += temp[lane + 57];\n        temp[lane + 5] += temp[lane + 37];\n        temp[lane + 21] += temp[lane + 53];\n        temp[lane + 13] += temp[lane + 45];\n        temp[lane + 29] += temp[lane + 61];\n        temp[lane + 3] += temp[lane + 35];\n        temp[lane + 19] += temp[lane + 51];\n        temp[lane + 11] += temp[lane + 43];\n        temp[lane + 27] += temp[lane + 59];\n        temp[lane + 7",
            "// TODO: Fill in this function\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if(idx < N){\n    bins[image[idx]] += 1;\n  }\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int i = tid / N;\n  const int j = tid % N;\n  if (i < N && j < N) {\n    atomicAdd(&bins[image[i * N + j]], 1);\n  }\n}",
            "// YOUR CODE HERE\n   int pixelIndex = threadIdx.x + blockIdx.x * blockDim.x;\n   if(pixelIndex < N){\n      int intensity = image[pixelIndex];\n      atomicAdd(&bins[intensity], 1);\n   }\n}",
            "/* TODO: implement this function */\n}",
            "// YOUR CODE HERE\n  // TODO:\n  //     * Each thread is responsible for counting the number of pixels with each\n  //       grayscale intensity in the array `image`.\n  //     * Store the counts in `bins`.\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n    // TODO\n    // Hint: Use atomicAdd\n    // Hint: Use a 1D block with N threads\n    // Hint: Use a 2D block with 16x16 threads\n\n    // YOUR CODE HERE\n    // TODO\n}",
            "int threadId = blockDim.x*blockIdx.y*gridDim.x \n\t\t\t\t+ blockDim.x*blockIdx.x \n\t\t\t\t+ threadIdx.x;\n\n\tif (threadId<N) {\n\t\tint val = image[threadId];\n\t\tatomicAdd(&bins[val],1);\n\t}\n}",
            "__shared__ int histogram[256];\n    // each thread processes one grayscale intensity\n    const int grayscale = blockIdx.x * blockDim.x + threadIdx.x;\n    if (grayscale < 256) {\n        // reset the histogram\n        histogram[grayscale] = 0;\n        __syncthreads();\n    }\n    // each thread processes one pixel\n    const int pixelId = blockIdx.x * blockDim.x * blockDim.y + threadIdx.x + threadIdx.y * blockDim.x;\n    if (pixelId < N) {\n        if (image[pixelId] <= 255) {\n            atomicAdd(&(histogram[image[pixelId]]), 1);\n        }\n    }\n    __syncthreads();\n    // write the histogram to global memory\n    if (grayscale < 256) {\n        atomicAdd(&(bins[grayscale]), histogram[grayscale]);\n    }\n}",
            "// TODO: Your code goes here\n}",
            "size_t threadID = blockDim.x * blockIdx.x + threadIdx.x;\n    if (threadID < N) {\n        bins[image[threadID]] += 1;\n    }\n}",
            "// YOUR CODE HERE\n  //...\n  __syncthreads();\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO: Your code here.\n  // Remember to take care of all possible block sizes, which can vary depending on\n  // how many threads there are.\n}",
            "/* Your code here */\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n    atomicAdd(&bins[image[i]], 1);\n  }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n  size_t n = 0;\n  while (i < N) {\n    ++bins[image[i]];\n    i += blockDim.x*gridDim.x;\n  }\n}",
            "// TODO: Implement this function\n  // Your code here\n\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n  while (i < N) {\n    atomicAdd(&bins[image[i]], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "// TODO: implement the CUDA kernel\n    // Use atomicAdd to update the bins array\n    // You can use 128 threads per block\n    // NOTE: remember to check your bounds when writing to bins\n}",
            "// Compute thread ID\n  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int val = image[tid];\n    atomicAdd(bins+val, 1);\n  }\n}",
            "int grayValue = image[blockIdx.x * blockDim.x + threadIdx.x];\n\tatomicAdd(&bins[grayValue], 1);\n}",
            "int intensity = threadIdx.x + blockDim.x * blockIdx.x;\n  if (intensity < 256)\n    atomicAdd(&bins[intensity], __popcll(image[intensity]));\n}",
            "int bin = image[blockIdx.x];\n  atomicAdd(&bins[bin], 1);\n}",
            "// YOUR CODE HERE\n  //...\n}",
            "// YOUR CODE HERE\n    // TODO\n    // threadIdx.x is the id of the current thread in the block\n    // blockIdx.x is the id of the current block\n    // blockDim.x is the size of the block (32, 64, 1024)\n    // The total number of threads in the grid is: gridDim.x * blockDim.x\n    // The total number of blocks in the grid is: gridDim.x\n    // The global ID is: (blockIdx.x * blockDim.x) + threadIdx.x\n\n    // TODO\n    // This kernel should return the number of pixels with each grayscale intensity\n    // and store them in the corresponding position of the output array\n\n    // TODO\n    // Remember that you can use the threadIdx.x to access individual elements of a\n    // global array. For example, in this case, the threadIdx.x can be used to access\n    // the array image: image[threadIdx.x]\n}",
            "// TODO: Fill in.\n  int sum = 0;\n  for(size_t i=threadIdx.x;i<N;i+=blockDim.x) {\n    sum+=image[i];\n  }\n  atomicAdd(&bins[sum],1);\n}",
            "// TODO: implement\n}",
            "/* YOUR CODE HERE */\n    __syncthreads();\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tatomicAdd(&bins[image[i]], 1);\n\t}\n}",
            "int threadId = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (threadId < N) {\n    int intensity = image[threadId];\n    atomicAdd(&bins[intensity], 1);\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  __shared__ int mybins[256];\n  int tid = threadIdx.x;\n\n  if (index < N) {\n    int grayscale = image[index];\n    atomicAdd(&mybins[grayscale], 1);\n  }\n  \n  for(int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n    __syncthreads();\n    if(tid < stride) {\n      mybins[tid] += mybins[tid + stride];\n    }\n  }\n  \n  if(tid < 256) {\n    atomicAdd(&bins[tid], mybins[tid]);\n  }\n}",
            "// TODO\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  while (idx < N) {\n    atomicAdd(&bins[image[idx]], 1);\n    idx += blockDim.x * gridDim.x;\n  }\n}",
            "int thread_id = threadIdx.x;\n  int block_id = blockIdx.x;\n  int grid_size = gridDim.x;\n  int i = block_id * block_size + thread_id;\n  \n  for (int idx = i; idx < N; idx += block_size * grid_size) {\n    int pixel = image[idx];\n    atomicAdd(&bins[pixel], 1);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "int id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (id < N) {\n        // Each thread adds one to the bin for the grayscale intensity it\n        // encounters in the image.\n        atomicAdd(bins + image[id], 1);\n    }\n}",
            "// TODO\n}",
            "/* YOUR CODE HERE */\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int value = image[idx];\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "int tid = threadIdx.x; // thread id\n   int gid = blockIdx.x * blockDim.x + threadIdx.x; // global thread id\n   if (gid < N) {\n      bins[image[gid]]++;\n   }\n}",
            "}",
            "// your code here\n\tint idx = threadIdx.x + blockIdx.x * blockDim.x;\n\tint stride = blockDim.x * gridDim.x;\n\tfor (int i = idx; i < N; i += stride) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "int idx = threadIdx.x;\n  int idy = threadIdx.y;\n  int bx = blockIdx.x;\n  int by = blockIdx.y;\n  int tx = blockDim.x;\n  int ty = blockDim.y;\n\n  int image_offset = N*bx + ty*tx + idy*tx*N + idx;\n  int bins_offset = 256*by + idx;\n\n  atomicAdd(bins + bins_offset, __ldg(image + image_offset));\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int numThreads = blockDim.x * gridDim.x;\n\n    int bins_size = bins.size();\n\n    // each thread processes one element in the array\n    while (idx < N) {\n        // update the value at bin\n        int gray = image[idx];\n        atomicAdd(&bins[gray], 1);\n        idx += numThreads;\n    }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x >= N) return;\n  atomicAdd(&bins[image[x]], 1);\n}",
            "// YOUR CODE HERE\n  int sum = 0;\n  for (int i = 0; i < N; i++) {\n    sum += image[i];\n  }\n  atomicAdd(bins + sum, 1);\n}",
            "// TODO: Your code here\n}",
            "// YOUR CODE HERE\n    __syncthreads();\n}",
            "const int x = blockIdx.x * blockDim.x + threadIdx.x;\n   const int stride = gridDim.x * blockDim.x;\n\n   for (int i = x; i < N; i += stride) {\n      bins[image[i]] += 1;\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i >= N) return;\n\n  atomicAdd(&bins[image[i]], 1);\n}",
            "// YOUR CODE HERE\n    __shared__ int s_image[BLOCK_SIZE];\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int i = tid * 256;\n    int j = tid + 256;\n    int k = tid + 512;\n    int l = tid + 768;\n\n    while(tid < N){\n        if(i < N){\n            s_image[threadIdx.x] = image[i];\n            __syncthreads();\n            atomicAdd(&bins[s_image[threadIdx.x]], 1);\n            __syncthreads();\n        }else if(j < N){\n            s_image[threadIdx.x] = image[j];\n            __syncthreads();\n            atomicAdd(&bins[s_image[threadIdx.x]], 1);\n            __syncthreads();\n        }else if(k < N){\n            s_image[threadIdx.x] = image[k];\n            __syncthreads();\n            atomicAdd(&bins[s_image[threadIdx.x]], 1);\n            __syncthreads();\n        }else if(l < N){\n            s_image[threadIdx.x] = image[l];\n            __syncthreads();\n            atomicAdd(&bins[s_image[threadIdx.x]], 1);\n            __syncthreads();\n        }\n        tid = blockIdx.x * blockDim.x + threadIdx.x;\n        i = tid * 256;\n        j = tid + 256;\n        k = tid + 512;\n        l = tid + 768;\n    }\n}",
            "// TODO\n}",
            "// TODO: Write kernel function\n}",
            "int x = threadIdx.x + blockIdx.x * blockDim.x;\n  if (x < N)\n    atomicAdd(&bins[image[x]], 1);\n}",
            "// TODO: Fill in your code here\n    size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        atomicAdd(&bins[image[tid]], 1);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) return;\n    atomicAdd(&(bins[image[idx]]), 1);\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[image[i]], 1);\n    }\n}",
            "//TODO: your code here\n\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    int pixel = image[index];\n    atomicAdd(&(bins[pixel]), 1);\n  }\n}",
            "// TODO: Implement pixelCounts kernel\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO: launch kernel\n   // TODO: count pixels in each thread\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function using CUDA\n\n\t// YOUR CODE HERE\n\t// Don't forget: __syncthreads(); at the end of each threadblock.\n\t// The host will launch this kernel with a blocksize of N threads per block.\n\t// Note that you can use \"threadIdx.x\" to index into \"bins\" for each block.\n\t// Be sure to index into the image array with \"image[threadIdx.x]\".\n\t// You can use the number of threads in this block to index into the bins array.\n\tint i = threadIdx.x;\n\tint image_pixel = image[i];\n\tif (i < N)\n\t{\n\t\tbins[image_pixel] += 1;\n\t}\n\n}",
            "// TODO: Your code here.\n  // Each thread block should sum the number of pixels in each grayscale intensity.\n}",
            "// Each thread processes one pixel, each thread has to read N pixels\n  // The input image has the following structure:\n  // 1, 2, 3, 4\n  // 2, 3, 4, 5\n  // 3, 4, 5, 6\n  // 4, 5, 6, 7\n  // \n  // Then the threads are arranged in a 2x2 grid of 4 threads.\n  // This thread should process the pixel at image[0], image[4], image[2], image[6].\n  // To do this, we must read 8 values of image, the pixel at image[0] and the pixel at image[4]\n  // will be read in the first thread of the grid, and the pixel at image[2] and the pixel\n  // at image[6] will be read in the second thread of the grid.\n  //\n  // 4 threads per pixel:\n  // Thread 0 reads image[0], image[1]\n  // Thread 1 reads image[2], image[3]\n  // Thread 2 reads image[4], image[5]\n  // Thread 3 reads image[6], image[7]\n  //\n  // 2x2 threads per pixel:\n  // Thread 0 reads image[0], image[2]\n  // Thread 1 reads image[4], image[6]\n  // Thread 2 reads image[1], image[3]\n  // Thread 3 reads image[5], image[7]\n  //\n  // A thread has to read 8 values of `image` to calculate the grayscale intensities of the\n  // pixels it processes. A thread will process the first 4 values and the second 4 values\n  // of `image` to calculate the grayscale intensity of the pixel at `image[0]` and `image[4]`\n  // respectively.\n  //\n  // threadIdx.x: 0 1\n  // blockDim.x: 2\n  // blockIdx.x: 0 1\n  // gridDim.x:  2 2\n  //\n  // threadIdx.y: 0 1\n  // blockDim.y: 2\n  // blockIdx.y: 0 1\n  // gridDim.y:  2 2\n  //\n  // threadIdx.x: 0 0 1 1\n  // threadIdx.y: 0 1 0 1\n  // blockDim.x: 2 2 2 2\n  // blockDim.y: 2 2 2 2\n  // blockIdx.x: 0 0 1 1\n  // blockIdx.y: 0 1 0 1\n  // gridDim.x:  2 2 2 2\n  // gridDim.y:  2 2 2 2\n\n  // Find the index of the pixel that this thread processes\n  // Since we want to process one pixel per thread, threadIdx.x == blockIdx.x == pixelIdx\n  //\n  // threadIdx.x: 0 1\n  // blockIdx.x: 0 1\n  // blockDim.x: 2\n  // gridDim.x:  2 2\n\n  // Find the indices of the pixels that this thread processes\n  // Since we want to process one pixel per thread, threadIdx.x == blockIdx.x == pixelIdx\n  //\n  // threadIdx.x: 0 1\n  // blockIdx.x: 0 1\n  // blockDim.x: 2\n  // gridDim.x:  2 2\n\n  // Get the value of the pixel that this thread processes\n  //\n  // threadIdx.x: 0 1\n  // blockIdx.x: 0 1\n  // blockDim.x: 2\n  // gridDim.x:  2 2\n  //\n  // threadIdx.x: 0 0 1 1\n  // threadIdx.y: 0 1 0 1\n  // blockDim.x: 2 2 2 2\n  // blockDim.y: 2 2 2 2\n  // blockIdx.x: 0 0 1 1\n  // blockIdx.y: 0 1 0 1\n  // gridDim.x:  2 2 2 2\n  // gridDim.y:  2 2 2 2\n\n  // Find the indices of the pixels that this thread processes\n  // Since we want to process one pixel per thread, threadIdx.x == blockIdx.x == pixelIdx\n  //\n  // threadIdx.x: 0 1\n  // blockIdx.x: 0 1\n  // blockDim.x:",
            "// TODO: Replace this code with your own CUDA implementation.\n   // The following is a simple reference implementation that will\n   // run on the CPU.\n   for(size_t i = 0; i < N; ++i) {\n      // TODO: Replace this code with your own CUDA implementation.\n      // The following is a simple reference implementation that will\n      // run on the CPU.\n      bins[image[i]]++;\n   }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO: Implement me.\n   // Note: This is a template function.\n   // TODO: Set the desired blocksize and grid size.\n   // TODO: Use the provided `blockIdx`, `threadIdx`, `gridDim`, and `blockDim` variables.\n   // TODO: Launch the kernel with `cudaLaunchKernel`\n   // TODO: Check for errors.\n   // TODO: Return.\n}",
            "unsigned int index = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int stride = blockDim.x * gridDim.x;\n  while (index < N) {\n    atomicAdd(&bins[image[index]], 1);\n    index += stride;\n  }\n}",
            "__shared__ size_t localBins[256];\n\n  // Each thread has its own id in the global (block) dimension.\n  size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    // Get the pixel intensity value.\n    int pixel = image[tid];\n\n    // Count the pixel in the local bin.\n    atomicAdd(&localBins[pixel], 1);\n  }\n\n  // Make sure the global bins has been updated before we use it.\n  __syncthreads();\n\n  if (tid < 256) {\n    // Store the local bin to the global bin.\n    atomicAdd(&bins[tid], localBins[tid]);\n  }\n}",
            "// TODO\n}",
            "int idx = threadIdx.x;\n  int stride = blockDim.x;\n  int start = blockDim.x * blockIdx.x;\n  int end = start + blockDim.x;\n  for (int i = start + idx; i < N; i += stride) {\n    bins[image[i]]++;\n  }\n}",
            "int bin_id = threadIdx.x;\n    atomicAdd(&bins[bin_id], __popc(image[blockIdx.x] >> bin_id & 1));\n}",
            "// YOUR CODE HERE\n  //\n  //\n  //\n  //\n  //\n  //\n}",
            "int my_gray = blockIdx.x * blockDim.x + threadIdx.x;\n   // TODO: YOUR CODE HERE\n   atomicAdd(&bins[my_gray], 1);\n}",
            "// YOUR CODE HERE\n}",
            "__shared__ int s_bins[256];\n    for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n        s_bins[i] = 0;\n    }\n    __syncthreads();\n\n    for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        s_bins[image[i]]++;\n    }\n    __syncthreads();\n\n    for (int i = threadIdx.x; i < 256; i += blockDim.x) {\n        atomicAdd(&(bins[i]), s_bins[i]);\n    }\n}",
            "// Fill in your code\n}",
            "}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n  if (x < N) {\n    atomicAdd(&bins[image[x]], 1);\n  }\n}",
            "// TODO: Your code here...\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        bins[image[index]]++;\n    }\n}",
            "__shared__ size_t sbins[256];\n   if (threadIdx.x < 256) {\n      sbins[threadIdx.x] = 0;\n   }\n   __syncthreads();\n   \n   int idx = threadIdx.x + blockDim.x * blockIdx.x;\n   while (idx < N) {\n      atomicAdd(&sbins[image[idx]], 1);\n      idx += blockDim.x * gridDim.x;\n   }\n   __syncthreads();\n   if (threadIdx.x < 256) {\n      atomicAdd(&bins[threadIdx.x], sbins[threadIdx.x]);\n   }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "16_histogram_pixel_histogram",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of pixels in image with each grayscale intensity.\n   The vector `image` is a grayscale image with values 0-255.\n   Store the results in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n   \n   input: image=[2, 116, 201, 11, 92, 92, 201, 4, 2]\n   output: [0, 0, 2, 0, 1, ...]\n*/\n void pixelCounts(std::vector<int> const& image, std::array<size_t, 256> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int num_threads = omp_get_max_threads();\n   int image_size = image.size();\n   \n   // Count number of pixels for each grayscale intensity\n   #pragma omp parallel for num_threads(num_threads) default(none) shared(image_size, image, bins)\n   for (int i = 0; i < image_size; ++i) {\n     bins[image[i]]++;\n   }\n\n   // Sum counts from all ranks\n   std::vector<size_t> local_bins(bins.begin(), bins.end());\n   std::vector<size_t> global_bins(bins.size());\n   MPI_Reduce(local_bins.data(), global_bins.data(), global_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n     // Divide by number of pixels to get the average count\n     for (auto &b : global_bins) {\n       b = static_cast<size_t>(static_cast<double>(b) / image.size());\n     }\n   }\n }",
            "const size_t n = image.size();\n  const size_t rank = 0;\n  const size_t numRanks = 1;\n\n  int thread_count = omp_get_max_threads();\n  int pixel_count_per_thread = n / thread_count;\n  int remainder_pixels = n % thread_count;\n\n  // Count the number of pixels for each thread and store the results in thread_bins\n  std::vector<std::array<size_t, 256>> thread_bins(thread_count);\n#pragma omp parallel\n  {\n    int thread_id = omp_get_thread_num();\n    int first_pixel = thread_id * pixel_count_per_thread;\n    int last_pixel = (thread_id + 1) * pixel_count_per_thread;\n    if (thread_id == thread_count - 1) {\n      last_pixel += remainder_pixels;\n    }\n    for (int i = first_pixel; i < last_pixel; ++i) {\n      thread_bins[thread_id][image[i]] += 1;\n    }\n  }\n\n  // Accumulate the counts from all threads into bins\n  // This is done by adding the values in each thread bin into the corresponding element in the global bins\n  for (size_t i = 0; i < bins.size(); ++i) {\n    for (int j = 0; j < thread_count; ++j) {\n      bins[i] += thread_bins[j][i];\n    }\n  }\n}",
            "// YOUR CODE HERE\n    \n    // END OF YOUR CODE\n}",
            "// TODO: Fill in this function.\n}",
            "std::array<size_t, 256> localBins{};\n    localBins.fill(0);\n#pragma omp parallel for\n\tfor(auto i = 0; i < image.size(); i++){\n\t\tlocalBins[image[i]]++;\n\t}\n\tMPI_Reduce(localBins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "// TODO: implement this function\n     int nprocs, rank;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n     if(rank == 0) {\n         bins.fill(0);\n     }\n     // compute local counts\n     int begin = 0;\n     int end = image.size();\n     int local_bins[256] = {0};\n     for(int i = begin; i < end; i++) {\n         int tmp = image[i];\n         local_bins[tmp]++;\n     }\n     // sum counts\n     int global_bins[256] = {0};\n     MPI_Reduce(local_bins, global_bins, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n     if(rank == 0) {\n         for(int i = 0; i < 256; i++) {\n             bins[i] = global_bins[i];\n         }\n     }\n }",
            "int num_procs = 0;\n   int rank = 0;\n\n   // TODO: get number of ranks\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   // TODO: get rank\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   size_t num_pixels = image.size();\n   size_t n = num_pixels / num_procs;\n\n   int *local_bins = new int[256]{};\n   int *recv_buffer = new int[256]{};\n\n   // TODO: compute bounds for each rank\n   int bounds[2] = {rank * n, (rank + 1) * n};\n\n   if(rank == 0) {\n     for(int i = 0; i < 256; ++i) {\n       bins[i] = 0;\n     }\n   }\n\n   // TODO: copy local data into local_bins\n   for(int i = bounds[0]; i < bounds[1]; ++i) {\n     local_bins[image[i]] += 1;\n   }\n\n   // TODO: gather local bins from all ranks\n   MPI_Reduce(local_bins, recv_buffer, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if(rank == 0) {\n     for(int i = 0; i < 256; ++i) {\n       bins[i] = recv_buffer[i];\n     }\n   }\n\n   // TODO: deallocate the MPI data structures\n   delete [] recv_buffer;\n   delete [] local_bins;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    // TODO: your code goes here\n    \n    // Each rank will compute the histogram of a part of the image.\n    // Determine the number of pixels each rank will count.\n    // For example, if the image has 1000 pixels and the number of processes is 10,\n    // then each rank will compute 100 pixels.\n    \n    // The number of pixels in each rank's part is determined by dividing the image size\n    // by the number of processes.\n    size_t pixels_per_rank = image.size()/size;\n    \n    // The size of the part that this process will compute is determined by adding\n    // the number of pixels in each rank before it to the previous result.\n    size_t first_pixel = pixels_per_rank*rank;\n    size_t last_pixel = pixels_per_rank*(rank+1);\n    \n    if (rank == 0) {\n        bins = std::array<size_t, 256> {};\n    }\n\n    int max_count = 0;\n    size_t current_count = 0;\n    size_t last_count = 0;\n\n    // Iterate over each pixel in the rank's part of the image.\n    // For each pixel, increment the count in the corresponding bin.\n    // Use OpenMP to parallelize this loop.\n    \n    // TODO: your code goes here\n    for (int i = first_pixel; i < last_pixel; i++) {\n        // TODO: your code goes here\n    }\n\n    // Gather the results on rank 0 so that all ranks have the complete histogram.\n    MPI_Reduce(&current_count, &last_count, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&last_count, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = std::array<size_t, 256> {};\n        for (size_t i = 0; i < last_count; i++) {\n            bins[image[i]]++;\n        }\n    }\n}",
            "// TODO\n}",
            "size_t n = image.size();\n  int num_ranks, rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  size_t stride = n/num_ranks;\n  size_t begin = rank*stride;\n  size_t end = (rank+1)*stride;\n\n  #pragma omp parallel\n  {\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    std::array<size_t, 256> local_bins{};\n    if(my_rank == 0) {\n      for (size_t i = begin; i < end; i++) {\n        local_bins[image[i]] += 1;\n      }\n    }\n    MPI_Gather(&local_bins, 256, MPI_UNSIGNED_LONG, &bins, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank = 0;\n    int nprocs = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    \n    int nrows = image.size();\n    int ncols = image.size() / nprocs;\n    int start_row = rank * nrows / nprocs;\n    int end_row = (rank + 1) * nrows / nprocs;\n    int start_col = 0;\n    int end_col = ncols;\n\n    int num_pixels = end_row - start_row;\n    int num_pixels_per_proc = num_pixels / nprocs;\n    int offset = start_row;\n    \n    int pixel_value = 0;\n    int local_bins[256];\n\n    for (int i = 0; i < 256; i++) {\n        local_bins[i] = 0;\n    }\n\n    for (int i = 0; i < num_pixels; i++) {\n        if ((i + offset) % nprocs == rank) {\n            pixel_value = image[(start_col + i) % nrows];\n            local_bins[pixel_value]++;\n        }\n    }\n\n    int sum = 0;\n    for (int i = 0; i < 256; i++) {\n        sum += local_bins[i];\n        local_bins[i] = sum;\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n  size_t rows = image.size()/width;\n  std::vector<int> results(bins.size());\n  #pragma omp parallel for\n  for (int i = 0; i < rows; i++) {\n    for (int j = 0; j < width; j++) {\n      results[image[i*width + j]]++;\n    }\n  }\n  MPI_Reduce(results.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < image.size(); i++) {\n            bins[image[i]]++;\n        }\n    }\n}",
            "int n = 0;\n  int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  int length = image.size();\n  int remainder = length % num_procs;\n  int start = rank * (length / num_procs);\n  int end = (rank + 1) * (length / num_procs);\n  if (rank == num_procs - 1) {\n    end += remainder;\n  }\n  if (rank == 0) {\n    bins.fill(0);\n  }\n  omp_set_num_threads(omp_get_max_threads());\n#pragma omp parallel for reduction(+:n)\n  for (int i = start; i < end; ++i) {\n    bins[image[i]]++;\n  }\n  MPI_Reduce(&n, &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for num_threads(16) schedule(static)\n  for (int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel for schedule(static)\n  for (int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// TODO: your code here\n    size_t nthreads = omp_get_max_threads();\n    size_t nprocs = 0;\n    size_t offset = 0;\n    size_t n = image.size();\n    // Find number of processes\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    // Find image chunk size\n    size_t chunk = n/nprocs;\n    // Each process gets a chunk of image\n    std::vector<int> tmp(chunk);\n    // For each process\n    for (size_t p = 0; p < nprocs; p++) {\n        if (p == nprocs-1) {\n            tmp.assign(image.begin() + offset, image.end());\n        } else {\n            tmp.assign(image.begin() + offset, image.begin() + offset + chunk);\n            offset += chunk;\n        }\n        // Count pixels\n        std::vector<size_t> bin(256);\n        omp_set_num_threads(nthreads);\n        #pragma omp parallel for\n        for (size_t i = 0; i < tmp.size(); i++) {\n            bin[tmp[i]]++;\n        }\n        // Reduce bins\n        MPI_Reduce(bin.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n }",
            "// your code here\n }",
            "// TODO: your code here\n}",
            "// TODO: Implement this function.\n }",
            "int rank, n_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    int local_bins[256];\n    size_t local_count = 0;\n    for (size_t i = 0; i < image.size(); i++) {\n        local_bins[image[i]]++;\n        local_count++;\n    }\n    //MPI_Allreduce(local_bins, bins, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    //MPI_Reduce(&local_count, &bins[256], 1, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    size_t total_count = 0;\n    MPI_Allreduce(&local_count, &total_count, 1, MPI_LONG, MPI_SUM, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins[256] = total_count;\n    }\n    MPI_Reduce(local_bins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// YOUR CODE HERE\n  bins = {};\n  size_t n = image.size();\n  std::vector<int> local_bins = {0};\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int start = rank * n / size;\n  int end = (rank + 1) * n / size;\n  std::vector<int> local_image = std::vector<int>(image.begin() + start, image.begin() + end);\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for(size_t i = 0; i < n; i++){\n      local_bins[0] += local_image[i];\n    }\n    std::array<int, 256> tmp_bins;\n    MPI_Reduce(&local_bins[0], &tmp_bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0){\n      for(size_t i = 0; i < 256; i++){\n        bins[i] = tmp_bins[i];\n      }\n    }\n  }\n  return;\n}",
            "size_t num_pixels = image.size();\n\n   // Initialize bins to 0 on all processors\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n   // Count pixels for each intensity in parallel\n   #pragma omp parallel for\n   for (size_t pixel = 0; pixel < num_pixels; pixel++) {\n      bins[image[pixel]]++;\n   }\n\n   // Sum counts on processors other than rank 0\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "int rank, nRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int nPixels = image.size();\n   std::vector<int> localBins(256);\n\n   if (rank == 0) {\n     std::fill(bins.begin(), bins.end(), 0);\n   }\n\n   #pragma omp parallel for\n   for (int i = 0; i < nPixels; ++i) {\n     int pixel = image[i];\n     localBins[pixel]++;\n   }\n\n   MPI_Gather(localBins.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "// TODO: implement this function\n  // TODO: use MPI\n  // TODO: use OpenMP\n}",
            "const int num_pixels = image.size();\n  // Fill your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<int> local_bins(256);\n  int thread_id;\n  #pragma omp parallel private(thread_id)\n  {\n    thread_id = omp_get_thread_num();\n    std::vector<int> local_image(image.begin() + num_pixels/size * rank,\n                                 image.begin() + num_pixels/size * (rank+1));\n    for (int i = 0; i < num_pixels/size; i++) {\n      local_bins[local_image[i]] += 1;\n    }\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int n = image.size();\n  // your code goes here\n}",
            "#pragma omp parallel\n{\n    int id = omp_get_thread_num();\n    int nthreads = omp_get_num_threads();\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = id * image.size() / nthreads;\n    int end = (id + 1) * image.size() / nthreads;\n\n    int numBins = 0;\n    for (int i = start; i < end; ++i) {\n        numBins += 1;\n    }\n\n    std::vector<size_t> localBins(numBins);\n    size_t localSum = 0;\n    for (int i = start; i < end; ++i) {\n        size_t index = image[i] - 0;\n        localBins[index] += 1;\n        localSum += 1;\n    }\n\n    MPI_Reduce(&localSum, &(bins[0]), 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(localBins.data(), &(bins[0]) + 1, numBins, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}\n}",
            "// your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    if (rank == 0) {\n        // Initialize bins to zero.\n        bins.fill(0);\n    }\n    \n    int start_index = rank * image.size() / size;\n    int end_index = (rank + 1) * image.size() / size;\n    \n    // Count image[start_index...end_index - 1]\n    // Do this with OpenMP.\n    int local_counts[256];\n    std::fill(local_counts, local_counts + 256, 0);\n    #pragma omp parallel for\n    for (int i = start_index; i < end_index; ++i) {\n        local_counts[image[i]]++;\n    }\n    \n    // Sum the counts on rank 0.\n    MPI_Reduce(local_counts, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int n = image.size();\n\tsize_t p = 0;\n\t#pragma omp parallel for reduction(+:p)\n\tfor(int i = 0; i < n; ++i) {\n\t\t++bins[image[i]];\n\t\t++p;\n\t}\n\tif(p!= n) {\n\t\tstd::cout << \"Didn't find all the pixels!\" << std::endl;\n\t}\n}",
            "// TODO\n}",
            "int rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tif (rank == 0) {\n\t\tstd::fill(bins.begin(), bins.end(), 0);\n\t}\n\n\tint nThreads = omp_get_max_threads();\n\n\t#pragma omp parallel num_threads(nThreads)\n\t{\n\t\tint tId = omp_get_thread_num();\n\t\tint nTh = omp_get_num_threads();\n\n\t\tint nThLocal = image.size() / nTh;\n\n\t\tint start = nThLocal * tId;\n\t\tint end = start + nThLocal;\n\n\t\tif (tId == (nTh - 1)) {\n\t\t\tend = image.size();\n\t\t}\n\n\t\tfor (int i = start; i < end; i++) {\n\t\t\tbins[image[i]]++;\n\t\t}\n\t}\n\n\tMPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// Your code here.\n}",
            "// TODO: Your code here\n}",
            "// your code here\n}",
            "// Your code here.\n   \n   size_t local_image_size = image.size();\n   size_t image_size = local_image_size*omp_get_num_threads();\n   \n   std::vector<int> local_image(image_size);\n   std::copy(image.begin(), image.end(), local_image.begin());\n   \n   #pragma omp parallel\n   {\n      int rank = omp_get_thread_num();\n      size_t start_index = rank*local_image_size;\n      size_t end_index = start_index + local_image_size;\n      std::array<size_t, 256> local_bins{};\n      \n      for(size_t i = start_index; i < end_index; i++)\n      {\n         local_bins[image[i]]++;\n      }\n      \n      #pragma omp critical\n      {\n         for(size_t i = 0; i < 256; i++)\n         {\n            bins[i] += local_bins[i];\n         }\n      }\n      \n   }\n   \n   #pragma omp barrier\n   MPI_Reduce(bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO: implement this function\n  // HINT: use MPI_Datatype and MPI_Reduce\n}",
            "// TODO\n }",
            "// TODO: Fill this in.\n}",
            "// TODO: Fill in code below\n  // You may add additional functions to help\n\n  int rank, size;\n  int i = 0;\n  int *hist;\n  int num_threads = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_pixels = image.size();\n  // int numpixels = image.size();\n  int chunksize = num_pixels / size;\n\n  if (rank == 0) {\n    hist = new int[256];\n    memset(hist, 0, sizeof(int) * 256);\n  }\n\n  hist = (int *)malloc(sizeof(int) * 256);\n  memset(hist, 0, sizeof(int) * 256);\n\n  MPI_Scatter(image.data(), chunksize, MPI_INT, image.data(), chunksize, MPI_INT, 0, MPI_COMM_WORLD);\n\n  num_threads = omp_get_max_threads();\n\n  // omp_set_num_threads(1);\n  // omp_set_num_threads(num_threads);\n  #pragma omp parallel num_threads(num_threads)\n  {\n    // int tid = omp_get_thread_num();\n    // int nthreads = omp_get_num_threads();\n    int i, j;\n    int hist_local[256];\n    memset(hist_local, 0, sizeof(int) * 256);\n\n    #pragma omp for schedule(static)\n    for (i = 0; i < chunksize; i++) {\n      hist_local[image[i]] += 1;\n    }\n\n    #pragma omp critical\n    {\n      for (i = 0; i < 256; i++) {\n        hist[i] += hist_local[i];\n      }\n    }\n  }\n\n  MPI_Reduce(hist, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    delete[] hist;\n  }\n\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n   //...\n}",
            "// TODO: Your code here.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n_pixels = image.size();\n  int n_pixels_per_rank = n_pixels / size;\n  int n_pixels_remaining = n_pixels % size;\n\n  std::vector<int> local_image;\n  // rank 0 reads all pixels\n  if (rank == 0) {\n    local_image.assign(image.begin(), image.end());\n    MPI_Scatter(local_image.data(), n_pixels_per_rank + n_pixels_remaining, MPI_INT,\n    local_image.data(), n_pixels_per_rank + n_pixels_remaining, MPI_INT, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Scatter(local_image.data(), n_pixels_per_rank, MPI_INT,\n    local_image.data(), n_pixels_per_rank, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n\n  // Count the number of pixels in the local image, store in bins.\n  // std::array<size_t, 256> bins;\n  // bins.fill(0);\n  #pragma omp parallel for\n  for (int i = 0; i < local_image.size(); i++) {\n    bins[local_image[i]]++;\n  }\n  \n  // sum bins of all ranks\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n  size_t n = image.size();\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    bins[image[i]]++;\n  }\n}",
            "int rank, size;\n    int *counts = nullptr;\n    int *local_bins = nullptr;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    counts = new int[size];\n    local_bins = new int[256];\n\n    MPI_Gather(&image[0], image.size(), MPI_INT, counts, image.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    std::fill(local_bins, local_bins + 256, 0);\n    std::fill(counts, counts + size, 0);\n\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        #pragma omp for schedule(static)\n        for (int i = 0; i < image.size(); i++) {\n            local_bins[image[i]]++;\n        }\n        counts[id] = local_bins[id];\n    }\n    MPI_Scatter(counts, 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n    delete[] counts;\n    delete[] local_bins;\n }",
            "// TODO\n  const int n_threads = omp_get_max_threads();\n  int n_procs;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = image.size() / n_procs;\n  int l_start = rank * n;\n  int l_end = (rank + 1) * n;\n  int n_rows = image.size();\n  int n_cols = n / n_procs;\n  std::vector<int> my_image(image.begin() + l_start, image.begin() + l_end);\n  std::vector<int> my_bins(n_threads);\n  std::vector<int> my_counts(n_threads);\n  #pragma omp parallel num_threads(n_threads)\n  {\n    int start_row = omp_get_thread_num() * n_rows / n_threads;\n    int end_row = (omp_get_thread_num() + 1) * n_rows / n_threads;\n    for(int i = start_row; i < end_row; ++i)\n    {\n      #pragma omp for\n      for(int j = 0; j < my_image.size(); ++j)\n      {\n        my_bins[omp_get_thread_num()] += my_image[j];\n      }\n    }\n  }\n  MPI_Reduce(my_bins.data(), my_counts.data(), n_threads, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank == 0)\n  {\n    for(int i = 0; i < my_counts.size(); ++i)\n    {\n      bins[i] = my_counts[i];\n    }\n  }\n}",
            "size_t n = image.size();\n    // TODO: Fill in the function\n    size_t localCount = 0;\n    size_t i = 0;\n    for(; i < n; i++) {\n        localCount++;\n    }\n    bins[image[0]] += localCount;\n    for(; i < n; i += omp_get_max_threads()) {\n        // TODO: Parallelize this loop with OpenMP\n    }\n}",
            "// Your code here\n }",
            "#pragma omp parallel for\n    for (int i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < image.size(); ++i) {\n        bins[image[i]] += 1;\n    }\n}",
            "// TO DO\n}",
            "// TODO: implement\n}",
            "#pragma omp parallel default(shared)\n   {\n      #pragma omp single\n      {\n          bins.fill(0);\n      }\n\n      #pragma omp for schedule(static)\n      for (size_t i=0; i < image.size(); ++i) {\n          bins[image[i]]++;\n      }\n   }\n }",
            "#pragma omp parallel for\n\tfor(unsigned int i = 0; i < image.size(); i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "// your code here\n  int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  int num_local_pixels = image.size()/num_procs;\n  if (rank == 0) {\n    bins = std::array<size_t, 256> {0};\n  }\n  std::array<size_t, 256> local_bins = std::array<size_t, 256> {0};\n  #pragma omp parallel for\n  for (size_t i = 0; i < num_local_pixels; i++) {\n    local_bins[image[rank*num_local_pixels + i]]++;\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* TODO: Fill in your code here. */\n\n}",
            "// Write your code here\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  \n  int localSize = image.size()/size;\n  int localSum = 0;\n  \n  std::vector<int> localBins(256);\n  // initialize bins to 0\n  for(int i = 0; i < 256; i++) {\n      localBins[i] = 0;\n  }\n  \n  // compute local sum\n  for(int i = rank*localSize; i < (rank+1)*localSize; i++) {\n      localSum = localSum + image[i];\n      localBins[image[i]] += 1;\n  }\n  \n  std::vector<int> globalSums(size, 0);\n  MPI_Gather(&localSum, 1, MPI_INT, &globalSums[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n  if(rank == 0) {\n      // accumulate local sum\n      for(int i = 1; i < size; i++) {\n          globalSums[i] = globalSums[i] + globalSums[i-1];\n      }\n      // update global bins\n      for(int i = 0; i < 256; i++) {\n          bins[i] = globalSums[rank] + localBins[i];\n      }\n  } else {\n      // update global bins\n      for(int i = 0; i < 256; i++) {\n          MPI_Send(&localBins[i], 1, MPI_INT, 0, rank, MPI_COMM_WORLD);\n      }\n  }\n}",
            "// TODO: complete\n}",
            "// TODO: Your code goes here\n}",
            "#pragma omp parallel for num_threads(20)\n  for (auto i = 0; i < image.size(); ++i) {\n    auto intensity = image[i];\n    bins[intensity] += 1;\n  }\n  /* Your code goes here! */\n  #pragma omp parallel for num_threads(20)\n  for (auto i = 0; i < image.size(); ++i) {\n    auto intensity = image[i];\n    bins[intensity] += 1;\n  }\n}",
            "// YOUR CODE HERE\n }",
            "// TODO: Your code here\n    int rank, size, maxval;\n    int *counts = NULL, *total = NULL;\n    int localCount[256];\n    int *offset = NULL;\n    int numPixels = image.size();\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        counts = (int *)malloc(sizeof(int) * size * 256);\n        total = counts + 256 * size;\n    }\n\n    memset(localCount, 0, sizeof(int) * 256);\n    for (int i = 0; i < numPixels; i++) {\n        localCount[image[i]]++;\n    }\n\n    MPI_Allreduce(localCount, counts, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        offset = (int *)malloc(sizeof(int) * size);\n\n        int sum = 0;\n        for (int i = 1; i < size; i++) {\n            offset[i] = sum;\n            sum += counts[i * 256];\n        }\n        offset[0] = 0;\n        for (int i = 0; i < 256; i++)\n            total[i] = sum;\n\n        for (int i = 1; i < size; i++)\n            for (int j = 0; j < 256; j++)\n                total[j] += counts[i * 256 + j];\n    }\n\n    //for (int i = 0; i < 256; i++)\n    //    printf(\"rank %d bin %d %lu\\n\", rank, i, counts[i]);\n\n    if (rank!= 0) {\n        counts = counts + 256 * rank;\n        offset = offset + rank;\n    }\n\n    MPI_Scatterv(counts, 256, offset, MPI_INT, &maxval, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int *localBins = (int *)malloc(sizeof(int) * maxval);\n    memset(localBins, 0, sizeof(int) * maxval);\n\n    #pragma omp parallel for\n    for (int i = 0; i < numPixels; i++) {\n        localBins[image[i]]++;\n    }\n\n    MPI_Reduce(localBins, counts, maxval, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 1; i < size; i++)\n            for (int j = 0; j < maxval; j++)\n                total[j] += counts[i * maxval + j];\n    }\n\n    MPI_Scatterv(counts, maxval, offset, MPI_INT, bins.data(), maxval, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "//TODO: Write implementation for OpenMP\n    //HINT: Remember to set the number of threads with omp_set_num_threads\n    omp_set_num_threads(2);\n\n    //TODO: Initialize bins to zero\n    #pragma omp parallel for\n    for (int i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n\n    //TODO: Use OpenMP to count the number of pixels in image with each intensity value\n    //HINT: Remember that you may need to increment the bins array\n#pragma omp parallel for\n    for (int i = 0; i < image.size(); ++i) {\n        ++bins[image[i]];\n    }\n}",
            "// Fill in your code here...\n }",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // Only rank 0 should perform the actual counting\n    if (rank == 0) {\n        // Initialize bins to 0\n        std::fill(bins.begin(), bins.end(), 0);\n        // Count the pixel values\n        for (int i = 0; i < image.size(); i++) {\n            bins[image[i]] += 1;\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n    // Each rank sends its bin count to rank 0\n    MPI_Gather(bins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    // Only rank 0 will print the result\n    if (rank == 0) {\n        for (int i = 0; i < 256; i++) {\n            std::cout << i << \": \" << bins[i] << \"\\n\";\n        }\n    }\n}",
            "}",
            "// TODO: implement\n    std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for(int i = 0; i < image.size(); ++i) {\n        int idx = image[i];\n        bins[idx] += 1;\n    }\n}",
            "// TODO: your code here\n }",
            "// Your code here\n  int myid, numprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n  std::vector<int> img;\n  int start, end;\n  int num_bins = bins.size();\n  int pixel_counts[num_bins];\n\n  // Divide the size of the image among the process\n  start = (image.size() * myid) / numprocs;\n  end = (image.size() * (myid + 1)) / numprocs;\n  for (int i = start; i < end; i++) {\n    img.push_back(image[i]);\n  }\n\n  // Count the number of pixels with each intensity\n  // Use OpenMP here\n  #pragma omp parallel for\n  for (int i = 0; i < num_bins; i++) {\n    pixel_counts[i] = 0;\n  }\n  for (int i = 0; i < img.size(); i++) {\n    pixel_counts[img[i]] += 1;\n  }\n\n  // Gather the results from every process and store them in bins\n  // Use MPI here\n  MPI_Gather(pixel_counts, num_bins, MPI_INT, bins.data(), num_bins, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "size_t n = image.size();\n    size_t rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int start = rank * n / size;\n    int end = (rank + 1) * n / size;\n\n    size_t count = 0;\n#pragma omp parallel for\n    for (int i = start; i < end; ++i) {\n        ++bins[image[i]];\n    }\n    MPI_Reduce(&bins, &bins, 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n}",
            "/* TODO */\n  //size_t size = image.size();\n  //bins.resize(256);\n  //omp_set_num_threads(16);\n//    #pragma omp parallel for\n//    for (int i = 0; i < 256; i++)\n//    {\n//      bins[i] = 0;\n//    }\n//\n//    #pragma omp parallel for\n//    for (int i = 0; i < size; i++)\n//    {\n//      bins[image[i]] += 1;\n//    }\n//\n//    #pragma omp parallel for\n//    for (int i = 0; i < size; i++)\n//    {\n//      int p = image[i];\n//      int s = bins[p] + 1;\n//      bins[p] = s;\n//    }\n\n  #pragma omp parallel for\n  for (int i = 0; i < 256; i++)\n    {\n      bins[i] = 0;\n    }\n\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++)\n    {\n      bins[image[i]] += 1;\n    }\n\n  #pragma omp parallel for\n  for (int i = 0; i < image.size(); i++)\n    {\n      int p = image[i];\n      int s = bins[p] + 1;\n      bins[p] = s;\n    }\n}",
            "// Your code goes here!\n}",
            "}",
            "// TODO: implement this function\n }",
            "// TODO: implement this\n    size_t width = image.size();\n    bins = std::array<size_t, 256>();\n    // bins = {0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0",
            "int const n = 4; // number of pixels in a block\n   int const m = 2; // number of blocks in x direction\n   int const l = 2; // number of blocks in y direction\n\n   // TODO: fill in the rest of this function.\n }",
            "int rank, num_procs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   // Every process has a complete copy of the image.\n   // We can count in parallel, just split up the work.\n   // (We could also do the counting in serial, but would have to split up the work.)\n\n   // Do the work in parallel.\n   // This example uses OpenMP for parallelism.\n   // We'll also need to use some MPI functions.\n   // (Note that MPI_Bcast is a collective operation.)\n   // We'll also need to use some OpenMP functions.\n   // (Note that omp_get_thread_num gives the ID of the current thread.)\n\n   // Start the parallel section.\n   #pragma omp parallel\n   {\n     // Get the thread ID.\n     int thread_id = omp_get_thread_num();\n\n     // Do something with the thread ID.\n\n     // Do some work for each thread.\n     // The first thread does a little more work.\n     if (thread_id == 0) {\n       // This is the first thread.\n\n       // Each thread processes 1/n slices.\n       // This is the work for the first thread.\n       // Since we're using OpenMP, this does a little more work than the other threads.\n\n       // Split the work.\n       // This is the work for all threads.\n       // We have to calculate the work split.\n       // (We could just do it manually, but that would be more error-prone.)\n\n       // Calculate the number of threads.\n       // (We could just get the number of threads using omp_get_max_threads,\n       // but it would be more error-prone.)\n\n       // Calculate the slice length.\n       // (We could just get the number of elements in image,\n       // but it would be more error-prone.)\n\n       // Calculate the start and end indices for the first thread.\n       // (We could just calculate the slice length and do a for loop,\n       // but that would be more error-prone.)\n\n       // Do some work.\n       // (We'll calculate the histogram of the slice,\n       // since we only have one thread doing the work.)\n       // For example, the histogram for slice [1, 6] is { 0, 0, 2, 0, 1, 0, 0, 0, 1, 0 }\n       // (Note that we have to explicitly initialize bins.)\n       // (Note that we have to explicitly copy the slice of the image.)\n\n       // Do more work.\n\n       // Copy the histogram to the main array.\n       // (Note that we have to explicitly copy the histogram.)\n     } else {\n       // This is a later thread.\n\n       // Each thread processes 1/n slices.\n       // This is the work for the later threads.\n       // Since we're using OpenMP, this does a little more work than the other threads.\n\n       // Split the work.\n       // This is the work for all threads.\n       // We have to calculate the work split.\n       // (We could just do it manually, but that would be more error-prone.)\n\n       // Calculate the number of threads.\n       // (We could just get the number of threads using omp_get_max_threads,\n       // but it would be more error-prone.)\n\n       // Calculate the slice length.\n       // (We could just get the number of elements in image,\n       // but it would be more error-prone.)\n\n       // Calculate the start and end indices for the first thread.\n       // (We could just calculate the slice length and do a for loop,\n       // but that would be more error-prone.)\n\n       // Do some work.\n       // (We'll calculate the histogram of the slice,\n       // since we only have one thread doing the work.)\n       // For example, the histogram for slice [1, 6] is { 0, 0, 2, 0, 1, 0, 0, 0, 1, 0 }\n       // (Note that we have to explicitly initialize bins.)\n       // (Note that we have to explicitly copy the slice of the image.)\n\n       // Do more work.\n\n       // Copy the histogram to the main array.\n       // (Note that we have to explicitly copy the histogram.)\n     }\n   }\n\n   // Stop the parallel section.\n }",
            "// TODO: Complete the implementation\n  // 1. First, each process needs to determine the number of pixels that have each grayscale intensity.\n  //    Store the results in a private array (bins) and then allreduce the bins into an array on rank 0.\n  //    The allreduce operation should be done in parallel.\n\n  // 2. Now the bins should be distributed among the processes.\n  //    Each process needs to determine the number of pixels that are above a certain threshold.\n  //    Store the results in a private array (pixels) and then allreduce the pixels into an array on rank 0.\n  //    The allreduce operation should be done in parallel.\n\n  // 3. Finally, each process should divide the results in bins by the number of pixels\n  //    and store the result in the array pixels in the proper position.\n  //    After all the processes finish, the array pixels should contain the desired result.\n }",
            "// TODO: Implement this function!\n}",
            "// TODO: Implement this function.\n   int comm_sz = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n   int comm_rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n   int chunk_sz = image.size() / comm_sz;\n   int last_sz = chunk_sz + image.size() % comm_sz;\n   std::vector<int> chunk;\n   if (comm_rank == 0) {\n    for (int i = 0; i < chunk_sz * comm_sz; i++) {\n      chunk.push_back(image[i]);\n    }\n    for (int i = chunk_sz * comm_sz; i < image.size(); i++) {\n      chunk.push_back(0);\n    }\n   }\n   std::array<size_t, 256> local_bins;\n   for (int i = 0; i < 256; i++) {\n     local_bins[i] = 0;\n   }\n   if (comm_rank == 0) {\n    for (int i = 0; i < chunk_sz * comm_sz; i++) {\n      local_bins[chunk[i]]++;\n    }\n    for (int i = chunk_sz * comm_sz; i < image.size(); i++) {\n      local_bins[0]++;\n    }\n   } else {\n     for (int i = 0; i < chunk_sz; i++) {\n      local_bins[image[i]]++;\n     }\n     for (int i = chunk_sz; i < last_sz; i++) {\n      local_bins[0]++;\n     }\n   }\n   for (int i = 0; i < 256; i++) {\n     int tmp_local = local_bins[i];\n     MPI_Reduce(&tmp_local, &(bins[i]), 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n }",
            "#pragma omp parallel for\n  for (int i = 0; i < 256; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for schedule(static)\n  for (int i = 0; i < image.size(); ++i) {\n    bins[image[i]]++;\n  }\n}",
            "int rank, num_procs;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n     size_t image_size = image.size();\n     size_t chunk_size = image_size / num_procs;\n     size_t extra = image_size % num_procs;\n     size_t start = rank * chunk_size;\n     size_t end = start + chunk_size;\n     if (rank < extra) {\n         end++;\n     }\n     // Add up to end index of image\n     int sum = 0;\n     for (size_t i = start; i < end; i++) {\n         sum += image[i];\n     }\n     // Reduce and store the result\n     MPI_Reduce(&sum, &bins[0], 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// Initialize bins to all zeros\n    for (auto &bin : bins) {\n        bin = 0;\n    }\n\n    // Compute the number of pixels in each intensity bin using parallelism\n\n    //...\n}",
            "int nThreads, myId;\n    MPI_Comm_size(MPI_COMM_WORLD, &nThreads);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myId);\n    size_t const imgSize = image.size();\n\n    std::vector<size_t> pixels(256);\n    #pragma omp parallel for\n    for (size_t i = 0; i < imgSize; ++i) {\n        ++pixels[image[i]];\n    }\n\n    // MPI_Gatherv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, const int recvcounts[],\n    //             const int displs[], MPI_Datatype recvtype, int root, MPI_Comm comm)\n    // sendbuf - pointer to send buffer\n    // sendcount - number of elements to send\n    // sendtype - data type of send buffer elements\n    // recvbuf - pointer to receive buffer\n    // recvcounts - array of integer values, one value per process, giving the number of elements that process i receives\n    // displs - array of integer values, one value per process, specifying the displacement relative to recvbuf at which to place the incoming data from process i\n    // recvtype - data type of receive buffer elements\n    // root - rank of receiving process\n    // comm - communicator\n    MPI_Gatherv(pixels.data(), 256, MPI_UNSIGNED_LONG, bins.data(), nullptr, nullptr, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (myId == 0) {\n        for (size_t i = 1; i < nThreads; ++i) {\n            for (size_t j = 0; j < 256; ++j) {\n                bins[j] += bins[j + 256 * i];\n            }\n        }\n    }\n}",
            "// TODO: count the pixels in image\n}",
            "}",
            "std::array<size_t, 256> local_bins;\n   std::fill(local_bins.begin(), local_bins.end(), 0);\n\n   int nthreads = omp_get_max_threads();\n   #pragma omp parallel num_threads(nthreads)\n   {\n     int myid = omp_get_thread_num();\n     int num_threads = omp_get_num_threads();\n     std::vector<int> thread_image(image.size());\n\n     // assign each thread its chunk of image\n     int start = (image.size() + num_threads - 1) / num_threads * myid;\n     int end = std::min((image.size() + num_threads - 1) / num_threads * (myid + 1), (int) image.size());\n     std::copy(image.begin() + start, image.begin() + end, thread_image.begin());\n\n     // count pixels in this chunk\n     for (int i = 0; i < end - start; i++) {\n       local_bins[thread_image[i]]++;\n     }\n   }\n\n   // combine bins across all threads\n   for (size_t i = 0; i < bins.size(); i++) {\n     #pragma omp atomic\n     bins[i] += local_bins[i];\n   }\n }",
            "int myRank, numProcs;\n     MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n     MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n     if(image.size()!= 0) {\n         bins.fill(0);\n#pragma omp parallel\n         {\n             int threadId = omp_get_thread_num();\n             int lowerBound = threadId * image.size() / numProcs;\n             int upperBound = (threadId + 1) * image.size() / numProcs;\n             for(size_t i = lowerBound; i < upperBound; ++i) {\n                 ++bins[image[i]];\n             }\n         }\n     }\n     MPI_Gather(bins.data(), bins.size(), MPI_INT, bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n }",
            "// YOUR CODE HERE\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads = 2;\n\n    int n = image.size();\n\n    int chunk = n / size;\n    int rem = n % size;\n    int start = rank * chunk + std::min(rank, rem);\n    int end = start + chunk + (rank < rem);\n\n    std::vector<int> local_image(image.begin() + start, image.begin() + end);\n    std::vector<size_t> counts(256, 0);\n\n    #pragma omp parallel num_threads(num_threads)\n    {\n        int thread_id = omp_get_thread_num();\n        int thread_count = omp_get_num_threads();\n        int chunk = local_image.size() / thread_count;\n        int rem = local_image.size() % thread_count;\n        int start = thread_id * chunk + std::min(thread_id, rem);\n        int end = start + chunk + (thread_id < rem);\n\n        for (int i = start; i < end; ++i) {\n            counts[local_image[i]]++;\n        }\n    }\n\n    MPI_Reduce(counts.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   size_t image_size = image.size();\n   size_t my_size = image_size/num_processes;\n\n   // Start and end indices for this process\n   size_t start = (rank * my_size);\n   size_t end = ((rank+1) * my_size);\n\n   // Number of pixels that will be counted in this process\n   int pixels_to_count = end-start;\n\n   // Store the number of pixels that will be counted by all processes\n   int n_pixels;\n   MPI_Reduce(&pixels_to_count, &n_pixels, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Start counting\n   size_t count = 0;\n   for(int i = 0; i < n_pixels; i++){\n      int pixel = image[start+i];\n      bins[pixel]++;\n      count++;\n   }\n\n}",
            "#pragma omp parallel for\n  for(int i = 0; i < 256; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for(int i = 0; i < image.size(); i++) {\n    bins[image[i]]++;\n  }\n}",
            "// Your code here\n\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for nowait\n        for (int i = 0; i < image.size(); ++i) {\n            // do nothing\n        }\n\n        #pragma omp for nowait\n        for (int i = 0; i < image.size(); ++i) {\n            // do nothing\n        }\n\n        #pragma omp for\n        for (int i = 0; i < image.size(); ++i) {\n            // do nothing\n        }\n\n        //...\n\n        #pragma omp for\n        for (int i = 0; i < image.size(); ++i) {\n            // do nothing\n        }\n    }\n}",
            "/* Write your solution here. */\n  size_t mySize = image.size();\n  int myStart = omp_get_thread_num() * (mySize / omp_get_num_threads());\n  int myEnd = (omp_get_thread_num() + 1) * (mySize / omp_get_num_threads());\n  int myMax = image[myStart];\n  int myMin = image[myStart];\n\n  for (size_t i = myStart; i < myEnd; i++) {\n    if (image[i] > myMax) {\n      myMax = image[i];\n    }\n    if (image[i] < myMin) {\n      myMin = image[i];\n    }\n  }\n  size_t myRange = myMax - myMin + 1;\n  std::vector<size_t> myBins(myRange);\n  for (size_t i = myStart; i < myEnd; i++) {\n    myBins[image[i] - myMin]++;\n  }\n  // now we can safely sum the results\n  // this is safe because we're not changing the values\n  // only summing them up\n  for (int i = 1; i < myRange; i++) {\n    myBins[i] += myBins[i - 1];\n  }\n  if (omp_get_thread_num() == 0) {\n    // first thread does the heavy lifting\n    MPI_Reduce(myBins.data(), bins.data(), myRange, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    // all other threads are just waiting for the first one to finish\n    MPI_Reduce(myBins.data(), bins.data(), myRange, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "/* WRITE YOUR CODE HERE */\n}",
            "bins = std::array<size_t, 256>{}; // initialize with zeros\n     int n = image.size();\n\n//  ================================================================\n// YOUR CODE GOES HERE\n// ================================================================\n\n}",
            "// add your code here\n }",
            "// TODO: write code here\n    #pragma omp parallel for\n    for(auto i = 0; i < image.size(); i++){\n        bins[image[i]]++;\n    }\n    return;\n}",
            "// TODO\n}",
            "/* Your code here */\n   /* Hint: Each rank will need to know how many pixels are in each intensity bin */\n   /* Hint: Each rank will need to know which bin each pixel belongs to */\n   /* Hint: The results from each rank will need to be gathered back on rank 0 */\n   /* Hint: Remember to synchronize when MPI calls are used */\n   /* Hint: OpenMP is already initialized for you */\n}",
            "// YOUR CODE HERE\n\n    // YOUR CODE HERE\n}",
            "// TODO\n}",
            "// TODO\n   #pragma omp parallel\n   #pragma omp single\n   {\n      auto bin = bins.begin();\n      for(int p = 0; p < image.size(); p++){\n         *(bin+image[p])++;\n      }\n   }\n}",
            "// TODO: Your code here.\n  //\n  // Hint:\n  // - each rank should have a complete copy of image\n  // - use MPI_Scatter to split image into a local copy for each rank\n  // - use OpenMP to parallelize the computation of each pixel's intensity in\n  //   each rank's copy of image.\n\n\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < image.size(); ++i) {\n    bins[image[i]]++;\n  }\n}",
            "//...\n}",
            "bins = std::array<size_t, 256> {0};\n\t// TODO: implement pixelCounts\n}",
            "int num_procs, rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   size_t local_start_index = (image.size() / num_procs) * rank;\n   size_t local_end_index = (image.size() / num_procs) * (rank + 1);\n\n   std::vector<int> local_image(image.begin() + local_start_index, image.begin() + local_end_index);\n\n   // Compute local histogram\n   std::array<size_t, 256> local_histogram;\n   std::fill(local_histogram.begin(), local_histogram.end(), 0);\n   for (auto p : local_image) {\n     local_histogram[p]++;\n   }\n\n   // Reduce local histogram to master rank\n   std::array<size_t, 256> global_histogram;\n   MPI_Reduce(local_histogram.data(), global_histogram.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Save histogram to output array\n   if (rank == 0) {\n     std::copy(global_histogram.begin(), global_histogram.end(), bins.begin());\n   }\n }",
            "int n = image.size();\n    std::vector<int> local_bins(256, 0);\n    std::vector<int> local_image(image.size());\n#pragma omp parallel\n    {\n        int thread_num = omp_get_thread_num();\n        int n_threads = omp_get_num_threads();\n#pragma omp for schedule(static)\n        for (int i = 0; i < n; i++) {\n            local_image[i] = image[i];\n        }\n#pragma omp for schedule(static)\n        for (int i = 0; i < n; i++) {\n            local_bins[local_image[i]]++;\n        }\n#pragma omp critical\n        for (int i = 0; i < 256; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "// TODO\n   //...\n }",
            "int num_ranks = 0;\n    int rank = 0;\n    int num_pixels = image.size();\n\n    // TODO: Compute the number of ranks (num_ranks) and your rank (rank) in MPI_COMM_WORLD\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Use MPI_Gather to distribute image to every rank\n    // Each rank should have a copy of image (except rank 0)\n    int* recvbuf = new int[num_pixels];\n    MPI_Gather(image.data(), num_pixels, MPI_INT, recvbuf, num_pixels, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    // TODO: Count number of pixels in each rank's copy of image\n    // For example, rank 1 has 116 pixels (2, 116, 201, 11, 92, 92, 201, 4, 2)\n    int local_count = 0;\n    for(int i = 0; i < num_pixels; i++)\n    {\n        if (recvbuf[i]!= 0)\n        {\n            local_count++;\n        }\n    }\n\n    // TODO: Use MPI_Gather to collect all of the counts from each rank to rank 0\n    // In other words, all ranks send their local_count to rank 0\n    int* recvcounts = new int[num_ranks];\n    int* displs = new int[num_ranks];\n\n    recvcounts[rank] = local_count;\n    displs[rank] = 0;\n    \n    MPI_Gatherv(recvcounts, 1, MPI_INT, recvcounts, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // TODO: Use MPI_Gatherv to distribute the counts to every rank\n    // In other words, rank 0 sends all of the counts to all of the ranks\n    // Example:\n    // rank 0: [0, 0, 2, 0, 1,...]\n    // rank 1: [0, 0, 1, 0, 0,...]\n    // rank 2: [0, 0, 1, 0, 0,...]\n\n    MPI_Gatherv(recvcounts, 1, MPI_INT, recvcounts, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int global_count = 0;\n    for(int i = 0; i < num_ranks; i++)\n    {\n        global_count += recvcounts[i];\n    }\n\n    // TODO: Use MPI_Gatherv to collect all of the counts from each rank to rank 0\n    // In other words, all ranks send their local_count to rank 0\n    // The result is stored in bins on rank 0\n    // For example, rank 1 sends [0, 0, 1, 0, 0,...]\n    // rank 2 sends [0, 0, 1, 0, 0,...]\n    MPI_Gatherv(recvcounts, 1, MPI_INT, recvcounts, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // TODO: Add all of the counts from every rank\n    // For example, rank 0 receives [0, 0, 2, 0, 1,...]\n    // rank 1 receives [0, 0, 1, 0, 0,...]\n    // rank 2 receives [0, 0, 1, 0, 0,...]\n    bins[0] = recvcounts[0];\n    for(int i = 1; i < 256; i++)\n    {\n        bins[i] += recvcounts[i];\n    }\n\n    // TODO: Compute the total number of pixels\n    int total_count = 0;\n    for(int i = 0; i < num_ranks; i++)\n    {\n        total_count += recvcounts[i];\n    }\n\n    // TODO: Use MPI_Gatherv to distribute the counts to every rank\n    // In other words, rank 0 sends all of the counts to all of the ranks\n    // Example:\n    // rank 0: [0, 0, 2, 0, 1,...]\n    // rank 1: [0, 0, 1, 0, 0,...]\n    // rank 2: [0, 0, 1, 0, 0,...]\n    MPI_Gatherv(recvcounts, 1, MPI_INT",
            "// TODO: Write your solution here.\n}",
            "// TODO: implement me\n }",
            "// TODO: Your code here\n }",
            "// TODO: implement\n  return;\n}",
            "//  your code here\n}",
            "size_t image_size = image.size();\n    int rank;\n    int num_procs;\n    int chunk_size = image_size / omp_get_max_threads();\n    int global_chunk_size;\n\n    std::array<size_t, 256> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    global_chunk_size = chunk_size * num_procs;\n\n    for(int i = rank * chunk_size; i < image_size; i += global_chunk_size) {\n        local_bins[image[i]]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0) {\n        for(size_t i = 0; i < 256; i++) {\n            bins[i] /= num_procs;\n        }\n    }\n}",
            "// YOUR CODE HERE\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  \n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  \n  int num_pixels = image.size();\n  int i;\n  \n  int num_threads = omp_get_max_threads();\n  \n  // Calculate the portion of the image to be assigned to each thread\n  int stride = num_pixels/num_threads;\n  \n  std::vector<int> my_image(image.begin()+stride*world_rank, image.begin()+stride*(world_rank+1));\n  \n  // Initialize bins with zero counts\n  std::fill(bins.begin(), bins.end(), 0);\n  \n  // Count the number of pixels in my portion of the image\n  for (auto pixel: my_image){\n    bins[pixel]++;\n  }\n  \n  // Sum the counts across all threads\n  for (int i = 1; i < num_threads; i++){\n    int sum = 0;\n    MPI_Reduce(&bins[i*world_size], &sum, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (world_rank == 0){\n      bins[i*world_size] = sum;\n    }\n  }\n  \n  if (world_rank == 0){\n    // Sum the counts across all threads\n    for (int i = 1; i < num_threads; i++){\n      int sum = 0;\n      MPI_Reduce(&bins[i*world_size], &sum, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n      bins[i*world_size] = sum;\n    }\n  }\n}",
            "/*... */\n}",
            "// TODO\n }",
            "std::fill(bins.begin(), bins.end(), 0);\n  // TODO: write your code here\n  \n  // Write your code here\n  omp_set_num_threads(2);\n  //#pragma omp parallel for\n  for (int i = 0; i < 1000; i++) {\n      bins[image[i]]++;\n  }\n}",
            "// TODO: your code here\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t len = image.size();\n  size_t num_proc = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  int block_size = len / num_proc;\n  int remainder = len % num_proc;\n  int start_index = block_size * rank;\n\n  if (rank < remainder) {\n    block_size++;\n  } else {\n    start_index += remainder;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = start_index; i < start_index + block_size; i++) {\n    bins[image[i]]++;\n  }\n\n  if (rank == 0) {\n    for (size_t i = 1; i < num_proc; i++) {\n      int recv_size = 0;\n      MPI_Status status;\n      MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n      MPI_Get_count(&status, MPI_INT, &recv_size);\n      std::vector<int> recv_buf(recv_size);\n      MPI_Recv(recv_buf.data(), recv_size, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      for (size_t j = 0; j < recv_size; j++) {\n        bins[recv_buf[j]] += recv_buf[recv_size + j];\n      }\n    }\n  } else {\n    std::vector<int> send_buf;\n    for (size_t i = 0; i < 256; i++) {\n      send_buf.push_back(bins[i]);\n    }\n    MPI_Send(send_buf.data(), 256, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Initialize histogram with zeros\n  // #omp parallel for\n  for (auto &bin : bins) bin = 0;\n  \n  #pragma omp parallel\n  {\n    #pragma omp for schedule(dynamic) nowait\n    for (size_t i = 0; i < image.size(); ++i) {\n      #pragma omp atomic\n      bins[image[i]]++;\n    }\n  }\n}",
            "int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   if (rank == 0) {\n    bins = std::array<size_t, 256> {};\n   }\n   MPI_Bcast(bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   int pixels = image.size();\n   int chunk = pixels / size;\n   int start = rank * chunk;\n   int end = (rank + 1) * chunk;\n   if (rank == size - 1) {\n    end = pixels;\n   }\n   int count = end - start;\n   std::vector<int> image_rank(count);\n#pragma omp parallel for schedule(static) num_threads(omp_get_max_threads())\n   for (int i = 0; i < count; i++) {\n    int gray = image[start + i];\n    bins[gray]++;\n   }\n   MPI_Reduce(MPI_IN_PLACE, bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n }",
            "// TODO\n    std::fill(bins.begin(), bins.end(), 0);\n}",
            "//TODO\n\n }",
            "if (image.size() == 0) return;\n    \n    // TODO: implement the parallel version of this function\n    // TODO: you may want to use an MPI_Datatype for passing the image\n    // hint: use MPI_Type_contiguous()\n    // hint: use MPI_Type_commit()\n    \n    // TODO: implement the parallel version of this function\n    // hint: use the following function to count the pixels\n    // int count_pixels(int *image, int size);\n    // hint: you can either iterate over the bins in parallel or\n    //       use a reduction to sum the values\n}",
            "// TODO: Your code here\n   #pragma omp parallel for\n   for (int i = 0; i < image.size(); i++) {\n     #pragma omp atomic\n     bins[image[i]]++;\n   }\n}",
            "// TODO\n   bins = std::array<size_t, 256>();\n   for (auto &bin : bins) {\n       bin = 0;\n   }\n}",
            "// TODO: implement this function\n\n   // Use the following function to make a copy of image on each MPI rank\n   std::vector<int> my_image = copyImage(image);\n\n   //TODO: implement this function\n\n   return;\n}",
            "// Your code goes here!\n }",
            "}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int rowsPerRank = image.size() / size;\n  int firstRow = rank * rowsPerRank;\n\n  int *pixelCount = new int[256];\n  for (int i = 0; i < 256; ++i) {\n    pixelCount[i] = 0;\n  }\n\n  int *localPixelCount = new int[256];\n  for (int i = 0; i < 256; ++i) {\n    localPixelCount[i] = 0;\n  }\n\n  int imageSize = image.size();\n  #pragma omp parallel for\n  for (int i = firstRow; i < imageSize; ++i) {\n    localPixelCount[image[i]]++;\n  }\n\n  MPI_Reduce(localPixelCount, pixelCount, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 256; ++i) {\n      bins[i] = pixelCount[i];\n    }\n  }\n\n  delete[] localPixelCount;\n  delete[] pixelCount;\n}",
            "// Your code here...\n}",
            "// TODO: implement me\n   // Hint: bins.size() is the number of bins (grayscale values)\n   // Hint: bins[bin] is the number of pixels with grayscale value `bin`\n   // Hint: omp_get_max_threads() is the number of threads\n   // Hint: MPI_Comm_size(MPI_COMM_WORLD) is the number of ranks\n   // Hint: MPI_Comm_rank(MPI_COMM_WORLD) is the rank\n   // Hint: MPI_Gather(src, n, MPI_INT, dst, n, MPI_INT, root)\n   //   copies `n` values from `src` to `dst` at rank `root`.\n\n   // Count the number of pixels for each intensity in the image\n   std::array<size_t, 256> localBins = {0};\n   for (int pixel : image) {\n       localBins[pixel] += 1;\n   }\n\n   // Gather bins from all ranks\n   MPI_Gather(localBins.data(), 256, MPI_UNSIGNED_LONG, bins.data(), 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n }",
            "int rank, size;\n     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n     MPI_Comm_size(MPI_COMM_WORLD, &size);\n     \n     // Your code here\n     \n }",
            "size_t N = image.size();\n  bins = std::array<size_t, 256>();\n  #pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    bins[image[i]]++;\n  }\n}",
            "// YOUR CODE HERE\n   const int image_size = image.size();\n   const int num_threads = 4;\n   int total_threads = omp_get_max_threads();\n   // Initialize bins to 0\n   for (size_t i = 0; i < 256; ++i) {\n     bins[i] = 0;\n   }\n   // Each thread will count its own grayscale intensity\n   #pragma omp parallel num_threads(total_threads)\n   {\n     int rank = omp_get_thread_num();\n     int local_bins[256];\n     memset(local_bins, 0, sizeof(local_bins));\n     #pragma omp for\n     for (int i = 0; i < image_size; ++i) {\n       local_bins[image[i]]++;\n     }\n     #pragma omp critical\n     {\n       // Combine the counts from all threads\n       for (size_t i = 0; i < 256; ++i) {\n         bins[i] += local_bins[i];\n       }\n     }\n   }\n   \n   // Combine the result from all ranks\n   MPI_Allreduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n }",
            "int total_pixels = image.size();\n    int my_pixels = total_pixels/num_procs;\n    int remainder_pixels = total_pixels%num_procs;\n    \n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    if(rank < remainder_pixels) {\n        for(int i = rank*my_pixels+1; i <= rank*my_pixels+my_pixels; i++) {\n            bins[image[i]]++;\n        }\n    } else {\n        for(int i = rank*my_pixels; i <= rank*my_pixels+my_pixels-1; i++) {\n            bins[image[i]]++;\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    std::array<size_t, 256> temp_bins;\n\n    if(rank == 0) {\n        for(int i = 1; i < num_procs; i++) {\n            MPI_Recv(temp_bins.data(), 256, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < 256; j++) {\n                bins[j] += temp_bins[j];\n            }\n        }\n    } else {\n        MPI_Send(bins.data(), 256, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD);\n    }\n\n }",
            "// MPI variables\n    int myrank, size, nthreads;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    nthreads = omp_get_max_threads();\n\n    // determine the number of pixels per rank based on the total number of pixels\n    // and the number of ranks\n    auto totalPixels = image.size();\n    auto pixelsPerRank = (totalPixels + size - 1) / size;\n\n    // calculate the start and end indices for this rank\n    auto rankStart = std::min(pixelsPerRank * myrank, totalPixels);\n    auto rankEnd = std::min(rankStart + pixelsPerRank, totalPixels);\n\n    // calculate the number of pixels this rank will count\n    auto rankPixels = rankEnd - rankStart;\n    if (rankPixels <= 0) {\n        return;\n    }\n\n    // each rank counts its pixels in parallel\n    // the output is stored in bins\n    #pragma omp parallel num_threads(nthreads) default(none)\n    {\n        #pragma omp for schedule(static)\n        for (int i = rankStart; i < rankEnd; i++) {\n            int index = image[i] + 1; // convert to 0-255 range\n            #pragma omp atomic\n            bins[index]++;\n        }\n    }\n\n    // reduce bins across ranks\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // if this rank is the master, bins contains the number of pixels in each grayscale value\n    if (myrank == 0) {\n        for (size_t i = 0; i < bins.size(); i++) {\n            bins[i] /= size;\n        }\n    }\n}",
            "/* TODO */\n }",
            "// TODO: Your code here\n}",
            "const size_t n = image.size();\n\n    // YOUR CODE HERE\n    //\n    // You will probably want to create a 2D array of size 256x256.\n    // The first index represents the grayscale intensity,\n    // the second index represents the pixel number.\n    // For example, bins[10][5] is the number of pixels with grayscale intensity 10\n    // in the 6th row of image.\n\n    // Initialize the 2D array\n    for(size_t i = 0; i < 256; i++)\n        for(size_t j = 0; j < 256; j++)\n            bins[i][j] = 0;\n\n    #pragma omp parallel for schedule(dynamic)\n    for(size_t i = 0; i < n; i++)\n        bins[image[i]][i]++;\n\n    // Reduce the size of the 2D array for every other rank so that only the ranks with a complete copy of image\n    // (the first rank) need to reduce the 2D array.\n    #pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            if(omp_get_thread_num() == 0)\n            {\n                for(size_t i = 0; i < 256; i++)\n                    for(size_t j = 0; j < 256; j++)\n                        if(i!= j)\n                            bins[i][j] = 0;\n            }\n        }\n    }\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < 256; i++)\n        for(size_t j = 0; j < 256; j++)\n        {\n            size_t sum = 0;\n\n            for(size_t k = 0; k < n; k++)\n                sum += bins[i][k];\n\n            bins[i][j] = sum;\n        }\n\n    // TODO: End of solution\n\n    // Check that the result is correct\n    if(omp_get_thread_num() == 0)\n        for(size_t i = 0; i < 256; i++)\n            for(size_t j = 0; j < 256; j++)\n                if(bins[i][j]!= (i == j))\n                    std::cout << \"Error! bins[\" << i << \"][\" << j << \"] should be \" << (i == j) << std::endl;\n}",
            "int num_procs = 0;\n  int rank = 0;\n  \n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  int *counts = new int[num_procs];\n  int *displacements = new int[num_procs];\n\n  int pixels_per_proc = image.size() / num_procs;\n  int extra_pixels = image.size() % num_procs;\n\n  for (int i = 0; i < num_procs; i++) {\n    counts[i] = pixels_per_proc;\n  }\n  for (int i = 0; i < extra_pixels; i++) {\n    counts[i]++;\n  }\n\n  MPI_Scan(counts, displacements, num_procs, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  std::vector<int> image_local;\n  image_local.resize(counts[rank]);\n\n  std::copy(image.begin() + displacements[rank], image.begin() + displacements[rank] + counts[rank], image_local.begin());\n\n  int local_bins[256];\n  for (int i = 0; i < 256; i++) {\n    local_bins[i] = 0;\n  }\n\n  for (int i = 0; i < image_local.size(); i++) {\n    local_bins[image_local[i]]++;\n  }\n\n  MPI_Reduce(local_bins, counts, 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = std::array<size_t, 256>{};\n    for (int i = 0; i < 256; i++) {\n      bins[i] = static_cast<size_t>(counts[i]);\n    }\n  }\n\n  delete[] counts;\n  delete[] displacements;\n}",
            "// TODO\n    // Hint:\n    // - image.size() is the size of the image\n    // - image[i] gives the intensity of pixel i in the image\n    // - use a shared memory parallelism to count the number of pixels in each grayscale intensity level\n    // - to keep the result correct on all ranks, use MPI_Reduce to sum up all the local results\n\n    //TODO\n\n    // Your code here\n    int image_size = image.size();\n\n    #pragma omp parallel for schedule(static, 1024)\n    for(int i = 0; i < image_size; ++i){\n        bins[image[i]] += 1;\n    }\n\n    int total_count = 0;\n    MPI_Allreduce(bins.data(), &total_count, 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    #pragma omp parallel for schedule(static, 1024)\n    for(int i = 0; i < image_size; ++i){\n        bins[image[i]] = total_count - bins[image[i]];\n    }\n\n}",
            "// write code here\n }",
            "// TODO\n }",
            "// TODO: Your code here.\n\n}",
            "// TODO: Your code goes here\n    int n = image.size();\n\n    bins = std::array<size_t, 256>(std::fill_n(bins.begin(), 256, 0));\n\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (int i = 0; i < n; i++) {\n            bins[image[i]]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (auto const& i : image)\n    bins[i]++;\n}",
            "// TODO: fill in this function\n    int num_procs, rank, n;\n    size_t pixels;\n    int color, count, color_total;\n    int color_list[256];\n    int color_total_list[256];\n    int color_total_sum[256];\n    int color_list_sum[256];\n    int color_list_sum_final[256];\n    int color_total_list_sum[256];\n    int color_total_list_sum_final[256];\n    n = image.size();\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // 1. Each process should calculate its own pixel number and then combine with MPI reduce\n    pixels = 0;\n    for (int i = 0; i < n; i++) {\n        color = image[i];\n        count = 0;\n        for (int j = i; j < n; j++) {\n            if (image[j] == color)\n                count++;\n        }\n        pixels += count;\n    }\n\n    MPI_Reduce(&pixels, &bins[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // 2. Each process should count its own pixels and then combine with MPI reduce\n    color_total = 0;\n    for (int i = 0; i < 256; i++) {\n        color_total += bins[i];\n    }\n\n    MPI_Reduce(&color_total, &bins[256], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // 3. Each process should calculate its own colors and then combine with MPI reduce\n    MPI_Scatter(&bins[0], 256, MPI_UNSIGNED_LONG, &color_list[0], 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    MPI_Scatter(&bins[256], 256, MPI_UNSIGNED_LONG, &color_total_list[0], 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // 4. Each process should calculate its own colors and then combine with MPI reduce\n    for (int i = 0; i < 256; i++) {\n        color_list_sum[i] = color_list[i];\n        color_total_list_sum[i] = color_total_list[i];\n    }\n\n    MPI_Reduce(&color_list_sum[0], &color_list_sum_final[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&color_total_list_sum[0], &color_total_list_sum_final[0], 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // 5. Each process should calculate its own colors and then combine with MPI reduce\n    for (int i = 0; i < 256; i++) {\n        bins[i] = color_list_sum_final[i];\n        bins[256 + i] = color_total_list_sum_final[i];\n    }\n}",
            "//TODO: Your code here.\n }",
            "// Your code here\n }",
            "std::array<size_t, 256> bins_private;\n\n    for (size_t i = 0; i < 256; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < image.size(); ++i) {\n        bins_private[image[i]] += 1;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < 256; ++i) {\n        bins[i] += bins_private[i];\n    }\n\n    return;\n}",
            "//TODO: IMPLEMENT\n  int num_threads = 10;\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int image_size = image.size();\n  std::array<int, 256> local_bins = {0};\n  int *local_image = (int *)malloc(sizeof(int) * image_size);\n  MPI_Scatter(image.data(), image_size, MPI_INT, local_image, image_size, MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    omp_set_num_threads(num_threads);\n  }\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < image_size; i++) {\n    local_bins[local_image[i]]++;\n  }\n  MPI_Gather(local_bins.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      for (int j = 0; j < 256; j++) {\n        bins[j] += bins[j + 256];\n      }\n    }\n  }\n  free(local_image);\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "int const rank = 0;\n    int const n = image.size();\n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    #pragma omp parallel for schedule(static)\n    for(size_t i = 0; i < n; i++) {\n        bins[image[i]] += 1;\n    }\n}",
            "size_t numPixels = image.size();\n   bins = std::array<size_t, 256>();\n   for (size_t i = 0; i < numPixels; i++) {\n      bins[image[i]]++;\n   }\n}",
            "// TODO\n }",
            "// TODO: Implement this function\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int div = image.size() / size;\n  int rem = image.size() % size;\n  std::vector<int> imageRank(image.begin() + rank * div, image.begin() + rank * div + div);\n  if (rank == size - 1) imageRank.insert(imageRank.end(), image.end() + rank * div, image.end() + image.size());\n  MPI_Barrier(MPI_COMM_WORLD);\n  std::array<size_t, 256> binsRank;\n  MPI_Reduce(imageRank.data(), binsRank.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::array<size_t, 256> binsMaster(0);\n    size_t pos = 0;\n    for (int i = 0; i < size; i++) {\n      std::copy(binsRank.begin() + pos, binsRank.begin() + pos + 256, binsMaster.begin() + pos);\n      pos += 256;\n    }\n    if (rem!= 0) {\n      std::copy(binsRank.begin() + pos, binsRank.begin() + pos + rem, binsMaster.begin() + pos);\n    }\n    bins = binsMaster;\n  }\n}",
            "// TODO\n}",
            "int my_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // distribute image to all processes\n    std::vector<int> image_local(image.size()/world_size);\n    MPI_Scatter(&image[0], image_local.size(), MPI_INT, &image_local[0], image_local.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    // get max/min value of image_local for each process\n    int min_image_local = *(std::min_element(image_local.begin(), image_local.end()));\n    int max_image_local = *(std::max_element(image_local.begin(), image_local.end()));\n\n    std::vector<int> counts(max_image_local - min_image_local + 1, 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < (int)image_local.size(); i++) {\n        int gray_value = image_local[i];\n        counts[gray_value]++;\n    }\n\n    MPI_Reduce(&counts[0], &bins[0], counts.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n\n  return;\n}",
            "//  size_t i = 0;\n//  int num_threads = 0;\n//  int tid = 0;\n//  #pragma omp parallel private(i, num_threads, tid)\n//  {\n//    tid = omp_get_thread_num();\n//    num_threads = omp_get_num_threads();\n//    i = tid * (image.size() / num_threads);\n//\n//    if (tid == num_threads - 1)\n//      i = image.size();\n//    int sum = 0;\n//    for (; i < image.size(); ++i)\n//      sum += image[i];\n//\n//    bins[tid] = sum;\n//  }\n//\n//  for (auto x : bins) {\n//    std::cout << x << \", \";\n//  }\n//  std::cout << std::endl;\n\n  size_t i;\n  int num_threads = 0;\n  int tid = 0;\n#pragma omp parallel private(i, num_threads, tid)\n  {\n    tid = omp_get_thread_num();\n    num_threads = omp_get_num_threads();\n    i = tid * (image.size() / num_threads);\n\n    if (tid == num_threads - 1)\n      i = image.size();\n    int sum = 0;\n    for (; i < image.size(); ++i)\n      sum += image[i];\n\n    bins[tid] = sum;\n  }\n\n  if (0 == tid) {\n    for (auto x : bins) {\n      bins[x] = 0;\n    }\n  }\n\n#pragma omp parallel private(i, num_threads, tid)\n  {\n    tid = omp_get_thread_num();\n    num_threads = omp_get_num_threads();\n    i = tid * (image.size() / num_threads);\n\n    if (tid == num_threads - 1)\n      i = image.size();\n    for (; i < image.size(); ++i)\n      ++bins[image[i]];\n  }\n\n}",
            "//TODO\n     size_t n = image.size();\n     int max = *std::max_element(image.begin(), image.end());\n     int min = *std::min_element(image.begin(), image.end());\n     size_t N = max-min+1;\n     std::vector<size_t> local_bins(N,0);\n     size_t local_sum=0;\n     if (omp_get_num_procs() == 1) {\n         #pragma omp parallel for\n         for (size_t i=0; i < n; ++i) {\n             local_bins[image[i]-min]+=1;\n         }\n     }\n     else {\n         int myrank, nprocs;\n         MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n         MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n         std::vector<size_t> counts_per_proc(nprocs);\n         std::vector<int> local_bins_int(N,0);\n         std::vector<size_t> offsets(nprocs);\n         std::vector<int> offsets_int(nprocs);\n         std::vector<int> sendcounts(nprocs);\n         std::vector<int> displs(nprocs);\n         std::vector<int> recvcounts(nprocs);\n         std::vector<int> recvdispls(nprocs);\n\n         offsets[0] = 0;\n         offsets_int[0] = 0;\n         for (int i=1; i<nprocs; ++i) {\n             offsets[i] = offsets[i-1] + (n+1)/nprocs;\n             offsets_int[i] = offsets_int[i-1] + (N+1)/nprocs;\n         }\n\n         MPI_Scatter(&image[0], 1, MPI_INT, &local_bins_int[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n         int offset = 0;\n         int local_size = n+1 - (n+1)%nprocs;\n         for (int i=1; i<nprocs; ++i) {\n             MPI_Scatter(&image[offset], 1, MPI_INT, &local_bins_int[offset_int[i]], 1, MPI_INT, i, MPI_COMM_WORLD);\n             offset += (n+1)/nprocs;\n         }\n\n         std::vector<size_t> recv_count_from_proc(nprocs, 0);\n         for (int i=0; i<nprocs; ++i) {\n             for (size_t j=0; j<local_bins_int.size(); ++j) {\n                 recv_count_from_proc[i] += local_bins_int[j];\n             }\n         }\n         MPI_Reduce_scatter(&recv_count_from_proc[0], &counts_per_proc[0], 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n         recvcounts[0] = counts_per_proc[0];\n         recvdispls[0] = 0;\n         for (int i=1; i<nprocs; ++i) {\n             recvcounts[i] = counts_per_proc[i];\n             recvdispls[i] = recvdispls[i-1] + recvcounts[i-1];\n         }\n         sendcounts[0] = 0;\n         displs[0] = 0;\n         for (int i=1; i<nprocs; ++i) {\n             sendcounts[i] = recvcounts[i];\n             displs[i] = recvdispls[i-1] + recvcounts[i-1];\n         }\n         MPI_Gatherv(&local_bins_int[0], recvcounts[myrank], MPI_INT, &local_bins_int[0], &recvcounts[0], &recvdispls[0], MPI_INT, 0, MPI_COMM_WORLD);\n         if (myrank==0) {\n             for (int i=0; i<nprocs; ++i) {\n                 for (int j=0; j<local_bins_int.size(); ++j) {\n                     local_bins[local_bins_int[j]] += counts_per_proc[i];\n                 }\n             }\n         }\n     }\n     if (myrank==0) {\n         std::copy(local_bins.begin(), local_bins.end(), bins.begin());\n     }\n }",
            "// TODO: your code here\n}",
            "std::array<size_t, 256> local_bins{};\n    size_t n = image.size();\n    const int num_threads = omp_get_max_threads();\n\n    // Compute the number of elements per thread\n    size_t elements_per_thread = n / num_threads;\n\n    // Count the pixels\n    #pragma omp parallel for\n    for (int i = 0; i < num_threads; ++i) {\n        size_t start = i * elements_per_thread;\n        size_t end = (i == (num_threads - 1))? n : start + elements_per_thread;\n        for (size_t j = start; j < end; ++j) {\n            ++local_bins[image[j]];\n        }\n    }\n\n    // Combine the results\n    #pragma omp critical\n    {\n        for (size_t i = 0; i < 256; ++i) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "// omp_set_num_threads(4);  // if you want to force a specific number of threads\n   \n   \n   int n_threads = 1;\n   #pragma omp parallel\n   {\n    n_threads = omp_get_num_threads();\n   }\n   \n   int n_procs = 1;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n   \n   std::vector<int> image_local(image);  // we don't want to mess with the input vector, so make a copy for each rank\n   std::array<size_t, 256> bins_local{};\n   \n   #pragma omp parallel\n   {\n    int n_local_threads = omp_get_num_threads();\n    int id = omp_get_thread_num();\n\n    // std::cout << \"rank: \" << rank << \", n_local_threads: \" << n_local_threads << \", id: \" << id << std::endl;\n    int id_local = id % n_local_threads;\n    int n_local_procs = 1;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &n_local_procs);\n    // std::cout << \"rank: \" << rank << \", n_local_procs: \" << n_local_procs << std::endl;\n\n    int n_local_rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &n_local_rank);\n    // std::cout << \"rank: \" << rank << \", n_local_rank: \" << n_local_rank << std::endl;\n\n    size_t pixels_per_proc = image.size() / n_local_procs;\n    size_t start = pixels_per_proc * n_local_rank;\n    size_t end = pixels_per_proc * (n_local_rank + 1);\n    size_t n_pixels = (end - start) / n_local_procs;\n    // std::cout << \"rank: \" << rank << \", start: \" << start << \", end: \" << end << \", n_pixels: \" << n_pixels << std::endl;\n\n    std::vector<int> image_local_omp(image_local.begin() + start, image_local.begin() + end);\n    std::array<size_t, 256> bins_local_omp{};\n    for (auto &i : image_local_omp) {\n        bins_local_omp[i]++;\n    }\n\n    // MPI_Gather(&bins_local_omp, 256, MPI_INT, bins_local.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // for (size_t i=0; i < 256; i++) {\n    //     bins_local[i] += bins_local_omp[i];\n    // }\n\n    MPI_Reduce(bins_local_omp.data(), bins_local.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   }\n\n   if (rank == 0) {\n    for (size_t i=0; i < 256; i++) {\n        bins[i] += bins_local[i];\n    }\n   }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n}",
            "// Your code here\n }",
            "#pragma omp parallel for schedule(static)\n  for (size_t i=0; i<image.size(); i++) {\n    bins[image[i]] += 1;\n  }\n}",
            "// TODO: write this function\n\n   // Get number of processes and rank\n   int rank, num_procs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   // TODO: Compute the number of pixels in each grayscale intensity in the image.\n   // Hint: the function to call is `omp_get_num_threads`\n\n}",
            "size_t const num_pixels = image.size();\n   int rank, world_size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n   size_t const num_pixels_per_rank = num_pixels / world_size;\n   size_t const num_pixels_leftover = num_pixels % world_size;\n   size_t const start = rank * num_pixels_per_rank;\n\n   // Each rank counts its assigned portion of the image\n   std::array<size_t, 256> local_bins{};\n   size_t const stop = (rank == world_size - 1? start + num_pixels_per_rank + num_pixels_leftover : start + num_pixels_per_rank);\n   for (size_t i = start; i < stop; ++i) {\n     ++local_bins[image[i]];\n   }\n\n   // Each rank broadcasts its counts to all ranks\n   std::vector<size_t> local_bins_vec(local_bins.begin(), local_bins.end());\n   std::vector<size_t> counts_per_rank(world_size);\n   MPI_Gather(&local_bins_vec, 256, MPI_UNSIGNED_LONG, &counts_per_rank, 256, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // Each rank adds its counts to the running total\n   if (rank == 0) {\n     for (size_t i = 0; i < world_size; ++i) {\n       size_t rank_offset = i * 256;\n       for (size_t j = 0; j < 256; ++j) {\n         bins[j] += counts_per_rank[rank_offset + j];\n       }\n     }\n   }\n }",
            "size_t image_size = image.size();\n    int rank, num_ranks;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    int block_size = image_size / num_ranks;\n    int last_block_size = image_size % num_ranks;\n\n    // compute start and end for each rank\n    int start = block_size * rank + std::min(rank, last_block_size);\n    int end = (block_size + 1) * rank + std::min(rank + 1, last_block_size);\n\n    std::vector<int> local_bins(256, 0);\n\n    // Your code here\n    #pragma omp parallel for reduction(+:local_bins[0:256])\n    for(int i=start; i<end; ++i) {\n        local_bins[image[i]]++;\n    }\n\n    // add up all local counts\n    std::array<size_t, 256> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 256, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // aggregate global counts\n    if (rank == 0) {\n        bins = global_bins;\n    }\n}",
            "//TODO:\n}",
            "// Write your code here...\n}",
            "int rank = 0, numRanks = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n   \n   // TODO: implement this function.\n }",
            "// TODO: YOUR CODE HERE\n }",
            "size_t size = image.size();\n   std::vector<int> image_local(size);\n   std::copy(image.begin(), image.end(), image_local.begin());\n\n   int rank;\n   int num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   \n   int range = size/num_ranks;\n   int first = rank * range;\n   int last = first + range;\n   \n   if (rank == num_ranks - 1) {\n     last = size;\n   }\n\n//   std::cout << \"Rank \" << rank << \": [\" << first << \", \" << last << \")\" << std::endl;\n   int local_count = 0;\n   #pragma omp parallel for schedule(static) reduction(+:local_count)\n   for (int i = first; i < last; ++i) {\n     local_count++;\n   }\n   \n   std::vector<int> local_bins(256);\n   std::fill(local_bins.begin(), local_bins.end(), 0);\n\n   MPI_Reduce(&local_count, &(local_bins[0]), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n     for (int i = 0; i < 256; ++i) {\n       bins[i] += local_bins[i];\n     }\n   }\n}",
            "/* TODO: Implement the function */\n  #pragma omp parallel for\n  for (int i = 0; i < 256; ++i) {\n    bins[i] = 0;\n  }\n  #pragma omp parallel for\n  for (size_t i = 0; i < image.size(); ++i) {\n    ++bins[image[i]];\n  }\n}",
            "// TODO: write code here\n}",
            "// TODO(student): Implement this function.\n\n   return;\n}",
            "// Your code here\n}",
            "// TODO: Your code goes here\n}",
            "//...\n}",
            "int world_size, world_rank, num_pixels, remainder, num_threads, min_pixels, min_rank;\n  std::vector<int> sub_image, temp;\n  std::array<size_t, 256> temp_bins;\n  // TODO: write function to compute the number of threads\n\n  // TODO: compute number of pixels\n  // TODO: compute remainder\n\n  // TODO: compute the number of threads per rank\n  // TODO: compute the number of pixels per rank\n  // TODO: compute the number of pixels for this rank\n\n  // TODO: use std::vector::size() and std::vector::begin() to get the number of pixels in image\n  // TODO: use std::vector::resize() and std::vector::insert() to split the image into a sub-image\n\n  // TODO: compute the min and max pixel values\n\n  // TODO: loop over the pixels in the sub-image, updating `temp_bins`\n\n  // TODO: use MPI and OpenMP to combine the bins on all ranks\n  // TODO: note: this needs to happen on rank 0, otherwise it will crash!\n\n  // TODO: if `world_rank` is 0, print the contents of `bins`\n  // TODO: if `world_rank` is 0, print the contents of `temp_bins`\n\n}",
            "int const numBins = 256;\n   int const numRows = image.size();\n   int const rank = 0;\n   int const numRanks = 1;\n   // compute local histogram\n   std::array<size_t, numBins> localBins{};\n   #pragma omp parallel for schedule(static)\n   for (int i = 0; i < numRows; i++) {\n     localBins[image[i]] += 1;\n   }\n   // sum across ranks\n   int recvCounts[numRanks];\n   int sendCounts[numRanks];\n   int displs[numRanks];\n   sendCounts[rank] = numBins;\n   MPI_Allgather(&sendCounts[rank], 1, MPI_INT, recvCounts, 1, MPI_INT, MPI_COMM_WORLD);\n   displs[0] = 0;\n   for (int i = 1; i < numRanks; i++) {\n     displs[i] = displs[i - 1] + recvCounts[i - 1];\n   }\n   std::vector<size_t> globalBins(recvCounts[rank], 0);\n   MPI_Allgatherv(localBins.data(), numBins, MPI_INT, globalBins.data(), recvCounts, displs, MPI_INT, MPI_COMM_WORLD);\n   // combine local histogram and global histogram\n   std::copy(globalBins.begin(), globalBins.end(), bins.begin());\n }",
            "int rank, numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    // TODO: calculate the appropriate amount of work (total pixels)\n    //  and assign to each rank\n    int pixels = image.size();\n    int n = pixels/numRanks;\n    int rem = pixels%numRanks;\n    // printf(\"rank = %d\\n\",rank);\n    if(rank == numRanks-1){\n        pixels = n+rem;\n    }\n    else{\n        pixels = n;\n    }\n    // printf(\"rank = %d\\n\",rank);\n\n    // TODO: figure out how many pixels each rank is responsible for\n    //  and count those pixels\n\n    // std::array<size_t,256> bins(256, 0);\n    // bins.fill(0);\n\n    int localBins[256] = {0};\n    int local_count = 0;\n    for(int i=0; i < pixels; i++){\n        localBins[image[i]]++;\n        local_count++;\n    }\n\n    // bins.at(image[i])++;\n    // local_count++;\n\n    MPI_Allreduce(localBins, bins.data(), 256, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    // printf(\"rank = %d\\n\",rank);\n    // MPI_Reduce(localBins, bins.data(), 256, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // printf(\"rank = %d\\n\",rank);\n\n    // if(rank == 0){\n    //     int rem_count = 0;\n    //     for(int i=0; i<256; i++){\n    //         printf(\"%d \", bins[i]);\n    //         rem_count += bins[i];\n    //     }\n    //     printf(\"\\n\");\n    //     if(local_count!= rem_count){\n    //         printf(\"Something went wrong, total number of pixels!= sum of bins[i]\\n\");\n    //     }\n    // }\n\n    // TODO: make each rank do this calculation on its own copy of image,\n    //  store the results in its own copy of bins\n    // bins.fill(0);\n    // for(int i=0; i < pixels; i++){\n    //     bins.at(image[i])++;\n    // }\n\n    // TODO: sum up the results from all ranks into one global bins array\n\n    // TODO: print the result\n    // printf(\"rank = %d\\n\",rank);\n    // if(rank == 0){\n    //     for(int i=0; i<256; i++){\n    //         printf(\"%d \", bins[i]);\n    //     }\n    //     printf(\"\\n\");\n    // }\n\n    return;\n}",
            "size_t n = image.size();\n#pragma omp parallel for schedule(static)\n\tfor (size_t i = 0; i < n; i++) {\n\t\tbins[image[i]]++;\n\t}\n}",
            "// YOUR CODE HERE\n  bins = std::array<size_t, 256>();\n  int n = image.size();\n  // Counting the number of pixels in image with each grayscale intensity.\n  for (int i = 0; i < n; i++) {\n    bins[image[i]]++;\n  }\n}",
            "// your code here\n\n   // You need to use OpenMP\n   // Hint: use the following as an example\n   #pragma omp parallel for\n   for (int i=0; i<image.size(); i++) {\n       #pragma omp atomic\n       bins[image[i]]++;\n   }\n}",
            "// TODO: compute result and fill bins array\n}",
            "// your code here\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int world_size = size;\n   int world_rank = rank;\n   int max_threads = omp_get_max_threads();\n\n   // TODO: YOUR CODE HERE\n\n   // TODO: YOUR CODE HERE\n }",
            "// YOUR CODE HERE\n   // Hint:\n   // * MPI_Comm_size and MPI_Comm_rank\n   // * omp_get_num_threads\n   // * omp_get_thread_num\n   // * std::vector<int>::const_iterator\n   // * std::array<size_t, 256>::iterator\n   // * std::vector<int>::iterator\n   // * std::pair\n   // * std::for_each\n   // * std::vector<int>::begin\n   // * std::vector<int>::end\n   // * std::array<size_t, 256>::begin\n   // * std::array<size_t, 256>::end\n   // * std::array<size_t, 256>::operator[]\n }",
            "// TODO: implement this function\n    size_t n = image.size();\n    std::vector<int> bins_local(256);\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    double start, end;\n    start = omp_get_wtime();\n\n    int i = 0;\n    while(i < n){\n        int j = 0;\n        while(i < n && j < 256) bins_local[j++] = image[i++];\n        MPI_Gather(bins_local.data(), 256, MPI_INT, bins.data(), 256, MPI_INT, 0, MPI_COMM_WORLD);\n        if(rank == 0){\n            for(auto &val : bins_local){\n                val = 0;\n            }\n        }\n    }\n\n    if(rank == 0){\n        for(auto &val : bins){\n            val = 0;\n        }\n        for(auto &val : bins_local){\n            bins[val] += 1;\n        }\n    }\n    end = omp_get_wtime();\n    std::cout << \"Elapsed time: \" << end-start << std::endl;\n}",
            "// Fill in code here...\n }",
            "int rank, nproc;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    if(rank == 0){\n        bins.fill(0);\n    }\n    const int size = image.size();\n    const int chunk = (size / nproc);\n    std::vector<int> chunk_image(image.begin() + rank * chunk, image.begin() + rank * chunk + chunk);\n    std::array<size_t, 256> chunk_bins;\n    std::fill(chunk_bins.begin(), chunk_bins.end(), 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < chunk; i++) {\n        chunk_bins[image[i]] += 1;\n    }\n\n    MPI_Reduce(chunk_bins.data(), bins.data(), 256, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(Kokkos::View<const char**> const& s, Kokkos::View<size_t[26]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "size_t num_strings = s.extent(0);\n   size_t num_bins = bins.extent(0);\n   Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, num_strings),\n      KOKKOS_LAMBDA(size_t i) {\n         char c = s(i)[0];\n         if (c < 'a')\n            c = 'a';\n         else if (c > 'z')\n            c = 'z';\n         Kokkos::atomic_fetch_add(&(bins(c - 'a')), 1);\n      });\n}",
            "// TODO: fill this in\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(size_t i) {\n        int start = s(i)[0];\n        if (start >= 'a' && start <= 'z') {\n            ++bins(start - 'a');\n        }\n    });\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA (const size_t i) {\n      for (char c = 'a'; c < 'z'; ++c) {\n         if (s(i)[0] == c) {\n            ++bins(c - 'a');\n         }\n      }\n   });\n}",
            "// Your code goes here...\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), KOKKOS_LAMBDA (int i) {\n      auto const& ss = s(i);\n      for (auto c : ss) {\n        // convert to lower case, which is ASCII-safe\n        bins(c - 'a')++;\n      }\n    });\n}",
            "Kokkos::TeamPolicy<Kokkos::TeamMember<size_t[26]>> team_policy(s.extent(0), Kokkos::AUTO);\n  Kokkos::parallel_for(team_policy, KOKKOS_LAMBDA(Kokkos::TeamMember<size_t[26]> member) {\n    for(size_t i = member.league_rank(); i < s.extent(0); i += member.league_size()) {\n      // TODO: Replace with better code.\n      char c = s(i)[0];\n      if(c >= 'a' && c <= 'z') {\n        member.team_barrier();\n        member.team_shfl(c - 'a', 0, 26)++;\n      }\n    }\n  }, Kokkos::make_tuple(bins));\n}",
            "// Fill bins with zeroes\n  Kokkos::deep_copy(bins, 0);\n\n  // Compute the number of strings starting with each letter, in parallel\n  // TODO\n\n  // Wait for the parallel part to complete\n  // TODO\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(size_t i) {\n    // Initialize bins array to 0\n    for (int j=0; j<26; j++) bins(j) = 0;\n    // Iterate through each string\n    for (size_t j = 0; j < strlen(s(i)); j++) {\n      // Increment the bin corresponding to the letter at that position\n      bins(s(i)[j] - 'a')++;\n    }\n  });\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, s.extent(0));\n\n    Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int i) {\n        for (size_t j = 0; j < strlen(s(i)); j++) {\n            if (s(i)[j] >= 'a' && s(i)[j] <= 'z') {\n                Kokkos::atomic_fetch_add(&bins(s(i)[j] - 'a'), 1);\n            }\n        }\n    });\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(\n      \"firstLetterCounts\", s.extent(0),\n      KOKKOS_LAMBDA(size_t i) {\n        for (char c = 'a'; c <= 'z'; c++) {\n          if (s(i)[0] == c) {\n            bins(c - 'a') += 1;\n            break;\n          }\n        }\n      });\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.size(), KOKKOS_LAMBDA(int i) {\n    char c = s(i)[0];\n    if (c >= 'a' && c <= 'z') {\n      bins(c - 'a')++;\n    }\n  });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, 26),\n      KOKKOS_LAMBDA (size_t i) {\n         size_t counter = 0;\n         for (size_t j = 0; j < s.extent(0); ++j) {\n            if (s(j)[0] == 'a' + i) {\n               counter++;\n            }\n         }\n         bins(i, 0) = i;\n         bins(i, 1) = counter;\n      });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n      KOKKOS_LAMBDA (int i) {\n        size_t j = s(i);\n        while (j!= 0) {\n          bins(j - 'a')++;\n          j = j >> 8;\n        }\n      });\n}",
            "// TODO\n}",
            "auto device_bins = Kokkos::create_mirror_view(bins);\n    Kokkos::deep_copy(device_bins, bins);\n\n    Kokkos::parallel_for(s.extent(0), [=](int i) {\n        const char* string = s(i);\n        if (string[0] >= 'a' && string[0] <= 'z') {\n            device_bins(string[0] - 'a')++;\n        }\n    });\n\n    Kokkos::deep_copy(bins, device_bins);\n}",
            "// TODO\n}",
            "auto start = Kokkos::Experimental::require(s, Kokkos::Experimental::WorkItemProperty::HintLightWeight);\n    auto strs = Kokkos::Experimental::subview(start, Kokkos::ALL(), 0);\n    auto strSizes = Kokkos::Experimental::subview(start, Kokkos::ALL(), 1);\n    auto counts = Kokkos::Experimental::subview(bins, Kokkos::ALL());\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Experimental::OpenMP>(0, s.extent(0)), KOKKOS_LAMBDA(size_t i) {\n        counts[strs[i] - 'a'] += strSizes[i];\n    });\n}",
            "// TODO\n}",
            "// Your code goes here.\n    // You are free to use functions from Kokkos, but you may NOT use Kokkos::parallel_for or other\n    // Kokkos parallel primitives.\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n    using MemorySpace = Kokkos::DefaultHostSpace;\n\n    // YOUR CODE HERE\n    // Kokkos does not provide a parallel_for method, but we can simulate this functionality using a parallel_reduce\n    // This will require a functor, which contains the loop body and is executed by the parallel_reduce\n    // The parallel_reduce method requires a value, and a reduction, which tells the parallel_reduce how to combine values\n    // In this case, we will use a Kokkos::Sum<size_t> to add all of the values in the range into a single value.\n    // The reduction is a template argument, which means we can use a different type of reduction in each invocation.\n\n    // First, let's figure out which threads will be assigned the ranges.\n    // We can get the thread id in the block and block id in the grid with Kokkos::ThreadVectorRange\n    // Let's make a range for each letter of the alphabet.\n    // The thread id of the range is the letter, and the block id of the range is the index of the block in the grid.\n    // We can combine these into a single index with the block_id * blockDim.x + thread_id\n\n    // Next, we need to compute a range of indices that we will iterate over for each block.\n    // We can compute the size of the input array with the length() method on the Kokkos::View object.\n    // We can compute the number of threads in the block with Kokkos::TeamPolicy::team_size().\n    // We can compute the number of blocks in the grid with Kokkos::TeamPolicy::team_size().\n    // These should be the same, since we only have one block.\n    // Now we can get the index range for each block.\n\n    // Next, we need to create a functor.\n    // For this, we can create a class and inherit from Kokkos::TeamPolicy.\n    // To create the functor, we just need to overload the operator() method of the functor.\n    // In this case, we will take in a block range and a subview of the input array and a subview of the output array.\n    // This should be pretty straightforward to implement.\n    // We will need to access the subviews using the block_id and thread_id of the block, and then perform the computation.\n    // There are several ways to do this, but it should be similar to the serial case.\n\n    // Finally, we need to execute the parallel_reduce using the new functor and the input array.\n    // Remember that parallel_reduce will need a value (of type S) and a reduction (of type R), so we will need to create these.\n\n    // HINT:\n    // Try not to compute the output values in the functor.\n    // We can use the subview of the output array to store the final result in the functor.\n    // We can then simply return the value after the parallel_reduce is done executing.\n\n    // HINT:\n    // Try to avoid using blockDim.x and blockDim.y.\n    // Try to use gridDim.x, which is the number of blocks in the grid, instead.\n    // The blockDim.x and blockDim.y are the number of threads in the block, and the gridDim.x is the number of blocks in the grid.\n\n    // HINT:\n    // Try to write your code in a way that it can be executed on both CPU and GPU.\n    // To do this, you will need to use Kokkos::View in the functor, which allows you to access data on both CPU and GPU.\n    // You will also need to be mindful of the type of the data you are trying to access, since you cannot access\n    // the host space from the device space.\n\n}",
            "// YOUR CODE HERE\n}",
            "Kokkos::parallel_for(1, KOKKOS_LAMBDA (size_t, int) {\n        for (size_t i = 0; i < 26; ++i)\n            bins(i) = 0;\n        for (size_t i = 0; i < s.extent(0); ++i)\n            ++bins(s(i)[0] - 'a');\n    });\n}",
            "// TODO: implement this method\n}",
            "size_t n = s.extent(0);\n\tKokkos::parallel_for(n, KOKKOS_LAMBDA(const int i) {\n\t\tchar c = s(i)[0];\n\t\tif (c >= 'a' && c <= 'z') {\n\t\t\tsize_t id = c - 'a';\n\t\t\tbins(id) += 1;\n\t\t}\n\t});\n}",
            "// Your code here\n}",
            "// TODO\n\n  // Hint: you will probably want to loop over the strings s and increment a counter for each letter.\n  // You can use the first character of each string to determine the appropriate counter.\n  // You'll need to use parallel_for_each (see below).\n\n}",
            "/* YOUR CODE HERE */\n}",
            "/* TODO */\n}",
            "/* You code here */\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < 26; j++) {\n      if (s(i)[0] == (char)j + 'a')\n        bins(j) += 1;\n    }\n  });\n  Kokkos::fence();\n}",
            "}",
            "auto policy = Kokkos::TeamPolicy<Kokkos::HostSpace>(s.extent(0), 1, 1);\n  policy.execute(KOKKOS_LAMBDA(const typename Kokkos::TeamPolicy<Kokkos::HostSpace>::member_type &team) {\n    //TODO\n  });\n}",
            "}",
            "// TODO\n\n    // TODO: fill in this function.\n\n}",
            "#if defined(KOKKOS_ENABLE_CUDA)\n  printf(\"Device %s: \", Kokkos::Cuda::select_device().name().c_str());\n#elif defined(KOKKOS_ENABLE_HIP)\n  printf(\"Device %s: \", Kokkos::Experimental::HIP::hip_impl::device_name().c_str());\n#endif\n  Kokkos::Timer timer;\n  timer.reset();\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n                       [=](int i) {\n                         for (size_t j = 0; j < 26; ++j) {\n                           if (s(i)[0] == 'a' + j) {\n                             ++bins(j);\n                           }\n                         }\n                       });\n  printf(\"Time: %9.6f s\\n\", timer.seconds());\n}",
            "// TODO: Use Kokkos to compute the counts in parallel\n\n  // 1. Initialize bins to zero.\n  // TODO: Fill bins with zeros. You can use Kokkos to execute a parallel_for in parallel\n  // Use Kokkos::View(array, Kokkos::ALL) to create a view of a preallocated array\n\n  // 2. Loop over strings in s.\n  // For each string s_i:\n  // 2a. Increment `bins[s_i[0] - 'a']`\n  // Use Kokkos::atomic_fetch_add to increment the value in bins\n\n  // 3. Print the output\n  // TODO: Print the contents of bins to stdout\n\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "size_t num_strings = s.extent(0);\n    size_t num_bins = 26;\n\n    // TODO\n    // Initialize `bins` to zeros (by setting `bins(0, 0) = 0`)\n    // Use `Kokkos::parallel_for` to compute the result\n}",
            "// TODO: compute firstLetterCounts\n}",
            "// TODO: Implement the firstLetterCounts function.\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA(int i){\n    for (int j=0; j<26; j++) {\n      if (s(i)[0]==('a'+j)) {\n        bins(j) += 1;\n      }\n    }\n  });\n  Kokkos::fence();\n}",
            "using Kokkos::RangePolicy;\n  Kokkos::parallel_for(\"first-letter-counts\", RangePolicy(0, s.extent(0)), [=](size_t i) {\n    const char* str = s(i);\n    const char c = str[0];\n    if (c >= 'a' && c <= 'z')\n      bins(c - 'a')++;\n  });\n}",
            "// TODO: fill out this function\n}",
            "size_t numRows = s.extent(0);\n\n  // TODO: allocate the output array for `bins` and initialize it to zero\n  Kokkos::View<size_t[26]> outputBins(\"outputBins\", 26);\n  Kokkos::deep_copy(outputBins, 0);\n\n  // TODO: parallelize the following loop with Kokkos\n  for (int i = 0; i < numRows; i++) {\n    int ch = tolower(s(i)[0]);\n    if (ch < 0 || ch > 25)\n      continue;\n    outputBins(ch)++;\n  }\n\n  // TODO: copy the contents of `outputBins` into `bins`\n  Kokkos::deep_copy(bins, outputBins);\n}",
            "// TODO:\n  // TODO:\n}",
            "// Your code goes here\n}",
            "// TODO\n}",
            "// TODO: Implement firstLetterCounts\n\n}",
            "// This is the size of the alphabet.  The size is hardcoded, but could be determined by\n  // the user.\n  const int ALPHABET_SIZE = 26;\n\n  // We use a Kokkos parallel_for to iterate over the strings in `s`\n  Kokkos::parallel_for(\"first letter counts\", s.size(), KOKKOS_LAMBDA(size_t i) {\n    // For each string, we'll iterate over its letters one by one.  We'll look at the\n    // first letter and add to the corresponding value in `bins`.  This is the\n    // equivalent of adding 1 to the array bins[s[i][0] - 'a'].\n    for (size_t j = 0; j < strlen(s(i)); j++) {\n      bins(s(i)[j] - 'a')++;\n    }\n  });\n}",
            "auto const& n = s.extent(0);\n\n    Kokkos::parallel_for(\"firstLetterCounts\", n, KOKKOS_LAMBDA(const int i) {\n\n        for (size_t j = 0; j < 26; ++j) {\n            if (s(i)[0] == 'a' + j)\n                ++bins(j);\n        }\n    });\n}",
            "// TODO: Fill in this function.\n}",
            "// TODO: Fill in the body of this function to compute the histogram.\n  //\n  // For the sake of this problem, you might want to consider a few ways to do this:\n  //   - Use a parallel reduction to compute the histogram in parallel.\n  //   - Use a parallel_for loop to compute the histogram in parallel.\n  //   - Use a parallel_reduce to compute the histogram in parallel.\n  //   - Use a parallel_scan to compute the histogram in parallel.\n  //\n  // If you have an interesting way to parallelize this computation, you're encouraged to share it with\n  // the community via a pull request on Github.\n}",
            "// TODO: your code here\n}",
            "auto s_at_i = [&s](const size_t i) { return s(i); };\n\n  auto do_work = KOKKOS_LAMBDA(const int i) {\n    const char* string = s_at_i(i);\n    int char_i = string[0] - 'a';\n    if (char_i >= 0 && char_i <= 25)\n      ++bins(char_i);\n  };\n\n  Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), do_work);\n  Kokkos::fence();\n}",
            "// TODO: Fill this in\n}",
            "// Implement this function\n}",
            "/* TODO: Your code goes here. */\n}",
            "// TODO: implement this function\n}",
            "// TODO: YOUR CODE HERE\n\n    // For example, if s[0] = \"dog\", then bins[0] should be 1, and so on.\n    // This should be a parallel loop\n\n    // Kokkos provides a parallel for loop. It is much easier to use than a parallel_for.\n    // You can use the parallel for loop on a view. For example, if you have a view called \"view\", then\n    // the parallel for loop is called like this: Kokkos::parallel_for(\"parallel loop name\", view, functor)\n    // You can see more about the parallel for loop here: https://github.com/kokkos/kokkos/wiki/User-Manual#parallel-for-loops-kokkos-32\n\n    // Remember to add the KOKKOS_ENABLE_OPENMP option to the cmake file, and the\n    // \"KOKKOS_ENABLE_PRAGMA_IVDEP\" and \"KOKKOS_ENABLE_PRAGMA_VECTOR\" options to the Makefile.\n}",
            "Kokkos::parallel_for(\"count first letters\", s.extent(0), KOKKOS_LAMBDA(size_t i) {\n    char c = tolower(s(i)[0]);\n    if (isalpha(c)) {\n      bins(c - 'a')++;\n    }\n  });\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 26),\n    KOKKOS_LAMBDA(const int i) {\n      bins(i) = 0;\n    });\n  Kokkos::fence();\n\n  Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      const char *str = s(i);\n      if (str[0] >= 'a' && str[0] <= 'z') {\n        bins(str[0] - 'a') += 1;\n      }\n    });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(1, KOKKOS_LAMBDA (int) {\n        for (char i = 'a'; i <= 'z'; ++i) {\n            size_t count = 0;\n            for (size_t j = 0; j < s.extent(0); ++j) {\n                if (s(j)[0] == i) {\n                    ++count;\n                }\n            }\n            bins(i - 'a') = count;\n        }\n    });\n}",
            "// TODO: fill in this function\n}",
            "auto s_host = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace{}, s);\n    auto bins_host = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace{}, bins);\n    for (size_t i = 0; i < s_host.extent(0); ++i) {\n        char c = tolower(s_host(i, 0));\n        if (c >= 'a' && c <= 'z') {\n            bins_host(c - 'a') += 1;\n        }\n    }\n    Kokkos::deep_copy(bins, bins_host);\n}",
            "}",
            "/* TODO: implement me */\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", Kokkos::RangePolicy<>(0, s.extent(0)), KOKKOS_LAMBDA(size_t i) {\n      const char* str = s(i);\n      for (size_t j = 0; j < 26; ++j) {\n        if (str[0] == 'a' + j) {\n          bins(j) += 1;\n        }\n      }\n    });\n}",
            "// TODO: Add implementation\n}",
            "// TODO: Implement this function.\n\n  // Hint: Use Kokkos::parallel_for to run a parallel for-loop over the\n  // length of the array s. Use Kokkos::atomic to perform atomic updates\n  // to the bins array.\n\n  // If you use the Kokkos::parallel_for API, here is a short\n  // reference guide:\n  // https://github.com/kokkos/kokkos/wiki/Programming-with-Kokkos#kokkosparallel_for\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::TeamPolicy<>::team_member_invoke(\n    Kokkos::TeamThreadRange(Kokkos::TeamThreadRange(0, 26)),\n    [&] (const size_t& i) {\n      size_t n = 0;\n      for (size_t j = 0; j < s.extent(0); j++) {\n        if (s(j)[0] == i + 'a') {\n          n++;\n        }\n      }\n      bins(i) = n;\n    });\n}",
            "// TODO: Implement this function!\n}",
            "// YOUR CODE HERE\n}",
            "/* TODO: your code goes here */\n  const size_t n = s.extent(0);\n  for (int i = 0; i < 26; i++) {\n    bins(i) = 0;\n  }\n  for (size_t i = 0; i < n; i++) {\n    auto s_i = s(i);\n    char first_char = tolower(s_i[0]);\n    if (first_char >= 'a' && first_char <= 'z') {\n      bins(first_char - 'a')++;\n    }\n  }\n}",
            "// TODO:\n    //...\n}",
            "const size_t num_strings = s.extent(0);\n\n  // TODO: Implement this function.\n\n}",
            "const size_t num_strings = s.extent(0);\n\n    // TODO\n}",
            "// TODO: YOUR CODE HERE\n    // 1. Allocate bins in parallel\n    // 2. Count in parallel\n}",
            "Kokkos::parallel_for(1, KOKKOS_LAMBDA(int) {\n      // YOUR CODE HERE\n  });\n\n}",
            "Kokkos::parallel_for(s.extent(0), KOKKOS_LAMBDA (const int& i) {\n        if (s(i)[0] >= 'a' && s(i)[0] <= 'z') {\n            bins[s(i)[0] - 'a']++;\n        }\n    });\n    Kokkos::fence();\n}",
            "// TODO: Your code here\n}",
            "// TODO: fill in this function\n\n}",
            "// TODO\n}",
            "// TODO: YOUR CODE HERE\n  Kokkos::RangePolicy<Kokkos::HostSpace::execution_space> policy(0, s.size());\n\n  auto input = Kokkos::create_mirror_view(s);\n  Kokkos::deep_copy(input, s);\n  size_t* output = new size_t[26];\n  for (int i = 0; i < 26; i++) output[i] = 0;\n  Kokkos::deep_copy(bins, output);\n\n  for (auto i : policy) {\n    for (int j = 0; j < s.extent(1); j++) {\n      if (input(i, j) < 'a' || input(i, j) > 'z') {\n        printf(\"Invalid input.\\n\");\n        break;\n      }\n      output[input(i, j) - 'a'] += 1;\n    }\n  }\n  Kokkos::deep_copy(bins, output);\n  delete[] output;\n}",
            "}",
            "// TODO: fill in\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this\n  // You might want to use the following function:\n  //    Kokkos::parallel_for(Kokkos::RangePolicy<DeviceType>(0, N), functor);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), [&] (size_t i) {\n        auto word = s(i);\n        for (char c = 'a'; c <= 'z'; c++) {\n            if (word[0] == c) {\n                bins(c - 'a')++;\n            }\n        }\n    });\n}",
            "// TODO\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, s.extent(0));\n\n  // Count the number of strings starting with each letter.\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int& i) {\n    const auto str = s(i);\n    for (auto c = *str; c!= '\\0'; c++) {\n      const auto index = c - 'a';\n      const auto count = bins(index);\n      bins(index) = count + 1;\n    }\n  });\n}",
            "// TODO: Your code goes here\n    throw std::runtime_error(\"TODO: Implement this function\");\n}",
            "}",
            "// YOUR CODE HERE\n    Kokkos::parallel_for(\"First Letter Counts\", 26, KOKKOS_LAMBDA(const int i) {\n        for (size_t j = 0; j < s.size(); j++) {\n            if (s(j)[0] - 'a' == i) {\n                bins(i) += 1;\n            }\n        }\n    });\n}",
            "// your code here\n}",
            "// TODO: implement me\n}",
            "// TODO: Implement your code here\n    // TODO: Replace KOKKOS_LAMBDA with KOKKOS_FUNCTION\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), KOKKOS_LAMBDA (const int i) {\n        const char* str = s(i);\n        if(str[0]!= '\\0') {\n            bins(str[0] - 'a')++;\n        }\n    });\n}",
            "// Insert your code here.\n}",
            "// TODO: complete this function to count the number of strings in `s` that start with each letter\n    size_t num_str = s.extent(0);\n    Kokkos::parallel_for(\"firstLetterCounts\", num_str, KOKKOS_LAMBDA(int i){\n        char c = s(i, 0);\n        if(c >= 'a' && c <= 'z') {\n            ++bins(c - 'a');\n        }\n    });\n    // TODO: you will need to synchronize this kernel, and then you can verify that `bins` has the expected results\n}",
            "auto s_beg = s.data();\n  auto s_end = s.data() + s.extent(0);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, s.extent(0)), KOKKOS_LAMBDA (size_t i) {\n    const char* c = s_beg[i];\n\n    while (*c) {\n      bins[*c - 'a']++;\n      c++;\n    }\n  });\n}",
            "// TODO: replace this dummy code with your own code to compute the output\n  // using Kokkos views.\n\n  // Create a lambda to get a single character from a string\n  auto get_char = KOKKOS_LAMBDA (const size_t& idx) {\n    return s(idx)[0];\n  };\n\n  // Create a lambda to increment the count for a given character\n  auto update_count = KOKKOS_LAMBDA (const size_t& idx) {\n    bins(get_char(idx))++;\n  };\n\n  // Call Kokkos for_each() on each element in the array\n  Kokkos::parallel_for(s.extent(0), update_count);\n}",
            "// TODO: Implement Kokkos parallel_for here.\n}",
            "Kokkos::parallel_for(26, KOKKOS_LAMBDA(const int i) {\n    bins(i) = 0;\n  });\n  Kokkos::fence();\n\n  const size_t n = s.extent(0);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int i) {\n    const char *str = s(i);\n    for (int j = 0; j < 26; ++j) {\n      if (str[0] == 'a'+j) {\n        ++bins(j);\n      }\n    }\n  });\n}",
            "// TODO: Implement me\n}",
            "using Kokkos::RangePolicy;\n  using Kokkos::MDRangePolicy;\n  using Kokkos::TeamPolicy;\n  using Kokkos::TeamThreadRange;\n\n  // Use a TeamPolicy to create a team of threads for each string.\n  // TeamThreadRange iterates over the strings within a team.\n  // TeamThreadRange returns an index into the string array and a reference to the string.\n  // TeamThreadRange iterates over the strings within a team.\n  // Use the string reference to do the counting for that string.\n  // The output can be written directly to the array since we're using the TeamThreadRange\n  // iterator.\n  // A TeamPolicy is used since Kokkos is using SIMD to parallelize.\n  // Each thread within a team gets a unique set of data to work on.\n  // In this case, each thread will get a unique string to process.\n  // If you wanted to use a loop, you would have to synchronize each time\n  // you read a string.\n  // https://github.com/kokkos/kokkos/wiki/FAQ#kokkos-has-simd-and-its-like-magic\n\n  // This is the size of the alphabet we're working with.\n  // 26 characters\n  constexpr size_t ALPHABET_SIZE = 26;\n\n  // Create a TeamPolicy with as many threads as there are elements in the array.\n  // Since each thread is responsible for a string, the number of threads is the\n  // number of elements in the array.\n  // https://github.com/kokkos/kokkos/wiki/TeamPolicy\n  // https://github.com/kokkos/kokkos/wiki/Execution-Model\n  TeamPolicy<>::member_type teamMember;\n  size_t threadCount = s.extent(0);\n  TeamPolicy<> policy(threadCount, Kokkos::AUTO);\n  // Loop over the strings in the array, counting the number of strings that start with a given letter.\n  // For each letter in the alphabet, loop through the strings.\n  // A string starts with the letter if the ASCII value of the first character in the string\n  // is equal to the integer representation of the letter.\n  // https://en.wikipedia.org/wiki/Letter_frequency#Relative_frequencies_of_letters_in_other_languages\n  // https://www.w3schools.com/charsets/ref_html_ascii.asp\n  for (size_t i = 0; i < ALPHABET_SIZE; ++i) {\n    size_t count = 0;\n    for (auto sIter = Kokkos::TeamThreadRange<TeamPolicy<>>(policy, s); sIter.team_rank() < sIter.team_size();\n         ++sIter) {\n      if (sIter.team_rank() < sIter.team_size()) {\n        const char* str = sIter.reference();\n        const char firstLetter = *str;\n        const size_t ascii = static_cast<size_t>(firstLetter);\n        if (ascii == i) {\n          // Increment the count.\n          count++;\n        }\n      }\n    }\n    // Write the results back to the array.\n    // Since each thread is working on a unique string, we can write the results\n    // directly to the array.\n    // https://github.com/kokkos/kokkos/wiki/View#assigning-values-to-a-view\n    // https://github.com/kokkos/kokkos/wiki/View-semantics\n    // https://github.com/kokkos/kokkos/wiki/Views#reduction-views\n    bins(i) = count;\n  }\n}",
            "// TODO: your code here\n}",
            "// TODO: Your code here\n}",
            "Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int&, size_t i) {\n        // Initialize bins to 0.\n        for (size_t j = 0; j < 26; j++) {\n            bins[j] = 0;\n        }\n\n        // Iterate over the strings and increment the correct bin.\n        for (size_t j = 0; j < s.size(); j++) {\n            bins[s[j][0] - 'a']++;\n        }\n    });\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n  using MemorySpace = ExecutionSpace::memory_space;\n  using RangePolicy = Kokkos::RangePolicy<ExecutionSpace>;\n  size_t count;\n  Kokkos::parallel_reduce(\"firstLetterCounts\", RangePolicy(0, s.extent(0)),\n    KOKKOS_LAMBDA(const size_t i, size_t &local_count) {\n      // Loop over the letters of the alphabet to count\n      for (size_t j = 0; j < 26; ++j) {\n        if (s(i)[0] == 'a' + j) local_count++;\n      }\n    }, count);\n  bins() = count;\n}",
            "auto i = Kokkos::TeamThreadRange(Kokkos::ThreadTeamMember(), 26);\n    auto j = Kokkos::ThreadVectorRange(Kokkos::ThreadTeamMember(), 26);\n    auto result = Kokkos::View<size_t>(\"result\", 26);\n    Kokkos::parallel_for(\"firstLetterCounts\", i, [=](auto& item) {\n        for (auto k = 0; k < s.extent(0); k++) {\n            for (auto l = 0; l < 26; l++) {\n                if (*(s(k) + 0) == l + 97) {\n                    result[l] += 1;\n                }\n            }\n        }\n    });\n    Kokkos::parallel_for(\"combineResults\", j, [&result, &bins](auto& item) {\n        bins(item) = result(item);\n    });\n}",
            "Kokkos::View<size_t*, Kokkos::LayoutLeft> counts(\"counts\");\n    counts = Kokkos::View<size_t*, Kokkos::LayoutLeft>(\"counts\", 26);\n    Kokkos::parallel_for(\"Initialize counts to zero\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 26), KOKKOS_LAMBDA(int i) {counts(i) = 0;});\n\n    Kokkos::parallel_for(\"Count letters\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), KOKKOS_LAMBDA(int i) {counts(s[i][0] - 'a')++;});\n\n    Kokkos::parallel_for(\"Copy counts to bins\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 26), KOKKOS_LAMBDA(int i) {bins(i) = counts(i);});\n}",
            "Kokkos::MDRangePolicy<Kokkos::Rank<2>,Kokkos::Iterate::Default,Kokkos::Schedule<Kokkos::Static> > mr_policy({0,0},{s.extent(0),26});\n    Kokkos::parallel_for(\"firstLetterCounts\", mr_policy, KOKKOS_LAMBDA(const int i, const int j) {\n        const size_t len = std::strlen(s(i));\n        if(len > 0 && s(i)[0] == (j + 'a')) {\n            bins(j)++;\n        }\n    });\n}",
            "// TODO: Your code here\n}",
            "// TODO\n}",
            "auto parallel_for = Kokkos::TeamPolicy<>::team_policy(s.size(), Kokkos::AUTO);\n\n    parallel_for.execute([&] (const Kokkos::TeamPolicy<>::member_type& team) {\n        size_t i = team.league_rank();\n\n        Kokkos::parallel_for(team, s[i], [&] (const char* s) {\n            if (isalpha(*s)) {\n                bins(tolower(*s) - 'a')++;\n            }\n        });\n    });\n}",
            "const size_t num_strings = s.extent(0);\n\tconst char* s_host = Kokkos::create_mirror_view(s);\n\tsize_t num_bins_host[26];\n\tsize_t num_bins_host_total = 0;\n\tfor (int i = 0; i < 26; i++) {\n\t\tnum_bins_host[i] = 0;\n\t\tfor (size_t j = 0; j < num_strings; j++) {\n\t\t\tif (s_host(j)[0] == 'a' + i) {\n\t\t\t\tnum_bins_host[i]++;\n\t\t\t}\n\t\t}\n\t\tnum_bins_host_total += num_bins_host[i];\n\t}\n\tKokkos::deep_copy(s, s_host);\n\tKokkos::View<size_t[26], Kokkos::HostSpace> num_bins(num_bins_host);\n\tKokkos::View<size_t, Kokkos::HostSpace> num_bins_total(\"num_bins_total\", 1);\n\tKokkos::deep_copy(num_bins_total, num_bins_host_total);\n\tKokkos::View<size_t*, Kokkos::HostSpace> num_bins_out = Kokkos::create_mirror_view(bins);\n\tKokkos::parallel_reduce(num_bins.extent(0), KOKKOS_LAMBDA(const int i, size_t &sum) {\n\t\tsum += num_bins(i);\n\t}, num_bins_host_total);\n\tKokkos::deep_copy(num_bins_out, num_bins);\n}",
            "// TODO\n}",
            "// Insert your solution here\n}",
            "// YOUR CODE HERE\n}",
            "Kokkos::View<size_t*> counts(\"counts\", 26);\n  Kokkos::deep_copy(counts, 0);\n\n  auto num_strings = s.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace, Kokkos::Schedule<Kokkos::Dynamic>>>(0, num_strings), \n    [=](Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace, Kokkos::Schedule<Kokkos::Dynamic>> &team_policy, Kokkos::View<size_t*> const& counts) {\n      auto team = team_policy.team_size_recommended(Kokkos::ParallelForTag());\n      auto vector_length = 1 + (s.extent(1) - 1) / team;\n\n      Kokkos::parallel_for(Kokkos::TeamThreadRange(team_policy, 0, team * vector_length), [&] (size_t idx) {\n        auto vector_idx = idx / team;\n        auto my_string = s(vector_idx, vector_idx * team + idx % team);\n        if (my_string) {\n          counts[my_string[0] - 'a'] += 1;\n        }\n      });\n    }, counts);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 26), [=] (size_t i) {\n    Kokkos::atomic_fetch_add(&(bins(i)), counts(i));\n  });\n}",
            "// Initialize the values of bins to 0\n  Kokkos::deep_copy(bins, 0);\n\n  // TODO: Complete the following code to count the number of strings starting with each letter.\n  // Hint: use Kokkos parallel_reduce\n\n}",
            "// Fill bins with zeroes. This is a no-op if bins is already filled with zeros.\n    Kokkos::deep_copy(bins, 0);\n\n    // TODO: replace this for-loop with parallel kernel\n    for (size_t i = 0; i < s.extent(0); i++) {\n        size_t letter = s(i)[0] - 'a';\n        bins(letter) += 1;\n    }\n}",
            "// TODO: implement me\n}",
            "// TODO: Your code goes here.\n}",
            "// TODO: implement me\n  // HINT: you may find the string_view library helpful here\n  // https://en.cppreference.com/w/cpp/header/experimental/string_view\n  // You may find this helpful:\n  // https://en.cppreference.com/w/cpp/string/basic_string_view/data\n}",
            "// TODO: Use the Kokkos Kernels parallel_for function to loop over the strings\n  // and count the number of occurrences of the letter at position 0.\n  // The output is written to `bins`.\n  // Do not count the null-terminated character at the end of each string.\n  // See https://github.com/kokkos/kokkos-tutorials/blob/master/ParallelProgramming/00_KokkosKernels/KokkosKernels_Tutorial.cpp\n  // for an example.\n}",
            "// create vector of indices\n  size_t n = s.extent(0);\n  Kokkos::View<size_t*, Kokkos::HostSpace> indices(\"indices\", n);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA (size_t i) { indices(i) = i; });\n\n  // create vector of letters\n  Kokkos::View<char*, Kokkos::HostSpace> letters(\"letters\", n);\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA (size_t i) { letters(i) = tolower(s(i)[0]); });\n\n  // create vector of unique letters\n  Kokkos::View<char*, Kokkos::HostSpace> unique_letters(\"unique_letters\", 26);\n  unique_letters(0) = 'a';\n  Kokkos::parallel_for(1, 26, KOKKOS_LAMBDA (size_t i) { unique_letters(i) = unique_letters(i-1) + 1; });\n\n  // find the indices where each unique letter appears in the input array\n  Kokkos::View<size_t*, Kokkos::HostSpace> unique_indices(\"unique_indices\", 26);\n  Kokkos::parallel_for(26, KOKKOS_LAMBDA (size_t i) { unique_indices(i) = Kokkos::Impl::find_first_index(unique_letters(i), letters, n); });\n\n  // store the number of strings for each unique letter in `bins`\n  Kokkos::parallel_for(26, KOKKOS_LAMBDA (size_t i) { bins(i) = Kokkos::Impl::find_last_index(unique_letters(i), letters, n); });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, s.extent(0)), KOKKOS_LAMBDA(const int i) {\n    auto str = s(i);\n    size_t count = 0;\n    for (size_t j = 0; str[j]; ++j) {\n      if (str[j] >= 'a' && str[j] <= 'z') {\n        ++count;\n      }\n    }\n    bins(str[0] - 'a') = count;\n  });\n}",
            "// Your code here\n   size_t n = s.extent(0);\n   Kokkos::parallel_for(\"first_letter_counts\", n, KOKKOS_LAMBDA(const int i) {\n      size_t index = 0;\n      for (int j = 0; j < 26; j++) {\n         if (s(i)[0] == 'a' + j) {\n            index = j;\n            break;\n         }\n      }\n      Kokkos::atomic_fetch_add(&bins[index], 1);\n   });\n}",
            "Kokkos::parallel_for(\"firstLetterCounts\", s.extent(0), [&] (size_t i) {\n    for (size_t j = 0; j < 26; j++) {\n      if (tolower(s(i)[0]) == 'a' + j) {\n        ++bins(j);\n      }\n    }\n  });\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.size());\n    Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int& i) {\n        auto str = s(i);\n        for (size_t j = 0; str[j]!= '\\0'; ++j) {\n            ++bins(str[j] - 'a');\n        }\n    });\n}",
            "// TODO: fill out this function\n}",
            "// TODO\n}",
            "// Write your Kokkos code here\n  Kokkos::parallel_for(\"FirstLetterCounts\", s.extent(0), KOKKOS_LAMBDA (const size_t i) {\n      const auto* word = s(i);\n      for (size_t j = 0; j < 26; j++) {\n          if (*word == 'a' + j) {\n              bins(j)++;\n          }\n      }\n  });\n}",
            "using execution_space = Kokkos::DefaultExecutionSpace;\n  using memory_space = execution_space::memory_space;\n\n  // TODO:\n  //\n  // Implement this function.\n  //\n}",
            "Kokkos::parallel_for(\"First Letter Counts\", 26, KOKKOS_LAMBDA(const size_t i) {\n    bins(i) = 0;\n    for(size_t j = 0; j < s.extent(0); j++) {\n      if(tolower(s(j)[0]) == i + 'a') {\n        bins(i) += 1;\n      }\n    }\n  });\n}",
            "// TODO\n  // You may find the following Kokkos subviews useful:\n  //   Kokkos::View<const char**> s_subview(s.data(), s.extent(0), s.extent(1));\n  //   Kokkos::View<size_t[26]> bins_subview(bins.data(), bins.extent(0));\n  auto i = Kokkos::ThreadVectorRange(Kokkos::ThreadVectorRange(0, s.extent(0)), s.extent(1));\n  auto j = Kokkos::ThreadVectorRange(Kokkos::ThreadVectorRange(0, 26));\n  Kokkos::parallel_for(\"first-letter-counts\", i, KOKKOS_LAMBDA(int i) {\n    Kokkos::parallel_for(j, KOKKOS_LAMBDA(int j) {\n      if(s_subview(i,0) == 'a' + j)\n        bins_subview(j)++;\n    });\n  });\n}",
            "const size_t n = s.extent(0);\n\n  Kokkos::parallel_for(\"First Letter Counts\", Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA(int i) {\n    char firstLetter = s(i)[0];\n    bins(firstLetter - 'a') += 1;\n  });\n}",
            "// TODO\n  // Hint:\n  // - Kokkos::parallel_for can be used to parallelize.\n  // - Kokkos::RangePolicy can be used to specify the range of the for loop.\n  // - Each parallel_for requires a Kokkos::TeamPolicy to indicate the mapping of threads to blocks.\n  // - Use the blockId() function to determine which block the current thread is in.\n  // - You can think of the blockId() function as a way to identify the block in the output vector.\n  // - Kokkos::TeamPolicy can be constructed using a Kokkos::RangePolicy and the blockId() function.\n  // - You can also use Kokkos::parallel_reduce to perform a reduction operation.\n  // - Kokkos::Sum<int> can be used as the reduction type.\n  // - Use the Kokkos::TeamPolicy() constructor to create a policy that uses a default mapping.\n  // - Each block should increment the correct element in the output vector.\n  // - Use Kokkos::TeamPolicy::team_size() and Kokkos::TeamPolicy::team_rank() to determine the\n  //   number of threads in the team and the rank of the current thread in the team.\n}",
            "// YOUR CODE HERE\n}",
            "auto const n = s.extent(0);\n\n   Kokkos::parallel_for(\"first letter counts\", n, KOKKOS_LAMBDA (size_t i) {\n      for (auto const c : s(i)) {\n         bins(c - 'a')++;\n      }\n   });\n}",
            "// TODO: Kokkos parallel for.\n    // TODO: Count the number of strings that start with each letter in the alphabet.\n    // TODO: Store the output in `bins` array.\n\n}",
            "size_t len = s.size();\n\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int&) {\n    for (size_t i = 0; i < len; i++) {\n      if (s(i)!= NULL) {\n        switch (tolower(s(i)[0])) {\n          case 'a': bins(0)++; break;\n          case 'b': bins(1)++; break;\n          case 'c': bins(2)++; break;\n          case 'd': bins(3)++; break;\n          case 'e': bins(4)++; break;\n          case 'f': bins(5)++; break;\n          case 'g': bins(6)++; break;\n          case 'h': bins(7)++; break;\n          case 'i': bins(8)++; break;\n          case 'j': bins(9)++; break;\n          case 'k': bins(10)++; break;\n          case 'l': bins(11)++; break;\n          case'm': bins(12)++; break;\n          case 'n': bins(13)++; break;\n          case 'o': bins(14)++; break;\n          case 'p': bins(15)++; break;\n          case 'q': bins(16)++; break;\n          case 'r': bins(17)++; break;\n          case's': bins(18)++; break;\n          case 't': bins(19)++; break;\n          case 'u': bins(20)++; break;\n          case 'v': bins(21)++; break;\n          case 'w': bins(22)++; break;\n          case 'x': bins(23)++; break;\n          case 'y': bins(24)++; break;\n          case 'z': bins(25)++; break;\n          default: break;\n        }\n      }\n    }\n  });\n}",
            "auto const N = s.extent(0);\n  Kokkos::parallel_for(\"firstLetterCounts\", N, KOKKOS_LAMBDA(size_t i) {\n    char c = s(i)[0];\n    if ('a' <= c && c <= 'z') {\n      ++bins(c - 'a');\n    }\n  });\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> >(0, s.extent(0)), KOKKOS_LAMBDA(const int i) {\n    size_t firstLetter = s(i)[0] - 'a';\n    bins(firstLetter) += 1;\n  });\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::DynamicRank>>;\n    Kokkos::parallel_for(\"firstLetterCounts\", policy, KOKKOS_LAMBDA(size_t i) {\n        int c = s(i)[0] - 'a';\n        ++bins(c);\n    });\n}",
            "}",
            "// TODO: Implement this function.\n}",
            "// TODO: write function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic> >(0, s.extent(0)),\n    KOKKOS_LAMBDA (int i) {\n      for (char c = 'a'; c <= 'z'; c++) {\n         if (s(i)[0] == c) {\n            Kokkos::atomic_fetch_add(&(bins(c - 'a')), 1);\n         }\n      }\n   });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)),\n    KOKKOS_LAMBDA (size_t i) {\n      size_t start = 0;\n      size_t end = strlen(s(i));\n      int curr = (int) s(i)[start];\n      if (curr >= 'a' && curr <= 'z') {\n        curr = curr - 'a';\n        bins(curr) += 1;\n      }\n    }\n  );\n}",
            "size_t len = s.extent(0);\n    Kokkos::parallel_for(\"firstLetterCounts\", len, KOKKOS_LAMBDA(size_t i) {\n        bins[s(i)[0] - 'a'] += 1;\n    });\n}",
            "Kokkos::parallel_for(26, KOKKOS_LAMBDA(int i) {\n        size_t count = 0;\n        for (size_t j = 0; j < s.extent(0); j++) {\n            if (s(j)[0] == 'a' + i) {\n                count++;\n            }\n        }\n        bins(i) = count;\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t>(0, s.size()),\n    KOKKOS_LAMBDA(size_t i) {\n    const char* string = s(i);\n    size_t count = 0;\n    while (string[count]!= 0) {\n      bins(string[count] - 'a')++;\n      count++;\n    }\n  });\n}",
            "// YOUR CODE HERE\n    // Do not edit anything below this line\n}",
            "// TODO: your code goes here\n    // size_t N = s.extent(0);\n\t// for (int i = 0; i < 26; i++) {\n\t// \tbins(i) = 0;\n\t// }\n    // for (size_t i = 0; i < N; i++) {\n    //     char ch = s(i)[0];\n    //     int idx = (int)(ch - 'a');\n    //     bins(idx) += 1;\n    // }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Implement\n}",
            "// YOUR CODE HERE\n  throw std::runtime_error(\"TODO\");\n}",
            "// TODO: Implement this function.\n  // Hint: use Kokkos::parallel_for()\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 26),\n    KOKKOS_LAMBDA (const int & i) {\n        bins(i) = 0;\n        for (int j = 0; j < s.extent(0); ++j) {\n            if (s(j)[0] == 'a' + i) {\n                bins(i) += 1;\n            }\n        }\n    });\n}",
            "// Your code here\n\n  size_t num_strings = s.extent_int(0);\n\n  for (size_t i=0; i<num_strings; i++) {\n    const char* str = s(i);\n    size_t len = strlen(str);\n    char c = str[0];\n    if (c >= 'a' && c <= 'z') {\n      bins(c - 'a') += 1;\n    }\n  }\n\n  for (size_t i=0; i<26; i++) {\n    printf(\"bin %zu = %zu\\n\", i, bins(i));\n  }\n}",
            "// TODO: Implement this function.\n}",
            "// Insert code here\n}",
            "size_t length = s.extent(0);\n    Kokkos::parallel_for(\"firstLetterCounts\", length, KOKKOS_LAMBDA(size_t i) {\n        const char* str = s(i);\n        if ('a' <= *str && *str <= 'z') {\n            bins(str - 'a')++;\n        }\n    });\n}",
            "// TODO: implement\n}",
            "// TODO: implement this function\n}",
            "// Initialize bins to zero.\n  Kokkos::deep_copy(bins, 0);\n\n  // TODO: Implement your algorithm.\n\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<ExecutionSpace>(0, s.extent(0)),\n    KOKKOS_LAMBDA (const int i) {\n      const char *const p = s(i);\n      const char ch = *p;\n      bins(ch - 'a')++;\n    }\n  );\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, s.extent(0)), [=] (size_t i) {\n        // get the first character of the ith string\n        const char first = s(i)[0];\n\n        // if it's an alphabetic character\n        if (isalpha(first)) {\n            // increment the appropriate counter in the `bins` view\n            Kokkos::atomic_fetch_add(&(bins(first - 'a')), 1);\n        }\n    });\n}",
            "// TODO: your implementation goes here\n}",
            "// YOUR CODE HERE\n    // TODO: 1. Create a Kokkos execution space\n    // TODO: 2. Iterate over strings in s, counting each string's first letter in the alphabet\n    // TODO: 3. Copy result into output array\n}",
            "// TODO(student): Implement firstLetterCounts.\n}",
            "// TODO\n}",
            "using reducer_type = Kokkos::BinOp1D<Kokkos::Sum<size_t>, size_t[26]>;\n\n    // TODO: Implement this function.\n\n}",
            "const size_t n = s.extent(0);\n  const size_t m = s.extent(1);\n\n  // TODO: fill bins with number of strings for each letter\n\n  // TODO: call Kokkos::parallel_for with a lambda function to do this\n}",
            "// TODO: Implement\n  // TODO: Initialize bins to 0\n  // TODO: Determine size of range and create parallel_for\n  // TODO: Sum the counts in parallel\n}",
            "Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(s.extent(0));\n  KOKKOS_INLINE_FUNCTION\n  void operator()(const typename Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>::member_type& team) const {\n    const int i = team.league_rank();\n    const char* const str = s(i);\n    if (str[0] > 'a')\n      ++bins(str[0] - 'a');\n  }\n  Kokkos::parallel_for(\"firstLetterCounts\", policy, *this);\n  bins(); // flush the View to host\n}",
            "Kokkos::TeamPolicy<>::TeamMember teamMember;\n\n  teamMember.team_barrier();\n\n  // TODO: your code goes here\n}",
            "// Kokkos::TeamPolicy<> policy(s.extent(0));\n  Kokkos::TeamPolicy<> policy(s.extent(0), Kokkos::AUTO);\n  Kokkos::parallel_for(\"First letter counts\", policy, KOKKOS_LAMBDA (const Kokkos::TeamPolicy<>::member_type& team) {\n    const size_t i = team.league_rank();\n    const char* str = s(i);\n    bins(str[0] - 'a') += 1;\n  });\n  Kokkos::fence();\n}",
            "size_t n = s.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n), [&] (int i) {\n    if (s(i)[0] >= 'a' && s(i)[0] <= 'z')\n      bins(s(i)[0] - 'a')++;\n  });\n  Kokkos::fence();\n}",
            "/* TODO */\n}",
            "// TODO\n}",
            "// TODO\n}",
            "auto n = s.extent(0);\n  auto s_ptr = s.data();\n  auto bins_ptr = bins.data();\n\n  // TODO: replace this Kokkos parallel_reduce with Kokkos::RangePolicy()\n  //  Kokkos::parallel_reduce\n  //    (Kokkos::RangePolicy<ExecutionSpace>(0, n),\n  //     KOKKOS_LAMBDA (const int i, int &update) {\n  //       update += std::tolower(s_ptr[i][0]) - 'a';\n  //     },\n  //     KOKKOS_LAMBDA (const int i, int &update, int &update2) {\n  //       update += std::tolower(s_ptr[i][0]) - 'a';\n  //     }\n  //     );\n  //  Kokkos::parallel_for(Kokkos::RangePolicy<ExecutionSpace>(0, n), KOKKOS_LAMBDA(const int i) {\n  //    update += std::tolower(s_ptr[i][0]) - 'a';\n  //  });\n}",
            "//TODO\n}",
            "Kokkos::parallel_for(\"first_letter_counts\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, s.extent(0)), KOKKOS_LAMBDA(const int i) {\n      const char *str = s(i);\n      size_t *bin = &bins[str[0] - 'a'];\n      Kokkos::atomic_fetch_add(bin, 1);\n   });\n}",
            "// TODO: compute firstLetterCounts\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Compute the counts of each first letter in parallel using Kokkos.\n  // Hints:\n  // 1. Use Kokkos parallel_for\n  // 2. Use a functor\n  // 3. Use the parallel_reduce() method\n  // 4. The functor should have a single argument type of size_t (or equivalent)\n\n  // YOUR CODE HERE\n}",
            "// Your code goes here\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "#pragma omp parallel for\n  for(size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "// Your code goes here\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\t// get the first character of the string\n\t\tchar c = s[i][0];\n\t\t// increment the count of first character in the array\n\t\tbins[c - 'a']++;\n\t}\n}",
            "// initialize bins to zero\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // iterate through vector s\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    // get the index of the first character in the string\n    int index = (int)s[i][0] - (int)'a';\n\n    // check if the character is a letter\n    if (index >= 0 && index < 26) {\n      // increment the bin for the index of the first character\n      // in the string\n      ++bins[index];\n    }\n  }\n}",
            "#pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < s.size(); i++) {\n    size_t letter = s[i][0] - 'a';\n    if (letter < 26) {\n      __sync_fetch_and_add(&bins[letter], 1);\n    }\n  }\n}",
            "/* Your code here */\n  // The number of bins is the number of letters in the alphabet\n  // The range of each bin is 0 to 25, inclusive\n  // The bins represent the letters in the alphabet\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    // This loop can be parallelized\n    if (s[i].length()!= 0) {\n      // If s[i] is not empty\n      bins[s[i][0] - 'a']++; // Add one to the corresponding bin\n    }\n  }\n}",
            "size_t const num_strings = s.size();\n\n   // Initialize bins to 0\n   std::fill(bins.begin(), bins.end(), 0);\n\n   // Outer loop will iterate through each string\n   #pragma omp parallel for\n   for (size_t i = 0; i < num_strings; i++) {\n      std::string const &str = s[i];\n\n      // Inner loop will increment the bin for each letter in the string\n      for (char c : str) {\n         bins[c - 'a']++;\n      }\n   }\n}",
            "// Initialize output array to zero\n  std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    // Increment the count for the first letter of the current string\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        #pragma omp atomic\n        bins[i] = 0;\n    }\n    for (std::string const &w : s) {\n        #pragma omp atomic\n        ++bins[w[0] - 'a'];\n    }\n}",
            "bins = {};\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++) {\n        #pragma omp atomic\n        ++bins[static_cast<size_t>(s[i][0] - 'a')];\n    }\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::array<int, 26> counts;\n  counts.fill(0);\n\n#pragma omp parallel for\n  for (int i = 0; i < s.size(); ++i) {\n    char firstLetter = s[i][0];\n    ++counts[firstLetter - 'a'];\n  }\n\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = counts[i];\n  }\n}",
            "/* Initialize bins array */\n    std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        /* Use OpenMP's atomic directive to count */\n        #pragma omp atomic\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: Fill this in.\n    for (int i=0; i<s.size(); i++) {\n        bins[s[i][0]-'a']++;\n    }\n}",
            "omp_set_num_threads(4);\n#pragma omp parallel for\n  for (auto i = 0; i < 26; ++i) {\n    for (auto const& str : s) {\n      if (str[0] == 'a' + i) {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        ++bins[static_cast<size_t>(s[i][0]) - static_cast<size_t>('a')];\n    }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "if (s.size() == 0) return;\n  std::fill(bins.begin(), bins.end(), 0);\n\n  omp_set_num_threads(omp_get_max_threads());\n\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        for (auto const& word : s) {\n            if (word.front() - 'a' == i) {\n                bins[i]++;\n                break;\n            }\n        }\n    }\n\n}",
            "// Your code here\n    //\n    // hint: use 26 bins to count the occurrences of all letters in the alphabet.\n    //       you can parallelize the for loop with OpenMP\n    //\n}",
            "// TODO: implement\n}",
            "bins = std::array<size_t, 26>();\n\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); i++){\n    for(size_t j = 0; j < s[i].length(); j++){\n      int letter_index = s[i][j] - 'a';\n      if(letter_index >= 0 && letter_index <= 25) {\n        bins[letter_index] += 1;\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); ++i) {\n\t\tbins[s[i][0] - 'a'] += 1;\n\t}\n}",
            "size_t const num_strings = s.size();\n  for (size_t i = 0; i < num_strings; ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    if (s[i].size() > 0) {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n}",
            "for (int i = 0; i < 26; ++i) {\n      bins[i] = 0;\n   }\n\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for (size_t i = 0; i < s.size(); ++i) {\n         if (s[i].length() > 0) {\n            bins[s[i][0] - 'a'] += 1;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        auto c = s[i][0];\n        if (std::isalpha(c)) {\n            bins[c - 'a']++;\n        }\n    }\n}",
            "size_t num_threads = 0;\n    #pragma omp parallel\n    {\n        num_threads = omp_get_num_threads();\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i) {\n        // We compute only the first character.\n        std::string str = s[i];\n        char first = str[0];\n        if ((first >= 'a' && first <= 'z') || (first >= 'A' && first <= 'Z')) {\n            size_t bin_num = static_cast<size_t>(first - 'a');\n            #pragma omp atomic\n            ++bins[bin_num];\n        }\n    }\n\n    for (size_t i = 0; i < bins.size(); ++i) {\n        bins[i] /= num_threads;\n    }\n}",
            "int threads = std::thread::hardware_concurrency();\n  std::vector<size_t> tmpBins(threads * 26, 0);\n\n#pragma omp parallel num_threads(threads)\n  {\n    std::vector<size_t> &bins = tmpBins;\n    size_t id = omp_get_thread_num();\n    size_t idStart = 26 * id;\n    size_t idEnd = 26 * (id + 1);\n    for (size_t i = 0; i < s.size(); ++i) {\n      bins[idStart + std::tolower(s[i][0]) - 'a']++;\n    }\n\n#pragma omp barrier\n\n#pragma omp single\n    {\n      for (size_t i = 0; i < 26; ++i) {\n        bins[i] += std::accumulate(std::next(bins.begin(), i), bins.end(), 0);\n      }\n    }\n  }\n\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = tmpBins[i];\n  }\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\t// Use the first letter of the string to get the index in the bins array.\n\t\t// Add 1 to the index to count the string.\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "// Initialize the bins array to zeros.\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  // Use OpenMP to distribute the work across threads.\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "for(std::string str : s) {\n    #pragma omp atomic\n      bins[str[0] - 'a']++;\n  }\n}",
            "// Your code goes here\n}",
            "// TODO: Fill in this function\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> locBins;\n        #pragma omp for\n        for(size_t i = 0; i < 26; i++) {\n            locBins[i] = 0;\n        }\n\n        #pragma omp for\n        for(size_t i = 0; i < s.size(); i++) {\n            if(s[i].size() > 0) {\n                locBins[(int)s[i][0] - 'a']++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for(size_t i = 0; i < 26; i++) {\n                bins[i] += locBins[i];\n            }\n        }\n    }\n}",
            "size_t n = s.size();\n\t#pragma omp parallel for schedule(static)\n\tfor (size_t i = 0; i < n; ++i) {\n\t\tbins[s[i][0] - 'a']++;\n\t}\n}",
            "// Replace the following code with your solution.\n  // You may not need all the code below.\n  bins.fill(0);\n  #pragma omp parallel for schedule(static)\n  for(size_t i=0;i<s.size();i++){\n    int pos = s[i][0]-'a';\n    #pragma omp critical\n    bins[pos] += 1;\n  }\n}",
            "// TODO: implement firstLetterCounts\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    std::string const& str = s[i];\n    if (str.size() > 0) {\n      auto const c = str[0];\n      ++bins[static_cast<size_t>(std::tolower(c)) - static_cast<size_t>('a')];\n    }\n  }\n}",
            "// TODO\n}",
            "size_t const num_threads = 4;\n    std::vector<std::array<size_t, 26>> sums(num_threads, std::array<size_t, 26>{0});\n\n    // TODO: your code goes here\n}",
            "auto const &size = s.size();\n  for(size_t i = 0; i < size; ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO: implement this function\n  // hint: use omp_set_num_threads(num_threads) to set the number of threads\n\n}",
            "// initialize\n    for (size_t i = 0; i < bins.size(); i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        if (isalpha(s[i][0]) && islower(s[i][0])) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "/* Your code here */\n}",
            "#pragma omp parallel for schedule(static)\n  for (auto& c: s) {\n    ++bins[c[0] - 'a'];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (auto const& word : s) {\n        #pragma omp atomic\n        ++bins[word[0] - 'a'];\n    }\n}",
            "// TODO\n}",
            "// TODO: Your code goes here\n}",
            "#pragma omp parallel for\n  for (unsigned int i = 0; i < s.size(); ++i) {\n    if (isalpha(s[i][0])) {\n      int j = s[i][0] - 'a';\n      ++bins[j];\n    }\n  }\n}",
            "// Your code here\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (std::string const& str : s) {\n        int index = str[0] - 'a';\n        if (index >= 0 && index < 26) {\n            bins[index]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; ++i) {\n    for (auto const& w: s) {\n      if (w[0] == i + 'a') {\n        #pragma omp atomic\n        ++bins[i];\n      }\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    size_t index = static_cast<size_t>(s[i][0] - 'a');\n    bins[index]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for schedule(dynamic, 100)\n   for (auto const& str : s) {\n      bins[str[0] - 'a']++;\n   }\n}",
            "omp_set_num_threads(8);\n    #pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[int(s[i][0] - 'a')]++;\n    }\n}",
            "/* Replace the following line with omp parallel for */\n  for (auto i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO: Fill in this function.\n  // Don't forget to use the bins array, which is initialized to all zeros.\n\n  for (std::string::const_iterator it = s[0].begin(); it!= s[0].end(); it++) {\n    bins[(*it) - 'a']++;\n  }\n\n  // #pragma omp parallel\n  // {\n  //   for (std::string::const_iterator it = s[0].begin(); it!= s[0].end(); it++) {\n  //     bins[(*it) - 'a']++;\n  //   }\n  // }\n}",
            "#pragma omp parallel for\n  for (auto const& word : s) {\n    auto const first_char = word[0];\n    if (first_char >= 'a' && first_char <= 'z') {\n      ++bins[first_char - 'a'];\n    }\n  }\n}",
            "bins.fill(0);\n  #pragma omp parallel for reduction(+:bins)\n  for (size_t i = 0; i < s.size(); i++) {\n    int base = (int)std::toupper(s[i][0]) - 'A';\n    bins[base]++;\n  }\n}",
            "for (auto const& str : s) {\n        if (str.size() > 0) {\n            auto firstLetter = std::tolower(str[0]);\n            if (firstLetter >= 'a' && firstLetter <= 'z') {\n                bins[firstLetter - 'a']++;\n            }\n        }\n    }\n}",
            "for (auto const& word: s) {\n    ++bins[static_cast<unsigned int>(word[0] - 'a')];\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++) {\n        char c = s[i][0];\n        if(c >= 'a' && c <= 'z') {\n            int index = c - 'a';\n            bins[index]++;\n        }\n    }\n}",
            "// Your code here\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (auto const& str : s) {\n        auto const first = str[0];\n        #pragma omp atomic\n        ++bins[first - 'a'];\n    }\n}",
            "bins.fill(0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < s.size(); ++i) {\n      if (isalpha(s[i][0])) {\n         ++bins[s[i][0] - 'a'];\n      }\n   }\n}",
            "/* TODO: Fill in the following code to count the occurrences of each letter in the vector s.\n       Be sure to use parallelism via OpenMP. */\n    int n = s.size();\n    #pragma omp parallel for\n    for(int i=0; i<n; i++){\n        std::string s = s[i];\n        for(char c: s){\n            int val = c - 'a';\n            bins[val]++;\n        }\n    }\n\n    // for(int i=0; i<26; i++){\n    //     std::cout << bins[i] << \" \";\n    // }\n    // std::cout << std::endl;\n}",
            "// TODO: implement me!\n}",
            "#pragma omp parallel\n  {\n  }\n}",
            "// Write your code here.\n}",
            "size_t num_strings = s.size();\n  int num_threads = 4;\n  int chunk_size = num_strings / num_threads;\n  int remainder = num_strings % num_threads;\n\n  std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for schedule(dynamic, chunk_size)\n  for (int i = 0; i < num_strings; ++i) {\n    int tid = omp_get_thread_num();\n    bins[static_cast<int>(s[i][0] - 'a')] += 1;\n  }\n}",
            "size_t length = s.size();\n\n  // Initialize the bins\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  // Do this in parallel\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < length; i++) {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n}",
            "#pragma omp parallel\n   #pragma omp for\n   for(size_t i = 0; i < s.size(); ++i) {\n      char c = s[i][0];\n      #pragma omp atomic\n      ++bins[c - 'a'];\n   }\n}",
            "if (s.empty()) {\n    return;\n  }\n\n  bins.fill(0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "size_t n = s.size();\n\tif (n == 0) {\n\t\treturn;\n\t}\n\n\tstd::array<size_t, 26> localBins;\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < n; i++) {\n\t\tchar c = std::tolower(s[i][0]);\n\t\tif (c >= 'a' && c <= 'z') {\n\t\t\tlocalBins[c - 'a']++;\n\t\t}\n\t}\n\n\t#pragma omp critical\n\tfor (size_t i = 0; i < 26; i++) {\n\t\tbins[i] += localBins[i];\n\t}\n}",
            "#pragma omp parallel for schedule(static)\n  for (int i = 0; i < s.size(); ++i) {\n    if (std::isalpha(s[i][0])) {\n      #pragma omp atomic\n      ++bins[std::tolower(s[i][0]) - 'a'];\n    }\n  }\n}",
            "// TODO: compute firstLetterCounts in parallel\n  const int num_threads = omp_get_max_threads();\n  bins.fill(0);\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i=0; i < s.size(); ++i) {\n    #pragma omp atomic\n      ++bins[(int)s[i].at(0) - 97];\n  }\n}",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < s.size(); ++i) {\n      char c = std::tolower(s[i][0]);\n      if (c >= 'a' && c <= 'z') {\n         ++bins[c - 'a'];\n      }\n   }\n}",
            "// TODO: parallelize this\n    for (std::string const& string : s) {\n        bins[string[0] - 'a']++;\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        auto const c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n        }\n    }\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: Implement the OpenMP version of this algorithm.\n}",
            "#pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < s.size(); ++i) {\n        size_t c = s[i][0] - 'a';\n        #pragma omp atomic\n        ++bins[c];\n    }\n}",
            "#pragma omp parallel for\n  for (auto const& it : s) {\n    ++bins[it[0] - 'a'];\n  }\n}",
            "// YOUR CODE HERE\n\n    #pragma omp parallel for\n    for (auto i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (auto i = 0; i < s.size(); ++i) {\n        if (!s[i].empty())\n            bins[s[i][0] - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    size_t index = s[i][0] - 'a';\n    bins[index]++;\n  }\n}",
            "std::memset(bins.data(), 0, bins.size() * sizeof(size_t));\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "#pragma omp parallel for schedule(static)\n    for (auto & letter: bins) {\n        letter = 0;\n    }\n\n    for (auto const& str: s) {\n#pragma omp atomic\n        ++bins[str[0] - 'a'];\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < s.size(); ++i) {\n      if (isalpha(s[i][0])) {\n         auto c = tolower(s[i][0]) - 'a';\n         #pragma omp atomic\n         ++bins[c];\n      }\n   }\n}",
            "// Initialize bins to 0\n\tfor (auto & bin : bins) {\n\t\tbin = 0;\n\t}\n\n\t// Compute in parallel\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tsize_t letter = s[i][0] - 'a';\n\n\t\t// Atomically increment the count for letter\n\t\t#pragma omp atomic\n\t\tbins[letter]++;\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[tolower(s[i][0]) - 'a']++;\n  }\n}",
            "// YOUR CODE HERE\n    std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n    // END OF YOUR CODE\n}",
            "// Compute first letter count for each letter.\n    // You may assume all strings are in lower case.\n    //\n    // TODO: Implement this function.\n}",
            "for(std::string const& str : s) {\n      size_t index = str[0] - 'a';\n      ++bins[index];\n   }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    // TODO\n  }\n}",
            "size_t n = s.size();\n    for(int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for(int i = 0; i < n; i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel for schedule(dynamic)\n   for (size_t i = 0; i < s.size(); ++i) {\n      auto ch = s[i][0];\n      if (ch >= 'a' && ch <= 'z') {\n         bins[ch - 'a']++;\n      }\n   }\n}",
            "// omp_set_num_threads(8); // use 8 threads\n  #pragma omp parallel for\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto const &str: s) {\n    #pragma omp atomic\n    bins[str[0] - 'a']++;\n  }\n}",
            "#pragma omp parallel for\n    for (unsigned i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "for (auto const& i: s) {\n        bins[i[0] - 'a']++;\n    }\n}",
            "const size_t thread_count = 4;\n    std::vector<std::thread> workers;\n\n    for (auto &worker : workers) {\n        worker.join();\n    }\n\n    #pragma omp parallel num_threads(thread_count)\n    {\n        const size_t thread_id = omp_get_thread_num();\n        const size_t string_count = s.size();\n        const size_t local_string_count = string_count / thread_count;\n        const size_t start_string_index = local_string_count * thread_id;\n        const size_t end_string_index = std::min(string_count, start_string_index + local_string_count);\n\n        for (size_t i = start_string_index; i < end_string_index; ++i) {\n            auto const& string = s[i];\n            auto const letter = string[0];\n            ++bins[letter - 'a'];\n        }\n    }\n}",
            "// TODO: YOUR CODE HERE\n    int length = s.size();\n    #pragma omp parallel for\n    for(int i = 0; i < length; i++){\n        char ch = s[i][0];\n        if(ch >= 'a' && ch <= 'z'){\n            bins[ch - 'a']++;\n        }\n    }\n\n}",
            "// TODO: Fill in this function.\n\n}",
            "for (auto const &x : s) {\n        #pragma omp atomic\n        ++bins[x[0] - 'a'];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   // use an unsigned type for iteration variable in omp pragma\n   // otherwise the compiler could complain that the upper bound is not representable by type size_t\n   #pragma omp parallel for\n   for (size_t i = 0; i < s.size(); ++i) {\n      ++bins[s[i][0] - 'a'];\n   }\n}",
            "// TODO: Fill in your code here.\n\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n#pragma omp parallel for\n  for (int i = 0; i < s.size(); i++) {\n    // std::cout << s[i] << std::endl;\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n\n}",
            "// TODO(student): Implement this function\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); ++i){\n    int index = s[i][0] - 'a';\n    ++bins[index];\n  }\n}",
            "std::array<size_t, 26> local_bins;\n#pragma omp parallel\n    {\n        #pragma omp for nowait\n        for (size_t i = 0; i < s.size(); i++) {\n            int idx = s[i][0] - 'a';\n            local_bins[idx]++;\n        }\n    }\n#pragma omp critical\n    {\n        for (size_t i = 0; i < 26; i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "const char * alphabet = \"abcdefghijklmnopqrstuvwxyz\";\n  #pragma omp parallel for\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = std::count_if(s.begin(), s.end(), [&](std::string str) {\n      return str[0] == alphabet[i];\n    });\n  }\n}",
            "// TODO: Your code goes here.\n\n  // for (auto const &str : s) {\n  //   bins[str[0] - 'a']++;\n  // }\n\n  size_t size = s.size();\n  if (size == 0) {\n    return;\n  }\n\n  bins[s[0][0] - 'a'] = 1;\n  for (int i = 1; i < size; ++i) {\n    if (s[i][0]!= s[i-1][0]) {\n      bins[s[i][0] - 'a'] = 1;\n    } else {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n\n}",
            "// your code goes here\n}",
            "/* TODO */\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < 26; i++) {\n\t\tbins[i] = 0;\n\t}\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); i++) {\n\t\tif (s[i][0] >= 'a' && s[i][0] <= 'z') {\n\t\t\tbins[s[i][0] - 'a']++;\n\t\t}\n\t}\n}",
            "int i;\n#pragma omp parallel for\n    for(i = 0; i < 26; i++) {\n        bins[i] = 0;\n        for(auto str : s) {\n            if(str[0] - 'a' == i) {\n                bins[i]++;\n            }\n        }\n    }\n}",
            "for (int i = 0; i < 26; i++)\n        bins[i] = 0;\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++)\n        bins[s[i][0] - 'a']++;\n}",
            "const size_t N = s.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        const char c = tolower(s[i][0]);\n        if (c >= 'a' && c <= 'z') {\n            #pragma omp atomic\n            bins[c - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    size_t index = s[i][0] - 'a';\n    bins[index] += 1;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (std::string const& str : s) {\n        #pragma omp atomic\n        ++bins[str[0] - 'a'];\n    }\n}",
            "// TODO: Fill in this function\n  // This code is correct, just not optimized\n  for(int i = 0; i < 26; i++) {\n    bins[i] = 0;\n    for(int j = 0; j < s.size(); j++) {\n      if(tolower(s[j][0]) == (char)i + 'a') {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "// TODO(student): implement this function\n\n  // Hint: use the omp parallel for directive\n  // Hint: for each letter, count the number of strings in s that start with that letter\n}",
            "// TODO: fill the bins array\n    omp_set_num_threads(8);\n\n    size_t length = s.size();\n#pragma omp parallel for\n    for (int i = 0; i < length; i++) {\n        std::string str = s[i];\n        if (str[0] < 'a') {\n            continue;\n        }\n\n        if (str[0] > 'z') {\n            continue;\n        }\n\n        int index = str[0] - 'a';\n        #pragma omp atomic\n        bins[index]++;\n    }\n\n    for (size_t i = 0; i < 26; i++) {\n        if (bins[i] % 2!= 0) {\n            bins[i]++;\n        }\n    }\n}",
            "size_t const size = s.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < size; ++i) {\n        if (s[i].empty())\n            continue;\n\n        size_t const letter = s[i][0] - 'a';\n        if (letter >= 26)\n            continue;\n\n        __atomic_fetch_add(&bins[letter], 1, __ATOMIC_SEQ_CST);\n    }\n}",
            "for (int i = 0; i < 26; ++i) {\n\t\tbins[i] = 0;\n\t}\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < s.size(); ++i) {\n\t\t#pragma omp critical\n\t\t{\n\t\t\t++bins[s[i][0] - 'a'];\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// YOUR CODE HERE\n\n   // The following line sets the number of threads\n   // Use at most 8 threads.\n   // Feel free to change this number.\n   omp_set_num_threads(8);\n\n   // For each thread, we compute a different range of letters.\n   // Each range consists of 26 buckets.\n   #pragma omp parallel for\n   for (int letter = 0; letter < 26; letter++) {\n      int start = letter * 26;\n      int end = start + 26;\n\n      // We'll use 26 buckets for each thread to store the count for that letter.\n      std::array<size_t, 26> count;\n      std::fill(count.begin(), count.end(), 0);\n\n      // For each string, we'll increment the counter for its first letter.\n      for (auto const& str: s) {\n         if (start <= str[0] && str[0] < end) {\n            count[str[0] - start]++;\n         }\n      }\n\n      // After counting, we'll accumulate the counts into the final result.\n      // We have to be careful that multiple threads don't overwrite each other's results.\n      #pragma omp critical\n      {\n         for (int j = 0; j < 26; j++) {\n            bins[j] += count[j];\n         }\n      }\n   }\n}",
            "// Your code here\n}",
            "omp_set_num_threads(omp_get_max_threads());\n    #pragma omp parallel for default(none) shared(bins, s)\n    for (size_t i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "// Your code goes here\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        bins[i] = std::count_if(s.begin(), s.end(), [&i](std::string const& s) { return s[0] - 'a' == i; });\n    }\n}",
            "#pragma omp parallel for\n    for(auto i = 0; i < 26; ++i) {\n        for(auto const& str : s) {\n            if(str.size() > 0 && (str[0] - 'a') == i) {\n                #pragma omp atomic\n                ++bins[i];\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for (auto const& str : s) {\n        size_t index = str[0] - 'a';\n        bins[index]++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "// TODO: replace this with your parallel implementation\n  omp_set_num_threads(4);\n#pragma omp parallel for\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n    for (size_t j = 0; j < s.size(); j++) {\n      if (s[j][0] - 'a' == i) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "const size_t n = s.size();\n#pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    char ch = s[i][0];\n    // std::cout << i << \" \" << ch << std::endl;\n    if ('a' <= ch and ch <= 'z') {\n      ++bins[ch - 'a'];\n    }\n  }\n\n}",
            "#pragma omp parallel for schedule(static)\n\tfor (size_t i = 0; i < 26; ++i)\n\t\tbins[i] = 0;\n\tfor (size_t i = 0; i < s.size(); ++i)\n\t\tbins[s[i][0] - 'a']++;\n}",
            "// YOUR CODE HERE\n    // You should store the output of this function in the variable `bins`.\n    // The variable `bins` has a type `std::array<size_t, 26>`\n    // Each element of `bins` is an integer that is initialized to 0.\n\n    // The outer loop is over the alphabet\n    for (int i = 0; i < 26; i++) {\n        // The inner loop is over the input array s\n        for (int j = 0; j < s.size(); j++) {\n            // Check if the current string in s has the ith character\n            if (s[j][0] - 'a' == i) {\n                // If the character is found, increase the ith element of `bins`\n                bins[i] += 1;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < 26; i++) {\n        std::string letter = std::string(1, i + 'a');\n        std::size_t count = std::count_if(s.begin(), s.end(), [&](std::string const& str){ return str.compare(0, 1, letter) == 0; });\n        bins[i] = count;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  // TODO: your code goes here\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = 0;\n        }\n\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            ++bins[s[i][0] - 'a'];\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (auto const &str : s) {\n        #pragma omp atomic\n        ++bins[str[0] - 'a'];\n    }\n}",
            "std::array<size_t, 26> privateBins;\n  // initialize bins to zero.\n  for (auto &i : bins) {\n    i = 0;\n  }\n  // for each string in s, compute bins[first letter of string].\n#pragma omp parallel for\n  for (auto &i : s) {\n    size_t firstLetter = 0;\n    if (i.length() > 0) {\n      firstLetter = (int)i[0];\n    }\n    privateBins[firstLetter]++;\n  }\n  // add privateBins to bins.\n  for (int i = 0; i < 26; i++) {\n    bins[i] = privateBins[i] + bins[i];\n  }\n}",
            "bins = std::array<size_t, 26>();\n\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[static_cast<unsigned char>(s[i][0]) - 'a']++;\n    }\n\n}",
            "for (std::string const &str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> localBins;\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            localBins[s[i][0] - 'a']++;\n        }\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 26; ++i) {\n                bins[i] += localBins[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (auto &&e : s) {\n    auto l = e[0] - 'a';\n    __sync_fetch_and_add(&bins[l], 1);\n  }\n}",
            "/* Your code goes here. */\n}",
            "// YOUR CODE HERE\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z')\n            #pragma omp atomic\n            bins[c - 'a']++;\n    }\n}",
            "for (auto const& str: s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "#pragma omp parallel for num_threads(8)\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "size_t n = s.size();\n    std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        int num = s[i][0] - 'a';\n        if (num >= 0 && num < 26) {\n            bins[num]++;\n        }\n    }\n}",
            "for(char c = 'a'; c <= 'z'; ++c) {\n        bins[c - 'a'] = 0;\n    }\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); ++i) {\n        char c = s[i][0];\n        #pragma omp critical\n        {\n            bins[c - 'a']++;\n        }\n    }\n}",
            "bins = std::array<size_t, 26>();\n    #pragma omp parallel for\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "auto const n = s.size();\n\n    // Each thread processes its own subset of the input.\n    auto const chunkSize = (n + omp_get_max_threads() - 1) / omp_get_max_threads();\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < n; i += chunkSize) {\n        auto const j = i + chunkSize;\n        auto const end = std::min(n, j);\n        for (auto const &str : s | std::views::slice(i, end)) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = std::count_if(s.begin(), s.end(), [&i](auto const& str) {\n            return (str[0] - 'a') == i;\n        });\n    }\n}",
            "size_t threads = 4;\n\n    #pragma omp parallel num_threads(threads)\n    {\n        // Initialize the array to zero.\n        #pragma omp for schedule(static)\n        for (int i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n\n        // Add 1 to each index of the array that corresponds to the starting letter of each string.\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); i++) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "// Write your solution here.\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    char c = std::tolower(s[i][0]);\n    if ('a' <= c && c <= 'z')\n      ++bins[c - 'a'];\n  }\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 26> local_bins;\n        std::fill(local_bins.begin(), local_bins.end(), 0);\n\n#pragma omp for\n        for (size_t i = 0; i < s.size(); i++) {\n            ++local_bins[std::tolower(s[i][0]) - 'a'];\n        }\n\n#pragma omp critical\n        for (size_t i = 0; i < bins.size(); i++) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      ++bins[s[i][0] - 'a'];\n    }\n  }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < 26; ++i) {\n      bins[i] = 0;\n   }\n   #pragma omp parallel for\n   for (size_t i = 0; i < s.size(); ++i) {\n      size_t const offset = 'a';\n      bins[s[i][0] - offset]++;\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// fill in bins here\n  omp_set_num_threads(4);\n  #pragma omp parallel\n  {\n    #pragma omp for schedule(dynamic)\n    for (int i=0; i<26; i++) bins[i] = 0;\n  }\n  for (int i=0; i<26; i++) {\n    std::string search_letter = \"\";\n    search_letter.push_back(i + 'a');\n    search_letter.push_back('\\0');\n    #pragma omp parallel\n    {\n      #pragma omp for schedule(dynamic)\n      for (int j=0; j<s.size(); j++) {\n        std::string str = s[j];\n        if (str[0] == search_letter[0]) {\n          bins[i]++;\n        }\n      }\n    }\n  }\n}",
            "std::memset(bins.data(), 0, 26 * sizeof(size_t));\n    #pragma omp parallel for\n    for (auto const& str: s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "bins.fill(0);\n\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> binsThreadLocal;\n\n        #pragma omp for schedule(static)\n        for (auto i = 0U; i < s.size(); ++i)\n        {\n            auto const& word = s[i];\n            binsThreadLocal[word[0] - 'a'] += 1;\n        }\n\n        #pragma omp critical\n        {\n            for (auto i = 0U; i < 26; ++i)\n            {\n                bins[i] += binsThreadLocal[i];\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n  #pragma omp parallel\n  {\n    int nthreads = omp_get_num_threads();\n    int tid = omp_get_thread_num();\n    int blockSize = s.size() / nthreads;\n    int start = std::min(blockSize * tid, s.size());\n    int end = std::min(start + blockSize, s.size());\n    std::array<size_t, 26> counters;\n    for (int i = 0; i < 26; i++) {\n      counters[i] = 0;\n    }\n    for (int i = start; i < end; i++) {\n      auto first = s[i][0] - 'a';\n      if (first >= 0 && first < 26) {\n        counters[first]++;\n      }\n    }\n    for (int i = 0; i < 26; i++) {\n      bins[i] += counters[i];\n    }\n  }\n  // END OF YOUR CODE\n}",
            "// Fill bins array with zeros\n  // for (size_t i = 0; i < bins.size(); ++i) {\n  //   bins[i] = 0;\n  // }\n  // OR\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n  // OpenMP stuff\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    // std::cout << omp_get_thread_num() << \" is here\" << std::endl;\n    // std::cout << \"i = \" << i << std::endl;\n    #pragma omp atomic\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < (int)s.size(); ++i) {\n        size_t c = s[i][0] - 'a';\n        if (c < 26) {\n            bins[c]++;\n        }\n    }\n\n}",
            "#pragma omp parallel for\n  for(auto const& s : s) {\n    if(s.length() > 0) {\n      bins[s[0] - 'a'] += 1;\n    }\n  }\n}",
            "bins.fill(0);\n    std::for_each(s.cbegin(), s.cend(), [&bins](const std::string& s) {\n        bins[s[0] - 'a']++;\n    });\n}",
            "bins.fill(0);\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); i++) {\n            if (s[i].size() > 0) {\n                bins[s[i][0] - 'a']++;\n            }\n        }\n    }\n}",
            "auto const n = s.size();\n  #pragma omp parallel\n  {\n    std::array<size_t, 26> localBins;\n    #pragma omp for schedule(static)\n    for (size_t i = 0; i < n; ++i) {\n      auto const c = s[i][0];\n      if (c >= 'a' && c <= 'z') {\n        ++localBins[c - 'a'];\n      }\n    }\n    #pragma omp critical\n    for (size_t i = 0; i < 26; ++i) {\n      bins[i] += localBins[i];\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (auto i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n  for (auto const& i : s) {\n    ++bins[i[0] - 'a'];\n  }\n}",
            "// TODO: Your code goes here.\n\n}",
            "// TODO: Your code here.\n\n  //#pragma omp parallel for\n  //for (auto i=0;i<s.size();i++){\n  //  std::string w=s[i];\n  //  if (w[0] <= 'z' && w[0] >= 'a'){\n  //    bins[w[0] - 'a']++;\n  //  }\n  //}\n\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "/* TODO: add your code here */\n    std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        ++bins[static_cast<unsigned char>(s[i][0]) - 'a'];\n    }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for(size_t i = 0; i < s.size(); ++i) {\n            bins[static_cast<unsigned char>(s[i][0] - 'a')]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        size_t c = s[i][0] - 'a';\n        ++bins[c];\n    }\n}",
            "size_t n = s.size();\n    memset(&bins, 0, sizeof(size_t)*26);\n\n#pragma omp parallel\n    {\n        int num_threads = omp_get_num_threads();\n        int thread_id = omp_get_thread_num();\n\n        size_t chunk = n / num_threads;\n        size_t start = chunk * thread_id;\n        size_t end = start + chunk;\n        if (thread_id == (num_threads - 1)) {\n            end = n;\n        }\n\n        for (size_t i = start; i < end; ++i) {\n            ++bins[s[i][0] - 'a'];\n        }\n    }\n}",
            "// omp_set_num_threads(omp_get_max_threads());\n    // std::cout << omp_get_max_threads() << std::endl;\n\n    std::memset(&bins[0], 0, sizeof(size_t) * 26);\n    #pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i].size() > 0) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "int threads = omp_get_max_threads();\n    #pragma omp parallel num_threads(threads)\n    {\n        int tid = omp_get_thread_num();\n        #pragma omp for\n        for (int i=0; i<26; i++)\n            bins[i] = 0;\n\n        #pragma omp for schedule(dynamic)\n        for (int i=0; i<s.size(); i++)\n            bins[s[i][0]-'a']++;\n    }\n}",
            "// write your code here\n  bins.fill(0);\n#pragma omp parallel for\n  for (auto i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "#pragma omp parallel for schedule(static)\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  #pragma omp parallel for\n  for (auto const& word : s) {\n    #pragma omp atomic\n    ++bins[word[0] - 'a'];\n  }\n}",
            "auto const num_strings = s.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < num_strings; ++i) {\n        // Use `std::tolower` to convert character to lower case.\n        // Remember that `std::tolower` is not a constant expression.\n        bins[std::tolower(s[i][0]) - 'a'] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    #pragma omp atomic\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        if (isalpha(s[i][0])!= 0) {\n            ++bins[tolower(s[i][0]) - 'a'];\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); i++) {\n        auto letter = s[i][0];\n        bins[letter - 'a']++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: Fill in.\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n    for (auto const& str : s) {\n      if (str[0] == (char)('a' + i)) {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "//# omp parallel for\n\tfor (unsigned i = 0; i < s.size(); ++i) {\n\t\t++bins[s[i][0] - 'a'];\n\t}\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "serial",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "bins.fill(0);\n   for (auto const& str : s) {\n      ++bins[str[0] - 'a'];\n   }\n}",
            "// TODO: implement me!\n}",
            "bins.fill(0);\n  for (std::string const& str : s) {\n    if (str.size() > 0) {\n      bins[str[0] - 'a'] += 1;\n    }\n  }\n}",
            "for (auto const& i : s) {\n        bins[i[0] - 'a']++;\n    }\n}",
            "bins = {};\n    for(auto const& str : s) {\n        ++bins[static_cast<int>(str[0]) - 'a'];\n    }\n}",
            "for (auto const& word : s) {\n        size_t const index = (size_t) (word[0] - 'a');\n        ++bins[index];\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (std::string const& str : s) {\n      if (!str.empty()) {\n         ++bins[str[0] - 'a'];\n      }\n   }\n}",
            "for(char c = 'a'; c <= 'z'; ++c) {\n        for(auto const &word : s) {\n            if (word[0] == c) {\n                ++bins[c - 'a'];\n                break;\n            }\n        }\n    }\n}",
            "for (auto const& elem : s) {\n        ++bins[elem[0] - 'a'];\n    }\n}",
            "for (auto const& str: s) {\n    if (!str.empty()) {\n      ++bins[static_cast<unsigned char>(str[0]) - 'a'];\n    }\n  }\n}",
            "for (auto const& str : s) {\n        if (str.empty()) {\n            ++bins[0];\n        } else {\n            ++bins[static_cast<size_t>(str[0]) - 'a'];\n        }\n    }\n}",
            "/* Your code here */\n}",
            "for(auto const& item : s) {\n    bins[item[0]-'a']++;\n  }\n\n  // TODO: Implement this function\n}",
            "bins.fill(0);\n   for (std::string const& str : s) {\n      if (str.length() == 0)\n         continue;\n      bins[str[0] - 'a']++;\n   }\n}",
            "for (auto const &word : s) {\n        if (word.size() > 0) {\n            bins[static_cast<size_t>(word[0]) - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n    if (!str.empty())\n      bins[str[0] - 'a']++;\n  }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n    if (isalpha(s[i][0])) {\n      bins[tolower(s[i][0]) - 'a']++;\n    }\n  }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        bins[c - 'a'] = std::count_if(s.begin(), s.end(), [&c](std::string const& s) {\n            return!s.empty() && (s.front() == c);\n        });\n    }\n}",
            "for(auto str : s) {\n        for(char c : str) {\n            int index = c - 'a';\n            bins[index]++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& w : s) {\n        ++bins[tolower(w.front()) - 'a'];\n    }\n}",
            "std::array<bool, 26> lettersFound{false};\n  for (auto const& x : s) {\n    if (x.size() > 0) {\n      size_t const index = x[0] - 'a';\n      if (!lettersFound[index]) {\n        lettersFound[index] = true;\n        ++bins[index];\n      }\n    }\n  }\n}",
            "for (auto const& word : s) {\n    bins[word[0] - 'a']++;\n  }\n}",
            "for (std::string const& str : s) {\n    bins[static_cast<int>(str[0]) - 'a'] += 1;\n  }\n}",
            "for (auto const& str: s) {\n    if (str.length() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "for (auto const& word : s) {\n        size_t index = word[0] - 'a';\n        bins[index]++;\n    }\n}",
            "for (auto const& str : s) {\n        // if the first letter of the string is a-z\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            // increment the counter for that letter\n            ++bins[str[0] - 'a'];\n        }\n    }\n}",
            "// TODO\n}",
            "// Your code here.\n}",
            "for (auto &str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "// Your code goes here\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        size_t count = 0;\n        for (auto const& str : s) {\n            if (str.size() > 0 && str.front() == c) {\n                count++;\n            }\n        }\n        bins[c - 'a'] = count;\n    }\n}",
            "for (auto c: s) {\n    bins[c[0] - 'a']++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& item : s) {\n    bins[item[0] - 'a']++;\n  }\n}",
            "for (auto const &str: s) {\n    if (str.length() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "for (auto const& string : s) {\n    bins[static_cast<size_t>(string[0]) - static_cast<size_t>('a')]++;\n  }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (std::string const& word : s) {\n    // for (char c : word) {\n    for (int c = 0; c < word.length(); ++c) {\n      // std::cout << c << std::endl;\n      bins[static_cast<size_t>(word[c] - 'a')] += 1;\n    }\n  }\n\n  // for (size_t i = 0; i < bins.size(); ++i) {\n  //   std::cout << bins[i] << std::endl;\n  // }\n}\n\n/*\n  Write a function that takes two vectors of ints and returns a new vector of ints that is a concatenation of the\n  two input vectors.\n\n  Example:\n  input: [1, 2, 3], [4, 5, 6]\n  output: [1, 2, 3, 4, 5, 6]\n*/\nstd::vector<int> concat(std::vector<int> const& v1, std::vector<int> const& v2) {\n  std::vector<int> v;\n  v.reserve(v1.size() + v2.size());\n\n  std::copy(v1.begin(), v1.end(), std::back_inserter(v));\n  std::copy(v2.begin(), v2.end(), std::back_inserter(v));\n\n  return v;\n}\n\n/*\n  Write a function that takes in a vector of ints and a number, and returns a vector of ints where all of the values\n  in the original vector are multiplied by the number.\n\n  Example:\n  input: [1, 2, 3], 2\n  output: [2, 4, 6]\n*/\nstd::vector<int> multiplyEach(std::vector<int> const& v, int const& n) {\n  std::vector<int> v2;\n  v2.reserve(v.size());\n\n  for (int n2 : v) {\n    v2.push_back(n * n2);\n  }\n\n  return v2;\n}\n\n/*\n  Write a function that takes in two vectors of ints and returns a new vector of ints that contains the values from\n  the second vector that come after the values in the first vector. The values in the first vector should be in the\n  original order in the new vector.\n\n  Example:\n  input: [1, 2, 3], [4, 5, 6]\n  output: [5, 6]\n*/\nstd::vector<int> returnAllValuesFromVectorAfterFirst(std::vector<int> const& v1, std::vector<int> const& v2) {\n  std::vector<int> v3;\n  v3.reserve(v2.size());\n\n  bool found = false;\n\n  for (int n : v1) {\n    for (int n2 : v2) {\n      if (n == n2 &&!found) {\n        found = true;\n      } else if (n == n2 && found) {\n        v3.push_back(n);\n      }\n    }\n  }\n\n  return v3;\n}\n\n/*\n  Write a function that takes in a vector of ints and returns a vector of ints where the values are the squares of the\n  values in the original vector.\n\n  Example:\n  input: [1, 2, 3]\n  output: [1, 4, 9]\n*/\nstd::vector<int> squareEach(std::vector<int> const& v) {\n  std::vector<int> v2;\n  v2.reserve(v.size());\n\n  for (int n : v) {\n    v2.push_back(n * n);\n  }\n\n  return v2;\n}\n\n/*\n  Write a function that takes in a vector of ints and returns a vector of ints where the values are the cubes of the\n  values in the original vector.\n\n  Example:\n  input: [1, 2, 3]\n  output: [1, 8, 27]\n*/\nstd::vector<int> cubeEach(std::vector<int> const& v) {\n  std::vector<int> v2;\n  v2.reserve(v.size());\n\n  for (int n : v) {\n    v2.push_back(n * n * n);\n  }\n\n  return v2;\n}\n\n/*\n  Write a function that takes in a vector of ints and returns a vector of ints where the values are the quartics of the\n  values in the original vector.\n\n  Example:\n  input: [1, 2, 3]\n  output",
            "for (char ch = 'a'; ch <= 'z'; ++ch) {\n        bins[ch - 'a'] = 0;\n    }\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const& i : s) {\n    ++bins[i[0] - 'a'];\n  }\n}",
            "for (auto const& str: s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& word : s) {\n        if (!word.empty()) {\n            bins[word[0] - 'a']++;\n        }\n    }\n}",
            "for (auto const& str : s) {\n        size_t i{0};\n        if (str.length() > 0) {\n            i = str[0] - 'a';\n            bins[i] = bins[i] + 1;\n        }\n    }\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& str : s) {\n      auto const c = str[0];\n      if ('a' <= c && c <= 'z') {\n         ++bins[c - 'a'];\n      }\n   }\n}",
            "for (auto const& str : s) {\n      ++bins[str[0] - 'a'];\n   }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (auto const& i : s) {\n        // Increment the count of the first letter in the word.\n        ++bins[i[0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "// TODO: Your code here\n}",
            "for(std::string const& word : s) {\n        size_t pos = word.find_first_of(' ');\n        if(pos!= std::string::npos) {\n            word = word.substr(0, pos);\n        }\n        if(word.length() > 0) {\n            ++bins[tolower(word[0]) - 'a'];\n        }\n    }\n}",
            "// Your code goes here\n   // Use std::count_if\n}",
            "for (auto const& str : s) {\n      if (!str.empty()) {\n         bins[str[0] - 'a']++;\n      }\n   }\n}",
            "for (auto const& str : s) {\n        bins[static_cast<int>(str[0] - 'a')]++;\n    }\n}",
            "for (std::string const& x : s) {\n        bins[x[0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n    if (!str.empty()) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "for (auto const &str : s) {\n        auto first_letter = str[0];\n        auto index = static_cast<size_t>(first_letter - 'a');\n        ++bins[index];\n    }\n}",
            "for (std::string const& str : s) {\n    ++bins[str[0] - 'a'];\n  }\n}",
            "for (auto const& str : s) {\n        size_t index = static_cast<size_t>(str[0] - 'a');\n        ++bins[index];\n    }\n}",
            "// Initialize bins array\n    for (int i = 0; i < 26; i++)\n        bins[i] = 0;\n\n    // For each string in s\n    for (auto const& str : s) {\n        // Get first letter\n        char c = tolower(str[0]);\n\n        // Increment the letter's count\n        bins[c - 'a']++;\n    }\n}",
            "for (auto const& str: s) {\n      if (str.size() > 0) {\n         bins[str[0] - 'a']++;\n      }\n   }\n}",
            "for (auto const& w : s) {\n    if (!w.empty()) {\n      bins[w[0] - 'a'] += 1;\n    }\n  }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n    if (s[i].size() > 0)\n      ++bins[s[i][0] - 'a'];\n  }\n}",
            "for (char ch = 'a'; ch <= 'z'; ch++) {\n        for (std::string const& str : s) {\n            if (str.size() > 0 && str[0] == ch) {\n                bins[ch - 'a']++;\n                break;\n            }\n        }\n    }\n}",
            "for (auto const& c : s) {\n    if (c.empty()) {\n      continue;\n    }\n    ++bins[std::tolower(c.front()) - 'a'];\n  }\n}",
            "for (auto const& elem : s) {\n    if (elem.size() > 0) {\n      ++bins[elem[0] - 'a'];\n    }\n  }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "for(char ch = 'a'; ch <= 'z'; ++ch) {\n        for(const auto &str : s) {\n            if(str[0] == ch) {\n                ++bins[ch - 'a'];\n            }\n        }\n    }\n}",
            "// Your code goes here\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& x : s)\n        ++bins[std::tolower(x.front()) - 'a'];\n}",
            "for (auto const& w: s) {\n        bins[static_cast<char>(w[0]) - 'a'] += 1;\n    }\n}",
            "bins.fill(0);\n\n    for (auto const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "bins.fill(0);\n    for (std::string const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "// Your code goes here\n}",
            "bins.fill(0);\n   for (auto const& e : s) {\n      ++bins[static_cast<size_t>(e[0] - 'a')];\n   }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n    for (std::string const& str: s)\n      if (str[0] == c)\n        ++bins[c - 'a'];\n  }\n}",
            "// TODO: replace this for-loop with your code\n  for (auto const &word : s) {\n    int index = word.at(0) - 'a';\n    bins[index]++;\n  }\n}",
            "for (auto const &element : s) {\n        bins[element[0] - 'a']++;\n    }\n}",
            "for (char ch = 'a'; ch <= 'z'; ++ch) {\n    for (std::string const& str : s)\n      if (str.front() == ch)\n        ++bins[ch - 'a'];\n  }\n}",
            "for (char c = 'a'; c <= 'z'; c++) {\n        size_t count{0};\n        for (auto const& e : s) {\n            if (e.front() == c) {\n                count++;\n            }\n        }\n        bins[c - 'a'] = count;\n    }\n}",
            "// Fill array with 0's\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // For each string,\n  //  if it starts with that letter, increment the corresponding index of bins array\n  for (auto const& str : s) {\n    if (!str.empty()) {\n      bins[str[0] - 'a']++;\n    }\n  }\n\n  // Note: we could use a std::map for this instead, but this is more concise\n}",
            "// TODO: fill in this function\n  for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "// Fill bins with the number of strings in s that start with that letter\n  for (auto &c : s) {\n    if (!c.empty()) {\n      ++bins[static_cast<int>(c[0]) - 'a'];\n    }\n  }\n}",
            "for(size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto &str : s) {\n\t\tbins[str[0] - 'a']++;\n\t}\n}",
            "for (auto const& str: s) {\n      if (str.empty()) {\n         continue;\n      }\n      bins[str[0] - 'a']++;\n   }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    size_t index = s[i][0] - 'a';\n    if (index >= 0 && index <= 25) {\n      bins[index]++;\n    }\n  }\n}",
            "for (auto const& str: s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "// Your code here\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& item : s) {\n    if (!item.empty()) {\n      ++bins[item.front() - 'a'];\n    }\n  }\n}",
            "for (auto const& string : s) {\n      bins[string[0] - 'a']++;\n   }\n}",
            "// Your code here\n}",
            "for (auto const &word: s) {\n    if (not word.empty()) {\n      bins[word[0] - 'a'] += 1;\n    }\n  }\n}",
            "for (auto const& str : s) {\n    size_t const index = str[0] - 'a';\n    bins[index]++;\n  }\n}",
            "for (auto str : s) {\n      if (!str.empty()) {\n         ++bins[str[0] - 'a'];\n      }\n   }\n}",
            "for (auto const& str : s) {\n    bins[static_cast<unsigned char>(str[0] - 'a')]++;\n  }\n}",
            "for (auto const& elem : s) {\n        ++bins[elem[0] - 'a'];\n    }\n}",
            "for (auto const &str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& str : s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto const& str : s) {\n        if (str.empty()) {\n            continue;\n        }\n\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        size_t count = 0;\n        for (auto const &i : s) {\n            if (i.find(c) == 0)\n                ++count;\n        }\n        bins[c - 'a'] = count;\n    }\n}",
            "for (std::string const& w : s) {\n        bins[w[0] - 'a']++;\n    }\n}",
            "for (auto const& string : s) {\n      ++bins[string[0] - 'a'];\n   }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "for (auto const &str : s) {\n    if (str.empty()) {\n      ++bins[0];\n      continue;\n    }\n\n    if (str[0] >= 'a' && str[0] <= 'z')\n      ++bins[str[0] - 'a'];\n  }\n}",
            "for (auto const& str : s) {\n        if (not str.empty()) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "std::for_each(s.cbegin(), s.cend(), [&bins](auto const& str) {\n    auto ch = std::tolower(str[0]);\n    if (std::isalpha(ch)) {\n      ++bins[ch - 'a'];\n    }\n  });\n}",
            "// Your code here\n   bins.fill(0);\n\n   for (std::string const& word : s) {\n      if (!word.empty()) {\n         ++bins[word[0] - 'a'];\n      }\n   }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        bins[c - 'a'] = 0;\n        for (size_t i = 0; i < s.size(); ++i) {\n            if (s[i].size() > 0 && s[i][0] == c) {\n                ++bins[c - 'a'];\n            }\n        }\n    }\n}",
            "for (auto const& str: s) {\n    if (!str.empty()) {\n      ++bins[str[0] - 'a'];\n    }\n  }\n}",
            "for (auto &s : s) {\n        if (!s.empty())\n            bins[s[0] - 'a']++;\n    }\n}",
            "for (auto const& str : s) {\n      bins[static_cast<size_t>(str[0] - 'a')]++;\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& str : s) {\n    if (str.empty()) continue;\n    ++bins[str[0] - 'a'];\n  }\n}",
            "for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n        bins[static_cast<size_t>(str[0]) - 'a']++;\n    }\n}",
            "for (auto const &x : s) {\n        bins[x[0] - 'a']++;\n    }\n}",
            "for (std::string const& str : s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (const std::string &str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for(auto const &i:s) {\n        bins[i[0]-97]++;\n    }\n}",
            "for (auto const& str : s) {\n    if (str.size() > 0) {\n      ++bins[str[0] - 'a'];\n    }\n  }\n}",
            "// Your code here\n    for (auto i = 0; i < s.size(); i++) {\n        if (s[i].size() > 0) {\n            bins[s[i][0] - 'a'] += 1;\n        }\n    }\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (std::string const& str : s) {\n    if (str.size() > 0) {\n      bins[str[0] - 'a'] += 1;\n    }\n  }\n}",
            "// TODO: Fill this in.\n}",
            "for (auto const& item : s) {\n        ++bins[static_cast<size_t>(item[0] - 'a')];\n    }\n}",
            "for (auto const& str : s) {\n    if (!str.empty()) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "for (auto const& str : s) {\n        bins[tolower(str[0]) - 'a']++;\n    }\n}",
            "for (auto const& elem : s) {\n        if (elem.empty()) {\n            continue;\n        }\n\n        ++bins[static_cast<size_t>(elem[0]) - static_cast<size_t>('a')];\n    }\n}",
            "for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n    bins[c - 'a'] = count_if(s.begin(), s.end(), [c](std::string const& s) { return s[0] == c; });\n  }\n}",
            "for (auto const& str: s) {\n    if (str.length() > 0)\n      bins[str[0] - 'a']++;\n  }\n}",
            "for (std::string const& i : s) {\n\t\tif (i.size() > 0)\n\t\t\tbins[i[0] - 'a']++;\n\t}\n}",
            "for (auto const& x : s) {\n        if (x.length() > 0) {\n            bins[x[0] - 'a']++;\n        }\n    }\n}",
            "bins.fill(0);\n\n    for (auto const& str : s) {\n        if (str.empty()) {\n            continue;\n        }\n        ++bins[std::tolower(str.front()) - 'a'];\n    }\n}",
            "for (size_t i = 0; i < 26; ++i) {\n    for (std::string const& word: s) {\n      if (word[0] == 'a' + i) {\n        ++bins[i];\n        break;\n      }\n    }\n  }\n}",
            "for (auto const& string : s) {\n        bins[string[0] - 'a']++;\n    }\n}",
            "// Initialize bins to 0\n  for(size_t &e: bins) e = 0;\n\n  // Count the number of strings with each letter as the first character in the string\n  for(std::string const& str: s) {\n    if (str.length() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "for (auto const& str : s) {\n        bins[static_cast<unsigned char>(str[0])] += 1;\n    }\n}",
            "for (auto const& word : s) {\n    if (word.size() == 0) {\n      bins[0] += 1;\n    } else {\n      bins[word[0] - 'a'] += 1;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "for (auto const& c : s) {\n    if (c!= \"\") {\n      bins[c[0] - 'a']++;\n    }\n  }\n}",
            "// Initialize the count of each letter to zero in the bins array\n  for (auto &bin : bins) {\n    bin = 0;\n  }\n\n  // Increment the count of the letter in the bins array for each string in the vector\n  for (auto const &s : s) {\n    if (not s.empty()) {\n      ++bins[static_cast<unsigned char>(s[0]) - 'a'];\n    }\n  }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        if (islower(s[i][0])) {\n            bins[s[i][0] - 'a']++;\n        }\n    }\n}",
            "// TODO: Your code here\n}",
            "for (auto const& str : s) {\n    ++bins[static_cast<size_t>(str[0]) - 'a'];\n  }\n}",
            "// Your code here.\n}",
            "for(size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        bins[c - 'a'] = 0;\n    }\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& element : s) {\n    ++bins[static_cast<size_t>(element[0]) - 'a'];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (std::string const& str : s) {\n        ++bins[static_cast<size_t>(str[0] - 'a')];\n    }\n}",
            "for (auto const& i : s) {\n        ++bins[i[0] - 'a'];\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n    bins[c - 'a'] = count_if(s.begin(), s.end(), [&](std::string const& str) {\n      return str.find(c) == 0;\n    });\n  }\n}",
            "for (std::string const& str: s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const &str : s) {\n        if (!str.empty()) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (const auto& str : s) {\n    if (str.length()!= 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "for (size_t i = 0; i < s.size(); i++) {\n        size_t count = bins[s[i][0] - 'a'];\n        bins[s[i][0] - 'a'] = count + 1;\n    }\n}",
            "// Your code here\n  for (const std::string &str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (std::string const& s : s) {\n      ++bins[s[0] - 'a'];\n   }\n}",
            "for (auto const& s_i : s) {\n        ++bins[s_i[0] - 'a'];\n    }\n}",
            "for (auto &str : s) {\n      bins[tolower(str.front()) - 'a']++;\n   }\n}",
            "for (auto const& word : s) {\n    auto const firstLetter = static_cast<int>(word[0]);\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      bins[firstLetter - 'a']++;\n    }\n  }\n}",
            "// your code here\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "// TODO:\n}",
            "for (const std::string &str : s) {\n        if (!str.empty()) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n    for (auto const& word : s)\n      if (word[0] == c) ++bins[c - 'a'];\n  }\n}",
            "for (auto& word : s) {\n        if (word.size() > 0) {\n            bins[static_cast<size_t>(word[0]) - 'a']++;\n        }\n    }\n}",
            "// TODO: Fill this in.\n}",
            "for (auto const& str : s) {\n      if (!str.empty()) {\n         bins[str[0] - 'a']++;\n      }\n   }\n}",
            "for (size_t i = 0; i < s.size(); ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "for (auto const& str : s) {\n    // Store the first letter's ASCII code in a variable.\n    auto firstLetter = str.front();\n    // Increase count in the letter's ASCII code in the `bins` array.\n    ++bins[firstLetter - 'a'];\n  }\n}",
            "// TODO: YOUR CODE HERE\n}",
            "for (auto const& st: s) {\n        if (!st.empty()) {\n            ++bins[st[0] - 'a'];\n        }\n    }\n}",
            "for (auto const& str : s) {\n        if (!str.empty()) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n    for (std::string const& word : s) {\n      if (!word.empty() && word[0] == c)\n        ++bins[c - 'a'];\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (auto const& str : s) {\n        ++bins[str[0] - 'a'];\n    }\n}",
            "for (char c = 'a'; c <= 'z'; ++c) {\n        for (std::string const &str : s) {\n            if (str.size() > 0 && c == std::tolower(str.front())) {\n                ++bins[c - 'a'];\n                break;\n            }\n        }\n    }\n}",
            "for(auto const& element : s) {\n    ++bins[tolower(element[0]) - 'a'];\n  }\n}",
            "// TODO: Write your code here\n}",
            "for (auto const& str : s) {\n    bins[static_cast<size_t>(str[0] - 'a')]++;\n  }\n}",
            "/*\n     * Complete this function.\n     */\n    // for (int i = 0; i < 26; i++) {\n    //     bins[i] = 0;\n    // }\n    // for (const auto &str: s) {\n    //     if (str.size() > 0) {\n    //         bins[str[0] - 'a']++;\n    //     }\n    // }\n\n    for (char c = 'a'; c <= 'z'; c++) {\n        bins[c - 'a'] = 0;\n    }\n    for (const auto &str: s) {\n        if (str.size() > 0) {\n            bins[str[0] - 'a']++;\n        }\n    }\n}",
            "for (const std::string& s : s) {\n        if (s.size() > 0) {\n            ++bins[s[0] - 'a'];\n        }\n    }\n}",
            "for (auto const& str : s) {\n    if (!str.empty())\n      ++bins[str[0] - 'a'];\n  }\n}",
            "for (auto const& str: s) {\n        bins[str[0] - 'a']++;\n    }\n}",
            "for (char letter = 'a'; letter <= 'z'; ++letter) {\n        for (std::string const& word : s) {\n            if (word.at(0) == letter) {\n                ++bins.at(static_cast<unsigned>(letter) - 'a');\n                break;\n            }\n        }\n    }\n}",
            "// for each letter in the alphabet, count the number of strings in the vector s that start with that letter\n    for (size_t i = 0; i < bins.size(); ++i) {\n        for (size_t j = 0; j < s.size(); ++j) {\n            if (s[j].size() > i) {\n                if (tolower(s[j][i]) - 'a' == i) {\n                    ++bins[i];\n                    break;\n                }\n            }\n            else {\n                break;\n            }\n        }\n    }\n}",
            "for (char letter = 'a'; letter <= 'z'; ++letter) {\n        size_t count = 0;\n        for (auto &str : s) {\n            if (str.length() > 0 && str[0] == letter)\n                ++count;\n        }\n        bins[letter - 'a'] = count;\n    }\n}",
            "for (auto& letter : s) {\n      if (islower(letter[0])) {\n         bins[letter[0] - 'a']++;\n      }\n   }\n}",
            "for (auto const &str : s) {\n    if (str.length() > 0) {\n      bins[static_cast<size_t>(str[0] - 'a')]++;\n    }\n  }\n}",
            "for(auto const& word: s) {\n        size_t index = word.front() - 'a';\n        bins[index]++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "hip",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    for (size_t i = tid; i < N; i += hipBlockDim_x * hipGridDim_x) {\n        size_t idx = 0;\n        while (s[i][idx]!= '\\0') {\n            bins[s[i][idx] - 'a'] += 1;\n            idx++;\n        }\n    }\n}",
            "const int tid = hipThreadIdx_x;\n    const int gid = hipBlockIdx_x*hipBlockDim_x + tid;\n    const int stride = hipBlockDim_x*hipGridDim_x;\n    int i;\n\n    for (i=gid; i<N; i+=stride) {\n        atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n}",
            "for (size_t i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n  }\n}",
            "__shared__ size_t shBins[26];\n\n\tconst size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\tconst char *p = s[tid];\n\t\tif (*p >= 'a' && *p <= 'z') {\n\t\t\tconst size_t i = *p - 'a';\n\t\t\tatomicAdd(&shBins[i], 1);\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tfor (size_t i = 0; i < 26; i++) {\n\t\tatomicAdd(&bins[i], shBins[i]);\n\t}\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (threadId < N) {\n    const char *p = s[threadId];\n\n    char c = *p;\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n    size_t lid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (lid >= N) {\n        return;\n    }\n\n    // Each thread works on one input string\n    const char *str = s[lid];\n\n    if (str[0] >= 'a' && str[0] <= 'z') {\n        // Note: In HIP, we can have a race condition with the __syncthreads() statement.\n        // In this example, each thread's bin is incremented independently.\n        // We can use a device atomic operation to ensure each thread increments its own bin.\n        atomicAdd(&bins[str[0] - 'a'], 1);\n    }\n}",
            "unsigned tid = threadIdx.x;\n  for (size_t i = blockIdx.x * blockDim.x + tid; i < N; i += blockDim.x * gridDim.x) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "// Initialize a local copy of `bins` on each thread\n  //   (no need to do this if the kernel is launched with N threads)\n  size_t mybins[26];\n  for (int i = 0; i < 26; ++i) mybins[i] = 0;\n  // Compute the number of strings in `s` that start with each letter\n  for (int i = 0; i < N; ++i) ++mybins[s[i][0] - 'a'];\n  // Add the local values of `bins` to the global values\n  for (int i = 0; i < 26; ++i) atomicAdd(&(bins[i]), mybins[i]);\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint numThreads = blockDim.x;\n\n\tchar c = s[bid][tid];\n\n\t__shared__ int counts[26];\n\n\tif (tid < 26) {\n\t\tcounts[tid] = 0;\n\t}\n\n\t__syncthreads();\n\n\tif (tid < N && c >= 'a' && c <= 'z') {\n\t\tatomicAdd(&counts[c - 'a'], 1);\n\t}\n\n\t__syncthreads();\n\n\tif (tid < 26) {\n\t\tatomicAdd(&bins[tid], counts[tid]);\n\t}\n}",
            "// TODO: implement this function\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      // TODO\n   }\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   const char *str = s[bid];\n\n   // Initialize the output bins for this block\n   for(int i=0; i<26; i++) bins[i] = 0;\n\n   // Compute the histogram for this block\n   while(str[tid]!= '\\0') {\n      int c = str[tid] - 'a';\n      atomicAdd(&bins[c], 1);\n      tid += blockDim.x;\n   }\n}",
            "// 1. Each block must compute a distinct output value.\n    // 2. Each block must only be responsible for 1 output value.\n    // 3. Blocks must not share any data.\n    // 4. Blocks must not share any thread.\n    // 5. Only 1 warp is allowed to execute in each block at any one time.\n    // 6. Blocks must not be allowed to syncronise with each other.\n    // 7. Threads must not share any data.\n    // 8. Threads must not share any thread.\n    // 9. Threads must not share any warp.\n    // 10. Only 1 block is allowed to execute in each warp at any one time.\n    // 11. Only 1 warp is allowed to execute in each SM at any one time.\n    // 12. Only 1 SM is allowed to execute on the GPU at any one time.\n    // 13. Blocks must be in a 1D arrangement.\n    // 14. Threads must be in a 1D arrangement.\n    // 15. Warps must be in a 1D arrangement.\n    // 16. Threads in a warp must be in a 1D arrangement.\n    // 17. All blocks must be in the same warp.\n    // 18. All threads in a warp must be in the same block.\n    // 19. All threads in a block must be in the same SM.\n    // 20. All threads in an SM must be in the same CU.\n    // 21. All threads in a CU must be in the same device.\n    // 22. All threads in a device must be in the same multiprocessor.\n\n    // TODO: Your code goes here.\n    // This is where you need to put your CUDA code.\n}",
            "int j = threadIdx.x;\n  int i = blockIdx.x;\n  int c = s[i][j];\n  if (c == '0') {\n    return;\n  }\n  atomicAdd(bins + (c - 'a'), 1);\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N)\n    return;\n\n  // Compute the first letter of string s[idx] and use atomicAdd() to add the count to the bins[letter]\n  // bin.\n  auto letter = tolower(s[idx][0]);\n  atomicAdd(&bins[letter], 1);\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    atomicAdd(&bins[s[tid][0] - 'a'], 1);\n  }\n}",
            "int tid = hipThreadIdx_x;\n  int bid = hipBlockIdx_x;\n  int stride = hipBlockDim_x;\n  int i = bid * stride + tid;\n\n  if (i < N) {\n    unsigned char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n\n   for (size_t i = bid * 256 + tid; i < N; i += 256 * 256) {\n      if (s[i]!= NULL) {\n         int c = s[i][0] - 'a';\n         atomicAdd(&bins[c], 1);\n      }\n   }\n}",
            "// TODO: Your code here.\n}",
            "// TODO: Your code here\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  int idy = threadIdx.y + blockIdx.y * blockDim.y;\n  int idz = threadIdx.z + blockIdx.z * blockDim.z;\n\n  if(idx >= N || idy >= 26 || idz >= 1) return;\n\n  char c = s[idx][0];\n  int c_idx = c - 'a';\n\n  atomicAdd(&bins[c_idx], 1);\n\n  // Print\n  // printf(\"%d, %d, %d, %d\\n\", idx, idy, idz, c_idx);\n}",
            "int tid = threadIdx.x;\n\n  if (tid < 26) {\n    size_t count = 0;\n\n    for (size_t i = 0; i < N; ++i) {\n      if (s[i] && s[i][0] == 'a' + tid)\n        ++count;\n    }\n\n    atomicAdd(&(bins[tid]), count);\n  }\n}",
            "int tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + tid;\n\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "unsigned tid = threadIdx.x;\n  unsigned bid = blockIdx.x;\n  unsigned bidx = bid * blockDim.x + tid;\n\n  // Compute the number of strings that start with the current letter.\n  if (bidx < N) {\n    if (islower(s[bidx][0])) {\n      atomicAdd(&bins[s[bidx][0] - 'a'], 1);\n    }\n  }\n\n  // Ensure the last thread in the block does the atomicAdd\n  // and also ensures a synchronization of all threads in the block\n  if (tid == blockDim.x - 1) {\n    atomicAdd(&bins[s[bidx][0] - 'a'], 1);\n  }\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    const int blockSize = blockDim.x * gridDim.x;\n    for (int i = tid; i < 26; i += blockSize) {\n        size_t sum = 0;\n        for (size_t j = 0; j < N; ++j) {\n            if (s[j][0] == 'a' + i) {\n                sum += 1;\n            }\n        }\n        bins[i] = sum;\n    }\n}",
            "int tid = hipThreadIdx_x;\n  int block = hipBlockIdx_x;\n  int lane = tid & 0x1f;\n  int blockOffset = (block << 7) + (tid >> 5);\n  int blockOffset2 = blockOffset + 128;\n\n  // Load 128 characters at a time.\n  __shared__ char strs[128];\n  if (blockOffset < N) {\n    strs[lane] = s[blockOffset][lane];\n    if (lane == 15) {\n      strs[31] = s[blockOffset2][0];\n    }\n  }\n\n  __syncthreads();\n\n  // Each thread looks at 1 character\n  if (blockOffset < N) {\n    int c = strs[lane] - 'a';\n    if (c >= 0 && c < 26) {\n      atomicAdd(&bins[c], 1);\n    }\n  }\n}",
            "__shared__ size_t s_bins[26];\n   int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   if (bid == 0) {\n      s_bins[tid] = 0;\n   }\n   __syncthreads();\n\n   int i = tid;\n   while (i < N) {\n      char c = s[i][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&s_bins[c - 'a'], 1);\n      }\n      i += blockDim.x;\n   }\n   __syncthreads();\n\n   if (bid == 0) {\n      for (int i = 0; i < 26; i++) {\n         atomicAdd(&bins[i], s_bins[i]);\n      }\n   }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    // get the first letter\n    char ch = s[tid][0];\n    // increment the count for that letter\n    atomicAdd(&(bins[ch - 'a']), 1);\n  }\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x * blockDim.x + i;\n    size_t idx = 0;\n\n    for (; j < N; j += blockDim.x * gridDim.x) {\n        if (s[j]!= NULL) {\n            idx = (int)s[j][0] - 97;\n            atomicAdd(&bins[idx], 1);\n        }\n    }\n}",
            "// Compute each block-wide sum of the lengths of the strings starting with each letter.\n   __shared__ size_t blockSums[26];\n\n   // Each thread computes the sum of lengths starting with one letter.\n   unsigned tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n      char c = tolower(s[tid][0]);\n      atomicAdd(&blockSums[c - 'a'], strlen(s[tid]));\n   }\n\n   // Wait for all threads to finish their work.\n   __syncthreads();\n\n   // Compute block-wide sums of the letters starting with each letter.\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 26; ++i) {\n         atomicAdd(&bins[i], blockSums[i]);\n      }\n   }\n}",
            "int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + tid;\n  int c;\n  while (i < N) {\n    c = (unsigned char)s[i][0];\n    atomicAdd(&bins[c], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "// The number of threads in a block is a compile-time constant and is stored in __CUDA_ARCH__\n   int tid = threadIdx.x;\n   size_t nthreads = blockDim.x;\n   // The thread block is divided into segments of size nthreads\n   // Calculate the segment that this thread belongs to\n   int blockId = blockIdx.x;\n   int blockOffset = blockId * nthreads;\n   int i = blockOffset + tid;\n   // Loop until all segments are complete\n   while (i < N) {\n      int c = s[i][0];\n      atomicAdd(&bins[c - 'a'], 1);\n      i += nthreads;\n   }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = 0;\n    while (i < N) {\n        j = s[i][0];\n        atomicAdd(&bins[j - 'a'], 1);\n        i += blockDim.x * gridDim.x;\n    }\n}",
            "// Get global thread ID\n   int tid = threadIdx.x;\n   // Compute global thread ID\n   int global_tid = blockIdx.x * blockDim.x + threadIdx.x;\n   // Each block processes N/BLOCK_SIZE strings\n   int stride = blockDim.x * gridDim.x;\n   // Each block processes BLOCK_SIZE strings\n   // Each block processes at most 26 characters\n   // Each block processes at most 26*BLOCK_SIZE characters\n   // Each block processes at most N/26 characters\n   // Each block processes at most N/BLOCK_SIZE/26 characters\n   // Each block processes at most N/BLOCK_SIZE/26*BLOCK_SIZE characters\n   // Each block processes at most N/BLOCK_SIZE/26*BLOCK_SIZE*26 characters\n   // Each block processes at most N/BLOCK_SIZE/26*BLOCK_SIZE*26*BLOCK_SIZE characters\n   // Each block processes at most N/BLOCK_SIZE/26*BLOCK_SIZE*26*BLOCK_SIZE*26 characters\n   // Each block processes at most N/BLOCK_SIZE/26*BLOCK_SIZE*26*BLOCK_SIZE*26*BLOCK_SIZE characters\n   for (size_t i = global_tid; i < N; i += stride) {\n      // Get the string\n      const char *str = s[i];\n      // Loop over all characters in the string\n      for (size_t j = 0; j < strlen(str); j++) {\n         // Get the character\n         char c = str[j];\n         // Increment the count in bins[c - 'a']\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "}",
            "// For each string\n  for (size_t i = 0; i < N; i++) {\n    const char *str = s[i];\n    // For each letter in the alphabet\n    for (char c = 'a'; c <= 'z'; c++) {\n      // If the character is in the string\n      if (strchr(str, c)!= NULL) {\n        // Increment the count\n        atomicAdd(bins + (c - 'a'), 1);\n      }\n    }\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = idx; i < N; i += stride) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z')\n      atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "int tid = hipThreadIdx_x;\n  __shared__ size_t s_bins[26];\n  for (size_t i = tid; i < 26; i += hipBlockDim_x) {\n    s_bins[i] = 0;\n  }\n  __syncthreads();\n  for (size_t i = tid; i < N; i += hipBlockDim_x) {\n    s_bins[s[i][0] - 'a']++;\n  }\n  __syncthreads();\n  for (size_t i = tid; i < 26; i += hipBlockDim_x) {\n    atomicAdd(&bins[i], s_bins[i]);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\t//printf(\"tid = %d\\n\", tid);\n\tif (tid < N) {\n\t\tchar c = s[tid][0];\n\t\tif (c >= 'a' && c <= 'z') {\n\t\t\tatomicAdd(&(bins[c - 'a']), 1);\n\t\t}\n\t}\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < 26) {\n    for (size_t i = 0; i < N; i++) {\n      if (s[i]!= 0) {\n        size_t c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n          if (c == 'a' + tid)\n            atomicAdd(&bins[tid], 1);\n        }\n      }\n    }\n  }\n}",
            "// Each block handles a character from the alphabet\n    int letter = hipThreadIdx_x;\n    size_t sum = 0;\n    // Each thread in the block checks if any string starts with the character\n    for (int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x; i < N; i += hipBlockDim_x * hipGridDim_x) {\n        if (s[i][0] == letter + 'a') {\n            sum++;\n        }\n    }\n    __syncthreads();\n    // The block updates the counter\n    atomicAdd(&bins[letter], sum);\n}",
            "__shared__ size_t localBins[26];\n  if(threadIdx.x < 26) {\n    localBins[threadIdx.x] = 0;\n  }\n\n  __syncthreads();\n\n  // TODO: Fill in this function.\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  while (tid < N) {\n    for (int i = 0; i < 26; i++) {\n      if (s[tid][0] == 'a' + i) {\n        atomicAdd(&localBins[i], 1);\n      }\n    }\n    tid += blockDim.x * gridDim.x;\n  }\n\n  __syncthreads();\n\n  if(threadIdx.x < 26) {\n    atomicAdd(&bins[threadIdx.x], localBins[threadIdx.x]);\n  }\n}",
            "int tid = threadIdx.x;\n  size_t bin = tid - 'a';\n  for (size_t i = blockIdx.x * blockDim.x + tid; i < N; i += blockDim.x * gridDim.x) {\n    atomicAdd(&bins[bin], (size_t)(s[i][0] == (char)tid));\n  }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int stride = blockDim.x;\n    int nblocks = N / stride;\n    if (bid >= nblocks) return;\n    int start = bid * stride;\n    for (int i = tid + start; i < N; i += stride) {\n        char c = s[i][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "int threadId = threadIdx.x + blockDim.x * blockIdx.x;\n  int tid = threadIdx.x;\n  __shared__ int counts[26];\n  if (tid < 26)\n    counts[tid] = 0;\n  __syncthreads();\n  if (threadId < N) {\n    int i = 0;\n    while (s[threadId][i]!= '\\0') {\n      ++counts[s[threadId][i] - 'a'];\n      ++i;\n    }\n  }\n  __syncthreads();\n  if (tid < 26)\n    atomicAdd(&bins[tid], counts[tid]);\n}",
            "size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  size_t gid = bid * blockDim.x + tid;\n  if (gid < N) {\n    bins[s[gid][0] - 'a'] += 1;\n  }\n}",
            "// TODO: Your code goes here\n}",
            "// Your kernel code here\n  // Each block will process 32 elements of s\n  const int block_size = 32;\n  const int i = threadIdx.x;\n  int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  int mybins[26] = {0};\n\n  while (tid < N) {\n    mybins[s[tid][0] - 'a']++;\n    tid += blockDim.x * gridDim.x;\n  }\n\n  // Sum up the number of elements in the current block\n  int sum = 0;\n  for (int i = 0; i < 26; i++) {\n    sum += mybins[i];\n  }\n\n  // Each thread updates one entry in the global array\n  int offset = blockIdx.x * block_size;\n  atomicAdd(&bins[offset + i], sum);\n}",
            "int tid = hipThreadIdx_x;\n  int bid = hipBlockIdx_x;\n  int bsz = hipBlockDim_x;\n  int tidInBlock = tid % 26;\n  int bidInBlock = tid / 26;\n  int block = bidInBlock * bsz;\n\n  int global_index = block + bid * bsz + tid;\n  int local_index = tidInBlock;\n  int strLen = 0;\n  int strChar = 0;\n\n  if (global_index < N) {\n    strLen = strlen(s[global_index]);\n    strChar = s[global_index][0];\n  }\n\n  __shared__ int shared_bins[26];\n  __syncthreads();\n\n  // Each thread counts the number of strings that start with its letter.\n  // `shared_bins` array is initialized to zero before each threadblock is run.\n  if (global_index < N) {\n    atomicAdd(&shared_bins[strChar - 'a'], 1);\n  }\n\n  // Wait for all threads to complete\n  __syncthreads();\n\n  // Each thread adds up the result from each threadblock\n  if (local_index < 26) {\n    atomicAdd(&bins[local_index], shared_bins[local_index]);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t nthreads = hipBlockDim_x * hipGridDim_x;\n\n    for (size_t i = tid; i < N; i += nthreads) {\n        size_t bin = s[i][0] - 'a';\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t i = hipThreadIdx_x;\n  if (i < N) {\n    const char *str = s[i];\n    if (str[0] == 'c') {\n      atomicAdd(&bins[0], 1);\n    } else if (str[0] == 'd') {\n      atomicAdd(&bins[1], 1);\n    } else if (str[0] == 'x') {\n      atomicAdd(&bins[2], 1);\n    } else if (str[0] == 'c') {\n      atomicAdd(&bins[3], 1);\n    } else if (str[0] == 't') {\n      atomicAdd(&bins[4], 1);\n    } else if (str[0] == 'f') {\n      atomicAdd(&bins[5], 1);\n    } else {\n      atomicAdd(&bins[25], 1);\n    }\n  }\n}",
            "// TODO: Your code here\n    const int idx = hipThreadIdx_x;\n    const char *p = s[idx];\n    while (p!= NULL) {\n        atomicAdd(bins + *p - 'a', 1);\n        p++;\n    }\n}",
            "size_t tid = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  int i;\n  int j;\n  int k;\n  int bin;\n  // Your code goes here\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid >= N) {\n    return;\n  }\n  size_t count = 0;\n  if (s[tid][0] >= 'a' && s[tid][0] <= 'z') {\n    count = 1;\n  }\n  atomicAdd(&bins[s[tid][0] - 'a'], count);\n}",
            "int tid = threadIdx.x;\n    // TODO\n}",
            "// HIP has only one thread block, with one thread per warp\n  // HIP threads are numbered 0..warpSize-1, with warpSize == 32 for the AMD GPU\n  // Warp-level parallelism is used to increase performance on GPU\n  size_t laneId = threadIdx.x & (warpSize - 1); // get the thread id within the current warp\n  size_t warpId = threadIdx.x / warpSize;      // get the number of the current warp\n\n  // Each warp starts by computing its part of the shared array bins\n  // It then checks to see if any other warp has the same bins\n  // and if so, it halts\n\n  // Each warp is assigned a sequential thread number,\n  // starting at 0 within the warp.\n  size_t localThreadId = laneId + warpId * warpSize;\n\n  // Each warp keeps track of whether it has found a duplicate.\n  // The initial value depends on whether this is the first warp.\n  bool duplicate = warpId!= 0;\n\n  // Each warp keeps track of whether it has found a matching letter in s[i]\n  // for some i.\n  bool match = false;\n\n  // Each warp keeps track of whether it has found a letter that is not in the alphabet.\n  // The initial value depends on whether this is the first warp.\n  bool outOfBounds = warpId!= 0;\n\n  // Each warp keeps track of whether it has found a letter that is not lower case.\n  // The initial value depends on whether this is the first warp.\n  bool notLowerCase = warpId!= 0;\n\n  // Each thread within a warp loops through the entire array, looking for a match.\n  // It does so sequentially. The threads that find a match are responsible\n  // for adding to the bins.\n\n  // When the loop ends, each warp adds its part of the bins to the total.\n  // If no duplicate was found, the thread with the lowest localThreadId adds\n  // its part to the total. If a duplicate was found, all threads add their\n  // part to the total.\n\n  // In order to avoid thread divergence, each warp loops through the entire array\n  // and looks for a match. When a match is found, all threads in the warp\n  // add their part of the bins to the total.\n\n  for (size_t i = localThreadId; i < N; i += blockDim.x) {\n\n    // Each thread in the warp looks for a match.\n\n    // Check for a match of the first character in s[i]\n    // to the first character of the alphabet (lower case).\n    // If so, set match to true.\n\n    // Each warp starts with match = false, but threads that find a match\n    // set match = true.\n    // When all threads have finished the loop, each warp will have\n    // a value of match that is true for exactly one thread.\n\n    // The first character is the one in the lower case alphabet\n    // corresponding to this thread's localThreadId within the warp.\n\n    char c = s[i][0] - 'a'; // convert s[i] to lower case\n    if (c < 0 || c > 25) {\n      outOfBounds = true;\n      continue;\n    }\n    if (s[i][0]!= (char)(c + 'a')) {\n      notLowerCase = true;\n      continue;\n    }\n\n    match = true;\n  }\n\n  // Synchronize all threads in this warp\n  __syncthreads();\n\n  // Each warp adds its part of the bins to the total\n  // when all threads have finished the loop.\n  // The first warp to finish the loop adds its part\n  // to the total.\n\n  // Each warp adds to the bins array if and only if no duplicate was found.\n  // Duplicate is true if at least one thread in the warp set match = true.\n  if (!duplicate) {\n    // Each warp adds to the bins array only if no duplicate was found.\n    // For the first warp, the following condition is true.\n    // For other warps, the following condition is false.\n    // Each warp adds to the bins array if and only if no duplicate was found.\n    // Each warp adds to the bins array only if it did not find a match.\n    // For the first warp, the following condition is true.\n    // For other warps, the following condition is false.\n    if (!match) {\n      // Each warp adds to the bins array if and only if it did not find a match.\n      // For the first warp, the following condition is true.\n      // For other warps, the following condition is false.\n      // Each warp adds to the bins array only if it did not find a match.\n      // The value of localThreadId is the same for all threads in the",
            "const int t = hipThreadIdx_x;\n  const int blockId = hipBlockIdx_x;\n  const int stride = hipBlockDim_x;\n\n  // Load a set of strings into shared memory\n  __shared__ char s_shared[128][128];\n  for (int i = t; i < N; i += stride)\n    s_shared[blockId][i] = s[blockId][i];\n\n  // Compute prefix sums in parallel\n  __shared__ int s_prefix_shared[26];\n  s_prefix_shared[t] = s_shared[blockId][t] ==''? 0 : 1;\n  for (int i = stride; i < 26; i += stride)\n    s_prefix_shared[i] = (s_shared[blockId][t + i] ==''? 0 : 1) + s_prefix_shared[i - stride];\n\n  __syncthreads();\n  for (int i = stride; i < 26; i += stride)\n    s_prefix_shared[i] += s_prefix_shared[i - stride];\n\n  // Write the results\n  if (t < 26)\n    atomicAdd(&bins[t], s_prefix_shared[t]);\n}",
            "#ifndef HIP_PLATFORM\n  // If not running on AMD HIP, run on a CPU:\n  // Compute the first letter of each string, and store the counts in bins array.\n  for (size_t i = 0; i < N; i++) {\n    bins[s[i][0] - 'a']++;\n  }\n#else\n  // If running on AMD HIP, run on a GPU:\n  // AMD HIP uses the __shfl_xor instruction to implement shfl_xor, which can be used\n  // to implement the reduction.\n  // The first letter of each string is found using the modulo operation, so it can\n  // be computed in parallel.\n  // The output of this function is the bin counts for each of the 26 letters of the alphabet.\n  // Each thread in the warp computes the bin count for a single letter. The warp then performs\n  // a reduction operation to compute the total bin count for each of the 26 letters.\n  // The output of the reduction is the bin count for each of the 26 letters.\n  // The code below will compute the total bin count for each of the 26 letters.\n  // The total bin count for each of the 26 letters is stored in the \"local\" variable total_bin_count.\n  // Then, the total_bin_count value for each of the 26 letters is copied to the global memory.\n  // The global memory stores the total bin counts for each of the 26 letters.\n\n  int lane_id = threadIdx.x % warpSize;\n  int warp_id = threadIdx.x / warpSize;\n  int total_bin_count = 0;\n\n  // Each warp performs the computation for one of the letters in the alphabet.\n  for (int i = 0; i < 26; i++) {\n    // Compute the bin count for one of the letters in the alphabet.\n    // For the first letter of each string, the letter is found by taking the modulo of each string length.\n    // The modulo operation is implemented by the __activemask_f() function, so it can be\n    // computed in parallel.\n    // Compute the bin count by adding the bin counts for each string starting with that letter.\n    // __shfl_xor() implements the shfl_xor operation.\n    total_bin_count += __popc(__activemask_f());\n    // Synchronize the thread after performing __shfl_xor() operation\n    __syncthreads();\n  }\n\n  // Copy the total bin count for each of the 26 letters to the global memory.\n  if (lane_id == 0) {\n    bins[warp_id] = total_bin_count;\n  }\n#endif\n}",
            "int tid = threadIdx.x;\n  int blockIdx = blockIdx.x;\n  int blockDim = blockDim.x;\n  int numBlocks = gridDim.x;\n  int blockOffset = numBlocks * blockDim * blockIdx;\n\n  // Counts to return to the host\n  int myCounts[26];\n  for (int i = 0; i < 26; i++) {\n    myCounts[i] = 0;\n  }\n\n  // For each string in the vector\n  for (int i = blockOffset + tid; i < N; i += blockDim * numBlocks) {\n    // Find the first letter in the string\n    char first = s[i][0];\n\n    // Only interested in letters a to z\n    if ((first >= 'a') && (first <= 'z')) {\n      // Increment the count of the letter\n      myCounts[first - 'a']++;\n    }\n  }\n\n  // Reduce counts within the block\n  for (int i = tid; i < 26; i += blockDim) {\n    atomicAdd(&bins[i], myCounts[i]);\n  }\n}",
            "size_t idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (idx >= N) return;\n\n  char c = s[idx][0];\n  if (c >= 'a' && c <= 'z')\n    atomicAdd(&bins[c - 'a'], 1);\n}",
            "// TODO: implement this\n    const int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    // for (int tid = 0; tid < N; tid++) {\n    //     char c = s[tid][0];\n    //     atomicAdd(&bins[c - 'a'], 1);\n    // }\n\n    __syncthreads();\n    size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    __shared__ int temp[26];\n    temp[hipThreadIdx_x] = 0;\n    __syncthreads();\n    if (tid < N) {\n        char c = s[tid][0];\n        atomicAdd(&temp[c - 'a'], 1);\n    }\n    __syncthreads();\n    if (hipThreadIdx_x < 26) {\n        atomicAdd(&bins[hipThreadIdx_x], temp[hipThreadIdx_x]);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  int count = 0;\n\n  if (tid < N) {\n    int firstLetter = tolower(s[tid][0]);\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n  }\n}",
            "__shared__ int count[26];\n  __shared__ size_t len[26];\n  int tid = threadIdx.x;\n  int lid = threadIdx.y;\n\n  // initialize shared memory\n  for (int i = lid; i < 26; i += 2) {\n    len[i] = 0;\n    count[i] = 0;\n  }\n  __syncthreads();\n\n  for (size_t i = blockIdx.x * blockDim.y + threadIdx.y; i < N; i += gridDim.x * blockDim.y) {\n    size_t len_i = strlen(s[i]) + 1;\n    for (size_t j = 0; j < len_i; j++) {\n      int c = s[i][j] - 'a';\n      if (j == 0) {\n        atomicAdd(&len[c], len_i);\n      }\n      atomicAdd(&count[c], 1);\n    }\n  }\n  __syncthreads();\n\n  // add up the counts\n  for (int i = lid; i < 26; i += 2) {\n    atomicAdd(&bins[i], count[i]);\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "__shared__ size_t tmp[26];\n   tmp[threadIdx.x] = 0;\n   size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n   while (i < N) {\n      const char *p = s[i];\n      while (*p) {\n         tmp[(*p) - 'a']++;\n         p++;\n      }\n      i += blockDim.x * gridDim.x;\n   }\n   __syncthreads();\n   if (threadIdx.x < 26) {\n      atomicAdd(&bins[threadIdx.x], tmp[threadIdx.x]);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "// Each thread is responsible for updating one element of bins\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  if (bid*blockDim.x+tid < 26) bins[bid*blockDim.x+tid] = 0;\n\n  __syncthreads();\n\n  // Each thread is responsible for updating one string in s\n  for (int i = bid*blockDim.x+tid; i < N; i += blockDim.x*gridDim.x) {\n    char c = s[i][0];\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  size_t bid0 = blockIdx.y;\n  size_t bid1 = blockIdx.z;\n  size_t num_blocks_x = gridDim.x;\n  size_t num_blocks_y = gridDim.y;\n  size_t num_blocks_z = gridDim.z;\n\n  __shared__ char strs[100][128];\n\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    size_t j = bid0 + bid1 * num_blocks_x + bid * num_blocks_x * num_blocks_y;\n    strs[j][i] = s[bid][i];\n  }\n  __syncthreads();\n\n  size_t local_bins[26] = {0};\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    size_t c = strs[bid][i];\n    if (c >= 'a' && c <= 'z') {\n      local_bins[c - 'a']++;\n    }\n  }\n  __syncthreads();\n\n  if (tid == 0) {\n    for (size_t i = 0; i < 26; i++) {\n      atomicAdd(&bins[i], local_bins[i]);\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    for (int i = tid; i < N; i += blockDim.x * gridDim.x) {\n        int c = s[i][0] - 'a';\n        atomicAdd(&bins[c], 1);\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t bin = tid - 'a';\n\n    // Iterate over all strings\n    for (size_t i = hipBlockIdx_x * hipBlockDim_x + tid; i < N; i += hipGridDim_x * hipBlockDim_x) {\n        // If the first letter matches, increase the count\n        if (s[i][0] == tid)\n            atomicAdd(bins + bin, 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n  size_t block_id = blockIdx.x;\n  size_t block_size = blockDim.x;\n\n  for(size_t i = block_id * block_size + tid; i < N; i += block_size * gridDim.x) {\n    size_t index = s[i][0] - 'a';\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "int id = threadIdx.x;\n\tif (id >= 26)\n\t\treturn;\n\tfor (size_t i = 0; i < N; i++) {\n\t\tif (s[i]!= NULL)\n\t\t\tbins[id] += s[i][0] == 'a' + id;\n\t}\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t bid = hipBlockIdx_x;\n  if (tid < 26) {\n    for (size_t i = bid; i < N; i += hipGridDim_x)\n      if (s[i][0] == tid + 'a')\n        ++bins[tid];\n  }\n}",
            "int tid = threadIdx.x;\n    int blkid = blockIdx.x;\n    int blksize = blockDim.x;\n    int i;\n\n    int firstLetter = (int)s[blkid][tid];\n\n    for (i = tid; i < N; i += blksize) {\n        int letter = (int)s[blkid][i];\n        if (firstLetter == letter) {\n            atomicAdd(&bins[letter], 1);\n        }\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (tid < 26) {\n    for (size_t i = 0; i < N; ++i) {\n      if (s[i][0] == tid + 'a') {\n        atomicAdd(&bins[tid], 1);\n      }\n    }\n  }\n}",
            "int t = hipThreadIdx_x;\n  int i = hipBlockIdx_x * hipBlockDim_x + t;\n\n  int2 counts = make_int2(0, 0);\n  if (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z')\n      counts.x++;\n    else if (c >= 'A' && c <= 'Z')\n      counts.y++;\n  }\n\n  int2* smem = (int2*)hipSynchronousDynamicShared(2 * sizeof(int2));\n  smem[t] = counts;\n  __syncthreads();\n\n  if (hipBlockDim_x <= 64) {\n    if (t < 32) {\n      smem[t] = counts += smem[t + 32];\n    }\n  }\n  __syncthreads();\n\n  if (hipBlockDim_x <= 32) {\n    if (t < 16) {\n      smem[t] = counts += smem[t + 16];\n    }\n  }\n  __syncthreads();\n\n  if (hipBlockDim_x <= 16) {\n    if (t < 8) {\n      smem[t] = counts += smem[t + 8];\n    }\n  }\n  __syncthreads();\n\n  if (hipBlockDim_x <= 8) {\n    if (t < 4) {\n      smem[t] = counts += smem[t + 4];\n    }\n  }\n  __syncthreads();\n\n  if (hipBlockDim_x <= 4) {\n    if (t < 2) {\n      smem[t] = counts += smem[t + 2];\n    }\n  }\n  __syncthreads();\n\n  if (hipBlockDim_x <= 2) {\n    if (t < 1) {\n      smem[t] = counts += smem[t + 1];\n    }\n  }\n  __syncthreads();\n\n  if (t == 0)\n    bins[s[i][0] - 'a'] = smem[0].x + smem[0].y;\n}",
            "// YOUR CODE HERE\n   __shared__ int localBins[26];\n   int laneId = threadIdx.x % warpSize;\n   int warpId = threadIdx.x / warpSize;\n   int start = (blockIdx.x * blockDim.x + threadIdx.x) * 26;\n   int stride = blockDim.x * gridDim.x * 26;\n   int i = 0;\n   for (i = start; i < N; i += stride) {\n      localBins[laneId] += (s[i][0] >= 'a' && s[i][0] <= 'z');\n   }\n   for (int i = 16; i > 0; i /= 2) {\n      localBins[laneId] += __shfl_xor_sync(0xFFFFFFFF, localBins[laneId], i, 32);\n   }\n   if (laneId == 0) {\n      atomicAdd(&bins[0], localBins[0]);\n      atomicAdd(&bins[1], localBins[1]);\n      atomicAdd(&bins[2], localBins[2]);\n      atomicAdd(&bins[3], localBins[3]);\n      atomicAdd(&bins[4], localBins[4]);\n      atomicAdd(&bins[5], localBins[5]);\n      atomicAdd(&bins[6], localBins[6]);\n      atomicAdd(&bins[7], localBins[7]);\n      atomicAdd(&bins[8], localBins[8]);\n      atomicAdd(&bins[9], localBins[9]);\n      atomicAdd(&bins[10], localBins[10]);\n      atomicAdd(&bins[11], localBins[11]);\n      atomicAdd(&bins[12], localBins[12]);\n      atomicAdd(&bins[13], localBins[13]);\n      atomicAdd(&bins[14], localBins[14]);\n      atomicAdd(&bins[15], localBins[15]);\n      atomicAdd(&bins[16], localBins[16]);\n      atomicAdd(&bins[17], localBins[17]);\n      atomicAdd(&bins[18], localBins[18]);\n      atomicAdd(&bins[19], localBins[19]);\n      atomicAdd(&bins[20], localBins[20]);\n      atomicAdd(&bins[21], localBins[21]);\n      atomicAdd(&bins[22], localBins[22]);\n      atomicAdd(&bins[23], localBins[23]);\n      atomicAdd(&bins[24], localBins[24]);\n      atomicAdd(&bins[25], localBins[25]);\n   }\n}",
            "int tid = threadIdx.x;\n  int gid = threadIdx.x + blockIdx.x * blockDim.x;\n  int bid = blockIdx.x;\n  __shared__ size_t count[26];\n\n  for (int i = bid; i < 26; i += gridDim.x) {\n    count[i] = 0;\n  }\n  __syncthreads();\n\n  while (gid < N) {\n    char ch = s[gid][0];\n    if (ch < 'a') {\n      gid += blockDim.x * gridDim.x;\n      continue;\n    }\n    if (ch > 'z') {\n      gid += blockDim.x * gridDim.x;\n      continue;\n    }\n    ch = ch - 'a';\n    atomicAdd(&count[ch], 1);\n    gid += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n\n  for (int i = bid; i < 26; i += gridDim.x) {\n    atomicAdd(&bins[i], count[i]);\n  }\n}",
            "// Each thread is assigned a character to count\n  char c = 'a';\n  if (threadIdx.x == 0)\n    c = 'a';\n\n  // Each thread reads in its assigned character and then counts the number of strings that start with it\n  size_t count = 0;\n  for (size_t i = 0; i < N; i++) {\n    if (s[i][0] == c) {\n      count++;\n    }\n  }\n\n  // Each thread will increment the count for the assigned character\n  if (threadIdx.x == 0)\n    atomicAdd(&bins[c - 'a'], count);\n}",
            "size_t tid = threadIdx.x;\n  __shared__ size_t localbins[26];\n\n  // set localbins to 0\n  for (int i = tid; i < 26; i += blockDim.x) {\n    localbins[i] = 0;\n  }\n\n  // each thread process one string\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  while (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&localbins[c - 'a'], 1);\n    }\n    i += blockDim.x * gridDim.x;\n  }\n\n  // combine results from all threads\n  for (int i = tid; i < 26; i += blockDim.x) {\n    atomicAdd(&bins[i], localbins[i]);\n  }\n}",
            "// TODO: add your code here\n}",
            "__shared__ unsigned int smem[27];\n\n    int tid = threadIdx.x;\n    int id = blockIdx.x * blockDim.x + tid;\n    unsigned int c;\n\n    while (id < N) {\n        c = s[id][0];\n        if ((c >= 'a') && (c <= 'z'))\n            atomicAdd(&bins[c - 'a'], 1);\n        id += blockDim.x * gridDim.x;\n    }\n}",
            "#define BLOCK_SIZE 512\n  int blockId = blockIdx.x;\n  int threadId = threadIdx.x;\n  int globalThreadId = blockId * BLOCK_SIZE + threadId;\n\n  __shared__ int count[BLOCK_SIZE];\n  int local_bins[26];\n  for (int i = 0; i < 26; i++) {\n    local_bins[i] = 0;\n  }\n\n  while (globalThreadId < N) {\n    // get the first letter\n    int first_letter = (int)s[globalThreadId][0] - (int)'a';\n    // check if we're in range\n    if (first_letter >= 0 && first_letter < 26) {\n      // atomic add the count of that letter\n      atomicAdd(&(local_bins[first_letter]), 1);\n    }\n    globalThreadId += BLOCK_SIZE;\n  }\n  // do a reduction in shared memory, one thread per bin\n  for (int i = threadId; i < 26; i += BLOCK_SIZE) {\n    atomicAdd(&(count[i]), local_bins[i]);\n  }\n  __syncthreads();\n\n  // now, each block has a count of letters in [0, 25], sum them up\n  if (threadId == 0) {\n    for (int i = 0; i < 26; i++) {\n      atomicAdd(&(bins[i]), count[i]);\n    }\n  }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      char firstLetter = tolower(s[i][0]);\n      if ((firstLetter >= 'a') && (firstLetter <= 'z')) {\n         atomicAdd(&(bins[firstLetter - 'a']), 1);\n      }\n   }\n}",
            "#ifdef __HIP_PLATFORM_HCC__\n    constexpr int block = 256;\n    __shared__ size_t s_bins[26];\n    for (int i = 0; i < 26; i++)\n        s_bins[i] = 0;\n\n    for (size_t i = threadIdx.x + block * blockIdx.x; i < N; i += block * blockDim.x) {\n        if (s[i] == nullptr)\n            continue;\n        if (islower(s[i][0]))\n            s_bins[s[i][0] - 'a']++;\n    }\n    __syncthreads();\n    for (int i = 0; i < 26; i++)\n        atomicAdd(&bins[i], s_bins[i]);\n#else\n    for (size_t i = threadIdx.x + blockDim.x * blockIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (s[i] == nullptr)\n            continue;\n        if (islower(s[i][0]))\n            atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n#endif\n}",
            "int tid = hipThreadIdx_x;\n  int bid = hipBlockIdx_x;\n  int gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  if (gid < N) {\n    size_t i = 0;\n    while (s[gid][i]!= '\\0') {\n      if ((s[gid][i] >= 'a') && (s[gid][i] <= 'z')) {\n        atomicAdd(&bins[s[gid][i] - 'a'], 1);\n      }\n      i += 1;\n    }\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid >= N) return;\n\n    const char *str = s[tid];\n    int idx = (int)str[0] - 97;\n    atomicAdd(&bins[idx], 1);\n}",
            "const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  // TODO: 1. write a kernel that counts the number of strings in s that start with each letter\n  // The threads should process 26 letters in parallel, one letter per thread\n  // (You can use shared memory for each thread to count strings that start with that letter).\n  // Each thread should write one output element.\n  // The output should be an array of 26 elements.\n  // For example, if there are 3 strings that start with letter 'd', the first thread should\n  // write 3 to element [0] of the output array.\n\n  // TODO: 2. Modify the kernel so that it outputs the final result at the end.\n  // Launch the kernel using a grid that has (N + 25) / 26 blocks and 26 threads per block.\n  // You can use the following code to print the output array:\n  // printf(\"First letter counts: %s\\n\", bins);\n}",
            "size_t t = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t i = 0;\n  while (t < N) {\n    i = (size_t)s[t][0];\n    if (i >= 'a' && i <= 'z')\n      atomicAdd(&bins[i - 'a'], 1);\n    t += blockDim.x * gridDim.x;\n  }\n}",
            "__shared__ size_t s_bins[26];\n\n    int tid = threadIdx.x;\n    for (int i = tid; i < 26; i += blockDim.x) {\n        s_bins[i] = 0;\n    }\n    __syncthreads();\n\n    int i = blockIdx.x * blockDim.x + tid;\n    if (i < N) {\n        int c = s[i][0] - 'a';\n        if (c >= 0 && c < 26) {\n            s_bins[c]++;\n        }\n    }\n    __syncthreads();\n\n    for (int i = tid; i < 26; i += blockDim.x) {\n        bins[i] += s_bins[i];\n    }\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x;\n    __shared__ size_t smem[26];\n    smem[i] = 0;\n    for (; j < N; j += gridDim.x) {\n        if (s[j][0] == (i + 'a'))\n            smem[i]++;\n    }\n    __syncthreads();\n    if (i < 26)\n        atomicAdd(bins + i, smem[i]);\n}",
            "int tid = threadIdx.x;\n\tbins[tid] = 0;\n\n\t// Parallel loop over all strings.\n\tfor (size_t i = blockIdx.x * blockDim.x + tid; i < N; i += blockDim.x * gridDim.x) {\n\t\t// Increment the count for the first letter in the string.\n\t\tif (s[i] && *s[i] >= 'a' && *s[i] <= 'z')\n\t\t\t++bins[*s[i] - 'a'];\n\t}\n}",
            "size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x;\n  if (gid < N) {\n    size_t c = s[gid][0];\n    atomicAdd(bins + c - 'a', 1);\n  }\n}",
            "// threadIdx is a constant, so it can be used in the if condition.\n  if (threadIdx.x < 26) {\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      // get the first character\n      char c = s[i][0];\n\n      // only increment if the char is a letter\n      if (c >= 'a' && c <= 'z') {\n        atomicAdd(&(bins[c - 'a']), 1);\n      }\n    }\n  }\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (i < N) {\n    bins[tolower(s[i][0]) - 'a']++;\n  }\n}",
            "size_t bin = (size_t)threadIdx.x;\n  for (size_t i = 0; i < N; i++) {\n    if (s[i][0] == bin + 'a')\n      bins[bin]++;\n  }\n}",
            "const char *start = s[blockIdx.x];\n  int tid = threadIdx.x;\n  int local_bins[26];\n  for (int i = 0; i < 26; i++)\n    local_bins[i] = 0;\n  for (int i = tid; i < N; i += blockDim.x) {\n    int letter = start[i] - 'a';\n    if (letter >= 0 && letter < 26)\n      local_bins[letter]++;\n  }\n  __syncthreads();\n  for (int i = 1; i < 26; i *= 2) {\n    int value = local_bins[i];\n    int step = 1;\n    if ((i & 1) && tid + i < 26) {\n      value += local_bins[tid + i];\n      step *= 2;\n    }\n    __syncthreads();\n    if (tid % step == 0)\n      local_bins[tid / step] += value;\n    __syncthreads();\n  }\n  if (tid == 0) {\n    int value = local_bins[0];\n    for (int i = 1; i < 26; i++)\n      value += local_bins[i];\n    bins[blockIdx.x] = value;\n  }\n}",
            "}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    bins[s[tid][0] - 'a']++;\n  }\n}",
            "int tid = hipThreadIdx_x;\n    // Count the number of times a letter appears in the strings.\n    for (int i = tid; i < N; i += hipBlockDim_x) {\n        char c = tolower(s[i][0]);\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "__shared__ size_t s1[26], s2[26];\n\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int lane = tid % warpSize;\n    int wid = tid / warpSize;\n    int len = strlen(s[bid]);\n    char c = s[bid][0];\n\n    // The total number of threads in a warp is 64.\n    // This is to ensure that all threads within a warp\n    // have the same value of `c` and `len`\n    c = __shfl_sync(0xffffffff, c, lane);\n    len = __shfl_sync(0xffffffff, len, lane);\n\n    // Each warp processes a single element of the array.\n    // Hence each warp processes 64 elements\n    // The number of warps required to process the entire array is ceil(N/64)\n    // Each warp needs to initialize its local arrays before performing parallel prefix sum on them\n    if (wid == 0) {\n        for (int i = 0; i < 26; ++i)\n            s1[i] = 0;\n\n        for (int i = 0; i < len; ++i)\n            ++s1[s[bid][i] - 'a'];\n    }\n\n    __syncthreads();\n\n    // Prefix sum on s1\n    // The result is stored in s2\n    // This is to ensure that each thread within a warp\n    // has the result of all other threads\n    if (wid < 26)\n        s2[wid] = s1[wid];\n    else\n        s2[wid] = 0;\n\n    __syncthreads();\n\n    for (int i = 1; i < warpSize; ++i) {\n        if (wid * warpSize + i < 26)\n            s2[wid] += s2[wid * warpSize + i];\n    }\n\n    // Each thread in the warp now has the number of strings that start with `c`\n    // Now, each thread in the warp has the sum of the counts of the strings that start with all the letters\n    // from 'a' to 'z'\n    // Hence we need to update the total count of strings starting with `c` in the global array `bins`\n    // The index of the array is the value of `c` - 'a'\n    // This value is obtained by converting the `c` to `int`\n    if (wid == 0)\n        ++bins[c - 'a'];\n}",
            "__shared__ size_t count[26];\n\n  // Each thread will process one of 26 letters\n  char c = blockIdx.x;\n\n  // Each thread must zero out its local count before starting\n  if (threadIdx.x < 26) {\n    count[threadIdx.x] = 0;\n  }\n\n  // Synchronize all threads before starting to count\n  __syncthreads();\n\n  // Compute the number of strings that start with c\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    if (s[i][0] == c) {\n      count[c - 'a']++;\n    }\n  }\n\n  // Synchronize all threads before writing to global memory\n  __syncthreads();\n\n  // Use atomics to update the global count\n  if (threadIdx.x < 26) {\n    atomicAdd(&bins[c - 'a'], count[c - 'a']);\n  }\n}",
            "const int tid = hipThreadIdx_x;\n  const int lid = hipThreadIdx_x % 26;\n  const int wid = hipThreadIdx_x / 26;\n\n  __shared__ int counters[26][2];\n  if (lid == 0) counters[wid][0] = 0;\n  if (lid == 1) counters[wid][1] = 0;\n  __syncthreads();\n\n  const int n = N - wid * 26;\n  for (int i = tid; i < n; i += 26) {\n    char c = s[wid * 26 + i][0];\n    if (c >= 'a' && c <= 'z')\n      atomicAdd(&counters[c - 'a'][1], 1);\n  }\n  __syncthreads();\n\n  for (int i = lid; i < 26; i += 2) {\n    atomicAdd(&bins[i], counters[i][0]);\n    atomicAdd(&bins[i], counters[i][1]);\n  }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    const char *first = s[tid];\n    const char c = first[0];\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "size_t thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n   if (thread_id < N) {\n      int c = s[thread_id][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "// Initialize the bins array to zero.\n  for (int i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  // Compute the bin counts for each thread.\n  size_t tid = threadIdx.x;\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    int ch = (int)s[i][0] - (int)'a';\n    atomicAdd(&bins[ch], 1);\n  }\n}",
            "int tid = hipThreadIdx_x; // This is the index of the current thread, in the range 0.. 15\n    int blkid = hipBlockIdx_x; // This is the block number, in the range 0.. 12\n    int bpg = hipGridDim_x;   // This is the total number of blocks\n\n    int my_tid = tid + blkid * blockDim.x;\n\n    // Launch 16 threads per block.\n    if (my_tid < N) {\n        int c = s[my_tid][0] - 'a'; // Look up character in alphabet.\n        atomicAdd(&bins[c], 1); // Update count in the appropriate bin.\n    }\n}",
            "size_t tid = threadIdx.x;\n    size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    for (size_t i = gid; i < N; i += blockDim.x * gridDim.x) {\n        size_t val = (size_t) s[i][0];\n        atomicAdd(&bins[val - 97], 1);\n    }\n}",
            "// TODO: Implement kernel\n}",
            "// TODO: Your code goes here!\n}",
            "int laneId = threadIdx.x & (WARP_SIZE-1);\n  int laneId2 = (threadIdx.x & ~0x3f) >> 6;\n  int warpId = threadIdx.x >> 5;\n  int warpId2 = (threadIdx.x + 0x40) >> 5;\n  int warpId3 = (threadIdx.x + 0x80) >> 5;\n  int warpId4 = (threadIdx.x + 0xc0) >> 5;\n  int warpId5 = (threadIdx.x + 0x100) >> 5;\n\n  __shared__ size_t smem[WARP_SIZE][26];\n\n  for (int i=laneId; i<26; i+=WARP_SIZE) {\n    smem[warpId][i] = 0;\n  }\n  __syncthreads();\n\n  for (int i=laneId2; i<N; i+=32) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      smem[warpId][c-'a']++;\n      smem[warpId2][c-'a']++;\n      smem[warpId3][c-'a']++;\n      smem[warpId4][c-'a']++;\n      smem[warpId5][c-'a']++;\n    }\n  }\n  __syncthreads();\n\n  for (int i=laneId; i<26; i+=WARP_SIZE) {\n    atomicAdd(&bins[i], smem[warpId][i]);\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        size_t index = s[tid][0] - 'a';\n        if (index < 26) {\n            atomicAdd(&bins[index], 1);\n        }\n    }\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n   if (tid < N) {\n      bins[tolower(s[tid][0]) - 'a']++;\n   }\n}",
            "size_t myId = threadIdx.x + blockIdx.x * blockDim.x;\n  if (myId >= N) return;\n  char c = s[myId][0];\n  size_t i = 0;\n  if (c >= 'a' && c <= 'z') {\n    atomicAdd(&(bins[c - 'a']), 1);\n  }\n}",
            "// Each block corresponds to an element in the vector s.\n    // Each thread corresponds to a letter in the string s[blockIdx.x].\n    int letter = s[blockIdx.x][threadIdx.x] - 'a';\n    atomicAdd(&bins[letter], 1);\n}",
            "// TODO\n}",
            "unsigned tid = hipThreadIdx_x;\n  unsigned bid = hipBlockIdx_x;\n\n  size_t b = bid * 26 + tid;\n  if (b < 26) {\n    bins[b] = 0;\n  }\n  __syncthreads();\n\n  size_t i = bid * hipBlockDim_x + tid;\n  while (i < N) {\n    if (s[i][0] == 'a' + b) {\n      atomicAdd(&bins[b], 1);\n    }\n    i += hipBlockDim_x * hipGridDim_x;\n  }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    __shared__ size_t firstLetter[26];\n\n    if (tid == 0)\n        for (int i = 0; i < 26; i++)\n            firstLetter[i] = 0;\n\n    __syncthreads();\n\n    for (size_t i = bid * blockDim.x + tid; i < N; i += blockDim.x * gridDim.x) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z')\n            atomicAdd(&firstLetter[s[i][0] - 'a'], 1);\n    }\n\n    __syncthreads();\n\n    if (tid == 0)\n        for (int i = 0; i < 26; i++)\n            atomicAdd(&bins[i], firstLetter[i]);\n}",
            "// Get the thread's index in the grid\n   size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n   // Create a local copy of the bins array\n   size_t binsLocal[26];\n   for (int i = 0; i < 26; i++)\n      binsLocal[i] = 0;\n\n   // Only one thread should work on the data\n   if (tid < N) {\n      // Get the character at the index\n      const char c = s[tid][0];\n\n      // If it's a lowercase letter, increment the bins array for that letter\n      if (c >= 'a' && c <= 'z')\n         binsLocal[c - 'a']++;\n   }\n\n   // Add the values from each thread to the global bins array\n   for (int i = 0; i < 26; i++)\n      atomicAdd(&bins[i], binsLocal[i]);\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    const char *p = s[idx];\n    while (*p) {\n      ++bins[tolower(*p) - 'a'];\n      ++p;\n    }\n  }\n}",
            "__shared__ size_t bins_shared[26];\n\n\tconst size_t tid = threadIdx.x;\n\tconst size_t block_size = blockDim.x;\n\tconst size_t block_id = blockIdx.x;\n\n\t// Load strings into local memory\n\tchar strings[block_size][20];\n\tif (tid < N) {\n\t\tfor (int i = 0; i < 20; i++) {\n\t\t\tstrings[tid][i] = s[block_id * block_size + tid][i];\n\t\t}\n\t}\n\n\t// Wait for all threads to be ready\n\t__syncthreads();\n\n\t// Count the number of strings starting with each letter\n\tint count = 0;\n\tif (tid == 0) {\n\t\tfor (int i = 0; i < 26; i++) {\n\t\t\tbins_shared[i] = 0;\n\t\t}\n\n\t\tfor (int i = 0; i < N; i++) {\n\t\t\tfor (int j = 0; j < 20; j++) {\n\t\t\t\tif ((int)strings[i][j] - 97 == i) {\n\t\t\t\t\tbins_shared[i]++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// Copy the contents of bins_shared to global memory\n\t\tfor (int i = 0; i < 26; i++) {\n\t\t\tbins[i] = bins_shared[i];\n\t\t}\n\t}\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        size_t ch = s[idx][0] - 'a';\n        atomicAdd(&bins[ch], 1);\n    }\n}",
            "int tid = threadIdx.x;\n    for (int i = tid; i < N; i += blockDim.x) {\n        int c = s[i][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "int tid = hipThreadIdx_x;\n    if (tid < 26) {\n        for (size_t i = 0; i < N; i++) {\n            if (s[i][0] == tid + 'a') {\n                atomicAdd(&bins[tid], 1);\n            }\n        }\n    }\n}",
            "size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (gid < N) {\n      int c = s[gid][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "for (size_t i = 0; i < N; i++) {\n      bins[tolower(s[i][0]) - 'a']++;\n   }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i < N) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      bins[c - 'a']++;\n    }\n  }\n}",
            "for (int i = 0; i < N; ++i) {\n        if (isalpha(s[i][0])) {\n            atomicAdd(&bins[tolower(s[i][0]) - 'a'], 1);\n        }\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t localBins[26];\n  for (size_t i = 0; i < 26; i++) {\n    localBins[i] = 0;\n  }\n\n  for (size_t i = tid; i < N; i += gridDim.x * blockDim.x) {\n    size_t len = strlen(s[i]);\n    if (len > 0) {\n      size_t c = s[i][0] - 'a';\n      localBins[c]++;\n    }\n  }\n\n  for (size_t i = 0; i < 26; i++) {\n    __syncthreads();\n    atomicAdd(&bins[i], localBins[i]);\n  }\n}",
            "int index = threadIdx.x + blockDim.x * blockIdx.x;\n    if (index < N) {\n        char c = s[index][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "__shared__ size_t smem[26];\n\n    int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // Compute the number of strings that start with the current letter\n    size_t count = 0;\n    while (i < N) {\n        if (s[i][0] == 'a' + threadIdx.x) {\n            count++;\n        }\n        i += blockDim.x * gridDim.x;\n    }\n\n    smem[threadIdx.x] = count;\n\n    // Synchronize to make sure that all the partial sums are computed\n    __syncthreads();\n\n    // Reduce the counts in the shared memory into the global memory\n    for (int offset = 1; offset < blockDim.x; offset *= 2) {\n        if (threadIdx.x % (2 * offset) == 0) {\n            smem[threadIdx.x] += smem[threadIdx.x + offset];\n        }\n        __syncthreads();\n    }\n\n    // Write the counts in the global memory\n    if (threadIdx.x < 26) {\n        bins[threadIdx.x] = smem[threadIdx.x];\n    }\n}",
            "const int tid = hipThreadIdx_x;\n  const int lane = tid % 32;\n  const int block = tid / 32;\n  const size_t stride = hipBlockDim_x / 32;\n\n  for (size_t i = block * stride + lane; i < N; i += stride * hipBlockDim_x) {\n    const char c = s[i][0];\n    if (c >= 'a' && c <= 'z')\n      atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "int tid = threadIdx.x;\n    int blkId = blockIdx.x;\n    size_t stride = blockDim.x;\n\n    int index = blkId * stride + tid;\n\n    // One kernel to count each letter in parallel\n    if (index < N) {\n        char firstLetter = tolower(s[index][0]);\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            atomicAdd(&bins[firstLetter - 'a'], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    int i = 0;\n    if (tid < N) {\n        char c = s[tid][0];\n        if ('a' <= c && c <= 'z') {\n            ++bins[c - 'a'];\n        }\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int gid = tid + bid*blockDim.x;\n\n  int bins_size = 26;\n\n  // initialize shared memory\n  extern __shared__ char s_shared[];\n  for (int i = tid; i < 26*26; i += blockDim.x) {\n    s_shared[i] = 0;\n  }\n\n  __syncthreads();\n\n  for (int i = gid; i < N; i += gridDim.x * blockDim.x) {\n    int j = 0;\n    char c = s[i][j];\n    while (c!= '\\0') {\n      if (c >= 'a' && c <= 'z') {\n        // increment index c in bins\n        atomicAdd(bins + c - 'a', 1);\n      }\n      j++;\n      c = s[i][j];\n    }\n  }\n\n  __syncthreads();\n\n  for (int i = tid; i < bins_size; i += blockDim.x) {\n    atomicAdd(bins + i, bins[i+26]);\n  }\n}",
            "__shared__ size_t localBins[26]; // this kernel uses 26 bins\n\n  // each thread processes an input string\n  int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    size_t letter = (size_t) s[tid][0]; // first character of a string\n    if (letter < 91) // we are assuming the strings are in lower case\n      atomicAdd(&localBins[letter - 97], 1); // use atomicAdd from HIP to update a local variable\n  }\n\n  // now every thread in the block has computed the bin count for some letter,\n  // so we need to do a reduction across the block to get the bin counts for\n  // every letter\n  // NOTE: if you are trying to understand this code and wondering where the\n  // shared memory comes from, it is because each block has a block-local\n  // memory region with 26 elements. That memory region is shared across all\n  // threads in the block, so we can store the bin counts in there and do a\n  // reduction across the block.\n  __syncthreads();\n  for (int i = hipBlockDim_x / 2; i >= 1; i >>= 1) {\n    if (hipThreadIdx_x < i)\n      localBins[hipThreadIdx_x] += localBins[hipThreadIdx_x + i];\n    __syncthreads();\n  }\n  if (hipThreadIdx_x == 0) { // we are only the first thread in the block\n    size_t sum = 0;\n    for (int i = 0; i < 26; i++)\n      sum += localBins[i];\n    bins[hipBlockIdx_x] = sum;\n  }\n}",
            "const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  const int numThreads = blockDim.x;\n  const int blockSize = N / numThreads + 1;\n  const int start = tid * blockSize;\n\n  __shared__ bool found;\n  if (tid == 0) found = false;\n  __syncthreads();\n\n  while (true) {\n    if (start >= N || found)\n      break;\n    char c = s[bid][start];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n      found = true;\n    }\n    start += numThreads;\n  }\n}",
            "size_t tid = threadIdx.x;\n   size_t i = blockIdx.x * blockDim.x + tid;\n   size_t bin;\n   size_t len;\n\n   if (i < N) {\n      len = strlen(s[i]);\n      bin = tolower(s[i][0]);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "__shared__ size_t tempBins[26];\n  __shared__ char firstLetters[N];\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    firstLetters[tid] = (char)tolower(s[tid][0]);\n  }\n\n  __syncthreads();\n\n  // For each letter in the alphabet, count the number of strings that start with that letter\n  for (int i = 0; i < 26; ++i) {\n    tempBins[i] = 0;\n  }\n  if (tid < N) {\n    atomicAdd(&tempBins[firstLetters[tid] - 'a'], 1);\n  }\n\n  // Add the results from each thread\n  for (int i = 0; i < 26; ++i) {\n    atomicAdd(&bins[i], tempBins[i]);\n  }\n}",
            "const int tid = hipThreadIdx_x;\n  const int bid = hipBlockIdx_x;\n  const int lid = hipThreadIdx_x % 32;\n\n  __shared__ size_t sharedBins[32];\n  sharedBins[lid] = 0;\n\n  const char *sptr = s[bid * 32 + lid];\n\n  if (sptr!= 0) {\n    int c = *sptr;\n\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&sharedBins[lid], 1);\n    }\n  }\n\n  __syncthreads();\n\n  for (int i = 16; i > 0; i >>= 1) {\n    size_t tmp = sharedBins[lid + i];\n\n    if (lid < i) {\n      sharedBins[lid] += tmp;\n    }\n\n    __syncthreads();\n  }\n\n  if (lid == 0) {\n    int c = *s[bid * 32 + 31];\n\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&sharedBins[0], 1);\n    }\n  }\n\n  __syncthreads();\n\n  if (lid == 0) {\n    atomicAdd(&bins[c - 'a'], sharedBins[0]);\n  }\n}",
            "// 1. Compute global thread id.\n    const int tid = hipThreadIdx_x;\n    const int bin = tid % 26;\n    // 2. Initialize local accumulator.\n    int acc = 0;\n    // 3. Compute local sum.\n    for (int i = 0; i < N; i++) {\n        if (s[i][0] == 'a' + bin) acc++;\n    }\n    // 4. Merge the per-thread results into bins[bin].\n    bins[bin] += acc;\n}",
            "int tid = hipThreadIdx_x;\n    int bid = hipBlockIdx_x;\n\n    // Only work on 26 letters\n    if (bid >= 26) return;\n    bins[bid] = 0;\n\n    // Loop over all strings in the array\n    for (size_t i = tid + bid * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n        if (s[i][0] == (char)(bid + 'a'))\n            atomicAdd(&bins[bid], 1);\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  for (size_t i = gid; i < N; i += hipBlockDim_x * hipGridDim_x) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "size_t idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n    for (int i = idx; i < N; i += hipGridDim_x * hipBlockDim_x) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// Compute each letter position in the alphabet in parallel\n  size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x*blockDim.x + tid;\n  if (gid < 26) {\n    size_t count = 0;\n    for (size_t i = 0; i < N; i++)\n      if (s[i][0] == 'a'+gid)\n        count++;\n    atomicAdd(&bins[gid], count);\n  }\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < N) {\n        ++bins[s[tid][0] - 'a'];\n    }\n}",
            "size_t id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  if (id < N) {\n    bins[s[id][0] - 'a']++;\n  }\n}",
            "// Compute the local histogram for the string starting with s[i]\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO\n    const size_t tid = threadIdx.x;\n    const size_t bid = blockIdx.x;\n    const size_t blockSize = blockDim.x;\n    const size_t gridSize = blockDim.x * gridDim.x;\n    const size_t i = bid * blockSize + tid;\n\n    const char *str = s[i];\n    const size_t strLen = strlen(str);\n\n    // Each thread will process the elements corresponding to their block\n    // The number of blocks must be a factor of the array length\n    if (i < N) {\n        // TODO\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int idx = s[tid][0] - 'a';\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  for (size_t i = 0; i < N; i++) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n  }\n  return;\n}",
            "}",
            "__shared__ unsigned int s_bins[26];\n  unsigned int i = threadIdx.x + blockIdx.x * blockDim.x;\n  unsigned int block_id = blockIdx.x;\n  if (i < 26) {\n    unsigned int count = 0;\n    for (unsigned int j = 0; j < N; j++)\n      if (s[j][0] == 'a' + i)\n        count++;\n    s_bins[i] = count;\n  }\n  __syncthreads();\n  for (unsigned int s = 1; s < blockDim.x; s *= 2) {\n    if (i % (2 * s) == 0) {\n      unsigned int t = i + s;\n      if (t < 26)\n        s_bins[t] += s_bins[i];\n    }\n    __syncthreads();\n  }\n  if (i < 26)\n    atomicAdd(&bins[i], s_bins[i]);\n}",
            "int tid = hipThreadIdx_x;\n    int bid = hipBlockIdx_x;\n\n    int sum = 0;\n    for (int i = bid; i < N; i += hipGridDim_x) {\n        sum += s[i][0] == 'd'? 1 : 0;\n        sum += s[i][0] == 'c'? 1 : 0;\n        sum += s[i][0] == 'x'? 1 : 0;\n        sum += s[i][0] == 'c'? 1 : 0;\n        sum += s[i][0] == 'c'? 1 : 0;\n        sum += s[i][0] == 't'? 1 : 0;\n        sum += s[i][0] == 'f'? 1 : 0;\n    }\n\n    // Update global memory.\n    atomicAdd(&bins[tid], sum);\n}",
            "__shared__ size_t sharedBins[26]; // one block per letter, 26 blocks => 26 letters\n  for (int i = threadIdx.x; i < 26; i += blockDim.x) sharedBins[i] = 0;\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    const char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&sharedBins[c - 'a'], 1);\n    }\n  }\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < 26; i += blockDim.x) atomicAdd(&bins[i], sharedBins[i]);\n}",
            "}",
            "// TODO\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n  if (i >= N) {\n    return;\n  }\n\n  int c = s[i][0] - 'a';\n  atomicAdd(bins + c, 1);\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  __shared__ size_t s_size[BLOCK_SIZE];\n  __shared__ char s_start[BLOCK_SIZE];\n  __shared__ char s_end[BLOCK_SIZE];\n  if (tid == 0) {\n    s_size[bid] = strlen(s[bid]);\n    s_start[bid] = s[bid][0];\n    s_end[bid] = s[bid][s_size[bid] - 1];\n  }\n  __syncthreads();\n  if (tid < 26) {\n    bins[tid] = 0;\n  }\n  __syncthreads();\n  if (tid < s_size[bid]) {\n    char c = s_start[bid] + tid;\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "const int tid = threadIdx.x;\n   const int bid = blockIdx.x;\n   size_t [26] counts;\n   for (size_t i = 0; i < 26; i++)\n      counts[i] = 0;\n   for (size_t i = tid; i < N; i += blockDim.x) {\n      counts[(size_t)s[i][0] - (size_t)'a']++;\n   }\n   __syncthreads();\n   for (size_t i = tid; i < 26; i += blockDim.x) {\n      atomicAdd(&bins[i], counts[i]);\n   }\n}",
            "// Add your code here.\n}",
            "unsigned int tid = threadIdx.x;\n  unsigned int gid = blockIdx.x;\n  __shared__ unsigned int num_words;\n  if (tid == 0) {\n    num_words = N;\n  }\n  __syncthreads();\n\n  // We will only process a subset of the input strings, to reduce the work to do per thread.\n  unsigned int word_idx = gid * N / gridDim.x + tid;\n\n  if (word_idx < num_words) {\n    char letter = tolower(s[word_idx][0]);\n    if ('a' <= letter && letter <= 'z') {\n      atomicAdd(&bins[letter - 'a'], 1);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int i = (int)tolower(s[tid][0]);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "// TODO: Fill in the kernel code here\n}",
            "int tid = hipThreadIdx_x;\n  for (int i=tid; i<N; i+=hipBlockDim_x) {\n    char ch = s[i][0];\n    if (ch >= 'a' && ch <= 'z') {\n      atomicAdd(&bins[ch-'a'], 1);\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < 26) {\n      for (int i = 0; i < N; ++i) {\n         if (s[i][0] == tid + 'a') {\n            atomicAdd(&bins[tid], 1);\n         }\n      }\n   }\n}",
            "size_t tid = threadIdx.x;\n\n   // Use one block per string\n   for (size_t i = blockIdx.x; i < N; i += gridDim.x) {\n      const char *s_i = s[i];\n      // Use one warp per string\n      for (int j = tid; j < 26; j += 32) {\n         if (s_i[0] == (char) j + 97) {\n            atomicAdd(&bins[j], 1);\n         }\n      }\n   }\n}",
            "const unsigned int tid = hipThreadIdx_x;\n\n\tif (tid < 26) {\n\t\t// Initialize the counter for each letter to 0.\n\t\tbins[tid] = 0;\n\t}\n\t__syncthreads();\n\n\tfor (size_t i = tid; i < N; i += hipBlockDim_x) {\n\t\t// Compute the index of the letter of s[i] in the alphabet.\n\t\tconst char *letter = s[i];\n\t\tbins[letter[0] - 'a']++;\n\t}\n}",
            "}",
            "int tid = hipThreadIdx_x;\n    int bid = hipBlockIdx_x;\n    int nthreads = hipBlockDim_x;\n    __shared__ int count[26];\n    int myCount = 0;\n\n    for (int i = tid; i < N; i += nthreads) {\n        if (s[i][0]!= 'x')\n            ++myCount;\n    }\n\n    // First thread of each block reduces the count of all letters in the alphabet.\n    // Then all threads of the block wait for all the counts to be reduced before writing to global memory.\n    count[bid] = myCount;\n    __syncthreads();\n    if (tid == 0) {\n        for (int i = 1; i < 26; i *= 2) {\n            myCount += count[bid + i];\n            __syncthreads();\n            count[bid] = myCount;\n            __syncthreads();\n        }\n        bins[bid] = count[bid];\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    char c = tolower(s[tid][0]);\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&(bins[c - 'a']), 1);\n    }\n  }\n}",
            "// Get the blockId and threadId in the current block\n    int blockId = blockIdx.x;\n    int threadId = threadIdx.x;\n\n    // Each block processes the letters of the alphabet in parallel\n    // Each block processes all strings in the input vector s in parallel\n    int begin = blockId*N/26;\n    int end = (blockId+1)*N/26;\n    int step = N/26;\n    if (threadId < 26) {\n        int count = 0;\n        for (int i = begin+threadId; i < end; i+=step) {\n            if (s[i][0] == 'a'+threadId) {\n                count++;\n            }\n        }\n        // Store the result in global memory\n        atomicAdd(&bins[threadId], count);\n    }\n}",
            "size_t tid = threadIdx.x;\n    size_t i;\n    for (i = tid; i < N; i += blockDim.x) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "int tid = hipThreadIdx_x; // 0, 1, 2,..., N\n    int idx = hipBlockIdx_x;  // 0, 1, 2,..., 25\n\n    __shared__ int count[26];\n    count[tid] = 0;\n\n    for (int i = tid; i < N; i += hipBlockDim_x)\n        if (s[i][0] == (idx + 'a'))\n            ++count[tid];\n\n    __syncthreads();\n\n    for (int i = hipBlockDim_x / 2; i > 0; i /= 2) {\n        __syncthreads();\n        if (tid < i) {\n            count[tid] += count[tid + i];\n        }\n        __syncthreads();\n    }\n\n    if (tid == 0) {\n        bins[idx] = count[0];\n    }\n}",
            "size_t thread_id = hipThreadIdx_x;\n  size_t block_id = hipBlockIdx_x;\n\n  // 1. Loop over the strings in the vector s.\n  // 2. For each string, loop over the first letter of the string, and increment the count in the `bins` array.\n  for (size_t i = block_id * block_size + thread_id; i < N; i += grid_size * block_size) {\n    char c = s[i][0];\n    // 1.\n    // for (int j = 0; j < s[i].size(); j++) {\n    //   char c = s[i][j];\n    //   // 2.\n    //   if (c >= 'a' && c <= 'z') {\n    //     atomicAdd(&bins[c - 'a'], 1);\n    //   }\n    // }\n    // 1.\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    char ch = s[tid][0];\n    atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t global_tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    size_t n = 0;\n    if (global_tid < N) {\n        char c = s[global_tid][0];\n        if (c >= 'a' && c <= 'z') {\n            c -= 'a';\n            atomicAdd(&bins[c], 1);\n        }\n    }\n}",
            "// The thread ID is the index of the result in the bins array\n  int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // The alphabet is 26 letters long\n  for (int i = threadId; i < 26; i += blockDim.x * gridDim.x) {\n    size_t count = 0;\n\n    // Iterate over all the strings in the vector\n    for (size_t j = 0; j < N; j++) {\n      // Check if the string starts with this letter\n      if (s[j][0] == 'a' + i)\n        count++;\n    }\n\n    // Store the result\n    bins[i] = count;\n  }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int bsize = blockDim.x;\n  int idx = tid + bsize * bid;\n  int b = 0;\n\n  if (idx < N) {\n    char firstLetter = *s[idx];\n    while (firstLetter!= '\\0') {\n      if (firstLetter >= 'a' && firstLetter <= 'z') {\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n      }\n      ++b;\n      ++firstLetter;\n    }\n  }\n}",
            "// YOUR CODE HERE\n  // HINT: To be able to access the array of results from the host,\n  // you need to declare it as `extern __shared__`\n}",
            "// Find this thread's block and global position.\n   int block_id = blockIdx.x;\n   int tid = block_id * blockDim.x + threadIdx.x;\n\n   int global_count;\n\n   if (tid < N) {\n      // Do some work here.\n      char c = tolower(s[tid][0]);\n      global_count = atomicAdd(&bins[c - 'a'], 1);\n   }\n\n   // Aggregate results from all threads in block.\n   for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n      __syncthreads();\n      if (tid < stride)\n         global_count += __shfl_down(global_count, stride);\n   }\n\n   // Write result for this block to global memory.\n   if (tid == 0)\n      atomicAdd(&bins[26], global_count);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    size_t letter = s[tid][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "int thread_id = threadIdx.x;\n  int global_id = blockIdx.x * blockDim.x + thread_id;\n\n  // the inner loop is unrolled\n  for (int n = global_id; n < N; n += blockDim.x * gridDim.x) {\n    char c = s[n][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "__shared__ int count[26];\n  if (threadIdx.x < 26) count[threadIdx.x] = 0;\n  __syncthreads();\n\n  int start = blockIdx.x * 32 + threadIdx.x;\n  int i = 0;\n  for (i = start; i < N; i += blockDim.x * gridDim.x) {\n    if (s[i]!= 0) {\n      atomicAdd(&count[s[i][0] - 'a'], 1);\n    }\n  }\n  __syncthreads();\n\n  if (threadIdx.x < 26) {\n    atomicAdd(&bins[threadIdx.x], count[threadIdx.x]);\n  }\n}",
            "extern __shared__ size_t temp[];\n\tif(threadIdx.x<26) temp[threadIdx.x] = 0;\n\t__syncthreads();\n\n\t// Only one thread needs to run the loop\n\tif(threadIdx.x==0) {\n\t\tfor (int i=0; i<N; i++) {\n\t\t\tchar ch = s[i][0];\n\t\t\tif (ch >= 'a' && ch <= 'z') {\n\t\t\t\tatomicAdd(&temp[ch - 'a'], 1);\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\n\t// Atomic adds to output are not atomic on the device, so we need to sync\n\t// before reading them.\n\tif (threadIdx.x<26) {\n\t\tatomicAdd(&bins[threadIdx.x], temp[threadIdx.x]);\n\t}\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < 26) {\n    char c = tid + 'a';\n    bins[tid] = 0;\n    for (int i = 0; i < N; i++)\n      if (s[i][0] == c)\n        bins[tid]++;\n  }\n}",
            "// Your code goes here.\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  __shared__ int firstLetterCounts[26];\n  for (int i=tid; i<26; i+=blockDim.x)\n    firstLetterCounts[i] = 0;\n  __syncthreads();\n\n  for (int i=bid; i<N; i+=gridDim.x) {\n    int ch = s[i][0] - 'a';\n    if (ch >= 0 && ch < 26)\n      atomicAdd(&firstLetterCounts[ch], 1);\n  }\n  __syncthreads();\n\n  for (int i=tid; i<26; i+=blockDim.x)\n    atomicAdd(&bins[i], firstLetterCounts[i]);\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (tid < N) {\n        char c = tolower(s[tid][0]);\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "__shared__ char shared[26];\n   // Each thread counts the number of strings starting with its letter\n   for(size_t i=threadIdx.x; i<26; i+=blockDim.x) {\n      shared[i] = 0;\n      for(size_t j=0; j<N; j++) {\n         shared[i] += s[j][0] == ('a' + i);\n      }\n   }\n   // Write the count into global memory\n   for(size_t i=threadIdx.x; i<26; i+=blockDim.x) {\n      atomicAdd(bins + i, shared[i]);\n   }\n}",
            "int tid = hipThreadIdx_x;\n  int wid = hipBlockIdx_x;\n  int stride = hipBlockDim_x;\n  int gid = wid * stride + tid;\n  char c = 'a';\n  if (gid < N && s[gid][0]!= 0) {\n    c = s[gid][0];\n  }\n  __syncthreads();\n  int i = atomicInc(&bins[c-'a'], N);\n  __syncthreads();\n  if (gid < N && s[gid][0] == c && i == 0) {\n    printf(\"Block %d: found word %s\\n\", wid, s[gid]);\n  }\n}",
            "size_t idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (idx < N) {\n        size_t c = s[idx][0] - 'a';\n        atomicAdd(&bins[c], 1);\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n\n    char c = s[bid][0];\n    if (c >= 'a' && c <= 'z') {\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t i = tid;\n\n    if (i < N) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// TODO\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n\n   for (int i = bid; i < N; i += gridDim.x) {\n      if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n         atomicAdd(&(bins[s[i][0] - 'a']), 1);\n      }\n   }\n}",
            "const int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n\tif(threadId < N) {\n\t\tbins[*(s[threadId] + 0)]++;\n\t}\n}",
            "const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  int sum = 0;\n  for (size_t i = bid*N; i < (bid+1)*N; i++) {\n    if (s[i]!= 0) {\n      sum++;\n    }\n  }\n  // Store sum in global memory.\n  atomicAdd(bins+tid, sum);\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (tid < 26) {\n        size_t count = 0;\n        for (size_t i = 0; i < N; i++) {\n            if (s[i][0] == tid + 'a')\n                count++;\n        }\n        atomicAdd(&bins[tid], count);\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    __shared__ int temp[26];\n    int ltid = tid;\n    int offset = 0;\n    // Load the data into the shared memory\n    for (int i = 0; i < 26; i++) {\n        if (bid * 26 + i < N) {\n            char c = s[bid * 26 + i][0];\n            temp[ltid] = c - 'a';\n            offset = c - 'a';\n            ltid += blockDim.x;\n        } else {\n            temp[ltid] = 0;\n        }\n        __syncthreads();\n        // Reduce the count for each thread\n        for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n            if (ltid < stride)\n                temp[ltid] += temp[ltid + stride];\n            __syncthreads();\n        }\n        // Store the total count in the global array\n        if (tid == 0)\n            bins[offset] = temp[0];\n        __syncthreads();\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: Compute `bins` using MPI here.\n    // You may use `size_t`s to represent the counts.\n    // You may assume that the size of `bins` is always 26.\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size <= 0)\n        return;\n    int start, end;\n    int n = static_cast<int>(s.size());\n    int temp = n / size;\n    start = rank * temp;\n    end = (rank + 1) * temp;\n    for (auto &str : s)\n    {\n        if (rank == 0)\n        {\n            if (str[0] >= 'a' && str[0] <= 'z')\n                ++bins[str[0] - 'a'];\n        }\n    }\n    std::array<int, 26> arr;\n    if (rank == 0)\n    {\n        arr = bins;\n    }\n    MPI_Bcast(arr.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    bins = arr;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tif (s.size() < size) throw std::runtime_error(\"Error: need more processes than strings\");\n\n\tstd::vector<size_t> counts(size);\n\tstd::vector<std::string> s_local = s;\n\tMPI_Scatter(s_local.data(), s_local.size(), MPI_CHAR, counts.data(), counts.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n\n\tstd::vector<size_t> counts_local(counts[rank]);\n\tfor (auto i = 0; i < s[rank].size(); i++) {\n\t\tcounts_local[s[rank][i] - 'a']++;\n\t}\n\n\tMPI_Gather(counts_local.data(), counts_local.size(), MPI_UNSIGNED_LONG_LONG, counts.data(), counts_local.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tstd::array<size_t, 26> result{};\n\t\tfor (auto i = 0; i < counts.size(); i++) {\n\t\t\tfor (auto j = 0; j < counts[i]; j++) {\n\t\t\t\tresult[j] += counts[i];\n\t\t\t}\n\t\t}\n\t\tbins = result;\n\t}\n}",
            "if (s.size() <= 1) {\n        if (s.size() == 1) {\n            auto firstLetter = s[0][0] - 'a';\n            bins[firstLetter]++;\n        }\n        return;\n    }\n\n    int rank, numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    size_t n = s.size() / numProcs;\n    size_t remainder = s.size() % numProcs;\n\n    size_t start = n * rank + std::min(rank, remainder);\n    size_t end = n * (rank + 1) + std::min(rank + 1, remainder);\n\n    std::vector<std::string> myStrings(s.begin() + start, s.begin() + end);\n\n    std::array<size_t, 26> subBins;\n    firstLetterCounts(myStrings, subBins);\n\n    MPI_Reduce(subBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t count = 0;\n\n  std::string::const_iterator start = s.at(0).begin();\n  std::string::const_iterator end = s.at(0).end();\n\n  for (auto const& c: s) {\n\n    std::string::const_iterator it = c.begin();\n\n    while (it!= c.end()) {\n\n      if (*it!= *start) {\n        break;\n      }\n\n      if (it == end) {\n        count++;\n        break;\n      }\n\n      it++;\n    }\n\n    start = c.begin();\n    end = c.end();\n  }\n\n  bins.at(*start - 'a') = count;\n}",
            "// TODO: Fill this in.\n\tint count;\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint count_local, count_total = 0;\n\tstd::vector<std::string> s_local;\n\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < size; i++) {\n\t\t\tMPI_Send(s.data(), s.size(), MPI_CHAR, i, 1, MPI_COMM_WORLD);\n\t\t}\n\t}\n\telse {\n\t\ts_local.resize(s.size());\n\t\tMPI_Recv(s_local.data(), s.size(), MPI_CHAR, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\tfor (std::string const& str : s_local) {\n\t\t\tcount_local = 0;\n\t\t\tfor (char c : str) {\n\t\t\t\tcount_local++;\n\t\t\t}\n\t\t\tMPI_Send(&count_local, 1, MPI_INT, 0, 2, MPI_COMM_WORLD);\n\t\t\tcount_total += count_local;\n\t\t}\n\t}\n\tMPI_Gather(&count_total, 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "auto const world_rank = MPI_Comm_rank(MPI_COMM_WORLD, NULL);\n  auto const world_size = MPI_Comm_size(MPI_COMM_WORLD, NULL);\n\n  // Each rank gets a full copy of s.\n  std::vector<std::string> local_s(s);\n\n  // The number of strings that start with a letter is the sum of the `count` values for all ranks.\n  std::vector<size_t> counts(26, 0);\n\n  // Each rank computes the number of strings that start with each letter.\n  for (auto &str : local_s) {\n    counts[str[0] - 'a']++;\n  }\n\n  // The total number of strings in the vector is the sum of the `local_count` values.\n  size_t const total_count = std::accumulate(counts.cbegin(), counts.cend(), 0);\n\n  // Use MPI reduce to sum the `counts` vectors.\n  MPI_Reduce(counts.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Rank 0 prints the final result.\n  if (world_rank == 0) {\n    for (size_t i = 0; i < 26; i++) {\n      std::cout << static_cast<char>('a' + i) << \" \" << bins[i] << std::endl;\n    }\n    std::cout << std::endl;\n  }\n}",
            "// TODO: Your code goes here!\n}",
            "int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    // Divide the strings evenly between processes\n    size_t strsPerProc = s.size() / nprocs;\n    std::vector<std::string> localStrings;\n    if (rank < (s.size() % nprocs)) {\n        // Add strings to local vector for current process\n        for (size_t i = rank * strsPerProc; i < (rank + 1) * strsPerProc; ++i) {\n            localStrings.push_back(s[i]);\n        }\n    }\n    else {\n        // Add strings to local vector for current process\n        for (size_t i = rank * strsPerProc; i < s.size(); ++i) {\n            localStrings.push_back(s[i]);\n        }\n    }\n\n    // Count the letters in each string\n    std::array<size_t, 26> localCount;\n    std::fill(localCount.begin(), localCount.end(), 0);\n    for (size_t i = 0; i < localStrings.size(); ++i) {\n        if (localStrings[i].length() > 0) {\n            ++localCount[localStrings[i][0] - 'a'];\n        }\n    }\n\n    // Gather the counts to rank 0\n    std::vector<size_t> counts(localCount.begin(), localCount.end());\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here\n\n    // MPI variables\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::string strs[world_size]; // each process has its own copy of strs\n    size_t chunk_size = s.size() / world_size; // number of strings each process will handle\n    if (world_rank == world_size - 1) chunk_size += s.size() % world_size; // for the remaining strings\n\n    // slicing\n    for (int i = 0; i < world_size; i++) {\n        if (i < world_size - 1) {\n            strs[i] = std::string(s.begin() + i * chunk_size, s.begin() + (i + 1) * chunk_size);\n        } else {\n            strs[i] = std::string(s.begin() + i * chunk_size);\n        }\n    }\n\n    size_t local_bins[26];\n    std::fill(local_bins, local_bins + 26, 0); // initialize all bins to 0\n\n    // Counting\n    for (std::string const &str : strs) {\n        for (char c : str) {\n            local_bins[c - 'a']++;\n        }\n    }\n\n    // Summarizing\n    std::array<size_t, 26> local_bins_total;\n    MPI_Allreduce(local_bins, local_bins_total.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        bins = local_bins_total;\n    }\n}",
            "// Compute number of letters in s, and send this count to each rank\n  int const num_ranks = 4;\n  int const my_rank = 0;\n  int n = 0;\n  if (my_rank == 0) {\n    n = s.size();\n  }\n  int counts[num_ranks];\n  MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Gather(&n, 1, MPI_INT, counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Determine which letters each rank should process, and send that to the others.\n  int start = 0;\n  for (int rank = 0; rank < my_rank; ++rank) {\n    start += counts[rank];\n  }\n  int end = start + counts[my_rank];\n  int num_letters = 0;\n  if (my_rank == 0) {\n    num_letters = counts[0];\n  }\n  int letter_counts[num_letters];\n  MPI_Bcast(&num_letters, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Scatter(&counts, 1, MPI_INT, letter_counts, num_letters, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Process each letter, and store the result\n  std::array<size_t, 26> local_bins;\n  for (int rank = 0; rank < num_ranks; ++rank) {\n    if (my_rank == rank) {\n      for (int i = start; i < end; ++i) {\n        int const ch = s[i][0] - 'a';\n        ++local_bins[ch];\n      }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n  }\n\n  // Add local bins to global bins\n  MPI_Reduce(local_bins.data(), bins.data(), num_letters, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Print the final bins, if this is rank 0\n  if (my_rank == 0) {\n    std::cout << \"Final bins: \";\n    for (int i = 0; i < 26; ++i) {\n      std::cout << bins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "// TODO\n}",
            "const size_t num_strings = s.size();\n  if(num_strings == 0) {\n    for(auto& count : bins) count = 0;\n    return;\n  }\n\n  // split the list of strings evenly between the ranks\n  const size_t my_first = num_strings/MPI_size;\n  const size_t my_last = my_first + (num_strings % MPI_size == 0? my_first : 1);\n\n  // determine the range of the letters that each rank is responsible for\n  size_t rank = MPI_rank;\n  size_t first_rank = rank == 0? 0 : (rank-1)*my_first;\n  size_t last_rank = rank == MPI_size-1? num_strings-1 : rank*my_last;\n\n  // initialize the array of counts for each rank\n  std::array<size_t, 26> my_bins;\n  for(auto& count : my_bins) count = 0;\n\n  // iterate over all strings in the local range\n  for(size_t i = first_rank; i <= last_rank; ++i) {\n    const std::string& word = s[i];\n    if(word.length() > 0) my_bins[word[0] - 'a']++;\n  }\n\n  // use MPI reduce operation to combine the counts for all ranks\n  MPI_Reduce(&my_bins, &bins, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int n = s.size();\n    const int rank = MPI::COMM_WORLD.Get_rank();\n    const int nproc = MPI::COMM_WORLD.Get_size();\n\n    int localBins[26] = { 0 };\n    std::for_each(s.begin(), s.end(), [&localBins](std::string const& s) {\n        ++localBins[static_cast<int>(s[0] - 'a')];\n    });\n\n    std::array<int, 26> globalBins;\n    MPI::COMM_WORLD.Allreduce(localBins, globalBins.data(), 26, MPI_INT, MPI_SUM);\n\n    if (rank == 0) {\n        std::copy(globalBins.begin(), globalBins.end(), bins.begin());\n    }\n}",
            "auto const rank = MPI_Comm_rank(MPI_COMM_WORLD, nullptr);\n    if(rank == 0) {\n        for(auto const& e : s) {\n            ++bins[e[0] - 'a'];\n        }\n    }\n    MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// MPI_Init(argc, argv);\n    size_t count = 0;\n    // MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    for (auto const& word : s) {\n        // if (rank == 0) {\n            auto first = word[0];\n            if (first >= 'a' && first <= 'z') {\n                bins[first - 'a'] += 1;\n            }\n        // }\n        // MPI_Bcast(&bins[first - 'a'], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n    // MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Fill in the blanks\n\n  // compute the number of strings in s that start with 'a'\n  size_t aCount = 0;\n  // compute the number of strings in s that start with 'b'\n  size_t bCount = 0;\n\n  // for each string in s\n  for(std::string const& str : s) {\n    // find the first character of the string\n    char firstChar = str[0];\n    // increment the appropriate bin\n    if(firstChar == 'a') {\n      ++aCount;\n    } else if(firstChar == 'b') {\n      ++bCount;\n    }\n  }\n\n  // set the appropriate bins\n  bins[0] = aCount;\n  bins[1] = bCount;\n}",
            "// TODO: your code here\n\n  // MPI code\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Divide vector into equal pieces\n  size_t size_per_rank = s.size() / size;\n  size_t remainder = s.size() % size;\n\n  // Rank 0 reads all the strings in the vector and sends to other ranks\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      std::vector<std::string> local_s;\n      // Add strings in range [i*size_per_rank, i*size_per_rank + size_per_rank)\n      for (int j = 0; j < size_per_rank; j++) {\n        local_s.push_back(s[i*size_per_rank + j]);\n      }\n      if (i < remainder) {\n        local_s.push_back(s[size_per_rank * size + i]);\n      }\n\n      std::array<size_t, 26> local_bins;\n      // Count letters in range [i*size_per_rank, i*size_per_rank + size_per_rank)\n      // Add to local_bins\n      for (int j = 0; j < local_s.size(); j++) {\n        for (int k = 0; k < 26; k++) {\n          if (local_s[j].find(k + 97) == 0) {\n            local_bins[k]++;\n          }\n        }\n      }\n      // Send local_bins to each rank\n      MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    // Receive and count letters from rank 0\n    std::array<size_t, 26> local_bins;\n    MPI_Recv(&local_bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    for (int i = 0; i < size_per_rank; i++) {\n      for (int j = 0; j < 26; j++) {\n        if (s[rank*size_per_rank + i].find(j + 97) == 0) {\n          local_bins[j]++;\n        }\n      }\n    }\n    if (rank < remainder) {\n      for (int j = 0; j < 26; j++) {\n        if (s[size_per_rank*size + rank].find(j + 97) == 0) {\n          local_bins[j]++;\n        }\n      }\n    }\n    MPI_Send(&local_bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // Receive and sum up bins\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      std::array<size_t, 26> recv;\n      MPI_Recv(&recv, 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      for (int j = 0; j < 26; j++) {\n        bins[j] += recv[j];\n      }\n    }\n  }\n}",
            "bins.fill(0);\n  //std::cout << s.size() << std::endl;\n  for (size_t i = 0; i < s.size(); ++i) {\n    char first_letter = std::tolower(s[i][0]);\n    //std::cout << first_letter << std::endl;\n    bins[first_letter - 'a']++;\n  }\n}",
            "auto rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n\n    std::vector<int> firstLetters;\n    for(auto const& str : s) {\n        firstLetters.push_back((str[0] - 'a'));\n    }\n\n    MPI_Scatter(firstLetters.data(), firstLetters.size(), MPI_INT, bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "auto rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (rank == 0) {\n        auto n = s.size();\n        bins.fill(0);\n\n        for (auto i = 0; i < n; ++i) {\n            bins[s[i][0] - 'a'] += 1;\n        }\n    } else {\n        auto bins_send = bins;\n\n        MPI_Send(&bins_send, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        std::vector<std::array<size_t, 26>> bins_recv(MPI_COMM_WORLD->size - 1);\n\n        for (auto i = 1; i < MPI_COMM_WORLD->size; ++i) {\n            MPI_Status status;\n            MPI_Recv(&bins_recv[i - 1], 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n\n        for (auto i = 0; i < bins_recv.size(); ++i) {\n            for (auto j = 0; j < 26; ++j) {\n                bins[j] += bins_recv[i][j];\n            }\n        }\n    }\n}",
            "bins.fill(0);\n    // TODO\n}",
            "// YOUR CODE HERE\n}",
            "int rank, size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t// Each rank computes a subset of bins\n\tsize_t range = s.size() / size;\n\tstd::vector<std::string> local(s.begin() + range * rank, s.begin() + range * (rank + 1));\n\t// Count each local vector\n\tstd::array<size_t, 26> localBins;\n\tfor (auto const& i : local) {\n\t\tlocalBins[i[0] - 'a']++;\n\t}\n\t// Send to 0\n\tMPI_Reduce(\n\t\tlocalBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD\n\t);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (std::string const &str: s) {\n    if (str.length() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n}",
            "int num_tasks, task_id;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_tasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &task_id);\n\n    if (task_id == 0) {\n\n        for (int task_num = 1; task_num < num_tasks; ++task_num) {\n\n            std::array<size_t, 26> other_bins{};\n\n            MPI_Status status;\n            MPI_Recv(other_bins.data(), 26, MPI_UNSIGNED_LONG, task_num, 0, MPI_COMM_WORLD, &status);\n\n            for (size_t i = 0; i < 26; ++i) {\n                bins[i] += other_bins[i];\n            }\n        }\n    } else {\n\n        std::array<size_t, 26> local_bins{};\n\n        for (auto const& str : s) {\n            ++local_bins[str[0] - 'a'];\n        }\n\n        MPI_Send(local_bins.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// 1. get size of communicator, and rank\n  int comm_size = 0;\n  int rank = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // 2. partition work with MPI\n  size_t work_per_rank = s.size() / comm_size;\n  size_t remainder = s.size() % comm_size;\n  size_t start = rank * work_per_rank + std::min(rank, remainder);\n  size_t end = start + work_per_rank + (rank < remainder? 1 : 0);\n\n  // 3. local copy of s\n  std::vector<std::string> local_s(s.begin() + start, s.begin() + end);\n\n  // 4. local bins\n  std::array<size_t, 26> local_bins = {};\n\n  // 5. compute local bins\n  for (auto const& s : local_s) {\n    local_bins[s[0] - 'a']++;\n  }\n\n  // 6. merge local bins with bins on rank 0\n  if (rank == 0) {\n    for (int rank = 1; rank < comm_size; rank++) {\n      MPI_Recv(local_bins.data(), 26, MPI_INT, rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (size_t i = 0; i < 26; i++) {\n      bins[i] += local_bins[i];\n    }\n  } else {\n    MPI_Send(local_bins.data(), 26, MPI_INT, 0, 1, MPI_COMM_WORLD);\n  }\n}",
            "size_t numRanks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t length = s.size();\n  size_t lengthOfS = length/numRanks;\n\n  size_t start = lengthOfS * rank;\n  size_t end = lengthOfS * (rank+1);\n\n  if(rank == numRanks-1){\n    end = length;\n  }\n\n  std::array<size_t, 26> localBins{};\n\n  for(size_t i = start; i < end; ++i){\n    std::string currString = s[i];\n    char firstLetter = std::tolower(currString[0]);\n    localBins[firstLetter - 'a']++;\n  }\n\n  // Use MPI to aggregate local bins to master\n  MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int length = s.size();\n  int chunk_size = length/size;\n  int remainder = length%size;\n  if (rank==0) {\n    std::vector<std::string> chunks(size);\n    std::vector<int> count(size);\n    for (int i=0; i<size-1; i++) {\n      chunks[i].reserve(chunk_size);\n      for (int j=0; j<chunk_size; j++)\n        chunks[i].push_back(s[i*chunk_size + j]);\n      count[i] = chunk_size;\n    }\n    if (remainder) {\n      chunks[size-1].reserve(chunk_size+remainder);\n      for (int j=0; j<chunk_size+remainder; j++)\n        chunks[size-1].push_back(s[(size-1)*chunk_size+j]);\n      count[size-1] = chunk_size+remainder;\n    }\n    std::vector<std::array<size_t, 26>> all_bins(size);\n    for (int i=1; i<size; i++) {\n      MPI_Send(&chunks[i], count[i], MPI_CHAR, i, 0, MPI_COMM_WORLD);\n      MPI_Recv(&all_bins[i], 26, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i=0; i<26; i++)\n      bins[i] = all_bins[0][i];\n    for (int i=1; i<size; i++)\n      for (int j=0; j<26; j++)\n        bins[j] += all_bins[i][j];\n  }\n  else {\n    std::vector<std::string> chunks(chunk_size);\n    for (int i=0; i<chunk_size; i++)\n      chunks[i].push_back(s[rank*chunk_size + i]);\n    MPI_Send(&chunks[0], chunk_size, MPI_CHAR, 0, 0, MPI_COMM_WORLD);\n    MPI_Recv(&bins, 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n  }\n}",
            "if (s.empty()) {\n    return;\n  }\n\n  std::vector<size_t> counts(s.size());\n\n  MPI_Scatter(&s[0], counts.size(), MPI_UNSIGNED_LONG_LONG, &counts[0], counts.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < counts.size(); i++) {\n    ++bins[static_cast<size_t>(counts[i] - 'a')];\n  }\n\n  MPI_Gather(&bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, &bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int const root = 0;\n\n   size_t const numWords = s.size();\n   size_t const chunk = numWords / MPI_COMM_WORLD->Get_size();\n\n   std::array<size_t, 26> counts = {0};\n   for (size_t i = 0; i < numWords; ++i) {\n      char const letter = s[i][0];\n      if (std::isalpha(letter)) {\n         ++counts[letter - 'a'];\n      }\n   }\n   MPI_Reduce(counts.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, root, MPI_COMM_WORLD);\n\n   if (MPI_COMM_WORLD->Get_rank() == root) {\n      for (size_t i = 0; i < 26; ++i) {\n         bins[i] = bins[i] / MPI_COMM_WORLD->Get_size();\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "int const myRank{MPI_Rank(MPI_COMM_WORLD)};\n  int const numRanks{MPI_Size(MPI_COMM_WORLD)};\n\n  // 1. Initialize all of the bins to zero\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // 2. Count letters from strings in the vector s and store the results in bins\n  for (auto &str : s) {\n    bins[str[0] - 'a']++;\n  }\n\n  // 3. Sum the values across all of the ranks.\n  std::vector<size_t> sumBins(bins.size());\n  MPI_Reduce(bins.data(), sumBins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // 4. Use the result on rank 0 to update the contents of bins.\n  // This is the only rank that needs to do this, since the result is the same on all ranks.\n  if (myRank == 0) {\n    std::copy(sumBins.begin(), sumBins.end(), bins.begin());\n  }\n}",
            "if (s.size() == 0) {\n    return;\n  }\n\n  // Get the total number of strings\n  size_t n = 0;\n  for (auto str : s) {\n    n += str.size();\n  }\n  // Allocate a buffer for the strings\n  char *strings = new char[n + 1];\n  // Copy the strings into the buffer\n  size_t i = 0;\n  for (auto str : s) {\n    for (char ch : str) {\n      strings[i++] = tolower(ch);\n    }\n  }\n  strings[i] = '\\0';\n\n  // Use an array of integers to store the result\n  int *result = new int[26];\n\n  // Use MPI to split up the strings evenly between the processes\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // Calculate the number of strings each process gets\n  int nPerRank = (n + size - 1) / size;\n  // Calculate the index of the first string that this process gets\n  int first = rank * nPerRank;\n  // Calculate the index of the last string that this process gets\n  int last = (rank + 1) * nPerRank - 1;\n  if (rank == size - 1) {\n    // If this is the last process,\n    // Calculate the index of the last string that this process gets\n    last = n - 1;\n  }\n\n  // Find the number of strings that each letter starts with\n  for (int i = first; i <= last; ++i) {\n    if (strings[i] >= 'a' && strings[i] <= 'z') {\n      ++result[strings[i] - 'a'];\n    }\n  }\n\n  // Sum the result arrays from each process\n  MPI_Reduce(result, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 26; ++i) {\n      bins[i] += bins[i - 1];\n    }\n  }\n\n  // Clean up\n  delete[] strings;\n  delete[] result;\n}",
            "size_t count = s.size();\n\n    // calculate number of strings in each letter\n    for(char c='a'; c<='z'; c++) {\n\n        for(std::string const& str : s) {\n            if (str.empty()) continue;\n            if (str[0] == c) {\n                count--;\n                break;\n            }\n        }\n\n        bins[c-'a'] = count;\n    }\n\n}",
            "// your code here\n    size_t len = s.size();\n    std::vector<size_t> local_bins(26);\n    for(int i=0;i<len;i++)\n    {\n        local_bins[int(s[i][0])-97]++;\n    }\n    //std::cout<<local_bins[0]<<\" \"<<local_bins[1]<<\" \"<<local_bins[2]<<\" \"<<local_bins[3]<<\" \"<<local_bins[4]<<\" \"<<local_bins[5]<<\" \"<<local_bins[6]<<std::endl;\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    //std::cout<<bins[0]<<\" \"<<bins[1]<<\" \"<<bins[2]<<\" \"<<bins[3]<<\" \"<<bins[4]<<\" \"<<bins[5]<<\" \"<<bins[6]<<std::endl;\n}",
            "/*\n    // the following code is for reference only\n    // in case you want to implement it yourself\n\n    // each MPI rank has its own copy of s\n    std::vector<std::string> s_rank;\n    for (auto &str : s)\n        s_rank.push_back(str);\n\n    // count the number of strings in s that start with each letter\n    for (auto &letter : s_rank) {\n        if (letter[0] >= 'a' && letter[0] <= 'z')\n            bins[letter[0] - 'a']++;\n    }\n    */\n\n    std::vector<size_t> s_len;\n    for (auto &str : s)\n        s_len.push_back(str.size());\n\n    // calculate the starting position of each rank in s\n    std::vector<int> starts(s_len.size());\n    int len = s_len.size();\n    MPI_Allgather(&len, 1, MPI_INT, starts.data(), 1, MPI_INT, MPI_COMM_WORLD);\n    std::partial_sum(starts.begin(), starts.end(), starts.begin());\n    std::vector<int> recv_starts(len);\n    MPI_Allgather(starts.data(), len, MPI_INT, recv_starts.data(), len, MPI_INT, MPI_COMM_WORLD);\n\n    // find out which rank is responsible for each letter\n    std::vector<int> letter_ranks(26, 0);\n    for (size_t i = 0; i < len; i++) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            int c_idx = c - 'a';\n            // check if the current rank already has the letter\n            if (letter_ranks[c_idx] == 0) {\n                // find out which rank has the letter\n                for (size_t j = 0; j < 26; j++) {\n                    if (letter_ranks[j] == 0 && j!= c_idx) {\n                        // this rank has both letters\n                        // we will have to send the result of this letter's computation to rank letter_ranks[j]\n                        letter_ranks[j] = recv_starts[i];\n                        break;\n                    }\n                }\n            }\n        }\n    }\n\n    // get the total number of letters\n    int num_letters = 0;\n    MPI_Allreduce(&num_letters, &num_letters, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // get the result from the rank responsible for each letter\n    std::vector<size_t> recv_counts(len);\n    for (size_t i = 0; i < 26; i++) {\n        if (letter_ranks[i]!= 0) {\n            // only one rank will receive the result\n            recv_counts[letter_ranks[i]] = bins[i];\n            // clear bins[i] so that we don't count the letter twice\n            bins[i] = 0;\n        }\n    }\n    MPI_Allgather(recv_counts.data(), len, MPI_INT, recv_counts.data(), len, MPI_INT, MPI_COMM_WORLD);\n\n    // accumulate the results from different ranks\n    size_t prev_recv_count = 0;\n    for (size_t i = 0; i < len; i++) {\n        size_t cur_recv_count = recv_counts[i];\n        MPI_Scan(&cur_recv_count, &prev_recv_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n        bins[i] = prev_recv_count;\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<size_t> local_bins(26, 0);\n    for (auto const& str : s) {\n        ++local_bins[static_cast<unsigned char>(str[0]) - 'a'];\n    }\n\n    std::vector<size_t> local_bins_per_rank(26, 0);\n    if (rank == 0) {\n        local_bins_per_rank = local_bins;\n    } else {\n        local_bins_per_rank.resize(26);\n    }\n\n    MPI_Reduce(local_bins.data(), local_bins_per_rank.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::transform(local_bins_per_rank.begin(), local_bins_per_rank.end(), bins.begin(), [](size_t x) { return static_cast<size_t>(x); });\n    }\n}",
            "// Your code here.\n}",
            "// TODO: implement this\n  int n=s.size();\n  int numprocs,myrank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  std::vector<int> localcounts(26);\n  for(int i=0; i<n; i++){\n    localcounts[s[i][0]-'a']++;\n  }\n  std::vector<int> globalcounts(26,0);\n  MPI_Allreduce(localcounts.data(), globalcounts.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  for(int i=0; i<26; i++){\n    bins[i]=globalcounts[i];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const& str : s) {\n    auto const& ch = str[0];\n    if (ch >= 'a' && ch <= 'z') {\n      ++bins[ch - 'a'];\n    }\n  }\n}",
            "// Fill in here\n}",
            "//TODO\n}",
            "// get number of elements in the vector\n    size_t n = s.size();\n    int rank = 0;\n    int world_size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    // calculate number of elements each rank processes\n    size_t n_per_rank = n/world_size;\n    if (rank == world_size - 1) {\n        n_per_rank += n % world_size;\n    }\n    size_t start = n_per_rank * rank;\n    size_t end = n_per_rank * (rank+1);\n    // create temporary array to store local results\n    std::array<size_t, 26> tmp;\n    std::fill(tmp.begin(), tmp.end(), 0);\n    for (size_t i=start; i<end; ++i) {\n        std::string str = s[i];\n        char c = str[0];\n        if ('a' <= c && c <= 'z') {\n            tmp[c-'a']++;\n        }\n    }\n    // gather all results on rank 0\n    MPI_Gather(tmp.data(), 26, MPI_UNSIGNED_LONG, bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    // on rank 0, update bins\n    if (rank == 0) {\n        for (size_t i=1; i<world_size; ++i) {\n            for (size_t j=0; j<26; ++j) {\n                bins[j] += bins[j+26*i];\n            }\n        }\n    }\n}",
            "// TODO: implement me!\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int total_length = s.size();\n  int local_length = total_length / world_size;\n\n  int last_local_length = total_length % world_size;\n  int first_local_index = rank * local_length;\n\n  //  int first_local_index = rank * local_length;\n  //  int last_local_index = (rank+1) * local_length;\n  int last_local_index = first_local_index + local_length;\n  if (rank == world_size - 1) {\n    last_local_index += last_local_length;\n  }\n\n  std::vector<std::string> local_s = std::vector<std::string>(s.begin() + first_local_index, s.begin() + last_local_index);\n\n  std::map<char, int> local_counts;\n\n  for (auto const& string : local_s) {\n    for (char letter : string) {\n      if (letter >= 'a' && letter <= 'z') {\n        local_counts[letter]++;\n      }\n    }\n  }\n\n  std::map<char, int> total_counts;\n  MPI_Reduce(&local_counts, &total_counts, local_counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 26; i++) {\n      bins[i] = total_counts[static_cast<char>(i + 'a')];\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int comm_sz;\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int count = s.size() / comm_sz;\n  int remainder = s.size() % comm_sz;\n\n  if (rank == 0) {\n    int offset = 0;\n    for (int i = 0; i < comm_sz; ++i) {\n      MPI_Send(&s[offset], count + (i < remainder? 1 : 0), MPI_CHAR, i, 0, MPI_COMM_WORLD);\n      offset += count + (i < remainder? 1 : 0);\n    }\n  } else {\n    MPI_Status status;\n    MPI_Recv(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, &status);\n  }\n\n  for (size_t i = 0; i < 26; ++i)\n    bins[i] = 0;\n\n  int chars_count = s.size() / comm_sz;\n\n  for (size_t i = rank * chars_count; i < (rank + 1) * chars_count; ++i)\n    ++bins[s[i] - 'a'];\n\n  MPI_Gather(bins.data(), 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int worldSize, worldRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n    std::vector<size_t> localBins(26);\n    size_t size = s.size();\n    if (size > 0) {\n        size_t stride = size / worldSize;\n        for (size_t i = 0; i < size; ++i) {\n            if (i % stride == 0 && i > 0) {\n                MPI_Status status;\n                MPI_Recv(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, worldRank - 1, 0, MPI_COMM_WORLD, &status);\n            }\n            localBins[s[i][0] - 'a']++;\n        }\n    }\n    MPI_Send(localBins.data(), localBins.size(), MPI_UNSIGNED_LONG_LONG, worldRank + 1, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n}",
            "// 1. Count the number of strings starting with each letter\n    std::array<size_t, 26> localBins;\n    for (auto const& s : s) {\n        localBins[s[0] - 'a']++;\n    }\n\n    // 2. Reduce across all ranks\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (size > 1) {\n        std::vector<size_t> recvcounts(size, 0);\n        std::vector<size_t> displacements(size, 0);\n        MPI_Gather(&localBins, 26, MPI_UNSIGNED, recvcounts.data(), 26, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n        if (rank == 0) {\n            for (size_t i = 1; i < size; i++) {\n                displacements[i] = displacements[i - 1] + recvcounts[i - 1];\n            }\n        }\n        MPI_Bcast(displacements.data(), size, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n        MPI_Gatherv(&localBins, 26, MPI_UNSIGNED, recvcounts.data(), recvcounts.data(), displacements.data(), MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n\n        if (rank == 0) {\n            for (size_t i = 1; i < size; i++) {\n                for (size_t j = 0; j < 26; j++) {\n                    recvcounts[j] += recvcounts[i];\n                }\n            }\n            for (size_t i = 0; i < 26; i++) {\n                localBins[i] = recvcounts[i];\n            }\n        }\n    }\n\n    // 3. Store result in bins on rank 0\n    MPI_Scatter(localBins.data(), 26, MPI_UNSIGNED, bins.data(), 26, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "// Your code goes here\n}",
            "// TODO: implement me.\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<std::string> str_s(s.begin(), s.end());\n    std::vector<std::string> str_s_split(s.begin(), s.end());\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Send(str_s.data(), str_s.size(), MPI_CHAR, i, 1, MPI_COMM_WORLD);\n            MPI_Send(str_s_split.data(), str_s_split.size(), MPI_CHAR, i, 2, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Status status;\n        MPI_Recv(str_s.data(), str_s.size(), MPI_CHAR, 0, 1, MPI_COMM_WORLD, &status);\n        MPI_Recv(str_s_split.data(), str_s_split.size(), MPI_CHAR, 0, 2, MPI_COMM_WORLD, &status);\n    }\n\n    int str_s_size = str_s.size() / size;\n    int str_s_size_rest = str_s.size() - str_s_size * size;\n    for (int i = 0; i < str_s.size(); ++i) {\n        if (i % size!= rank) {\n            str_s.erase(str_s.begin() + i);\n            --i;\n        }\n    }\n\n    std::vector<std::string> str_s_split_rank;\n    if (rank < str_s_size_rest) {\n        str_s_split_rank.resize(str_s_size + 1);\n        for (int i = rank * (str_s_size + 1); i < (rank + 1) * (str_s_size + 1); ++i) {\n            str_s_split_rank[i - rank * (str_s_size + 1)] = str_s_split[i];\n        }\n    } else {\n        str_s_split_rank.resize(str_s_size);\n        for (int i = rank * str_s_size; i < (rank + 1) * str_s_size; ++i) {\n            str_s_split_rank[i - rank * str_s_size] = str_s_split[i];\n        }\n    }\n\n    for (auto const& str : str_s_split_rank) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            bins[str[0] - 'a']++;\n        }\n    }\n\n    if (rank == 0) {\n        MPI_Status status;\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, i, 3, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, 3, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code goes here.\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if(rank == 0){\n        std::string word = \"code\";\n        for(int i = 0; i < word.size(); i++){\n            std::string sub = word.substr(i,1);\n            bins[sub[0] - 'a']++;\n        }\n    }\n    MPI_Scatter(bins.data(), 26, MPI_INT, bins.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_strings = s.size();\n\n  // Partition the strings across the nodes\n  std::vector<int> partitions(size);\n  int num_strings_per_rank = (num_strings + size - 1) / size;\n  partitions[rank] = num_strings_per_rank;\n  for (int i = 0; i < size - 1; i++) {\n    partitions[i + 1] += partitions[i];\n  }\n\n  // Compute the start index in s for each rank\n  std::vector<int> start_indices(size);\n  if (rank == 0) {\n    start_indices[0] = 0;\n  } else {\n    start_indices[rank] = partitions[rank - 1];\n  }\n  for (int i = 0; i < size - 1; i++) {\n    start_indices[i + 1] = partitions[i + 1] + start_indices[i];\n  }\n\n  // Compute the number of strings that start with each letter\n  std::array<size_t, 26> counts = {};\n  for (int i = start_indices[rank]; i < num_strings && i < start_indices[rank] + num_strings_per_rank; i++) {\n    counts[s[i][0] - 'a']++;\n  }\n\n  // Reduce counts from all ranks to rank 0\n  MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int myRank, commSize;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n\n  int div = s.size() / commSize;\n  int rem = s.size() % commSize;\n\n  if (myRank < rem) {\n    size_t l = div + 1;\n    size_t r = div * (myRank + 1) + 1;\n    for (size_t i = l; i < r; ++i) {\n      size_t letterIndex = s[i][0] - 'a';\n      ++bins[letterIndex];\n    }\n  } else {\n    size_t l = div * (myRank - rem) + rem;\n    size_t r = div * (myRank + 1) + rem;\n    for (size_t i = l; i < r; ++i) {\n      size_t letterIndex = s[i][0] - 'a';\n      ++bins[letterIndex];\n    }\n  }\n\n  std::vector<std::array<size_t, 26>> local_bins(26, std::array<size_t, 26>());\n\n  MPI_Gather(&bins, 26, MPI_UNSIGNED_LONG, &local_bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (myRank == 0) {\n    for (size_t i = 0; i < 26; ++i) {\n      for (size_t j = 0; j < commSize; ++j) {\n        bins[i] += local_bins[j][i];\n      }\n    }\n  }\n}",
            "// Your code here\n}",
            "/* TODO: Your code here */\n  for (char c = 'a'; c <= 'z'; ++c) {\n    bins[c - 'a'] = std::count_if(s.begin(), s.end(), [=](std::string const& str) {\n      return str.find(c) == 0;\n    });\n  }\n}",
            "// TODO\n}",
            "int rank, world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int even_or_odd = rank%2;\n  int my_start = (s.size()/world_size)*rank;\n  int my_end = (s.size()/world_size)*(rank+1);\n\n  std::array<int, 26> my_bins;\n\n  if(even_or_odd == 0) {\n    for(int i = my_start; i < my_end; ++i) {\n      ++my_bins[s[i][0]-'a'];\n    }\n  }\n  MPI_Reduce(my_bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO\n    int rank;\n    int nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    int size = s.size();\n    int local_size = (size + nproc - 1) / nproc;\n    int start = rank * local_size;\n    int end = std::min(start + local_size, size);\n\n    std::array<int, 26> counts;\n    for (auto& i : counts) i = 0;\n    for (int i = start; i < end; ++i) {\n        int index = s[i][0] - 'a';\n        ++counts[index];\n    }\n\n    std::vector<int> sendcounts(nproc);\n    std::vector<int> displs(nproc);\n    for (int i = 0; i < nproc; ++i) {\n        sendcounts[i] = counts[i];\n        displs[i] = i * 26;\n    }\n\n    std::vector<int> recvcounts(nproc);\n    std::vector<int> recvdispls(nproc);\n    MPI_Alltoall(&sendcounts[0], 1, MPI_INT, &recvcounts[0], 1, MPI_INT, MPI_COMM_WORLD);\n    int global_size = std::accumulate(recvcounts.begin(), recvcounts.end(), 0);\n    int global_displs = 0;\n    for (int i = 0; i < rank; ++i) {\n        global_displs += recvcounts[i];\n    }\n\n    std::vector<int> local_counts(26, 0);\n    MPI_Alltoallv(&counts[0], &sendcounts[0], &displs[0], MPI_INT,\n                  &local_counts[0], &recvcounts[0], &recvdispls[0], MPI_INT,\n                  MPI_COMM_WORLD);\n\n    MPI_Reduce(&local_counts[0], &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, nRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    // Count the strings starting with each letter.\n    std::array<size_t, 26> letterCounts = {};\n    for (std::string const& word : s) {\n        ++letterCounts[word[0] - 'a'];\n    }\n\n    // Gather the counts from all processes.\n    std::array<size_t, 26> globalLetterCounts = {};\n    MPI_Gather(letterCounts.data(), letterCounts.size(), MPI_UNSIGNED_LONG, globalLetterCounts.data(), letterCounts.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // Only rank 0 will store the result.\n    if (rank == 0) {\n        // Copy the results into the output array.\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = globalLetterCounts[i];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int elements_per_rank = s.size() / world_size;\n    int remainder = s.size() % world_size;\n    if (rank < remainder) {\n        elements_per_rank++;\n    }\n\n    size_t start = rank * elements_per_rank;\n    size_t end = (rank + 1) * elements_per_rank;\n\n    if (rank == 0) {\n        end = end + remainder;\n    }\n\n    std::array<size_t, 26> local_bins{};\n    for (size_t i = start; i < end; ++i) {\n        if (!s[i].empty()) {\n            local_bins[static_cast<size_t>(s[i].at(0) - 'a')]++;\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// YOUR CODE HERE\n    // The code below uses a for-loop to check each character of every string.\n    // Complete this function using MPI to parallelize the computation.\n    // Your function should not modify `bins` on any rank but rank 0.\n    for (int i = 0; i < s.size(); i++) {\n        for (int j = 0; j < s[i].size(); j++) {\n            char c = s[i][j];\n            int rank = j % 2;\n            if (c >= 'a' && c <= 'z') {\n                bins[c - 'a'] = bins[c - 'a'] + 1;\n            }\n        }\n    }\n}",
            "// size of s\n  size_t n = s.size();\n\n  // create the map which maps a letter to its count\n  std::array<size_t, 26> firstLetterCounts;\n  for(size_t i = 0; i < 26; ++i) {\n    firstLetterCounts[i] = 0;\n  }\n\n  // count the number of strings in `s` that start with each letter in the alphabet\n  for(size_t i = 0; i < n; ++i) {\n    if(s[i].size() > 0) {\n      // get the first letter in the string, convert to lowercase, and then get its index\n      auto firstLetter = static_cast<int>(s[i][0]);\n      if(firstLetter >= 65 && firstLetter <= 90) {\n        // convert to lowercase\n        firstLetter += 32;\n      }\n      // count the letter in the array `firstLetterCounts`\n      ++firstLetterCounts[firstLetter];\n    }\n  }\n\n  // send the counts to each rank\n  MPI_Allreduce(firstLetterCounts.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO: compute firstLetterCounts\n    size_t length = s.size();\n    int nproc = 0, rank = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int block_size = length / nproc;\n    if (block_size * nproc < length) {\n        block_size++;\n    }\n    int local_bin[26] = {0};\n    int loc_count = 0;\n    int rem_count = 0;\n\n    for (size_t i = rank * block_size; i < (rank + 1) * block_size && i < length; i++) {\n        if (isalpha(s[i][0])) {\n            if (islower(s[i][0])) {\n                local_bin[s[i][0] - 'a']++;\n            } else {\n                loc_count++;\n            }\n        } else {\n            loc_count++;\n        }\n    }\n\n    MPI_Reduce(local_bin, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Reduce(&loc_count, &rem_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] += rem_count;\n        }\n    }\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int len = s.size();\n  int* s_lower = new int[len];\n  int* local_bins = new int[26];\n\n  // Count the number of lowercase letters\n  for (int i = 0; i < len; i++) {\n    s_lower[i] = tolower(s[i][0]);\n  }\n\n  // Use MPI to get the rank of each lowercase letter\n  std::vector<int> ranks(26, 0);\n  for (int i = 0; i < len; i++) {\n    ranks[s_lower[i]]++;\n  }\n\n  // Get the size of each bin\n  std::vector<int> size_of_bins(26, 0);\n  for (int i = 0; i < 26; i++) {\n    size_of_bins[i] = ranks[i] / size;\n  }\n  int remaining_ranks = ranks[25] % size;\n  if (rank == 0) {\n    for (int i = 24; i >= 0; i--) {\n      if (remaining_ranks > 0) {\n        size_of_bins[i]++;\n        remaining_ranks--;\n      }\n    }\n  }\n\n  // Compute the starting index of each bin\n  std::vector<int> start_of_bins(26, 0);\n  if (rank > 0) {\n    for (int i = 0; i < 25; i++) {\n      start_of_bins[i] = size_of_bins[i];\n    }\n    start_of_bins[25] = size_of_bins[25] + remaining_ranks;\n  } else {\n    start_of_bins[25] = size_of_bins[25];\n  }\n\n  // Each rank counts the number of lowercase letters that start with the letters of its bin\n  int count = 0;\n  for (int i = start_of_bins[rank]; i < start_of_bins[rank] + size_of_bins[rank]; i++) {\n    if (s_lower[i] == tolower(rank)) {\n      local_bins[rank]++;\n    }\n  }\n\n  // Sum up the bins from each rank\n  MPI_Reduce(local_bins, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  delete[] s_lower;\n  delete[] local_bins;\n}",
            "// TODO: your code here\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // local work\n    size_t localBins[26] = {0};\n    for (std::string const& str : s) {\n        ++localBins[str[0] - 'a'];\n    }\n\n    // send results to other ranks\n    std::vector<size_t> results(26);\n    MPI_Gather(localBins, 26, MPI_UNSIGNED_LONG, results.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // compute global results\n    if (rank == 0) {\n        std::array<size_t, 26> globalBins = {};\n        for (size_t i = 0; i < size; ++i) {\n            for (size_t j = 0; j < 26; ++j) {\n                globalBins[j] += results[i * 26 + j];\n            }\n        }\n\n        bins = globalBins;\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<std::string> localS = s;\n\n    // Divide the work\n    int stringCount = localS.size();\n    int chunkSize = stringCount / size;\n    int remainder = stringCount % size;\n\n    int start, end;\n    start = rank * chunkSize;\n    end = start + chunkSize;\n\n    // For the last rank, account for remainder\n    if (rank == size - 1) {\n        end += remainder;\n    }\n\n    int localSum = 0;\n    for (size_t i = start; i < end; ++i) {\n        if (std::isalpha(localS[i][0])) {\n            ++localSum;\n        }\n    }\n\n    // Sum up all of the local sums\n    int globalSum = 0;\n    MPI_Reduce(&localSum, &globalSum, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Assign the global sum to the appropriate index in the bins array\n    if (rank == 0) {\n        for (int i = 0; i < size; ++i) {\n            int start = i * chunkSize;\n            int end = start + chunkSize;\n\n            // Account for the remainder\n            if (i == size - 1) {\n                end += remainder;\n            }\n\n            for (int j = start; j < end; ++j) {\n                if (std::isalpha(localS[j][0])) {\n                    ++bins[localS[j][0] - 'a'];\n                }\n            }\n        }\n    }\n}",
            "// 0. Count up the number of strings that start with each letter and store in bins.\n    // Hint: look at the example code below.\n    size_t n = s.size();\n    std::string letter;\n    for (size_t i = 0; i < n; ++i) {\n        letter = s[i].substr(0, 1);\n        if (letter.compare(\"a\") == 0)\n            ++bins[0];\n        else if (letter.compare(\"b\") == 0)\n            ++bins[1];\n        else if (letter.compare(\"c\") == 0)\n            ++bins[2];\n        else if (letter.compare(\"d\") == 0)\n            ++bins[3];\n        else if (letter.compare(\"e\") == 0)\n            ++bins[4];\n        else if (letter.compare(\"f\") == 0)\n            ++bins[5];\n        else if (letter.compare(\"g\") == 0)\n            ++bins[6];\n        else if (letter.compare(\"h\") == 0)\n            ++bins[7];\n        else if (letter.compare(\"i\") == 0)\n            ++bins[8];\n        else if (letter.compare(\"j\") == 0)\n            ++bins[9];\n        else if (letter.compare(\"k\") == 0)\n            ++bins[10];\n        else if (letter.compare(\"l\") == 0)\n            ++bins[11];\n        else if (letter.compare(\"m\") == 0)\n            ++bins[12];\n        else if (letter.compare(\"n\") == 0)\n            ++bins[13];\n        else if (letter.compare(\"o\") == 0)\n            ++bins[14];\n        else if (letter.compare(\"p\") == 0)\n            ++bins[15];\n        else if (letter.compare(\"q\") == 0)\n            ++bins[16];\n        else if (letter.compare(\"r\") == 0)\n            ++bins[17];\n        else if (letter.compare(\"s\") == 0)\n            ++bins[18];\n        else if (letter.compare(\"t\") == 0)\n            ++bins[19];\n        else if (letter.compare(\"u\") == 0)\n            ++bins[20];\n        else if (letter.compare(\"v\") == 0)\n            ++bins[21];\n        else if (letter.compare(\"w\") == 0)\n            ++bins[22];\n        else if (letter.compare(\"x\") == 0)\n            ++bins[23];\n        else if (letter.compare(\"y\") == 0)\n            ++bins[24];\n        else if (letter.compare(\"z\") == 0)\n            ++bins[25];\n    }\n\n    // 1. Each process sends the bins array to rank 0.\n    //    Hint: look at the example code below.\n\n    // 2. Rank 0 adds up the bins arrays on each process.\n    //    Hint: look at the example code below.\n\n    // 3. Rank 0 sends the results to each process.\n    //    Hint: look at the example code below.\n\n    // 4. Each process overwrites its own bins array with the bins array sent by rank 0.\n    //    Hint: look at the example code below.\n}",
            "int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t chunk = s.size() / size;\n    size_t offset = rank * chunk;\n    size_t local_size = std::min(s.size() - offset, chunk);\n\n    std::vector<std::string> local_s(local_size);\n    std::copy(s.cbegin() + offset, s.cbegin() + offset + local_size, local_s.begin());\n\n    std::array<size_t, 26> local_bins = {0};\n\n    for (auto const& string : local_s) {\n        for (char c : string) {\n            if (c >= 'a' && c <= 'z') {\n                ++local_bins[c - 'a'];\n            }\n        }\n    }\n\n    std::array<size_t, 26> global_bins;\n    MPI_Reduce(local_bins.data(), global_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::copy(global_bins.begin(), global_bins.end(), bins.begin());\n    }\n}",
            "// TODO: Your code here\n}",
            "}",
            "// TODO: Your implementation here\n}",
            "// TODO: Fill in this function to do parallel histogramming\n\n  // Hint: s.size() can be divided evenly by the number of processes\n  const int N = s.size();\n  const int size = MPI_COMM_SIZE;\n  const int rank = MPI_COMM_RANK;\n  int counts[26];\n  // Initialize count array\n  for (int i = 0; i < 26; i++) counts[i] = 0;\n\n  // Calculate total count for each letter\n  for (int i = 0; i < N; i++) {\n    counts[s[i][0] - 'a']++;\n  }\n\n  // Calculate count for each letter for each rank\n  int total = 0;\n  for (int i = 0; i < 26; i++) {\n    int local_count = counts[i];\n    int global_count = 0;\n    MPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n      bins[i] = global_count;\n    }\n  }\n}",
            "// TODO: complete this function\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  // bins.size() = number of processes\n  // bins[i] = # of strings starting with 'a' + 'b'... 'z'\n\n  // create a new array of the same size on each process\n  std::array<size_t, 26> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  // local_bins will be updated with the correct counts when this function returns\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // count how many strings start with each letter\n  for (auto& str : s) {\n    if (str.size() > 0) {\n      local_bins[str[0] - 'a'] += 1;\n    }\n  }\n\n  // sum up the numbers of strings starting with each letter\n  std::array<size_t, 26> global_bins;\n  MPI_Reduce(local_bins.data(), global_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // assign bins to the global bins on rank 0\n  if (rank == 0) {\n    bins = global_bins;\n  }\n}",
            "/* MPI variables */\n\tint rank, size;\n\n\t/* initialize bins array */\n\tfor (size_t i = 0; i < bins.size(); ++i) {\n\t\tbins[i] = 0;\n\t}\n\n\t/* get the rank of the current process */\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\t/* get the size of the world */\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t/* get length of vector s */\n\tsize_t len = s.size();\n\n\t/* find the indices of the subset of strings which contain the current process's rank */\n\tstd::vector<size_t> indices;\n\tfor (size_t i = 0; i < len; ++i) {\n\t\tif (rank == 0) {\n\t\t\tstd::cout << s[i] << std::endl;\n\t\t}\n\t\tif (s[i][0] - 'a' == rank) {\n\t\t\tindices.push_back(i);\n\t\t}\n\t}\n\n\t/* distribute the size of the subset to each process */\n\tstd::vector<int> counts(size, 0);\n\tfor (size_t i = 0; i < indices.size(); ++i) {\n\t\tcounts[s[indices[i]][0] - 'a']++;\n\t}\n\n\t/* compute cumulative sum of counts in counts */\n\tint sum = 0;\n\tfor (size_t i = 0; i < counts.size(); ++i) {\n\t\tint tmp = counts[i];\n\t\tcounts[i] = sum;\n\t\tsum += tmp;\n\t}\n\n\t/* count the number of strings in the subset that start with the current process's rank's letter */\n\tint local_bins[26] = {0};\n\tfor (size_t i = 0; i < indices.size(); ++i) {\n\t\tint idx = s[indices[i]][0] - 'a';\n\t\tlocal_bins[idx]++;\n\t}\n\n\t/* gather the bins from each process */\n\tint g_bins[26] = {0};\n\tMPI_Allgatherv(local_bins, 26, MPI_INT, g_bins, counts.data(), counts.data() + 1, MPI_INT, MPI_COMM_WORLD);\n\n\t/* copy the result to bins */\n\tfor (size_t i = 0; i < bins.size(); ++i) {\n\t\tbins[i] = g_bins[i];\n\t}\n}",
            "// Compute the number of strings for each letter in s.\n  // Do so using MPI.\n\n  // MPI variables\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // The number of strings for each letter in s.\n  std::array<size_t, 26> local_bins{};\n\n  // Loop over the number of strings in s\n  for (size_t i = 0; i < s.size(); ++i) {\n\n    // Get the i'th string.\n    std::string const& ss = s[i];\n\n    // Compute the index of the first letter in the string.\n    char first_letter = ss[0];\n    size_t j = first_letter - 'a';\n\n    // Increment the number of strings for the first letter.\n    ++local_bins[j];\n  }\n\n  // Put the local count of strings for each letter in the global count of strings for each letter.\n  MPI_Reduce(&local_bins[0], &bins[0], 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* TODO: your code here */\n}",
            "// TODO\n}",
            "int nRanks;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        for (size_t i = 0; i < 26; i++) {\n            bins[i] = 0;\n        }\n    }\n    std::vector<size_t> localBins = bins;\n\n    for (auto const& str : s) {\n        char firstLetter = str[0];\n        char asciiCode = firstLetter - 'a';\n        localBins[asciiCode]++;\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (rank == 0) {\n      std::array<size_t, 26> localBins = { 0 };\n\n      // TODO: compute the localBin array\n      for (const auto &s_i : s) {\n         for (const auto &c_i : s_i) {\n            if ((c_i >= 'a') && (c_i <= 'z')) {\n               localBins[c_i - 'a'] += 1;\n            }\n         }\n      }\n\n      // Reduce the results to the root\n      MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n   else {\n      std::array<size_t, 26> localBins = { 0 };\n\n      // TODO: compute the localBin array\n      for (const auto &s_i : s) {\n         for (const auto &c_i : s_i) {\n            if ((c_i >= 'a') && (c_i <= 'z')) {\n               localBins[c_i - 'a'] += 1;\n            }\n         }\n      }\n\n      // Send result to root\n      MPI_Send(localBins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "std::array<int, 26> bin_counts;\n  bin_counts.fill(0);\n  for (auto const& x : s) {\n    ++bin_counts[x[0] - 'a'];\n  }\n  MPI_Reduce(bin_counts.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here.\n}",
            "int rank, numRanks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n  size_t len = s.size();\n  int blockSize = len / numRanks;\n\n  std::vector<std::string> localStrings = getStringsForRank(s, rank, numRanks, blockSize);\n  std::array<size_t, 26> localCounts = firstLetterCountsOnRank(localStrings);\n  MPI_Reduce(&localCounts, &bins, 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int n = MPI_COMM_SIZE;\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Count up the letters\n  for (auto const& e : s) {\n    if (!e.empty())\n      ++bins[e[0] - 'a'];\n  }\n\n  // Sum the counts\n  size_t sum = 0;\n  for (size_t i = 0; i < 26; ++i) {\n    size_t tmp = bins[i];\n    MPI_Allreduce(&tmp, &sum, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n    bins[i] = sum;\n  }\n}",
            "if (s.size() == 0) {\n        return;\n    }\n    int world_rank, world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int n = s.size();\n    int n_local = n / world_size;\n    if (world_rank == world_size - 1) {\n        n_local += n % world_size;\n    }\n    int n_start = world_rank * n_local;\n    std::vector<std::string> my_strings(n_local);\n    for (int i = n_start; i < n_start + n_local; ++i) {\n        my_strings[i - n_start] = s[i];\n    }\n\n    std::array<size_t, 26> my_bins;\n    for (int i = 0; i < my_strings.size(); ++i) {\n        my_bins[my_strings[i][0] - 'a'] += 1;\n    }\n\n    std::array<size_t, 26> global_bins;\n    MPI_Reduce(my_bins.data(), global_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = global_bins[i];\n        }\n    }\n}",
            "const size_t strSize = s.size();\n    std::vector<size_t> localCount(26, 0);\n    for (size_t i = 0; i < strSize; ++i) {\n        size_t index = s[i][0] - 'a';\n        ++localCount[index];\n    }\n    MPI_Gather(&localCount[0], 26, MPI_UNSIGNED_LONG, &bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "int myRank, numRanks;\n\tMPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n\tsize_t const numStrings = s.size();\n\t// Every rank has its own copy of the data. The input is not sorted.\n\t// To count the letters, we only care about the first character of each string.\n\t// So we assign each rank a contiguous chunk of the alphabet, and only\n\t// process the data in that chunk.\n\t// Example:\n\t// - 4 processes. First letter of each string is 'a'\n\t// - rank 0 will be responsible for processing the alphabet starting with 'a'\n\t// - rank 1 will be responsible for processing the alphabet starting with 'b'\n\t// - rank 2 will be responsible for processing the alphabet starting with 'c'\n\t// - rank 3 will be responsible for processing the alphabet starting with 'd'\n\t// So, each rank will count the number of strings that start with the first character in its alphabet.\n\t// (The number of strings that start with 'b' is equal to the number of strings that start with 'a' + 1)\n\t// So, each rank will write its result into the array bins starting at the index that corresponds to\n\t// the alphabet character that it is responsible for counting.\n\t//\n\t// This is done by:\n\t// - First, assigning each rank a starting index based on its rank\n\t// - Then, iterating through the alphabet and each rank will count the strings in s that\n\t//   start with each letter\n\t// - Finally, each rank sends its result to rank 0\n\t// - Note that rank 0 will only receive a result from each rank once\n\t// - The result from rank 0 will be the final result\n\n\tstd::vector<int> letterCounts(26, 0);\n\t// Store the counts of strings that start with each letter\n\tfor (auto const& str : s) {\n\t\t// Only the first character is used for counting\n\t\tletterCounts[static_cast<size_t>(str[0]) - 'a']++;\n\t}\n\n\tint const alphabetStartIndex = myRank * 26;\n\t// Count the strings that start with each letter in the alphabet\n\tfor (size_t i = 0; i < 26; i++) {\n\t\tletterCounts[i] += letterCounts[i - 1];\n\t}\n\tstd::vector<int> result;\n\t// Send the letter counts to rank 0\n\tif (myRank == 0) {\n\t\tfor (int rank = 0; rank < numRanks; rank++) {\n\t\t\tint letterCount;\n\t\t\tMPI_Recv(&letterCount, 1, MPI_INT, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t\tresult.push_back(letterCount);\n\t\t}\n\t\t// The counts are sent in reverse order because each rank sends a\n\t\t// result from the 'left' alphabet to rank 0. So, the leftmost\n\t\t// alphabet character is in the last index of result\n\t\tfor (size_t i = 0; i < 26; i++) {\n\t\t\tresult.push_back(letterCounts[25 - i]);\n\t\t}\n\t\tfor (size_t i = 0; i < 26; i++) {\n\t\t\tbins[i] = result[i + alphabetStartIndex];\n\t\t}\n\t} else {\n\t\tMPI_Send(letterCounts.data() + alphabetStartIndex, 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "// TODO: implement this function.\n\n  // Count the length of the vector s.\n  int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  // Check if the vector s is empty. If it is, do not continue.\n  if (s.size() == 0) {\n    return;\n  }\n\n  // Partition the vector s.\n  int part_size = (s.size() + nproc - 1) / nproc;\n  int part_start = part_size * rank;\n  int part_end = std::min(part_size * (rank + 1), s.size());\n  std::vector<std::string> s_local(s.begin() + part_start, s.begin() + part_end);\n\n  // Count the number of strings that start with each letter.\n  std::array<size_t, 26> local_bins;\n  for (auto& elem : s_local) {\n    int index = elem[0] - 'a';\n    local_bins[index]++;\n  }\n\n  // Get the total number of strings that start with each letter.\n  std::array<int, 26> total_bins;\n  MPI_Reduce(local_bins.data(), total_bins.data(), total_bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Store the total number of strings that start with each letter.\n  if (rank == 0) {\n    std::copy(total_bins.begin(), total_bins.end(), bins.begin());\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // local variables\n  size_t s_size = s.size();\n  size_t s_size_rank = (s_size + size - 1) / size;\n  size_t s_size_start = rank * s_size_rank;\n  size_t s_size_end = std::min((rank + 1) * s_size_rank, s_size);\n\n  // local variables to count first letter\n  size_t count = 0;\n\n  // count first letters in local copy of s\n  for (size_t i = s_size_start; i < s_size_end; ++i) {\n    std::string const& s_i = s[i];\n    if (!s_i.empty()) {\n      count += 1;\n    }\n  }\n\n  // reduce count to rank 0\n  MPI_Reduce(&count, &bins[0], 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: replace with your code here\n    int world_size, world_rank, name_len;\n    char name[MPI_MAX_PROCESSOR_NAME];\n\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Get_processor_name(name, &name_len);\n    std::cout << \"MPI Hello World from \" << name << \" rank \" << world_rank << \"\\n\";\n\n    if (world_rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n    int const num_chunks = s.size() / world_size;\n    int const remainder = s.size() % world_size;\n    int const chunk_offset = num_chunks * world_rank + std::min(remainder, world_rank);\n    int const chunk_size = num_chunks + (world_rank < remainder? 1 : 0);\n    std::vector<std::string> local_s(s.begin() + chunk_offset, s.begin() + chunk_offset + chunk_size);\n    std::array<size_t, 26> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    for (auto const& word : local_s) {\n        ++local_bins[word[0] - 'a'];\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), local_bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // We are using the fact that the alphabet is the lower case letters.\n    std::array<int, 26> counts = {};\n    // If the size is one, we have nothing to do\n    if (size <= 1) {\n        return;\n    }\n\n    // Counts how many strings start with each letter\n    for (std::string const& str : s) {\n        if (!str.empty()) {\n            ++counts[str[0] - 'a'];\n        }\n    }\n\n    // Compute the prefix sum of counts\n    std::array<int, 26> partial_sums = {};\n    partial_sums[0] = counts[0];\n    for (int i = 1; i < 26; ++i) {\n        partial_sums[i] = partial_sums[i - 1] + counts[i];\n    }\n\n    // Send partial sums to every processor\n    std::vector<int> counts_sends(26);\n    std::vector<int> partial_sums_sends(26);\n    for (int i = 0; i < 26; ++i) {\n        counts_sends[i] = counts[i];\n        partial_sums_sends[i] = partial_sums[i];\n    }\n    MPI_Scatter(counts_sends.data(), 26, MPI_INT, counts.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(partial_sums_sends.data(), 26, MPI_INT, partial_sums.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Each rank has a complete copy of s.\n    // Each rank needs to know how many strings start with each letter.\n    // Compute the partial sum on each rank.\n    std::array<int, 26> local_partial_sums = {};\n    for (int i = 0; i < 26; ++i) {\n        local_partial_sums[i] = partial_sums[i] - counts[i];\n    }\n\n    // Each rank only computes the counts for the letters that it owns.\n    // Send the counts for each letter to rank 0, which will then compute the prefix sum\n    std::vector<int> counts_recv(26);\n    MPI_Gather(counts.data(), 26, MPI_INT, counts_recv.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(local_partial_sums.data(), 26, MPI_INT, partial_sums.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Compute the prefix sum on rank 0, and send it to all ranks\n    std::array<int, 26> all_partial_sums = {};\n    if (rank == 0) {\n        // Compute the prefix sum on rank 0, and send it to all ranks\n        all_partial_sums[0] = partial_sums[0];\n        for (int i = 1; i < 26; ++i) {\n            all_partial_sums[i] = all_partial_sums[i - 1] + partial_sums[i];\n        }\n        MPI_Bcast(all_partial_sums.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Bcast(all_partial_sums.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n\n    // Compute the counts of each letter on rank 0, and send it to all ranks\n    std::vector<int> counts_all(26);\n    if (rank == 0) {\n        for (int i = 0; i < 26; ++i) {\n            counts_all[i] = counts_recv[i];\n        }\n    }\n    MPI_Scatter(counts_all.data(), 26, MPI_INT, counts.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Compute the result on each rank\n    bins = {};\n    for (int i = 0; i < 26; ++i) {\n        bins[i] = all_partial_sums[i] - partial_sums[i];\n    }\n}",
            "/* Your solution should go here  */\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int n = s.size();\n  int p_s, p_bins;\n  p_s = n / world_size;\n  p_bins = p_s + 1;\n  int r = n % world_size;\n\n  std::string *send_buf = new std::string[p_s];\n  std::array<size_t, 26> *recv_buf = new std::array<size_t, 26>[p_bins];\n  size_t start = p_s * world_rank;\n  for (int i = 0; i < p_s; i++) {\n    send_buf[i] = s[start + i];\n  }\n  MPI_Scatter(send_buf, p_s, MPI_CHAR, recv_buf, p_bins, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 26> my_bins;\n  for (int i = 0; i < p_s; i++) {\n    int val = recv_buf[i][0] - 'a';\n    if (val >= 0 && val < 26) {\n      my_bins[val] += 1;\n    }\n  }\n  MPI_Gather(&my_bins, 26, MPI_INT, bins, 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    for (int i = 1; i < world_size; i++) {\n      for (int j = 0; j < 26; j++) {\n        bins[j] += bins[j + 26];\n      }\n    }\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  delete[] send_buf;\n  delete[] recv_buf;\n}",
            "int rank, size;\n\n    // get rank and size of process grid\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int const total = static_cast<int>(s.size());\n    int const chunk = total / size;\n    int const remainder = total % size;\n    int const start = chunk * rank + std::min(rank, remainder);\n    int const end = start + chunk + (rank < remainder);\n    int const localCount = end - start;\n\n    std::array<size_t, 26> counts{};\n    // count occurences of first letters in local portion of s\n    for (std::string const& str : s) {\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            ++counts[str[0] - 'a'];\n        }\n    }\n\n    // reduce to get global counts\n    MPI_Allreduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TO DO\n}",
            "//TODO\n}",
            "// TODO: implement this function\n}",
            "if (s.empty()) {\n    return;\n  }\n  int mpi_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n  if (s.size() < mpi_size) {\n    throw std::runtime_error(\"Vector s is too small to be distributed across the ranks.\");\n  }\n  std::vector<size_t> local_bins;\n  local_bins.resize(26);\n  for (auto const& str : s) {\n    if (str.empty()) {\n      continue;\n    }\n    local_bins[str[0] - 'a']++;\n  }\n  std::vector<int> counts(mpi_size, 26);\n  MPI_Allgather(&local_bins[0], 26, MPI_UNSIGNED_LONG_LONG, counts.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n  for (int i = 0; i < 26; i++) {\n    bins[i] = counts[i];\n  }\n}",
            "// Put your code here\n}",
            "/* Compute the number of strings starting with each letter */\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    /* This line makes a copy of `s` on each rank */\n    std::vector<std::string> local_s = s;\n    int n = local_s.size();\n    int my_count;\n    for (int i = rank; i < n; i += nproc) {\n        if (local_s[i][0] >= 'a' && local_s[i][0] <= 'z') {\n            my_count++;\n        }\n    }\n    /* Do a reduction to compute the count of strings starting with each letter */\n    int total_count;\n    MPI_Reduce(&my_count, &total_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        /* Each rank computes the number of strings starting with each letter */\n        for (int i = 0; i < 26; ++i) {\n            int count = 0;\n            for (int j = 0; j < n; ++j) {\n                if (local_s[j][0] == i + 'a') {\n                    count++;\n                }\n            }\n            bins[i] = count;\n        }\n    }\n}",
            "int num_strings = s.size();\n  std::array<int, 26> counts;\n  counts.fill(0);\n  for(auto const& s : s) {\n    if(s.size() == 0) {\n      continue;\n    }\n    int idx = s[0] - 'a';\n    ++counts[idx];\n  }\n  for(int i = 0; i < 26; ++i) {\n    bins[i] = counts[i];\n  }\n}",
            "// TODO: your code goes here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count_per_rank = s.size() / size;\n  int remain_per_rank = s.size() % size;\n\n  int start = count_per_rank * rank + std::min(rank, remain_per_rank);\n  int end = start + count_per_rank + (rank < remain_per_rank);\n  auto local_vec = std::vector<std::string>(s.begin() + start, s.begin() + end);\n\n  std::vector<size_t> count_per_letter;\n  size_t total_count = 0;\n  for (size_t i = 0; i < local_vec.size(); ++i) {\n    int count = 0;\n    for (char c : local_vec[i]) {\n      ++count;\n    }\n    count_per_letter.push_back(count);\n    total_count += count;\n  }\n\n  MPI_Reduce(count_per_letter.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 26; ++i) {\n      bins[i] /= total_count;\n    }\n  }\n}",
            "if (s.size() < 10000) { // O(n) algorithm for small input\n    for (char c = 'a'; c <= 'z'; ++c) {\n      for (auto const& word : s) {\n        if (word[0] == c) {\n          ++bins[c - 'a'];\n          break;\n        }\n      }\n    }\n    return;\n  }\n\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Determine local range\n  int start = 0;\n  int end = 0;\n  int localSize = 0;\n  int localStart = 0;\n\n  if (rank == 0) {\n    localSize = s.size() / size;\n    localStart = localSize;\n    start = localSize * rank;\n    end = localSize * (rank + 1);\n  }\n\n  MPI_Bcast(&localSize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&localStart, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&start, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&end, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Find my portion of the input\n  std::vector<std::string> localVec;\n  if (rank < s.size() % size) {\n    localVec.push_back(s[localStart]);\n  }\n\n  // Distribute work\n  MPI_Scatter(s.data() + start, localSize, MPI_CHAR, localVec.data(), localSize, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n  // Process locally\n  std::array<size_t, 26> localBins;\n  for (char c = 'a'; c <= 'z'; ++c) {\n    for (auto const& word : localVec) {\n      if (word[0] == c) {\n        ++localBins[c - 'a'];\n        break;\n      }\n    }\n  }\n\n  // Aggregate results\n  MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "int const size = s.size();\n    int const rank = 0;\n\n    int const my_size = size / size;\n    int const left_over = size % size;\n\n    // The first letter in each string in the vector is a number from 0 to 25.\n    // Calculate the offset for this rank.\n    int my_offset = (rank * my_size) + std::min(rank, left_over);\n    int const last_rank = size - left_over;\n\n    // Count the number of times each letter is seen in the first part of the vector.\n    std::array<int, 26> counts;\n    for (size_t i = 0; i < counts.size(); ++i) {\n        counts[i] = 0;\n    }\n\n    for (int i = my_offset; i < my_offset + my_size; ++i) {\n        ++counts[s[i][0] - 'a'];\n    }\n\n    // Sum the counts from the other ranks.\n    int const right_side = my_offset + my_size;\n    int const left_side = my_offset;\n    int const right_most = last_rank * my_size;\n\n    int const right_side_offset = right_side - right_most;\n    int const left_side_offset = left_side - right_most;\n\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Add the counts for the last part of the vector.\n        for (int i = left_side_offset; i < left_side_offset + left_over; ++i) {\n            ++bins[s[i][0] - 'a'];\n        }\n    }\n}",
            "assert(bins.size() == 26);\n\n   // Your code goes here.\n\n   // This is how you can create a random string:\n   // static auto rnd = std::default_random_engine {};\n   // static auto dist = std::uniform_int_distribution<int> {0, 25};\n   // std::string str = std::string(100000, dist(rnd));\n\n   // std::vector<std::string> const s = {\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"};\n   // std::array<size_t, 26> bins;\n   // firstLetterCounts(s, bins);\n\n   // std::vector<std::string> const s = {\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"};\n   // std::array<size_t, 26> bins;\n   // firstLetterCounts(s, bins);\n\n   // std::vector<std::string> const s = {\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"};\n   // std::array<size_t, 26> bins;\n   // firstLetterCounts(s, bins);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Each rank will compute a different subset of the input strings.\n  std::vector<std::string> localStrings(s.size() / size);\n  for (size_t i = 0; i < localStrings.size(); i++) {\n    localStrings[i] = s[rank * localStrings.size() + i];\n  }\n\n  // Every rank will do its own computation.\n  for (char c = 'a'; c <= 'z'; c++) {\n    size_t count = 0;\n    for (auto const& s : localStrings) {\n      if (s[0] == c) {\n        count++;\n      }\n    }\n    MPI_Reduce(&count, &bins[c - 'a'], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  if (rank == 0) {\n    // Combine the results from every rank\n    std::vector<size_t> temp(bins);\n    MPI_Reduce(MPI_IN_PLACE, temp.data(), temp.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < bins.size(); i++) {\n      bins[i] = temp[i];\n    }\n  }\n}",
            "/* TODO */\n}",
            "if (s.size() < 1000) {\n    // Sequential implementation\n    for (auto const& w : s) {\n      bins[w[0] - 'a']++;\n    }\n    return;\n  }\n\n  // Your code here\n\n  return;\n}",
            "size_t const rank = getRank();\n  size_t const size = getSize();\n\n  // Each rank does its own count, but only rank 0 does the reduction.\n  std::array<size_t, 26> counts{};\n\n  for (auto const& word : s) {\n    counts[word[0] - 'a'] += 1;\n  }\n\n  // Rank 0 does the reduction.\n  if (rank == 0) {\n    // Initialize with 0.\n    for (size_t i = 0; i < counts.size(); ++i) {\n      bins[i] = 0;\n    }\n\n    // Add all the counts from the other ranks.\n    for (size_t i = 1; i < size; ++i) {\n      std::array<size_t, 26> otherCounts{};\n      receive(i, otherCounts);\n      for (size_t j = 0; j < counts.size(); ++j) {\n        bins[j] += otherCounts[j];\n      }\n    }\n\n    // Send the result to the other ranks.\n    for (size_t i = 1; i < size; ++i) {\n      send(i, bins);\n    }\n  } else {\n    // Send the counts to rank 0.\n    send(0, counts);\n  }\n}",
            "size_t n = s.size();\n  std::string const& s0 = s[0];\n  std::string const& s1 = s[n/2];\n  std::string const& s2 = s[n-1];\n  std::array<size_t, 26> counts;\n  for(size_t i = 0; i < 26; ++i) {\n    counts[i] = 0;\n  }\n  for(auto const& s : s) {\n    ++counts[static_cast<size_t>(s[0] - 'a')];\n  }\n  for(size_t i = 0; i < 26; ++i) {\n    bins[i] = counts[i];\n  }\n}",
            "// Your code goes here\n}",
            "// TODO: complete this function\n\n    int rank, worldSize;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n    int blockSize = s.size() / worldSize;\n    int remainder = s.size() % worldSize;\n\n    if (rank == 0) {\n        for (int i = 1; i < worldSize; ++i) {\n            MPI_Send(&s[i * blockSize + (i <= remainder? i : remainder)], blockSize, MPI_CHAR, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    std::array<char, blockSize> local_input;\n    MPI_Status status;\n\n    if (rank == 0) {\n        for (int i = 1; i < worldSize; ++i) {\n            MPI_Recv(local_input.data(), blockSize, MPI_CHAR, i, 0, MPI_COMM_WORLD, &status);\n\n            for (int j = 0; j < blockSize; ++j) {\n                ++bins[static_cast<int>(local_input[j] - 'a')];\n            }\n        }\n    } else {\n        MPI_Recv(local_input.data(), blockSize, MPI_CHAR, 0, 0, MPI_COMM_WORLD, &status);\n\n        for (int i = 0; i < blockSize; ++i) {\n            ++bins[static_cast<int>(local_input[i] - 'a')];\n        }\n\n        MPI_Send(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "const int rank = MPI::COMM_WORLD.Get_rank();\n  const int size = MPI::COMM_WORLD.Get_size();\n  const int n = s.size();\n  if(rank == 0) {\n    bins.fill(0);\n  }\n  const int chunks = n / size + 1;\n  int start = rank * chunks;\n  int end = start + chunks;\n  if(rank == size - 1) {\n    end = n;\n  }\n  int local_count = 0;\n  for(int i = start; i < end; ++i) {\n    if(i == n) {\n      break;\n    }\n    if(s[i][0] >= 'a' && s[i][0] <= 'z') {\n      ++local_count;\n    }\n  }\n  std::array<int, 26> local_bins;\n  local_bins.fill(0);\n  if(rank == 0) {\n    for(int i = 1; i < size; ++i) {\n      MPI::COMM_WORLD.Recv(local_bins.data(), local_bins.size(), MPI::INT, i, 0);\n      for(int j = 0; j < 26; ++j) {\n        bins[j] += local_bins[j];\n      }\n    }\n  } else {\n    MPI::COMM_WORLD.Send(local_count, 1, MPI::INT, 0, 0);\n  }\n  if(rank == 0) {\n    for(int i = 0; i < 26; ++i) {\n      if(s[i][0] >= 'a' && s[i][0] <= 'z') {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int numStrings = s.size();\n\n    std::vector<int> stringsPerRank(size);\n    for (int i = 0; i < numStrings; i++) {\n        // the first rank that contains the string i\n        int rank = i % size;\n        stringsPerRank[rank]++;\n    }\n\n    std::vector<int> offsets(size + 1);\n    std::partial_sum(stringsPerRank.begin(), stringsPerRank.end(), offsets.begin() + 1);\n\n    std::vector<int> localFirstLetterCounts(26);\n    for (std::string const& str : s) {\n        // calculate the local number of strings with the first letter in the alphabet\n        char firstLetter = str[0];\n        if (firstLetter >= 'a' && firstLetter <= 'z') {\n            localFirstLetterCounts[firstLetter - 'a']++;\n        }\n    }\n\n    std::vector<int> localSums(size);\n    MPI_Alltoall(localFirstLetterCounts.data(), 26, MPI_INT, localSums.data(), 26, MPI_INT, MPI_COMM_WORLD);\n\n    std::vector<int> globalSums(size);\n    MPI_Allreduce(localSums.data(), globalSums.data(), size, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    bins.fill(0);\n    for (int i = 0; i < size; i++) {\n        for (char c = 'a'; c <= 'z'; c++) {\n            bins[c - 'a'] += globalSums[i];\n        }\n    }\n}",
            "// Your code here\n    size_t rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t n = s.size();\n    size_t count = n / size;\n    size_t remainder = n % size;\n\n    if (rank == 0) {\n        for (size_t i = 1; i < size; ++i) {\n            MPI_Send(&count, 1, MPI_INT, i, 1, MPI_COMM_WORLD);\n            MPI_Send(&remainder, 1, MPI_INT, i, 2, MPI_COMM_WORLD);\n        }\n    }\n\n    std::vector<size_t> temp_bins(26);\n    for (size_t i = 0; i < 26; ++i) {\n        temp_bins[i] = 0;\n    }\n\n    if (rank == 0) {\n        for (size_t i = 0; i < count; ++i) {\n            for (size_t j = 0; j < 26; ++j) {\n                temp_bins[j] += (s[i].at(0) == 'a' + j);\n            }\n        }\n\n        if (remainder!= 0) {\n            for (size_t j = 0; j < 26; ++j) {\n                temp_bins[j] += (s[count].at(0) == 'a' + j);\n            }\n        }\n    }\n\n    MPI_Scatter(&temp_bins[0], 26, MPI_INT, &bins[0], 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> localBins = { 0 };\n    size_t const myRank = mpi::Rank();\n    for (std::string const& elem : s) {\n        if (elem.length() > 0) {\n            ++localBins[static_cast<size_t>(elem[0]) - 'a'];\n        }\n    }\n    if (myRank == 0) {\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = 0;\n            for (size_t j = 0; j < mpi::NumRanks(); ++j) {\n                bins[i] += mpi::Recv(j);\n            }\n        }\n    }\n    else {\n        mpi::Send(localBins);\n    }\n}",
            "size_t world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  size_t world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Find first letter\n  std::string first_letter = s[0].substr(0, 1);\n  for (char c = 'a'; c <= 'z'; c++) {\n    if (first_letter[0] == c) {\n      first_letter = std::string(1, c);\n      break;\n    }\n  }\n\n  // Calculate first letter counts\n  for (size_t i = 0; i < s.size(); i++) {\n    if (s[i].substr(0, 1) == first_letter) {\n      bins[first_letter[0] - 'a']++;\n    }\n  }\n\n  // Allreduce results\n  int error = MPI_Allreduce(bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n  if (error!= MPI_SUCCESS) {\n    std::cout << \"Error: \" << error << std::endl;\n  }\n}",
            "size_t len = s.size();\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int local_bins[26]{};\n    if (world_rank == 0) {\n        for (auto const& str : s) {\n            local_bins[str[0] - 'a']++;\n        }\n    }\n    std::array<size_t, 26> recv_buf;\n    MPI_Gather(local_bins, 26, MPI_INT, recv_buf.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        bins = recv_buf;\n    }\n}",
            "int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Each rank will process some subset of the strings in s.\n  // Each rank gets the first p strings, where p = size / nproc\n  size_t p = s.size() / size;\n  size_t first = p * rank;\n  size_t last = p * (rank + 1);\n  std::vector<std::string> subset;\n  if (rank == size - 1) {\n    subset = std::vector<std::string>(s.begin() + first, s.end());\n  } else {\n    subset = std::vector<std::string>(s.begin() + first, s.begin() + last);\n  }\n\n  // Each rank will compute its own bin counts.\n  std::array<size_t, 26> binCounts;\n  for (auto const& str : subset) {\n    char c = str[0];\n    binCounts[c - 'a']++;\n  }\n\n  // Gather the counts from each rank into a single array on rank 0.\n  MPI_Reduce(\n    binCounts.data(),\n    bins.data(),\n    26,\n    MPI_UNSIGNED,\n    MPI_SUM,\n    0,\n    MPI_COMM_WORLD\n  );\n}",
            "// TODO: write your implementation here\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function.\n}",
            "size_t n = s.size();\n\n    // count the number of strings that start with each letter\n    for (size_t i = 0; i < n; i++) {\n        bins[static_cast<size_t>(s[i][0]) - 'a']++;\n    }\n}",
            "// Your code here.\n}",
            "int numProcesses;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<size_t> localBins;\n\n  if (rank == 0) {\n    localBins = std::vector<size_t>(26, 0);\n  }\n\n  // Compute the number of strings in s that start with each letter\n  for (std::string const& s1 : s) {\n    localBins[s1[0] - 'a']++;\n  }\n\n  if (rank == 0) {\n    // Each process sends its local data to each other process\n    int source = 1;\n    for (int destination = 1; destination < numProcesses; destination++) {\n      MPI_Send(localBins.data(), 26, MPI_INT, destination, 0, MPI_COMM_WORLD);\n    }\n\n    // Process 0 receives from each other process\n    for (int source = 1; source < numProcesses; source++) {\n      MPI_Status status;\n      MPI_Recv(bins.data(), 26, MPI_INT, source, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    // Each process sends its local data to process 0\n    MPI_Send(localBins.data(), 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, numProcs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n  if (numProcs > 26) {\n    throw std::runtime_error(\"More processes than letters in alphabet.\");\n  }\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  size_t chunkSize = s.size() / numProcs;\n  size_t extra = s.size() % numProcs;\n\n  size_t start = rank * chunkSize + std::min(rank, extra);\n  size_t end = start + chunkSize;\n\n  if (rank < extra) {\n    end++;\n  }\n\n  std::vector<std::string> localSubs(s.begin() + start, s.begin() + end);\n\n  int localBins[26] = {0};\n\n  for (const auto& str : localSubs) {\n    localBins[str[0] - 'a']++;\n  }\n\n  MPI_Reduce(localBins, bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Fill in code\n}",
            "// TODO: compute the counts here, assuming s has been partitioned to all ranks\n\tbins.fill(0);\n\tfor (size_t i = 0; i < s.size(); ++i)\n\t{\n\t\t++bins[s[i][0] - 'a'];\n\t}\n}",
            "// MPI communicator\n    MPI_Comm comm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n\n    // Get number of ranks and rank\n    int rank, numRanks;\n    MPI_Comm_size(comm, &numRanks);\n    MPI_Comm_rank(comm, &rank);\n\n    // Distribute s across all ranks\n    int numStrings = s.size();\n    std::vector<int> stringStarts;\n    if (rank == 0) {\n        stringStarts.resize(numRanks);\n    }\n    MPI_Scatter(\n        &numStrings, 1, MPI_INT,\n        stringStarts.data(), 1, MPI_INT,\n        0, comm);\n    std::vector<std::string> myStrings;\n    if (rank == 0) {\n        for (int r = 0; r < numRanks; ++r) {\n            std::copy(s.begin() + stringStarts[r], s.begin() + stringStarts[r] + stringStarts[r + 1], std::back_inserter(myStrings));\n        }\n    }\n\n    // Count the letters\n    std::array<size_t, 26> localBins = { { 0 } };\n    for (std::string const& s : myStrings) {\n        localBins[s[0] - 'a'] += 1;\n    }\n\n    // Combine the results\n    MPI_Reduce(\n        localBins.data(),\n        bins.data(),\n        26,\n        MPI_UNSIGNED_LONG_LONG,\n        MPI_SUM,\n        0,\n        comm);\n\n    // Free communicator\n    MPI_Comm_free(&comm);\n}",
            "if (s.empty()) {\n    return;\n  }\n\n  // Get the number of MPI processes\n  int numProcesses = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n  // Get the number of MPI processes\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int length = s.size();\n  size_t chunkSize = (length + numProcesses - 1) / numProcesses;\n  size_t start = std::min(length, rank * chunkSize);\n  size_t end = std::min(length, (rank + 1) * chunkSize);\n  if (start == end) {\n    return;\n  }\n\n  std::array<size_t, 26> localBins{};\n  std::fill(localBins.begin(), localBins.end(), 0);\n\n  for (size_t i = start; i < end; ++i) {\n    char firstLetter = s[i][0];\n    if ('a' <= firstLetter && firstLetter <= 'z') {\n      ++localBins[firstLetter - 'a'];\n    }\n  }\n\n  // Gather the results\n  MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> localBins = {0};\n\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Each rank has a complete copy of s.\n  // Divide s equally and let each rank work on its part.\n  int elementsPerRank = s.size() / size;\n  int start = elementsPerRank * rank;\n  int end = start + elementsPerRank;\n  if (rank == size - 1) {\n    // Assign the remainder to the last rank.\n    end = s.size();\n  }\n\n  // count occurences of each letter in strings on this rank.\n  for (size_t i = start; i < end; i++) {\n    localBins[s[i][0] - 'a']++;\n  }\n\n  // Add local bins to global bins.\n  MPI_Reduce(localBins.data(), bins.data(), localBins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "size_t len = s.size();\n    // Initialize bins to 0\n    for (auto& b : bins) {\n        b = 0;\n    }\n\n    // Each rank has a copy of s\n    std::vector<std::string> s_local(s.begin(), s.end());\n\n    // Send local data to every other rank\n    int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    std::vector<int> send_counts(num_procs, 0);\n    std::vector<int> recv_counts(num_procs, 0);\n\n    for (size_t i = 0; i < s_local.size(); ++i) {\n        int r = i % num_procs;\n        ++send_counts[r];\n    }\n\n    MPI_Alltoall(send_counts.data(), 1, MPI_INT, recv_counts.data(), 1, MPI_INT, MPI_COMM_WORLD);\n    std::vector<int> displacements(num_procs);\n    std::partial_sum(recv_counts.begin(), recv_counts.end() - 1, displacements.begin() + 1);\n    std::vector<std::string> s_recv(displacements[num_procs - 1] + recv_counts[num_procs - 1]);\n    MPI_Alltoallv(s_local.data(), send_counts.data(), displacements.data(), MPI_CHAR, s_recv.data(), recv_counts.data(), displacements.data(), MPI_CHAR, MPI_COMM_WORLD);\n\n    // Count letters\n    for (auto const& str : s_recv) {\n        if (str.size() > 0) {\n            ++bins[str[0] - 'a'];\n        }\n    }\n\n    // Reduce all counts to root\n    MPI_Reduce(MPI_IN_PLACE, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t chunk_size = s.size() / size;\n  size_t last_chunk = s.size() % size;\n\n  // create a copy of the strings to process\n  std::vector<std::string> local_s(s.begin() + rank * chunk_size, s.begin() + (rank + 1) * chunk_size);\n  if (rank == size - 1) {\n    local_s.insert(local_s.end(), s.begin() + (size - 1) * chunk_size, s.begin() + s.size());\n  }\n\n  // create a counter array for each rank\n  std::array<size_t, 26> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  // for each string in the local s, increment the corresponding counter in the local_bins array\n  for (std::string const& string : local_s) {\n    ++local_bins[string[0] - 'a'];\n  }\n\n  // gather the local bins to get the final count on each rank\n  MPI_Gather(local_bins.data(), 26, MPI_UNSIGNED_LONG, bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // add the last chunk to the rank that has it, if there is one\n  if (last_chunk > 0 && rank < last_chunk) {\n    ++bins[local_s.back()[0] - 'a'];\n  }\n}",
            "std::array<int, 26> counts;\n   std::fill(counts.begin(), counts.end(), 0);\n   for (auto const& str : s) {\n      counts[str[0] - 'a']++;\n   }\n   MPI_Reduce(counts.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: compute here\n    MPI_Datatype MPI_String = MPI_CHAR;\n    MPI_Type_contiguous(s[0].length(), MPI_String, &MPI_String);\n    MPI_Type_commit(&MPI_String);\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int *counts = new int[size];\n    int *displs = new int[size];\n    int *sendcounts = new int[size];\n    int *recvcounts = new int[size];\n    int *recvdispls = new int[size];\n    for (int i = 0; i < size; i++) {\n        counts[i] = 0;\n        sendcounts[i] = 0;\n        recvcounts[i] = 0;\n    }\n    for (int i = 0; i < s.size(); i++) {\n        counts[i % size]++;\n    }\n    int totalCount = 0;\n    for (int i = 0; i < size; i++) {\n        displs[i] = totalCount;\n        totalCount += counts[i];\n        sendcounts[i] = counts[i];\n    }\n    int recvtotalCount = 0;\n    for (int i = 0; i < size; i++) {\n        recvcounts[i] = counts[i];\n        recvdispls[i] = recvtotalCount;\n        recvtotalCount += recvcounts[i];\n    }\n    int *sendcounts_t = new int[size];\n    int *recvcounts_t = new int[size];\n    for (int i = 0; i < size; i++) {\n        sendcounts_t[i] = sendcounts[i];\n        recvcounts_t[i] = recvcounts[i];\n    }\n    int *recvcounts_s = new int[size];\n    MPI_Alltoall(sendcounts, 1, MPI_INT, recvcounts, 1, MPI_INT, MPI_COMM_WORLD);\n    int *displs_s = new int[size];\n    int *recvdispls_s = new int[size];\n    for (int i = 0; i < size; i++) {\n        displs_s[i] = displs[i];\n        recvdispls_s[i] = recvdispls[i];\n    }\n    int *recvcounts_r = new int[size];\n    MPI_Alltoallv(sendcounts_t, sendcounts, displs_s, MPI_INT, recvcounts_r, recvcounts, recvdispls, MPI_INT, MPI_COMM_WORLD);\n    delete[] displs_s;\n    delete[] sendcounts_t;\n    delete[] recvcounts_r;\n    int *recvcounts_r2 = new int[size];\n    for (int i = 0; i < size; i++) {\n        recvcounts_r2[i] = recvcounts_s[i];\n    }\n    int *recvdispls_r = new int[size];\n    for (int i = 0; i < size; i++) {\n        recvdispls_r[i] = recvdispls_s[i];\n    }\n    for (int i = 0; i < size; i++) {\n        recvdispls_s[i] = recvdispls_s[i];\n        recvdispls_r[i] = recvdispls_r[i];\n    }\n    delete[] sendcounts_t;\n    delete[] sendcounts;\n    delete[] recvcounts_s;\n    delete[] recvcounts;\n    delete[] recvdispls;\n    MPI_Alltoallv(counts, counts, displs, MPI_INT, recvcounts_t, recvcounts_r2, recvdispls_r, MPI_INT, MPI_COMM_WORLD);\n    delete[] counts;\n    delete[] displs;\n    delete[] recvcounts_r2;\n    delete[] recvdispls_r;\n    delete[] recvcounts_r;\n    delete[] recvdispls_s;\n    for (int i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < size; i++) {\n        for (int j = recvdispls_s[i]; j < recvdispls_s[i] + recvcounts_s[i]; j++) {\n            bins[s[j][0] - 'a'] += recvcounts_t[i];\n        }\n    }",
            "std::map<char, size_t> counts;\n  for (auto const& str : s) {\n    counts[str[0]]++;\n  }\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = counts[i + 'a'];\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int const chunk = s.size() / size;\n  if (s.size() % size) chunk++;\n\n  int const start = chunk * rank;\n  int const end = std::min(start + chunk, (int)s.size());\n\n  std::array<size_t, 26> counts;\n  for (int i = start; i < end; ++i) {\n    ++counts[s[i][0] - 'a'];\n  }\n\n  MPI_Reduce(counts.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int numRanks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Calculate the number of elements each rank will have\n    size_t numElements = s.size() / numRanks;\n\n    // Get the elements this rank will process\n    size_t start = rank * numElements;\n    size_t end = rank == (numRanks - 1)? s.size() : start + numElements;\n\n    // Create a subset of s for this rank\n    std::vector<std::string> subset(s.begin() + start, s.begin() + end);\n\n    // Count the strings that start with each letter\n    std::array<size_t, 26> count;\n    for (size_t i = 0; i < subset.size(); i++) {\n        if (subset[i].empty()) {\n            continue;\n        }\n\n        // Get the first letter of the string\n        char firstLetter = tolower(subset[i][0]);\n\n        // Make sure it is a letter\n        if (firstLetter < 'a' || firstLetter > 'z') {\n            continue;\n        }\n\n        count[firstLetter - 'a']++;\n    }\n\n    // Add the counts of strings that start with each letter\n    for (int i = 0; i < 26; i++) {\n        MPI_Reduce(&count[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t n = s.size();\n  size_t rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  for (size_t i = rank; i < n; i += nprocs) {\n    size_t j = i;\n    if (s[j][0] >= 'a' && s[j][0] <= 'z') {\n      bins[s[j][0] - 'a']++;\n    }\n  }\n  MPI_Gather(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // distribute the strings to each processor\n    std::vector<size_t> counts(world_size);\n    size_t chunk_size = s.size() / world_size;\n    size_t start_idx = chunk_size * world_rank;\n    size_t end_idx = std::min(start_idx + chunk_size, s.size());\n    std::vector<std::string> my_strings(s.begin() + start_idx, s.begin() + end_idx);\n\n    // count the number of strings that start with each letter\n    std::array<int, 26> letter_counts = {0};\n    for (auto const& str : my_strings) {\n        for (char ch : str) {\n            if (ch >= 'a' && ch <= 'z') {\n                letter_counts[ch - 'a']++;\n            }\n        }\n    }\n\n    // get the sum of the letter counts\n    MPI_Reduce(letter_counts.data(), counts.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // put the results into bins\n    if (world_rank == 0) {\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = counts[i];\n        }\n    }\n}",
            "size_t count;\n    MPI_Datatype string_type, count_type;\n    MPI_Type_contiguous(s[0].length(), MPI_CHAR, &string_type);\n    MPI_Type_contiguous(1, MPI_INT, &count_type);\n    MPI_Type_commit(&string_type);\n    MPI_Type_commit(&count_type);\n\n    MPI_Comm_size(MPI_COMM_WORLD, &count);\n\n    std::vector<size_t> counts(count, 0);\n    for (int i = 0; i < count; i++)\n        counts[i] = 26;\n\n    int sum_count = 0;\n\n    for (auto const& string : s) {\n        MPI_Gather(&string[0], string.length(), MPI_CHAR, NULL, string.length(), MPI_CHAR, 0, MPI_COMM_WORLD);\n        MPI_Gather(&string[0], 1, string_type, NULL, 1, string_type, 0, MPI_COMM_WORLD);\n        MPI_Gather(&string[0], 1, count_type, counts.data(), 1, count_type, 0, MPI_COMM_WORLD);\n    }\n\n    if (rank == 0) {\n        for (auto const& count : counts) {\n            sum_count += count;\n            std::cout << sum_count <<'';\n        }\n    }\n\n    MPI_Gather(&sum_count, 1, MPI_INT, NULL, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    MPI_Type_free(&string_type);\n    MPI_Type_free(&count_type);\n}",
            "// TODO: implement me\n}",
            "std::string startsWith(1, 'a');\n    for (auto const& str : s) {\n        if (startsWith == str)\n            ++bins[0];\n        else\n            ++bins[std::tolower(str[0]) - 'a' + 1];\n    }\n}",
            "if (s.empty()) {\n        return;\n    }\n\n    const size_t n = s.size();\n    std::array<size_t, 26> local_bins = {};\n    local_bins.fill(0);\n    for (auto const& str : s) {\n        local_bins[str[0] - 'a']++;\n    }\n    MPI_Allreduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO: implement me\n}",
            "// TODO: implement this function\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int len = s.size() / size;\n    if (rank == 0) {\n        std::vector<int> counts(26, 0);\n        for (size_t i = 0; i < s.size(); i++) {\n            if (s[i].size() > 0) {\n                counts[s[i][0] - 'a']++;\n            }\n        }\n        for (size_t i = 1; i < size; i++) {\n            MPI_Send(&counts[0], 26, MPI_INT, i, 0, MPI_COMM_WORLD);\n        }\n        for (size_t i = 0; i < 26; i++) {\n            bins[i] = counts[i];\n        }\n    } else {\n        std::vector<int> counts(26, 0);\n        MPI_Status status;\n        MPI_Recv(&counts[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        for (size_t i = 0; i < len; i++) {\n            if (s[rank * len + i].size() > 0) {\n                counts[s[rank * len + i][0] - 'a']++;\n            }\n        }\n        MPI_Send(&counts[0], 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: implement this function.\n}",
            "std::array<size_t, 26> localBins{};\n\n    for (auto const& str : s) {\n        char c = str[0];\n        if ((c >= 'a') && (c <= 'z'))\n            ++localBins[c - 'a'];\n    }\n\n    std::array<int, 26> localSums{};\n    MPI_Allreduce(localBins.data(), localSums.data(), localBins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < localSums.size(); ++i)\n        bins[i] += localSums[i];\n}",
            "// TODO: Your code here\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = s.size();\n    int n_rank = n / MPI_SIZE + (rank < n % MPI_SIZE? 1 : 0);\n\n    std::vector<std::string> s_rank(s.begin() + rank * n_rank, s.begin() + rank * n_rank + n_rank);\n\n    std::array<int, 26> bin_rank;\n    for (size_t i = 0; i < 26; i++) {\n        bin_rank[i] = 0;\n    }\n    for (std::string const& item : s_rank) {\n        bin_rank[item[0] - 'a']++;\n    }\n\n    MPI_Reduce(bin_rank.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 1; i < MPI_SIZE; i++) {\n            MPI_Recv(bins.data(), 26, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n    } else {\n        MPI_Send(bin_rank.data(), 26, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// rank of this process\n\tint rank;\n\n\t// number of processes\n\tint size;\n\n\t// get MPI rank\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// get MPI size\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (size < 1) {\n\t\tthrow std::invalid_argument(\"invalid MPI size: \" + std::to_string(size));\n\t}\n\tif (rank < 0) {\n\t\tthrow std::invalid_argument(\"invalid MPI rank: \" + std::to_string(rank));\n\t}\n\n\t// compute number of strings that start with each letter\n\t// if the string starts with that letter, put it in that bin\n\t// assume all strings are lowercase\n\tsize_t numStrings = s.size();\n\tfor (size_t i = rank; i < numStrings; i += size) {\n\t\tif (s[i][0] >= 'a' && s[i][0] <= 'z') {\n\t\t\tbins[s[i][0] - 'a']++;\n\t\t}\n\t}\n\n\t// collect bins from each process to rank 0\n\tif (rank == 0) {\n\t\tstd::array<size_t, 26> tempBins;\n\t\t// combine bins from each rank\n\t\tfor (int i = 0; i < size; i++) {\n\t\t\t// recv all bins from each rank\n\t\t\tMPI_Recv(tempBins.data(), tempBins.size(), MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n\t\t\t// combine bins from each rank\n\t\t\tfor (size_t j = 0; j < tempBins.size(); j++) {\n\t\t\t\tbins[j] += tempBins[j];\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// send all bins to rank 0\n\t\tMPI_Send(bins.data(), bins.size(), MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "const size_t n = s.size();\n   for (size_t i = 0; i < n; ++i) {\n      if (islower(s[i][0])) {\n         ++bins[s[i][0] - 'a'];\n      }\n   }\n}",
            "MPI_Datatype string_type;\n    MPI_Type_contiguous(s[0].size(), MPI_CHAR, &string_type);\n    MPI_Type_commit(&string_type);\n\n    int n = s.size();\n    int r = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &r);\n    int p = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n\n    if (r == 0) {\n        bins.fill(0);\n    }\n    // For each string, update the bin for the first letter in the string.\n    MPI_Scatter(s.data(), 1, string_type, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Now each processor has its own copy of the first letter of all strings.\n    // For each string, update the bin for the first letter in the string.\n    for (int i = 0; i < n; i++) {\n        bins[s[i][0] - 'a']++;\n    }\n\n    // Now each processor has its own copy of the first letter of all strings.\n    // For each string, update the bin for the first letter in the string.\n    MPI_Gather(bins.data(), 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Release datatype.\n    MPI_Type_free(&string_type);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // local variables\n    size_t const rank_count = s.size() / size;\n    size_t const rank_start = rank * rank_count;\n    size_t const rank_end = rank_start + rank_count;\n    // rank 0 receives the results of all other ranks\n    if (rank == 0) {\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = 0;\n        }\n    }\n    // the local variables of rank 0 are initialized to 0 (see above)\n    std::array<size_t, 26> local_bins = { 0 };\n    for (size_t i = rank_start; i < rank_end; ++i) {\n        // only rank 0 will iterate through this loop\n        std::string str = s[i];\n        if (!str.empty()) {\n            // increment the letter count for the first character of the string\n            // this ensures that 'a' comes before 'b', which comes before 'c', etc\n            ++local_bins[str[0] - 'a'];\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n    int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int n = s.size();\n    int size = n / world_size;\n    int start = size * world_rank;\n    int end = size * (world_rank + 1);\n    if (world_rank == world_size - 1)\n        end = n;\n    std::vector<std::string> local_s(s.begin() + start, s.begin() + end);\n    int local_n = local_s.size();\n    std::array<size_t, 26> local_bins;\n    for (int i = 0; i < 26; i++) {\n        for (auto it = local_s.begin(); it!= local_s.end(); it++) {\n            if (tolower((*it)[0]) == 'a' + i) {\n                local_bins[i]++;\n            }\n        }\n    }\n    std::vector<std::array<size_t, 26>> local_bins_vector;\n    local_bins_vector.push_back(local_bins);\n    std::vector<std::array<size_t, 26>> all_bins_vector;\n    MPI_Reduce(&local_bins_vector[0], &all_bins_vector[0], 1, MPI_ARR_SIZE(all_bins_vector.size(), all_bins_vector[0].size()), MPI_SUM, 0, MPI_COMM_WORLD);\n    if (world_rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = all_bins_vector[0][i];\n        }\n    }\n}",
            "std::array<int, 26> rankCounts;\n\n    MPI_Allreduce(MPI_IN_PLACE, rankCounts.data(), rankCounts.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    for (int i = 0; i < rankCounts.size(); i++) {\n        bins[i] = rankCounts[i];\n    }\n\n    std::array<int, 26> localCounts;\n    size_t myStringCount = s.size();\n    for (auto const& c : s) {\n        ++localCounts[c[0] - 'a'];\n    }\n\n    for (int i = 0; i < 26; i++) {\n        rankCounts[i] += localCounts[i];\n    }\n\n    MPI_Reduce(rankCounts.data(), rankCounts.data(), rankCounts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < 26; i++) {\n        bins[i] += rankCounts[i];\n    }\n}",
            "// TODO: implement this\n    //...\n}",
            "// TODO: your code goes here\n}",
            "// MPI_Init must be called before any other MPI function.\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tif (size < 2) {\n\t\tstd::cout << \"Must be run with at least two MPI ranks\" << std::endl;\n\t\treturn;\n\t}\n\n\t// Count the number of strings that start with each letter and store the result in the\n\t// array bins. Each rank has a copy of the array s.\n\t// To be sure, each rank puts its answer in the same place in the array.\n\tfor (auto const& str : s) {\n\t\tbins[str[0] - 'a']++;\n\t}\n\n\t// Sum the bins from all ranks\n\tstd::array<size_t, 26> bins_total;\n\tMPI_Reduce(bins.data(), bins_total.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t// Copy the answer to the first rank\n\tif (rank == 0) {\n\t\tbins = bins_total;\n\t}\n}",
            "// TODO: implement the function\n}",
            "// TODO: your code here\n}",
            "std::vector<size_t> counts;\n    counts.resize(s.size());\n    std::transform(s.begin(), s.end(), counts.begin(),\n                   [](std::string const& s){return s[0] - 'a';});\n    std::array<size_t, 26> localCounts;\n    std::fill(localCounts.begin(), localCounts.end(), 0);\n    std::transform(counts.begin(), counts.end(), localCounts.begin(),\n                   std::plus<size_t>{});\n    std::array<size_t, 26> totalCounts;\n    MPI_Reduce(localCounts.data(), totalCounts.data(), localCounts.size(),\n               MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (MPI_COMM_WORLD) {\n        std::copy(totalCounts.begin(), totalCounts.end(), bins.begin());\n    }\n}",
            "// TODO: implement me!\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n    MPI_Comm_rank(MPI_COMM_WORLD, &bins.size());\n}",
            "// TODO: Fill in\n}",
            "// TODO: implement me\n    // Hint: use std::count_if\n}",
            "// TODO: implement me\n}",
            "// Do not change this code!\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int n = s.size();\n  int chunkSize = n / size;\n  int remainder = n % size;\n  int start = rank * (chunkSize + (rank < remainder));\n  int end = start + chunkSize + (rank < remainder);\n\n  std::array<size_t, 26> localBins{};\n\n  // your code here\n\n  bins = localBins;\n}",
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tint chunkSize = s.size() / size;\n\tint remaining = s.size() % size;\n\tint start = rank * chunkSize + std::min(rank, remaining);\n\tint end = (rank + 1) * chunkSize + std::min(rank + 1, remaining);\n\tstd::vector<std::string> slice = std::vector<std::string>(s.begin() + start, s.begin() + end);\n\tstd::map<char, size_t> letterCount;\n\n\tfor (std::string const& str : slice) {\n\t\tfor (char c : str) {\n\t\t\t++letterCount[c];\n\t\t}\n\t}\n\n\tif (rank == 0) {\n\t\tfor (char c = 'a'; c <= 'z'; ++c) {\n\t\t\tbins[c - 'a'] = letterCount[c];\n\t\t}\n\t}\n}",
            "// Your code here.\n}",
            "// Get the number of processes\n  int num_procs = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  // Get the rank of this process\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Calculate the number of strings that start with each letter\n  for (auto const & str : s) {\n    if (str.size() > 0) {\n      bins[str[0] - 'a']++;\n    }\n  }\n\n  // Gather the results from the individual ranks\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* TODO */\n}",
            "// Your code goes here.\n}",
            "/* INSERT YOUR CODE HERE */\n  /*... */\n\n  for (auto const& str : s) {\n    bins[str[0] - 'a']++;\n  }\n}",
            "int myRank, nRanks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n  std::vector<size_t> partialBins(26);\n\n  for (auto &str : s) {\n    size_t index = str[0] - 'a';\n    ++partialBins[index];\n  }\n\n  std::array<int, 26> sendCounts;\n  std::array<int, 26> recvCounts;\n  std::array<int, 26> sendDispls;\n  std::array<int, 26> recvDispls;\n\n  for (int i = 0; i < 26; ++i) {\n    sendCounts[i] = partialBins[i];\n    recvCounts[i] = 0;\n    sendDispls[i] = i * nRanks;\n    recvDispls[i] = i * nRanks;\n  }\n\n  MPI_Scatter(sendCounts.data(), 1, MPI_INT, recvCounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  std::vector<size_t> recvBins(recvCounts[myRank]);\n\n  MPI_Scatterv(partialBins.data(), sendCounts.data(), sendDispls.data(), MPI_INT,\n               recvBins.data(), recvCounts[myRank], MPI_INT, 0, MPI_COMM_WORLD);\n\n  MPI_Gatherv(recvBins.data(), recvCounts[myRank], MPI_INT,\n              bins.data(), recvCounts.data(), recvDispls.data(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, numProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n    /* Determine which letters this rank should process.\n       For example, for 4 processes, 0,1,2 and 3 are assigned.\n       For 6 processes, 0,1,2,3,4,5 are assigned.\n       For 7 processes, 0,1,2,3,4,5,6 are assigned.\n    */\n    int const firstLetterIdx = rank * s.size() / numProcs;\n    int const lastLetterIdx = (rank + 1) * s.size() / numProcs;\n\n    /* Determine the letters to count. */\n    char firstLetter = std::tolower(s[firstLetterIdx][0]);\n    char lastLetter = std::tolower(s[lastLetterIdx - 1][0]);\n\n    /* Count the number of strings in s that start with each letter. */\n    std::array<size_t, 26> localBins{};\n    for (size_t i = firstLetterIdx; i < lastLetterIdx; ++i) {\n        localBins[std::tolower(s[i][0]) - 'a']++;\n    }\n\n    /* Use MPI to sum the results. */\n    std::array<size_t, 26> globalBins{};\n    MPI_Reduce(localBins.data(), globalBins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    /* Store the results. */\n    if (rank == 0) {\n        for (int i = firstLetter - 'a'; i <= lastLetter - 'a'; ++i) {\n            bins[i] = globalBins[i];\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // Broadcast the array\n    MPI_Bcast(bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    // For each string in s\n    for (auto const& str : s) {\n        // If the string starts with the current letter\n        if (str[0] >= 'a' && str[0] <= 'z') {\n            // Get the index of the first letter\n            size_t firstLetterIndex = str[0] - 'a';\n            // Increment the corresponding array element\n            MPI_Increment(&bins[firstLetterIndex]);\n        }\n    }\n\n    // Reduce each array element\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype myType;\n  MPI_Datatype typeLengths[2] = { MPI_UNSIGNED_LONG, MPI_CHAR };\n  int blockLengths[2] = { 1, 1 };\n  MPI_Aint displacements[2];\n  displacements[0] = offsetof(LetterCount, counts);\n  displacements[1] = offsetof(LetterCount, letter);\n  MPI_Type_create_struct(2, blockLengths, displacements, typeLengths, &myType);\n  MPI_Type_commit(&myType);\n\n  if (s.size() < 1) {\n    MPI_Type_free(&myType);\n    return;\n  }\n\n  MPI_Datatype myType_with_counts;\n  MPI_Type_contiguous(26, MPI_UNSIGNED_LONG, &myType_with_counts);\n  MPI_Type_commit(&myType_with_counts);\n\n  LetterCount *letterCounts = new LetterCount[s.size()];\n  for (size_t i = 0; i < s.size(); i++) {\n    letterCounts[i].counts = 0;\n    letterCounts[i].letter = s[i][0] - 'a';\n  }\n\n  for (size_t i = 0; i < s.size(); i++) {\n    letterCounts[i].counts++;\n  }\n\n  LetterCount *result = new LetterCount[26];\n\n  MPI_Allreduce(letterCounts, result, 26, myType_with_counts, MPI_SUM, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = result[i].counts;\n  }\n\n  delete[] result;\n  delete[] letterCounts;\n  MPI_Type_free(&myType);\n  MPI_Type_free(&myType_with_counts);\n}",
            "/* Counts how many times each letter appears in the array */\n    std::array<int, 26> letterCounts;\n    for (char c = 'a'; c <= 'z'; ++c) {\n        letterCounts[c - 'a'] = 0;\n    }\n    for (auto const& str : s) {\n        letterCounts[str[0] - 'a'] += 1;\n    }\n\n    /* Distribute the work of counting to the other ranks */\n    std::array<int, 26> localLetterCounts;\n    MPI_Allreduce(letterCounts.data(), localLetterCounts.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    /* For each letter, add the count to the correct bin */\n    for (char c = 'a'; c <= 'z'; ++c) {\n        bins[c - 'a'] += localLetterCounts[c - 'a'];\n    }\n}",
            "// Your code here\n}",
            "// TODO: Implement this function.\n  // You are free to modify this function as you see fit.\n  int n = s.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (n < size) {\n    int local_bins[26] = {0};\n    for (int i = 0; i < n; i++) {\n      local_bins[(int) s[i][0] - 'a']++;\n    }\n    MPI_Reduce(local_bins, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    return;\n  }\n  int subsize = n / size;\n  int rem = n % size;\n\n  std::vector<std::string> s1(subsize + (rank < rem? 1 : 0));\n  std::vector<std::string> s2(subsize + (rank < rem? 1 : 0));\n  for (int i = 0; i < subsize; i++) {\n    s1[i] = s[i * size + rank];\n  }\n  for (int i = 0; i < subsize + (rank < rem? 1 : 0); i++) {\n    s2[i] = s[i * size + rank + 1];\n  }\n  std::array<size_t, 26> subBins1, subBins2;\n  firstLetterCounts(s1, subBins1);\n  firstLetterCounts(s2, subBins2);\n  for (int i = 0; i < 26; i++) {\n    bins[i] = subBins1[i] + subBins2[i];\n  }\n}",
            "// TODO: implement the first letter counts function\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: implement\n    size_t count = 0;\n    if (rank == 0) {\n        for (const auto& str : s) {\n            for (const auto& c : str) {\n                if ('a' <= c && c <= 'z') {\n                    bins[c - 'a'] += 1;\n                }\n            }\n        }\n    }\n    MPI_Reduce(&count, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int num_procs, proc_id;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &proc_id);\n\n    // size of a partition is the number of items in the string divided by the total number of processes\n    int partition_size = (s.size() / num_procs) + 1;\n    int lower_bound = partition_size * proc_id;\n    int upper_bound = (proc_id == num_procs - 1)? s.size() : lower_bound + partition_size;\n\n    std::array<size_t, 26> local_bins{};\n\n    for (size_t i = lower_bound; i < upper_bound; i++) {\n        local_bins[static_cast<int>(s[i][0] - 'a')]++;\n    }\n\n    // Collect all the results from each processor to rank 0\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Fill in this function.\n  // You may want to use MPI_Scatter to distribute the work across all ranks.\n  // You may also want to use MPI_Gather to collect the results at rank 0.\n\n  // NOTE: You may only use MPI functions that are allowed in this assignment.\n  // You may not use other MPI functions.\n\n  // YOUR CODE HERE\n}",
            "/* TODO: implement me */\n}",
            "// Your code goes here!\n}",
            "size_t num_processes;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n\n  size_t process_id;\n  MPI_Comm_rank(MPI_COMM_WORLD, &process_id);\n\n  size_t local_count = 0;\n  for (auto word : s) {\n    if (word[0] >= 'a' && word[0] <= 'z') {\n      local_count++;\n    }\n  }\n\n  size_t total_count = 0;\n  MPI_Reduce(&local_count, &total_count, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  size_t start = 0;\n  size_t end = total_count / num_processes;\n  if (process_id == num_processes - 1) {\n    end = total_count;\n  }\n\n  std::array<size_t, 26> local_bins;\n  for (size_t i = 0; i < 26; i++) {\n    local_bins[i] = 0;\n  }\n\n  for (auto word : s) {\n    if (word[0] >= 'a' && word[0] <= 'z') {\n      local_bins[word[0] - 'a']++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Status status;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if(rank == 0) {\n        for (const auto& str : s) {\n            bins[str[0] - 'a']++;\n        }\n    } else {\n        for (const auto& str : s) {\n            int sender = (rank - 1) % 2;\n            int receiver = (rank + 1) % 2;\n\n            int tag = 1;\n\n            MPI_Send(str.data(), str.size() + 1, MPI_CHAR, receiver, tag, MPI_COMM_WORLD);\n\n            MPI_Recv(bins.data(), bins.size(), MPI_UNSIGNED_LONG, sender, tag, MPI_COMM_WORLD, &status);\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int comm_rank, comm_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n    int local_bins[26];\n    for (int i = 0; i < 26; ++i) {\n        local_bins[i] = 0;\n    }\n\n    // for each string in vector s, increment the count of its first letter in local_bins\n    for (const auto &str : s) {\n        int letter = str[0] - 'a';\n        local_bins[letter]++;\n    }\n\n    // MPI reduction\n    int global_bins[26];\n    MPI_Reduce(local_bins, global_bins, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // update bins only on rank 0\n    if (comm_rank == 0) {\n        for (int i = 0; i < 26; ++i) {\n            bins[i] = global_bins[i];\n        }\n    }\n}",
            "/*\n     * TODO: Your code here\n     */\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t local_size = s.size();\n    size_t global_size = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &global_size);\n    size_t global_count[26] = {0};\n    for (size_t i = 0; i < local_size; i++) {\n        global_count[(int)s[i][0] - (int)'a']++;\n    }\n    MPI_Reduce(global_count, bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<size_t> counts(size);\n  std::vector<size_t> offsets(size);\n\n  std::vector<std::vector<size_t>> split_s(size);\n  for (size_t i = 0; i < s.size(); i++) {\n    size_t rank = (i + rank) % size;\n    split_s[rank].push_back(i);\n  }\n\n  for (size_t i = 0; i < size; i++) {\n    offsets[i] = split_s[i].size() / size * i;\n  }\n\n  for (size_t i = 0; i < size; i++) {\n    for (size_t j = offsets[i]; j < offsets[i] + split_s[i].size() / size; j++) {\n      if (s[split_s[i][j]].size() > 0) {\n        counts[i]++;\n      }\n    }\n  }\n\n  std::vector<size_t> all_counts(size * size);\n  MPI_Gather(&counts[0], counts.size(), MPI_UNSIGNED_LONG, &all_counts[0], counts.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < size; i++) {\n      for (size_t j = 0; j < all_counts[i]; j++) {\n        bins[s[i * size + j][0] - 'a']++;\n      }\n    }\n  }\n}",
            "int rank, world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // distribute array s to processes\n    int chunk_size = s.size() / world_size;\n    int lower_bound = rank * chunk_size;\n    int upper_bound = (rank + 1) * chunk_size;\n    std::vector<std::string> chunk(s.begin() + lower_bound, s.begin() + upper_bound);\n\n    std::array<size_t, 26> local_bins;\n    for (char c = 'a'; c <= 'z'; ++c) {\n        local_bins[c - 'a'] = 0;\n    }\n\n    for (auto const& str : chunk) {\n        local_bins[str[0] - 'a']++;\n    }\n\n    // now compute global counts\n    if (rank == 0) {\n        std::array<size_t, 26> all_bins;\n        for (int i = 0; i < world_size; ++i) {\n            MPI_Status status;\n            MPI_Recv(all_bins.data(), 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (char c = 'a'; c <= 'z'; ++c) {\n                all_bins[c - 'a'] += local_bins[c - 'a'];\n            }\n        }\n        bins = all_bins;\n    } else {\n        MPI_Send(local_bins.data(), 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "cuda",
        "prompt": "/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\n__global__ void firstLetterCounts(const char **s, size_t N, size_t[26] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "/* TODO: implement this function. Use atomicAdd to increase count of the\n   * corresponding character in `bins` array. */\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i<N) {\n    atomicAdd(&bins[(s[i][0]-'a')], 1);\n  }\n}",
            "__shared__ int partial_count[26];\n\n  // Each thread starts with a unique letter in the alphabet\n  const char my_letter = 'a' + blockIdx.x;\n\n  // Each thread adds its count to the corresponding slot in `partial_count`\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    if (s[i] && *s[i] == my_letter) {\n      atomicAdd(&partial_count[my_letter - 'a'], 1);\n    }\n  }\n\n  // Wait for all threads in this block to finish.\n  __syncthreads();\n\n  // Each thread combines the partial results in `partial_count` into `bins` array\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 26; i++) {\n      atomicAdd(&bins[i], partial_count[i]);\n    }\n  }\n}",
            "int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n   int stride = blockDim.x * gridDim.x;\n   for (int i = thread_id; i < 26; i += stride) {\n      bins[i] = 0;\n   }\n\n   for (int i = thread_id; i < N; i += stride) {\n      bins[s[i][0] - 'a']++;\n   }\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: implement this function\n}",
            "}",
            "// Each thread processes one letter.\n  int tid = threadIdx.x;\n  int gid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  while (gid < N) {\n    if (s[gid][0]!= '\\0') {\n      atomicAdd(&bins[s[gid][0] - 'a'], 1);\n    }\n    gid += blockDim.x * gridDim.x;\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n    size_t stride = blockDim.x;\n    size_t start = bid * stride + tid;\n\n    for(size_t i = start; i < N; i += stride * gridDim.x) {\n        char c = s[i][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// Get the thread id and number of threads in the block\n  const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  const int nthreads = blockDim.x * gridDim.x;\n\n  for (int i = tid; i < N; i += nthreads) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z')\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n  }\n}",
            "int myId = threadIdx.x + blockIdx.x*blockDim.x;\n  int myBin;\n  char myFirst;\n\n  if (myId < N) {\n    myFirst = tolower(s[myId][0]);\n    myBin = (myFirst - 'a');\n    atomicAdd(&bins[myBin], 1);\n  }\n}",
            "// TODO: replace the following line with your code\n  __syncthreads();\n}",
            "// TODO: Fill in the kernel code\n}",
            "//TODO: Implement\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      char c = tolower(s[tid][0]);\n      atomicAdd(bins + c - 'a', 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    char c = s[i][0];\n    atomicAdd(&(bins[c - 'a']), 1);\n  }\n}",
            "// TODO: Fill in your code here.\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) return;\n   size_t x = s[idx][0] - 'a';\n   atomicAdd(&bins[x], 1);\n}",
            "int t = threadIdx.x + blockIdx.x*blockDim.x;\n  if (t < N) {\n    int c = s[t][0] - 'a';\n    atomicAdd(&bins[c], 1);\n  }\n}",
            "int tid = threadIdx.x; // thread id\n    int bid = blockIdx.x;  // block id\n\n    __shared__ size_t threadBins[26];\n    threadBins[tid] = 0;\n\n    // Each thread processes one string.\n    // Each thread processes N / blockDim.x strings.\n    // Each thread processes N / (blockDim.x * gridDim.x) strings.\n    // The first block processes strings 0, 1,..., N / (blockDim.x * gridDim.x) - 1.\n    // The second block processes strings N / (blockDim.x * gridDim.x),..., N - 1.\n    //...\n    for (int i = bid * blockDim.x + tid; i < N; i += blockDim.x * gridDim.x) {\n        threadBins[s[i][0] - 'a']++;\n    }\n    __syncthreads();\n    // Aggregate threadBins into bins.\n    // Each block processes 26 strings.\n    // Each block processes N / blockDim.x / gridDim.x strings.\n    // The first block processes strings 0, 1,..., N / (blockDim.x * gridDim.x) - 1.\n    // The second block processes strings N / (blockDim.x * gridDim.x),..., N - 1.\n    //...\n    atomicAdd(&bins[tid], threadBins[tid]);\n}",
            "__shared__ size_t localBins[26];\n  localBins[0] = 0;\n  localBins[1] = 0;\n  localBins[2] = 0;\n  localBins[3] = 0;\n  localBins[4] = 0;\n  localBins[5] = 0;\n  localBins[6] = 0;\n  localBins[7] = 0;\n  localBins[8] = 0;\n  localBins[9] = 0;\n  localBins[10] = 0;\n  localBins[11] = 0;\n  localBins[12] = 0;\n  localBins[13] = 0;\n  localBins[14] = 0;\n  localBins[15] = 0;\n  localBins[16] = 0;\n  localBins[17] = 0;\n  localBins[18] = 0;\n  localBins[19] = 0;\n  localBins[20] = 0;\n  localBins[21] = 0;\n  localBins[22] = 0;\n  localBins[23] = 0;\n  localBins[24] = 0;\n  localBins[25] = 0;\n\n  int tid = threadIdx.x;\n  size_t block = blockIdx.x;\n  size_t start = block * (N / gridDim.x) + (tid * (N / gridDim.x) / blockDim.x);\n  size_t end = start + (N / gridDim.x) / blockDim.x + 1;\n\n  for (size_t i = start; i < end; i++) {\n    if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n      int idx = s[i][0] - 'a';\n      atomicAdd(&localBins[idx], 1);\n    }\n  }\n\n  __syncthreads();\n\n  atomicAdd(&bins[0], localBins[0]);\n  atomicAdd(&bins[1], localBins[1]);\n  atomicAdd(&bins[2], localBins[2]);\n  atomicAdd(&bins[3], localBins[3]);\n  atomicAdd(&bins[4], localBins[4]);\n  atomicAdd(&bins[5], localBins[5]);\n  atomicAdd(&bins[6], localBins[6]);\n  atomicAdd(&bins[7], localBins[7]);\n  atomicAdd(&bins[8], localBins[8]);\n  atomicAdd(&bins[9], localBins[9]);\n  atomicAdd(&bins[10], localBins[10]);\n  atomicAdd(&bins[11], localBins[11]);\n  atomicAdd(&bins[12], localBins[12]);\n  atomicAdd(&bins[13], localBins[13]);\n  atomicAdd(&bins[14], localBins[14]);\n  atomicAdd(&bins[15], localBins[15]);\n  atomicAdd(&bins[16], localBins[16]);\n  atomicAdd(&bins[17], localBins[17]);\n  atomicAdd(&bins[18], localBins[18]);\n  atomicAdd(&bins[19], localBins[19]);\n  atomicAdd(&bins[20], localBins[20]);\n  atomicAdd(&bins[21], localBins[21]);\n  atomicAdd(&bins[22], localBins[22]);\n  atomicAdd(&bins[23], localBins[23]);\n  atomicAdd(&bins[24], localBins[24]);\n  atomicAdd(&bins[25], localBins[25]);\n}",
            "int threadId = blockDim.x * blockIdx.y * gridDim.x\t//rows preceeding current row in grid\n\t\t\t\t\t\t+ blockDim.x * blockIdx.x\t\t\t\t//blocks preceeding current block\n\t\t\t\t\t\t+ threadIdx.x;\t\t\t\t\t\t\t//threads preceeding current thread\n\tint threadId1 = threadId;\n\tif (threadId1 >= N) {\n\t\treturn;\n\t}\n\tchar letter = s[threadId1][0];\n\tatomicAdd(&bins[letter - 'a'], 1);\n}",
            "int tid = threadIdx.x;\n   int i = blockIdx.x;\n   if (i < N) {\n      char c = s[i][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n\tif(tid < N) {\n\t\tbins[s[tid][0] - 'a']++;\n\t}\n}",
            "__shared__ int bins_shared[26];\n    size_t thread_id = threadIdx.x;\n    size_t block_id = blockIdx.x;\n    size_t global_id = block_id * (blockDim.x * 2) + thread_id;\n\n    int i;\n    if (global_id < N) {\n        // Get the first letter of the string\n        char c = tolower(s[global_id][0]);\n        // Increment the number of elements in the array with this value\n        atomicAdd(&bins_shared[c - 'a'], 1);\n    }\n    __syncthreads();\n\n    if (global_id < 26) {\n        // Add the values from the shared array to the global array\n        atomicAdd(&bins[global_id], bins_shared[thread_id]);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    int c;\n    while(i < N) {\n        c = s[i][0];\n        if (isalpha(c)) bins[c - 'a']++;\n        i += blockDim.x * gridDim.x;\n    }\n}",
            "extern __shared__ char s_shared[];\n    int tid = threadIdx.x;\n    int i = blockIdx.x;\n    size_t c;\n\n    for (c = tid; c < 26; c += blockDim.x) {\n        bins[c] = 0;\n    }\n    __syncthreads();\n\n    char ch = 0;\n    if (i < N) {\n        ch = s[i][0];\n    }\n    __syncthreads();\n\n    if (ch >= 'a' && ch <= 'z') {\n        atomicAdd(&bins[ch - 'a'], 1);\n    }\n}",
            "int id = threadIdx.x;\n  int offset = blockIdx.x * N;\n  int stride = gridDim.x * N;\n  int count = 0;\n  for (size_t i = offset + id; i < N; i += stride) {\n    if (s[i][0] < 'a' || s[i][0] > 'z') {\n      bins[0]++;\n    } else {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (tid < N) {\n        atomicAdd(&bins[s[tid][0] - 'a'], 1);\n    }\n}",
            "// TODO: implement me.\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n\n    __shared__ size_t count[26];\n\n    // Initialize the shared memory\n    if (tid < 26) {\n        count[tid] = 0;\n    }\n\n    // Wait for all threads to finish initializing the shared memory\n    __syncthreads();\n\n    size_t len = strlen(s[bid]);\n\n    // Loop over the string\n    for (size_t i = tid; i < len; i += 26) {\n        // If the character is a letter, increment the counter for that letter\n        if (s[bid][i] >= 'a' && s[bid][i] <= 'z') {\n            atomicAdd(&count[s[bid][i] - 'a'], 1);\n        }\n    }\n\n    // Aggregate the counts from the shared memory to global memory\n    __syncthreads();\n\n    if (tid < 26) {\n        atomicAdd(&bins[tid], count[tid]);\n    }\n}",
            "// Your code goes here!\n    // Please do not delete this comment.\n    // You can remove this entire comment block and all code within this block.\n    extern __shared__ size_t sharedBins[];\n    unsigned int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    unsigned int threadId2D = threadIdx.y * blockDim.x + threadIdx.x;\n\n    if (threadId >= N) {\n        return;\n    }\n    const char *str = s[threadId];\n    int c = str[0] - 'a';\n    atomicAdd(&sharedBins[c], 1);\n\n    __syncthreads();\n\n    if (threadId2D < 26) {\n        atomicAdd(&bins[threadId2D], sharedBins[threadId2D]);\n    }\n}",
            "unsigned int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Check if threadId is within bounds of strings\n    if (threadId >= N) {\n        return;\n    }\n\n    // Check if current string starts with letter\n    // If so, increment the bin corresponding to the current letter\n    char firstLetter = s[threadId][0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "// Get the current thread ID and the number of threads in the block.\n  const int tid = threadIdx.x;\n  const int num_threads = blockDim.x;\n  const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Each thread does its own work, by updating its element in the histogram.\n  // This works because all threads are working on different values of i.\n  if (i < N) {\n    char first_letter = s[i][0];\n    atomicAdd(&bins[first_letter - 'a'], 1);\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  __shared__ int counts[26];\n  if (tid < 26) {\n    counts[tid] = 0;\n  }\n  __syncthreads();\n\n  if (tid < N) {\n    char c = s[tid][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&counts[c - 'a'], 1);\n    }\n  }\n  __syncthreads();\n\n  if (tid < 26) {\n    atomicAdd(&bins[tid], counts[tid]);\n  }\n}",
            "// thread id\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // check if the thread id is in the range of the array\n  if (i < N) {\n    // get the first character from the string\n    char c = *s[i];\n\n    // check if the character is in the range of the array\n    if (c >= 'a' && c <= 'z') {\n      // increment the count of the first letter of the string\n      atomicAdd(&bins[c - 'a'], 1);\n    }\n  }\n}",
            "__shared__ size_t bins_s[26];\n  if (threadIdx.x < 26) {\n    bins_s[threadIdx.x] = 0;\n  }\n\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  while (i < N) {\n    char c = s[i][0];\n    atomicAdd(&bins_s[c - 'a'], 1);\n    i += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n\n  // reduce\n  if (threadIdx.x < 26) {\n    atomicAdd(&bins[threadIdx.x], bins_s[threadIdx.x]);\n  }\n}",
            "}",
            "__shared__ size_t s_bins[26];\n\n\tsize_t tid = threadIdx.x;\n\tsize_t bid = blockIdx.x;\n\tsize_t gid = bid * (blockDim.x * gridDim.y) + tid;\n\tsize_t i = tid;\n\n\tif (i < 26) {\n\t\ts_bins[i] = 0;\n\t}\n\n\twhile (gid < N) {\n\t\tchar c = s[gid][0];\n\t\tif (i < 26) {\n\t\t\ts_bins[i] += (c == 'a' + i);\n\t\t}\n\t\tgid += (blockDim.x * gridDim.y);\n\t}\n\n\t// Merge bins in parallel\n\tfor (i = blockDim.x / 2; i > 0; i /= 2) {\n\t\t__syncthreads();\n\t\tif (i <= tid) {\n\t\t\tsize_t j = tid + i;\n\t\t\tif (j < 26) {\n\t\t\t\ts_bins[j] += s_bins[j - i];\n\t\t\t}\n\t\t}\n\t}\n\n\tif (tid == 0) {\n\t\tbins[bid] = s_bins[0];\n\t\tfor (i = 1; i < 26; i++) {\n\t\t\tbins[bid] += s_bins[i];\n\t\t}\n\t}\n}",
            "__shared__ size_t block_bins[26];\n    const size_t tid = threadIdx.x;\n    const size_t gid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (gid < N) {\n        char c = s[gid][0];\n        if ((c >= 'a') && (c <= 'z')) {\n            atomicAdd(&block_bins[c - 'a'], 1);\n        }\n    }\n    __syncthreads();\n    if (tid < 26) {\n        atomicAdd(&bins[tid], block_bins[tid]);\n    }\n}",
            "// TODO: replace this with your own code\n  // Fill in the missing code here.\n  // You should compute the number of strings in `s` that start with each letter of the alphabet.\n  // Store the output in `bins` array.\n\n  // TODO: remove this line\n  __shared__ size_t sharedBins[26];\n  for(int i = 0; i < 26; i++) sharedBins[i] = 0;\n  __syncthreads();\n\n  for(int i = 0; i < N; i++) {\n    if(s[i][0] >= 'a' && s[i][0] <= 'z') sharedBins[s[i][0] - 'a']++;\n  }\n\n  for(int i = 0; i < 26; i++) {\n    int index = threadIdx.x + i * blockDim.x;\n    if(index < 26) {\n      bins[index] += sharedBins[index];\n    }\n  }\n}",
            "const int i = blockIdx.x*blockDim.x + threadIdx.x;\n\tif (i >= N) return;\n\n\tconst char *word = s[i];\n\tif (!word) return;\n\n\tconst char c = *word;\n\tif (c < 'a' || c > 'z') return;\n\tatomicAdd(&bins[c - 'a'], 1);\n}",
            "__shared__ size_t tmp[26];\n  for (int i = 0; i < 26; ++i) {\n    tmp[i] = 0;\n  }\n\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&tmp[c - 'a'], 1);\n    }\n  }\n  __syncthreads();\n\n  for (int i = 0; i < 26; ++i) {\n    atomicAdd(&bins[i], tmp[i]);\n  }\n}",
            "// TODO\n\t// Implement this function\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n\n  __shared__ size_t s_data[BLOCK_SIZE];\n  __shared__ size_t bins_data[26];\n\n  if (tid < 26) {\n    bins_data[tid] = 0;\n  }\n  if (tid == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins_data[i] = 0;\n    }\n  }\n  __syncthreads();\n\n  for (size_t i = bid * BLOCK_SIZE; i < N; i += BLOCK_SIZE * gridDim.x) {\n    if (i < N) {\n      s_data[tid] = 1;\n      for (size_t j = 0; j < tid; j++) {\n        if (s[i][j]!= s[i][j + 1]) {\n          s_data[tid] = 0;\n        }\n      }\n      s_data[tid] = s_data[tid] & (s[i][tid] >= 'a' && s[i][tid] <= 'z');\n      atomicAdd(&bins_data[s[i][tid] - 'a'], s_data[tid]);\n    }\n  }\n  __syncthreads();\n  for (int i = tid; i < 26; i += blockDim.x) {\n    atomicAdd(&bins[i], bins_data[i]);\n  }\n}",
            "__shared__ unsigned char s_shared[26];\n  // The thread id in the block is t\n  // The block id is b\n  // The block size is m\n  // The number of blocks is n\n  // There are N threads\n  size_t b = blockIdx.x; // block id\n  size_t t = threadIdx.x; // thread id\n  size_t n = gridDim.x; // number of blocks\n  size_t m = blockDim.x; // block size\n  size_t i = b * m + t; // the index of the current thread in the array\n  size_t n_shared = 0;\n\n  // Each thread loads a char into shared memory\n  if (i < N) {\n    s_shared[t] = tolower(s[i][0]);\n  }\n\n  // Synchronize so all the threads load the char in shared memory\n  __syncthreads();\n\n  // Each thread looks at the first letter in its local array\n  // If the char is in the alphabet, increment the corresponding value in the bins array\n  if (t < 26) {\n    for (size_t j = 0; j < m; j++) {\n      if (i < N) {\n        // Only increment the bin count if the first letter of the string matches\n        if (s_shared[t] == s[i][j]) {\n          atomicAdd(&bins[t], 1);\n        }\n      }\n      i += n;\n    }\n  }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= 26) return;\n  for (int i = 0; i < N; ++i) {\n    if (s[i]!= NULL && s[i][0] == tid + 'a') ++bins[tid];\n  }\n}",
            "unsigned int threadId = threadIdx.x;\n  unsigned int blockId = blockIdx.x;\n  unsigned int blockSize = blockDim.x;\n  unsigned int gridSize = gridDim.x;\n\n  unsigned int stride = blockSize * gridSize;\n  unsigned int start = blockId * blockSize * 26;\n  unsigned int end = min(start + stride * 26, N);\n\n  for (unsigned int i = threadId + start; i < end; i += blockSize) {\n    char letter = s[i][0];\n    bins[letter - 'a']++;\n  }\n}",
            "// Get the unique thread ID for the block and the total number of threads in the block.\n    const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    const int numThreads = blockDim.x * gridDim.x;\n\n    // Find the first character of the string.\n    while (tid < N) {\n        // Read the string from global memory.\n        const char *str = s[tid];\n        int c = 0;\n        // Stop if the first character of the string is not a letter.\n        if (str[c] >= 'a' && str[c] <= 'z') {\n            // Increment the bin for the first character of the string.\n            atomicAdd(&bins[str[c] - 'a'], 1);\n        }\n        tid += numThreads;\n    }\n}",
            "// TODO: compute the number of strings that start with each letter\n}",
            "const unsigned int threadId = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (threadId >= N)\n        return;\n\n    // Get the first character of the string.\n    const char firstChar = s[threadId][0];\n    const unsigned int idx = firstChar - 'a';\n    atomicAdd(&(bins[idx]), 1);\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int bin = bid % 26;\n    int stride = blockDim.x * gridDim.x;\n\n    for (int i = bid; i < N; i += stride) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n\n    __syncthreads();\n\n    if (tid == 0) {\n        atomicAdd(&bins[bin], stride);\n    }\n}",
            "int tid = threadIdx.x; // thread index\n  int bid = blockIdx.x;  // block index\n  int bidz = blockIdx.y;\n  int threadsPerBlock = blockDim.x;\n  int blocksPerGrid = gridDim.x;\n\n  int idx = bid * threadsPerBlock + tid;\n  if (idx < N) {\n    int ch = s[idx][0] - 'a';\n    atomicAdd(&bins[ch], 1);\n  }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code goes here.\n  // Use 26 threads and 1 block.\n}",
            "__shared__ int s_bins[26];\n  int tid = threadIdx.x;\n  int idx = blockIdx.x * blockDim.x + tid;\n\n  if (idx < N) {\n    char c = s[idx][0];\n    int bin = (c - 'a') % 26;\n    s_bins[bin] += 1;\n  }\n  __syncthreads();\n\n  // Sum the per-thread sums to get the total counts for each bin.\n  if (tid < 26) {\n    atomicAdd(&bins[tid], s_bins[tid]);\n  }\n}",
            "__shared__ size_t partials[26];\n\n  // Each thread will count the number of strings that start with a particular letter.\n  int threadID = threadIdx.x;\n  int nthreads = blockDim.x;\n  int localIdx = threadID;\n  int globalIdx = threadID + blockIdx.x * blockDim.x;\n\n  // Each thread gets its own copy of the string, but we want to be careful not to\n  // read past the end of the string.\n  // It is a bad idea to copy the entire string from global memory at once.\n  // Instead, we copy the first 4 characters of the string into a shared memory\n  // array, and use atomicAdd to increment the count.\n  // To be clear, each thread copies a *different* string.\n  char string[4];\n  if (globalIdx < N) {\n    string[0] = s[globalIdx][0];\n    string[1] = s[globalIdx][1];\n    string[2] = s[globalIdx][2];\n    string[3] = s[globalIdx][3];\n  }\n\n  // Count the number of strings that start with each letter.\n  // For example, if there are 3 strings that start with letter 'd', and 5 that\n  // start with 'c', then partials should be initialized to [3, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n  // After all the threads have executed this block, partials should be\n  // [3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 3, 5, 5]\n  // The final value of partials should be the same as the return value of\n  // firstLetterCountsSerial.\n  for (int i = 0; i < 26; i++) {\n    partials[i] = 0;\n  }\n  while (localIdx < 26) {\n    atomicAdd(&partials[localIdx], 1);\n    localIdx += nthreads;\n  }\n\n  // Each thread atomically adds the partials to the output array.\n  // The first thread to finish will copy the first 26 elements of\n  // partials to the output array, the second thread will copy the\n  // next 26, etc.\n  // Note that we must use atomicAdd here, because we are copying the result of\n  // the count to an output array.\n  for (int i = 0; i < 26; i++) {\n    atomicAdd(&bins[i], partials[i]);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index < N) {\n        bins[s[index][0] - 'a']++;\n    }\n}",
            "// Compute the index of the thread in the block\n   int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N)\n      return;\n   // Read the string from the array of strings\n   const char *ss = s[idx];\n   // Increment the bin corresponding to the first letter\n   atomicAdd(&bins[ss[0] - 'a'], 1);\n}",
            "extern __shared__ size_t sh[];\n    const int threadId = threadIdx.x;\n    const int blockId = blockIdx.x;\n    const int threadCount = blockDim.x;\n    const int stride = threadCount * 26;\n\n    sh[threadId] = 0;\n    if (threadId < 26) {\n        sh[threadId + threadCount] = 0;\n    }\n\n    __syncthreads();\n\n    const int start = blockId * stride;\n    const int end = min(start + stride, N);\n\n    int i = start + threadId;\n    while (i < end) {\n        const int j = s[i][0] - 'a';\n        atomicAdd(sh + j, 1);\n        i += threadCount;\n    }\n\n    __syncthreads();\n\n    for (int i = threadId; i < 26; i += threadCount) {\n        atomicAdd(bins + i, sh[i]);\n    }\n}",
            "// TODO: implement this function\n}",
            "size_t idx = threadIdx.x;\n    size_t bin = idx + 'a';\n    size_t cnt = 0;\n    for (size_t i = idx; i < N; i += blockDim.x) {\n        if (s[i][0] == bin) {\n            cnt++;\n        }\n    }\n    bins[bin] = cnt;\n}",
            "const int threadId = blockDim.x * blockIdx.x + threadIdx.x;\n    if (threadId < N) {\n        size_t letter = s[threadId][0] - 'a';\n        atomicAdd(&bins[letter], 1);\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    size_t letter = s[idx][0] - 'a';\n    atomicAdd(&bins[letter], 1);\n  }\n}",
            "int t = threadIdx.x;\n  int b = blockIdx.x;\n  int i = b*blockDim.x+t;\n  if (i<N) {\n    bins[s[i][0]-'a']++;\n  }\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadId >= N) return;\n\n  char firstLetter = tolower(s[threadId][0]);\n  atomicAdd(&bins[firstLetter - 'a'], 1);\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\tif(tid < N) {\n\t\tconst char *start = s[tid];\n\t\tchar c = *start;\n\t\twhile(c!= '\\0') {\n\t\t\tbins[c - 'a']++;\n\t\t\tc = *(++start);\n\t\t}\n\t}\n}",
            "// Each thread computes one output array element.\n  // Compute the array index using thread ID.\n  size_t i = threadIdx.x;\n  // Compute the number of strings that start with 'a'\n  if (i == 0) {\n    for (int k = 0; k < N; ++k) {\n      const char *c = s[k];\n      if (*c == 'a') ++bins[0];\n      ++c;\n      if (*c == 'b') ++bins[1];\n      ++c;\n      if (*c == 'c') ++bins[2];\n      ++c;\n      if (*c == 'd') ++bins[3];\n      ++c;\n      if (*c == 'e') ++bins[4];\n      ++c;\n      if (*c == 'f') ++bins[5];\n      ++c;\n      if (*c == 'g') ++bins[6];\n      ++c;\n      if (*c == 'h') ++bins[7];\n      ++c;\n      if (*c == 'i') ++bins[8];\n      ++c;\n      if (*c == 'j') ++bins[9];\n      ++c;\n      if (*c == 'k') ++bins[10];\n      ++c;\n      if (*c == 'l') ++bins[11];\n      ++c;\n      if (*c =='m') ++bins[12];\n      ++c;\n      if (*c == 'n') ++bins[13];\n      ++c;\n      if (*c == 'o') ++bins[14];\n      ++c;\n      if (*c == 'p') ++bins[15];\n      ++c;\n      if (*c == 'q') ++bins[16];\n      ++c;\n      if (*c == 'r') ++bins[17];\n      ++c;\n      if (*c =='s') ++bins[18];\n      ++c;\n      if (*c == 't') ++bins[19];\n      ++c;\n      if (*c == 'u') ++bins[20];\n      ++c;\n      if (*c == 'v') ++bins[21];\n      ++c;\n      if (*c == 'w') ++bins[22];\n      ++c;\n      if (*c == 'x') ++bins[23];\n      ++c;\n      if (*c == 'y') ++bins[24];\n      ++c;\n      if (*c == 'z') ++bins[25];\n    }\n  }\n}",
            "// TODO: Your code here\n}",
            "// Each block has its own copy of thread-local array\n  __shared__ size_t local_bins[26];\n  // Initialize array to zero\n  for (int i = threadIdx.x; i < 26; i += blockDim.x) {\n    local_bins[i] = 0;\n  }\n  __syncthreads();\n\n  for (int tid = blockDim.x * blockIdx.x + threadIdx.x; tid < N; tid += blockDim.x * gridDim.x) {\n    local_bins[s[tid][0] - 'a']++;\n  }\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < 26; i += blockDim.x) {\n    atomicAdd(&bins[i], local_bins[i]);\n  }\n}",
            "// TODO: implement this function\n}",
            "// blockIdx.x is the block number\n  // blockDim.x is the number of threads per block\n  // threadIdx.x is the thread number within the block\n\n  // compute the index of the current block, and the stride between blocks\n  const int id = blockIdx.x * blockDim.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n\n  // compute the index of the string for the current thread\n  int i = id;\n  // loop over all strings\n  for (; i < N; i += stride) {\n    // check if the string starts with the current thread's letter\n    if (s[i][0] - 'a' >= 0 && s[i][0] - 'a' < 26) {\n      // if so, increment the corresponding element in the bins array\n      atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n  }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        int c = s[i][0] - 'a';\n        atomicAdd(&bins[c], 1);\n    }\n}",
            "// YOUR CODE GOES HERE\n}",
            "//TODO: Your code goes here\n  const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int id = tid % 26;\n  const int chunkSize = (N + blockDim.x - 1)/blockDim.x;\n  for (int i = tid; i < N; i += chunkSize) {\n    int c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      atomicAdd(&(bins[id]), 1);\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  for (; tid < N; tid += blockDim.x * gridDim.x) {\n    bins[s[tid][0] - 'a']++;\n  }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if(id < N) {\n        char c = tolower(s[id][0]);\n        atomicAdd(&bins[c-'a'], 1);\n    }\n}",
            "int tid = blockDim.x*blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        bins[s[tid][0] - 'a']++;\n    }\n}",
            "unsigned int tid = threadIdx.x;\n    unsigned int bid = blockIdx.x;\n    unsigned int id = tid + bid * blockDim.x;\n    if (id < N) {\n        // find the first letter\n        char c = tolower(s[id][0]);\n        if (c < 'a' || c > 'z') {\n            bins[26] += 1;\n        } else {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n    // Use shared memory to keep track of the bins for each thread\n    // Hint: you need to use __syncthreads() to synchronize threads in each block\n    // YOUR CODE HERE\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n\n    int bin = 0;\n    if (bid < N) {\n        // YOUR CODE HERE\n        bin = s[bid][0] - 'a';\n        __syncthreads();\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: Implement the kernel\n}",
            "size_t tid = threadIdx.x;\n    size_t gid = blockIdx.x;\n    size_t blocksize = blockDim.x;\n    extern __shared__ size_t s_bins[];\n\n    for (int i = tid; i < 26; i += blocksize) {\n        s_bins[i] = 0;\n    }\n\n    for (int i = tid; i < N; i += blocksize) {\n        s_bins[s[i][0] - 'a']++;\n    }\n\n    __syncthreads();\n\n    for (int i = tid; i < 26; i += blocksize) {\n        atomicAdd(&bins[i], s_bins[i]);\n    }\n}",
            "unsigned int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    unsigned int idx = 0;\n    while (tid < N) {\n        char c = s[tid][idx];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n        }\n        tid += gridDim.x * blockDim.x;\n    }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    for (int i = index; i < N; i += stride) {\n        char c = s[i][0];\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// Fill in your code\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n}",
            "const size_t tid = threadIdx.x;\n   const size_t bid = blockIdx.x;\n   const size_t gid = bid*blockDim.x + tid;\n\n   // Each block will process exactly one letter in the alphabet\n   if (gid < 26) {\n      size_t cnt = 0;\n      for (size_t i = 0; i < N; ++i) {\n         if (s[i][0] == 'a' + gid)\n            ++cnt;\n      }\n      atomicAdd(&bins[gid], cnt);\n   }\n}",
            "size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n    if (threadId >= 26) return;\n\n    bins[threadId] = 0;\n    for (size_t i = threadId; i < N; i += blockDim.x * gridDim.x) {\n        if (s[i][0] == threadId + 'a') bins[threadId]++;\n    }\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    char ch = s[i][0];\n    unsigned int tid = threadIdx.x;\n\n    //atomicAdd(bins + ch - 'a', 1);\n    atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "const unsigned int idx = threadIdx.x;\n\n   if (idx < 26) {\n      for (unsigned int i = 0; i < N; ++i) {\n         if (s[i][0] == 'a' + idx)\n            atomicAdd(&bins[idx], 1);\n      }\n   }\n}",
            "// YOUR CODE HERE\n    // Make sure that your kernel can use at least N threads and that there is\n    // a 1D block of N threads for each grid element.\n    \n    // YOUR CODE HERE\n}",
            "// TODO: Fill in the kernel code\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        char c = tolower(s[tid][0]);\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// Your code goes here\n}",
            "size_t id = threadIdx.x;\n    size_t localBins[26];\n\n    for (int i = 0; i < 26; i++) {\n        localBins[i] = 0;\n    }\n\n    // 1. Read the string and count the number of each letter\n\n    for (int i = id; i < N; i += blockDim.x) {\n        const char *str = s[i];\n\n        for (int j = 0; j < 26; j++) {\n            if (str[0] == 'a' + j) {\n                localBins[j]++;\n            }\n        }\n    }\n\n    // 2. Reduce localBins to globalBins\n    for (int i = 1; i < 26; i *= 2) {\n        if (id < i) {\n            localBins[id] += localBins[id + i];\n        }\n\n        __syncthreads();\n    }\n\n    if (id == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = localBins[i];\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int i = bid * 32 + tid;\n\n    if (i < N) {\n        atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n}",
            "// YOUR CODE HERE\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    char c = s[i][0];\n    if ('a' <= c && c <= 'z') {\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < 26) {\n    for (int i = 0; i < N; i++) {\n      if (s[i][0] == tid + 'a') {\n        atomicAdd(&bins[tid], 1);\n      }\n    }\n  }\n}",
            "unsigned tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid >= N) {\n      return;\n   }\n   atomicAdd(bins + (*s[tid] - 'a'), 1);\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        bins[s[idx][0] - 'a']++;\n    }\n}",
            "// TODO: Your code here\n}",
            "const int thread_id = threadIdx.x; // current thread\n    const int block_id = blockIdx.x; // current block\n    const int num_threads = blockDim.x; // number of threads per block\n    const int num_blocks = gridDim.x; // number of blocks\n    const int total_threads = num_blocks * num_threads; // total number of threads\n\n    // each thread is responsible for a different letter\n    const int letter = thread_id;\n\n    // each block is responsible for computing the sum of the counts for a different set of letters\n    const int start = block_id * num_threads + thread_id; // compute the start index of the array for the current block\n\n    // compute the end index of the array for the current block\n    const int end = start + num_threads;\n\n    // each thread contributes to the count of the number of strings that start with the letter\n    size_t count = 0;\n\n    // compute the count for the current letter in parallel\n    for (int i = start; i < end; i++) {\n        // for each string in the array\n        const char *s_i = s[i];\n        while (*s_i!= '\\0') {\n            // if the string starts with the current letter\n            if (*s_i == letter + 'a') {\n                count++;\n            }\n            // increment to the next character\n            s_i++;\n        }\n    }\n\n    // each thread stores its results in an array\n    __shared__ size_t temp_array[26];\n\n    // store the result of the current thread\n    temp_array[letter] = count;\n\n    // synchronize all threads in the block\n    __syncthreads();\n\n    // add up the results from the other threads in the block\n    for (int i = num_threads / 2; i > 0; i /= 2) {\n        // only the first half of the threads contribute\n        if (thread_id < i) {\n            // add up the results from the other threads\n            temp_array[letter] += temp_array[letter + i];\n        }\n        // synchronize the threads\n        __syncthreads();\n    }\n\n    // only the thread with thread_id == 0 contributes\n    if (thread_id == 0) {\n        // store the result in the bins array\n        bins[letter] = temp_array[letter];\n    }\n}",
            "int tid = threadIdx.x;\n    size_t sum = 0;\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        if (s[i][0] >= 'a' && s[i][0] <= 'z')\n            atomicAdd(&bins[s[i][0] - 'a'], 1);\n    }\n    for (size_t i = 0; i < 26; i++)\n        atomicAdd(&bins[i], bins[i]);\n}",
            "// TODO: implement the kernel\n}",
            "/* YOUR CODE HERE */\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if(i < N) {\n    char letter = s[i][0];\n    atomicAdd(&bins[letter - 'a'], 1);\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        char firstLetter = tolower(s[index][0]);\n        atomicAdd(bins + firstLetter, 1);\n    }\n}",
            "__shared__ size_t sdata[256];\n    int tid = threadIdx.x;\n    int block = blockIdx.x;\n\n    // Compute first char of each string in block.\n    char ch = tolower(s[block][0]);\n\n    // Each thread loads one character.\n    sdata[tid] = ch == 'x'? 1 : 0;\n\n    // Load data into shared memory.\n    __syncthreads();\n\n    // Aggregate the counts.\n    for (int s = 1; s < blockDim.x; s *= 2) {\n        if (tid % (2 * s) == 0) {\n            sdata[tid] += sdata[tid + s];\n        }\n\n        __syncthreads();\n    }\n\n    // Store the final results.\n    if (tid == 0) {\n        // Write the block results to global memory.\n        for (int i = 0; i < 26; i++) {\n            if (i == ch - 'a') {\n                bins[i] = sdata[i];\n            } else {\n                bins[i] = 0;\n            }\n        }\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    atomicAdd(&bins[s[tid][0] - 'a'], 1);\n  }\n}",
            "// TODO: Implement this function.\n    // TODO: Remember to use the 26 bins for each thread.\n}",
            "size_t thread = threadIdx.x;\n\tsize_t bin = s[thread][0] - 'a';\n\tatomicAdd(&bins[bin], 1);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   int tsize = blockDim.x * gridDim.x;\n   for (int i = tid; i < N; i += tsize) {\n      int c = s[i][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "// Each thread processes a single letter,\n    // so get the id of the thread (an integer between 0 and N-1)\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // If the id is less than N, process the string at s[tid].\n    // Otherwise, do nothing.\n    if (tid < N) {\n        char c = s[tid][0];\n        // Increment the bin for the letter at s[tid][0].\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t count;\n    if (id < N) {\n        char firstLetter = tolower(s[id][0]);\n        atomicAdd(&bins[firstLetter - 'a'], 1);\n    }\n}",
            "__shared__ size_t counts[26];\n\tsize_t tid = threadIdx.x;\n\tsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\t// For each thread, count the first letter\n\tif (i < N) {\n\t\tchar c = s[i][0];\n\t\tatomicAdd(&counts[c - 'a'], 1);\n\t}\n\t// Synchronize at the end of each block.\n\tif (tid == 0) {\n\t\tfor (int i = 0; i < 26; i++) {\n\t\t\tatomicAdd(&bins[i], counts[i]);\n\t\t}\n\t}\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int id = bid * blockDim.x + tid;\n    if (id < N) {\n        char c = s[id][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid < N) {\n\t\t// Add one to the bin for each character in the string.\n\t\tfor (size_t i = 0; i < strlen(s[tid]); i++)\n\t\t\tatomicAdd(&bins[s[tid][i] - 'a'], 1);\n\t}\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t idx = s[i][0] - 'a';\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "// TODO: Your code goes here!\n\n}",
            "// Fill me in!\n}",
            "// YOUR CODE HERE\n\n    // 1. Calculate the global thread ID\n    // 2. Calculate the local thread ID within the block\n    // 3. Calculate the position of the string to process (i.e. the string to process for global ID j\n    // is s[j-offset])\n    // 4. Calculate the first letter of the string.\n    // 5. Increment the count of the first letter.\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) return;\n\n    char ch = s[tid][0];\n    int bin = (ch >= 'a' && ch <= 'z')? ch - 'a' : 26;\n    atomicAdd(&bins[bin], 1);\n}",
            "// each thread works on a letter\n    int letter = threadIdx.x;\n    // each thread computes one of the bins\n    int bin = letter + 'a';\n\n    // each thread loops over all the strings\n    for (int i = 0; i < N; i++) {\n        // each thread checks if the current letter is in the current string\n        if (tolower(s[i][0]) == bin) {\n            // if so, it increases the corresponding bin\n            atomicAdd(&bins[letter], 1);\n        }\n    }\n}",
            "// TODO\n  /*\n    * Use threadIdx.x as index\n    * Initialize bins array with zeros\n    * Loop over all strings\n    * For each string s[i]\n    *   If s[i][0] is an alphabetic letter\n    *     bins[s[i][0] - 'a']++\n    *   End if\n  */\n}",
            "int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    extern __shared__ int sh[];\n    if (tid < 26) {\n        sh[tid] = 0;\n    }\n    __syncthreads();\n    if (bid * blockDim.x + tid < N) {\n        sh[s[bid * blockDim.x + tid][0] - 'a']++;\n    }\n    __syncthreads();\n    if (tid < 26) {\n        atomicAdd(&(bins[tid]), sh[tid]);\n    }\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n}",
            "int tid = threadIdx.x;\n    // Each block is assigned 32 threads\n    for (int i = tid; i < 26; i += blockDim.x) {\n        // Initialize the count to 0\n        bins[i] = 0;\n    }\n    __syncthreads();\n    for (int i = tid; i < N; i += blockDim.x) {\n        int c = s[i][0] - 'a';\n        if (c >= 0 && c < 26) {\n            // Add one to the count of the first letter\n            atomicAdd(&bins[c], 1);\n        }\n    }\n    __syncthreads();\n    for (int i = tid; i < 26; i += blockDim.x) {\n        // Each block needs to output the count for each letter\n        printf(\"%d: %d\\n\", i, bins[i]);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "__shared__ char buffer[BLOCK_SIZE][MAX_STR_LEN];\n  __shared__ size_t buffer_counter;\n  const int tid = threadIdx.x;\n  const int bid = blockIdx.x;\n  const int laneId = threadIdx.x & 0x1f;\n  const int warpId = threadIdx.x >> 5;\n  const int warpSize = 32;\n  // printf(\"BLOCK_SIZE = %d, N = %d\\n\", BLOCK_SIZE, N);\n  // printf(\"bid = %d, tid = %d\\n\", bid, tid);\n  // printf(\"laneId = %d, warpId = %d, warpSize = %d\\n\", laneId, warpId, warpSize);\n  // printf(\"s = %p, s[bid] = %p\\n\", s, s[bid]);\n  // printf(\"bins = %p\\n\", bins);\n  // printf(\"bins[0] = %d\\n\", bins[0]);\n  // printf(\"buffer = %p\\n\", buffer);\n  // printf(\"buffer_counter = %d\\n\", buffer_counter);\n\n  // load a segment of string array into the buffer\n  size_t offset = bid * BLOCK_SIZE;\n  while (offset < N) {\n    buffer[tid][tid] = s[offset][tid];\n    offset += BLOCK_SIZE;\n  }\n  // printf(\"buffer[%d][%d] = %c\\n\", tid, tid, buffer[tid][tid]);\n\n  // wait for all threads to load their data\n  __syncthreads();\n\n  // each thread finds the first letter of their assigned section of strings\n  if (buffer[tid][0] >= 'a' && buffer[tid][0] <= 'z') {\n    atomicAdd(&bins[buffer[tid][0] - 'a'], 1);\n  }\n\n  // wait for all threads to find their letter\n  __syncthreads();\n\n  // each thread adds its value to its neighboring counter\n  if (tid < 32) {\n    for (int d = 16; d >= 1; d /= 2) {\n      size_t temp = __shfl_xor_sync(0xffffffff, bins[tid], d);\n      if (laneId < d) {\n        bins[tid] += temp;\n      }\n    }\n  }\n\n  // wait for all threads to add their counters\n  __syncthreads();\n\n  // store the final results\n  if (tid == 0) {\n    bins[26] = 0;\n  }\n  __syncthreads();\n  if (tid < 26) {\n    atomicAdd(&bins[26], bins[tid]);\n  }\n\n  // wait for all threads to store their results\n  __syncthreads();\n\n  // write back to global memory\n  if (tid < 26) {\n    atomicAdd(&bins[tid], bins[26]);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    int j = i % 26;\n    i /= 26;\n    if (i < N && j < 26) {\n        bins[j] += (s[i] && s[i][0] == (char)('a' + j));\n    }\n}",
            "unsigned int t = threadIdx.x;\n  unsigned int i = blockIdx.x*blockDim.x + threadIdx.x;\n\n  __shared__ unsigned int counter[26];\n\n  if (i < 26) {\n    counter[i] = 0;\n  }\n\n  while (i < N) {\n    if (s[i][t] >= 'a' && s[i][t] <= 'z') {\n      atomicAdd(&counter[s[i][t]-'a'], 1);\n    }\n    i += blockDim.x*gridDim.x;\n  }\n\n  // Sum the counts from each thread\n  for (int d=1; d<blockDim.x; d *= 2) {\n    if (t % (2*d) == 0) {\n      unsigned int idx = t + d;\n      if (idx < blockDim.x) {\n        counter[t/d] += counter[idx];\n      }\n    }\n  }\n\n  // Reduce to a single value per block\n  if (t == 0) {\n    bins[blockIdx.x] = counter[0];\n    for (int d=1; d<blockDim.x; d*=2) {\n      bins[blockIdx.x] += counter[d];\n    }\n  }\n}",
            "// Each thread will process one letter in the alphabet.\n    // Each thread will also process one string in the array `s`.\n    // You need to do a sequential scan over the string array,\n    // and then count how many strings start with that letter.\n    // You will need to store the results in the `bins` array.\n    // You will also need to declare local memory to store intermediate results\n    // of the scan and count. You will need to allocate one integer for each\n    // thread.\n    //\n    // Hint: see the documentation for cudascan and cudacount, which are\n    // similar to the built-in scan and count functions except that they\n    // can be used with CUDA arrays.\n    //\n    // Hint: You can get the index of the thread you are processing by calling\n    //       blockDim.x * blockIdx.x + threadIdx.x\n}",
            "// For each string in s, increment the counter for its first letter.\n   // This is the only CUDA code you need to write for this task.\n   // Your code should work for any number of strings and any alphabet size.\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   int step = blockDim.x * gridDim.x;\n   for (int i = tid; i < N; i += step) {\n      size_t offset = 97;\n      bins[s[i][0] - offset]++;\n   }\n}",
            "/* Your implementation here. */\n  int i = threadIdx.x;\n  while (i < N)\n  {\n    bins[s[i][0] - 'a']++;\n    i += blockDim.x;\n  }\n}",
            "// Write your code here\n}",
            "// Each block processes one character in the alphabet.\n    // Find the index of the block in the grid and use it to find the first string\n    // in the block that starts with the current letter. Then, check that the character\n    // is indeed the same for each string.\n    int block = blockIdx.x;\n    int idx = threadIdx.x;\n    int start = block * N;\n    if (idx < 26) {\n        size_t i = start;\n        while (i < start + N) {\n            if (s[i][0] == 'a' + idx) {\n                bins[idx]++;\n                break;\n            }\n            i++;\n        }\n    }\n}",
            "int tid = threadIdx.x;\n    int wid = blockIdx.x;\n    int lid = threadIdx.x;\n\n    // blockDim.x is the size of the block and is equal to the number of threads in a block\n    for (size_t i = wid * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n        if (s[i][0] - 'a' >= 0 && s[i][0] - 'a' < 26) {\n            atomicAdd(&bins[s[i][0] - 'a'], 1);\n        }\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      char c = s[i][0];\n      if (c >= 'a' && c <= 'z') {\n         atomicAdd(&bins[c - 'a'], 1);\n      }\n   }\n}",
            "// TODO: Fill in this method.\n}",
            "// TODO: write your code here\n   __shared__ int myBins[26];\n   int tid = threadIdx.x;\n   int myCount = 0;\n   int myIdx = tid;\n   while (myIdx < N) {\n      char c = s[myIdx][0];\n      myIdx += blockDim.x;\n      atomicAdd(&myBins[c - 'a'], 1);\n   }\n   myBins[tid] = myBins[tid] + (myIdx > N - 1);\n   __syncthreads();\n\n   for (int i = blockDim.x / 2; i > 0; i >>= 1) {\n      if (tid < i) {\n         myBins[tid] = myBins[tid] + myBins[tid + i];\n      }\n      __syncthreads();\n   }\n   if (tid == 0) {\n      for (int i = 0; i < 26; i++) {\n         atomicAdd(&bins[i], myBins[i]);\n      }\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    char c = tolower(s[tid][0]);\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n\tif(tid >= N) {\n\t\treturn;\n\t}\n\n\tchar c = s[tid][0];\n\tif(c >= 'a' && c <= 'z') {\n\t\tatomicAdd(&bins[c - 'a'], 1);\n\t}\n}",
            "/* TODO */\n    __shared__ size_t counters[26];\n    // atomicAdd(&counters[letter], 1) is atomic operation which adds the value of 1 to the current value of counters[letter].\n    atomicAdd(&counters[s[threadIdx.x][0]-97], 1);\n    __syncthreads();\n    // Each thread adds its counter value to the corresponding bin.\n    atomicAdd(&bins[s[threadIdx.x][0]-97], counters[threadIdx.x]);\n}",
            "// TODO\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    char c = s[tid][0];\n    if ('a' <= c && c <= 'z')\n      atomicAdd(&(bins[c - 'a']), 1);\n  }\n}",
            "__shared__ size_t sdata[26]; // Shared memory for each thread\n\n  // Get the first letter of the current string\n  char firstLetter = tolower(s[threadIdx.x][0]);\n\n  // First thread of each block adds its first letter to the shared memory.\n  if (threadIdx.x == 0) sdata[firstLetter - 'a']++;\n\n  // Synchronize threads to make sure that all threads have completed\n  // the first step. This ensures that only the first thread of each\n  // block writes to the shared memory.\n  __syncthreads();\n\n  // Each thread adds to its bin.\n  bins[firstLetter - 'a'] += sdata[threadIdx.x];\n}",
            "__shared__ char firstLetter[26];\n  size_t tid = threadIdx.x;\n  firstLetter[tid] = s[0][0];\n\n  for (size_t i = 1; i < N; i++) {\n    if (firstLetter[tid]!= s[i][0]) {\n      firstLetter[tid] = s[i][0];\n      __syncthreads();\n      atomicAdd(&bins[tid], 1);\n    }\n  }\n}",
            "int index = blockDim.x * blockIdx.x + threadIdx.x;\n    while (index < N) {\n        if (s[index]!= NULL) {\n            atomicAdd(&bins[(*s[index] - 'a')], 1);\n        }\n        index += gridDim.x * blockDim.x;\n    }\n}",
            "__shared__ size_t s_bins[26];\n    if (threadIdx.x < 26)\n        s_bins[threadIdx.x] = 0;\n    __syncthreads();\n\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if (index < N) {\n        char c = s[index][0];\n        int idx = c - 'a';\n        if (idx >= 0 && idx <= 25)\n            atomicAdd(&s_bins[idx], 1);\n    }\n\n    __syncthreads();\n\n    if (threadIdx.x < 26)\n        atomicAdd(&bins[threadIdx.x], s_bins[threadIdx.x]);\n}",
            "// For each thread\n    // 1. Load string into register\n    // 2. If start with 0-25 (a-z), increment corresponding index in bins\n    // 3. Otherwise, increment 26 index in bins\n    // 4. Return\n\n    // TODO: your code goes here\n}",
            "// TODO: Implement me\n  for (size_t i = 0; i < N; i++) {\n    int first_char = s[i][0];\n    atomicAdd(&(bins[first_char - 'a']), 1);\n  }\n}",
            "// TODO: Fill in code\n    int tid = threadIdx.x + blockDim.x * blockIdx.x;\n    int idx = 0;\n    if (tid < N) {\n        char c = tolower(s[tid][0]);\n        for (size_t i = 1; i < strlen(s[tid]); i++) {\n            if (tolower(s[tid][i])!= c) {\n                idx = i;\n                break;\n            }\n        }\n        atomicAdd(&(bins[c - 'a']), 1);\n    }\n}",
            "int letter = threadIdx.x;\n  for (int n = blockIdx.x; n < N; n += gridDim.x) {\n    if (s[n][0] == letter + 'a') {\n      atomicAdd(&bins[letter], 1);\n    }\n  }\n}",
            "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int i = gid;\n    while (i < N) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n        i += gridDim.x * blockDim.x;\n    }\n}",
            "const char *a = s[blockIdx.x * blockDim.x + threadIdx.x];\n  if (a) {\n    int x = *a - 'a';\n    if (x >= 0 && x < 26) {\n      atomicAdd(&bins[x], 1);\n    }\n  }\n}",
            "size_t thread_idx = threadIdx.x;\n    size_t num_threads = blockDim.x;\n\n    for (size_t i = thread_idx; i < N; i += num_threads) {\n        char c = s[i][0];\n        if ('a' <= c && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Insert your code here\n    __syncthreads();\n}",
            "// TODO: Your code here\n  int tid = threadIdx.x;\n  __shared__ int counts[26];\n  counts[tid] = 0;\n  for (size_t i = tid; i < N; i += 26) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      counts[c - 'a']++;\n    }\n  }\n\n  __syncthreads();\n  for (int i = 1; i < 26; i *= 2) {\n    if (tid % (2 * i) == 0) {\n      counts[tid] += counts[tid + i];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    bins[0] = counts[0];\n    bins[1] = counts[1];\n    bins[2] = counts[2];\n    bins[3] = counts[3];\n    bins[4] = counts[4];\n    bins[5] = counts[5];\n    bins[6] = counts[6];\n    bins[7] = counts[7];\n    bins[8] = counts[8];\n    bins[9] = counts[9];\n    bins[10] = counts[10];\n    bins[11] = counts[11];\n    bins[12] = counts[12];\n    bins[13] = counts[13];\n    bins[14] = counts[14];\n    bins[15] = counts[15];\n    bins[16] = counts[16];\n    bins[17] = counts[17];\n    bins[18] = counts[18];\n    bins[19] = counts[19];\n    bins[20] = counts[20];\n    bins[21] = counts[21];\n    bins[22] = counts[22];\n    bins[23] = counts[23];\n    bins[24] = counts[24];\n    bins[25] = counts[25];\n  }\n}",
            "__shared__ size_t bins_shared[26];\n\n  // Each thread copies its own bin counter to the shared memory.\n  bins_shared[threadIdx.x] = 0;\n  for (int i = 0; i < N; i += blockDim.x) {\n    // Wait for all threads to be ready to write to shared memory.\n    __syncthreads();\n\n    // The thread that has a match of a starting letter increases the counter in the bins_shared array.\n    if (s[i][0] == 'a' + threadIdx.x) {\n      atomicAdd(&bins_shared[threadIdx.x], 1);\n    }\n\n    // Wait for all threads to be ready to read from shared memory.\n    __syncthreads();\n  }\n\n  // Each thread adds the counters in the shared memory to its own counter.\n  for (int i = 0; i < 26; i++) {\n    atomicAdd(&bins[i], bins_shared[i]);\n  }\n}",
            "int i = threadIdx.x;\n  int j = blockIdx.x;\n  if (i < 26) {\n    bins[i] = 0;\n  }\n  __syncthreads();\n  if (j < N) {\n    if (s[j][0] >= 'a' && s[j][0] <= 'z') {\n      atomicAdd(&bins[s[j][0] - 'a'], 1);\n    }\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index < N) {\n    char ch = s[index][0];\n    if (ch >= 'a' && ch <= 'z')\n      atomicAdd(&bins[ch - 'a'], 1);\n  }\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        atomicAdd(&bins[tolower(s[i][0]) - 'a'], 1);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    bins[s[tid][0] - 'a']++;\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        // Use only lower-case letters for simplicity.\n        char c = s[tid][0];\n        if (c >= 'a' && c <= 'z') {\n            atomicAdd(&bins[c - 'a'], 1);\n        }\n    }\n}",
            "// YOUR CODE HERE\n  __shared__ int sh_bins[26];\n\n  for (int i = 0; i < 26; i++)\n    sh_bins[i] = 0;\n\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    char letter = s[i][0] - 'a';\n    sh_bins[letter]++;\n  }\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < 26; i += blockDim.x) {\n    bins[i] += sh_bins[i];\n  }\n}",
            "// TODO\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (id < N) {\n\t\tbins[s[id][0] - 'a'] += 1;\n\t}\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    while(tid < 26) {\n        for(int i = 0; i < N; i++) {\n            //if the string starts with letter 'tid' then increase the count\n            if(s[i][0] == tid + 'a') {\n                atomicAdd(&bins[tid], 1);\n            }\n        }\n        tid += blockDim.x * gridDim.x;\n    }\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n\n    char c = tolower(s[i][0]);\n    if (c >= 'a' && c <= 'z') {\n        atomicAdd(&bins[c - 'a'], 1);\n    }\n}",
            "extern __shared__ size_t temp[];\n\n  // Each thread processes one input word\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    // Get first letter\n    char first = s[tid][0];\n\n    // We use __syncthreads() here to make sure all threads have copied their data before\n    // we use them in the reduction.\n    __syncthreads();\n\n    // First, this thread adds one to the count for its letter\n    atomicAdd(&temp[first - 'a'], 1);\n\n    // Next, we need to make sure all threads have finished before we can do the reduction.\n    __syncthreads();\n  }\n\n  // Finally, each thread needs to perform a reduction on the temp array.\n  // Use a single thread to do the reduction.\n  if (threadIdx.x == 0) {\n    for (int i = 1; i < blockDim.x; i++) {\n      temp[i] += temp[i - 1];\n    }\n  }\n\n  __syncthreads();\n\n  // Each thread writes a single count.\n  if (tid < 26) {\n    bins[tid] = temp[tid];\n  }\n}",
            "// YOUR CODE HERE\n  int tid = threadIdx.x;\n  __shared__ int count[26];\n  count[tid] = 0;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n  while(i < N){\n    if(s[i][0] - 'a' < 26){\n      count[s[i][0] - 'a'] += 1;\n    }\n    i += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  atomicAdd(&bins[tid], count[tid]);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      int c = s[i][0];\n      int bin = 0;\n      if ((c >= 'a') && (c <= 'z')) {\n         bin = c - 'a';\n         atomicAdd(&bins[bin], 1);\n      }\n   }\n}",
            "// Each thread computes a single bin\n  const size_t thread = blockIdx.x*blockDim.x + threadIdx.x;\n\n  if (thread < 26) {\n    // Compute start and end indices for the current bin\n    const size_t start = thread * N / 26;\n    const size_t end = (thread+1) * N / 26;\n    size_t count = 0;\n    for (size_t i = start; i < end; ++i) {\n      if (s[i] && s[i][0] == (char)thread+'a') ++count;\n    }\n    bins[thread] = count;\n  }\n}",
            "// compute block and thread IDs\n  int blockID = blockIdx.x;\n  int threadID = threadIdx.x;\n\n  // compute global thread ID\n  int globalThreadID = blockID * blockDim.x + threadID;\n\n  // loop over the letters\n  for (int letterID = 0; letterID < 26; letterID++) {\n    // loop over all strings in the vector\n    int count = 0;\n    for (int i = globalThreadID; i < N; i += blockDim.x * gridDim.x) {\n      if (s[i][0] == letterID + 'a')\n        count++;\n    }\n    // increment the counts\n    atomicAdd(&bins[letterID], count);\n  }\n}",
            "// YOUR CODE HERE\n    // Hint: Use cuda::atomic()\n}",
            "}",
            "// Get the index of the thread in the block.\n  int i = threadIdx.x;\n\n  // Each thread computes one element of the output.\n  // This version does not check the range of the input array, which\n  // is why we get a thread-safe crash.\n  if (i < 26) {\n    bins[i] = 0;\n    for (size_t j = 0; j < N; j++) {\n      if (s[j][0] == 'a' + i) {\n        // This is safe, since the pointer array is managed by the kernel.\n        bins[i]++;\n      }\n    }\n  }\n}",
            "__shared__ size_t localBins[26];\n  // zero all bins\n  for (int i = 0; i < 26; i++) {\n    localBins[i] = 0;\n  }\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    char c = tolower(s[i][0]);\n    if (c >= 'a' && c <= 'z') {\n      localBins[c - 'a']++;\n    }\n  }\n  // combine all partial results\n  for (int i = 0; i < 26; i++) {\n    atomicAdd(&(bins[i]), localBins[i]);\n  }\n}",
            "// TODO: Implement this function\n  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int i = 0;\n  if(tid < N)\n  {\n    char firstLetter = s[tid][0];\n    i = (firstLetter-'a');\n    atomicAdd(&bins[i],1);\n  }\n}",
            "const int tid = threadIdx.x;\n  const int id = blockIdx.x * blockDim.x + tid;\n  if (id >= N)\n    return;\n  atomicAdd(&bins[s[id][0] - 'a'], 1);\n}",
            "extern __shared__ char s_data[];\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    // TODO: load string from device memory (s[i]) into shared memory (s_data)\n    // TODO: process the string (using s_data), updating bins array\n    // TODO: save the updated bins array to device memory (bins)\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t i = tid % N;\n    size_t j = tid / N;\n\n    if (i < N) {\n        int x = s[i][j] - 'a';\n        if (x >= 0 && x < 26)\n            atomicAdd(&bins[x], 1);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < 26)\n        for (size_t i = 0; i < N; i++)\n            if (s[i][0] == tid + 'a')\n                atomicAdd(&bins[tid], 1);\n}",
            "/* Your solution goes here  */\n    int thread_idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if(thread_idx < N){\n        size_t c = s[thread_idx][0];\n        bins[c - 'a']++;\n    }\n}",
            "// Your code goes here\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  while (idx < N) {\n    char c = s[idx][0];\n    atomicAdd(&bins[c - 'a'], 1);\n    idx += gridDim.x * blockDim.x;\n  }\n}",
            "int tid = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + tid;\n    while (i < N) {\n        bins[s[i][0] - 'a'] += 1;\n        i += blockDim.x * gridDim.x;\n    }\n}",
            "const int t = threadIdx.x;\n    int x = t + 1;\n    int sum = 0;\n    for (size_t i = 0; i < N; i++) {\n        if (s[i][0] == x) {\n            sum++;\n        }\n    }\n    atomicAdd(bins + t, sum);\n}",
            "// YOUR CODE HERE\n    // (you may have to replace this line with the real code)\n    // printf(\"kernel is launched on block %d of %d with thread %d of %d\\n\", blockIdx.x, gridDim.x, threadIdx.x, blockDim.x);\n    size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        char letter = tolower(s[tid][0]);\n        if (letter >= 'a' && letter <= 'z') {\n            atomicAdd(&bins[letter - 'a'], 1);\n        }\n    }\n}",
            "unsigned tid = threadIdx.x;\n  unsigned blkid = blockIdx.x;\n  unsigned blksz = blockDim.x;\n  size_t i = tid + blksz * blkid;\n  if (i < N) {\n    char c = s[i][0];\n    atomicAdd(&bins[c - 'a'], 1);\n  }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "19_histogram_first_letter_counts",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* For each letter in the alphabet, count the number of strings in the vector s that start with that letter.\n   Assume all strings are in lower case. Store the output in `bins` array.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of s. The result is stored in bins on rank 0.\n   Example:\n\n   input: [\"dog\", \"cat\", \"xray\", \"cow\", \"code\", \"type\", \"flower\"]\n   output: [0, 0, 3, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0]\n*/\nvoid firstLetterCounts(std::vector<std::string> const& s, std::array<size_t, 26> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t const rank = MPI_Rank();\n    size_t const n = s.size();\n\n#pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        std::string const& str = s[i];\n        bins[str[0] - 'a']++;\n    }\n\n#pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        std::string const& str = s[i];\n        bins[str[0] - 'a']--;\n    }\n}",
            "// OpenMP:\n    // First, find the minimum number of threads that the machine has (using omp_get_max_threads())\n    // For each thread, run the first letter count function that you wrote.\n    // Call this function with a different thread id for each thread.\n\n    int nthreads = omp_get_max_threads();\n    #pragma omp parallel num_threads(nthreads)\n    {\n        int tid = omp_get_thread_num();\n\n        size_t offset = s.size() / nthreads * tid;\n        size_t end = offset + s.size() / nthreads;\n\n        if (tid == nthreads - 1) {\n            end = s.size();\n        }\n\n        firstLetterCounts(s, offset, end, bins);\n    }\n\n    // MPI:\n    // Split the vector into sub-vectors with equal length (hint: use std::vector<std::vector<std::string>>).\n    // Call firstLetterCounts on each of the sub-vectors, and reduce the results on rank 0.\n    // You can assume there are at least 2 MPI processes.\n    // Use MPI_Reduce on the bins array to get the result.\n\n    int nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    std::vector<std::vector<std::string>> substrings;\n    substrings.reserve(nprocs);\n\n    // Split the vector into substrings\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t step = s.size() / nprocs;\n    size_t start = step * rank;\n    size_t end = (step * (rank + 1));\n    end = end == s.size()? s.size() : end;\n    substrings.emplace_back(s.begin() + start, s.begin() + end);\n\n    // Call firstLetterCounts for each substring\n    std::array<size_t, 26> local_bins{};\n    firstLetterCounts(substrings[0], local_bins);\n\n    // Reduce results\n    std::array<size_t, 26> temp_bins{};\n    if (rank == 0) {\n        temp_bins = std::array<size_t, 26>{};\n        temp_bins = local_bins;\n    }\n\n    MPI_Reduce(local_bins.data(), temp_bins.data(), temp_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = temp_bins;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for(int i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "bins.fill(0);\n\tint numRanks;\n\tMPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tstd::vector<std::string> localS(s.begin() + s.size()/numRanks*rank, s.begin() + s.size()/numRanks*(rank+1));\n\n#pragma omp parallel\n\t{\n#pragma omp for\n\t\tfor (int i = 0; i < localS.size(); ++i) {\n\t\t\tbins[localS[i][0] - 'a']++;\n\t\t}\n\t}\n\n\tstd::array<size_t, 26> binsSum;\n\tMPI_Reduce(bins.data(), binsSum.data(), bins.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\tbins = binsSum;\n}",
            "// TODO: Add your code here.\n  int rank;\n  int nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  size_t n = s.size();\n  if (rank == 0) {\n    bins.fill(0);\n  }\n  int chunksize = n/nprocs;\n  int offset = chunksize*rank;\n  int mysize = rank == nprocs-1? n-offset : chunksize;\n  std::vector<std::string> chunk(mysize);\n  MPI_Scatter(s.data() + offset, mysize, MPI_CHAR, chunk.data(), mysize, MPI_CHAR, 0, MPI_COMM_WORLD);\n  #pragma omp parallel for\n  for (int i=0; i<mysize; i++) {\n    int idx = chunk[i][0] - 'a';\n    bins[idx]++;\n  }\n}",
            "if (s.size() < 26) {\n        // if we have less than 26 strings, then don't bother doing the rest of the exercise\n        std::fill(bins.begin(), bins.end(), 0);\n    } else {\n        // if we have 26 or more strings, then we need to do this exercise\n        // first, split up the strings between all the available processes\n        size_t n = s.size();\n        size_t chunkSize = n / MPI_size();\n        std::vector<std::string> splitStrings(s.begin(), s.begin() + chunkSize);\n        for (int i = 0; i < s.size() - chunkSize; i++) {\n            splitStrings.push_back(s[i + chunkSize]);\n        }\n\n        // now initialize an array of counts of each letter to 0\n        std::array<size_t, 26> localCounts;\n        std::fill(localCounts.begin(), localCounts.end(), 0);\n\n        // now compute the counts of each letter\n        // each process will have a different amount of strings in splitStrings\n        // thus, we have to use a parallel for loop\n        #pragma omp parallel for\n        for (int i = 0; i < splitStrings.size(); i++) {\n            // loop over the string and increment the count of each letter\n            for (int j = 0; j < splitStrings[i].size(); j++) {\n                char c = splitStrings[i][j];\n                if (c >= 'a' && c <= 'z') {\n                    // if the character is a letter, then increment the appropriate count\n                    localCounts[c - 'a'] += 1;\n                }\n            }\n        }\n\n        // now, each process needs to collect all of its counts of letters into a global array\n        MPI_Reduce(localCounts.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code here\n    int N = s.size();\n    size_t total_len = 0;\n#pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        total_len += s[i].size();\n    }\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int num_threads = omp_get_max_threads();\n    std::vector<size_t> tmp_bins(26, 0);\n    std::vector<size_t> partial_bins(26, 0);\n    std::vector<size_t> total_bins(26, 0);\n    std::vector<size_t> final_bins(26, 0);\n    int size = N / num_ranks;\n    if (rank == num_ranks - 1) {\n        size = N % num_ranks;\n    }\n    std::vector<std::string> partial_s(size);\n#pragma omp parallel for\n    for (int i = 0; i < size; i++) {\n        partial_s[i] = s[rank * size + i];\n    }\n    int len = partial_s.size();\n    int t = 0;\n    while (t < num_threads) {\n        if (rank == 0) {\n            for (int i = 0; i < 26; i++) {\n                partial_bins[i] = 0;\n            }\n            for (int i = 0; i < len; i++) {\n                if (partial_s[i][0] >= 'a' && partial_s[i][0] <= 'z') {\n                    partial_bins[partial_s[i][0] - 'a']++;\n                }\n            }\n            MPI_Gather(&partial_bins[0], 26, MPI_UNSIGNED_LONG, &total_bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        }\n        if (rank == 0) {\n            for (int i = 0; i < 26; i++) {\n                final_bins[i] = total_bins[i];\n            }\n        }\n        MPI_Bcast(&final_bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        MPI_Barrier(MPI_COMM_WORLD);\n        MPI_Gather(&final_bins[0], 26, MPI_UNSIGNED_LONG, &tmp_bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        if (rank == 0) {\n            for (int i = 0; i < 26; i++) {\n                final_bins[i] = tmp_bins[i];\n            }\n        }\n        MPI_Bcast(&final_bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        t++;\n    }\n    MPI_Reduce(&total_len, &bins[0], 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = s.size();\n  std::array<int, 26> counts;\n  std::fill(counts.begin(), counts.end(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    int rank = omp_get_thread_num();\n    int letter = s[i][0] - 'a';\n    counts[letter] += 1;\n  }\n  MPI_Reduce(counts.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, num_ranks;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  size_t size = s.size();\n\n  // Initialize bins array.\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  // Determine how many strings to process on each rank.\n  size_t chunk_size = size / num_ranks;\n  size_t remainder = size % num_ranks;\n\n  // If rank is the last one, process the remainder.\n  if (rank == num_ranks - 1) {\n    chunk_size += remainder;\n  }\n\n  std::vector<std::string> chunk(s.begin() + chunk_size * rank, s.begin() + chunk_size * (rank + 1));\n\n  // Compute first letter count for each string in the chunk.\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < chunk.size(); ++i) {\n    size_t pos = chunk[i][0] - 'a';\n    bins[pos]++;\n  }\n\n  // Reduce results for rank 0.\n  if (rank == 0) {\n    std::array<size_t, 26> result;\n\n    MPI_Reduce(&bins, &result, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = result;\n  } else {\n    MPI_Reduce(&bins, nullptr, 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "}",
            "#pragma omp parallel for\n  for (int i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n  // TODO: Your code goes here!\n  for (int i = 0; i < s.size(); ++i) {\n    int idx = s[i][0] - 'a';\n    if (idx >= 0 && idx <= 25) {\n      ++bins[idx];\n    }\n  }\n}",
            "// Write your code here\n  int rank, nproc;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n  size_t n = s.size();\n  size_t chunksize = n/nproc;\n\n  std::vector<std::string> local_s;\n  if(rank == 0){\n      for(int i = 0; i < nproc; ++i){\n          std::vector<std::string> tmp;\n          if(i!= nproc-1) tmp.assign(s.begin()+i*chunksize, s.begin()+(i+1)*chunksize);\n          else tmp.assign(s.begin()+i*chunksize, s.end());\n          local_s.insert(local_s.end(), tmp.begin(), tmp.end());\n      }\n  }\n  else{\n      std::vector<std::string> tmp(s.begin()+rank*chunksize, s.begin()+(rank+1)*chunksize);\n      local_s.insert(local_s.end(), tmp.begin(), tmp.end());\n  }\n\n  std::array<size_t, 26> local_bins{};\n  #pragma omp parallel for\n  for(int i = 0; i < local_s.size(); ++i){\n      for(int j = 0; j < local_s[i].size(); ++j)\n          local_bins[local_s[i][j] - 'a']++;\n  }\n\n  MPI_Gather(&local_bins, 26, MPI_UNSIGNED_LONG, bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// rank=0 and size=1 do the work in serial\n    if (MPI_COMM_WORLD.rank() == 0 && MPI_COMM_WORLD.size() == 1) {\n        std::array<size_t, 26> counts;\n        std::fill(counts.begin(), counts.end(), 0);\n\n        for (size_t i = 0; i < s.size(); ++i) {\n            counts[static_cast<size_t>(s[i][0] - 'a')]++;\n        }\n\n        bins = counts;\n        return;\n    }\n\n    // Every other rank has to do the work\n    int rank = MPI_COMM_WORLD.rank();\n    int size = MPI_COMM_WORLD.size();\n\n    // Send counts to all other ranks. Every other rank will get an empty std::array to fill in.\n    std::array<size_t, 26> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n    MPI_Gather(&counts[0], 26, MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // If we are the rank 0, we compute the counts in serial and send them back to all other ranks.\n    // This is the only rank that actually does the work.\n    if (rank == 0) {\n        std::array<size_t, 26> localCounts;\n        std::fill(localCounts.begin(), localCounts.end(), 0);\n\n        // Add up the counts for each string\n        for (size_t i = 0; i < s.size(); ++i) {\n            localCounts[static_cast<size_t>(s[i][0] - 'a')]++;\n        }\n\n        // Get the totals for each letter in the array.\n        // We will use MPI_Reduce to get the totals for all the letters.\n        std::array<size_t, 26> totals;\n        MPI_Reduce(localCounts.data(), totals.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n        // Send the totals back to all the other ranks.\n        MPI_Scatter(totals.data(), 26, MPI_UNSIGNED_LONG, counts.data(), 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n        bins = counts;\n    }\n}",
            "// TODO: your code goes here\n\n    size_t n = s.size();\n    bins = std::array<size_t, 26>(0);\n\n    if (n == 0) return;\n\n    if (n < 1000) {\n        for (const std::string& str : s) {\n            if (str.size() > 0)\n                bins[static_cast<size_t>(str[0] - 'a')]++;\n        }\n    }\n    else {\n#pragma omp parallel\n        {\n            std::array<size_t, 26> local_bins = std::array<size_t, 26>(0);\n\n#pragma omp for\n            for (size_t i = 0; i < n; i++) {\n                const std::string& str = s[i];\n                if (str.size() > 0)\n                    local_bins[static_cast<size_t>(str[0] - 'a')]++;\n            }\n\n#pragma omp critical\n            for (size_t i = 0; i < 26; i++)\n                bins[i] += local_bins[i];\n        }\n    }\n}",
            "size_t n = s.size();\n    std::vector<size_t> tmpBins(26, 0);\n    size_t chunkSize = n/omp_get_max_threads();\n    size_t start = omp_get_thread_num()*chunkSize;\n    size_t end = std::min(n, start+chunkSize);\n\n#pragma omp parallel for\n    for (size_t i = start; i < end; ++i) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            tmpBins[c - 'a']++;\n        }\n    }\n\n#pragma omp parallel for\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] += tmpBins[i];\n    }\n}",
            "// TODO: Your code goes here.\n    \n}",
            "const size_t stringCount = s.size();\n    const int rank = MPI::COMM_WORLD.Get_rank();\n    const int rankCount = MPI::COMM_WORLD.Get_size();\n\n    // TODO: Count number of strings starting with each letter.\n    // TODO: Sum counts of each letter on each rank.\n    // TODO: Send totals of each letter to rank 0.\n    // TODO: Gather results of each rank on rank 0.\n}",
            "// Your code here.\n    int size;\n    int rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size_per_rank = s.size()/size;\n    size_t my_begin = rank*size_per_rank;\n    size_t my_end = my_begin + size_per_rank;\n    if (rank == 0) {\n        my_end += s.size() % size;\n    }\n    std::array<size_t, 26> local_bins;\n    for (size_t i = 0; i < 26; i++) {\n        local_bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (size_t i = my_begin; i < my_end; i++) {\n        std::string &str = s[i];\n        char ch = tolower(str[0]);\n        if (isalpha(ch)) {\n            size_t index = ch - 'a';\n            local_bins[index]++;\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// initialize bins to all zeros\n    // note: std::fill is a parallel for loop\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // loop over all strings\n    // note: std::for_each is a parallel for loop\n    // note: std::for_each takes a const reference to the string\n    //      and modifies it; the const reference is passed\n    //      to the lambda function\n    // note: the lambda function is run in parallel for each element in the vector\n    // note: if you want to modify a vector in a parallel for loop,\n    //      you must make a copy of the vector (e.g. using std::vector::operator=)\n    //      and then modify the copy and assign it to the vector\n    std::for_each(s.begin(), s.end(), [&](std::string const& str) {\n        // note: if str was modified in the lambda function,\n        //      the copy of str will not be modified\n        // note: the lambda function is run in parallel for each element in the vector\n        // note: the lambda function is not a member function, so cannot access `bins`\n\n        // only increment bin for first letter of string\n        size_t index = str[0] - 'a';\n        bins[index]++;\n    });\n}",
            "bins = std::array<size_t, 26>{};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "int rank, numProcesses;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    // First, compute the number of strings that start with each letter.\n    // Put the result in bins array.\n    // Each rank has the same copy of s.\n    std::array<size_t, 26> localBins;\n    for (auto const& str : s) {\n        ++localBins[str[0] - 'a'];\n    }\n\n    // Next, broadcast the local result.\n    MPI_Bcast(localBins.data(), localBins.size(), MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = localBins;\n    }\n\n    // Finally, add the local bins together to get the global bins array.\n    // This will only be executed on rank 0.\n    MPI_Reduce(localBins.data(), bins.data(), localBins.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // We have to do this to make sure that we have a complete copy of s on each rank.\n  // Since this is a one-liner, we do it here and move on.\n  auto local_s = s;\n  size_t local_len = s.size() / size;\n  if (rank < s.size() % size) {\n    local_s.push_back(s.back());\n    ++local_len;\n  }\n\n  // We'll now try to compute in parallel.\n  // First, we get the number of strings in each bin.\n  std::vector<size_t> count(26);\n  #pragma omp parallel for\n  for (size_t i = 0; i < local_len; ++i) {\n    // We have to do this to make sure that each thread gets a separate copy of local_s.\n    // If we were to use std::vector::operator[] or std::vector::at, then a single copy would be shared across all threads.\n    // Thus, we have to use std::vector::data to get a pointer to the vector's data, and then dereference it\n    // to get the individual strings.\n    std::string str = local_s.data()[i];\n    if (str.length() > 0) {\n      ++count[str[0] - 'a'];\n    }\n  }\n\n  // Now, we sum the counts of each bin across all ranks.\n  // We'll use MPI_Reduce to perform this.\n  // We need to pass in the address of the first element of the vector count,\n  // so we get the address of count.front() and cast it to be a pointer to a C type.\n  // This will allow us to pass in the address of count.front().\n  // We need to pass in the address of bins as well,\n  // but we want to send it to each rank separately, not all at once.\n  // So we pass in the address of the first element of the array bins,\n  // but we also specify a stride of sizeof(bins.front())\n  // to tell MPI that we want to send sizeof(bins.front()) bytes.\n  // We pass in the number of elements in the array.\n  // We also need to specify a datatype.\n  // In this case, we want to send a size_t.\n  // We also need to specify the operation we want to perform.\n  // In this case, we want to sum the counts of each bin across all ranks.\n  // Finally, we need to specify the communicator we want to use.\n  // In this case, MPI_COMM_WORLD.\n  // If we don't specify a communicator, then MPI_COMM_WORLD is the default.\n  MPI_Datatype bin_type;\n  MPI_Type_contiguous(sizeof(bins.front()), MPI_BYTE, &bin_type);\n  MPI_Type_commit(&bin_type);\n  MPI_Allreduce(count.data(), bins.data(), 26, bin_type, MPI_SUM, MPI_COMM_WORLD);\n  MPI_Type_free(&bin_type);\n}",
            "// TODO: your code here\n\n}",
            "if (s.size() == 0) {\n    return;\n  }\n  std::array<size_t, 26> counts{};\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      counts[c - 'a']++;\n    }\n  }\n  std::array<size_t, 26> local_counts{};\n  MPI_Reduce(counts.data(), local_counts.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (MPI_COMM_WORLD.rank() == 0) {\n    for (size_t i = 0; i < local_counts.size(); i++) {\n      bins[i] = local_counts[i];\n    }\n  }\n}",
            "// Your code here.\n}",
            "int rank = 0;\n    int world_size = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // Count how many elements in s start with each letter\n    std::array<size_t, 26> letter_counts{};\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i) {\n        ++letter_counts[static_cast<size_t>(s[i][0]) - static_cast<size_t>('a')];\n    }\n\n    // Accumulate the counts\n    std::array<size_t, 26> letter_counts_all{};\n    MPI_Allreduce(letter_counts.data(), letter_counts_all.data(), 26, MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n    // Store the counts in the bins\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = letter_counts_all[i];\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    const char *chars = \"abcdefghijklmnopqrstuvwxyz\";\n\n    int local_counts[26] = { 0 };\n\n    for (auto const& str : s) {\n        if (str.size() == 0) {\n            continue;\n        }\n        local_counts[str[0] - 'a']++;\n    }\n\n    std::array<int, 26> all_counts;\n\n    MPI_Reduce(local_counts, all_counts.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] = all_counts[i];\n        }\n    }\n}",
            "// Compute the number of strings in each bin\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n\n  // Compute the prefix sums\n  std::partial_sum(bins.begin(), bins.end(), bins.begin());\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (size_t i = 0; i < s.size(); i++) {\n    bins[s[i][0] - 'a']++;\n  }\n}",
            "}",
            "// Your code here\n  bins.fill(0);\n  auto const rank = MPI::COMM_WORLD.Get_rank();\n  auto const size = MPI::COMM_WORLD.Get_size();\n  #pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    auto const &str = s[i];\n    auto const idx = static_cast<size_t>(str[0] - 'a');\n    bins[idx] += 1;\n  }\n  if (rank!= 0)\n    return;\n\n  auto const n = s.size();\n  for (int i = 1; i < size; ++i) {\n    MPI::COMM_WORLD.Recv(bins.data(), bins.size(), MPI::INT, i, i, MPI::STATUSES_IGNORE);\n  }\n}",
            "// TODO: Your code goes here\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int div = s.size() / world_size;\n    int start, end;\n    if (world_rank == 0) {\n        start = 0;\n        end = div;\n    } else {\n        start = div * world_rank + 1;\n        end = div * (world_rank + 1);\n    }\n    std::vector<std::string> my_s(s.begin() + start, s.begin() + end);\n    std::array<size_t, 26> my_bins;\n    memset(my_bins.data(), 0, sizeof(my_bins));\n    #pragma omp parallel for\n    for (int i = 0; i < my_s.size(); i++) {\n        for (int j = 0; j < my_s[i].size(); j++) {\n            my_bins[my_s[i][j] - 'a'] += 1;\n        }\n    }\n    MPI_Gather(my_bins.data(), 26, MPI_INT, bins.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Find start and end of my array slice\n    int myStart = rank * (s.size() / size);\n    int myEnd = (rank + 1) * (s.size() / size);\n    if (rank == size - 1) {\n        myEnd = s.size();\n    }\n\n    std::array<size_t, 26> threadBins;\n    memset(threadBins.data(), 0, threadBins.size() * sizeof(size_t));\n#pragma omp parallel default(none) shared(myStart, myEnd, s, bins, threadBins)\n    {\n        int threadId = omp_get_thread_num();\n\n        for (int i = myStart; i < myEnd; i++) {\n            char firstLetter = s[i][0];\n            if (firstLetter >= 'a' && firstLetter <= 'z') {\n                threadBins[firstLetter - 'a']++;\n            }\n        }\n\n        for (int i = 0; i < 26; i++) {\n            bins[i] += threadBins[i];\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t length = s.size();\n\n  int rank;\n  int procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &procs);\n\n  // Compute the number of elements each thread will have to process\n  int local_length = length / procs;\n\n  // Compute the number of remaining elements in the first\n  // `procs - length % procs` processes\n  int remainder = length % procs;\n  if (rank < remainder) {\n    local_length++;\n  }\n\n  std::string str = s[0];\n  int local_bin = str[0] - 'a';\n\n  bins[local_bin] = 1;\n\n  #pragma omp parallel for reduction(+:bins[local_bin]) schedule(runtime)\n  for (int i = 1; i < local_length; i++) {\n    str = s[i];\n    int new_bin = str[0] - 'a';\n    if (new_bin!= local_bin) {\n      bins[local_bin]++;\n      local_bin = new_bin;\n    }\n  }\n\n  std::array<size_t, 26> bins_local;\n  MPI_Gather(bins.data(), 26, MPI_UNSIGNED_LONG, bins_local.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] += bins_local[i];\n    }\n  }\n}",
            "}",
            "// TODO\n}",
            "// bins[0] = number of strings starting with letter 'a'\n    // bins[1] = number of strings starting with letter 'b'\n    //...\n    // bins[25] = number of strings starting with letter 'z'\n    std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "// Write your code here!\n}",
            "// TODO: your implementation here\n  bins = std::array<size_t, 26>{};\n  std::fill(bins.begin(), bins.end(), 0);\n\n  auto const rank = omp_get_thread_num();\n  auto const size = omp_get_num_threads();\n  auto const stride = s.size() / size;\n  auto const rem = s.size() % size;\n\n  auto const start = rank * stride + (rank < rem? rank : rem);\n  auto const end = start + stride + (rank < rem? 1 : 0);\n\n  for (auto i = start; i < end; ++i) {\n    auto const& str = s[i];\n    auto const letter = str[0] - 'a';\n    ++bins[letter];\n  }\n}",
            "// 1. Broadcast s to all ranks\n    // 2. Each rank computes its own histogram of the first letters of strings\n    // 3. Each rank sends its histogram back to rank 0\n    // 4. Rank 0 combines all the histograms to form a complete histogram\n\n    // 1. Broadcast s to all ranks\n    std::vector<std::string> s_bcast;\n    MPI_Bcast(&s, s.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // 2. Each rank computes its own histogram of the first letters of strings\n    std::array<size_t, 26> hists;\n    for(size_t i = 0; i < s.size(); ++i) {\n        // Each rank has its own copy of the s[i] string\n        const char c = s[i][0];\n        hists[c - 'a']++;\n    }\n\n    // 3. Each rank sends its histogram back to rank 0\n    MPI_Gather(&hists, 26, MPI_INT, &bins, 26, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if(0 == rank) {\n        // 4. Rank 0 combines all the histograms to form a complete histogram\n        #pragma omp parallel for\n        for(size_t i = 1; i < 26; ++i) {\n            bins[i] += bins[i - 1];\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n}",
            "// TODO: IMPLEMENT ME\n    \n    #pragma omp parallel for schedule(static)\n    for(int i = 0; i < s.size(); i++){\n        bins[s[i][0]-'a']++;\n    }\n}",
            "bins.fill(0);\n    size_t n = s.size();\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int idx = (s[i][0] - 'a');\n        bins[idx]++;\n    }\n}",
            "const int P = MPI::COMM_WORLD.Get_size();\n  const int rank = MPI::COMM_WORLD.Get_rank();\n  std::vector<std::string> local_s(s.size());\n  std::copy(s.begin(), s.end(), local_s.begin());\n  const int local_size = local_s.size();\n  size_t local_bins[26] = {0};\n  #pragma omp parallel for\n  for (int i = 0; i < local_size; ++i) {\n    ++local_bins[local_s[i][0] - 'a'];\n  }\n\n  std::array<size_t, 26> sums[P];\n  for (int i = 0; i < P; ++i) {\n    sums[i].fill(0);\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < 26; ++i) {\n    for (int j = 0; j < P; ++j) {\n      if (j == rank) {\n        sums[j][i] = local_bins[i];\n      }\n    }\n  }\n\n  for (int i = 0; i < 26; ++i) {\n    for (int j = 1; j < P; ++j) {\n      MPI::COMM_WORLD.Recv(sums[j].data() + i, 1, MPI::INT, j, i);\n    }\n  }\n\n  for (int i = 1; i < P; ++i) {\n    for (int j = 0; j < 26; ++j) {\n      sums[0][j] += sums[i][j];\n    }\n  }\n\n  if (rank == 0) {\n    std::copy(sums[0].begin(), sums[0].end(), bins.begin());\n  }\n}",
            "// TODO: Your code here!\n}",
            "//...\n}",
            "// TODO:\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int n = s.size();\n  int chunk = n / size;\n\n  int start = rank * chunk;\n  int end = start + chunk;\n\n  int* counts = new int[26];\n  int* loc_counts = new int[26];\n\n  int my_result = 0;\n\n  // each process will do the same\n  for(int i = start; i < end; i++)\n  {\n    std::string str = s[i];\n    for(int j = 0; j < 26; j++)\n    {\n      if(str[0] == 'a' + j)\n      {\n        counts[j]++;\n      }\n    }\n  }\n\n  MPI_Reduce(counts, loc_counts, 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(rank == 0)\n  {\n    for(int i = 0; i < 26; i++)\n    {\n      bins[i] = loc_counts[i];\n    }\n  }\n\n  delete[] counts;\n  delete[] loc_counts;\n}",
            "std::array<size_t, 26> localBins;\n  for (auto &l: localBins) {\n    l = 0;\n  }\n#pragma omp parallel\n  {\n    std::array<size_t, 26> localBins_private;\n    for (auto &l: localBins_private) {\n      l = 0;\n    }\n#pragma omp for schedule(static)\n    for (size_t i = 0; i < s.size(); ++i) {\n      char letter = s[i][0];\n      ++localBins_private[letter - 'a'];\n    }\n#pragma omp critical\n    {\n      for (size_t i = 0; i < 26; ++i) {\n        localBins[i] += localBins_private[i];\n      }\n    }\n  }\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] += localBins[i];\n  }\n}",
            "// TODO: fill in your code\n    // Your code should run in parallel using OpenMP and MPI\n    size_t n = s.size();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int range = n / size;\n    int start = rank * range;\n    int end = start + range;\n    if(rank == size - 1)\n    {\n        end = n;\n    }\n\n    std::vector<size_t> local_bins(26, 0);\n    for (int i = start; i < end; i++) {\n        size_t val = s[i][0] - 96;\n        local_bins[val]++;\n    }\n\n    //std::array<size_t, 26> global_bins = {0};\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    //bins = global_bins;\n}",
            "// TODO: implement\n    int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int max_size = 26;\n    int chunks = 100;\n    std::vector<int> counters(max_size);\n    int my_size = s.size() / world_size;\n    int my_start = s.size() / world_size * world_rank;\n    int my_end = my_start + my_size;\n    if (world_rank == world_size - 1) {\n        my_end = s.size();\n    }\n    for (int i = my_start; i < my_end; ++i) {\n        for (char c : s[i]) {\n            counters[c - 'a']++;\n        }\n    }\n    std::vector<int> sums(max_size);\n    MPI_Allreduce(counters.data(), sums.data(), max_size, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    std::copy(sums.begin(), sums.end(), bins.begin());\n}",
            "// TODO: fill in\n}",
            "int nprocs = 0;\n  int myrank = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n  size_t chunk = s.size() / nprocs;\n  auto start = myrank * chunk;\n  auto end = start + chunk;\n  if (myrank == nprocs - 1) {\n    end = s.size();\n  }\n  std::array<size_t, 26> mybins{};\n  for (size_t i = start; i < end; i++) {\n    mybins[s[i][0] - 'a']++;\n  }\n  MPI_Allreduce(mybins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "size_t n = s.size();\n  bins.fill(0);\n  int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  // parallel implementation\n  omp_set_num_threads(nprocs);\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    int thread_id = omp_get_thread_num();\n    bins[s[i][0] - 'a'] += 1;\n  }\n\n  // reduce\n  MPI_Reduce(bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "std::array<size_t, 26> localBins{};\n  int world_size;\n  int world_rank;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  size_t totalCount = s.size();\n  size_t portion = totalCount/world_size;\n  std::vector<std::string> local_s(portion);\n\n  #pragma omp parallel for\n  for(int i = 0; i < portion; i++) {\n    local_s[i] = s[i];\n  }\n\n  std::array<size_t, 26> local_bins{};\n  for (char c = 'a'; c <= 'z'; c++) {\n    #pragma omp parallel for\n    for (size_t i = 0; i < local_s.size(); i++) {\n      if(local_s[i][0] == c) {\n        #pragma omp critical\n        {\n          local_bins[c - 'a']++;\n        }\n      }\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* Your code here */\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // We'll use `counts` to store the number of occurrences of each letter\n  std::array<size_t, 26> counts;\n  for (size_t i = 0; i < 26; i++) {\n    counts[i] = 0;\n  }\n\n  // Split the array of strings evenly among the ranks.\n  // This is O(n) since it's a constant time operation.\n  // We also need to split up the counts array such that each rank gets a complete copy.\n  std::vector<std::string> local_s = s;\n  std::array<size_t, 26> local_counts = counts;\n\n  // The following is O(n) as well.\n  size_t n_items = local_s.size();\n  int n_per_rank = n_items / size;\n  int leftover = n_items % size;\n\n  // Each rank needs to know how many elements to count\n  int n_to_count = n_per_rank;\n  if (rank < leftover) n_to_count++;\n\n  #pragma omp parallel for\n  for (int i = 0; i < n_to_count; i++) {\n    // For each string, increment the count for the first letter\n    size_t first_letter = local_s[rank * n_per_rank + i][0] - 'a';\n    local_counts[first_letter]++;\n  }\n\n  // Reduction. Each rank sends its counts to rank 0, who gets all of them.\n  MPI_Reduce(counts.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n#pragma omp master\n{\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n}\n    #pragma omp parallel for\n#pragma omp master\n    for (size_t i = 0; i < s.size(); ++i) {\n        #pragma omp atomic\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "if (s.size() == 0) return;\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tint chunk_size = s.size() / size;\n\tint remainder = s.size() % size;\n\tstd::vector<std::string> chunk;\n\tbins = std::array<size_t, 26> {};\n\tif (rank < remainder) {\n\t\tchunk_size++;\n\t}\n\n\tfor (int r = 0; r < size; ++r) {\n\t\tint start = rank * chunk_size + std::min(r, remainder);\n\t\tint end = start + chunk_size + (r < remainder);\n\t\tif (rank == r) {\n\t\t\tchunk.assign(s.begin() + start, s.begin() + end);\n\t\t}\n\t\tint count = end - start;\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t\t#pragma omp parallel for\n\t\tfor (int i = 0; i < count; ++i) {\n\t\t\tauto& s = chunk[i];\n\t\t\tif (s.size() > 0) {\n\t\t\t\tbins[s[0] - 'a']++;\n\t\t\t}\n\t\t}\n\t\tMPI_Barrier(MPI_COMM_WORLD);\n\t}\n\tMPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Replace this with your implementation.\n\n  int rank;\n  int world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  size_t num_elements = s.size();\n\n  std::vector<size_t> count_array(26);\n  size_t count;\n  int num_threads;\n\n  if (rank == 0) {\n    for (size_t i = 0; i < num_elements; ++i) {\n      count = 0;\n      num_threads = omp_get_max_threads();\n      if (num_threads > 1) {\n        #pragma omp parallel for reduction(+:count)\n        for (int thread_id = 0; thread_id < num_threads; ++thread_id) {\n          int thread_count = count;\n          for (size_t j = 0; j < s[i].length(); ++j) {\n            if (tolower(s[i][j]) - 'a' == thread_id) {\n              ++thread_count;\n            }\n          }\n          count += thread_count;\n        }\n      } else {\n        for (size_t j = 0; j < s[i].length(); ++j) {\n          if (tolower(s[i][j]) - 'a' == 0) {\n            ++count;\n          }\n        }\n      }\n      count_array[tolower(s[i][0]) - 'a'] += count;\n    }\n  }\n\n  MPI_Scatter(&count_array, 26, MPI_INT, &bins, 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n  int my_rank = 0, nproc = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  const size_t n = s.size();\n  std::string current;\n  std::array<size_t, 26> count{};\n  if (my_rank == 0) {\n    count.fill(0);\n    for (int i = 0; i < n; i++) {\n      current = s[i];\n      count[current[0] - 97]++;\n    }\n  }\n  MPI_Bcast(count.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  size_t start_index = 0;\n  if (my_rank == 0) {\n    bins = count;\n  } else {\n    start_index = 1000000;\n  }\n  const size_t end_index = n / nproc + (my_rank == nproc - 1? n % nproc : 0);\n  for (int i = start_index; i < end_index; i++) {\n    current = s[i];\n    count[current[0] - 97]++;\n  }\n  MPI_Gather(count.data(), 26, MPI_UNSIGNED_LONG_LONG, bins.data(), 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t n = s.size();\n    std::array<size_t, 26> local_bins;\n\n    /* each rank has a local copy of s, and computes the local_bins array */\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < n; i++) {\n            size_t index = s[i][0] - 'a';\n            local_bins[index]++;\n        }\n    }\n\n    /* reduce local_bins to the global array */\n    MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "auto const count = s.size();\n\n    #pragma omp parallel\n    {\n        int rank;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n        #pragma omp for\n        for (int i = 0; i < count; i++) {\n            auto const letter = s[i][0] - 'a';\n            if (letter >= 0 && letter <= 25) {\n                #pragma omp critical\n                {\n                    bins[letter]++;\n                }\n            }\n        }\n    }\n}",
            "int rank, numRanks;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int chunkSize = s.size() / numRanks;\n\n    std::vector<std::string> sChunk;\n    std::vector<int> letterCounts(26, 0);\n    for(int i = rank * chunkSize; i < (rank + 1) * chunkSize && i < s.size(); i++) {\n        sChunk.push_back(s[i]);\n        for(char const& c : s[i]) {\n            letterCounts[c - 'a']++;\n        }\n    }\n\n    std::array<int, 26> binsChunk;\n    for(int i = 0; i < 26; i++) {\n        binsChunk[i] = letterCounts[i];\n    }\n\n    MPI_Reduce(binsChunk.data(), bins.data(), binsChunk.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        for(int i = 0; i < 26; i++) {\n            bins[i] /= chunkSize;\n        }\n    }\n}",
            "// TODO: Your code here\n  // First compute the sum of lengths of all strings in `s`.\n  // Send the sum to all processes.\n  int n = s.size();\n  int sum = 0;\n#pragma omp parallel for reduction(+ : sum)\n  for (int i = 0; i < n; ++i)\n    sum += s[i].length();\n  MPI_Allreduce(MPI_IN_PLACE, &sum, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  // Now, for each string in `s`, send the first letter in the string to `rank` that owns the string.\n  // Every rank should have a copy of the string.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<int> counts(size, 0);\n  for (int i = 0; i < n; ++i) {\n    int first = s[i].length() > 0? s[i][0] - 'a' : 26;\n    int owner = first % size;\n    MPI_Send(&first, 1, MPI_INT, owner, 0, MPI_COMM_WORLD);\n  }\n\n  // Receive counts of how many strings start with each letter.\n  MPI_Status status;\n  MPI_Recv(&counts[0], size, MPI_INT, MPI_ANY_SOURCE, 0, MPI_COMM_WORLD, &status);\n\n  // Now, accumulate the counts in bins.\n  for (int i = 0; i < size; ++i) {\n    for (int j = 0; j < counts[i]; ++j) {\n      int first;\n      MPI_Recv(&first, 1, MPI_INT, status.MPI_SOURCE, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[first] += 1;\n    }\n  }\n}",
            "auto const n = s.size();\n    std::array<size_t, 26> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n    int my_rank, n_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    std::vector<size_t> counts(n_ranks, 0);\n\n#pragma omp parallel default(none) shared(local_bins, counts)\n{\n#pragma omp for schedule(static, 1)\n    for (auto i = 0; i < n; ++i) {\n        ++local_bins[s[i][0] - 'a'];\n    }\n    ++counts[my_rank];\n}\n\n    MPI_Reduce(counts.data(), counts.data() + n_ranks - 1, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    std::vector<size_t> recvcounts(counts.data(), counts.data() + n_ranks - 1);\n    std::vector<size_t> displs(n_ranks - 1);\n    displs[0] = 0;\n    for (auto i = 1; i < n_ranks - 1; ++i) {\n        displs[i] = displs[i - 1] + recvcounts[i - 1];\n    }\n\n    std::vector<size_t> sendcounts(n_ranks - 1);\n    std::copy(recvcounts.begin(), recvcounts.end(), sendcounts.begin());\n    std::array<size_t, 26> recv_local_bins;\n\n    MPI_Scatterv(local_bins.data(), sendcounts.data(), displs.data(), MPI_UNSIGNED_LONG, recv_local_bins.data(),\n                recvcounts[my_rank], MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        std::copy(recv_local_bins.begin(), recv_local_bins.end(), bins.begin());\n    }\n}",
            "// write your code here\n    // NOTE: your code should be parallelized with OpenMP and MPI\n    const size_t my_rank = omp_get_thread_num();\n    const size_t num_threads = omp_get_num_threads();\n    const size_t length = s.size();\n    const size_t chunk = length / num_threads;\n    const size_t remainder = length % num_threads;\n\n    std::array<int, 26> count_per_thread;\n    std::array<int, 26> count_total;\n\n    #pragma omp parallel\n    {\n        std::array<int, 26> thread_count;\n        #pragma omp for schedule(static, 1)\n        for (size_t i = my_rank * chunk; i < (my_rank + 1) * chunk + remainder; i++) {\n            size_t j = 0;\n            size_t len = s[i].size();\n            while (j < len) {\n                if (isalpha(s[i][j])) {\n                    thread_count[tolower(s[i][j]) - 'a']++;\n                    j++;\n                } else {\n                    j++;\n                }\n            }\n        }\n        for (int i = 0; i < 26; i++) {\n            count_per_thread[i] += thread_count[i];\n        }\n    }\n\n    MPI_Reduce(count_per_thread.data(), count_total.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins[i] += count_total[i];\n        }\n    }\n}",
            "int rank, ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n    size_t len = s.size();\n    int first_letter_of_word = 0;\n\n    // TODO: replace this with your code\n    int count = 0;\n    int first_letter_index = 0;\n    int first_letter_rank = 0;\n\n    // compute how many words begin with the letter index\n    for (auto i = 0; i < len; i++)\n    {\n        if (s[i][0]!= first_letter_index + 'a')\n        {\n            if (first_letter_rank == 0)\n                first_letter_rank = rank;\n            else if (first_letter_rank!= rank)\n            {\n                MPI_Send(&count, 1, MPI_INT, first_letter_rank, first_letter_index, MPI_COMM_WORLD);\n                count = 0;\n                first_letter_rank = rank;\n            }\n            first_letter_index = s[i][0] - 'a';\n        }\n        count++;\n    }\n\n    MPI_Send(&count, 1, MPI_INT, first_letter_rank, first_letter_index, MPI_COMM_WORLD);\n\n    // compute how many words begin with the letter index\n    for (int letter = 0; letter < 26; letter++)\n    {\n        if (rank == 0)\n            bins[letter] = 0;\n        else\n            bins[letter] = -1;\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // receive counts from other processors\n    MPI_Status status;\n    int letter_counts[26];\n    int letter_count;\n    for (int letter = 0; letter < 26; letter++)\n    {\n        if (rank!= 0)\n        {\n            MPI_Recv(&letter_count, 1, MPI_INT, 0, letter, MPI_COMM_WORLD, &status);\n            letter_counts[letter] = letter_count;\n        }\n        else\n            letter_counts[letter] = 0;\n    }\n\n    // receive counts from other processors\n    for (int letter = 0; letter < 26; letter++)\n    {\n        if (rank!= 0)\n        {\n            MPI_Recv(&letter_count, 1, MPI_INT, 0, letter, MPI_COMM_WORLD, &status);\n            bins[letter] = letter_count;\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // send the counts to other processors\n    for (int letter = 0; letter < 26; letter++)\n    {\n        if (rank!= 0)\n            MPI_Send(&bins[letter], 1, MPI_INT, 0, letter, MPI_COMM_WORLD);\n    }\n}",
            "size_t const numRanks = 4;\n    size_t const numStrings = s.size();\n\n    size_t const numPerRank = numStrings / numRanks;\n    size_t const remainder = numStrings % numRanks;\n\n    // First do the first few strings on each rank\n    // Use a linear scan\n    // The number of strings is a multiple of the number of ranks\n    // If there is a remainder, some ranks will do more work\n    for (size_t i = 0; i < numPerRank; ++i) {\n        size_t const rank = omp_get_thread_num();\n        char const c = s[rank * numPerRank + i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n        }\n    }\n\n    // Now do the remainder. This is not as efficient as it could be\n    // because the number of strings is not a multiple of the number of ranks\n    // We could do some work in each rank that does not involve global communication\n    for (size_t i = 0; i < remainder; ++i) {\n        size_t const rank = omp_get_thread_num();\n        size_t const rankFirst = rank * numPerRank + numPerRank;\n        char const c = s[rankFirst + i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n        }\n    }\n\n    // Now combine the results of each rank using MPI\n    // Note that we could do this in parallel also, but it is not needed here\n    // The output array is not large, so this is not a bottleneck\n\n    // We want to have 26 integers on each rank, each with the sum of the firstLetterCounts on the rank\n    // We can't store these on each rank separately, we need to have one big array to put them all in\n    // So each rank needs to send its firstLetterCounts to rank 0\n    // Then rank 0 combines them\n    // If we want to make this efficient, we can reduce the amount of communication by doing\n    // this in two steps:\n    // 1. Each rank sends its firstLetterCounts to rank 0\n    // 2. Rank 0 combines them, storing the result in its bins array\n    // The above would be more efficient if the size of the array was small, say 10\n    std::array<size_t, 26> firstLetterCountsOnRank;\n    MPI_Gather(&bins, 26, MPI_UNSIGNED_LONG_LONG, &firstLetterCountsOnRank, 26, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // If we are rank 0, we can now combine all the firstLetterCountsOnRank into bins\n    // We can do this with a parallel reduction as well\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        std::array<size_t, 26> localBins;\n        for (size_t i = 0; i < 26; ++i) {\n            localBins[i] = firstLetterCountsOnRank[i];\n        }\n        // Now do a parallel reduction\n        // This is a bit complicated because we do not have a std::reduce function that accepts a lambda\n        // So we use a parallel scan\n        // This gives us a vector of partial sums\n        std::vector<size_t> partialSums;\n        partialSums.reserve(26);\n        // We start with a vector of 26 zeros\n        partialSums.push_back(0);\n        for (size_t i = 0; i < 26; ++i) {\n            partialSums.push_back(partialSums.back() + localBins[i]);\n        }\n\n        // Now combine the partial sums into the final bins\n        // This uses a parallel prefix sum\n        for (size_t i = 0; i < 26; ++i) {\n            bins[i] = partialSums[i + 1] - partialSums[i];\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n}",
            "std::array<size_t, 26> localBins;\n\n  #pragma omp parallel for\n  for(size_t i = 0; i < s.size(); i++) {\n    localBins[s[i][0] - 'a']++;\n  }\n\n  // Each rank now has a local copy of the global `bins` array, sum them up:\n  MPI_Reduce(localBins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    ++bins[s[i][0] - 'a'];\n  }\n}",
            "// TODO: Your code here\n\n  /* Use the following to test your function. The code will not be graded. */\n  /*\n  if(MPI_COMM_WORLD!= MPI_COMM_NULL) {\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if(rank == 0) {\n      if(size == 2) {\n        auto result = firstLetterCounts(s);\n        std::cout << \"The result is: \";\n        for(auto r : result) {\n          std::cout << r << \" \";\n        }\n        std::cout << std::endl;\n      } else {\n        std::cout << \"Number of ranks is not equal to 2\" << std::endl;\n      }\n    }\n  }\n  */\n}",
            "auto n_procs = static_cast<int>(s.size());\n  std::vector<std::vector<size_t>> bins_local(n_procs);\n#pragma omp parallel\n  {\n#pragma omp for\n    for (auto i = 0; i < s.size(); i++) {\n      auto c = std::tolower(s[i][0]);\n      if (c >= 'a' && c <= 'z') {\n        bins_local[i].push_back(c - 'a');\n      }\n    }\n  }\n\n  MPI_Allgather(bins_local.data(), bins_local.size(), MPI_LONG_LONG_INT, bins.data(),\n                bins_local.size(), MPI_LONG_LONG_INT, MPI_COMM_WORLD);\n}",
            "// TODO\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int length = s.size();\n  int *count = new int[size];\n  for (int i = 0; i < size; i++) {\n    count[i] = 0;\n  }\n  int *s_count = new int[26];\n  for (int i = 0; i < 26; i++) {\n    s_count[i] = 0;\n  }\n  int *count_all = new int[26];\n  for (int i = 0; i < 26; i++) {\n    count_all[i] = 0;\n  }\n  MPI_Scatter(count, 1, MPI_INT, &length, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  for (int i = 0; i < length; i++) {\n    s_count[s[i][0] - 'a']++;\n  }\n  MPI_Gather(s_count, 26, MPI_INT, count_all, 26, MPI_INT, 0, MPI_COMM_WORLD);\n  for (int i = 0; i < 26; i++) {\n    bins[i] = count_all[i];\n  }\n}",
            "// TODO: Implement me!\n}",
            "// TODO: implement this function\n  int num_threads = 4;\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  std::vector<std::string> s_copy(s.begin(), s.end());\n  std::vector<std::string> temp_bins(26, \"\");\n  std::vector<std::vector<std::string> > bins_thread(num_threads);\n\n  for (int i = 0; i < num_threads; i++) {\n    bins_thread[i].resize(26);\n  }\n\n  int block_size = size / num_threads;\n  int block_remainder = size % num_threads;\n\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < 26; i++) {\n    size_t count = 0;\n    for (int j = i * block_size + std::min(i, block_remainder); j < s.size(); j += size) {\n      if (s[j][0] - 'a' == i) {\n        count++;\n      }\n    }\n    bins[i] = count;\n  }\n\n  MPI_Reduce(bins.data(), temp_bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = temp_bins[i];\n    }\n  }\n}",
            "auto numThreads = std::thread::hardware_concurrency();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    auto myBins = std::array<size_t, 26>{};\n    auto myCount = std::array<size_t, 26>{};\n\n    for (auto i = 0; i < numThreads; ++i) {\n        myBins[i] = 0;\n        myCount[i] = 0;\n    }\n    size_t const myNumStrings = s.size();\n\n    int threadId = 0;\n    int start = rank * myNumStrings / size;\n    int end = (rank == (size - 1))? (s.size()) : ((rank + 1) * myNumStrings / size);\n    #pragma omp parallel\n    {\n        threadId = omp_get_thread_num();\n        size_t count = 0;\n        for (size_t i = start; i < end; ++i) {\n            if (s[i][0] >= 'a' && s[i][0] <= 'z') {\n                myBins[s[i][0] - 'a']++;\n                myCount[s[i][0] - 'a']++;\n            }\n        }\n    }\n\n    MPI_Reduce(myBins.data(), bins.data(), myBins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(myCount.data(), myBins.data(), myBins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (auto i = 0; i < 26; ++i) {\n            if (myBins[i] > 0) {\n                bins[i] /= myBins[i];\n            }\n        }\n    }\n}",
            "const int rank = MPI::COMM_WORLD.Get_rank();\n    const int n_ranks = MPI::COMM_WORLD.Get_size();\n\n    // if rank is zero, all ranks will output the results into bins\n    const bool is_rank_0 = rank == 0;\n\n    const size_t chunk_size = s.size() / n_ranks;\n\n    // create a partial copy of s for this rank\n    std::vector<std::string> partial_copy(chunk_size);\n    std::copy_n(s.begin() + rank * chunk_size, chunk_size, partial_copy.begin());\n\n    // perform the parallel computation\n    size_t chunk_start = rank * chunk_size;\n    size_t chunk_end = chunk_start + chunk_size;\n    size_t num_strings_in_chunk = partial_copy.size();\n\n    std::vector<size_t> local_bins(26);\n#pragma omp parallel for\n    for (size_t i = 0; i < num_strings_in_chunk; i++) {\n        auto const& str = partial_copy[i];\n        if (str.size() > 0) {\n            // increment the count for the letter at the first index in the string\n            local_bins[str[0] - 'a']++;\n        }\n    }\n\n    // sum the local results together\n    std::array<size_t, 26> global_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++) {\n        global_bins[i] = local_bins[i];\n    }\n\n#pragma omp parallel for\n    for (size_t i = 1; i < n_ranks; i++) {\n        std::vector<size_t> remote_bins(26);\n        if (is_rank_0) {\n            MPI::COMM_WORLD.Recv(remote_bins.data(), 26, MPI::UNSIGNED_LONG, i, 1234);\n        }\n        for (size_t i = 0; i < 26; i++) {\n            global_bins[i] += remote_bins[i];\n        }\n        if (is_rank_0) {\n            MPI::COMM_WORLD.Send(global_bins.data(), 26, MPI::UNSIGNED_LONG, i, 1234);\n        }\n    }\n\n    // copy the result from rank zero into bins\n    if (is_rank_0) {\n        std::copy(global_bins.begin(), global_bins.end(), bins.begin());\n    }\n}",
            "// TODO: implement this function\n\n\tint n = s.size();\n\tint rank, nprocs;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\tint count = 0;\n\tstd::vector<int> local_count(26, 0);\n\tstd::vector<int> local_bins(26, 0);\n\tfor (size_t i = 0; i < n; i++) {\n\t\tint j = s[i][0] - 'a';\n\t\tif (j >= 0 && j < 26) {\n\t\t\tlocal_count[j]++;\n\t\t}\n\t}\n\n\tMPI_Reduce(&local_count[0], &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int numthreads = omp_get_num_threads();\n  int tid = omp_get_thread_num();\n  int num_elts = s.size();\n  int elt_per_thread = num_elts / numthreads;\n  int offset = rank * elt_per_thread;\n\n  size_t num_elts_local = (rank == size - 1)? (num_elts - (offset + elt_per_thread)) : elt_per_thread;\n\n  std::vector<size_t> local_bins(26);\n  for (int i = tid; i < num_elts_local; i += numthreads) {\n    std::string current_string = s[i + offset];\n    if (current_string.length() > 0) {\n      // local_bins[current_string[0] - 'a']++;\n      local_bins[current_string[0] - 'a'] += 1;\n    }\n  }\n\n  std::array<size_t, 26> all_local_bins;\n  MPI_Allreduce(local_bins.data(), all_local_bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 26; i++) {\n      bins[i] = all_local_bins[i];\n    }\n  }\n}",
            "// TODO: replace this with your implementation\n    //std::cout << \"Hello from rank \" << rank << std::endl;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int p = 0;\n    std::string current = s[0];\n    bins[p] = 1;\n\n    for(unsigned int i = 0; i < s.size(); i++){\n        if(s[i] == current){\n            bins[p]++;\n        }\n        else{\n            current = s[i];\n            p++;\n            bins[p] = 1;\n        }\n    }\n\n    if(rank == 0){\n        MPI_Reduce(bins.data(), bins.data() + bins.size(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n    else{\n        MPI_Reduce(bins.data(), bins.data() + bins.size(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n}",
            "size_t n = s.size();\n  bins = std::array<size_t, 26>();\n  auto bins_local = bins;\n  // OMP\n  #pragma omp parallel\n  {\n    // MPI\n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    size_t chunk_size = n / num_ranks;\n    size_t start = my_rank * chunk_size;\n    size_t end = (my_rank + 1) * chunk_size;\n    if (my_rank == num_ranks - 1) {\n      end = n;\n    }\n    for (auto i = start; i < end; i++) {\n      // count letters in string s[i]\n      // and update bins_local\n      // add code here\n      size_t idx = s[i].find_first_not_of(\"abcdefghijklmnopqrstuvwxyz\");\n      if (idx == std::string::npos)\n        bins_local[s[i][0] - 'a']++;\n      else\n        bins_local[s[i][idx] - 'a']++;\n    }\n    // add code here\n  }\n  // add code here\n  return;\n}",
            "int rank, num_procs;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  int string_length = s.size();\n  int first_letter = 0;\n  std::string first_letter_str;\n\n  size_t size = string_length/num_procs;\n  int left = string_length % num_procs;\n  if(rank < left) {\n    size++;\n  }\n\n  std::vector<std::string> sub_s;\n  sub_s.reserve(size);\n\n  if(rank < left) {\n    for(int i = 0; i < size; i++) {\n      sub_s.push_back(s[rank*size+i]);\n    }\n  } else {\n    for(int i = 0; i < size; i++) {\n      sub_s.push_back(s[left*size+i]);\n    }\n  }\n\n  std::vector<size_t> temp_bins;\n  temp_bins.reserve(26);\n\n  if(rank == 0) {\n    for(int i = 0; i < 26; i++) {\n      temp_bins.push_back(0);\n    }\n  }\n\n  for(auto it = sub_s.begin(); it!= sub_s.end(); ++it) {\n    if((*it)[0] >= 97 && (*it)[0] <= 122) {\n      first_letter = (*it)[0]-97;\n    }\n    temp_bins[first_letter]++;\n  }\n\n  MPI_Reduce(&temp_bins[0], &bins[0], 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if(rank == 0) {\n    for(int i = 0; i < 26; i++) {\n      bins[i] += bins[i-1];\n    }\n  }\n}",
            "int numTasks, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numTasks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::array<size_t, 26> counts;\n    for(size_t i = 0; i < 26; i++) {\n        counts[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for(size_t i = 0; i < s.size(); i++) {\n        size_t firstLetter = static_cast<size_t>(s[i][0]) - 'a';\n        counts[firstLetter]++;\n    }\n\n    size_t chunkSize = s.size() / numTasks;\n    std::vector<std::array<size_t, 26>> chunks(numTasks);\n    for(size_t i = 0; i < numTasks; i++) {\n        std::array<size_t, 26> chunk;\n        for(size_t j = 0; j < 26; j++) {\n            chunk[j] = counts[j];\n        }\n        chunks[i] = chunk;\n    }\n\n    std::vector<size_t> recvCounts(numTasks);\n    std::vector<size_t> displs(numTasks);\n\n    for(int i = 0; i < numTasks; i++) {\n        recvCounts[i] = chunkSize;\n        displs[i] = i * chunkSize;\n    }\n\n    recvCounts[numTasks - 1] += s.size() - (numTasks - 1) * chunkSize;\n    MPI_Scatterv(counts.data(), recvCounts.data(), displs.data(), MPI_INT, chunks[rank].data(), recvCounts[rank], MPI_INT, 0, MPI_COMM_WORLD);\n\n    if(rank == 0) {\n        #pragma omp parallel for\n        for(int i = 1; i < numTasks; i++) {\n            for(size_t j = 0; j < 26; j++) {\n                counts[j] += chunks[i][j];\n            }\n        }\n    }\n\n    MPI_Gatherv(counts.data(), counts.size(), MPI_INT, bins.data(), recvCounts.data(), displs.data(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// size_t my_n = s.size() / n;\n    // size_t my_offset = my_n * rank;\n    // size_t my_count = my_n;\n    // if (rank == n - 1) {\n    //     my_count = s.size() - my_offset;\n    // }\n\n    // bins = std::array<size_t, 26>();\n    // for (size_t i = my_offset; i < my_offset + my_count; i++) {\n    //     char letter = s[i][0] - 'a';\n    //     bins[letter] += 1;\n    // }\n\n    bins = std::array<size_t, 26>();\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        char letter = s[i][0] - 'a';\n        bins[letter] += 1;\n    }\n}",
            "// TODO: your code here\n}",
            "bins.fill(0);\n    std::array<size_t, 26> localBins;\n    localBins.fill(0);\n\n#pragma omp parallel\n    {\n#pragma omp for\n        for (size_t i = 0; i < s.size(); ++i) {\n            if (!s[i].empty()) {\n                localBins[static_cast<unsigned char>(s[i][0])]++;\n            }\n        }\n    }\n\n    // compute prefix sum of local counts\n    std::array<size_t, 26> tmpBins;\n    tmpBins.fill(0);\n\n#pragma omp parallel\n    {\n#pragma omp for\n        for (size_t i = 0; i < 26; ++i) {\n            tmpBins[i] += localBins[i];\n        }\n    }\n\n    // add tmpBins to bins\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] += tmpBins[i];\n    }\n}",
            "// Your code here...\n}",
            "// Write your code here\n}",
            "#pragma omp parallel for schedule(static, 2) num_threads(4)\n    for (size_t i = 0; i < s.size(); ++i) {\n        bins[tolower(s[i][0]) - 'a']++;\n    }\n}",
            "size_t myCount = 0;\n    size_t rank;\n    int numTasks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numTasks);\n    int id = omp_get_thread_num();\n    size_t start = id * s.size() / numTasks;\n    size_t end = (id + 1) * s.size() / numTasks;\n\n    for (size_t i = start; i < end; i++) {\n        if (std::islower(s[i][0])) {\n            ++myCount;\n        }\n    }\n    MPI_Reduce(&myCount, &bins[0], 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  int n = s.size();\n  int nthreads = omp_get_max_threads();\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    std::string s_i = s[i];\n    int n_i = s_i.length();\n    std::array<size_t, 26> bins_i;\n    std::fill(bins_i.begin(), bins_i.end(), 0);\n    #pragma omp parallel for\n    for (int j = 0; j < n_i; ++j) {\n      char c = s_i[j];\n      if ('a' <= c && c <= 'z') {\n        bins_i[c - 'a']++;\n      }\n    }\n    #pragma omp critical\n    for (int j = 0; j < 26; ++j) {\n      bins[j] += bins_i[j];\n    }\n  }\n}",
            "size_t local_counts[26] = {0};\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        ++local_counts[s[i][0] - 'a'];\n    }\n#pragma omp parallel for\n    for (int i = 0; i < 26; ++i) {\n        bins[i] += local_counts[i];\n    }\n}",
            "// TODO\n}",
            "// your code here\n}",
            "// TODO: Your code here\n}",
            "#pragma omp parallel\n    {\n        int thread_num = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n        int nthreads_per_rank = nthreads/MPI_Comm_size(MPI_COMM_WORLD);\n        std::string my_s;\n        int rank;\n        int total_threads = nthreads_per_rank*MPI_Comm_size(MPI_COMM_WORLD);\n\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n        int my_start = rank*nthreads_per_rank;\n        int my_end = my_start + nthreads_per_rank;\n\n        my_s = s[thread_num];\n\n        int my_count = 0;\n        int i = my_start;\n        for (; i < my_end; ++i) {\n            if (i < total_threads) {\n                if (my_s.find(my_s[0]) == 0) {\n                    my_count++;\n                }\n            }\n            my_s = s[i];\n        }\n\n        MPI_Reduce(&my_count, &(bins[0]), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: write your code here\n    size_t size = s.size();\n    int rank;\n    int sizeOfProcs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &sizeOfProcs);\n    int partSize = size / sizeOfProcs;\n    int firstRank = rank * partSize;\n    int lastRank = firstRank + partSize;\n    if (rank == 0) {\n        lastRank = size;\n    }\n    int start = firstRank;\n    int end = lastRank;\n    int count = 0;\n    for (int i = start; i < end; i++) {\n        std::string word = s.at(i);\n        if (word.length() == 0) {\n            continue;\n        }\n        char firstLetter = word.at(0);\n        int index = firstLetter - 'a';\n        bins.at(index)++;\n        count++;\n    }\n    MPI_Reduce(bins.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (int i = 0; i < 26; i++) {\n            bins.at(i) /= count;\n        }\n    }\n    // std::vector<std::string> result;\n    // if (rank == 0) {\n    //     for (int i = 0; i < 26; i++) {\n    //         result.push_back(std::to_string(bins.at(i)));\n    //     }\n    // }\n    // std::string resultStr = \"\";\n    // MPI_Reduce(result.data(), result.data(), result.size(), MPI_CHAR, MPI_SUM, 0, MPI_COMM_WORLD);\n    // if (rank == 0) {\n    //     for (int i = 0; i < result.size(); i++) {\n    //         resultStr += result.at(i);\n    //     }\n    // }\n}",
            "// TODO: Fill in this function\n  // You may need to add code to count the number of threads you have\n  // omp_set_num_threads(4);\n#pragma omp parallel for\n  for(int i=0;i<s.size();i++)\n  {\n    bins[s[i][0]-'a']++;\n  }\n}",
            "// initialize bins to 0\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // find the length of the longest string in s\n    size_t maxLength = 0;\n    for (auto const& str : s) {\n        maxLength = std::max(maxLength, str.size());\n    }\n\n    // every rank computes the count of each letter and stores the result in bins\n    // each rank processes the strings that start with its rank\n    #pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        if (s[i].size() >= maxLength) {\n            bins[s[i][0] - 'a'] += 1;\n        }\n    }\n}",
            "// Initialize all bins to 0\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Compute bins\n    #pragma omp parallel for\n    for(int i = 0; i < s.size(); i++) {\n        char ch = s[i][0];\n        bins[ch - 'a']++;\n    }\n\n}",
            "// TODO: implement me!\n}",
            "// Your code here...\n}",
            "// TODO: Fill in this function\n}",
            "//TODO: IMPLEMENT ME\n}",
            "size_t n = s.size();\n  std::array<size_t, 26> counts;\n  for (size_t i = 0; i < 26; ++i)\n    counts[i] = 0;\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i)\n    ++counts[s[i][0] - 'a'];\n  #pragma omp parallel for\n  for (size_t i = 0; i < 26; ++i)\n    bins[i] = counts[i];\n}",
            "int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    int len = s.size();\n    std::vector<int> counts(26, 0);\n    for (int i = 0; i < len; i++) {\n        char c = std::tolower(s[i][0]);\n        if (c >= 'a' && c <= 'z') {\n            counts[c - 'a']++;\n        }\n    }\n    // TODO: Use MPI_Gather to gather the count from all processes into counts on rank 0.\n    if (rank == 0) {\n        bins = std::array<size_t, 26>();\n        for (int i = 0; i < nprocs; i++) {\n            for (int j = 0; j < 26; j++) {\n                bins[j] += counts[j];\n            }\n        }\n    }\n    // TODO: Use MPI_Scatter to scatter the counts array from rank 0 to all the other processes.\n    // TODO: Compute the correct counts from rank 0 and place the results in `bins`.\n    MPI_Scatter(counts.data(), 26, MPI_INT, bins.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < s.size(); ++i) {\n    bins[s[i][0] - 'a'] += 1;\n  }\n}",
            "size_t n = s.size();\n    std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        ++bins[s[i][0] - 'a'];\n    }\n    if (omp_get_thread_num() == 0) {\n        size_t total = 0;\n        MPI_Reduce(bins.data(), bins.data() + 26, 1, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Implement\n    // MPI_Init();\n    // MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // if(rank == 0)\n    //   MPI_Send(s.data(), s.size(), MPI_CHAR, 1, 0, MPI_COMM_WORLD);\n    // else if(rank == 1)\n    //   MPI_Recv(s.data(), s.size(), MPI_CHAR, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    // // for (int i = 0; i < s.size(); ++i)\n    // //   std::cout << s[i] << \" \";\n    // // std::cout << std::endl;\n    // // std::cout << \"rank = \" << rank << \", ranks = \" << ranks << std::endl;\n    // #pragma omp parallel\n    // {\n    //   #pragma omp single\n    //   #pragma omp taskloop grainsize(1)\n    //   for (int i = 0; i < s.size(); ++i) {\n    //     bins[s[i][0] - 'a']++;\n    //   }\n    // }\n    // MPI_Finalize();\n}",
            "// TODO: replace this with your implementation\n\n    // TODO: implement me\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // compute number of strings per letter\n  std::array<size_t, 26> localBins = {};\n\n  // for each string\n  for (auto const& str : s) {\n    // for each character in the string\n    for (auto const& c : str) {\n      // increase the count of the corresponding character\n      localBins[c - 'a']++;\n    }\n  }\n\n  // Reduce across MPI ranks.\n  // The result is stored on rank 0.\n  std::array<size_t, 26> globalBins = {};\n  MPI_Reduce(localBins.data(), globalBins.data(), localBins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    // Each rank is responsible for a subset of the data.\n    // Compute the number of strings that start with a given letter.\n    // Store the result in bins.\n    for (size_t i = 0; i < localBins.size(); i++) {\n      bins[i] = globalBins[i];\n    }\n  }\n}",
            "// Your code here\n}",
            "/* Get rank and number of ranks */\n  int rank;\n  int nRanks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n  /* Get size of input */\n  size_t nStrings = s.size();\n  size_t nLetters = 26;\n\n  /* If we are rank 0, then we need to initialize the bins */\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  /* Get the number of letters per rank */\n  size_t nLettersPerRank = (nLetters + nRanks - 1) / nRanks;\n  size_t nLettersStart = rank * nLettersPerRank;\n  size_t nLettersEnd = std::min((rank + 1) * nLettersPerRank, nLetters);\n  size_t nLettersThisRank = nLettersEnd - nLettersStart;\n\n  /* Initialize counter to each letter */\n  std::vector<size_t> nLettersEachRank(nLettersThisRank);\n#pragma omp parallel for\n  for (size_t i = 0; i < nLettersThisRank; i++) {\n    char letter = nLettersStart + i + 'a';\n    nLettersEachRank[i] = 0;\n    for (std::string const& str : s) {\n      if (str[0] == letter) {\n        nLettersEachRank[i] += 1;\n      }\n    }\n  }\n\n  /* Compute the counts of the letters */\n  std::vector<size_t> counts(nLettersThisRank, 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < nLettersThisRank; i++) {\n    for (size_t j = 0; j < nStrings; j++) {\n      char letter = nLettersStart + i + 'a';\n      if (s[j][0] == letter) {\n        counts[i] += 1;\n      }\n    }\n  }\n\n  /* Combine the counts */\n  std::vector<size_t> recvCounts(nLettersThisRank, 0);\n  std::vector<size_t> recvDispls(nLettersThisRank, 0);\n  MPI_Gather(&nLettersEachRank[0], nLettersThisRank, MPI_UNSIGNED_LONG, &recvCounts[0], nLettersThisRank, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  recvDispls[0] = 0;\n  for (size_t i = 1; i < nLettersThisRank; i++) {\n    recvDispls[i] = recvDispls[i - 1] + recvCounts[i - 1];\n  }\n  MPI_Gatherv(&counts[0], nLettersThisRank, MPI_UNSIGNED_LONG, &bins[0], &recvCounts[0], &recvDispls[0], MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n}",
            "// 1. Fill bins with zeroes\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // 2. Use OpenMP to iterate over the vector s and count the number of strings that start with each letter in\n  // the alphabet. Store the result in the bins array. Use the following code as a template to complete this\n  // task.\n\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); ++i) {\n    char firstLetter = s[i][0];\n    if (firstLetter >= 'a' && firstLetter <= 'z') {\n      ++bins[firstLetter - 'a'];\n    }\n  }\n\n  // 3. Use MPI to gather the result on rank 0.\n  MPI_Gather(bins.data(), 26, MPI_INT, bins.data(), 26, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n  std::for_each(std::execution::par_unseq, s.cbegin(), s.cend(),\n                [&bins](std::string const &str) {\n                  ++bins[str[0] - 'a'];\n                });\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 26; i++) {\n        bins[i] = 0;\n    }\n\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        char c = tolower(s[i][0]);\n        bins[c - 'a']++;\n    }\n\n#pragma omp parallel for\n    for (size_t i = 1; i < bins.size(); i++) {\n        bins[i] += bins[i - 1];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n\tfor (auto const& str : s) {\n\t\tbins[str[0] - 'a']++;\n\t}\n}",
            "std::array<std::array<size_t, 26>, MPI_COMM_WORLD->size> bins_local;\n\n#pragma omp parallel for\n  for (size_t i = 0; i < s.size(); i++) {\n    auto letter = s[i].front();\n    auto index = letter - 'a';\n    bins_local[omp_get_thread_num()][index]++;\n  }\n\n  MPI_Allreduce(bins_local.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "const size_t n = s.size();\n\tsize_t local_counts[26];\n\tfor (int i = 0; i < 26; ++i) {\n\t\tlocal_counts[i] = 0;\n\t}\n#pragma omp parallel for\n\tfor (size_t i = 0; i < n; ++i) {\n\t\tint index = s[i].front() - 'a';\n\t\tif (index >= 0 && index < 26) {\n\t\t\tlocal_counts[index] += 1;\n\t\t}\n\t}\n#pragma omp parallel for\n\tfor (size_t i = 0; i < 26; ++i) {\n\t\tbins[i] = local_counts[i];\n\t}\n}",
            "// your code here\n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    int n = s.size();\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    int stride = n / num_procs;\n    if (rank == num_procs - 1) {\n        stride += n % num_procs;\n    }\n    int first = stride * rank;\n    int last = first + stride;\n    std::vector<std::string> slice(s.begin() + first, s.begin() + last);\n    auto count_fn = [&slice, &bins](int i) {\n        char c = slice[i][0];\n        if (c >= 'a' && c <= 'z') {\n            bins[c - 'a']++;\n        }\n    };\n    if (rank == 0) {\n#pragma omp parallel\n        {\n            int thread_id = omp_get_thread_num();\n            std::vector<std::string> local_slice(slice.begin() + thread_id * slice.size() / omp_get_num_threads(),\n                                                slice.begin() + (thread_id + 1) * slice.size() / omp_get_num_threads());\n            for (size_t i = 0; i < local_slice.size(); i++) {\n                char c = local_slice[i][0];\n                if (c >= 'a' && c <= 'z') {\n                    bins[c - 'a']++;\n                }\n            }\n        }\n    } else {\n#pragma omp parallel\n        {\n            int thread_id = omp_get_thread_num();\n            std::vector<std::string> local_slice(slice.begin() + thread_id * slice.size() / omp_get_num_threads(),\n                                                slice.begin() + (thread_id + 1) * slice.size() / omp_get_num_threads());\n            std::for_each(local_slice.begin(), local_slice.end(), count_fn);\n        }\n    }\n    MPI_Gather(bins.data(), bins.size(), MPI_INT, bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// size_t is unsigned int on 64 bit machines\n    // https://en.cppreference.com/w/cpp/language/types\n    bins.fill(0);\n\n    int rank = 0;\n    int num_threads = 0;\n    int nprocs = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    omp_set_num_threads(nprocs);\n    //printf(\"rank %d\\n\", rank);\n\n    size_t n = s.size();\n    //printf(\"n = %ld\\n\", n);\n\n    // https://stackoverflow.com/questions/1870634/how-to-iterate-over-the-indices-of-an-array-in-c\n    // https://stackoverflow.com/questions/11264912/c-why-the-syntax-for-using-range-based-for-loop-is-different-from-that-in-for\n    // http://www.cplusplus.com/reference/clibrary/climits/\n    for (size_t i : std::ranges::iota_view<size_t>(0, n)) {\n        if (rank == 0) {\n            //printf(\"rank %d i = %ld\\n\", rank, i);\n            std::string const& str = s[i];\n            //printf(\"rank %d str = %s\\n\", rank, str.c_str());\n            size_t j = 0;\n            // https://stackoverflow.com/questions/3631493/how-can-i-iterate-over-the-characters-of-a-string-in-c\n            // https://en.cppreference.com/w/cpp/string/basic_string/begin\n            // https://en.cppreference.com/w/cpp/language/range-for\n            // https://stackoverflow.com/questions/13068818/c-range-based-for-loop-with-stdstring\n            // https://stackoverflow.com/questions/33389887/how-to-iterate-over-the-characters-of-a-stdstring\n            // https://en.cppreference.com/w/cpp/language/range-for\n            // https://stackoverflow.com/questions/1870634/how-to-iterate-over-the-indices-of-an-array-in-c\n            // https://en.cppreference.com/w/cpp/language/range-for\n            for (char c : str) {\n                if (j == 0) {\n                    ++bins[c-'a'];\n                }\n                ++j;\n            }\n        }\n        ++num_threads;\n    }\n\n    size_t bins_local[26] = {0};\n    //printf(\"rank %d num_threads = %d\\n\", rank, num_threads);\n    MPI_Gather(bins.data(), 26, MPI_UNSIGNED_LONG, bins_local, 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins.fill(0);\n        for (size_t j = 0; j < 26; ++j) {\n            for (size_t k = 0; k < nprocs; ++k) {\n                bins[j] += bins_local[j+k*26];\n            }\n        }\n    }\n\n    return;\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // Partition the work amongst the ranks\n    size_t const n = s.size();\n    int const chunk_size = (n + num_ranks - 1) / num_ranks;\n    size_t const start = std::min(n, rank * chunk_size);\n    size_t const end = std::min(n, (rank + 1) * chunk_size);\n\n    // Count the first letters of the strings in the chunk\n    std::array<size_t, 26> counts;\n    std::fill(counts.begin(), counts.end(), 0);\n    std::transform(s.begin() + start, s.begin() + end, counts.begin(), [](std::string const& s) {\n        return s.front() - 'a';\n    });\n\n    // Reduce the counts\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Add the counts of the first letters of the strings in the previous chunk\n    std::array<size_t, 26> prev_counts;\n    std::fill(prev_counts.begin(), prev_counts.end(), 0);\n    if (rank > 0) {\n        MPI_Recv(prev_counts.data(), prev_counts.size(), MPI_UNSIGNED, rank - 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    std::transform(counts.begin(), counts.end(), prev_counts.begin(), counts.begin(), std::plus<>());\n\n    // Reduce the counts\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement me\n}",
            "// write your code here\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < 26; i++) {\n\t\tbins[i] = 0;\n\t}\n\tfor (std::string const& str : s) {\n\t\tbins[str[0] - 'a']++;\n\t}\n}",
            "// TODO: Add your code here.\n    const auto N = s.size();\n    std::vector<int> counts(26, 0);\n    #pragma omp parallel for\n    for (auto i=0; i<N; i++) {\n        counts[s[i][0] - 'a']++;\n    }\n    MPI_Reduce(counts.data(), bins.data(), 26, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t my_rank = 0;\n    const size_t nproc = 4;\n    const size_t letters = 26;\n    const size_t chunkSize = letters / nproc;\n    const size_t myChunkStart = my_rank * chunkSize;\n    const size_t myChunkEnd = (my_rank + 1) * chunkSize;\n\n    std::array<size_t, letters> counts{};\n#pragma omp parallel\n    {\n        const size_t myThreadId = omp_get_thread_num();\n        const size_t myLocalChunkStart = myChunkStart + (myThreadId * chunkSize);\n        const size_t myLocalChunkEnd = myChunkEnd + (myThreadId * chunkSize);\n        for (auto& w : s) {\n            for (size_t i = myLocalChunkStart; i < myLocalChunkEnd; i++) {\n                if (w[0] == i + 'a') {\n                    counts[i]++;\n                }\n            }\n        }\n    }\n\n    std::array<size_t, letters> partialSums{};\n    MPI_Allreduce(counts.data(), partialSums.data(), letters, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n    if (my_rank == 0) {\n        std::copy(partialSums.begin(), partialSums.end(), bins.begin());\n    }\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::array<size_t, 26> localBins;\n  for (auto const &word: s) {\n    if (word[0] >= 'a' && word[0] <= 'z') {\n      localBins[word[0] - 'a']++;\n    }\n  }\n\n  // All the ranks will reduce their counts.\n  std::array<size_t, 26> reducedBins;\n  MPI_Reduce(localBins.data(), reducedBins.data(), reducedBins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < reducedBins.size(); i++) {\n      bins[i] = reducedBins[i];\n    }\n  }\n\n}",
            "MPI_Datatype mpi_type;\n  MPI_Type_contiguous(26, MPI_UNSIGNED_LONG, &mpi_type);\n  MPI_Type_commit(&mpi_type);\n\n  int myid, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // allocate a partition of bins\n  std::array<size_t, 26> mybins;\n  if (myid == 0) {\n    mybins = bins;\n  }\n\n  std::vector<size_t> scounts(size);\n  MPI_Gather(&s.size(), 1, MPI_UNSIGNED_LONG, scounts.data(), 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  std::vector<size_t> displs(size);\n  if (myid == 0) {\n    size_t curr = 0;\n    for (auto &i : displs) {\n      i = curr;\n      curr += scounts[i];\n    }\n  }\n  MPI_Scatter(displs.data(), 1, MPI_UNSIGNED_LONG, &displs[0], 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // loop over strings and update bins\n  for (size_t i = displs[myid]; i < displs[myid + 1]; i++) {\n    mybins[s[i][0] - 'a']++;\n  }\n\n  // reduce bins and scounts\n  MPI_Reduce(mybins.data(), bins.data(), 26, mpi_type, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(mybins.data(), scounts.data(), 26, mpi_type, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (myid == 0) {\n    // make bins array from scounts\n    for (auto &i : bins) {\n      i = 0;\n    }\n    for (size_t i = 0; i < scounts.size(); i++) {\n      bins[i] = scounts[i];\n    }\n  }\n\n  MPI_Type_free(&mpi_type);\n}",
            "// TODO: Fill in this function\n}",
            "int mpi_size, mpi_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &mpi_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &mpi_rank);\n\n    const auto n = s.size();\n\n    if (mpi_rank == 0) {\n        bins.fill(0);\n    }\n\n    MPI_Bcast(bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    const int chunk = n / mpi_size + (n % mpi_size!= 0);\n    const auto start = chunk * mpi_rank;\n    const auto end = std::min(chunk * (mpi_rank + 1), n);\n    for (size_t i = start; i < end; ++i) {\n        auto& str = s[i];\n        if (str.size() > 0) {\n            ++bins[str[0] - 'a'];\n        }\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (mpi_rank == 0) {\n        std::cout << \"Total number of strings: \" << n << std::endl;\n        for (size_t i = 0; i < bins.size(); ++i) {\n            if (bins[i] > 0) {\n                std::cout << char('a' + i) << \": \" << bins[i] << std::endl;\n            }\n        }\n    }\n}",
            "if (omp_get_thread_num()!= 0) {\n        return;\n    }\n\n    auto threadCounts = std::array<size_t, 26>();\n    for (auto i = 0; i < 26; ++i) {\n        threadCounts[i] = 0;\n    }\n\n    auto nthreads = omp_get_max_threads();\n    auto const n = s.size();\n    auto chunkSize = n / nthreads;\n    auto remainder = n % nthreads;\n\n    // Each thread will compute its range of indexes and will update the threadCount array\n    #pragma omp parallel num_threads(nthreads)\n    {\n        auto const tid = omp_get_thread_num();\n        auto const begin = tid * chunkSize;\n        auto const end = (tid == nthreads - 1)? begin + chunkSize + remainder : begin + chunkSize;\n\n        for (auto i = begin; i < end; ++i) {\n            auto letter = s[i][0];\n            if (letter >= 'a' && letter <= 'z') {\n                threadCounts[letter - 'a']++;\n            }\n        }\n    }\n\n    // Compute the global counts\n    for (auto i = 0; i < 26; ++i) {\n        #pragma omp atomic\n        bins[i] += threadCounts[i];\n    }\n}",
            "size_t numRanks;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n   size_t rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   size_t numStrings = s.size();\n\n   // TODO: initialize array bins\n\n   // TODO: each rank computes its portion of the histogram on the data that it owns, storing the result in bins\n\n   // TODO: result reduction\n\n   // TODO: print result if rank == 0\n}",
            "const size_t n = s.size();\n    const int nprocs = MPI_COMM_WORLD.Get_size();\n    const int myRank = MPI_COMM_WORLD.Get_rank();\n\n    int firstLetter;\n    std::string firstLetterStr;\n\n    std::vector<std::string> myStrings;\n    std::array<size_t, 26> myBins;\n    myBins.fill(0);\n\n    #pragma omp parallel for\n    for(int i = 0; i < n; ++i) {\n        firstLetter = s[i][0];\n        #pragma omp critical\n        myBins[firstLetter - 'a']++;\n    }\n\n    MPI_Gather(&myBins, 26, MPI_UNSIGNED_LONG, &bins, 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if(myRank == 0) {\n        for(int i = 1; i < nprocs; ++i) {\n            MPI_Recv(myBins, 26, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for(int j = 0; j < 26; ++j) {\n                bins[j] += myBins[j];\n            }\n        }\n    } else {\n        MPI_Send(&myBins, 26, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "const int numRanks = mpi_size();\n  const int rank = mpi_rank();\n  size_t local_count = 0;\n  if(rank == 0){\n    bins.fill(0);\n  }\n\n#pragma omp parallel\n{\n  std::array<size_t, 26> local_bins;\n  local_bins.fill(0);\n#pragma omp for\n  for(size_t i = 0; i < s.size(); i++){\n    char c = std::tolower(s[i][0]);\n    if(c >= 'a' && c <= 'z'){\n      local_bins[c - 'a']++;\n      local_count++;\n    }\n  }\n\n  // Merge counts from all threads\n#pragma omp critical\n  {\n    for(int i = 0; i < local_bins.size(); i++){\n      bins[i] += local_bins[i];\n    }\n  }\n}\n\n  // Reduce the count across all ranks\n  int sum;\n  MPI_Reduce(&local_count, &sum, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank == 0){\n    for(int i = 0; i < numRanks; i++){\n      bins[i] += sum;\n    }\n  }\n}",
            "// Write your solution here\n    // Number of threads in the OpenMP section\n    size_t n_threads = 2;\n\n    // Number of processes in the MPI section\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Calculate the length of the vector\n    size_t len = s.size();\n\n    // Split the input vector between the processes\n    size_t chunk_size = len / world_size;\n    // If the number of processes is not a factor of the length of the vector,\n    // then the first processes should take up extra elements, the others 0\n    if(world_rank < (len % world_size))\n        chunk_size++;\n    size_t start = chunk_size * world_rank;\n    size_t end = (chunk_size * (world_rank + 1));\n    if(world_rank == (world_size - 1))\n        end = len;\n    std::vector<std::string> chunk(s.begin() + start, s.begin() + end);\n\n    // Number of items in each thread's chunk\n    size_t chunk_size_thread = chunk_size / n_threads;\n    // If the number of threads is not a factor of the number of items\n    // in the chunk, then the first threads should take up extra\n    // elements, the others 0\n    if(omp_get_thread_num() < (chunk_size % n_threads))\n        chunk_size_thread++;\n    size_t start_thread = chunk_size_thread * omp_get_thread_num();\n    size_t end_thread = (chunk_size_thread * (omp_get_thread_num() + 1));\n    if(omp_get_thread_num() == (n_threads - 1))\n        end_thread = chunk_size;\n    std::vector<std::string> chunk_thread(chunk.begin() + start_thread, chunk.begin() + end_thread);\n\n    // Declare the bins array to be shared between the threads\n    // This array needs to be declared outside the parallel region\n    // because it is used in the for loop.\n    // It needs to be in scope for the thread to access it.\n    std::array<size_t, 26> bins_thread;\n\n    // Calculate the number of strings in the chunk\n    size_t n_str = 0;\n    for (std::string str : chunk_thread)\n    {\n        n_str++;\n    }\n\n    // Calculate the number of times a letter in the alphabet appears\n    // in the chunk\n    // Create the bins array\n    for (int i = 0; i < 26; i++)\n    {\n        bins_thread[i] = 0;\n    }\n\n    // Iterate through the chunk to count the number of times each letter appears\n    for (std::string str : chunk_thread)\n    {\n        for (char c : str)\n        {\n            // If the character is not a letter, continue to the next character\n            if (c < 'a' || c > 'z')\n            {\n                continue;\n            }\n            // Get the index of the character in the alphabet\n            // and increment its value\n            // This is only valid for ASCII characters\n            // Since the characters are in lower case\n            int idx = c - 'a';\n            bins_thread[idx]++;\n        }\n    }\n\n    // Update the bins array with the values calculated by the threads\n    #pragma omp parallel\n    {\n        // Get the thread id and the total number of threads\n        int thread_id = omp_get_thread_num();\n        int n_threads = omp_get_num_threads();\n\n        // Calculate the local portion of the array and\n        // get the portion of the array to be added to bins\n        size_t start_bins_thread = thread_id * chunk_size_thread;\n        size_t end_bins_thread = (thread_id + 1) * chunk_size_thread;\n\n        // Add the local portion to the bins array\n        for (int i = start_bins_thread; i < end_bins_thread; i++)\n        {\n            bins[i] += bins_thread[i];\n        }\n    }\n\n    // Reduce the bins array across all processes\n    // The result of the reduction is stored in the first element\n    MPI_Reduce(&bins[0], &bins[0], 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        if (s[i].size() > 0) {\n            bins[static_cast<size_t>(s[i][0]) - 'a']++;\n        }\n    }\n}",
            "// Initialize bins to 0\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n\n  // #pragma omp parallel\n  // {\n  //   std::vector<int> counts(26, 0);\n\n  //   for (std::string const &str : s) {\n  //     // #pragma omp atomic\n  //     ++counts[str[0] - 'a'];\n  //   }\n\n  //   for (size_t i = 0; i < counts.size(); ++i) {\n  //     // #pragma omp critical\n  //     bins[i] += counts[i];\n  //   }\n  // }\n\n  std::vector<int> counts(26, 0);\n  for (std::string const &str : s) {\n    ++counts[str[0] - 'a'];\n  }\n\n  for (size_t i = 0; i < counts.size(); ++i) {\n    bins[i] += counts[i];\n  }\n}",
            "// TODO: implement this function\n}",
            "int nprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Distribute the problem among all ranks\n  int n = s.size();\n  int local_n = n / nprocs;\n  std::vector<std::string> local_s(local_n);\n  int local_s_start = rank * local_n;\n  for (int i = 0; i < local_n; i++) {\n    local_s[i] = s[local_s_start + i];\n  }\n\n  // Compute result in parallel using OpenMP\n  std::array<size_t, 26> local_bins;\n  for (int i = 0; i < 26; i++) {\n    local_bins[i] = 0;\n  }\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < local_n; i++) {\n      int ch = local_s[i][0] - 'a';\n      local_bins[ch]++;\n    }\n  }\n\n  // Collect result from all ranks\n  MPI_Reduce(local_bins.data(), bins.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = s.size();\n    int num_threads = omp_get_max_threads();\n\n    /* MPI_Scatter lets rank 0 send different blocks of input to each of the other ranks.\n       We want to send an equal number of strings to each rank.\n       We also want to make sure that each rank gets the same number of blocks. */\n    // size_t my_num_strings = (n + num_ranks - 1) / num_ranks;\n    size_t my_num_strings = n / num_threads;\n    size_t my_offset = my_num_strings * rank;\n    size_t my_size = std::min(my_num_strings, n - my_offset);\n\n    std::vector<std::string> my_strings(my_size);\n    MPI_Scatter(s.data(), my_size, MPI_CHAR, my_strings.data(), my_size, MPI_CHAR, 0, MPI_COMM_WORLD);\n\n    /* Use OpenMP to parallelize the work */\n    omp_set_num_threads(num_threads);\n    #pragma omp parallel\n    {\n        int my_rank = omp_get_thread_num();\n        #pragma omp for schedule(static,1) nowait\n        for (size_t i = 0; i < my_strings.size(); i++) {\n            size_t offset = my_offset + i;\n            size_t c = my_strings[i][0] - 'a';\n            bins[c]++;\n        }\n    }\n\n    /* MPI_Reduce combines the results from all ranks into bins on rank 0. */\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* TODO */\n\n}",
            "const size_t numStrings = s.size();\n    const int numTasks = omp_get_max_threads();\n    const int rank = 0;\n    const int numRanks = 1;\n\n    // TODO: distribute numStrings to all ranks\n    // TODO: distribute numTasks to all ranks\n    // TODO: collect numStrings for each rank\n    // TODO: distribute numTasks to all ranks\n    // TODO: broadcast numStrings\n    // TODO: collect numTasks for each rank\n    // TODO: broadcast numTasks\n\n    // TODO: compute first letter counts in parallel\n    // TODO: send results back to rank 0\n\n}",
            "const size_t myRank = 0;\n    const size_t numProcs = 0;\n    const int numThreads = 0;\n\n    // TODO: fill this in\n\n}",
            "// TODO: replace this comment with your own code\n}",
            "#pragma omp parallel\n#pragma omp for\n\tfor (size_t i = 0; i < s.size(); ++i) {\n\t\tchar c = std::tolower(s[i][0]);\n\t\tif (c >= 'a' && c <= 'z') {\n\t\t\tbins[c - 'a']++;\n\t\t}\n\t}\n}",
            "// write your solution here\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        bins[s[i][0] - 'a']++;\n    }\n}",
            "MPI_Datatype vectorString;\n  MPI_Type_contiguous(1, MPI_CHAR, &vectorString);\n  MPI_Type_commit(&vectorString);\n\n  std::vector<size_t> myBins(bins.size());\n\n  size_t n_strings = s.size();\n\n  MPI_Scatter(&n_strings, 1, MPI_UNSIGNED_LONG, &myBins[0], bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < n_strings; i++) {\n    char c = s[i][0];\n    if (c >= 'a' && c <= 'z') {\n      size_t idx = c - 'a';\n      myBins[idx]++;\n    }\n  }\n\n  MPI_Gather(&myBins[0], bins.size(), MPI_UNSIGNED_LONG, &bins[0], bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "int const rank = getRank();\n    int const nRanks = getNRanks();\n    // TODO: your code here\n}",
            "auto local_bins = std::array<size_t, 26>();\n#pragma omp parallel\n#pragma omp for schedule(static)\n  for (size_t i = 0; i < s.size(); ++i) {\n    local_bins[s[i][0] - 'a']++;\n  }\n  // sum up results from all local bins\n#pragma omp parallel\n#pragma omp for schedule(static)\n  for (size_t i = 0; i < 26; ++i) {\n    bins[i] += local_bins[i];\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code here\n}",
            "// YOUR CODE HERE\n}",
            "// Write your code here.\n}",
            "size_t n = s.size();\n  std::vector<size_t> localCounts(26, 0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    char ch = s[i][0];\n    if ((ch >= 'a') && (ch <= 'z')) {\n      localCounts[ch - 'a']++;\n    }\n  }\n\n  std::array<int, 26> gSum;\n  MPI_Allreduce(localCounts.data(), gSum.data(), 26, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = gSum[i];\n  }\n}",
            "// TODO: fill in this code\n}",
            "//...\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        bins[s[i][0] - 'a'] += 1;\n    }\n\n    // MPI reduction:\n    // sum the bins across all ranks\n    MPI_Reduce(bins.data(), bins.data(), 26, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // rank 0 broadcasts the result to all ranks\n    MPI_Bcast(bins.data(), 26, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "int n_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = s.size();\n  std::array<int, 26> counts;\n  std::fill(counts.begin(), counts.end(), 0);\n\n#pragma omp parallel for\n  for (int i = 0; i < n; ++i) {\n    counts[s[i][0] - 'a']++;\n  }\n\n  MPI_Allreduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n}",
            "const size_t numThreads = 2;\n  const size_t numBins = bins.size();\n  const size_t numStrings = s.size();\n\n  /* Distribute strings to ranks using round robin */\n  size_t const blockSize = (numStrings + numThreads - 1) / numThreads;\n  size_t stringsPerRank[numThreads];\n  for (size_t i = 0; i < numThreads; i++) {\n    stringsPerRank[i] = std::min(blockSize, numStrings - i * blockSize);\n  }\n\n  size_t offsets[numThreads];\n  offsets[0] = 0;\n  for (size_t i = 1; i < numThreads; i++) {\n    offsets[i] = offsets[i - 1] + stringsPerRank[i - 1];\n  }\n\n  /* Store the output in bins on rank 0 */\n  if (0 == MPI::COMM_WORLD.Get_rank()) {\n    bins = std::array<size_t, 26>();\n  }\n\n  /* TODO: OpenMP parallelize */\n  #pragma omp parallel num_threads(numThreads)\n  {\n    int const threadId = omp_get_thread_num();\n    size_t const myFirstString = offsets[threadId];\n    size_t const myLastString = offsets[threadId] + stringsPerRank[threadId];\n    std::array<size_t, 26> &myBins = bins;\n\n    for (size_t stringId = myFirstString; stringId < myLastString; stringId++) {\n      std::string const &str = s[stringId];\n      if (str.size() > 0) {\n        myBins[str[0] - 'a']++;\n      }\n    }\n  }\n\n  /* Synchronize all ranks to get the final counts on rank 0 */\n  MPI::COMM_WORLD.Barrier();\n  MPI::COMM_WORLD.Gather(&bins, 1, MPI::UNSIGNED_LONG_LONG_INT, &bins, 1, MPI::UNSIGNED_LONG_LONG_INT, 0);\n}",
            "std::array<size_t, 26> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n\n#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); ++i) {\n        char c = s[i][0];\n        if (c >= 'a' && c <= 'z') {\n            ++localBins[c - 'a'];\n        }\n    }\n\n    // All reduce\n    MPI_Allreduce(localBins.data(), bins.data(), localBins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "const size_t num_strings = s.size();\n  const size_t num_threads = omp_get_max_threads();\n\n  // First, compute the local histogram using OpenMP\n  std::array<size_t, 26> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  // Each thread gets its own copy of s\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i=0; i<num_strings; ++i) {\n    char c = s[i].front();\n    if (c >= 'a' && c <= 'z') {\n      ++local_bins[c - 'a'];\n    }\n  }\n\n  // Next, gather the local results in the first letter of the rank\n  // where the result is stored. Assume the local_bins array is\n  // the same size on all ranks\n  std::vector<size_t> local_sums(26, 0);\n  std::partial_sum(local_bins.begin(), local_bins.end(), local_sums.begin());\n\n  // Then, reduce the local sums to get the total counts\n  std::vector<size_t> total_sums(26, 0);\n  MPI_Reduce(local_sums.data(), total_sums.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Finally, compute the partial sums to get the total counts\n  // for each rank\n  if (total_sums[0] == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n  else {\n    std::partial_sum(total_sums.begin(), total_sums.end(), bins.begin());\n  }\n}",
            "std::array<size_t, 26> counts = {};\n  size_t n = s.size();\n\n  // Compute the counts\n  for (auto const& s : s) {\n    if (s.length() > 0) {\n      counts[s[0] - 'a']++;\n    }\n  }\n\n  // MPI Allreduce to compute the result in the 0th rank\n  MPI_Allreduce(counts.data(), bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // Use OpenMP to compute the counts in parallel\n#pragma omp parallel for reduction(+:bins) schedule(static)\n  for (size_t i = 0; i < n; ++i) {\n    if (s[i].length() > 0) {\n      bins[s[i][0] - 'a']++;\n    }\n  }\n}",
            "// Initialize the bins to 0\n  for(size_t i = 0; i < 26; ++i) {\n    bins[i] = 0;\n  }\n\n  // MPI variables\n  int num_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // OpenMP variables\n  omp_set_num_threads(num_procs);\n\n  // Loop over the strings\n  for(std::string const& str : s) {\n    // Counts how many strings start with a particular letter\n    #pragma omp parallel for\n    for(size_t i = 0; i < 26; ++i) {\n      if(str[0] == 'a' + i) {\n        ++bins[i];\n      }\n    }\n  }\n\n  // Add up the counts from all the other ranks\n  std::array<size_t, 26> temp_bins;\n  MPI_Allreduce(bins.data(), temp_bins.data(), 26, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  // Copy the result back to the bins array on rank 0\n  MPI_Barrier(MPI_COMM_WORLD);\n  if(rank == 0) {\n    bins = temp_bins;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    // TODO: Implement this function\n\n    // Parallel Section\n    #pragma omp parallel\n    {\n        std::array<size_t, 26> bins_local;\n        std::fill(bins_local.begin(), bins_local.end(), 0);\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < s.size(); i++) {\n            std::string const& ss = s[i];\n            if (ss.size() > 0) {\n                bins_local[ss[0] - 'a'] += 1;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for (int i = 0; i < 26; i++) {\n                bins[i] += bins_local[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        int start = s[i].find_first_not_of(' ');\n        if (start >= 0) {\n            bins[s[i][start] - 'a']++;\n        }\n    }\n}",
            "// Your code goes here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // if there is only one process\n    if (size == 1) {\n        size_t index = 0;\n        for (const auto& word : s) {\n            bins[word[0] - 'a']++;\n        }\n    } else {\n        // split s into chunks\n        std::vector<std::vector<std::string>> chunks(size);\n        size_t chunkSize = s.size() / size;\n        size_t startIndex = rank * chunkSize;\n        size_t endIndex = startIndex + chunkSize;\n        // copy the data from the original vector to chunks\n        std::copy(s.begin() + startIndex, s.begin() + endIndex, chunks[rank].begin());\n        // get the size of chunks on the first process\n        size_t firstChunkSize;\n        MPI_Gather(&chunkSize, 1, MPI_UNSIGNED_LONG, &firstChunkSize, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n        // compute the remaining chunk sizes on the first process\n        if (rank == 0) {\n            for (int i = 1; i < size; ++i) {\n                MPI_Gather(&chunkSize, 1, MPI_UNSIGNED_LONG, &firstChunkSize, 1, MPI_UNSIGNED_LONG, i, MPI_COMM_WORLD);\n                chunkSize = firstChunkSize;\n            }\n        }\n\n        // broadcast the final chunk size to the other processes\n        MPI_Bcast(&chunkSize, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n        // split chunks\n        for (size_t i = 0; i < chunks.size(); ++i) {\n            if (i == 0) {\n                chunks[i].resize(chunkSize);\n            } else {\n                chunks[i].resize(chunkSize);\n            }\n        }\n\n        // compute the number of letters in each word of each chunk\n        std::vector<size_t> chunkFirstLetterCounts(size, 0);\n#pragma omp parallel for\n        for (size_t i = 0; i < size; ++i) {\n            std::string word;\n            for (const auto& w : chunks[i]) {\n                word = w;\n                chunkFirstLetterCounts[i] += word[0] - 'a';\n            }\n        }\n\n        // send the result to the first process\n        MPI_Gather(&chunkFirstLetterCounts[0], size, MPI_UNSIGNED_LONG, &bins[0], size, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        if (rank == 0) {\n            for (int i = 1; i < size; ++i) {\n                MPI_Gather(&chunkFirstLetterCounts[0], size, MPI_UNSIGNED_LONG, &bins[0], size, MPI_UNSIGNED_LONG, i, MPI_COMM_WORLD);\n            }\n        }\n\n        // gather the bins from the processes\n        for (size_t i = 0; i < bins.size(); ++i) {\n            bins[i] = 0;\n        }\n        MPI_Gather(&bins[0], size, MPI_UNSIGNED_LONG, &bins[0], size, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "const auto rank = getRank();\n    const auto size = getSize();\n\n    // each rank has a complete copy of s\n    // so we can compute the firstLetterCount without synchronization\n    auto firstLetterCounts = computeFirstLetterCounts(s);\n    if (rank == 0) bins = firstLetterCounts;\n\n    // MPI broadcasts the output of computeFirstLetterCounts from rank 0 to all ranks\n    broadcast(bins, 0);\n\n    // all ranks now have the first letter counts\n    if (rank == 0) {\n        // each rank computes its part of the result\n        // so we can compute the firstLetterCounts without synchronization\n        auto firstLetterCounts = computeFirstLetterCounts(s);\n\n        // each rank then updates the results with its part of the computation\n        for (size_t i = 1; i < size; ++i) {\n            updateFirstLetterCounts(bins, firstLetterCounts);\n        }\n    }\n}",
            "// Initialize bins to 0.\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // Get the number of strings.\n  int numStrings = s.size();\n#pragma omp parallel\n  {\n#pragma omp for schedule(static)\n    for (int j = 0; j < numStrings; j++) {\n      std::string const& str = s[j];\n      if (str.size() > 0) {\n        bins[static_cast<size_t>(str[0] - 'a')]++;\n      }\n    }\n  }\n\n  // Reduce bins across ranks.\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < s.size(); i++) {\n        std::string const& str = s[i];\n        size_t const len = str.length();\n        if (len == 0) {\n            continue;\n        }\n        char first = str[0];\n        if (first >= 'a' && first <= 'z') {\n            bins[first - 'a'] += 1;\n        }\n    }\n    std::vector<size_t> temp(bins.begin(), bins.end());\n    MPI_Reduce(temp.data(), bins.data(), temp.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        if (s[i].length() == 0) {\n            continue;\n        }\n        bins[s[i][0] - 'a'] += 1;\n    }\n}",
            "// Your code goes here.\n  // Do not remove this line.\n}",
            "/* TODO: your code here */\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < s.size(); i++)\n        {\n            size_t index = s[i][0] - 'a';\n            #pragma omp atomic\n            bins[index]++;\n        }\n    }\n}",
            "// TODO: fill in here\n}",
            "#if USE_MPI\n\t// Broadcast vector of strings to all processes\n\tint procCount;\n\tMPI_Comm_size(MPI_COMM_WORLD, &procCount);\n\tstd::vector<std::string> localStrings(procCount);\n\tif (procCount > 1) {\n\t\tMPI_Scatter(&s[0], s.size(), MPI_CHAR, &localStrings[0], s.size(), MPI_CHAR, 0, MPI_COMM_WORLD);\n\t} else {\n\t\tlocalStrings = s;\n\t}\n\n\t// Calculate local results\n\tstd::array<size_t, 26> localBins{};\n\tlocalBins.fill(0);\n\tfor (auto const& i : localStrings) {\n\t\tlocalBins[i[0] - 'a']++;\n\t}\n\n\t// Reduce results\n\tif (procCount > 1) {\n\t\tstd::array<size_t, 26> tmpBins{};\n\t\tMPI_Reduce(&localBins[0], &tmpBins[0], 26, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t\tlocalBins = tmpBins;\n\t}\n\n\t// Copy results to output\n\tif (procCount == 1) {\n\t\tbins = localBins;\n\t} else {\n\t\tMPI_Gather(&localBins[0], 26, MPI_UNSIGNED_LONG, &bins[0], 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\t}\n#else\n\tstd::array<size_t, 26> localBins{};\n\tlocalBins.fill(0);\n\tfor (auto const& i : s) {\n\t\tlocalBins[i[0] - 'a']++;\n\t}\n\tbins = localBins;\n#endif\n}",
            "size_t const n = s.size();\n  size_t const world_size = MPI::COMM_WORLD.Get_size();\n  size_t const world_rank = MPI::COMM_WORLD.Get_rank();\n\n  // Each rank computes its own count\n  for (size_t i = 0; i < n; i++) {\n    char c = s[i][0];\n    bins[c - 'a']++;\n  }\n\n  // Gather all counts onto rank 0\n  std::array<size_t, 26> all_bins = bins;\n  MPI::COMM_WORLD.Reduce(all_bins.data(), bins.data(), all_bins.size(), MPI::UNSIGNED_LONG_LONG, MPI::SUM, 0);\n\n#pragma omp parallel for\n  // Each thread now adds its count to the count on rank 0\n  for (size_t i = 0; i < 26; i++) {\n    if (world_rank == 0) {\n      all_bins[i] += bins[i];\n    }\n    MPI::COMM_WORLD.Bcast(&all_bins[i], 1, MPI::UNSIGNED_LONG_LONG, 0);\n  }\n\n  // Copy all_bins onto bins for rank 0\n  MPI::COMM_WORLD.Bcast(bins.data(), bins.size(), MPI::UNSIGNED_LONG_LONG, 0);\n}",
            "if (s.size() < static_cast<size_t>(omp_get_max_threads())) {\n        firstLetterCountsSerial(s, bins);\n    } else {\n        firstLetterCountsParallel(s, bins);\n    }\n}",
            "const size_t num_ranks = MPI::COMM_WORLD.Get_size();\n  const size_t my_rank = MPI::COMM_WORLD.Get_rank();\n\n  // Count the number of letters that come before the current rank's letter.\n  // We only need to know the number of letters before the current rank's,\n  // because each rank can use this information to figure out the number of\n  // strings that start with the current rank's letter.\n  size_t previous_letter_count = 0;\n  for (size_t rank = 0; rank < my_rank; ++rank) {\n    previous_letter_count += s[rank].size();\n  }\n\n  // Compute the number of strings that start with each letter in parallel.\n  std::vector<size_t> my_bins(26);\n  std::transform(s.begin(), s.end(), my_bins.begin(),\n                 [previous_letter_count](std::string const& s) { return s.front() - 'a'; });\n\n  // Accumulate the results of the parallel computations.\n  MPI::COMM_WORLD.Allreduce(my_bins.data(), bins.data(), 26, MPI::UNSIGNED_LONG_LONG, MPI::SUM);\n\n  // Prefix sum to compute the number of strings that start with each letter.\n  std::partial_sum(bins.begin(), bins.end(), bins.begin());\n\n  // Update the counts for the letters that came before the current rank's letter.\n  for (size_t rank = 0; rank < my_rank; ++rank) {\n    bins[s[rank].front() - 'a'] += previous_letter_count;\n  }\n}",
            "}",
            "// 1. find number of strings and size of strings (i.e. the length of the longest string)\n  size_t nstrings = s.size();\n  size_t maxstr = 0;\n  for (size_t i = 0; i < nstrings; i++) {\n    maxstr = std::max(maxstr, s[i].size());\n  }\n\n  // 2. initialize bins\n  for (size_t i = 0; i < 26; i++) {\n    bins[i] = 0;\n  }\n\n  // 3. count number of strings in bins\n  // 3a. for each rank\n  // 3b. do some work\n  // 3c. MPI_Reduce to combine results\n#pragma omp parallel for reduction(+:bins)\n  for (size_t i = 0; i < nstrings; i++) {\n    char first = std::tolower(s[i][0]);\n    bins[first - 'a']++;\n  }\n\n  // 4. for each rank\n  // 4a. MPI_Reduce to combine results\n  // 4b. only master rank has correct bins\n  // 4c. only master rank has correct nstrings and maxstr\n}",
            "// Your code here\n}",
            "const size_t num_strings = s.size();\n  const size_t rank = getRank();\n\n  // Initialize bins to 0\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // Compute the bins in parallel using OpenMP.\n  // We can't use the OpenMP parallel for loop because it would\n  // run the entire for loop on rank 0 and that would be wasteful.\n  // We'll need a more complex loop.\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < num_strings; ++i) {\n    // Get the first character of the string.\n    char first_letter = s[i][0];\n    // Since this is lower case, we can treat all letters as lower case.\n    // This ensures that the character 'a' is the same as the character 'A'\n    // and so on.\n    first_letter = tolower(first_letter);\n\n    // Compute the index for the bins.\n    // If the index is 0, then 'a' was the first character.\n    // Since there are only 26 possible letters, we need to take\n    // into account the rank of the process.\n    size_t letter_index = static_cast<size_t>(first_letter - 'a') + rank * 26;\n\n    // Increment the bin at the index corresponding to the first letter of the string.\n    ++bins[letter_index];\n  }\n\n  // Add the bins from all processes into rank 0.\n  if (rank == 0) {\n    // Initialize bins_to_rank to 0.\n    std::array<size_t, 26> bins_to_rank;\n    std::fill(bins_to_rank.begin(), bins_to_rank.end(), 0);\n\n    // Add up the bins from each process.\n    for (int i = 0; i < getSize(); ++i) {\n      // Add the bins from each process into rank 0.\n      for (size_t j = 0; j < 26; ++j) {\n        bins_to_rank[j] += bins[j + 26 * i];\n      }\n    }\n\n    // Now set bins to the bins from rank 0.\n    bins = bins_to_rank;\n  }\n}",
            "// 1. Compute the number of strings in each partition\n  size_t n_strings_in_partition = s.size() / static_cast<size_t>(omp_get_num_procs());\n\n  // 2. Each rank will be working with a subset of the strings\n  size_t offset = static_cast<size_t>(omp_get_proc_num()) * n_strings_in_partition;\n  size_t n_strings = (static_cast<size_t>(omp_get_proc_num()) == omp_get_num_procs() - 1)?\n                     s.size() - offset :\n                     n_strings_in_partition;\n\n  // 3. Compute the number of strings for each letter in the alphabet\n  std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for reduction(+:bins)\n  for (size_t i = 0; i < n_strings; i++) {\n    auto str = s[i + offset];\n    for (auto c : str) {\n      bins[c - 'a']++;\n    }\n  }\n\n  // 4. Gather to the master\n  MPI_Gather(bins.data(), 26, MPI_UNSIGNED_LONG, bins.data(), 26, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  int rank = 0;\n  MPI_Comm_rank(comm, &rank);\n  int nprocs = 0;\n  MPI_Comm_size(comm, &nprocs);\n\n  size_t n = s.size();\n\n  std::array<size_t, 26> counts = {};\n\n  // each rank computes its own part\n  size_t part = n / nprocs;\n  size_t start = rank * part;\n  size_t end = std::min(n, (rank + 1) * part);\n  for (size_t i = start; i < end; i++) {\n    char c = s[i][0];\n    if (std::isalpha(c)) {\n      counts[c - 'a']++;\n    }\n  }\n\n  std::array<size_t, 26> local_counts = counts;\n\n  MPI_Reduce(local_counts.data(), counts.data(), 26, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, comm);\n\n  if (rank == 0) {\n    bins = counts;\n  }\n}",
            "// TODO: Implement firstLetterCounts() here.\n\n  const int world_size = MPI::COMM_WORLD.Get_size();\n  const int world_rank = MPI::COMM_WORLD.Get_rank();\n\n  auto n = s.size();\n\n  std::vector<int> local_bins(bins.size(), 0);\n\n  for (auto i = 0; i < n; ++i) {\n    local_bins[s[i][0] - 'a'] += 1;\n  }\n\n  int local_count = 0;\n  for (auto bin : local_bins) {\n    local_count += bin;\n  }\n\n  int global_count = 0;\n  MPI::COMM_WORLD.Reduce(&local_count, &global_count, 1, MPI::INT, MPI::SUM, 0);\n\n  if (world_rank == 0) {\n    bins = std::array<size_t, 26> { 0 };\n    for (auto i = 0; i < local_bins.size(); ++i) {\n      bins[i] += local_bins[i];\n    }\n  } else {\n    bins = std::array<size_t, 26> { 0 };\n  }\n\n  MPI::COMM_WORLD.Bcast(bins.data(), bins.size(), MPI::INT, 0);\n}",
            "//TODO\n}",
            "// TODO: implement me\n}",
            "// TODO\n}",
            "// TODO: implement this function.\n}",
            "if (s.size() < 1) {\n        return;\n    }\n\n    // OpenMP parallelization:\n    // each thread computes a subset of the `bins` array\n    #pragma omp parallel for\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] = 0;\n    }\n\n    for (std::string const& string : s) {\n        size_t index = string[0] - 'a';\n        bins[index]++;\n    }\n\n    // MPI parallelization:\n    // Each rank computes a subset of the `bins` array and sends the result to rank 0.\n    // The results are then added together, and the result is stored in the bins array on rank 0.\n    int rank = 0;\n    int comm_sz = 1;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n    std::vector<int> local_bins(26);\n    std::copy(bins.begin(), bins.end(), local_bins.begin());\n\n    int *sendbuf = new int[26];\n    int *recvbuf = new int[26];\n\n    // send the counts to rank 0\n    MPI_Send(local_bins.data(), 26, MPI_INT, 0, 1, MPI_COMM_WORLD);\n\n    // receive the counts from rank 0\n    MPI_Recv(recvbuf, 26, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n    // sum the counts\n    for (size_t i = 0; i < 26; ++i) {\n        bins[i] += recvbuf[i];\n    }\n\n    delete[] sendbuf;\n    delete[] recvbuf;\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n    // The first 26 indices in bins will hold the counts for letters a to z.\n    std::fill(bins.begin(), bins.end(), 0);\n\n    std::vector<size_t> localBins;\n    if (myRank == 0) {\n        // The first letter of the first string determines the length of the local vector localBins.\n        localBins.resize(s[0][0] - 'a');\n    }\n\n    // Create a vector to hold the result, which will be sent to rank 0.\n    std::vector<size_t> globalBins;\n    if (myRank == 0) {\n        globalBins.resize(26);\n    }\n\n    for (size_t i = 0; i < s.size(); i++) {\n        // The first letter of the string s[i] determines the index in localBins.\n        size_t index = s[i][0] - 'a';\n        localBins[index]++;\n    }\n\n    if (myRank == 0) {\n        for (int i = 1; i < numRanks; i++) {\n            MPI_Recv(localBins.data(), 26, MPI_UNSIGNED, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 26; j++) {\n                globalBins[j] += localBins[j];\n            }\n        }\n    } else {\n        MPI_Send(localBins.data(), 26, MPI_UNSIGNED, 0, 1, MPI_COMM_WORLD);\n    }\n\n    if (myRank == 0) {\n        for (int i = 1; i < numRanks; i++) {\n            MPI_Recv(localBins.data(), 26, MPI_UNSIGNED, i, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 26; j++) {\n                globalBins[j] += localBins[j];\n            }\n        }\n    } else {\n        MPI_Send(localBins.data(), 26, MPI_UNSIGNED, 0, 2, MPI_COMM_WORLD);\n    }\n\n    if (myRank == 0) {\n        // Copy the result back to the bins array.\n        std::copy(globalBins.begin(), globalBins.end(), bins.begin());\n    } else {\n        // Copy the result to the correct position in the bins array.\n        std::copy(localBins.begin(), localBins.end(), bins.begin());\n    }\n}",
            "// TODO: write code here\n}",
            "// YOUR CODE HERE\n    bins.fill(0);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < s.size(); ++i) {\n        #pragma omp atomic\n        ++bins[s[i][0] - 'a'];\n    }\n}",
            "// TODO: Your code here\n    bins.fill(0);\n    omp_set_dynamic(0);\n    omp_set_num_threads(4);\n#pragma omp parallel for\n    for (int i = 0; i < s.size(); i++) {\n        char ch = s[i][0];\n        bins[ch - 'a']++;\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(Kokkos::View<const double*> const& x, Kokkos::View<size_t[10]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: your code goes here\n}",
            "// TODO\n}",
            "// Fill out this function, and delete this comment.\n}",
            "// initialize the bins array, which has one entry per bin\n\tKokkos::parallel_for(10, KOKKOS_LAMBDA(const int i) { bins[i] = 0; });\n\n\tKokkos::parallel_for(100, KOKKOS_LAMBDA(const int i) {\n\t\t// determine which bin the current value belongs in\n\t\tconst int bin = std::floor(x[i]/10);\n\t\t// add to the bin counter\n\t\tbins[bin]++;\n\t});\n}",
            "// TODO: implement your solution\n  return;\n}",
            "// TODO\n}",
            "// TODO: Replace with Kokkos parallel_reduce\n  // https://github.com/ORNL/scream/issues/329\n  // https://github.com/kokkos/kokkos/issues/3639\n  // TODO: Use Kokkos::View for bins instead of std::array\n  // https://github.com/ORNL/scream/issues/330\n  // https://github.com/kokkos/kokkos/issues/3640\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)), [=](const int i) {\n    int bin = (int) (x(i) / 10);\n    if (bin < 10) bins(bin) += 1;\n  });\n}",
            "// Implement your function here\n\n  return;\n}",
            "// Fill in your solution here\n}",
            "Kokkos::parallel_for(\"Bins By 10 Count\", x.extent(0), KOKKOS_LAMBDA(const size_t& i) {\n    for (int b = 0; b < 10; b++) {\n      if (x(i) >= b * 10 && x(i) < (b + 1) * 10) {\n        Kokkos::atomic_fetch_add(&(bins(b)), 1);\n      }\n    }\n  });\n}",
            "Kokkos::parallel_for(\"binsBy10Count\", x.extent(0), KOKKOS_LAMBDA(size_t i) {\n    double n = x(i) / 10.0;\n    size_t bin = (size_t)n;\n    if (n - bin >= 0.5) {\n      bin++;\n    }\n    bins(bin) += 1;\n  });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, Kokkos::Schedule<Kokkos::Dynamic> >(0, x.extent(0)),\n    KOKKOS_LAMBDA(const size_t& i) {\n        if (x(i) >= 0 && x(i) < 10)\n            ++bins[size_t(x(i))];\n        else if (x(i) >= 10 && x(i) < 20)\n            ++bins[10];\n        else if (x(i) >= 20 && x(i) < 30)\n            ++bins[20];\n        else if (x(i) >= 30 && x(i) < 40)\n            ++bins[30];\n        else if (x(i) >= 40 && x(i) < 50)\n            ++bins[40];\n        else if (x(i) >= 50 && x(i) < 60)\n            ++bins[50];\n        else if (x(i) >= 60 && x(i) < 70)\n            ++bins[60];\n        else if (x(i) >= 70 && x(i) < 80)\n            ++bins[70];\n        else if (x(i) >= 80 && x(i) < 90)\n            ++bins[80];\n        else if (x(i) >= 90 && x(i) <= 100)\n            ++bins[90];\n    });\n}",
            "// TODO\n}",
            "// Your code goes here\n  // HINT: Use Kokkos::parallel_for to iterate through the values.\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0,x.extent(0));\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int i) {\n      auto val = x(i);\n      if (val < 10) {\n         ++bins(val);\n      }\n   });\n}",
            "// Fill `bins` with zeros first\n  Kokkos::deep_copy(bins, 0);\n\n  // TODO: Fill in the body of this function to use parallel_reduce to sum the\n  // values in `bins`\n}",
            "// TODO: Implement this function.\n  // Hint: Kokkos::parallel_reduce is a convenient way to do reductions.\n  // Hint: Kokkos::subview is a convenient way to create a new view from a\n  // Kokkos::View\n  // Hint: Kokkos::atomic_fetch_add is a convenient way to add to an array.\n}",
            "}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// TODO: implement this function\n    // You may find it useful to reference https://j3-fortran.org/doc/year/12/12-2/12-2-1-53.html\n}",
            "const size_t numElements = x.extent(0);\n  const size_t numparallel = 4;\n\n  // parallel_for takes a lambda and a range\n  Kokkos::parallel_for(\"histogram_parallel_for\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, numElements), \n    [&x, &bins] (const int i) {\n\n      // if x[i] >= 0, get the bin number\n      if (x[i] >= 0.0) {\n        const size_t bin = size_t(x[i] / 10.0);\n        Kokkos::atomic_fetch_add(&(bins(bin)), 1);\n      }\n    });\n}",
            "// YOUR CODE HERE\n  // This loop is the outer iteration over the input vector x,\n  // and is the parallel section.\n  // Each thread in the parallel section is responsible for updating\n  // a single element of the histogram.\n  // This section is called once for each element in x, and so the\n  // number of threads in the parallel section should match the number\n  // of elements in x.\n\n  // The \"i\" variable is the local index of the element\n  // in the vector x that this thread is responsible for.\n  // We don't know which element in x we will be responsible for\n  // until we start the parallel section, so we don't know the value\n  // of i until then. So we use the loop index \"idx\" to iterate through\n  // the elements in x.\n  for (size_t idx = 0; idx < x.extent(0); idx++) {\n    size_t i = x(idx);\n\n    // TODO(you): compute which bin i falls into\n    // Store the count of that bin in the corresponding bin in `bins`.\n    //\n    // Hint: Use the modulo operator (%) to get the remainder of i divided by 10.\n\n    // We also need to store a value into the last bin for the case that i == 100.\n    // For example, if the first element of x is 100, then we don't want to store the\n    // count in the bin for 0 <= x < 10 (which would be stored in the first bin).\n    // Instead, we want to store the count in the last bin (the 10th bin).\n    //\n    // In the last bin, the lower bound is 100, and the upper bound is 1000.\n    // To determine which bin to store the count in, we need to know whether the\n    // value in x is less than or equal to 100, which is equivalent to asking\n    // whether i <= 100.\n\n    // TODO(you): store the count in the correct bin\n    //\n    // Hint: Use the operator (<=) to determine whether i <= 100.\n  }\n\n  // TODO(you): Wait for all of the parallel threads to complete.\n}",
            "// TODO: write code that uses Kokkos to compute the number of values in each bin\n\n  // Use the Kokkos parallel_for to compute the histogram\n  // Hint: use Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>,...)\n\n  // TODO: uncomment the following line to run on the GPU\n  //Kokkos::fence();\n\n  Kokkos::deep_copy(bins, Kokkos::View<size_t[10]>(\"\", Kokkos::DefaultExecutionSpace()));\n\n  // TODO: uncomment the following line to run on the GPU\n  //Kokkos::fence();\n\n}",
            "Kokkos::parallel_for(10, [&] (int i) {\n        auto lower_bound = i * 10;\n        auto upper_bound = (i+1) * 10;\n\n        auto begin = x.data();\n        auto end = x.data() + x.extent(0);\n\n        auto bin_begin = bins.data() + i;\n        auto bin_end = bin_begin + 1;\n\n        auto count = std::count_if(begin, end, [lower_bound, upper_bound] (double value) {\n            return value >= lower_bound && value < upper_bound;\n        });\n\n        *bin_begin = count;\n    });\n}",
            "// your code here\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int& i) {\n    bins(i) = 0;\n  });\n  Kokkos::fence();\n  size_t i = 0;\n  Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&x, &i, &bins](const int& j, size_t& l){\n    if (x(j) < 10) {\n      ++bins(i);\n    }\n    else {\n      i = static_cast<size_t>(x(j)/10);\n      l += bins(i);\n    }\n  }, std::plus<size_t>());\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0));\n  Kokkos::parallel_for(policy, [=] (size_t i) {\n    int ix = (int)(x(i) / 10.0);\n    if (ix < 0) {\n      ix = 0;\n    }\n    if (ix >= 10) {\n      ix = 9;\n    }\n    bins(ix)++;\n  });\n}",
            "// TODO: your code goes here\n}",
            "// TODO: Your code here.\n  // Hint: you can iterate over an array like\n  // for(size_t i = 0; i < array_length; i++) {\n  //     // do something with the array element\n  // }\n  Kokkos::parallel_for(\"bins-by-10-count\", 10, KOKKOS_LAMBDA(int i) {\n      bins(i) = 0;\n  });\n  Kokkos::parallel_for(\"bins-by-10-count\", x.extent(0), KOKKOS_LAMBDA(int i) {\n      for (int k = 0; k < 10; k++) {\n          if (x(i) < (k+1) * 10) {\n              bins(k) += 1;\n              break;\n          }\n      }\n  });\n}",
            "using ExecSpace = Kokkos::OpenMP;\n  const int num_threads = 64;\n\n  /* TODO: Use Kokkos to initialize bins to zero. */\n  /* TODO: Use Kokkos to run a parallel_for over each element of x to compute\n     the correct bin number and increment the count in the corresponding bin. */\n  /* TODO: Print the contents of bins to the screen. */\n}",
            "Kokkos::parallel_for(\n    \"Binning\",\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      // For each value in x, increment the bin that it is in.\n      // 0 \u2264 x[i] < 10\n      // x[i] / 10 is the bin index that this value should be in.\n      // floor() rounds down to an integer, so this will be the bin index.\n      int bin = static_cast<int>(floor(x[i] / 10));\n      Kokkos::atomic_fetch_add(&(bins(bin)), 1);\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n        KOKKOS_LAMBDA (const int64_t i) {\n            int bin = (int) ((x(i) / 10));\n            bins(bin) += 1;\n    });\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(int i) {\n        bins(i) = 0;\n    });\n    Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(int i) {\n        if (x(i) < 10.0) bins(0)++;\n        else if (x(i) < 20.0) bins(1)++;\n        else if (x(i) < 30.0) bins(2)++;\n        else if (x(i) < 40.0) bins(3)++;\n        else if (x(i) < 50.0) bins(4)++;\n        else if (x(i) < 60.0) bins(5)++;\n        else if (x(i) < 70.0) bins(6)++;\n        else if (x(i) < 80.0) bins(7)++;\n        else if (x(i) < 90.0) bins(8)++;\n        else if (x(i) < 100.0) bins(9)++;\n    });\n}",
            "// TODO: Compute the counts in `bins` using only `x` and `bins`. Do not use\n  // any other variables.\n\n  // ToDo: Fill in here!\n}",
            "// TODO: implement this function\n  // hint: use the Kokkos parallel_for function\n\n  auto bin_counts = Kokkos::View<size_t*>(\"bin_counts\", 10);\n  Kokkos::deep_copy(bin_counts, 0);\n\n  Kokkos::parallel_for(\"count\", 10, KOKKOS_LAMBDA (int i) {\n    for (size_t j=0; j<x.extent(0); j++){\n      if ((x(j)>=i*10) && (x(j)<(i+1)*10)){\n        bin_counts(i)+=1;\n      }\n    }\n  });\n\n  Kokkos::deep_copy(bins, bin_counts);\n\n}",
            "// TODO: Your code here\n\n  auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0));\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const size_t i) {\n    for (size_t j = 0; j < 10; j++) {\n      if (j * 10 <= x(i) && x(i) < (j + 1) * 10) {\n        Kokkos::atomic_fetch_add(&(bins(j)), 1);\n      }\n    }\n  });\n  Kokkos::fence();\n}",
            "// TODO: Fill in this function to calculate the histogram using Kokkos.\n}",
            "size_t N = x.size();\n\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int i) {\n    size_t v = (size_t) std::floor(x(i)/10);\n    if (v < 10)\n      bins(v) += 1;\n  });\n}",
            "// TODO: Implement this function\n}",
            "// Initialize bins to zero\n    Kokkos::parallel_for(\"Initialize bins\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0,10), KOKKOS_LAMBDA(const int i){\n        bins(i) = 0;\n    });\n\n    // Compute bins\n    Kokkos::parallel_for(\"Compute bins\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0,x.extent(0)), KOKKOS_LAMBDA(const int i){\n        if (0 <= x(i) && x(i) < 10) {\n            bins(size_t(x(i)))++;\n        }\n    });\n}",
            "}",
            "// TODO\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int bin) {\n      bins(bin) = 0;\n   });\n\n   // TODO: replace with Kokkos parallel_scan\n   Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int i) {\n      auto const xi = x(i);\n      if (xi >= 0 && xi < 10) {\n         Kokkos::atomic_fetch_add(&bins(xi), 1);\n      }\n   });\n}",
            "// TODO:\n  // 1. Compute the number of counts\n  // 2. Compute the histogram using the number of counts\n}",
            "Kokkos::parallel_for(\"BinsBy10Count\", 10, [&] (size_t i) {\n      bins(i) = 0;\n   });\n   Kokkos::parallel_for(\"BinsBy10Count\", x.extent(0), [&] (size_t i) {\n      size_t bin = (size_t) (x(i) / 10);\n      if (bin < 10) {\n         bins(bin) += 1;\n      }\n   });\n}",
            "const size_t n = x.extent(0);\n\n  // initialize bins\n  Kokkos::View<size_t[10], Kokkos::LayoutLeft, Kokkos::HostSpace> bins_host(\"bins\", 10);\n  Kokkos::deep_copy(bins_host, 0);\n\n  // TODO: replace this with a parallel reduction\n  for (size_t i = 0; i < n; i++) {\n    if (x(i) < 10)\n      bins_host(0) += 1;\n    else if (x(i) < 20)\n      bins_host(1) += 1;\n    else if (x(i) < 30)\n      bins_host(2) += 1;\n    else if (x(i) < 40)\n      bins_host(3) += 1;\n    else if (x(i) < 50)\n      bins_host(4) += 1;\n    else if (x(i) < 60)\n      bins_host(5) += 1;\n    else if (x(i) < 70)\n      bins_host(6) += 1;\n    else if (x(i) < 80)\n      bins_host(7) += 1;\n    else if (x(i) < 90)\n      bins_host(8) += 1;\n    else if (x(i) < 100)\n      bins_host(9) += 1;\n  }\n\n  // copy back\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_host, bins);\n  for (size_t i = 0; i < 10; i++) {\n    size_t count = 0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if ((x_host(j) >= i * 10) && (x_host(j) < (i + 1) * 10))\n        count++;\n    }\n    bins_host(i) = count;\n  }\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO: implement this\n  Kokkos::View<size_t[10]> counts(\"Counts\", 10);\n  Kokkos::parallel_for(\"Fill\", 10, KOKKOS_LAMBDA(int i) {\n    counts(i) = 0;\n  });\n  Kokkos::parallel_for(\"Fill\", x.extent(0), KOKKOS_LAMBDA(int i) {\n    Kokkos::atomic_fetch_add(&counts(x(i)/10), 1);\n  });\n  Kokkos::parallel_for(\"Fill\", 10, KOKKOS_LAMBDA(int i) {\n    bins(i) = counts(i);\n  });\n}",
            "}",
            "// TODO: fill in this function\n\n  // use default execution space\n  auto exec = Kokkos::DefaultExecutionSpace{};\n  // set up a parallel region\n  Kokkos::parallel_for(\"binsBy10Count\", exec, 10,\n    [&x, &bins](size_t i) {\n      size_t count = 0;\n      for (size_t j = i * 10; j < (i + 1) * 10; ++j) {\n        if (x(j) < 10)\n          ++count;\n      }\n      bins(i) = count;\n  });\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int i) {\n    // TODO: Fill in this function\n    size_t count = 0;\n    size_t lowerBound = 10 * i;\n    size_t upperBound = lowerBound + 10;\n    for (int j = 0; j < x.extent(0); j++) {\n      if (x(j) >= lowerBound && x(j) < upperBound) {\n        count += 1;\n      }\n    }\n    bins(i) = count;\n  });\n}",
            "// TODO: Implement this method!\n}",
            "// TODO\n}",
            "// TODO: insert code here\n}",
            "}",
            "// Fill up the histogram with 0s\n  for (size_t i=0; i<10; i++) {\n    bins(i) = 0;\n  }\n\n  // Compute the binning in parallel\n  Kokkos::parallel_for(\"BinsBy10\", x.size(), KOKKOS_LAMBDA(const size_t i) {\n    size_t bin = static_cast<size_t>(x(i)/10);\n    bins(bin) += 1;\n  });\n}",
            "// your code here\n}",
            "/* Your code here. */\n}",
            "// fill in the implementation\n  // your solution should call a Kokkos functor\n  // bins are a device_view, x is a const device_view, which means that you should not be modifying x\n  // but you can use x as a parameter in the Kokkos functor.\n  // For example, the following code is a valid solution\n  //   Kokkos::View<size_t[10]> bins(\"bins\");\n  //   Kokkos::View<const double*> x(\"x\", 10);\n  //   Kokkos::parallel_for(10, KOKKOS_LAMBDA(const size_t& i) {\n  //     if (x(i) < 10)\n  //       Kokkos::atomic_fetch_add(&(bins[0]), 1);\n  //     else if (x(i) < 20)\n  //       Kokkos::atomic_fetch_add(&(bins[1]), 1);\n  //     else if (x(i) < 30)\n  //       Kokkos::atomic_fetch_add(&(bins[2]), 1);\n  //   });\n\n  // To do this, you will need to use the Kokkos atomic_fetch_add function as shown above.\n\n  // You will also need to use Kokkos::single, which allows you to execute a single\n  // statement in parallel.  You can do something like:\n  // Kokkos::single(Kokkos::PerTeam(team), [&] () {\n  //   if (x(i) < 10)\n  //     Kokkos::atomic_fetch_add(&(bins[0]), 1);\n  // });\n\n  // You can also use Kokkos::parallel_reduce, which allows you to do something like:\n  // size_t local_bins[10] = {0};\n  // Kokkos::parallel_reduce(10, KOKKOS_LAMBDA(const size_t& i, size_t& lsum) {\n  //   if (x(i) < 10)\n  //     Kokkos::atomic_fetch_add(&(local_bins[0]), 1);\n  // });\n  // Kokkos::parallel_for(10, KOKKOS_LAMBDA(const size_t& i) {\n  //   Kokkos::atomic_fetch_add(&(bins[i]), local_bins[i]);\n  // });\n\n  // You can find documentation for Kokkos::single, Kokkos::parallel_for, and\n  // Kokkos::parallel_reduce in the Kokkos User Guide:\n  // https://github.com/kokkos/kokkos/wiki/User-Guide\n\n  // You can find the Kokkos atomic_fetch_add documentation here:\n  // https://github.com/kokkos/kokkos/wiki/Atomic-Operations\n}",
            "// Your code here.\n    //\n    // For example, to compute a sum across all the elements in `x` using\n    // Kokkos, one might use:\n    //\n    // Kokkos::View<double> sum(\"sum\", 1);\n    // Kokkos::parallel_for(\"Sum x\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)), KOKKOS_LAMBDA(size_t i) {\n    //     sum(0) += x(i);\n    // });\n    // double sum_x = sum(0);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0)), KOKKOS_LAMBDA(const int idx) {\n        double const value = x(idx);\n        for (int i = 0; i < 10; i++) {\n            if (value >= i * 10 && value < (i + 1) * 10)\n                bins(i)++;\n        }\n    });\n}",
            "// TODO: Add code here\n}",
            "// TODO: Compute the number of values in [0,10), [10, 20),... and store\n  // them in `bins`.\n  // You will need to define a Kokkos::TeamPolicy and loop over the values.\n}",
            "Kokkos::parallel_for(\"binsBy10Count\", x.extent(0), KOKKOS_LAMBDA(const int i) {\n        const int value = x(i);\n        // TODO: replace with Kokkos function\n        if(value < 10)\n            ++bins[value];\n    });\n}",
            "Kokkos::View<size_t*,Kokkos::LayoutLeft,Kokkos::CudaSpace> bins_ptr = Kokkos::create_mirror_view(bins);\n    auto x_ptr = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_ptr, x);\n    Kokkos::deep_copy(bins_ptr, bins);\n\n    Kokkos::parallel_for(\"Kokkos\", Kokkos::RangePolicy<Kokkos::Cuda>(0, 10),\n    KOKKOS_LAMBDA(int i) {\n        size_t count = 0;\n        for (int j = 0; j < 100; j += 10) {\n            if (x_ptr(j) >= i*10 && x_ptr(j) < (i+1)*10) {\n                count++;\n            }\n        }\n        bins_ptr(i) = count;\n    });\n\n    Kokkos::deep_copy(bins, bins_ptr);\n}",
            "// TODO: replace the following with your code.\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n                       KOKKOS_LAMBDA(const int i) {\n    for (int j = 0; j < 10; ++j) {\n      if (x(i) < (j + 1) * 10.0) {\n        ++bins(j);\n        break;\n      }\n    }\n  });\n}",
            "const size_t n = x.extent(0);\n  Kokkos::View<size_t> bins_temp(\"bins_temp\", 10);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n  [&] (size_t i) {\n    double value = x(i);\n    if (0 <= value && value < 10) {\n      Kokkos::atomic_fetch_add(&bins_temp(value), 1);\n    }\n  });\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 10),\n  [&] (size_t i) {\n    bins(i) = bins_temp(i);\n  });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, x.extent(0)), [=](size_t i) {\n    const size_t bin = static_cast<size_t>(x(i) / 10);\n    if (bin < 10)\n      Kokkos::atomic_fetch_add(&bins[bin], 1);\n  });\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, 0, 10>(), [&] (int i) {\n        bins(i) = 0;\n    });\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&] (int i) {\n        double x_val = x(i);\n        if (x_val >= 0 && x_val <= 10) {\n            int bin_index = x_val / 10;\n            Kokkos::atomic_fetch_add(&(bins(bin_index)), 1);\n        }\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, x.extent(0)),\n                         KOKKOS_LAMBDA(size_t i) {\n            size_t index = std::floor(x(i)/10.0);\n            Kokkos::atomic_fetch_add(&(bins(index)), 1);\n        });\n    Kokkos::fence();\n}",
            "const size_t N = x.extent(0);\n    size_t nThreads = std::min(100, (int)N); // at most 100 threads\n    Kokkos::TeamPolicy<>::team_size_recommended(nThreads);\n    Kokkos::parallel_for(\"binsBy10\", Kokkos::TeamPolicy<>(N, Kokkos::AUTO), [=](const Kokkos::TeamPolicy<>::member_type& teamMember) {\n        const size_t i = teamMember.league_rank();\n        const size_t j = teamMember.team_rank();\n        const size_t n = teamMember.team_size();\n        double value = x(i);\n        if(value >= j * 10 && value < (j+1) * 10) {\n            bins(j) += n;\n        }\n    });\n}",
            "auto const n = x.extent(0);\n  Kokkos::View<size_t[10]> bins_per_thread(\"Bins Per Thread\", Kokkos::ThreadVectorRange(Kokkos::ThreadTeamMember(), 10));\n  Kokkos::parallel_for(Kokkos::ThreadVectorRange(Kokkos::ThreadTeamMember(), 10), KOKKOS_LAMBDA(const int i) {\n    bins_per_thread(i) = 0;\n  });\n  Kokkos::parallel_for(Kokkos::TeamThreadRange(Kokkos::ThreadTeamMember(), 0, n), KOKKOS_LAMBDA(const int i) {\n    auto const j = static_cast<int>(x(i) / 10);\n    if (j < 10) {\n      Kokkos::atomic_increment(&bins_per_thread(j));\n    }\n  });\n  Kokkos::parallel_for(Kokkos::TeamThreadRange(Kokkos::ThreadTeamMember(), 10), KOKKOS_LAMBDA(const int i) {\n    Kokkos::atomic_fetch_add(&bins(i), bins_per_thread(i));\n  });\n}",
            "size_t count[10] = {};\n\n   // Your code goes here\n\n   // Note: this is a very simple example. You should use parallel_for with a\n   // chunk size that is close to the number of data points (as opposed to\n   // calling `binsBy10Count` in a for-loop).\n\n   // Note: This example will not work with Kokkos versions older than 3.2.\n   //       (Earlier versions of Kokkos did not have the parallel_for_work_item\n   //        function.)\n}",
            "// Insert your code here.\n\n\n}",
            "double lower = 0.0;\n    double upper = 10.0;\n    Kokkos::parallel_for(10, KOKKOS_LAMBDA(const size_t i) {\n        auto count = 0;\n        auto begin = x.data() + i * x.extent(0) / 10;\n        auto end = x.data() + (i+1) * x.extent(0) / 10;\n        for (auto xi = begin; xi!= end; ++xi) {\n            if (*xi >= lower && *xi < upper) {\n                ++count;\n            }\n        }\n        bins(i) = count;\n    });\n}",
            "using namespace Kokkos;\n\n    // TODO: fill in this function\n\n}",
            "auto x_size = x.extent(0);\n  auto bins_h = Kokkos::create_mirror_view(bins);\n\n  // Fill bins_h with zeros\n  for (auto& i : bins_h) {\n    i = 0;\n  }\n\n  // Fill bins_h with the correct count\n  Kokkos::parallel_for(\"BinsBy10Count\", x_size, [=](size_t i) {\n    size_t bin_index = std::floor((x(i) / 10.0));\n    bins_h(bin_index) += 1;\n  });\n\n  // Copy bins_h back to bins\n  Kokkos::deep_copy(bins, bins_h);\n}",
            "// TODO\n  // write the algorithm here\n}",
            "size_t num_values = x.extent(0);\n  bins = Kokkos::View<size_t[10]>(\"bins\", 10);\n  Kokkos::parallel_for(\"bins-count\", num_values, KOKKOS_LAMBDA(const size_t& i) {\n    if (x(i) >= 0.0 && x(i) < 10.0) {\n      bins(size_t(x(i)) - 0) += 1;\n    }\n  });\n}",
            "// TODO: write your code here\n\n}",
            "const size_t len = x.extent(0);\n  const double* input = x.data();\n  size_t* output = bins.data();\n\n  // TODO: Your code here\n}",
            "Kokkos::parallel_for(\"Filling bins\", 10, KOKKOS_LAMBDA(int i) {\n    bins(i) = 0;\n  });\n\n  // TODO: Your code goes here\n}",
            "}",
            "/* TODO */\n}",
            "/*\n  for (size_t i = 0; i < x.extent(0); ++i) {\n    double value = x(i);\n    if (value < 0 || value > 100) {\n      throw std::runtime_error(\"Input values must be between 0 and 100\");\n    }\n\n    // TODO: write a Kokkos operator for this if-else.\n    size_t index = 0;\n    if (value < 10) {\n      index = 0;\n    } else if (value < 20) {\n      index = 1;\n    } else if (value < 30) {\n      index = 2;\n    } else if (value < 40) {\n      index = 3;\n    } else if (value < 50) {\n      index = 4;\n    } else if (value < 60) {\n      index = 5;\n    } else if (value < 70) {\n      index = 6;\n    } else if (value < 80) {\n      index = 7;\n    } else if (value < 90) {\n      index = 8;\n    } else {\n      index = 9;\n    }\n\n    ++bins(index);\n  }\n  */\n}",
            "Kokkos::parallel_for(\n\t\tKokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n\t\t[=](size_t i) {\n\t\t\t// get value in ith element of x\n\t\t\tdouble x_val = x(i);\n\t\t\t// get index in the bins vector\n\t\t\tint idx = (int)std::floor(x_val/10.0);\n\t\t\t// increment the corresponding index in the bins vector\n\t\t\t++bins(idx);\n\t\t}\n\t);\n}",
            "// TODO: Fill this in.\n}",
            "/* TODO: Your code here. */\n}",
            "Kokkos::parallel_for(\n        \"Bins By 10\",\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n        KOKKOS_LAMBDA(const size_t i) {\n            size_t j = size_t(x(i) / 10);\n            if (j < 10) {\n                ++bins(j);\n            }\n        });\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int j) {\n      bins(j) = Kokkos::atomic_fetch_add(&bins(j), (size_t) 0);\n  });\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), KOKKOS_LAMBDA(const int i) {\n    int index = (int) (x(i) / 10);\n    Kokkos::atomic_fetch_add(&bins(index), 1);\n  });\n}",
            "// TODO: Fill in this function\n}",
            "auto const n = x.extent(0);\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n        KOKKOS_LAMBDA(const int i) {\n        auto const value = x(i);\n        // Compute the bin index\n        auto const bin = static_cast<int>(value / 10);\n        // Increment the bin count\n        Kokkos::atomic_fetch_add(&(bins(bin)), 1);\n    });\n}",
            "// TODO\n}",
            "}",
            "// TODO\n}",
            "// TODO: implement me.\n}",
            "/* TODO */\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int &i) {\n        double min = i * 10.0;\n        double max = (i + 1) * 10.0;\n\n        size_t count = 0;\n        for (size_t j = 0; j < x.extent(0); j++) {\n            if (x(j) >= min && x(j) < max) {\n                count += 1;\n            }\n        }\n        bins(i) = count;\n    });\n}",
            "// TODO: your code here\n}",
            "//...\n\n}",
            "// TODO: Implement this function!\n    // Note: you may find it helpful to look at the README to understand the interface\n\n    // You can assume x will have length at least 10\n\n    // For example, the first element of x might be 32 and the last element of x might be 39, so you\n    // can assume that the range of values in x is at least 39 - 32 + 1 = 10.\n\n    // You can assume that x is already allocated on the default device.\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [=] (int i) {\n    // TODO: implement\n    // Hint: see this page for a hint on what to do: https://github.com/kokkos/kokkos-tutorials/wiki/Kokkos-Tutorials-for-beginners:-Vector-Summation\n  });\n\n  // TODO: check that your implementation is correct\n}",
            "Kokkos::parallel_for( \"binsBy10Count\", x.extent(0), KOKKOS_LAMBDA (size_t i) {\n    if(x[i] >= 0 && x[i] < 10) {\n      bins(i) = 1;\n    } else if(x[i] >= 10 && x[i] < 20) {\n      bins(i) = 2;\n    } else if(x[i] >= 20 && x[i] < 30) {\n      bins(i) = 3;\n    } else if(x[i] >= 30 && x[i] < 40) {\n      bins(i) = 4;\n    } else if(x[i] >= 40 && x[i] < 50) {\n      bins(i) = 5;\n    } else if(x[i] >= 50 && x[i] < 60) {\n      bins(i) = 6;\n    } else if(x[i] >= 60 && x[i] < 70) {\n      bins(i) = 7;\n    } else if(x[i] >= 70 && x[i] < 80) {\n      bins(i) = 8;\n    } else if(x[i] >= 80 && x[i] < 90) {\n      bins(i) = 9;\n    } else if(x[i] >= 90 && x[i] < 100) {\n      bins(i) = 10;\n    } else {\n      bins(i) = 0;\n    }\n  });\n}",
            "// Create the Kokkos execution space\n  Kokkos::DefaultExecutionSpace exec_space;\n  // Create a Kokkos device view\n  Kokkos::View<size_t[10], Kokkos::DefaultExecutionSpace> bins_dv(\"bins\", 10);\n  // Create a Kokkos functor\n  auto functor = KOKKOS_LAMBDA(size_t i) {\n    int bin = (int)(x(i)/10.0);\n    if (bin >= 0 && bin < 10) {\n      bins_dv(bin) += 1;\n    }\n  };\n  // Execute the Kokkos functor on the device\n  Kokkos::parallel_for(x.extent(0), functor);\n  // Copy results to host\n  Kokkos::deep_copy(exec_space, bins, bins_dv);\n}",
            "}",
            "// Compute 10 bins, each of which is a View\n  Kokkos::View<size_t*> bins_by_10(\"bins by 10\", 10);\n\n  // Initialize bins\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 10), [=](int i) {\n    bins_by_10(i) = 0;\n  });\n  Kokkos::fence();\n\n  // Compute counts per bin\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [=](int i) {\n    int bin_id = int(x(i) / 10);\n    Kokkos::atomic_fetch_add(&bins_by_10(bin_id), 1);\n  });\n  Kokkos::fence();\n\n  // Copy counts to output\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 10), [=](int i) {\n    bins(i) = bins_by_10(i);\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), KOKKOS_LAMBDA(size_t i) {\n    // TODO: Fill in\n  });\n}",
            "#if defined(KOKKOS_ENABLE_CXX11_DISPATCH_LAMBDA)\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n                       [&] (const int i) {\n                         const double xval = x(i);\n                         if (xval < 10.0) {\n                           ++bins(0);\n                         } else if (xval < 20.0) {\n                           ++bins(1);\n                         } else if (xval < 30.0) {\n                           ++bins(2);\n                         } else if (xval < 40.0) {\n                           ++bins(3);\n                         } else if (xval < 50.0) {\n                           ++bins(4);\n                         } else if (xval < 60.0) {\n                           ++bins(5);\n                         } else if (xval < 70.0) {\n                           ++bins(6);\n                         } else if (xval < 80.0) {\n                           ++bins(7);\n                         } else if (xval < 90.0) {\n                           ++bins(8);\n                         } else {\n                           ++bins(9);\n                         }\n                       });\n#else\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n                       KOKKOS_LAMBDA (const int i) {\n                         const double xval = x(i);\n                         if (xval < 10.0) {\n                           ++bins(0);\n                         } else if (xval < 20.0) {\n                           ++bins(1);\n                         } else if (xval < 30.0) {\n                           ++bins(2);\n                         } else if (xval < 40.0) {\n                           ++bins(3);\n                         } else if (xval < 50.0) {\n                           ++bins(4);\n                         } else if (xval < 60.0) {\n                           ++bins(5);\n                         } else if (xval < 70.0) {\n                           ++bins(6);\n                         } else if (xval < 80.0) {\n                           ++bins(7);\n                         } else if (xval < 90.0) {\n                           ++bins(8);\n                         } else {\n                           ++bins(9);\n                         }\n                       });\n#endif\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n                         KOKKOS_LAMBDA(int i) {\n                             double d = x(i);\n                             if (0 <= d && d < 10) {\n                                 ++bins[d];\n                             } else {\n                                 if (10 <= d && d < 20) {\n                                     ++bins[10];\n                                 } else {\n                                     if (20 <= d && d < 30) {\n                                         ++bins[20];\n                                     } else {\n                                         if (30 <= d && d < 40) {\n                                             ++bins[30];\n                                         } else {\n                                             if (40 <= d && d < 50) {\n                                                 ++bins[40];\n                                             } else {\n                                                 if (50 <= d && d < 60) {\n                                                     ++bins[50];\n                                                 } else {\n                                                     if (60 <= d && d < 70) {\n                                                         ++bins[60];\n                                                     } else {\n                                                         if (70 <= d && d < 80) {\n                                                             ++bins[70];\n                                                         } else {\n                                                             if (80 <= d && d < 90) {\n                                                                 ++bins[80];\n                                                             } else {\n                                                                 if (90 <= d && d < 100) {\n                                                                     ++bins[90];\n                                                                 }\n                                                             }\n                                                         }\n                                                     }\n                                                 }\n                                             }\n                                         }\n                                     }\n                                 }\n                             }\n                         });\n}",
            "/* TODO: Your code goes here. */\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      for (size_t j = 0; j < 10; j++) {\n        if (j*10 <= x(i) && x(i) < (j+1)*10) {\n          Kokkos::atomic_fetch_add(&(bins(j)), 1);\n        }\n      }\n    });\n  Kokkos::fence();\n  Kokkos::deep_copy(bins, bins);\n}",
            "Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(0, x.extent(0));\n\n  //TODO: use a parallel_for()\n  size_t count[10] = { 0 };\n\n  for (size_t i = 0; i < x.extent(0); i++) {\n    count[x[i] / 10]++;\n  }\n\n  bins() = count;\n}",
            "// TODO: Implement this function\n}",
            "double threshold = 10;\n    double step = 10;\n    double first = 0;\n\n    // TODO: fill in the code to compute the histogram with Kokkos\n    // Hint: this can be done using Kokkos::parallel_reduce\n}",
            "// TODO: Compute the histogram for values in [0,10) and store the result in `bins`.\n}",
            "const size_t len = x.extent(0);\n  const double x0 = 0.0;\n  const double x10 = 10.0;\n  const size_t nBins = 10;\n\n  // TODO: Add parallel_for, fill array with zeroes, and then parallel_for again to compute the bins.\n  // The parallel_for for filling the bins should be executed by default as a parallel_for.\n  // Hint: Kokkos::RangePolicy is useful for specifying a range of elements to operate on.\n  // Hint: Use Kokkos::TeamPolicy to specify a team_size and vector_length.\n  // Hint: Kokkos::TeamPolicy::team_size_max() and vector_length_max() return the maximum allowed value.\n  // Hint: Use a view for the output of the first parallel_for (i.e., `bins`), and use a lambda for the second.\n  // Hint: See example in class for more information.\n  Kokkos::parallel_for(\"Initialize bins to zero\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, nBins), KOKKOS_LAMBDA(const int i) {\n    bins(i) = 0;\n  });\n\n  // TODO: Add parallel_for to compute the bins.\n  // Hint: Use a lambda for the second parallel_for.\n  // Hint: See example in class for more information.\n}",
            "Kokkos::parallel_for(\"histogram\", x.size(), KOKKOS_LAMBDA(size_t i) {\n    double value = x(i);\n    if (value < 10) {\n      bins(0)++;\n    } else if (value < 20) {\n      bins(1)++;\n    } else if (value < 30) {\n      bins(2)++;\n    } else if (value < 40) {\n      bins(3)++;\n    } else if (value < 50) {\n      bins(4)++;\n    } else if (value < 60) {\n      bins(5)++;\n    } else if (value < 70) {\n      bins(6)++;\n    } else if (value < 80) {\n      bins(7)++;\n    } else if (value < 90) {\n      bins(8)++;\n    } else {\n      bins(9)++;\n    }\n  });\n}",
            "Kokkos::parallel_for(\"Bin\", Kokkos::RangePolicy<Kokkos::ParallelFor>(0, x.size()), KOKKOS_LAMBDA(int i) {\n      if(x(i) < 10) bins(0) += 1;\n      else if(x(i) < 20) bins(1) += 1;\n      else if(x(i) < 30) bins(2) += 1;\n      else if(x(i) < 40) bins(3) += 1;\n      else if(x(i) < 50) bins(4) += 1;\n      else if(x(i) < 60) bins(5) += 1;\n      else if(x(i) < 70) bins(6) += 1;\n      else if(x(i) < 80) bins(7) += 1;\n      else if(x(i) < 90) bins(8) += 1;\n      else if(x(i) < 100) bins(9) += 1;\n    });\n}",
            "// TODO: fill this in\n}",
            "// YOUR CODE HERE\n    // Initialize `bins` to 0\n    // Do it in parallel, using `Kokkos::parallel_for`\n}",
            "// TODO\n}",
            "// fill bins with zeros\n    Kokkos::parallel_for(10, KOKKOS_LAMBDA(int i) {\n        bins(i) = 0;\n    });\n    Kokkos::fence();\n\n    Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n        const double x_i = x(i);\n        const int bin = (int) (x_i / 10.0);\n        Kokkos::atomic_fetch_add(&(bins(bin)), 1);\n    });\n    Kokkos::fence();\n}",
            "Kokkos::parallel_for(\n    \"bins_by_10_count\", Kokkos::RangePolicy<Kokkos::Schedule<Kokkos::Dynamic, Kokkos::Dynamic>>(0, x.extent(0)),\n    KOKKOS_LAMBDA(int i) {\n      int const bin = (int) (x(i) / 10);\n      Kokkos::atomic_fetch_add(&(bins(bin)), 1);\n    });\n}",
            "const size_t size = x.extent(0);\n\n  // TODO: Kokkos is not thread safe yet. We need to lock the vector to the current thread and synchronize when we are done\n  Kokkos::parallel_for(size, KOKKOS_LAMBDA (const int i) {\n    const size_t v = std::round(x(i) / 10);\n    if (v < 10) {\n      bins[v]++;\n    }\n  });\n\n  Kokkos::fence();\n\n  // TODO: Kokkos is not thread safe yet. We need to lock the vector to the current thread and synchronize when we are done\n  Kokkos::parallel_for(10, KOKKOS_LAMBDA (const int i) {\n    bins[i] = Kokkos::atomic_fetch_add(&bins[i], 0);\n  });\n\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [=](int i) {\n    auto v = x(i);\n    auto bin = (v / 10);\n    if (v < 10) bin = 0;\n    if (v >= 100) bin = 9;\n    ++bins(bin);\n  });\n}",
            "/* TODO */\n}",
            "/* TODO */\n    const size_t N = x.extent(0);\n    //const size_t numBins = 10;\n\n    Kokkos::View<size_t*, Kokkos::CudaUVMSpace> bins_d(\"bins\", 10);\n    Kokkos::deep_copy(bins_d, 0);\n    Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int i) {\n        Kokkos::parallel_for(N, KOKKOS_LAMBDA(const int j) {\n            if (x(j) >= 10*i && x(j) < 10*(i+1)) {\n                Kokkos::atomic_increment(&bins_d(i));\n            }\n        });\n    });\n    Kokkos::deep_copy(bins, bins_d);\n}",
            "/* TODO: fill in this function */\n    return;\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\"BinsBy10Count\", x.extent(0), [=](int i) {\n      auto x_i = x(i);\n      if (x_i < 10) bins(x_i) += 1;\n      else if (x_i < 20) bins(10) += 1;\n      else if (x_i < 30) bins(20) += 1;\n      else if (x_i < 40) bins(30) += 1;\n      else if (x_i < 50) bins(40) += 1;\n      else if (x_i < 60) bins(50) += 1;\n      else if (x_i < 70) bins(60) += 1;\n      else if (x_i < 80) bins(70) += 1;\n      else if (x_i < 90) bins(80) += 1;\n      else if (x_i < 100) bins(90) += 1;\n    });\n\n}",
            "// TODO: Replace this with your implementation\n  // hint: use a parallel_for on the bins view\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&x, &bins](size_t i) {\n    if (x(i) < 10) {\n      ++bins(0);\n    } else if (x(i) < 20) {\n      ++bins(1);\n    } else if (x(i) < 30) {\n      ++bins(2);\n    } else if (x(i) < 40) {\n      ++bins(3);\n    } else if (x(i) < 50) {\n      ++bins(4);\n    } else if (x(i) < 60) {\n      ++bins(5);\n    } else if (x(i) < 70) {\n      ++bins(6);\n    } else if (x(i) < 80) {\n      ++bins(7);\n    } else if (x(i) < 90) {\n      ++bins(8);\n    } else {\n      ++bins(9);\n    }\n  });\n}",
            "// Kokkos::parallel_for() can be called here\n\n    // TODO: Add your code here\n}",
            "}",
            "// TODO: implement this function using Kokkos\n}",
            "// Your code here\n}",
            "Kokkos::TeamPolicy policy(10);\n  Kokkos::parallel_for(\"binsBy10Count\", policy, KOKKOS_LAMBDA(const Kokkos::TeamMember & teamMember) {\n    const int i = teamMember.league_rank();\n    const double value = x[i];\n    const size_t bin = (value < 10? value : 9);\n    teamMember.team_barrier();\n    ++bins[bin];\n  });\n}",
            "// TODO: compute counts in parallel\n  size_t num_values = x.extent(0);\n  for (size_t i = 0; i < num_values; ++i) {\n    ++bins(static_cast<int>(std::floor(x(i)/10)));\n  }\n}",
            "// Your code here\n}",
            "// TODO: create and initialize a Kokkos View of size 10 to store `bins`\n\t// TODO: Create and initialize a Kokkos execution policy for parallel_for\n\t// TODO: Compute bins in parallel using the parallel_for policy.  Do\n\t//       not use parallel_reduce for this exercise.\n\t// Hint: Check out the Kokkos documentation for parallel_for:\n\t//    https://kokkos.github.io/core/doc/1.2/classKokkos_1_1TeamPolicy.html#ad702c374e6268472d5e373d02c05721e\n\t// Hint: Check out the Kokkos documentation for atomic_add:\n\t//    https://kokkos.github.io/core/doc/1.2/classKokkos_1_1View.html#a283809706a29d7e034c560a54f9e76b7\n\t// Hint: Use the Kokkos \"RangePolicy\" to access the input and bins\n\t//       View objects in parallel.\n\t// Hint: In parallel_for, use the team_member_invoke to call\n\t//       the function update_bins_by_10 on each team_member.\n}",
            "Kokkos::parallel_for(\"Compute histogram\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()), [&] (size_t i) {\n    double const value = x(i);\n    size_t index = floor(value/10);\n    if(0 <= index && index <= 9) {\n      bins(index) += 1;\n    }\n  });\n}",
            "// TODO: fill in this code\n}",
            "// YOUR CODE HERE\n    size_t size = x.extent(0);\n\n    Kokkos::parallel_for(size, KOKKOS_LAMBDA(const size_t i){\n        // YOUR CODE HERE\n    });\n\n    // Print out the result\n    for (size_t i = 0; i < 10; i++) {\n        std::cout << bins(i) << \" \";\n    }\n    std::cout << std::endl;\n}",
            "// TODO: your code here\n}",
            "// TODO(dlongnecke): Implement me!\n}",
            "// TODO\n}",
            "Kokkos::RangePolicy<Kokkos::Serial> rangePolicy(0, x.extent(0));\n    Kokkos::parallel_for(rangePolicy, KOKKOS_LAMBDA(const int& i) {\n        double v = x(i);\n        size_t index = size_t(v / 10);\n        bins(index) += 1;\n    });\n    Kokkos::deep_copy(bins, bins);\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&] (size_t i) {\n        if (x(i) < 10)\n            ++bins[0];\n        else if (x(i) < 20)\n            ++bins[1];\n        else if (x(i) < 30)\n            ++bins[2];\n        else if (x(i) < 40)\n            ++bins[3];\n        else if (x(i) < 50)\n            ++bins[4];\n        else if (x(i) < 60)\n            ++bins[5];\n        else if (x(i) < 70)\n            ++bins[6];\n        else if (x(i) < 80)\n            ++bins[7];\n        else if (x(i) < 90)\n            ++bins[8];\n        else\n            ++bins[9];\n    });\n}",
            "Kokkos::View<double*, Kokkos::HostSpace> tmp(\"tmp\");\n\n  // TODO 1: Implement this function, which should loop over x and fill\n  // `bins` with the counts in each bin.\n\n  // TODO 2: Compute the sums of the elements of `bins` in parallel and store\n  // them in `tmp`.\n\n  // TODO 3: Copy the results back to `bins` using `Kokkos::deep_copy`.\n}",
            "auto x_d = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), x);\n  auto bins_d = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), bins);\n\n  for (size_t i = 0; i < 10; ++i) {\n    bins_d(i) = 0;\n  }\n\n  Kokkos::deep_copy(bins, bins_d);\n\n  for (size_t i = 0; i < x_d.extent(0); ++i) {\n    ++bins_d((size_t) (x_d(i) / 10));\n  }\n\n  Kokkos::deep_copy(bins_d, bins);\n}",
            "/* Fill your code here */\n}",
            "// TODO: Fill in this function.\n  size_t begin = 0;\n  size_t end = x.extent(0);\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> policy(begin, end);\n\n  Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const size_t i) {\n    size_t index = static_cast<size_t>(floor(x(i) / 10.0));\n    if (index < 10) {\n      bins(index) += 1;\n    }\n  });\n}",
            "// TODO: implement this\n    throw std::logic_error(\"TODO: implement this\");\n}",
            "Kokkos::parallel_for(\n    Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n      for (int k = 0; k < 10; k++) {\n        if (x(i) < k * 10) {\n          bins(k)++;\n          break;\n        }\n      }\n    });\n  Kokkos::fence();\n}",
            "// TODO: fill in body\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t>(0, x.extent(0));\n\n  Kokkos::parallel_for(policy, [=] (size_t i) {\n    // TODO: your code here\n  });\n\n  Kokkos::fence();\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0));\n    auto h_x = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(h_x, x);\n\n    Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const int i) {\n        int const numBins = 10;\n        int const binSize = 10;\n        int const firstBin = 0;\n        int const lastBin = numBins - 1;\n        int const inRange = std::min(lastBin, static_cast<int>(h_x(i)/binSize));\n        int const outRange = std::max(firstBin, inRange);\n        bins(outRange) += 1;\n    });\n\n    Kokkos::deep_copy(bins, bins);\n}",
            "const auto N = x.extent(0);\n\n    /* Compute each thread's histogram by 10 by doing a reduction across the\n       thread's partial sums. */\n    Kokkos::View<size_t[10]> partial_sums(\"partial_sums\", 1);\n    Kokkos::parallel_reduce(N, KOKKOS_LAMBDA(const int i, size_t &partial_sum) {\n        const int j = (int) (x(i) / 10.0);\n        partial_sum += j >= 0 && j < 10;\n    }, Kokkos::Sum<size_t>(partial_sums));\n\n    /* Compute the final histogram by summing across the partial sums. */\n    Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int i) {\n        bins(i) = partial_sums(0);\n    });\n}",
            "auto const N = x.extent(0);\n  // TODO: replace with correct usage of Kokkos\n  Kokkos::parallel_for(\n      \"binsBy10Count\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, N),\n      KOKKOS_LAMBDA(const int i) {\n        if (0 <= x(i) && x(i) < 10)\n          Kokkos::atomic_fetch_add(&bins[x(i)], 1);\n      });\n  Kokkos::fence();\n}",
            "// TODO: Implement this function.\n}",
            "// Your code here\n}",
            "// TODO: Implement\n}",
            "// TODO: your code goes here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n                       [=](int i) {\n        size_t index = (size_t)floor(x(i)/10);\n        if(index < 10)\n            ++bins(index);\n    });\n}",
            "// TODO: Complete this function!\n  // You may find Kokkos's reductions useful.\n  // You may find Kokkos's subviews useful.\n  // You may find Kokkos's offsets useful.\n}",
            "// Your code goes here\n}",
            "// TODO: Implement this function\n  // You will need to fill in the body of the function to get the correct\n  // results.\n}",
            "auto i = Kokkos::ThreadVectorRange(Kokkos::ThreadVectorRange(bins.size()));\n  for (size_t j = 0; j < x.size(); j++) {\n    for (size_t k = 0; k < 10; k++) {\n      if (x[j] >= (k * 10) && x[j] < ((k + 1) * 10))\n        Kokkos::atomic_fetch_add(&(bins[k]), 1);\n    }\n  }\n}",
            "Kokkos::parallel_for(10, KOKKOS_LAMBDA(const int i) {\n    size_t count = 0;\n    for (size_t j=0; j < x.extent(0); j++)\n      if (i * 10 <= x(j) && x(j) < (i + 1) * 10)\n        count++;\n    bins(i) = count;\n  });\n}",
            "const size_t n = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace, size_t>(0, n),\n                       KOKKOS_LAMBDA(const size_t i) {\n    const size_t tens = 10 * (x(i) / 10);\n    ++bins(tens);\n  });\n}",
            "// TODO: Your code here\n}",
            "auto x_h = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_h, x);\n  auto bins_h = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_h, bins);\n\n  // parallel_for(0, 10, KOKKOS_LAMBDA (const int i) {\n    // for (int i = 0; i < 10; i++) {\n  Kokkos::parallel_for(10, KOKKOS_LAMBDA (const int i) {\n    size_t count = 0;\n    for (size_t j = 0; j < x_h.extent(0); j++) {\n      if (i * 10 <= x_h(j) && x_h(j) < (i + 1) * 10) {\n        count++;\n      }\n    }\n    bins_h(i) = count;\n  });\n  Kokkos::deep_copy(bins, bins_h);\n}",
            "//...\n}",
            "// TODO: Use Kokkos to compute the histogram.\n  // The data range is [0, 100) and the number of bins is 10.\n\n  // TODO: Return from this function the computed histogram.\n  // Hint: use the `bins` variable.\n\n  // TODO: Verify that the result is correct.\n}",
            "// Write your code here.\n}",
            "/* TODO: Your code here */\n}",
            "auto n = x.extent(0);\n  auto k = 10;\n  auto b = 0;\n\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA (const int i) {\n    b = (x(i) / k);\n    if (b < 10) bins(b) += 1;\n  });\n}",
            "// TODO\n}",
            "}",
            "// TODO\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::OpenMP>(0, x.extent(0));\n    Kokkos::parallel_for(\"Kokkos Bins by 10\", policy,\n    [&] (size_t i) {\n        size_t bin = (size_t) x(i) / 10;\n        if (bin == 0) {\n            ++bins(0);\n        } else if (bin == 1) {\n            ++bins(1);\n        } else if (bin == 2) {\n            ++bins(2);\n        } else if (bin == 3) {\n            ++bins(3);\n        } else if (bin == 4) {\n            ++bins(4);\n        } else if (bin == 5) {\n            ++bins(5);\n        } else if (bin == 6) {\n            ++bins(6);\n        } else if (bin == 7) {\n            ++bins(7);\n        } else if (bin == 8) {\n            ++bins(8);\n        } else if (bin == 9) {\n            ++bins(9);\n        }\n    });\n}",
            "// TODO: fill in your solution here\n}",
            "// Kokkos view to the bin array.\n    auto bins_ = Kokkos::create_mirror_view(bins);\n\n    // Loop over the values in the input array.\n    for (int i = 0; i < x.extent(0); i++) {\n        double value = x(i);\n\n        // Find which bin the value should go in.\n        int bin = static_cast<int>(std::floor(value / 10));\n\n        // Increment the count for this bin.\n        bins_(bin)++;\n    }\n\n    // Copy the counts back to the host.\n    Kokkos::deep_copy(bins, bins_);\n}",
            "// TODO: add your code here\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n  throw std::runtime_error(\"Not implemented yet\");\n}",
            "// TODO(you): Your code goes here.\n}",
            "// YOUR CODE HERE\n  // Hint: the number of elements in x should be the same as the number of\n  // elements in bins.\n\n  auto x_host = Kokkos::create_mirror_view_and_copy(Kokkos::HostSpace(), x);\n  auto bins_host = Kokkos::create_mirror_view(bins);\n\n  for (int i = 0; i < 10; ++i) {\n    bins_host(i) = std::count_if(x_host.data(), x_host.data() + x_host.extent(0),\n                                 [i](double x) { return x >= i * 10 && x < (i + 1) * 10; });\n  }\n\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement a solution here\n    return;\n}",
            "Kokkos::parallel_for(\n        Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n        KOKKOS_LAMBDA(const int i) {\n            auto x_val = x(i);\n            if (x_val >= 0 && x_val < 10) {\n                ++bins(x_val);\n            }\n        });\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::HostSpace, Kokkos::Schedule<Kokkos::Dynamic>, Kokkos::IndexType<size_t>>>(0, x.extent(0)),\n        KOKKOS_LAMBDA(size_t i) {\n            size_t ix = (size_t)(10 * x(i));\n            bins(ix) += 1;\n        });\n}",
            "// TODO: Implement me!\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(const int i) {\n    const int value = static_cast<int>(x(i));\n    for (int j = 0; j < 10; j++) {\n      if (value < (j + 1) * 10) {\n        bins[j]++;\n        break;\n      }\n    }\n  });\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for schedule(static,1)\n    for (size_t i = 0; i < x.size(); i++) {\n        size_t bin = x[i]/10;\n        if (bin <= 9) {\n            bins[bin] += 1;\n        }\n    }\n}",
            "/* To get a parallel for loop, we need to do the following:\n     - add the #pragma omp parallel for directive above the for loop\n     - add a private clause after the for loop.\n  */\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    int index = int(x[i]/10);\n    if (index >= 0 && index < 10) {\n      bins[index] += 1;\n    }\n  }\n}",
            "// Initialize `bins` to all zeros\n  bins = std::array<size_t, 10>();\n\n  // TODO: Compute `bins` in parallel\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    size_t j = std::floor(x[i] / 10);\n    if (j < bins.size()) {\n      bins[j] += 1;\n    }\n  }\n}",
            "// TODO\n  bins.fill(0);\n  for (size_t i = 0; i < x.size(); i++)\n  {\n    size_t bin = floor(x[i] / 10);\n    bins[bin]++;\n  }\n  // for (size_t i = 0; i < bins.size(); i++)\n  // {\n  //   std::cout << bins[i] << \" \";\n  // }\n  // std::cout << std::endl;\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t idx = (size_t) (x[i] / 10);\n    // std::cout << \"i: \" << i << \", idx: \" << idx << std::endl;\n    if (idx < bins.size()) {\n      // std::cout << \"incrementing bin \" << idx << std::endl;\n      bins[idx]++;\n    }\n  }\n}",
            "size_t numBins = bins.size();\n  size_t blockSize = x.size() / numBins;\n  size_t lastBlockSize = x.size() % numBins;\n\n  // Fill first and last bins\n  #pragma omp parallel for\n  for (int i=0; i < blockSize; ++i) {\n    if (x[i] < 10.0) {\n      ++bins[0];\n    } else if (x[i] < 20.0) {\n      ++bins[1];\n    } else if (x[i] < 30.0) {\n      ++bins[2];\n    } else if (x[i] < 40.0) {\n      ++bins[3];\n    } else if (x[i] < 50.0) {\n      ++bins[4];\n    } else if (x[i] < 60.0) {\n      ++bins[5];\n    } else if (x[i] < 70.0) {\n      ++bins[6];\n    } else if (x[i] < 80.0) {\n      ++bins[7];\n    } else if (x[i] < 90.0) {\n      ++bins[8];\n    } else if (x[i] < 100.0) {\n      ++bins[9];\n    }\n  }\n  for (int i=0; i < lastBlockSize; ++i) {\n    if (x[i+blockSize] < 10.0) {\n      ++bins[0];\n    } else if (x[i+blockSize] < 20.0) {\n      ++bins[1];\n    } else if (x[i+blockSize] < 30.0) {\n      ++bins[2];\n    } else if (x[i+blockSize] < 40.0) {\n      ++bins[3];\n    } else if (x[i+blockSize] < 50.0) {\n      ++bins[4];\n    } else if (x[i+blockSize] < 60.0) {\n      ++bins[5];\n    } else if (x[i+blockSize] < 70.0) {\n      ++bins[6];\n    } else if (x[i+blockSize] < 80.0) {\n      ++bins[7];\n    } else if (x[i+blockSize] < 90.0) {\n      ++bins[8];\n    } else if (x[i+blockSize] < 100.0) {\n      ++bins[9];\n    }\n  }\n}",
            "// Replace this!\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  bins[4] = 0;\n  bins[5] = 0;\n  bins[6] = 0;\n  bins[7] = 0;\n  bins[8] = 0;\n  bins[9] = 0;\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n\n    for(auto value : x) {\n        auto const bin = value / 10.0;\n        #pragma omp atomic update\n        ++bins[bin];\n    }\n}",
            "bins.fill(0);\n\n  omp_set_num_threads(2);\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    double x_i = x[i];\n    if (x_i >= 0 && x_i < 10) {\n      bins[x_i] += 1;\n    }\n  }\n}",
            "/* Add your implementation here */\n}",
            "bins.fill(0);\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      bins[(int) (x[i] / 10)] += 1;\n    }\n  }\n}",
            "// your code here\n  // TODO: replace this dummy code with your code\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  bins[4] = 0;\n  bins[5] = 0;\n  bins[6] = 0;\n  bins[7] = 0;\n  bins[8] = 0;\n  bins[9] = 0;\n\n  for (double value : x) {\n    if (value >= 0 && value < 10) {\n      bins[static_cast<size_t>(value)] += 1;\n    }\n  }\n}",
            "// YOUR CODE HERE\n   // Initialize the bins\n   for (auto &count : bins) {\n      count = 0;\n   }\n   // Count\n   int thread_count = omp_get_max_threads();\n   omp_set_num_threads(thread_count);\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < x.size(); i++) {\n      bins[static_cast<int>(x[i]/10)]++;\n   }\n   // YOUR CODE ENDS\n}",
            "// TODO: Your code here\n\n  // Hint: remember that an OpenMP loop is parallelized by dividing it into\n  // `omp_get_num_threads()` chunks, each executing the loop's body independently\n  // on different threads.\n  // Hint: use the omp pragma to indicate that a given variable is private, and\n  // use a reduction clause on a parallel loop to sum the values into bins.\n}",
            "// TODO\n}",
            "// initialize bins\n    for (size_t i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n\n    // find the bin for each element in parallel\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        // convert `x[i]` to an integer between 0 and 9\n        int bin = x[i] / 10;\n\n        // increment the bin for this value\n        ++bins[bin];\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// initialize\n    for (auto &b : bins) b = 0;\n\n    // count\n    for (auto const &i : x) {\n        auto const j = (i / 10);\n        if (j < 10)\n            bins[j]++;\n    }\n}",
            "bins.fill(0);\n  #pragma omp parallel for\n  for(size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 10.0) bins[0]++;\n    else if (x[i] < 20.0) bins[1]++;\n    else if (x[i] < 30.0) bins[2]++;\n    else if (x[i] < 40.0) bins[3]++;\n    else if (x[i] < 50.0) bins[4]++;\n    else if (x[i] < 60.0) bins[5]++;\n    else if (x[i] < 70.0) bins[6]++;\n    else if (x[i] < 80.0) bins[7]++;\n    else if (x[i] < 90.0) bins[8]++;\n    else if (x[i] < 100.0) bins[9]++;\n  }\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 10) {\n      bins[9]++;\n    } else {\n      bins[x[i]/10]++;\n    }\n  }\n}",
            "bins = std::array<size_t, 10> {};\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double val = x[i];\n    int bin = (int) ((val / 10) - (int) (val / 10));\n    bins[bin]++;\n  }\n}",
            "/* your solution here */\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t index = std::floor(x[i] / 10);\n    bins[index]++;\n  }\n}",
            "#pragma omp parallel for schedule(dynamic, 100)\n    for (int i = 0; i < x.size(); i++) {\n        int bin = (int)(x[i] / 10);\n        #pragma omp atomic\n        bins[bin]++;\n    }\n}",
            "int nthreads = 1;\n#pragma omp parallel\n   {\n      nthreads = omp_get_num_threads();\n   }\n   bins.fill(0);\n   size_t const n = x.size();\n   size_t const chunksize = n / nthreads;\n   std::vector<size_t> local_bins(bins.size());\n#pragma omp parallel\n   {\n      int const thread_num = omp_get_thread_num();\n      size_t const begin = chunksize * thread_num;\n      size_t const end = std::min(begin + chunksize, n);\n      for (size_t i = begin; i < end; ++i) {\n         size_t const value = static_cast<size_t>(x[i]);\n         if (value < 10)\n            ++local_bins[0];\n         else if (value < 20)\n            ++local_bins[1];\n         else if (value < 30)\n            ++local_bins[2];\n         else if (value < 40)\n            ++local_bins[3];\n         else if (value < 50)\n            ++local_bins[4];\n         else if (value < 60)\n            ++local_bins[5];\n         else if (value < 70)\n            ++local_bins[6];\n         else if (value < 80)\n            ++local_bins[7];\n         else if (value < 90)\n            ++local_bins[8];\n         else\n            ++local_bins[9];\n      }\n#pragma omp critical\n      {\n         for (size_t i = 0; i < bins.size(); ++i)\n            bins[i] += local_bins[i];\n      }\n   }\n}",
            "/* YOUR CODE HERE */\n    bins = std::array<size_t, 10>();\n}",
            "for (auto x : x) {\n    // Find the bin number for this value\n    int bin_number = (int)x / 10;\n    // Increment the corresponding bin counter\n    bins[bin_number]++;\n  }\n}",
            "int n_threads = 10;\n  omp_set_num_threads(n_threads);\n  size_t N = x.size();\n#pragma omp parallel for\n  for (size_t i = 0; i < N; i++) {\n    size_t index = (size_t) std::floor(x[i]/10);\n    bins[index] = bins[index] + 1;\n  }\n}",
            "#pragma omp parallel for schedule(static)\n  for (int i = 0; i < x.size(); ++i) {\n    size_t index = x[i] / 10;\n    bins[index]++;\n  }\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < 10; ++i) {\n\t\tbins[i] = 0;\n\t}\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tbins[x[i] / 10]++;\n\t}\n}",
            "// TODO: compute number of bins using data size and store in the variable\n  // `numBins`. Hint: You can use std::ceil to calculate the ceiling of a\n  // floating-point number.\n  size_t numBins = 0;\n\n  // TODO: add this outer loop as a parallel region using OpenMP.\n  // TODO: inside the parallel region, initialize `numBins` to zero.\n  // Hint: the `for` loop should be marked as a parallel region.\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    // TODO: add this inner loop as a parallel region using OpenMP.\n    // TODO: inside the parallel region, determine the correct bin for the\n    // current value in `x`. Hint: You can use the size of the array to\n    // determine the bin.\n    // Hint: the `for` loop should be marked as a parallel region.\n    #pragma omp parallel for\n    for (size_t j = 0; j < numBins; ++j) {\n      // TODO: add this inner loop as a parallel region using OpenMP.\n      // TODO: inside the parallel region, update `bins[j]` based on the\n      // current value in `x`. Hint: You can use std::floor to calculate the\n      // floor of a floating-point number.\n      #pragma omp parallel for\n      for (size_t k = 0; k < x.size(); ++k) {\n        if (x[k] > j * 10 && x[k] <= (j + 1) * 10) {\n          ++bins[j];\n        }\n      }\n    }\n  }\n}",
            "size_t n = x.size();\n  // TODO\n  //#pragma omp parallel for\n  //for (int i = 0; i < n; ++i) {\n  //  int idx = (int) (x[i] / 10);\n  //  if (idx >= 0 && idx < 10) ++bins[idx];\n  //}\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        bins[static_cast<int>(x[i] / 10)]++;\n    }\n}",
            "// Fill up the bins array\n  // Hint: use the modulo operator % to find the remainder\n  size_t total = x.size();\n#pragma omp parallel for schedule(static)\n  for (int i = 0; i < total; i++) {\n    int bucket = x[i] / 10;\n    bins[bucket % 10] += 1;\n  }\n}",
            "omp_set_num_threads(omp_get_max_threads());\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        bins[static_cast<int>(x[i]/10)]++;\n    }\n}",
            "// TODO\n  // Hint: see http://www.cplusplus.com/reference/algorithm/count/\n}",
            "int numBins = 10;\n    for (int i = 0; i < numBins; ++i) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        // Compute the bin for the value.\n        int bin = x[i] / 10;\n        // Increment the count for the bin.\n        bins[bin]++;\n    }\n}",
            "// Initialize the bins\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  // Loop over x values\n  int num_threads = omp_get_max_threads();\n  std::vector<std::array<size_t, 10>> bins_by_thread(num_threads);\n  for (int i = 0; i < num_threads; i++) {\n    for (int j = 0; j < 10; j++) {\n      bins_by_thread[i][j] = 0;\n    }\n  }\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    int id = omp_get_thread_num();\n    int bin = std::min(std::floor(x[i] / 10), 9);\n    bins_by_thread[id][bin] += 1;\n  }\n  for (int i = 0; i < num_threads; i++) {\n    for (int j = 0; j < 10; j++) {\n      bins[j] += bins_by_thread[i][j];\n    }\n  }\n}",
            "// Initialize bins to 0\n    for (auto &b: bins) {\n        b = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        size_t index = (size_t) std::floor(x[i] / 10);\n        bins[index] += 1;\n    }\n\n}",
            "auto n = x.size();\n    std::array<size_t, 10> sum_bins = {};\n    // for (int i = 0; i < n; i++) {\n    //     sum_bins[static_cast<int>(std::round(x[i] / 10))]++;\n    // }\n    // bins = sum_bins;\n\n    // parallelize the code above\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        sum_bins[static_cast<int>(std::round(x[i] / 10))]++;\n    }\n    bins = sum_bins;\n}",
            "auto const n = x.size();\n#pragma omp parallel for schedule(static, 10)\n  for (size_t i = 0; i < n; i++) {\n    double v = x[i];\n    if (0 <= v && v < 10) {\n      bins[static_cast<size_t>(v)]++;\n    }\n  }\n}",
            "}",
            "// Your code here\n\n  // You can test your implementation by uncommenting the following lines\n#if 0\n  std::cout << \"input:  \";\n  for(auto i: x) {\n    std::cout << i << \" \";\n  }\n  std::cout << std::endl;\n\n  std::cout << \"output: \";\n  for(auto i: bins) {\n    std::cout << i << \" \";\n  }\n  std::cout << std::endl;\n#endif\n}",
            "#pragma omp parallel for\n    for(size_t i=0; i<x.size(); i++) {\n        size_t bin = (size_t)(x[i]/10);\n        if(bin==10) {\n            bin = 0;\n        }\n        bins[bin] += 1;\n    }\n}",
            "size_t i;\n    #pragma omp parallel for\n    for (i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0.0 && x[i] < 10.0)\n            bins[0]++;\n        else if (x[i] >= 10.0 && x[i] < 20.0)\n            bins[1]++;\n        else if (x[i] >= 20.0 && x[i] < 30.0)\n            bins[2]++;\n        else if (x[i] >= 30.0 && x[i] < 40.0)\n            bins[3]++;\n        else if (x[i] >= 40.0 && x[i] < 50.0)\n            bins[4]++;\n        else if (x[i] >= 50.0 && x[i] < 60.0)\n            bins[5]++;\n        else if (x[i] >= 60.0 && x[i] < 70.0)\n            bins[6]++;\n        else if (x[i] >= 70.0 && x[i] < 80.0)\n            bins[7]++;\n        else if (x[i] >= 80.0 && x[i] < 90.0)\n            bins[8]++;\n        else if (x[i] >= 90.0 && x[i] < 100.0)\n            bins[9]++;\n    }\n}",
            "// Your code goes here\n}",
            "#pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] < 10) {\n      ++bins[0];\n    } else if (x[i] < 20) {\n      ++bins[1];\n    } else if (x[i] < 30) {\n      ++bins[2];\n    } else if (x[i] < 40) {\n      ++bins[3];\n    } else if (x[i] < 50) {\n      ++bins[4];\n    } else if (x[i] < 60) {\n      ++bins[5];\n    } else if (x[i] < 70) {\n      ++bins[6];\n    } else if (x[i] < 80) {\n      ++bins[7];\n    } else if (x[i] < 90) {\n      ++bins[8];\n    } else if (x[i] < 100) {\n      ++bins[9];\n    }\n  }\n}",
            "bins.fill(0);\n  int num_threads = 0;\n#pragma omp parallel\n  {\n    num_threads = omp_get_num_threads();\n    std::vector<size_t> bin(10);\n\n#pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] < 10) {\n        bin[size_t(x[i])] += 1;\n      }\n    }\n\n#pragma omp critical\n    for (int i = 0; i < 10; ++i) {\n      bins[i] += bin[i];\n    }\n  }\n  std::cout << \"num_threads = \" << num_threads << std::endl;\n}",
            "// TODO(student): implement parallel version of function using OpenMP\n\n}",
            "// TODO: write your code here\n    bins.fill(0);\n    omp_set_num_threads(4);\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < x.size(); i++){\n        if ((i % 10) == 0){\n            bins[i / 10] += 1;\n        }\n    }\n    //std::cout << \"Result: \";\n    //for (size_t b : bins) {\n    //    std::cout << b << \", \";\n    //}\n    //std::cout << \"\\n\";\n}",
            "// TODO: implement\n  //std::fill(bins.begin(), bins.end(), 0);\n  //#pragma omp parallel for\n  //for (size_t i = 0; i < x.size(); i++) {\n  //  bins[(size_t)(x[i]/10)]++;\n  //}\n\n}",
            "// Your code here\n}",
            "#pragma omp parallel\n   {\n      int numThreads = omp_get_num_threads();\n      int threadId = omp_get_thread_num();\n      int chunkSize = (x.size() + numThreads - 1) / numThreads;\n      int begin = threadId * chunkSize;\n      int end = std::min((threadId+1)*chunkSize, (int)x.size());\n\n      std::array<size_t, 10> threadBins{};\n      for (int i = begin; i < end; i++) {\n         int index = x[i] / 10;\n         threadBins[index]++;\n      }\n\n      #pragma omp critical\n      for (int i = 0; i < 10; i++) {\n         bins[i] += threadBins[i];\n      }\n   }\n}",
            "#pragma omp parallel\n\t{\n\t\tstd::array<size_t, 10> localBins;\n\t\t#pragma omp for\n\t\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\t\tlocalBins[std::min(x[i] / 10, 9)] += 1;\n\t\t}\n\t\t#pragma omp critical\n\t\t{\n\t\t\tfor (size_t i = 0; i < 10; ++i) {\n\t\t\t\tbins[i] += localBins[i];\n\t\t\t}\n\t\t}\n\t}\n}",
            "bins = std::array<size_t, 10> {};\n\n    #pragma omp parallel for\n    for(int i = 0; i < x.size(); i++) {\n        if(x[i] < 10)\n            bins[x[i]] += 1;\n    }\n}",
            "size_t size = x.size();\n  // TODO: Fill in your solution here.\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int idx = x[i] / 10;\n    if (0 <= idx && idx < 10) {\n      bins[idx]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int num = floor(x[i] / 10);\n        if (0 <= num && num <= 9) {\n            bins[num]++;\n        }\n    }\n}",
            "// initialize all bins to zero\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int index = (int) (x[i] / 10);\n        bins[index] = bins[index] + 1;\n    }\n}",
            "// TODO: Implement the function\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    auto xi = x[i];\n    if (xi >= 0 && xi < 10) {\n      bins[xi] += 1;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "/* TODO: replace the following line with your implementation.\n     (you may add additional lines of code) */\n  #pragma omp parallel\n  {\n    std::array<size_t, 10> bins_tmp;\n    for (int i = 0; i < x.size(); i++) {\n      if (x[i] >= 0 && x[i] < 10)\n        bins_tmp[x[i]]++;\n    }\n    for (int i = 0; i < bins.size(); i++)\n      bins[i] = bins_tmp[i];\n  }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (size_t i=0; i<x.size(); i++) {\n        if (x[i] < 10) {\n            bins[x[i]]++;\n        } else if (x[i] < 20) {\n            bins[10]++;\n        } else if (x[i] < 30) {\n            bins[20]++;\n        } else if (x[i] < 40) {\n            bins[30]++;\n        } else if (x[i] < 50) {\n            bins[40]++;\n        } else if (x[i] < 60) {\n            bins[50]++;\n        } else if (x[i] < 70) {\n            bins[60]++;\n        } else if (x[i] < 80) {\n            bins[70]++;\n        } else if (x[i] < 90) {\n            bins[80]++;\n        } else if (x[i] < 100) {\n            bins[90]++;\n        } else {\n            bins[100]++;\n        }\n    }\n}",
            "// your code here\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    bins[std::min(static_cast<size_t>(x[i]/10), 9)]++;\n  }\n}",
            "bins.fill(0);\n\t#pragma omp parallel for\n\tfor (int i = 0; i < (int)x.size(); i++) {\n\t\tbins[std::min(9, std::floor(x[i]/10))]++;\n\t}\n}",
            "// TODO: Your code here\n  // omp_set_num_threads(8);\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < 10) {\n            ++bins[x[i]];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (auto it = x.begin(); it!= x.end(); ++it) {\n    bins[std::min(size_t((*it)/10), size_t(9))]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < x.size(); ++i) {\n        bins[int((x[i] / 10) + 0.5)]++;\n    }\n}",
            "for(size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        auto const ix = static_cast<int>(std::floor(x[i] / 10.0));\n        bins[ix]++;\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        if(x[i] < 10) {\n            bins[size_t(x[i])] += 1;\n        }\n    }\n}",
            "// YOUR CODE HERE\n  // omp_set_num_threads(1);\n\n  std::memset(bins.data(), 0, sizeof(size_t) * bins.size());\n\n  omp_lock_t *locks = new omp_lock_t[bins.size()];\n  for (int i = 0; i < bins.size(); ++i)\n    omp_init_lock(&locks[i]);\n\n  #pragma omp parallel for schedule(dynamic, 1)\n  for (int i = 0; i < x.size(); ++i) {\n    int index = (int)((x[i] / 10.0) * 10);\n    if (index == 0)\n      ++bins[0];\n    else if (index == 10)\n      ++bins[1];\n    else if (index == 20)\n      ++bins[2];\n    else if (index == 30)\n      ++bins[3];\n    else if (index == 40)\n      ++bins[4];\n    else if (index == 50)\n      ++bins[5];\n    else if (index == 60)\n      ++bins[6];\n    else if (index == 70)\n      ++bins[7];\n    else if (index == 80)\n      ++bins[8];\n    else if (index == 90)\n      ++bins[9];\n    else {\n      omp_set_lock(&locks[index - 10]);\n      ++bins[index - 10];\n      omp_unset_lock(&locks[index - 10]);\n    }\n  }\n\n  for (int i = 0; i < bins.size(); ++i)\n    omp_destroy_lock(&locks[i]);\n  delete [] locks;\n}",
            "// Your code goes here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = static_cast<size_t>(x[i] / 10);\n        if (bin < 10) {\n            #pragma omp atomic\n            ++bins[bin];\n        }\n    }\n}",
            "size_t i;\n\n#pragma omp parallel for schedule(static)\n    for (i = 0; i < x.size(); i++) {\n        bins[int(x[i]/10)]++;\n    }\n}",
            "for (size_t i = 0; i < 10; ++i)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i)\n        ++bins[int(x[i] / 10)];\n}",
            "// TODO: Your code here\n  // You can compute the bins in parallel using OpenMP. See the following\n  // website for more information: http://www.openmp.org\n  // Use the provided skeleton code in the comment below to get started.\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    int nThreads = omp_get_max_threads();\n    size_t size = x.size();\n    int chunk = size/nThreads;\n    std::vector<size_t> partial_bins(10);\n    for (int i=0; i<nThreads; i++) {\n        size_t s = i*chunk;\n        size_t e = (i+1)*chunk;\n        std::fill(partial_bins.begin(), partial_bins.end(), 0);\n        for (size_t j=s; j<e; j++) {\n            if (x[j] < 10) {\n                partial_bins[static_cast<size_t>(x[j])] += 1;\n            }\n        }\n        #pragma omp critical\n        {\n            for (size_t j=0; j<10; j++) {\n                bins[j] += partial_bins[j];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (int i = 0; i < (int)x.size(); i++) {\n      bins[(int)(x[i] / 10)]++;\n   }\n}",
            "omp_set_dynamic(0);\n    omp_set_num_threads(10);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        #pragma omp atomic\n        ++bins[(size_t) (x[i]/10.0)];\n    }\n}",
            "bins = std::array<size_t, 10>();\n    for (size_t i = 0; i < x.size(); ++i) {\n        bins[std::floor(x[i]/10)]++;\n    }\n}",
            "// Write your OpenMP code here.\n  // To run this in a debugger,\n  // add a breakpoint after the `omp parallel` statement.\n}",
            "#pragma omp parallel for\n\tfor (int i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 10) {\n\t\t\tbins[x[i]] += 1;\n\t\t}\n\t}\n}",
            "for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n    int threadCount = omp_get_max_threads();\n    #pragma omp parallel for num_threads(threadCount) schedule(static)\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            bins[int(x[i])] += 1;\n        }\n    }\n}",
            "// YOUR CODE HERE\n    size_t i;\n#pragma omp parallel for private(i)\n    for(i = 0; i < x.size(); i++) {\n        size_t index = (x[i] / 10);\n        bins[index]++;\n    }\n    // END OF YOUR CODE\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tbins[static_cast<int>(x[i] / 10)]++;\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        bins[std::min(static_cast<size_t>((x[i] / 10)), 9)]++;\n    }\n}",
            "// YOUR CODE HERE\n\n}",
            "// Write your code here.\n}",
            "#pragma omp parallel for\n\tfor(size_t i = 0; i < x.size(); ++i) {\n\t\tsize_t bin = 10*x[i];\n\t\t++bins[bin];\n\t}\n}",
            "// TODO: parallelize with OpenMP\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    bins[static_cast<size_t>(x[i]) / 10]++;\n  }\n}",
            "// TO DO: implement this function!\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        for (size_t j = 0; j < 10; j++) {\n            if (x[i] >= j * 10 && x[i] < (j+1) * 10) {\n                #pragma omp critical\n                {\n                    bins[j]++;\n                }\n            }\n        }\n    }\n}",
            "size_t n = x.size();\n    bins = std::array<size_t, 10> {};\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        int i10 = std::floor(x[i] / 10);\n        bins[i10] = bins[i10] + 1;\n    }\n}",
            "// Initialize the bins array\n  for (auto& bin : bins) {\n    bin = 0;\n  }\n\n  // Your OpenMP parallel code here.\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: implement this\n    bins.fill(0);\n    size_t const n = x.size();\n#pragma omp parallel for reduction(+:bins)\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] < 10) {\n            ++bins[x[i]];\n        }\n    }\n}",
            "omp_set_num_threads(omp_get_max_threads());\n  std::fill(bins.begin(), bins.end(), 0);\n  #pragma omp parallel for\n  for (auto i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] < 10) {\n      bins[x[i]]++;\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "int N = x.size();\n    size_t sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for(int i = 0; i < N; i++) {\n        if(x[i] < 10){\n            sum += 1;\n        }\n    }\n    bins[0] = sum;\n\n    sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for(int i = 0; i < N; i++) {\n        if(x[i] >= 10 && x[i] < 20){\n            sum += 1;\n        }\n    }\n    bins[1] = sum;\n\n    //...\n\n    sum = 0;\n    #pragma omp parallel for reduction(+:sum)\n    for(int i = 0; i < N; i++) {\n        if(x[i] >= 90 && x[i] < 100){\n            sum += 1;\n        }\n    }\n    bins[9] = sum;\n}",
            "// TODO\n}",
            "// TODO: implement me!\n}",
            "#pragma omp parallel for\n  for (size_t i=0; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 10) {\n      ++bins[x[i]];\n    } else if (x[i] >= 10 && x[i] < 20) {\n      ++bins[10];\n    } else if (x[i] >= 20 && x[i] < 30) {\n      ++bins[20];\n    } else if (x[i] >= 30 && x[i] < 40) {\n      ++bins[30];\n    } else if (x[i] >= 40 && x[i] < 50) {\n      ++bins[40];\n    } else if (x[i] >= 50 && x[i] < 60) {\n      ++bins[50];\n    } else if (x[i] >= 60 && x[i] < 70) {\n      ++bins[60];\n    } else if (x[i] >= 70 && x[i] < 80) {\n      ++bins[70];\n    } else if (x[i] >= 80 && x[i] < 90) {\n      ++bins[80];\n    } else if (x[i] >= 90 && x[i] < 100) {\n      ++bins[90];\n    }\n  }\n}",
            "// your code here\n}",
            "# pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 10) {\n      bins[0]++;\n    } else if (x[i] >= 20) {\n      bins[1]++;\n    } else if (x[i] >= 30) {\n      bins[2]++;\n    } else if (x[i] >= 40) {\n      bins[3]++;\n    } else if (x[i] >= 50) {\n      bins[4]++;\n    } else if (x[i] >= 60) {\n      bins[5]++;\n    } else if (x[i] >= 70) {\n      bins[6]++;\n    } else if (x[i] >= 80) {\n      bins[7]++;\n    } else if (x[i] >= 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tdouble num = x[i] / 10;\n\t\tbins[static_cast<size_t>(num)]++;\n\t}\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        bins[x[i] / 10]++;\n    }\n}",
            "}",
            "#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tdouble element = x[i];\n\t\tint bin = element / 10.0;\n\t\tif (bin == 10)\n\t\t\tbin = 0;\n\t\t++bins[bin];\n\t}\n}",
            "// Your code here.\n  size_t N = x.size();\n\n  // #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    if ((int)x[i] >= 0 && (int)x[i] < 10) {\n      bins[x[i]]++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n  // You may want to add additional variables to hold intermediate data\n  \n  // Hint: use bins[i] to store the number of values in the [i*10,(i+1)*10) range\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  size_t size = x.size();\n  size_t min = 0;\n  size_t max = 9;\n\n  #pragma omp parallel for\n  for(size_t i = 0; i < size; i++){\n    int bin = (int)((x[i] - min) / (max - min) * 9);\n    #pragma omp atomic\n    ++bins[bin];\n  }\n}",
            "// TODO: implement this function\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < x.size(); i++) {\n      auto idx = std::floor(x[i] / 10.0);\n      std::atomic<size_t> &a = bins[idx];\n      a++;\n   }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp single\n        {\n            bins.fill(0);\n        }\n\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); ++i) {\n            size_t bin = std::floor(x[i] / 10);\n            bins[bin] += 1;\n        }\n    }\n}",
            "/* FIXME:\n   *\n   * Use a reduction clause to sum the elements in the `bins` array.\n   *\n   */\n  bins.fill(0);\n  #pragma omp parallel for\n  for(size_t i=0; i<x.size(); ++i) {\n    size_t bi = static_cast<size_t>(std::floor(10*x[i])) % 10;\n    ++bins[bi];\n  }\n}",
            "#pragma omp parallel for schedule(static, 10)\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] < 10)\n      ++bins[0];\n    else if (x[i] < 20)\n      ++bins[1];\n    else if (x[i] < 30)\n      ++bins[2];\n    else if (x[i] < 40)\n      ++bins[3];\n    else if (x[i] < 50)\n      ++bins[4];\n    else if (x[i] < 60)\n      ++bins[5];\n    else if (x[i] < 70)\n      ++bins[6];\n    else if (x[i] < 80)\n      ++bins[7];\n    else if (x[i] < 90)\n      ++bins[8];\n    else\n      ++bins[9];\n  }\n}",
            "#pragma omp parallel num_threads(4)\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      // Your code here\n    }\n  }\n}",
            "// TODO: Implement OpenMP version\n}",
            "for (size_t i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n\n    if (x[i] < 10) {\n      bins[0]++;\n    } else if (x[i] < 20) {\n      bins[1]++;\n    } else if (x[i] < 30) {\n      bins[2]++;\n    } else if (x[i] < 40) {\n      bins[3]++;\n    } else if (x[i] < 50) {\n      bins[4]++;\n    } else if (x[i] < 60) {\n      bins[5]++;\n    } else if (x[i] < 70) {\n      bins[6]++;\n    } else if (x[i] < 80) {\n      bins[7]++;\n    } else if (x[i] < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    double tmp = (x[i] - 0) / 10.0;\n    int index = (int) tmp;\n    bins[index]++;\n  }\n}",
            "// TODO: fill in this function\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n# pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    bins[(size_t)(x[i] / 10.0)] += 1;\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = (size_t)std::floor(x[i]/10);\n        ++bins[bin];\n    }\n}",
            "// TODO\n}",
            "int const num_threads = omp_get_max_threads();\n\n  size_t count = 0;\n  for(auto i = 0; i < num_threads; i++) {\n    bins[i] = 0;\n  }\n\n  for(auto const &v : x) {\n    #pragma omp atomic\n    ++count;\n    auto const index = (int)v / 10;\n    #pragma omp atomic\n    ++bins[index];\n  }\n\n  // std::cout << \"Sum: \" << count << std::endl;\n  for(auto i = 1; i < num_threads; i++) {\n    bins[0] += bins[i];\n  }\n}",
            "// TODO: implement this function\n}",
            "// initialize bins to 0\n   for (auto& b : bins) b = 0;\n\n   #pragma omp parallel for schedule(dynamic)\n   for (int i = 0; i < x.size(); i++) {\n      // compute which bin this number belongs in\n      auto index = int(x[i] / 10);\n      // increment the corresponding bin\n      if (index < 10) bins[index]++;\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        bins[(int)std::floor(x[i]/10)]++;\n    }\n}",
            "int count = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 10) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[0] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 20 && x[i] >= 10) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[1] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 30 && x[i] >= 20) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[2] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 40 && x[i] >= 30) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[3] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 50 && x[i] >= 40) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[4] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 60 && x[i] >= 50) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[5] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 70 && x[i] >= 60) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[6] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 80 && x[i] >= 70) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[7] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 90 && x[i] >= 80) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[8] = count;\n\tcount = 0;\n\t#pragma omp parallel for reduction(+:count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 100 && x[i] >= 90) {\n\t\t\tcount++;\n\t\t}\n\t}\n\tbins[9] = count;\n}",
            "// TODO: implement\n}",
            "// omp_set_num_threads(1);\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    size_t value = x[i] / 10.0;\n    // bins[value] = 1;\n    #pragma omp atomic\n      bins[value]++;\n  }\n}",
            "// YOUR CODE HERE\n  size_t n = x.size();\n  std::array<size_t, 10> my_bins;\n  std::array<size_t, 10> sum_bins;\n  std::fill(my_bins.begin(), my_bins.end(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    my_bins[(size_t)x[i] / 10] += 1;\n  }\n  // YOUR CODE HERE\n#pragma omp critical\n  {\n    for (size_t i = 0; i < 10; i++) {\n      sum_bins[i] += my_bins[i];\n    }\n  }\n  bins = sum_bins;\n}",
            "size_t const N = x.size();\n    std::fill(bins.begin(), bins.end(), 0);\n\n    size_t const step = 10;\n\n#pragma omp parallel\n    {\n        size_t tid = omp_get_thread_num();\n        std::array<size_t, 10> tidBins;\n\n#pragma omp for\n        for (size_t i = 0; i < N; ++i) {\n            size_t index = std::floor(x[i] / step);\n            if (index < 10) {\n                tidBins[index]++;\n            }\n        }\n\n        for (size_t i = 0; i < 10; ++i) {\n            bins[i] += tidBins[i];\n        }\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (auto i = 0; i < x.size(); i++) {\n        if (x[i] < 10.0) {\n            bins[static_cast<size_t>(x[i])]++;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 10.0)\n            bins[0]++;\n        else if (x[i] >= 10.0 && x[i] < 20.0)\n            bins[1]++;\n        else if (x[i] >= 20.0 && x[i] < 30.0)\n            bins[2]++;\n        else if (x[i] >= 30.0 && x[i] < 40.0)\n            bins[3]++;\n        else if (x[i] >= 40.0 && x[i] < 50.0)\n            bins[4]++;\n        else if (x[i] >= 50.0 && x[i] < 60.0)\n            bins[5]++;\n        else if (x[i] >= 60.0 && x[i] < 70.0)\n            bins[6]++;\n        else if (x[i] >= 70.0 && x[i] < 80.0)\n            bins[7]++;\n        else if (x[i] >= 80.0 && x[i] < 90.0)\n            bins[8]++;\n        else\n            bins[9]++;\n    }\n}",
            "// Your code goes here\n}",
            "// Your code goes here.\n  int nThreads;\n  #pragma omp parallel\n  {\n      nThreads = omp_get_num_threads();\n  }\n  \n  #pragma omp parallel for\n  for(size_t i = 0; i < x.size(); i++)\n  {\n      double value = x[i];\n      int bin = value/10;\n      if(bin >= 0 && bin < 10)\n          bins[bin]++;\n  }\n}",
            "// write omp code here\n  size_t i = 0;\n  size_t i_end = 0;\n  int nthreads = omp_get_max_threads();\n  int thread_num = omp_get_thread_num();\n  size_t n_per_thread = x.size() / nthreads;\n  size_t rest = x.size() % nthreads;\n  bins.fill(0);\n#pragma omp parallel num_threads(nthreads)\n  {\n#pragma omp for schedule(dynamic)\n    for (i = 0; i < nthreads; ++i) {\n      i_end = i * n_per_thread;\n      if (thread_num == i) {\n        i_end += rest;\n      }\n      for (size_t i = i_end; i < x.size(); ++i) {\n        if (x[i] < 10) {\n          bins[0]++;\n        } else if (x[i] < 20) {\n          bins[1]++;\n        } else if (x[i] < 30) {\n          bins[2]++;\n        } else if (x[i] < 40) {\n          bins[3]++;\n        } else if (x[i] < 50) {\n          bins[4]++;\n        } else if (x[i] < 60) {\n          bins[5]++;\n        } else if (x[i] < 70) {\n          bins[6]++;\n        } else if (x[i] < 80) {\n          bins[7]++;\n        } else if (x[i] < 90) {\n          bins[8]++;\n        } else {\n          bins[9]++;\n        }\n      }\n    }\n  }\n}",
            "bins.fill(0);\n#pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < x.size(); ++i) {\n        bins[int(x[i] / 10)] += 1;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    const size_t index = std::round(x[i] / 10.0) - 1;\n    if (index < bins.size()) {\n      ++bins[index];\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    bins[static_cast<size_t>((x[i] * 10) / 100.0)]++;\n  }\n}",
            "// TODO: Fill this in!\n}",
            "for(int i = 0; i < x.size(); i++) {\n\t\tbins[int(x[i]/10)]++;\n\t}\n}",
            "// TODO: Your code here!\n\n    size_t length = x.size();\n    size_t temp_length = length;\n    size_t num_threads = omp_get_num_procs();\n    size_t chunk_size = temp_length/num_threads;\n    size_t remainder = temp_length%num_threads;\n    size_t start = 0;\n    size_t end = 0;\n    std::vector<int> result(10, 0);\n    std::vector<int> temp_result;\n    \n    #pragma omp parallel for num_threads(num_threads)\n    for (int i=0; i<num_threads; i++){\n        start = end;\n        end = chunk_size + start;\n        if (i == (num_threads - 1)){\n            end = end + remainder;\n        }\n        temp_result = std::vector<int>(10, 0);\n        for (int j=start; j<end; j++){\n            if (x[j] >= 0 && x[j] < 10)\n                temp_result[size_t(x[j])] += 1;\n        }\n        for (int k=0; k<10; k++){\n            result[k] += temp_result[k];\n        }\n    }\n    \n    bins = std::array<size_t, 10>();\n    for (int i=0; i<10; i++){\n        bins[i] = result[i];\n    }\n    return;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    if (0 <= x[i] && x[i] < 10)\n      bins[static_cast<size_t>(x[i])] += 1;\n  }\n}",
            "size_t N = x.size();\n  size_t start, end;\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < N; i++) {\n      start = (size_t)(x[i] / 10.0);\n      if (start == 10)\n        start = 0;\n      end = (size_t)(x[i] / 10.0) + 1;\n      bins[start] += 1;\n      if (end < 10)\n        bins[end] += 1;\n    }\n  }\n}",
            "// compute the value of each bin\n    // you can use #pragma omp parallel for schedule(dynamic)\n    // or #pragma omp parallel for schedule(dynamic, 5)\n    // to set the number of threads used to the best value for your\n    // machine.\n    // you can also use #pragma omp parallel for schedule(dynamic, 0)\n    // to make the program run in sequential order\n    // you can also use #pragma omp parallel for schedule(static)\n    // to make the program run in the same order as in the input data\n    // you can also use #pragma omp parallel for schedule(guided)\n    // to make the program run in the same order as in the input data,\n    // but with some randomization\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); ++i) {\n        size_t bin = (x[i] / 10);\n        if (bin < 10) {\n            bins[bin] += 1;\n        }\n    }\n}",
            "size_t n = x.size();\n\n   bins = std::array<size_t, 10>();\n\n   // for (int i = 0; i < 10; ++i)\n   // {\n   //    bins[i] = 0;\n   // }\n\n   // std::fill(bins.begin(), bins.end(), 0);\n   // std::fill(bins.data(), bins.data() + 10, 0);\n\n   // omp_set_num_threads(4);\n   // #pragma omp parallel for\n   for (int i = 0; i < n; ++i)\n   {\n      bins[(int)(x[i] / 10)]++;\n   }\n}",
            "// FIXME: YOUR CODE HERE\n}",
            "#pragma omp parallel for\n    for(int i = 0; i < x.size(); ++i) {\n        if(x[i] >= 0 && x[i] < 10) ++bins[static_cast<size_t>(x[i])];\n    }\n}",
            "size_t n = x.size();\n  //  std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] >= 0 && x[i] <= 10) bins[0]++;\n    else if (x[i] > 10 && x[i] <= 20) bins[1]++;\n    else if (x[i] > 20 && x[i] <= 30) bins[2]++;\n    else if (x[i] > 30 && x[i] <= 40) bins[3]++;\n    else if (x[i] > 40 && x[i] <= 50) bins[4]++;\n    else if (x[i] > 50 && x[i] <= 60) bins[5]++;\n    else if (x[i] > 60 && x[i] <= 70) bins[6]++;\n    else if (x[i] > 70 && x[i] <= 80) bins[7]++;\n    else if (x[i] > 80 && x[i] <= 90) bins[8]++;\n    else if (x[i] > 90 && x[i] <= 100) bins[9]++;\n  }\n}",
            "int num_threads = omp_get_max_threads();\n  std::vector<size_t> partial_sums(num_threads + 1, 0);\n\n  #pragma omp parallel for\n  for(int i = 0; i < x.size(); i++) {\n    int tid = omp_get_thread_num();\n    if(x[i] < 10) {\n      partial_sums[tid] += 1;\n    }\n    else if(x[i] < 20) {\n      partial_sums[tid] += 2;\n    }\n    else if(x[i] < 30) {\n      partial_sums[tid] += 3;\n    }\n    else if(x[i] < 40) {\n      partial_sums[tid] += 4;\n    }\n    else if(x[i] < 50) {\n      partial_sums[tid] += 5;\n    }\n    else if(x[i] < 60) {\n      partial_sums[tid] += 6;\n    }\n    else if(x[i] < 70) {\n      partial_sums[tid] += 7;\n    }\n    else if(x[i] < 80) {\n      partial_sums[tid] += 8;\n    }\n    else if(x[i] < 90) {\n      partial_sums[tid] += 9;\n    }\n    else {\n      partial_sums[tid] += 10;\n    }\n  }\n  for(int i = 1; i < num_threads + 1; i++) {\n    partial_sums[i] += partial_sums[i - 1];\n  }\n  for(int i = 0; i < num_threads + 1; i++) {\n    bins[i] = partial_sums[i];\n  }\n}",
            "// TODO: fill in code here\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: implement me\n}",
            "for (auto &bin : bins) {\n        bin = 0;\n    }\n    #pragma omp parallel for num_threads(omp_get_num_procs())\n    for (size_t i = 0; i < x.size(); ++i) {\n        if ((int) x[i] < 10) {\n            bins[x[i]];\n        }\n    }\n}",
            "/* TODO: your code here */\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n  for(size_t i=0; i<x.size(); i++) {\n    bins[ std::floor(x[i]/10) ]++;\n  }\n}",
            "/* TO DO */\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 10) {\n            bins[x[i]]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for(int i=0;i<x.size();i++) {\n    double bin_id = x[i] / 10.0;\n    int bin = (int)bin_id;\n    if(bin_id - bin < 0.5) bins[bin] += 1;\n    else bins[bin+1] += 1;\n  }\n}",
            "// YOUR CODE HERE\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0. && x[i] < 10.) {\n      bins[0]++;\n    } else if (x[i] >= 10. && x[i] < 20.) {\n      bins[1]++;\n    } else if (x[i] >= 20. && x[i] < 30.) {\n      bins[2]++;\n    } else if (x[i] >= 30. && x[i] < 40.) {\n      bins[3]++;\n    } else if (x[i] >= 40. && x[i] < 50.) {\n      bins[4]++;\n    } else if (x[i] >= 50. && x[i] < 60.) {\n      bins[5]++;\n    } else if (x[i] >= 60. && x[i] < 70.) {\n      bins[6]++;\n    } else if (x[i] >= 70. && x[i] < 80.) {\n      bins[7]++;\n    } else if (x[i] >= 80. && x[i] < 90.) {\n      bins[8]++;\n    } else if (x[i] >= 90. && x[i] <= 100.) {\n      bins[9]++;\n    }\n  }\n}",
            "// TODO\n}",
            "int nThreads = 1;\n  #pragma omp parallel\n  {\n    nThreads = omp_get_num_threads();\n  }\n\n  omp_set_num_threads(nThreads);\n\n  bins.fill(0);\n  #pragma omp parallel\n  {\n    int threadId = omp_get_thread_num();\n    #pragma omp for\n    for (int i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0 && x[i] < 10) {\n        ++bins[static_cast<size_t>(x[i])];\n      }\n    }\n  }\n}",
            "// TODO\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] <= 10) {\n            bins[x[i]] += 1;\n        }\n    }\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "bins = {0,0,0,0,0,0,0,0,0,0};\n\n    // Your code goes here\n    for(int i=0; i<x.size(); i++){\n        int index = x[i]/10;\n        bins[index]++;\n    }\n}",
            "int n = x.size();\n    int tid = omp_get_thread_num();\n    int nThreads = omp_get_num_threads();\n\n    std::vector<size_t> threadBins(10);\n#pragma omp parallel for schedule(static)\n    for (int i = tid; i < n; i += nThreads) {\n        double val = x[i];\n        int bin = (int) floor((val / 10.0) * 10.0);\n        if (bin < 10) {\n            threadBins[bin] += 1;\n        }\n    }\n\n#pragma omp critical\n    {\n        for (int i = 0; i < 10; i++) {\n            bins[i] += threadBins[i];\n        }\n    }\n}",
            "// TODO: Compute histogram by dividing the interval [0, 10) into 10 bins\n  // (similar to the bins by 10 count in the README)\n  // Hint: This should be a pretty simple problem.\n  // Hint: You may find the following useful:\n  // https://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width\n\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++)\n  {\n    int index = (int) ((x[i] / 10.0) * 10);\n    if (index >= 0 && index < 10)\n      bins[index]++;\n  }\n}",
            "/* Your code goes here */\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        bins[int(x[i] / 10)]++;\n    }\n}",
            "// Fill your answer here\n    // This is just a sample implementation.\n    // Note that you cannot use std::fill here as it is not thread-safe.\n    for (int i = 0; i < 10; ++i)\n        bins[i] = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < x.size(); ++i) {\n        double xi = x[i];\n        int bin_index = (xi / 10);\n        if (xi >= 0 && xi < 100)\n            bins[bin_index]++;\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto value = x[i];\n    auto bin = static_cast<size_t>(value / 10);\n    bins[bin]++;\n  }\n}",
            "// fill the bins to 0\n  std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel\n  {\n    // determine how many values to work on\n    // each thread works on values from threads[tid] to threads[tid+1]\n    const size_t n = x.size();\n    const size_t chunk_size = (n+omp_get_num_threads()-1)/omp_get_num_threads();\n    const size_t start = omp_get_thread_num() * chunk_size;\n    const size_t end = std::min(start+chunk_size, n);\n\n    // count the values in the range [start, end)\n    for (size_t i=start; i<end; i++) {\n      size_t bin = std::floor(x[i]/10);\n      bins[bin] += 1;\n    }\n\n  }\n}",
            "// TODO: your code here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  //...\n}",
            "#pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i=0; i<x.size(); ++i) {\n      size_t bin = static_cast<size_t>(std::floor(x[i]/10));\n      ++bins[bin];\n    }\n  }\n}",
            "bins.fill(0);\n  int threadCount = omp_get_max_threads();\n  std::vector<std::array<size_t, 10>> partialBins(threadCount);\n\n  // Each thread will have 10 values\n  double valuesPerThread = x.size() / threadCount;\n  std::vector<int> work(threadCount);\n  int sum = 0;\n\n  for (int i = 0; i < threadCount; i++) {\n    work[i] = i;\n  }\n\n  #pragma omp parallel for schedule(static, 1)\n  for (int i = 0; i < threadCount; i++) {\n    int start = i * valuesPerThread;\n    int end = std::min(start + valuesPerThread, x.size());\n\n    for (int j = start; j < end; j++) {\n      int binIdx = std::floor(x[j] / 10);\n      partialBins[i][binIdx]++;\n    }\n  }\n\n  for (int i = 0; i < threadCount; i++) {\n    for (int j = 0; j < 10; j++) {\n      bins[j] += partialBins[i][j];\n    }\n  }\n}",
            "bins.fill(0);\n\n    #pragma omp parallel\n    {\n        int threadNum = omp_get_thread_num();\n\n        for (auto x_i : x) {\n            double value = (x_i / 10.0);\n            int index = int(value);\n            int remainder = int(value) - index;\n            if (remainder > 0.0) {\n                index += 1;\n            }\n            bins[index] += 1;\n        }\n    }\n}",
            "// Hint: use the OpenMP atomic API to implement this parallel for loop\n#pragma omp parallel for reduction(+:bins[0:9])\n    for (size_t i = 0; i < x.size(); i++) {\n        double x_i = x[i];\n        if (x_i < 10.0) {\n            // Hint: use the OpenMP atomic API here as well to\n            //       increment the correct bin\n            std::atomic<size_t> &bin_index = bins[(size_t)x_i];\n            bin_index++;\n        }\n    }\n}",
            "// TODO: Compute the number of values in each bin using OpenMP.\n  // Hint: See the omp_parallel_for example in week 2 lecture notes for a skeleton code.\n\n#pragma omp parallel for\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n    for (int j = 0; j < x.size(); j++) {\n      if (x[j] >= i * 10 && x[j] < (i + 1) * 10) {\n        bins[i] = bins[i] + 1;\n      }\n    }\n  }\n}",
            "// TODO: compute the histogram in parallel\n\n}",
            "//TODO: implement the function\n}",
            "// TODO: compute bins\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel\n   {\n      std::vector<size_t> localBins(10);\n#pragma omp for\n      for (int i = 0; i < x.size(); ++i) {\n         int idx = static_cast<int>(std::floor(x[i] / 10));\n         if (idx < 10) {\n            ++localBins[idx];\n         }\n      }\n\n#pragma omp critical\n      {\n         for (int i = 0; i < 10; ++i) {\n            bins[i] += localBins[i];\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  size_t n = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    int val = x[i];\n    if (val >= 0 && val < 10) {\n      bins[val]++;\n    }\n  }\n}",
            "// TODO: your code goes here\n  int threads = omp_get_max_threads();\n  //std::cout << threads << std::endl;\n  std::vector<int> count;\n  count.resize(threads);\n  for(size_t i = 0; i < threads; ++i){\n    count[i] = 0;\n  }\n  \n  for(size_t i = 0; i < x.size(); ++i){\n    int thread = omp_get_thread_num();\n    if(x[i] <= 10){\n      ++count[thread];\n    }\n  }\n  \n  // reduce\n  for(int i = 0; i < threads; ++i){\n    bins[i] = count[i];\n  }\n}",
            "size_t i = 0;\n\n  #pragma omp parallel for \n  for (i = 0; i < x.size(); i++) {\n    int bin = std::floor(x[i] / 10);\n    if (bin >= 0 && bin < 10) {\n      bins[bin]++;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (auto i = 0; i < x.size(); ++i) {\n    if (x[i] < 10) {\n      ++bins[x[i]];\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    // TODO: implement OpenMP parallelization\n    size_t size = x.size();\n#pragma omp parallel for\n    for (size_t i = 0; i < size; i++) {\n        size_t index = (size_t)x[i] / 10;\n        bins[index]++;\n    }\n}",
            "// initialize bins array to zero\n  std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    // get the current value in the vector\n    double val = x[i];\n    // check if it is in the range [0, 10)\n    if (val >= 0 && val < 10) {\n      // get the index of the bin in the output\n      int index = (int) (val / 10.0);\n      // increment the bin\n      bins[index] += 1;\n    }\n  }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        int val = x[i];\n        // val/10 is the first digit of val\n        // val%10 is the second digit of val\n        bins[val/10] += 1;\n    }\n}",
            "for (int i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n  for (double val: x) {\n    bins[std::min(10 - 1, static_cast<int>(val / 10))] += 1;\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "serial",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "bins.fill(0);\n    for (auto xi : x) {\n        auto idx = (size_t) std::round(xi / 10.0);\n        ++bins[idx];\n    }\n}",
            "/* TODO */\n}",
            "for (auto elem: x) {\n    if (elem >= 0 && elem < 10)\n      ++bins[static_cast<size_t>(elem)];\n  }\n}",
            "for (auto const& i: x) {\n        bins[std::min(size_t(std::floor(i/10)), 9)]++;\n    }\n}",
            "bins.fill(0);\n\n  for (auto val : x) {\n    if (val < 0 || val >= 100) {\n      throw std::invalid_argument(\"x should be between 0 and 100\");\n    }\n    bins[int(val / 10)]++;\n  }\n}",
            "// Your code here\n}",
            "// TODO: implement this function\n    // Hint: use the std::upper_bound function\n    for (double element : x) {\n        size_t index = static_cast<size_t>(element / 10);\n        if (element >= 0 && element < 100) {\n            ++bins[index];\n        }\n    }\n}",
            "for (auto x_i : x) {\n      bins[static_cast<size_t>(floor(x_i / 10))] += 1;\n   }\n}",
            "for (double xi: x) {\n    int idx = (int) (xi / 10.0);\n    ++bins[idx];\n  }\n}",
            "// TODO: implement me!\n  double step = 10;\n  int count = 0;\n  for (int i = 0; i < x.size(); i++) {\n    if ((x[i] < (i+1)*step) && (x[i] >= i*step)) {\n      count++;\n    }\n  }\n  return;\n}",
            "for (double d: x) {\n        size_t index = std::min((size_t)((d-0.0)/10.0), 9);\n        bins[index]++;\n    }\n}",
            "// Initialize vector bins to all zeros\n    for (auto &b : bins) {\n        b = 0;\n    }\n\n    // For each value of vector x, find out which bin it goes into\n    for (auto const& v : x) {\n        auto b = floor(v / 10);\n        bins[b]++;\n    }\n}",
            "assert(bins.size() == 10);\n    for (double xi : x) {\n        bins[(size_t)((xi/10.0))]++;\n    }\n}",
            "size_t index{0};\n  size_t bin{0};\n  for (auto const& value : x) {\n    if (value <= 10) {\n      index = 0;\n    } else if (value <= 20) {\n      index = 1;\n    } else if (value <= 30) {\n      index = 2;\n    } else if (value <= 40) {\n      index = 3;\n    } else if (value <= 50) {\n      index = 4;\n    } else if (value <= 60) {\n      index = 5;\n    } else if (value <= 70) {\n      index = 6;\n    } else if (value <= 80) {\n      index = 7;\n    } else if (value <= 90) {\n      index = 8;\n    } else {\n      index = 9;\n    }\n    bins[index]++;\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Replace this with a call to std::accumulate\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t index = std::floor(x[i]/10);\n    if (index == 10) index = 9;\n    bins[index]++;\n  }\n}",
            "auto const N = x.size();\n  bins = std::array<size_t, 10> {};\n  for (auto i = 0; i < N; i++) {\n    auto val = std::floor(x[i] / 10);\n    auto bin = (int) val;\n    bins[bin] += 1;\n  }\n}",
            "for (auto val : x) {\n    if (val < 10)\n      ++bins[0];\n    else if (val < 20)\n      ++bins[1];\n    else if (val < 30)\n      ++bins[2];\n    else if (val < 40)\n      ++bins[3];\n    else if (val < 50)\n      ++bins[4];\n    else if (val < 60)\n      ++bins[5];\n    else if (val < 70)\n      ++bins[6];\n    else if (val < 80)\n      ++bins[7];\n    else if (val < 90)\n      ++bins[8];\n    else\n      ++bins[9];\n  }\n}",
            "for(auto &item : bins)\n    item = 0;\n  for(auto i : x) {\n    if(i >= 0 && i < 10)\n      ++bins[static_cast<size_t>(i)];\n  }\n}",
            "for (int i=0; i<10; i++) {\n    bins[i]=0;\n  }\n  for (double xx : x) {\n    if (xx >= 0 && xx <= 10) {\n      bins[static_cast<int>(xx)]++;\n    }\n  }\n}",
            "auto const n = x.size();\n    std::array<size_t, 10> counts;\n    for (size_t i = 0; i < n; ++i) {\n        size_t bin = static_cast<size_t>(x[i] / 10);\n        if (bin == 10) bin = 9;\n        ++counts[bin];\n    }\n    bins = std::move(counts);\n}",
            "std::vector<size_t> counts(10);\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 10) {\n      counts[0]++;\n    } else if (x[i] < 20) {\n      counts[1]++;\n    } else if (x[i] < 30) {\n      counts[2]++;\n    } else if (x[i] < 40) {\n      counts[3]++;\n    } else if (x[i] < 50) {\n      counts[4]++;\n    } else if (x[i] < 60) {\n      counts[5]++;\n    } else if (x[i] < 70) {\n      counts[6]++;\n    } else if (x[i] < 80) {\n      counts[7]++;\n    } else if (x[i] < 90) {\n      counts[8]++;\n    } else {\n      counts[9]++;\n    }\n  }\n  for (size_t i = 0; i < counts.size(); i++) {\n    bins[i] = counts[i];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < 10) {\n            ++bins[0];\n        } else if (x[i] < 20) {\n            ++bins[1];\n        } else if (x[i] < 30) {\n            ++bins[2];\n        } else if (x[i] < 40) {\n            ++bins[3];\n        } else if (x[i] < 50) {\n            ++bins[4];\n        } else if (x[i] < 60) {\n            ++bins[5];\n        } else if (x[i] < 70) {\n            ++bins[6];\n        } else if (x[i] < 80) {\n            ++bins[7];\n        } else if (x[i] < 90) {\n            ++bins[8];\n        } else {\n            ++bins[9];\n        }\n    }\n}",
            "auto const n = x.size();\n  for (size_t i=0; i<n; ++i) {\n    auto const j = static_cast<size_t>(x[i]/10);\n    if (j>=bins.size()) {\n      continue;\n    }\n    bins[j]++;\n  }\n}",
            "for (auto const& val : x) {\n        ++bins[(size_t) (val / 10.0)];\n    }\n}",
            "bins.fill(0);\n   for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] < 10) {\n         ++bins[0];\n      }\n      else if (x[i] < 20) {\n         ++bins[1];\n      }\n      else if (x[i] < 30) {\n         ++bins[2];\n      }\n      else if (x[i] < 40) {\n         ++bins[3];\n      }\n      else if (x[i] < 50) {\n         ++bins[4];\n      }\n      else if (x[i] < 60) {\n         ++bins[5];\n      }\n      else if (x[i] < 70) {\n         ++bins[6];\n      }\n      else if (x[i] < 80) {\n         ++bins[7];\n      }\n      else if (x[i] < 90) {\n         ++bins[8];\n      }\n      else if (x[i] < 100) {\n         ++bins[9];\n      }\n   }\n}",
            "if (x.size() == 0) {\n\t\treturn;\n\t}\n\n\tsize_t idx = 0;\n\twhile (idx < x.size()) {\n\t\tdouble value = x[idx];\n\t\tint bin = (int) value / 10;\n\t\tif (bin >= 10) {\n\t\t\tbin = 9;\n\t\t}\n\t\tbins[bin] += 1;\n\t\tidx += 1;\n\t}\n}",
            "bins.fill(0);\n    for (double xi : x) {\n        if (xi >= 0.0 && xi < 10.0) {\n            bins[0]++;\n        } else if (xi >= 10.0 && xi < 20.0) {\n            bins[1]++;\n        } else if (xi >= 20.0 && xi < 30.0) {\n            bins[2]++;\n        } else if (xi >= 30.0 && xi < 40.0) {\n            bins[3]++;\n        } else if (xi >= 40.0 && xi < 50.0) {\n            bins[4]++;\n        } else if (xi >= 50.0 && xi < 60.0) {\n            bins[5]++;\n        } else if (xi >= 60.0 && xi < 70.0) {\n            bins[6]++;\n        } else if (xi >= 70.0 && xi < 80.0) {\n            bins[7]++;\n        } else if (xi >= 80.0 && xi < 90.0) {\n            bins[8]++;\n        } else if (xi >= 90.0 && xi < 100.0) {\n            bins[9]++;\n        }\n    }\n}",
            "for (auto val : x) {\n    if (val >= 100) {\n      ++bins[9];\n    } else if (val >= 90) {\n      ++bins[8];\n    } else if (val >= 80) {\n      ++bins[7];\n    } else if (val >= 70) {\n      ++bins[6];\n    } else if (val >= 60) {\n      ++bins[5];\n    } else if (val >= 50) {\n      ++bins[4];\n    } else if (val >= 40) {\n      ++bins[3];\n    } else if (val >= 30) {\n      ++bins[2];\n    } else if (val >= 20) {\n      ++bins[1];\n    } else {\n      ++bins[0];\n    }\n  }\n}",
            "for (auto const& v : x) {\n        if (v < 10) {\n            ++bins[0];\n        } else if (v < 20) {\n            ++bins[1];\n        } else if (v < 30) {\n            ++bins[2];\n        } else if (v < 40) {\n            ++bins[3];\n        } else if (v < 50) {\n            ++bins[4];\n        } else if (v < 60) {\n            ++bins[5];\n        } else if (v < 70) {\n            ++bins[6];\n        } else if (v < 80) {\n            ++bins[7];\n        } else if (v < 90) {\n            ++bins[8];\n        } else {\n            ++bins[9];\n        }\n    }\n}",
            "// Your code here...\n}",
            "bins = std::array<size_t, 10>{0};\n    for (size_t i = 0; i < x.size(); ++i) {\n        // Get the value of x[i] and add 1 to the bin\n        int bin = std::floor(x[i] / 10);\n        bins[bin] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (auto val : x) {\n        if (0 <= val && val < 10) {\n            bins[static_cast<size_t>(val)] += 1;\n        }\n    }\n}",
            "for (auto xi : x) {\n    auto bin = std::min(static_cast<size_t>(10.0*xi/100.0), 9UL);\n    ++bins[bin];\n  }\n}",
            "// TO BE IMPLEMENTED\n\n    auto const n = x.size();\n    auto const lower = 0;\n    auto const upper = 10;\n    auto const width = upper - lower;\n    for (auto i = 0; i < n; ++i) {\n        auto const val = x[i];\n        auto const bin = std::floor((val - lower) / width);\n        bins[bin]++;\n    }\n}",
            "for (size_t i=0; i<x.size(); ++i) {\n    bins[int(x[i] / 10)]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (size_t i = 0; i < x.size(); i++) {\n    bins[(int) std::floor(x[i] / 10)] += 1;\n  }\n}",
            "// TODO: Implement\n}",
            "// initialize bins array\n  std::fill(bins.begin(), bins.end(), 0);\n  // compute and store the number of elements in each bin\n  for (double value : x) {\n    if (value < 10) {\n      bins[0]++;\n    } else if (value < 20) {\n      bins[1]++;\n    } else if (value < 30) {\n      bins[2]++;\n    } else if (value < 40) {\n      bins[3]++;\n    } else if (value < 50) {\n      bins[4]++;\n    } else if (value < 60) {\n      bins[5]++;\n    } else if (value < 70) {\n      bins[6]++;\n    } else if (value < 80) {\n      bins[7]++;\n    } else if (value < 90) {\n      bins[8]++;\n    } else if (value < 100) {\n      bins[9]++;\n    }\n  }\n}",
            "// TODO: your code here\n}",
            "for (auto const& i : x) {\n    if (i < 0) {\n      throw std::invalid_argument(\"x is not positive.\");\n    }\n    if (i > 100) {\n      throw std::invalid_argument(\"x is greater than 100.\");\n    }\n    int index = std::floor(i / 10);\n    bins[index] += 1;\n  }\n}",
            "for (auto val : x) {\n    if (val < 10) {\n      ++bins[0];\n    } else if (val < 20) {\n      ++bins[1];\n    } else if (val < 30) {\n      ++bins[2];\n    } else if (val < 40) {\n      ++bins[3];\n    } else if (val < 50) {\n      ++bins[4];\n    } else if (val < 60) {\n      ++bins[5];\n    } else if (val < 70) {\n      ++bins[6];\n    } else if (val < 80) {\n      ++bins[7];\n    } else if (val < 90) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "// To keep track of which bin we are in\n  int current_bin = 0;\n  // The current position in the bin, i.e. how far into the current bin are we?\n  double current_pos = 0;\n  // The previous value of current_pos, i.e. how far into the current bin did we\n  // last land?\n  double prev_pos = 0;\n\n  // Loop through all the values in x\n  for (double xi : x) {\n    // Add 1 to current_pos if xi is in [prev_pos, current_pos), otherwise\n    // increment the current bin and reset current_pos to the beginning of the\n    // bin.\n    // Example:\n    // prev_pos = 20\n    // current_pos = 30\n    // prev_pos < xi < current_pos\n    // current_pos = 0\n    // current_bin = 1\n    // prev_pos = 30\n    if (prev_pos < xi && xi < current_pos) {\n      current_pos = 0;\n      current_bin += 1;\n    } else {\n      current_pos += 1;\n    }\n    prev_pos = current_pos;\n    // If we have gone past 9, reset the current bin and continue to the next\n    // element.\n    if (current_bin > 9) {\n      current_bin = 9;\n      continue;\n    }\n    // Increment the number of values in current bin by 1.\n    bins[current_bin] += 1;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    for (double val: x) {\n        if (val < 10) {\n            bins[0] += 1;\n        } else if (val < 20) {\n            bins[1] += 1;\n        } else if (val < 30) {\n            bins[2] += 1;\n        } else if (val < 40) {\n            bins[3] += 1;\n        } else if (val < 50) {\n            bins[4] += 1;\n        } else if (val < 60) {\n            bins[5] += 1;\n        } else if (val < 70) {\n            bins[6] += 1;\n        } else if (val < 80) {\n            bins[7] += 1;\n        } else if (val < 90) {\n            bins[8] += 1;\n        } else {\n            bins[9] += 1;\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n\tfor (auto i : x) {\n\t\tif (i >= 0 && i < 10) {\n\t\t\t++bins[i];\n\t\t}\n\t}\n}",
            "// Fill the counts with the correct values in bins.\n    for (size_t i = 0; i < x.size(); ++i) {\n        int j = std::floor(x[i] / 10);\n        bins[j] += 1;\n    }\n}",
            "size_t const N = x.size();\n\tfor (size_t i = 0; i < N; ++i) {\n\t\t++bins[static_cast<size_t>(x[i]) / 10];\n\t}\n}",
            "bins = {0};\n  for (auto v : x) {\n    if (0 <= v && v < 10) {\n      ++bins[static_cast<size_t>(v)];\n    }\n  }\n}",
            "// YOUR CODE HERE\n    bins = std::array<size_t, 10>{};\n    for (const auto& a : x) {\n        if (a >= 0 && a < 10) {\n            ++bins[size_t(a)];\n        }\n    }\n}",
            "// Initialize the bins to zero\n  std::fill(bins.begin(), bins.end(), 0);\n  // Update the bins\n  for (auto const& xi : x) {\n    // Compute the bin index for xi\n    int bin = xi / 10;\n    // Update the corresponding bin\n    ++bins[bin];\n  }\n}",
            "for (int i = 0; i < 10; ++i) {\n        bins[i] = 0;\n    }\n    for (int i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0 && x[i] < 10) {\n            ++bins[static_cast<size_t>(x[i])];\n        }\n    }\n}",
            "// Count the number of values in the 10 bins, inclusive.\n  for (size_t i=0; i<10; i++) {\n    for (double xi: x) {\n      if (xi >= i*10 && xi < (i+1)*10) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "bins = {};\n\n  for (auto const &xi : x) {\n    double xi_div_10 = xi / 10;\n    auto xi_mod_10 = static_cast<size_t>(xi_div_10) % 10;\n    bins[xi_mod_10]++;\n  }\n}",
            "for (double xi : x) {\n    size_t b = static_cast<size_t>((xi / 10.0) + 0.5);\n    if (b < 10)\n      ++bins[b];\n  }\n}",
            "assert(x.size() >= 1);\n  std::fill(bins.begin(), bins.end(), 0);\n  for (auto v : x) {\n    auto bin = (size_t) ((v / 10) % 10);\n    bins[bin] += 1;\n  }\n}",
            "for (auto elem : x) {\n    bins[std::floor(elem / 10)] += 1;\n  }\n}",
            "// Fill the bins array with counts from x.\n  // Store the result in bins\n  // TODO\n}",
            "auto const &x_0 = x[0];\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto const &x_i = x[i];\n    bins[std::floor((x_i - x_0) / 10)]++;\n  }\n}",
            "// Implement your solution here.\n}",
            "for (auto val : x) {\n    if (val < 10) {\n      ++bins[0];\n    } else if (val < 20) {\n      ++bins[1];\n    } else if (val < 30) {\n      ++bins[2];\n    } else if (val < 40) {\n      ++bins[3];\n    } else if (val < 50) {\n      ++bins[4];\n    } else if (val < 60) {\n      ++bins[5];\n    } else if (val < 70) {\n      ++bins[6];\n    } else if (val < 80) {\n      ++bins[7];\n    } else if (val < 90) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "/*\n   TODO:\n   Implement the function.\n   */\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   for (auto xi : x) {\n      if (xi < 0) xi = 0;\n      if (xi >= 100) xi = 99;\n\n      bins[static_cast<size_t>(xi / 10)] += 1;\n   }\n}",
            "std::array<size_t, 10> temp{0};\n  size_t i;\n\n  for (i = 0; i < x.size(); ++i) {\n    if (x[i] < 10) {\n      ++temp[x[i]];\n    }\n  }\n  bins = temp;\n}",
            "// TODO: Your code goes here\n}",
            "for (auto const& element : x) {\n    //...\n    // element is in [0,10)\n    //...\n    bins[size_t(element / 10)]++;\n  }\n}",
            "bins.fill(0);\n  for (auto const& value : x) {\n    ++bins[std::floor(value / 10.0)];\n  }\n}",
            "for (auto x : x) {\n    if (x < 10) {\n      bins[0] += 1;\n    } else if (x < 20) {\n      bins[1] += 1;\n    } else if (x < 30) {\n      bins[2] += 1;\n    } else if (x < 40) {\n      bins[3] += 1;\n    } else if (x < 50) {\n      bins[4] += 1;\n    } else if (x < 60) {\n      bins[5] += 1;\n    } else if (x < 70) {\n      bins[6] += 1;\n    } else if (x < 80) {\n      bins[7] += 1;\n    } else if (x < 90) {\n      bins[8] += 1;\n    } else {\n      bins[9] += 1;\n    }\n  }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n        double val = x[i];\n        if (val < 10)\n            bins[0]++;\n        else if (val < 20)\n            bins[1]++;\n        else if (val < 30)\n            bins[2]++;\n        else if (val < 40)\n            bins[3]++;\n        else if (val < 50)\n            bins[4]++;\n        else if (val < 60)\n            bins[5]++;\n        else if (val < 70)\n            bins[6]++;\n        else if (val < 80)\n            bins[7]++;\n        else if (val < 90)\n            bins[8]++;\n        else if (val < 100)\n            bins[9]++;\n    }\n}",
            "bins.fill(0);\n  for (double v : x) {\n    if (v >= 0 && v < 10) {\n      bins[std::round(v)] += 1;\n    }\n  }\n}",
            "for (double a : x) {\n        if (a < 10) {\n            bins[0]++;\n        }\n        else if (a < 20) {\n            bins[1]++;\n        }\n        else if (a < 30) {\n            bins[2]++;\n        }\n        else if (a < 40) {\n            bins[3]++;\n        }\n        else if (a < 50) {\n            bins[4]++;\n        }\n        else if (a < 60) {\n            bins[5]++;\n        }\n        else if (a < 70) {\n            bins[6]++;\n        }\n        else if (a < 80) {\n            bins[7]++;\n        }\n        else if (a < 90) {\n            bins[8]++;\n        }\n        else {\n            bins[9]++;\n        }\n    }\n}",
            "bins = std::array<size_t, 10>();\n   // YOUR CODE HERE\n   for (auto i = 0; i < x.size(); i++) {\n      bins[x[i] / 10]++;\n   }\n}",
            "// Write your code here.\n    bins[0]=0;\n    bins[1]=0;\n    bins[2]=0;\n    bins[3]=0;\n    bins[4]=0;\n    bins[5]=0;\n    bins[6]=0;\n    bins[7]=0;\n    bins[8]=0;\n    bins[9]=0;\n\n    for (size_t i=0; i<x.size(); ++i) {\n        if (x[i]>=0 && x[i]<=10){\n            bins[0]=bins[0]+1;\n        }\n        else if (x[i]>10 && x[i]<=20){\n            bins[1]=bins[1]+1;\n        }\n        else if (x[i]>20 && x[i]<=30){\n            bins[2]=bins[2]+1;\n        }\n        else if (x[i]>30 && x[i]<=40){\n            bins[3]=bins[3]+1;\n        }\n        else if (x[i]>40 && x[i]<=50){\n            bins[4]=bins[4]+1;\n        }\n        else if (x[i]>50 && x[i]<=60){\n            bins[5]=bins[5]+1;\n        }\n        else if (x[i]>60 && x[i]<=70){\n            bins[6]=bins[6]+1;\n        }\n        else if (x[i]>70 && x[i]<=80){\n            bins[7]=bins[7]+1;\n        }\n        else if (x[i]>80 && x[i]<=90){\n            bins[8]=bins[8]+1;\n        }\n        else if (x[i]>90 && x[i]<=100){\n            bins[9]=bins[9]+1;\n        }\n    }\n\n}",
            "for (auto &b : bins) {\n    b = 0;\n  }\n  for (double value : x) {\n    if (value < 10) {\n      ++bins[0];\n    } else if (value < 20) {\n      ++bins[1];\n    } else if (value < 30) {\n      ++bins[2];\n    } else if (value < 40) {\n      ++bins[3];\n    } else if (value < 50) {\n      ++bins[4];\n    } else if (value < 60) {\n      ++bins[5];\n    } else if (value < 70) {\n      ++bins[6];\n    } else if (value < 80) {\n      ++bins[7];\n    } else if (value < 90) {\n      ++bins[8];\n    } else if (value < 100) {\n      ++bins[9];\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (double v : x) {\n    bins[std::min(9, std::floor(v / 10))] += 1;\n  }\n}",
            "// TODO: implement this function!\n}",
            "// Write your solution here.\n}",
            "bins.fill(0);\n  for (auto i : x) {\n    bins[i / 10]++;\n  }\n}",
            "for (auto const& xx : x) {\n      int idx = std::floor(xx / 10);\n      bins[idx]++;\n   }\n}",
            "if (x.empty()) {\n    return;\n  }\n\n  if (x[0] < 0 || x[0] > 100) {\n    throw std::invalid_argument(\"Invalid value in x.\");\n  }\n\n  if (x[x.size() - 1] < 0 || x[x.size() - 1] > 100) {\n    throw std::invalid_argument(\"Invalid value in x.\");\n  }\n\n  for (auto i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto i = 0; i < x.size(); i++) {\n    bins[std::floor(x[i]/10)]++;\n  }\n}",
            "size_t idx{0};\n  for (auto const& elem : x) {\n    if (elem >= 0.0 && elem < 10.0)\n      ++bins[idx];\n    else if (elem >= 10.0 && elem < 20.0)\n      ++bins[++idx];\n    else if (elem >= 20.0 && elem < 30.0)\n      ++bins[++idx];\n    else if (elem >= 30.0 && elem < 40.0)\n      ++bins[++idx];\n    else if (elem >= 40.0 && elem < 50.0)\n      ++bins[++idx];\n    else if (elem >= 50.0 && elem < 60.0)\n      ++bins[++idx];\n    else if (elem >= 60.0 && elem < 70.0)\n      ++bins[++idx];\n    else if (elem >= 70.0 && elem < 80.0)\n      ++bins[++idx];\n    else if (elem >= 80.0 && elem < 90.0)\n      ++bins[++idx];\n    else if (elem >= 90.0 && elem < 100.0)\n      ++bins[++idx];\n  }\n}",
            "bins.fill(0);\n    for (auto d : x) {\n        size_t idx = d / 10;\n        if (idx < 10) {\n            bins[idx]++;\n        }\n    }\n}",
            "// Your code goes here\n}",
            "bins.fill(0);\n   for(auto const& value: x){\n      auto const index = std::floor(value/10);\n      if(index >= 0 and index < bins.size()){\n         ++bins[size_t(index)];\n      }\n   }\n}",
            "// Add your solution here.\n\n}",
            "std::vector<double> binsBy10(10, 0);\n\n   for (auto const& x_i : x) {\n      int index = std::floor(x_i / 10);\n      binsBy10[index] += 1;\n   }\n\n   for (auto const& bin : binsBy10) {\n      bins[bin] += 1;\n   }\n}",
            "bins.fill(0);\n    for (double xi : x) {\n        if (xi >= 0 && xi < 10) {\n            ++bins[xi];\n        }\n    }\n}",
            "bins.fill(0);\n\n  // The following algorithm will not work if the values in x are\n  // not sorted. This is a requirement of the function\n  // `binsBy10`. Sort the vector first.\n  std::vector<double> xSorted = x;\n  std::sort(xSorted.begin(), xSorted.end());\n\n  // For each value in x, see where it belongs in xSorted.\n  for (auto &xVal : x) {\n    // Find the index of `xVal` in `xSorted`.\n    auto it = std::lower_bound(xSorted.begin(), xSorted.end(), xVal);\n    // Increment the count for the bin that `xVal` belongs in.\n    bins[std::distance(xSorted.begin(), it)]++;\n  }\n}",
            "for (auto v : x) {\n    auto index = static_cast<int>(v / 10.0);\n    if (index < 0) {\n      ++bins[0];\n    } else if (index > 9) {\n      ++bins[9];\n    } else {\n      ++bins[index];\n    }\n  }\n}",
            "// Clear bins\n    for (auto &x : bins) {\n        x = 0;\n    }\n\n    for (double value : x) {\n        // Which bin is this in?\n        int idx = static_cast<int>((value / 10.0) * 10);\n\n        // Increment the counter for the bin.\n        bins[idx] += 1;\n    }\n}",
            "for (double v : x) {\n    if (v < 10) bins[0]++;\n    else if (v < 20) bins[1]++;\n    else if (v < 30) bins[2]++;\n    else if (v < 40) bins[3]++;\n    else if (v < 50) bins[4]++;\n    else if (v < 60) bins[5]++;\n    else if (v < 70) bins[6]++;\n    else if (v < 80) bins[7]++;\n    else if (v < 90) bins[8]++;\n    else if (v < 100) bins[9]++;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto v : x) {\n    // bins are 10 bins, each bin ranges from 0 to 100\n    if (v < 10)\n      ++bins[0];\n    else if (v < 20)\n      ++bins[1];\n    else if (v < 30)\n      ++bins[2];\n    else if (v < 40)\n      ++bins[3];\n    else if (v < 50)\n      ++bins[4];\n    else if (v < 60)\n      ++bins[5];\n    else if (v < 70)\n      ++bins[6];\n    else if (v < 80)\n      ++bins[7];\n    else if (v < 90)\n      ++bins[8];\n    else // if (v < 100)\n      ++bins[9];\n  }\n}",
            "size_t i = 0;\n    for (auto v : x) {\n        if (v < 10) {\n            bins[v]++;\n        }\n        i++;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto val : x) {\n    if (val >= 0 && val < 10) {\n      bins[static_cast<size_t>(val)] += 1;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto const value = x[i];\n    if (0 <= value && value < 10)\n      ++bins[size_t(value)];\n  }\n}",
            "for (auto xi: x) {\n        if (xi >= 0 && xi <= 10)\n            ++bins[xi];\n    }\n}",
            "std::array<double, 10> bin_edges = {{ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90 }};\n    bins.fill(0);\n    for (auto x_val : x) {\n        for (size_t bin_idx = 0; bin_idx < 9; bin_idx++) {\n            if (x_val < bin_edges[bin_idx + 1]) {\n                bins[bin_idx]++;\n                break;\n            }\n        }\n    }\n}",
            "bins.fill(0);\n    for (auto &d: x) {\n        int index = static_cast<int>(d / 10);\n        ++bins[index];\n    }\n}",
            "}",
            "for (size_t i = 0; i < 10; i++) {\n    bins[i] = std::count_if(x.begin(), x.end(),\n                             [=](auto const& e) { return e >= i * 10 && e < (i + 1) * 10; });\n  }\n}",
            "for (auto const& i : x) {\n        bins[std::min((size_t)(i/10.0), 9)]++;\n    }\n}",
            "for (auto const &value : x) {\n        bins[std::min(static_cast<size_t>(std::floor(value / 10.0)), 9)] += 1;\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n\n    // This handles the special case of x=100.\n    if (value == 100.0) {\n      bins[9]++;\n    } else {\n      double value_mod_10 = value % 10;\n      size_t index = value_mod_10;\n      bins[index]++;\n    }\n  }\n}",
            "bins = {};\n    for (auto d : x) {\n        if (d < 10) {\n            ++bins[0];\n        } else if (d < 20) {\n            ++bins[1];\n        } else if (d < 30) {\n            ++bins[2];\n        } else if (d < 40) {\n            ++bins[3];\n        } else if (d < 50) {\n            ++bins[4];\n        } else if (d < 60) {\n            ++bins[5];\n        } else if (d < 70) {\n            ++bins[6];\n        } else if (d < 80) {\n            ++bins[7];\n        } else if (d < 90) {\n            ++bins[8];\n        } else if (d < 100) {\n            ++bins[9];\n        } else {\n            std::cerr << \"WARNING: binsBy10Count: Value \" << d << \" out of range.\" << std::endl;\n        }\n    }\n}",
            "assert(x.size() > 0);\n    bins = std::array<size_t, 10>{};\n\n    for (auto const& value : x) {\n        auto const index = std::floor(value / 10);\n        assert(index >= 0 && index < 10);\n        bins[index] += 1;\n    }\n}",
            "for(int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  for(int i = 0; i < x.size(); i++) {\n    if(x[i] < 10) {\n      bins[x[i]]++;\n    } else if(x[i] < 20) {\n      bins[10]++;\n    } else if(x[i] < 30) {\n      bins[20]++;\n    } else if(x[i] < 40) {\n      bins[30]++;\n    } else if(x[i] < 50) {\n      bins[40]++;\n    } else if(x[i] < 60) {\n      bins[50]++;\n    } else if(x[i] < 70) {\n      bins[60]++;\n    } else if(x[i] < 80) {\n      bins[70]++;\n    } else if(x[i] < 90) {\n      bins[80]++;\n    } else if(x[i] < 100) {\n      bins[90]++;\n    }\n  }\n}",
            "bins = std::array<size_t, 10>();\n  // Count the number of values in each bin\n  for (auto v : x) {\n    if (v < 10) {\n      bins[0]++;\n    } else if (v < 20) {\n      bins[1]++;\n    } else if (v < 30) {\n      bins[2]++;\n    } else if (v < 40) {\n      bins[3]++;\n    } else if (v < 50) {\n      bins[4]++;\n    } else if (v < 60) {\n      bins[5]++;\n    } else if (v < 70) {\n      bins[6]++;\n    } else if (v < 80) {\n      bins[7]++;\n    } else if (v < 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
            "for (auto v : x) {\n    if (v <= 10) {\n      bins[0]++;\n    } else if (v <= 20) {\n      bins[1]++;\n    } else if (v <= 30) {\n      bins[2]++;\n    } else if (v <= 40) {\n      bins[3]++;\n    } else if (v <= 50) {\n      bins[4]++;\n    } else if (v <= 60) {\n      bins[5]++;\n    } else if (v <= 70) {\n      bins[6]++;\n    } else if (v <= 80) {\n      bins[7]++;\n    } else if (v <= 90) {\n      bins[8]++;\n    } else {\n      bins[9]++;\n    }\n  }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] < 10)\n      ++bins[x[i]];\n  }\n}",
            "// your code here\n    size_t len = x.size();\n    std::array<size_t, 10> cnts{0};\n    std::vector<size_t> cnts_arr(10, 0);\n    for (size_t i = 0; i < len; ++i) {\n        ++cnts_arr[static_cast<size_t>(std::floor(x[i] / 10))];\n    }\n    std::copy(cnts_arr.begin(), cnts_arr.end(), cnts.begin());\n\n    // for (size_t i = 0; i < len; ++i) {\n    //     ++cnts[static_cast<size_t>(std::floor(x[i] / 10))];\n    // }\n    // std::copy(cnts.begin(), cnts.end(), cnts_arr.begin());\n\n    std::copy(cnts.begin(), cnts.end(), bins.begin());\n}",
            "bins.fill(0);\n  for(double v : x) {\n    if(v >= 0 && v < 10) {\n      ++bins[size_t(v)];\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    // Your code goes here.\n}",
            "for (auto& i : bins)\n        i = 0;\n\n    for (auto& i : x) {\n        int i10 = static_cast<int>(i / 10);\n        if (i10 < 10)\n            ++bins[i10];\n    }\n}",
            "// your code here\n  for (auto const& value : x) {\n    // your code here\n    // you must use (size_t)((value / 10.0) - 0.5) to get the index of the bin\n    // in bins.\n  }\n}",
            "for (auto xi : x) {\n      if (xi >= 10)\n         ++bins[10];\n      else if (xi >= 0)\n         ++bins[xi];\n   }\n}",
            "// TODO\n}",
            "// TODO: Your code goes here.\n}",
            "bins.fill(0);\n  for (auto value : x) {\n    if (value < 10) {\n      ++bins[0];\n    } else if (value < 20) {\n      ++bins[1];\n    } else if (value < 30) {\n      ++bins[2];\n    } else if (value < 40) {\n      ++bins[3];\n    } else if (value < 50) {\n      ++bins[4];\n    } else if (value < 60) {\n      ++bins[5];\n    } else if (value < 70) {\n      ++bins[6];\n    } else if (value < 80) {\n      ++bins[7];\n    } else if (value < 90) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] >= 0 && x[i] < 10) {\n            bins[std::floor(x[i])] += 1;\n        }\n    }\n}",
            "for (double i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n  for (double i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 10) {\n      bins[static_cast<size_t>(x[i])]++;\n    }\n  }\n}",
            "// 10 bins, so we need to find out the range of x\n  double min = x[0];\n  double max = x[0];\n  for(double xi : x) {\n    min = std::min(min, xi);\n    max = std::max(max, xi);\n  }\n\n  bins.fill(0);\n  for(double xi : x) {\n    int index = std::floor((xi - min) / 10.0);\n    bins[index]++;\n  }\n}",
            "size_t i=0;\n  for (i=0; i<10; i++) {\n    bins[i] = 0;\n  }\n\n  for (auto xx : x) {\n    if (xx < 10.0) {\n      bins[0] += 1;\n    } else if (xx >= 10.0 && xx < 20.0) {\n      bins[1] += 1;\n    } else if (xx >= 20.0 && xx < 30.0) {\n      bins[2] += 1;\n    } else if (xx >= 30.0 && xx < 40.0) {\n      bins[3] += 1;\n    } else if (xx >= 40.0 && xx < 50.0) {\n      bins[4] += 1;\n    } else if (xx >= 50.0 && xx < 60.0) {\n      bins[5] += 1;\n    } else if (xx >= 60.0 && xx < 70.0) {\n      bins[6] += 1;\n    } else if (xx >= 70.0 && xx < 80.0) {\n      bins[7] += 1;\n    } else if (xx >= 80.0 && xx < 90.0) {\n      bins[8] += 1;\n    } else if (xx >= 90.0 && xx < 100.0) {\n      bins[9] += 1;\n    }\n  }\n}",
            "for (double i : x) {\n    if (i < 10) {\n      bins[0]++;\n    } else if (i < 20) {\n      bins[1]++;\n    } else if (i < 30) {\n      bins[2]++;\n    } else if (i < 40) {\n      bins[3]++;\n    } else if (i < 50) {\n      bins[4]++;\n    } else if (i < 60) {\n      bins[5]++;\n    } else if (i < 70) {\n      bins[6]++;\n    } else if (i < 80) {\n      bins[7]++;\n    } else if (i < 90) {\n      bins[8]++;\n    } else if (i < 100) {\n      bins[9]++;\n    } else {\n      std::cout << \"Invalid input! Value > 100: \" << i << std::endl;\n      std::exit(-1);\n    }\n  }\n}",
            "for (auto& b : bins) b = 0;\n  for (auto xi : x) {\n    if (xi < 10) {\n      bins[static_cast<size_t>(xi)]++;\n    } else if (xi < 20) {\n      bins[1] += 1;\n    } else if (xi < 30) {\n      bins[2] += 1;\n    } else if (xi < 40) {\n      bins[3] += 1;\n    } else if (xi < 50) {\n      bins[4] += 1;\n    } else if (xi < 60) {\n      bins[5] += 1;\n    } else if (xi < 70) {\n      bins[6] += 1;\n    } else if (xi < 80) {\n      bins[7] += 1;\n    } else if (xi < 90) {\n      bins[8] += 1;\n    } else if (xi < 100) {\n      bins[9] += 1;\n    }\n  }\n}",
            "for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = std::count_if(x.cbegin(), x.cend(), [=](auto value) {\n      return value >= static_cast<double>(i * 10) && value < (i + 1) * 10;\n    });\n  }\n}",
            "// Fill bins with zeros.\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n  // Iterate through x.\n  for (size_t i = 0; i < x.size(); i++) {\n    // Add 1 to bin for this value.\n    size_t bin = x[i] / 10;\n    bins[bin]++;\n  }\n}",
            "size_t n = x.size();\n  bins = {0};\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] < 10)\n      ++bins[0];\n    else if (x[i] < 20)\n      ++bins[1];\n    else if (x[i] < 30)\n      ++bins[2];\n    else if (x[i] < 40)\n      ++bins[3];\n    else if (x[i] < 50)\n      ++bins[4];\n    else if (x[i] < 60)\n      ++bins[5];\n    else if (x[i] < 70)\n      ++bins[6];\n    else if (x[i] < 80)\n      ++bins[7];\n    else if (x[i] < 90)\n      ++bins[8];\n    else if (x[i] < 100)\n      ++bins[9];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for(double d : x) {\n    if(d < 0) {\n      throw std::invalid_argument(\"x contains negative values\");\n    }\n    else if(d <= 10) {\n      bins[0] += 1;\n    }\n    else if(d <= 20) {\n      bins[1] += 1;\n    }\n    else if(d <= 30) {\n      bins[2] += 1;\n    }\n    else if(d <= 40) {\n      bins[3] += 1;\n    }\n    else if(d <= 50) {\n      bins[4] += 1;\n    }\n    else if(d <= 60) {\n      bins[5] += 1;\n    }\n    else if(d <= 70) {\n      bins[6] += 1;\n    }\n    else if(d <= 80) {\n      bins[7] += 1;\n    }\n    else if(d <= 90) {\n      bins[8] += 1;\n    }\n    else if(d <= 100) {\n      bins[9] += 1;\n    }\n    else {\n      throw std::invalid_argument(\"x contains values outside [0,100)\");\n    }\n  }\n}",
            "// write your code here\n  bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  for(const auto &a : x){\n    bins[std::floor(a/10)] += 1;\n  }\n}",
            "for (double const &xi : x) {\n    int idx = std::floor((xi / 10));\n    if (idx >= 0 && idx < 10) {\n      bins[idx]++;\n    }\n  }\n}",
            "for (auto v : x) {\n    int i = std::min(std::max(static_cast<int>(v), 0), 100);\n    bins[i/10] += 1;\n  }\n}",
            "// TODO: Implement this function.\n}",
            "for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0 || x[i] > 100) {\n            continue;\n        }\n\n        size_t index = static_cast<size_t>(x[i] / 10);\n        bins[index] += 1;\n    }\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < x.size(); i++)\n        bins[size_t(x[i] / 10.0)]++;\n}",
            "// Initialize bins to 0.\n  bins = std::array<size_t, 10>{};\n\n  // Loop over the values in x.\n  for (auto const& val : x) {\n    if (val >= 0.0 && val < 10.0) {\n      bins[static_cast<size_t>(val)] += 1;\n    }\n  }\n\n}",
            "// Initialize the bins to zero\n  for (int i = 0; i < 10; ++i) {\n    bins[i] = 0;\n  }\n\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] < 10) {\n      bins[static_cast<int>(x[i])] += 1;\n    } else if (x[i] >= 10 && x[i] < 20) {\n      bins[static_cast<int>(x[i]) - 10] += 1;\n    } else {\n      bins[static_cast<int>(x[i]) / 10] += 1;\n    }\n  }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] < 0 || x[i] >= 100) {\n      throw std::invalid_argument(\"x[i] must be in [0, 100).\");\n    }\n\n    int index = (int)std::floor(x[i] / 10);\n    ++bins[index];\n  }\n}",
            "// TODO\n    for (auto &i : bins) i = 0;\n    for (auto i = x.cbegin(); i < x.cend(); ++i) {\n        size_t n = *i / 10;\n        ++bins[n];\n    }\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    // add the appropriate bin\n    size_t b = static_cast<size_t>(x[i] / 10);\n    bins[b] += 1;\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (size_t i = 0; i < x.size(); ++i) {\n    int index = x[i] / 10;\n    if (index < 0)\n      ++bins[0];\n    else if (index < 10)\n      ++bins[index];\n  }\n}",
            "for (auto const& value : x) {\n    if (value < 10) {\n      ++bins[0];\n    } else if (value < 20) {\n      ++bins[1];\n    } else if (value < 30) {\n      ++bins[2];\n    } else if (value < 40) {\n      ++bins[3];\n    } else if (value < 50) {\n      ++bins[4];\n    } else if (value < 60) {\n      ++bins[5];\n    } else if (value < 70) {\n      ++bins[6];\n    } else if (value < 80) {\n      ++bins[7];\n    } else if (value < 90) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "for (double val : x) {\n        bins[(val / 10.0)] += 1;\n    }\n}",
            "// your code here\n  for (size_t i = 0; i < x.size(); ++i) {\n    bins[(size_t)(x[i] / 10)]++;\n  }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 10) {\n      bins[9]++;\n    }\n    if (x[i] >= 20) {\n      bins[8]++;\n    }\n    if (x[i] >= 30) {\n      bins[7]++;\n    }\n    if (x[i] >= 40) {\n      bins[6]++;\n    }\n    if (x[i] >= 50) {\n      bins[5]++;\n    }\n    if (x[i] >= 60) {\n      bins[4]++;\n    }\n    if (x[i] >= 70) {\n      bins[3]++;\n    }\n    if (x[i] >= 80) {\n      bins[2]++;\n    }\n    if (x[i] >= 90) {\n      bins[1]++;\n    }\n    if (x[i] >= 100) {\n      bins[0]++;\n    }\n  }\n}",
            "// Your code goes here\n}",
            "// TODO: implement this function\n}",
            "// write your code here\n   size_t count = 0;\n   for (double i : x) {\n      if (i >= 0 && i < 10)\n         ++count;\n   }\n   bins[0] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 10 && i < 20)\n         ++count;\n   }\n   bins[1] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 20 && i < 30)\n         ++count;\n   }\n   bins[2] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 30 && i < 40)\n         ++count;\n   }\n   bins[3] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 40 && i < 50)\n         ++count;\n   }\n   bins[4] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 50 && i < 60)\n         ++count;\n   }\n   bins[5] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 60 && i < 70)\n         ++count;\n   }\n   bins[6] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 70 && i < 80)\n         ++count;\n   }\n   bins[7] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 80 && i < 90)\n         ++count;\n   }\n   bins[8] = count;\n   count = 0;\n   for (double i : x) {\n      if (i >= 90 && i < 100)\n         ++count;\n   }\n   bins[9] = count;\n}",
            "// YOUR CODE HERE\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] <= 10) {\n      bins[static_cast<size_t>(x[i])] += 1;\n    } else {\n      throw std::invalid_argument(\"x contains value outside of [0,10)\");\n    }\n  }\n}",
            "std::array<size_t, 10> localBins{};\n    std::transform(x.begin(), x.end(), localBins.begin(), [](double v) {\n        if (v >= 10) {\n            return 0;\n        }\n        if (v < 0) {\n            return 0;\n        }\n        return (size_t)std::ceil((v / 10.0));\n    });\n\n    std::for_each(bins.begin(), bins.end(), [&localBins](size_t &b) {\n        b += localBins[b];\n    });\n}",
            "// Your code goes here!\n}",
            "for (auto const &value : x) {\n\t\tif (value < 10) {\n\t\t\tbins[0]++;\n\t\t} else if (value < 20) {\n\t\t\tbins[1]++;\n\t\t} else if (value < 30) {\n\t\t\tbins[2]++;\n\t\t} else if (value < 40) {\n\t\t\tbins[3]++;\n\t\t} else if (value < 50) {\n\t\t\tbins[4]++;\n\t\t} else if (value < 60) {\n\t\t\tbins[5]++;\n\t\t} else if (value < 70) {\n\t\t\tbins[6]++;\n\t\t} else if (value < 80) {\n\t\t\tbins[7]++;\n\t\t} else if (value < 90) {\n\t\t\tbins[8]++;\n\t\t} else if (value < 100) {\n\t\t\tbins[9]++;\n\t\t}\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  for (auto const &i : x) {\n    if (i >= 0 && i < 10)\n      bins[static_cast<size_t>(i)]++;\n    else if (i >= 10 && i < 20)\n      bins[1]++;\n    else if (i >= 20 && i < 30)\n      bins[2]++;\n    else if (i >= 30 && i < 40)\n      bins[3]++;\n    else if (i >= 40 && i < 50)\n      bins[4]++;\n    else if (i >= 50 && i < 60)\n      bins[5]++;\n    else if (i >= 60 && i < 70)\n      bins[6]++;\n    else if (i >= 70 && i < 80)\n      bins[7]++;\n    else if (i >= 80 && i < 90)\n      bins[8]++;\n    else if (i >= 90 && i <= 100)\n      bins[9]++;\n  }\n}",
            "bins.fill(0);\n    for (size_t i = 0; i < x.size(); ++i)\n        ++bins[(size_t)std::floor(x[i] / 10)];\n}",
            "std::for_each(x.begin(), x.end(), [&](double xi){\n    size_t i = (size_t) std::floor(xi / 10);\n    bins[i]++;\n  });\n}",
            "// TODO: Your code goes here!\n    bins = std::array<size_t, 10> {};\n\n    for(int i = 0; i < x.size(); ++i) {\n        if(x[i] >= 0 && x[i] < 10) {\n            bins[x[i]]++;\n        } else if(x[i] >= 10 && x[i] < 20) {\n            bins[10]++;\n        } else if(x[i] >= 20 && x[i] < 30) {\n            bins[20]++;\n        } else if(x[i] >= 30 && x[i] < 40) {\n            bins[30]++;\n        } else if(x[i] >= 40 && x[i] < 50) {\n            bins[40]++;\n        } else if(x[i] >= 50 && x[i] < 60) {\n            bins[50]++;\n        } else if(x[i] >= 60 && x[i] < 70) {\n            bins[60]++;\n        } else if(x[i] >= 70 && x[i] < 80) {\n            bins[70]++;\n        } else if(x[i] >= 80 && x[i] < 90) {\n            bins[80]++;\n        } else if(x[i] >= 90 && x[i] < 100) {\n            bins[90]++;\n        }\n    }\n}",
            "for (size_t i=0; i < x.size(); i++) {\n    auto ix = static_cast<size_t>(x[i]/10);\n    bins[ix] += 1;\n  }\n}",
            "for (size_t i = 0; i < 10; i++) {\n    for (double value: x) {\n      if (value >= i * 10.0 && value < (i + 1) * 10.0) {\n        bins[i] += 1;\n      }\n    }\n  }\n}",
            "// your code here\n\n  return;\n}",
            "// ToDo\n}",
            "// TODO: Implement me!\n  bins.fill(0);\n  // Iterate through the array\n  for(double i : x) {\n    // If value is within range, add one to corresponding bin\n    if(i >= 0 && i < 10)\n      bins[static_cast<size_t>(i)] += 1;\n  }\n}",
            "bins.fill(0);\n    for (double value : x) {\n        size_t bin = (size_t)(value / 10);\n        bins[bin]++;\n    }\n}",
            "// YOUR CODE HERE\n  bins.fill(0);\n  for (auto i : x) {\n    int bin_idx = std::floor(i / 10);\n    bins[bin_idx]++;\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// Complete this function\n  // Hint: fill out the body of this function.\n}",
            "// Write your code here\n}",
            "auto const n = x.size();\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] < 0 || x[i] >= 100) {\n      continue;\n    }\n    size_t const bin = static_cast<size_t>(x[i] / 10);\n    ++bins[bin];\n  }\n}",
            "for (int i = 0; i < x.size(); i++) {\n    int index = (int)floor(x[i] / 10);\n    bins[index]++;\n  }\n}",
            "for (auto i : x) {\n    auto idx = std::floor(i / 10);\n    bins[idx]++;\n  }\n}",
            "bins.fill(0);\n  for (double d : x) {\n    if (d >= 0 && d < 10) {\n      ++bins[static_cast<size_t>(d)];\n    }\n  }\n}",
            "// add your code here\n}",
            "// Your code here\n  bins.fill(0);\n  for (auto const& x_i : x) {\n    if (x_i < 10) {\n      ++bins[0];\n    } else if (x_i < 20) {\n      ++bins[1];\n    } else if (x_i < 30) {\n      ++bins[2];\n    } else if (x_i < 40) {\n      ++bins[3];\n    } else if (x_i < 50) {\n      ++bins[4];\n    } else if (x_i < 60) {\n      ++bins[5];\n    } else if (x_i < 70) {\n      ++bins[6];\n    } else if (x_i < 80) {\n      ++bins[7];\n    } else if (x_i < 90) {\n      ++bins[8];\n    } else if (x_i < 100) {\n      ++bins[9];\n    }\n  }\n}",
            "bins.fill(0);\n    for (double xi: x) {\n        int bin = std::min(static_cast<int>(xi / 10), 9);\n        bins[bin]++;\n    }\n}",
            "for (double value : x) {\n\t\tif (value < 10.0) {\n\t\t\tbins[0] += 1;\n\t\t} else if (value < 20.0) {\n\t\t\tbins[1] += 1;\n\t\t} else if (value < 30.0) {\n\t\t\tbins[2] += 1;\n\t\t} else if (value < 40.0) {\n\t\t\tbins[3] += 1;\n\t\t} else if (value < 50.0) {\n\t\t\tbins[4] += 1;\n\t\t} else if (value < 60.0) {\n\t\t\tbins[5] += 1;\n\t\t} else if (value < 70.0) {\n\t\t\tbins[6] += 1;\n\t\t} else if (value < 80.0) {\n\t\t\tbins[7] += 1;\n\t\t} else if (value < 90.0) {\n\t\t\tbins[8] += 1;\n\t\t} else if (value < 100.0) {\n\t\t\tbins[9] += 1;\n\t\t}\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    for (auto const& n : x) {\n        if (0 <= n && n < 10) {\n            ++bins[static_cast<size_t>(n)];\n        } else if (10 <= n && n < 20) {\n            ++bins[10];\n        } else if (20 <= n && n < 30) {\n            ++bins[20];\n        } else if (30 <= n && n < 40) {\n            ++bins[30];\n        } else if (40 <= n && n < 50) {\n            ++bins[40];\n        } else if (50 <= n && n < 60) {\n            ++bins[50];\n        } else if (60 <= n && n < 70) {\n            ++bins[60];\n        } else if (70 <= n && n < 80) {\n            ++bins[70];\n        } else if (80 <= n && n < 90) {\n            ++bins[80];\n        } else if (90 <= n && n <= 100) {\n            ++bins[90];\n        }\n    }\n}",
            "for (auto const& val : x) {\n    if (val < 10.0) {\n      ++bins[0];\n    } else if (val < 20.0) {\n      ++bins[1];\n    } else if (val < 30.0) {\n      ++bins[2];\n    } else if (val < 40.0) {\n      ++bins[3];\n    } else if (val < 50.0) {\n      ++bins[4];\n    } else if (val < 60.0) {\n      ++bins[5];\n    } else if (val < 70.0) {\n      ++bins[6];\n    } else if (val < 80.0) {\n      ++bins[7];\n    } else if (val < 90.0) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "std::vector<double> const& X = x;\n  std::array<size_t, 10>& BINS = bins;\n  size_t i = 0;\n\n  size_t start = 0;\n  while (start < X.size() && X[start] < 10) {\n    ++start;\n  }\n  while (i < start) {\n    ++i;\n  }\n\n  while (i < X.size()) {\n    if (X[i] >= 10) {\n      break;\n    }\n    ++BINS[X[i]];\n    ++i;\n  }\n}",
            "// TODO: implement this function\n\n  // Note:\n  // - The `std::array<size_t, 10>` bins argument will be updated by this\n  //   function.\n}",
            "// WRITE YOUR CODE HERE\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto i : x) {\n    if (i < 0 || i >= 100) {\n      continue;\n    }\n\n    int value = i / 10;\n    ++bins[value];\n  }\n}",
            "if (x.empty()) return;\n  size_t i = 0;\n  double cur_bin_start = 0;\n  double cur_bin_end = 10;\n  while (i < x.size()) {\n    if (x[i] >= cur_bin_start && x[i] < cur_bin_end) {\n      bins[i / 10] += 1;\n      i += 1;\n      continue;\n    }\n    if (cur_bin_end < 100) {\n      cur_bin_start += 10;\n      cur_bin_end += 10;\n      continue;\n    }\n    break;\n  }\n}",
            "for (double value : x) {\n    size_t bin = static_cast<size_t>(value / 10.0);\n    if (bin < 10) {\n      ++bins[bin];\n    }\n  }\n}",
            "for(auto xi : x) {\n    double x_fraction = (xi / 100.0);\n    size_t x_integer = static_cast<size_t>(x_fraction);\n    bins[x_integer] += 1;\n  }\n}",
            "std::vector<double> v(x);\n  std::sort(v.begin(), v.end());\n\n  for (size_t i = 0; i < v.size() - 1; ++i) {\n    int bucket = int(v[i] / 10);\n    if (bucket == 9) {\n      bucket = 9;\n    }\n    ++bins[bucket];\n  }\n}",
            "for (double value : x) {\n      if (value >= 0 && value < 10) {\n         bins[static_cast<size_t>(value)]++;\n      }\n   }\n}",
            "for(auto i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  for(auto i = 0; i < x.size(); i++) {\n    if(x[i] >= 0 && x[i] < 10) {\n      bins[x[i]]++;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  for (auto const& xi : x) {\n    if (xi < 10) {\n      ++bins[0];\n    } else if (xi < 20) {\n      ++bins[1];\n    } else if (xi < 30) {\n      ++bins[2];\n    } else if (xi < 40) {\n      ++bins[3];\n    } else if (xi < 50) {\n      ++bins[4];\n    } else if (xi < 60) {\n      ++bins[5];\n    } else if (xi < 70) {\n      ++bins[6];\n    } else if (xi < 80) {\n      ++bins[7];\n    } else if (xi < 90) {\n      ++bins[8];\n    } else if (xi < 100) {\n      ++bins[9];\n    } else {\n      throw std::out_of_range(\"binBy10Count called with value outside [0,100)\");\n    }\n  }\n}",
            "/* Your code goes here. */\n  for (double val : x) {\n    if (val < 10) {\n      ++bins[0];\n    } else if (val < 20) {\n      ++bins[1];\n    } else if (val < 30) {\n      ++bins[2];\n    } else if (val < 40) {\n      ++bins[3];\n    } else if (val < 50) {\n      ++bins[4];\n    } else if (val < 60) {\n      ++bins[5];\n    } else if (val < 70) {\n      ++bins[6];\n    } else if (val < 80) {\n      ++bins[7];\n    } else if (val < 90) {\n      ++bins[8];\n    } else {\n      ++bins[9];\n    }\n  }\n}",
            "for (auto xi : x) {\n    if (xi >= 0 && xi < 10) {\n      bins[size_t(xi)] += 1;\n    } else if (xi >= 10 && xi < 20) {\n      bins[10] += 1;\n    } else if (xi >= 20 && xi < 30) {\n      bins[11] += 1;\n    } else if (xi >= 30 && xi < 40) {\n      bins[12] += 1;\n    } else if (xi >= 40 && xi < 50) {\n      bins[13] += 1;\n    } else if (xi >= 50 && xi < 60) {\n      bins[14] += 1;\n    } else if (xi >= 60 && xi < 70) {\n      bins[15] += 1;\n    } else if (xi >= 70 && xi < 80) {\n      bins[16] += 1;\n    } else if (xi >= 80 && xi < 90) {\n      bins[17] += 1;\n    } else if (xi >= 90 && xi < 100) {\n      bins[18] += 1;\n    } else {\n      std::cerr << \"binsBy10Count: Error: x value is not in [0,100].\" << std::endl;\n    }\n  }\n}",
            "// TODO: Implement\n  size_t i;\n  for(i=0;i<10;i++){\n    bins[i]=0;\n  }\n\n  for(i=0;i<x.size();i++){\n    if(x[i]<10){\n      bins[x[i]]++;\n    }\n  }\n}",
            "bins.fill(0);\n  for (auto const& elem : x) {\n    if (elem < 10) {\n      ++bins[0];\n    } else if (elem < 20) {\n      ++bins[1];\n    } else if (elem < 30) {\n      ++bins[2];\n    } else if (elem < 40) {\n      ++bins[3];\n    } else if (elem < 50) {\n      ++bins[4];\n    } else if (elem < 60) {\n      ++bins[5];\n    } else if (elem < 70) {\n      ++bins[6];\n    } else if (elem < 80) {\n      ++bins[7];\n    } else if (elem < 90) {\n      ++bins[8];\n    } else if (elem < 100) {\n      ++bins[9];\n    } else {\n      throw std::invalid_argument(\"binsBy10Count: Input vector has values outside [0, 100].\");\n    }\n  }\n}",
            "for (auto& bin : bins)\n    bin = 0;\n\n  for (auto value : x) {\n    auto bin = static_cast<int>(floor(value/10.0));\n    if (bin == 10)\n      bin = 9;\n\n    bins[bin]++;\n  }\n}",
            "std::array<size_t, 10> binsTemp;\n    bins.fill(0);\n\n    size_t n = x.size();\n    for (size_t i = 0; i < n; i++) {\n        binsTemp[x[i] / 10]++;\n    }\n\n    bins = binsTemp;\n}",
            "size_t n = x.size();\n  bins.fill(0);\n\n  for (size_t i = 0; i < n; ++i) {\n    size_t bin = (x[i] - 0) / 10;\n    if (bin < 10) {\n      ++bins[bin];\n    }\n  }\n}",
            "for (auto v: x) {\n    if (v >= 0 && v < 10) {\n      ++bins[v];\n    }\n  }\n}",
            "bins.fill(0);\n    for (double xi : x)\n        bins[std::floor(xi / 10.)]++;\n}",
            "for (auto xi : x) {\n    if (xi >= 0 && xi < 10) {\n      ++bins[static_cast<size_t>(xi)];\n    }\n  }\n}",
            "for (auto xi: x) {\n    bins[xi / 10]++;\n  }\n}",
            "for (size_t i=0; i<bins.size(); ++i) {\n    bins[i] = 0;\n  }\n  for (auto value : x) {\n    if (value < 10) {\n      ++bins[0];\n    } else if (value < 20) {\n      ++bins[1];\n    } else if (value < 30) {\n      ++bins[2];\n    } else if (value < 40) {\n      ++bins[3];\n    } else if (value < 50) {\n      ++bins[4];\n    } else if (value < 60) {\n      ++bins[5];\n    } else if (value < 70) {\n      ++bins[6];\n    } else if (value < 80) {\n      ++bins[7];\n    } else if (value < 90) {\n      ++bins[8];\n    } else if (value < 100) {\n      ++bins[9];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "hip",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t stride = gridDim.x * blockDim.x;\n  size_t numBlocks = (N + blockDim.x - 1) / blockDim.x;\n  while (tid < numBlocks) {\n    size_t idx = tid;\n    // compute the integer part of the value\n    int idx1 = (int)x[idx];\n    if (idx1 < 0) {\n      idx1 = 0;\n    }\n    if (idx1 >= 100) {\n      idx1 = 9;\n    }\n    atomicAdd(&bins[idx1], 1);\n    tid += stride;\n  }\n}",
            "int threadIdx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (threadIdx < N) {\n    bins[int((x[threadIdx] / 10) % 10)]++;\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t bid = tid % 10;\n  if (tid < N) {\n    size_t value = x[tid];\n    if (value < 100) {\n      atomicAdd(&bins[bid], 1);\n    }\n  }\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n  if(tid < N) {\n    int val = (int)(x[tid]*10);\n    if(val < 10) bins[val] += 1;\n  }\n}",
            "size_t thread_idx = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t block_idx = blockIdx.x;\n    if (thread_idx < N) {\n        size_t bin = (size_t) (x[thread_idx] / 10);\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "__shared__ int sharedBins[10];\n\n  // initialize shared memory with zeros\n  for (int i = threadIdx.x; i < 10; i += blockDim.x)\n    sharedBins[i] = 0;\n  __syncthreads();\n\n  // compute bin counts in parallel\n  int tid = threadIdx.x;\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    int bin = (int)floor((x[i] + 1) / 10);\n    sharedBins[bin]++;\n  }\n  __syncthreads();\n\n  // sum the values of each bin across all blocks\n  for (int i = threadIdx.x; i < 10; i += blockDim.x)\n    atomicAdd(&bins[i], sharedBins[i]);\n}",
            "// Each thread is responsible for a single value, so set the block size to the\n    // number of values\n    int blockSize = N;\n    // The block ID determines the value ID\n    int id = threadIdx.x + blockIdx.x * blockSize;\n    // Accumulate values in bins[]\n    if(id < N) {\n        bins[(int)x[id]/10]++;\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t numThreads = hipBlockDim_x;\n    size_t i = hipBlockIdx_x * numThreads + tid;\n    size_t count[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n    for (; i < N; i += numThreads * hipGridDim_x) {\n        int bin = (int) (10 * (x[i] - 0)) / 10;\n        count[bin] += 1;\n    }\n    for (int i = 0; i < 10; i++) {\n        atomicAdd(&bins[i], count[i]);\n    }\n}",
            "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t[10] local_bins;\n  for(int i=0; i<10; ++i) {\n    local_bins[i] = 0;\n  }\n  if(gid < N) {\n    int bin = (int)(x[gid] / 10);\n    atomicAdd(&local_bins[bin], 1);\n  }\n  __syncthreads();\n  for(int i=0; i<10; ++i) {\n    atomicAdd(&bins[i], local_bins[i]);\n  }\n}",
            "// Get my ID in the block\n  const size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    // Get my value in [0, 10]\n    const double v = 10 * x[tid];\n    // Add 1 to bin corresponding to my value\n    bins[static_cast<size_t>(v)] += 1;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t bin = i / 10;\n  if (i < N) {\n    if (x[i] >= bin * 10 && x[i] < (bin + 1) * 10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    // Determine the index of the bin, and add 1 to the count in that bin.\n    bins[(x[tid] / 10)]++;\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid < N) {\n        int v = static_cast<int>(x[tid]);\n        if (v < 10) ++bins[v];\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N)\n    return;\n  size_t bin = (size_t)(x[i] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "double value = x[blockDim.x * blockIdx.x + threadIdx.x];\n  size_t bin = value / 10;\n  atomicAdd(&bins[bin], 1);\n}",
            "__shared__ size_t localBins[10];\n\n  size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double value = x[tid];\n    int bin = (int)((value + 10.0) / 10.0);\n    atomicAdd(&localBins[bin], 1);\n  }\n\n  __syncthreads();\n\n  if (threadIdx.x < 10) {\n    atomicAdd(&bins[threadIdx.x], localBins[threadIdx.x]);\n  }\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t block = hipBlockIdx_x;\n  if (tid < N) {\n    int idx = int(x[tid] / 10);\n    if (idx == 10) {\n      ++idx;\n    }\n    ++bins[idx];\n  }\n}",
            "int tid = hipThreadIdx_x;\n  int bid = hipBlockIdx_x;\n  int NperBlock = blockDim.x;\n\n  // Loop over 10 bins.\n  for (int i = 0; i < 10; i++) {\n    // Get the start and end of each bin.\n    double start = i * 10.0;\n    double end = (i + 1) * 10.0;\n\n    // Count the values in the bin.\n    int n = 0;\n    for (size_t j = bid * NperBlock + tid; j < N; j += NperBlock) {\n      if (x[j] >= start && x[j] < end)\n        n++;\n    }\n\n    // Store the count in the bins array.\n    __syncthreads();\n    if (tid == 0)\n      bins[i] = n;\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x*blockIdx.x;\n    if (tid >= N) return;\n    // The number of threads in each block is initialized to the number of values\n    // in x. If N is not a multiple of the number of threads in the block, the\n    // last block needs to be initialized with the remaining number of values.\n    // For this, this thread needs to store its count in its last position.\n    size_t bin = (x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    int bin = 10 * ((int)(x[i] / 10));\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "__shared__ double s_x[10];\n    int t = threadIdx.x;\n    if(t < 10)\n        s_x[t] = 0;\n    __syncthreads();\n\n    for(int i = blockIdx.x * blockDim.x + t; i < N; i += blockDim.x * gridDim.x) {\n        double value = x[i];\n        if(value >= 0 && value < 10)\n            atomicAdd(&s_x[value], 1);\n    }\n    __syncthreads();\n\n    for(int t = 0; t < 10; t++)\n        atomicAdd(&bins[t], s_x[t]);\n}",
            "int index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (index >= N) return;\n    int bin = static_cast<int>(x[index] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "__shared__ size_t tbins[10];\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  tbins[0] = 0;\n  if (i < N) {\n    if (x[i] >= 0 && x[i] < 10) tbins[x[i]]++;\n  }\n  __syncthreads();\n\n  for (int s = 1; s < 10; s *= 2) {\n    if (threadIdx.x % (s * 2) == 0) {\n      size_t ai = threadIdx.x / (s * 2);\n      tbins[s + ai] += tbins[s * 2 + ai];\n    }\n    __syncthreads();\n  }\n  if (threadIdx.x < 10) bins[threadIdx.x] += tbins[threadIdx.x];\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    int bin = (int)floor(x[idx] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx < N) {\n        int bin = (int) (x[idx] / 10);\n        if (bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "size_t tid = threadIdx.x;\n  size_t gid = tid + blockIdx.x * blockDim.x;\n  // For all values x[i] in [0,10),\n  // bins[x[i] / 10] += 1;\n  // For all values x[i] in [10,20),\n  // bins[x[i] / 10] += 1;\n  //...\n  // For all values x[i] in [90,100),\n  // bins[x[i] / 10] += 1;\n  __syncthreads();\n  if (gid < N) {\n    size_t i = gid;\n    atomicAdd(&bins[x[i] / 10], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] < 10) {\n            bins[0]++;\n        } else if (x[tid] < 20) {\n            bins[1]++;\n        } else if (x[tid] < 30) {\n            bins[2]++;\n        } else if (x[tid] < 40) {\n            bins[3]++;\n        } else if (x[tid] < 50) {\n            bins[4]++;\n        } else if (x[tid] < 60) {\n            bins[5]++;\n        } else if (x[tid] < 70) {\n            bins[6]++;\n        } else if (x[tid] < 80) {\n            bins[7]++;\n        } else if (x[tid] < 90) {\n            bins[8]++;\n        } else if (x[tid] < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "const size_t tid = hipThreadIdx_x;\n  const size_t bin = tid / 10;\n  const size_t binOffset = tid % 10;\n  size_t count = 0;\n\n  if (tid < N) {\n    const size_t i = tid;\n\n    if (x[i] >= bin * 10 && x[i] < (bin + 1) * 10) {\n      count++;\n    }\n  }\n\n  atomicAdd(&bins[bin], count);\n}",
            "size_t i = hipThreadIdx_x;\n    if (i < N) {\n        int k = floor(x[i] / 10);\n        atomicAdd(&(bins[k]), 1);\n    }\n}",
            "size_t thread = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = gridDim.x * blockDim.x;\n\n    for (size_t i = thread; i < N; i += stride) {\n        int bin = (int) x[i] / 10;\n        if (bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "auto tid = hipThreadIdx_x;\n    auto bid = hipBlockIdx_x;\n    auto bx = bid * hipBlockDim_x;\n\n    __shared__ size_t sbins[10];\n\n    if (tid < 10) {\n        sbins[tid] = 0;\n    }\n    __syncthreads();\n\n    for (size_t i = bx + tid; i < N; i += hipBlockDim_x * hipGridDim_x) {\n        auto j = std::floor(x[i] / 10);\n        if (j < 10) {\n            atomicAdd(&sbins[j], 1);\n        }\n    }\n    __syncthreads();\n\n    for (size_t i = tid; i < 10; i += hipBlockDim_x) {\n        atomicAdd(&bins[i], sbins[i]);\n    }\n}",
            "size_t threadID = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t bin;\n    if (threadID < N) {\n        if (x[threadID] >= 0 && x[threadID] < 10) bin = 0;\n        else if (x[threadID] >= 10 && x[threadID] < 20) bin = 1;\n        else if (x[threadID] >= 20 && x[threadID] < 30) bin = 2;\n        else if (x[threadID] >= 30 && x[threadID] < 40) bin = 3;\n        else if (x[threadID] >= 40 && x[threadID] < 50) bin = 4;\n        else if (x[threadID] >= 50 && x[threadID] < 60) bin = 5;\n        else if (x[threadID] >= 60 && x[threadID] < 70) bin = 6;\n        else if (x[threadID] >= 70 && x[threadID] < 80) bin = 7;\n        else if (x[threadID] >= 80 && x[threadID] < 90) bin = 8;\n        else bin = 9;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t blockTotal = N / blockDim.x;\n\n  while (threadId < blockTotal) {\n    int digit = x[threadId] / 10;\n    atomicAdd(&bins[digit], 1);\n    threadId += blockDim.x * gridDim.x;\n  }\n}",
            "// Initialize shared memory\n   __shared__ double values[100];\n   if (threadIdx.x < N) {\n      values[threadIdx.x] = x[threadIdx.x];\n   }\n   __syncthreads();\n\n   // Only 10 threads do the computation\n   if (threadIdx.x < 10) {\n      // Determine the bin index (starting at 1)\n      int bin = (int)(threadIdx.x + 1);\n\n      // Compute the index in the array\n      size_t index = bin - 1;\n      // Count the number of values in [0, 10)\n      bins[index] = count(values, N, index * 10.0, (index + 1) * 10.0);\n   }\n}",
            "int i = hipThreadIdx_x;\n    bins[hipBlockIdx_x * 10 + i] = 0;\n    for (int j = i; j < N; j += hipBlockDim_x) {\n        bins[hipBlockIdx_x * 10 + i] += x[j] < (hipBlockIdx_x + 1) * 10;\n    }\n}",
            "double thid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (thid < N) {\n    size_t b = (size_t) x[thid] / 10;\n    atomicAdd(&bins[b], 1);\n  }\n}",
            "int tid = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    size_t bin = static_cast<size_t>((x[i] / 10));\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N)\n    return;\n\n  int bin = (int)floor(x[i] / 10.0);\n  atomicAdd(bins + bin, 1);\n}",
            "size_t thread_id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t stride = hipGridDim_x * hipBlockDim_x;\n    size_t i = thread_id;\n    while (i < N) {\n        int bin = (int) (x[i] / 10.0);\n        atomicAdd(&bins[bin], 1);\n        i += stride;\n    }\n}",
            "int idx = hipThreadIdx_x;\n  size_t bin = 0;\n  if (idx < N) {\n    bin = x[idx] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n  __syncthreads();\n}",
            "// each thread gets its own copy of the array of bins\n    size_t binsLocal[10];\n    for(int i = 0; i < 10; i++) binsLocal[i] = 0;\n\n    // each thread processes a different entry in the array\n    for (size_t i = 0; i < N; i++) {\n        if (x[i] < 10.0)\n            atomicAdd(&binsLocal[0], 1);\n        else if (x[i] < 20.0)\n            atomicAdd(&binsLocal[1], 1);\n        else if (x[i] < 30.0)\n            atomicAdd(&binsLocal[2], 1);\n        else if (x[i] < 40.0)\n            atomicAdd(&binsLocal[3], 1);\n        else if (x[i] < 50.0)\n            atomicAdd(&binsLocal[4], 1);\n        else if (x[i] < 60.0)\n            atomicAdd(&binsLocal[5], 1);\n        else if (x[i] < 70.0)\n            atomicAdd(&binsLocal[6], 1);\n        else if (x[i] < 80.0)\n            atomicAdd(&binsLocal[7], 1);\n        else if (x[i] < 90.0)\n            atomicAdd(&binsLocal[8], 1);\n        else\n            atomicAdd(&binsLocal[9], 1);\n    }\n\n    // each thread puts its results into the global array\n    for (int i = 0; i < 10; i++) {\n        atomicAdd(&bins[i], binsLocal[i]);\n    }\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n\n    // Get the start/end of the data to process.\n    size_t start = bid * N / gridDim.x;\n    size_t end = (bid + 1) * N / gridDim.x;\n\n    // Number of values in [start, end)\n    size_t count = end - start;\n\n    // Number of bins in [0,10)\n    size_t nbins = 10;\n\n    // Accumulator\n    __shared__ size_t sums[10];\n\n    for (size_t j = 0; j < nbins; j++) {\n        sums[j] = 0;\n    }\n    for (size_t i = start + tid; i < end; i += blockDim.x) {\n        size_t bin = (size_t)(x[i] / 10);\n        if (bin < nbins) {\n            sums[bin]++;\n        }\n    }\n    for (size_t j = 0; j < nbins; j++) {\n        atomicAdd(&bins[j], sums[j]);\n    }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    int bin = (int) floor(x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int index = x[tid] / 10.0;\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  double value = x[thread_id];\n  size_t bin = 10*floor(value / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = blockDim.x*blockIdx.x + threadIdx.x;\n  size_t block_size = blockDim.x*gridDim.x;\n  size_t start = tid;\n  size_t end = N;\n  size_t count = 0;\n  while (start < end) {\n    count += (x[start] < 10);\n    start += block_size;\n  }\n  __syncthreads();\n  atomicAdd(&bins[0], count);\n  __syncthreads();\n  atomicAdd(&bins[1], count);\n  __syncthreads();\n  atomicAdd(&bins[2], count);\n  __syncthreads();\n  atomicAdd(&bins[3], count);\n  __syncthreads();\n  atomicAdd(&bins[4], count);\n  __syncthreads();\n  atomicAdd(&bins[5], count);\n  __syncthreads();\n  atomicAdd(&bins[6], count);\n  __syncthreads();\n  atomicAdd(&bins[7], count);\n  __syncthreads();\n  atomicAdd(&bins[8], count);\n  __syncthreads();\n  atomicAdd(&bins[9], count);\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t gid = hipBlockIdx_x * hipBlockDim_x + tid;\n    if(gid >= N) return;\n\n    size_t bin = (size_t) floor(x[gid] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "// Initialize threadId with the number of threads in block\n  unsigned int threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  unsigned int bin = threadId / 10;\n\n  if (threadId < N) {\n    if (x[threadId] < (bin+1)*10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t threadCount = blockDim.x * gridDim.x;\n    size_t nBins = 10;\n    while (i < N) {\n        bins[__float2uint_rd(x[i]/10.0)] += 1;\n        i += threadCount;\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (tid >= N) return;\n\n   double f = x[tid] / 10;\n   int i = f;\n   if (f - i >= 0.5) i++;\n   bins[i]++;\n}",
            "int tId = threadIdx.x;\n   __shared__ size_t smem[10];\n\n   // Initialize the shared memory with zeros\n   if (tId < 10) {\n      smem[tId] = 0;\n   }\n   __syncthreads();\n\n   for (size_t i = tId; i < N; i += blockDim.x) {\n      if (x[i] >= 0 && x[i] < 10) {\n         atomicAdd(&smem[x[i]], 1);\n      }\n   }\n   __syncthreads();\n\n   // Finalize the results\n   if (tId < 10) {\n      atomicAdd(&bins[tId], smem[tId]);\n   }\n}",
            "size_t gid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    if (gid < N) {\n        size_t idx = (size_t)(x[gid] / 10);\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "int t = threadIdx.x + blockDim.x * blockIdx.x;\n    int c = threadIdx.x;\n    while (t < N) {\n        bins[int((x[t] / 10.0) + 0.1)] += 1;\n        t += gridDim.x * blockDim.x;\n    }\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = gridDim.x * blockDim.x;\n    binsBy10Count(threadId, stride, x, N, bins);\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    if (index >= N) {\n        return;\n    }\n    size_t bin = size_t(x[index] / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "__shared__ double local[100];\n  // fill up the local array, each thread writes one value\n  local[threadIdx.x] = 0;\n  for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n    if (x[i] >= threadIdx.x * 10 && x[i] < (threadIdx.x + 1) * 10)\n      ++local[threadIdx.x];\n  }\n  // blockDim.x == 100 in this example, so there are 10 blocks and each thread\n  // in each block writes 10 values\n  __syncthreads();\n  // each block sums up the values that were written by each thread and writes\n  // the result in bins[]\n  for (int i = threadIdx.x; i < 10; i += blockDim.x)\n    bins[i] += local[i];\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        double val = x[tid];\n        if (val < 10) {\n            atomicAdd(bins + 0, 1);\n        } else if (val < 20) {\n            atomicAdd(bins + 1, 1);\n        } else if (val < 30) {\n            atomicAdd(bins + 2, 1);\n        } else if (val < 40) {\n            atomicAdd(bins + 3, 1);\n        } else if (val < 50) {\n            atomicAdd(bins + 4, 1);\n        } else if (val < 60) {\n            atomicAdd(bins + 5, 1);\n        } else if (val < 70) {\n            atomicAdd(bins + 6, 1);\n        } else if (val < 80) {\n            atomicAdd(bins + 7, 1);\n        } else if (val < 90) {\n            atomicAdd(bins + 8, 1);\n        } else if (val < 100) {\n            atomicAdd(bins + 9, 1);\n        }\n    }\n}",
            "for (int i = 0; i < N; i++) {\n    atomicAdd(&bins[static_cast<size_t>((x[i] / 10) + 0.5)], 1);\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    size_t bin = x[tid] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = threadId; i < N; i += stride) {\n    int bin = (int) (x[i] / 10);\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t bin = tid % 10;\n  bins[bin] += tid < N && x[tid] >= bin * 10.0 && x[tid] < (bin + 1) * 10.0;\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (i < N) {\n    size_t bin = (size_t) (x[i] / 10);\n    if (bin < 10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n    if (thread_id < N) {\n        int n = x[thread_id] / 10;\n        if (n < 10)\n            atomicAdd(bins + n, 1);\n    }\n}",
            "__shared__ double x_shared[blockDim.x];\n  size_t n_shared = 0;\n  for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n    x_shared[threadIdx.x] = x[i];\n    __syncthreads();\n\n    int bin = threadIdx.x / 10;\n    if (x_shared[threadIdx.x] >= bin * 10 && x_shared[threadIdx.x] < (bin + 1) * 10) {\n      n_shared++;\n    }\n    __syncthreads();\n  }\n  atomicAdd(&bins[threadIdx.x / 10], n_shared);\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  double y = x[tid];\n  if (y < 10) {\n    atomicAdd(&bins[0], 1);\n  } else if (y < 20) {\n    atomicAdd(&bins[1], 1);\n  } else if (y < 30) {\n    atomicAdd(&bins[2], 1);\n  } else if (y < 40) {\n    atomicAdd(&bins[3], 1);\n  } else if (y < 50) {\n    atomicAdd(&bins[4], 1);\n  } else if (y < 60) {\n    atomicAdd(&bins[5], 1);\n  } else if (y < 70) {\n    atomicAdd(&bins[6], 1);\n  } else if (y < 80) {\n    atomicAdd(&bins[7], 1);\n  } else if (y < 90) {\n    atomicAdd(&bins[8], 1);\n  } else {\n    atomicAdd(&bins[9], 1);\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        int bin = (int)(x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "}",
            "const size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  if (threadId < N) {\n    bins[x[threadId] / 10] += 1;\n  }\n}",
            "__shared__ double xs[1024];\n    const size_t tid = threadIdx.x;\n    xs[tid] = x[tid];\n    __syncthreads();\n    const int block_start = blockIdx.x * blockDim.x;\n    const int block_end = (blockIdx.x + 1) * blockDim.x;\n\n    for (size_t i = block_start + tid; i < N; i += blockDim.x * gridDim.x) {\n        if (xs[i] < 10)\n            bins[0]++;\n        else if (xs[i] < 20)\n            bins[1]++;\n        else if (xs[i] < 30)\n            bins[2]++;\n        else if (xs[i] < 40)\n            bins[3]++;\n        else if (xs[i] < 50)\n            bins[4]++;\n        else if (xs[i] < 60)\n            bins[5]++;\n        else if (xs[i] < 70)\n            bins[6]++;\n        else if (xs[i] < 80)\n            bins[7]++;\n        else if (xs[i] < 90)\n            bins[8]++;\n        else\n            bins[9]++;\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    bins[floor((x[idx] + 10) / 10)] += 1;\n  }\n}",
            "__shared__ double values[THREADS];\n  if (threadIdx.x < N) values[threadIdx.x] = x[threadIdx.x];\n  __syncthreads();\n  const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int N_threads = blockDim.x * gridDim.x;\n  for (int i = tid; i < N; i += N_threads) {\n    const int k = __double2int_rn(values[i] * 0.1) + 1;\n    atomicAdd(&bins[k], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double v = x[tid];\n    atomicAdd(bins + ((int)v/10), 1);\n  }\n}",
            "// HIP will divide this kernel among 2 blocks with 16 threads each\n    // The kernel will also run with 16 threads (total threads = 32)\n    // Thread 0 in block 0 will run with (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)\n    // Thread 1 in block 0 will run with (16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31)\n    // Thread 0 in block 1 will run with (32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47)\n    // Thread 1 in block 1 will run with (48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63)\n    //\n    // Thread 0 in block 0 will use data from x[0..15]\n    // Thread 1 in block 0 will use data from x[16..31]\n    // Thread 0 in block 1 will use data from x[32..47]\n    // Thread 1 in block 1 will use data from x[48..63]\n    //\n    // Thread 0 in block 0 will use data from bins[0..9]\n    // Thread 1 in block 0 will use data from bins[10..19]\n    // Thread 0 in block 1 will use data from bins[20..29]\n    // Thread 1 in block 1 will use data from bins[30..39]\n\n    int myId = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    int myStart = myId * N / 10;\n    int myEnd = (myId + 1) * N / 10;\n\n    double value = x[myId];\n    if (value >= 0 && value < 10) {\n        atomicAdd(&bins[value], 1);\n    }\n}",
            "__shared__ double sdata[BLOCKSIZE];\n  __shared__ int bin_cnt[10];\n  const int threadId = threadIdx.x;\n  const int blockId = blockIdx.x;\n  const int gridSize = gridDim.x;\n  const int blockSize = blockDim.x;\n  double start = blockId * (N - 1) / gridSize + 1;\n  double end = (blockId + 1) * (N - 1) / gridSize + 1;\n  if (threadId == 0) {\n    for (int i = 0; i < 10; i++)\n      bin_cnt[i] = 0;\n  }\n  __syncthreads();\n  for (size_t i = threadId; i < N; i += blockSize) {\n    int bin = (int)((x[i] - start) / (end - start) * 10);\n    if (bin < 0)\n      bin = 0;\n    if (bin >= 10)\n      bin = 9;\n    atomicAdd(&bin_cnt[bin], 1);\n  }\n  __syncthreads();\n  for (int s = blockSize / 2; s > 0; s >>= 1) {\n    if (threadId < s) {\n      for (int i = 0; i < 10; i++) {\n        bin_cnt[i] += bin_cnt[i + s];\n      }\n    }\n    __syncthreads();\n  }\n  if (threadId < 10)\n    bins[threadId] = bin_cnt[threadId];\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int val = floor(x[i] / 10);\n    atomicAdd(&bins[val], 1);\n  }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n   double val = x[idx];\n   int val10 = 10 * (val / 10);\n   int val100 = val10 + 10;\n   if (val10 == val)\n      atomicAdd(&bins[0], 1);\n   else if (val < 10)\n      atomicAdd(&bins[val], 1);\n   else if (val10 < 100)\n      atomicAdd(&bins[val100 - val10], 1);\n   else\n      atomicAdd(&bins[9], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        if (x[tid] < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[tid] < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[tid] < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (x[tid] < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (x[tid] < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (x[tid] < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (x[tid] < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (x[tid] < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (x[tid] < 90) {\n            atomicAdd(&bins[8], 1);\n        } else {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N)\n        return;\n    size_t idx = (size_t)(x[tid] / 10);\n    atomicAdd(&(bins[idx]), 1);\n}",
            "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n\n    for (; gid < N; gid += stride) {\n        double y = x[gid];\n        int i = (int)(y / 10);\n        if (i == 10) i = 9;\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t bin = tid % 10;\n    if (tid < N) {\n        bins[bin] += ((x[tid] < 10.0)? 1 : 0);\n    }\n}",
            "unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  unsigned int i = tid * 10;\n\n  if (i < N) {\n    for (int j = 0; j < 10; j++) {\n      if (x[i + j] >= j * 10 && x[i + j] < (j + 1) * 10)\n        atomicAdd(&(bins[j]), 1);\n    }\n  }\n}",
            "// thread ID\n  size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid >= N) return;\n\n  // first index of the 10-bin range\n  int bin = static_cast<int>(x[tid] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "// Each thread handles one element\n  int i = threadIdx.x;\n  double start = 0.0;\n  double end = 100.0;\n  int count = 0;\n\n  // Use `atomicAdd` to add to the counter at index `i`\n  // each time the condition is satisfied\n  if (i < N && x[i] >= start && x[i] < end) {\n    atomicAdd(&bins[i / 10], 1);\n  }\n}",
            "__shared__ double x_shared[2048]; // 2K\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    double sum = 0;\n    for (int i = tid; i < N; i += stride) {\n        sum += x[i] < 10.0;\n    }\n    x_shared[threadIdx.x] = sum;\n    __syncthreads();\n    if (threadIdx.x < 10) {\n        for (int i = 1; i < blockDim.x; i *= 2) {\n            x_shared[threadIdx.x] += x_shared[threadIdx.x + i];\n        }\n        bins[threadIdx.x] = x_shared[threadIdx.x];\n    }\n}",
            "size_t myid = threadIdx.x;\n    double myval = x[myid];\n    int j = 0;\n    while (j < 10) {\n        if (myval < j * 10) {\n            atomicAdd(&bins[j], 1);\n            break;\n        }\n        j++;\n    }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  size_t nthreads = blockDim.x * gridDim.x;\n  for (size_t i = tid; i < N; i += nthreads)\n    atomicAdd(&bins[(size_t)(x[i] / 10)], 1);\n}",
            "double value = x[hipBlockIdx_x];\n  size_t idx = (size_t)(value / 10);\n  atomicAdd(&bins[idx], 1);\n}",
            "// get the id of the thread within the block\n    size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // if id is not larger than N, the loop will execute N times\n    for (size_t i = id; i < N; i += gridDim.x * blockDim.x) {\n        double v = x[i];\n        size_t bin = (size_t)floor(v / 10);\n        bins[bin] += 1;\n    }\n}",
            "size_t id = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t i = id / 10;\n  size_t j = id % 10;\n  if (i < N) {\n    size_t k = floor(x[i] / 10) - j;\n    if (k >= 0) {\n      atomicAdd(&(bins[k]), 1);\n    }\n  }\n}",
            "// This example uses 10 bins\n  const size_t NUM_BINS = 10;\n  // Each thread will compute one bin\n  size_t bin = threadIdx.x % NUM_BINS;\n  // Each thread will compute one value\n  size_t i = threadIdx.x / NUM_BINS;\n  if (i >= N) return;\n  // Bin all values less than or equal to 10 * bin\n  size_t count = (bin == 0)? 1 + (x[i] < 10.0) : 0;\n  // Bin all values greater than 10 * bin, but less than 10 * (bin + 1)\n  count += (bin == 9)? 1 + (x[i] >= 90.0) : 0;\n  // Update the count\n  atomicAdd(&bins[bin], count);\n}",
            "__shared__ size_t bins_shared[10];\n\n  int i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i >= N) {\n    return;\n  }\n  bins_shared[int(x[i]) / 10] += 1;\n  __syncthreads();\n  for (int j = 0; j < 10; j++) {\n    atomicAdd(&bins[j], bins_shared[j]);\n  }\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n  if (threadId < N) {\n    int val = (int)x[threadId];\n    if (val < 10) bins[val]++;\n  }\n}",
            "unsigned int tid = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  double value = x[tid];\n  // Initialize `bins` to zeros\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  if (value < 10) {\n    atomicAdd(&bins[0], 1);\n  } else if (value < 20) {\n    atomicAdd(&bins[1], 1);\n  } else if (value < 30) {\n    atomicAdd(&bins[2], 1);\n  } else if (value < 40) {\n    atomicAdd(&bins[3], 1);\n  } else if (value < 50) {\n    atomicAdd(&bins[4], 1);\n  } else if (value < 60) {\n    atomicAdd(&bins[5], 1);\n  } else if (value < 70) {\n    atomicAdd(&bins[6], 1);\n  } else if (value < 80) {\n    atomicAdd(&bins[7], 1);\n  } else if (value < 90) {\n    atomicAdd(&bins[8], 1);\n  } else {\n    atomicAdd(&bins[9], 1);\n  }\n}",
            "size_t start = blockIdx.x * blockDim.x;\n  size_t end = (blockIdx.x + 1) * blockDim.x;\n  size_t i = threadIdx.x;\n\n  size_t bin = start / 10;\n  size_t count = 0;\n\n  while (start < end && bin < 10) {\n    if (x[start] >= start && x[start] < start + 10) {\n      count++;\n    }\n    start += blockDim.x * gridDim.x;\n    bin++;\n  }\n  if (i == 0) {\n    atomicAdd(&bins[bin], count);\n  }\n}",
            "// the kernel is initialized with at least as many threads as values in x\n  // the value below is the max number of threads supported by the HIP\n  // runtime for HIP devices\n  size_t maxThreads = 256;\n  size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t i = threadId / maxThreads * maxThreads;\n  while (i < N) {\n    double v = x[i];\n    int bin = int((v / 10.0) + 0.5);\n    atomicAdd(&bins[bin], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "int tid = threadIdx.x;\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n\n  double cur = 0;\n  for (int i = idx; i < N; i += stride) {\n    cur = (cur * 10) + x[i];\n    int bin = (int)cur;\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // Compute the count for [0,10), [10, 20),...\n  if (gid < N && x[gid] < 10.0) {\n    atomicAdd(&bins[static_cast<size_t>(x[gid])], 1);\n  }\n}",
            "int idx = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  int bin = idx / 10;\n  if (idx < N) {\n    bins[bin] += (x[idx] < bin + 1) * 1;\n  }\n}",
            "unsigned int index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (index >= N) return;\n    unsigned int bin = (unsigned int)floor((x[index] / 10));\n    atomicAdd(&bins[bin], 1);\n}",
            "#ifdef __HIPCC__\n  // TODO: use __HIP_DEVICE_COMPILE__ and conditionally add this to __global__\n  //__shared__ double x_shared[MAX_N];\n  //__syncthreads();\n  //size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  //if (tid < N) {\n  //  x_shared[tid] = x[tid];\n  //}\n  //__syncthreads();\n  //if (tid < N) {\n  //  int idx = (int)(x_shared[tid] / 10.0);\n  //  atomicAdd(&bins[idx], 1);\n  //}\n  //__syncthreads();\n#else\n  size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int idx = (int)(x[tid] / 10.0);\n    atomicAdd(&bins[idx], 1);\n  }\n#endif\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx >= N)\n        return;\n    bins[static_cast<int>(x[idx]/10)] += 1;\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N)\n        return;\n    int index = floor(x[i] / 10);\n    atomicAdd(&bins[index], 1);\n}",
            "__shared__ double x_shared[500];\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  x_shared[threadIdx.x] = idx < N? x[idx] : 0;\n  __syncthreads();\n  size_t bin_idx = (x_shared[threadIdx.x] * 10) / 100;\n  atomicAdd(&bins[bin_idx], 1);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid >= N)\n    return;\n\n  double value = x[tid];\n  if (value >= 0 && value < 10)\n    atomicAdd(&bins[0], 1);\n  else if (value >= 10 && value < 20)\n    atomicAdd(&bins[1], 1);\n  else if (value >= 20 && value < 30)\n    atomicAdd(&bins[2], 1);\n  else if (value >= 30 && value < 40)\n    atomicAdd(&bins[3], 1);\n  else if (value >= 40 && value < 50)\n    atomicAdd(&bins[4], 1);\n  else if (value >= 50 && value < 60)\n    atomicAdd(&bins[5], 1);\n  else if (value >= 60 && value < 70)\n    atomicAdd(&bins[6], 1);\n  else if (value >= 70 && value < 80)\n    atomicAdd(&bins[7], 1);\n  else if (value >= 80 && value < 90)\n    atomicAdd(&bins[8], 1);\n  else if (value >= 90 && value < 100)\n    atomicAdd(&bins[9], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t step = gridDim.x * blockDim.x;\n  for (size_t i = tid; i < N; i += step) {\n    if (x[i] < 10) {\n      bins[0] += 1;\n    } else if (x[i] < 20) {\n      bins[1] += 1;\n    } else if (x[i] < 30) {\n      bins[2] += 1;\n    } else if (x[i] < 40) {\n      bins[3] += 1;\n    } else if (x[i] < 50) {\n      bins[4] += 1;\n    } else if (x[i] < 60) {\n      bins[5] += 1;\n    } else if (x[i] < 70) {\n      bins[6] += 1;\n    } else if (x[i] < 80) {\n      bins[7] += 1;\n    } else if (x[i] < 90) {\n      bins[8] += 1;\n    } else {\n      bins[9] += 1;\n    }\n  }\n}",
            "__shared__ size_t buffer[10];\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t value = (i < N)? static_cast<size_t>(x[i] / 10) : 0;\n  if (i < N) {\n    atomicAdd(&buffer[value], 1);\n  }\n  __syncthreads();\n  atomicAdd(&bins[value], buffer[value]);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = gridDim.x * blockDim.x;\n\n    // Each thread processes a bin of 10 values\n    for (size_t i = tid; i < N; i += stride) {\n        // If the value is greater than or equal to 10, the remainder\n        // modulo 10 yields the index to update in the `bins` array\n        if (x[i] >= 10) {\n            bins[x[i] % 10]++;\n        }\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (i < N) {\n    size_t n = x[i] / 10.0;\n    atomicAdd(&bins[n], 1);\n  }\n}",
            "int tid = threadIdx.x;\n   int bin = tid / 10;\n   double value = x[tid];\n   bins[bin] += value < 10 * bin && value >= 10 * (bin + 1)? 1 : 0;\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    if (x[tid] < 10.0) {\n      atomicAdd(&bins[0], 1);\n    }\n    if (x[tid] >= 10.0 && x[tid] < 20.0) {\n      atomicAdd(&bins[1], 1);\n    }\n    if (x[tid] >= 20.0 && x[tid] < 30.0) {\n      atomicAdd(&bins[2], 1);\n    }\n    if (x[tid] >= 30.0 && x[tid] < 40.0) {\n      atomicAdd(&bins[3], 1);\n    }\n    if (x[tid] >= 40.0 && x[tid] < 50.0) {\n      atomicAdd(&bins[4], 1);\n    }\n    if (x[tid] >= 50.0 && x[tid] < 60.0) {\n      atomicAdd(&bins[5], 1);\n    }\n    if (x[tid] >= 60.0 && x[tid] < 70.0) {\n      atomicAdd(&bins[6], 1);\n    }\n    if (x[tid] >= 70.0 && x[tid] < 80.0) {\n      atomicAdd(&bins[7], 1);\n    }\n    if (x[tid] >= 80.0 && x[tid] < 90.0) {\n      atomicAdd(&bins[8], 1);\n    }\n    if (x[tid] >= 90.0 && x[tid] < 100.0) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "double value = x[blockIdx.x];\n  size_t bin = (size_t)floor(value / 10);\n  if (bin < 10) {\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int id = threadIdx.x + blockIdx.x * blockDim.x;\n  if (id < N) {\n    int value = (int)floor(x[id] / 10);\n    atomicAdd(&bins[value], 1);\n  }\n}",
            "size_t binId = threadIdx.x;\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (; idx < N; idx += stride)\n    if (x[idx] >= binId * 10.0 && x[idx] < (binId + 1) * 10.0)\n      atomicAdd(&bins[binId], 1);\n}",
            "size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  size_t bsize = blockDim.x;\n\n  __shared__ double sdata[100];\n\n  int binId = bid * 10;\n  sdata[tid] = 0.0;\n\n  for (int j = tid; j < N; j += bsize) {\n    sdata[tid] += (x[j] >= binId) & (x[j] < binId + 10);\n  }\n\n  // reduction in shared mem\n  for (int stride = bsize / 2; stride > 0; stride /= 2) {\n    __syncthreads();\n    if (tid < stride) {\n      sdata[tid] += sdata[tid + stride];\n    }\n  }\n\n  // write result for this block to global mem\n  if (tid == 0) {\n    bins[bid] = (size_t)(sdata[0]);\n  }\n}",
            "int idx = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (idx < N) {\n      bins[int(x[idx] / 10)]++;\n   }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n    size_t gid = tid;\n\n    __shared__ size_t sum[10];\n    if (tid < 10) {\n        sum[tid] = 0;\n    }\n    __syncthreads();\n\n    while (gid < N) {\n        double xi = x[gid];\n        size_t index = (size_t)(xi / 10);\n        if (index < 10) {\n            atomicAdd(sum + index, 1);\n        }\n        gid += hipBlockDim_x * hipGridDim_x;\n    }\n    __syncthreads();\n    if (tid < 10) {\n        atomicAdd(bins + tid, sum[tid]);\n    }\n}",
            "const int threadid = threadIdx.x;\n  const int blockid = blockIdx.x;\n  const int numblocks = gridDim.x;\n  const size_t blocksize = blockDim.x;\n\n  // Each block is responsible for a range of values.\n  const size_t start = (N * blockid) / numblocks;\n  const size_t end = (N * (blockid + 1)) / numblocks;\n\n  // `start` and `end` are the values to compute for this block.\n  double *blockbins = &bins[0];\n  int tid = threadid;\n  for (size_t i = start + tid; i < end; i += blocksize) {\n    const double value = x[i];\n    // Each thread performs one bin.\n    int bin = value / 10;\n    atomicAdd(&blockbins[bin], 1);\n  }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const double bin_width = 10;\n    const double bin_offset = 0;\n    if (tid < N) {\n        int bin = (x[tid] - bin_offset) / bin_width;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "__shared__ double smem[100];\n    size_t tid = threadIdx.x;\n    size_t block = blockIdx.x;\n    size_t i = block * blockDim.x + tid;\n    size_t stride = blockDim.x * gridDim.x;\n    // The number of values in [0,10)\n    size_t b = 0;\n    if (i < N) {\n        // Each thread loads one value from the array\n        double y = x[i];\n        // Each thread stores the bin index of `y` in smem\n        size_t bin = (size_t)y / 10;\n        smem[tid] = bin;\n        // Each thread atomically increments the bin count by one\n        b = atomicAdd(&bins[bin], 1);\n    }\n    // The last thread in the block atomically increments the bin counts by the number of threads in the block\n    if (tid == 0) {\n        atomicAdd(&bins[10], b);\n    }\n    // Wait for all threads in the block to complete\n    __syncthreads();\n    // Each thread increments the bin counts by the total number of threads in the previous blocks\n    if (tid == 0) {\n        size_t sum = 0;\n        for (int t = 0; t < blockDim.x; t++) {\n            sum += smem[t];\n        }\n        atomicAdd(&bins[10], sum);\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (tid < N) {\n    size_t binIdx = (x[tid] / 10);\n    atomicAdd(&bins[binIdx], 1);\n  }\n}",
            "const int nthreads = hipBlockDim_x * hipGridDim_x;\n  const int id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (id < N) {\n    bins[(x[id] / 10)]++;\n  }\n  __syncthreads();\n  for (int i = hipBlockDim_x / 2; i > 0; i /= 2) {\n    if (hipThreadIdx_x < i && id < N) {\n      bins[hipThreadIdx_x] += bins[hipThreadIdx_x + i];\n    }\n    __syncthreads();\n  }\n  if (hipThreadIdx_x == 0) {\n    bins[9] += bins[8];\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double value = x[tid];\n    if (value >= 0 && value < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (value >= 10 && value < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (value >= 20 && value < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (value >= 30 && value < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (value >= 40 && value < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (value >= 50 && value < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (value >= 60 && value < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (value >= 70 && value < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (value >= 80 && value < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (value >= 90 && value < 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double xi = x[tid];\n    int bin = (int)ceil(xi / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Each thread loads one value from the input array and bins it by 10\n  size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n  double value = 0;\n  if (gid < N) {\n    value = x[gid];\n  }\n  __syncthreads();\n  // each thread will count the number of values in the block\n  if (value < 10) {\n    atomicAdd(bins, 1);\n  }\n  // make sure all the threads in the block have completed the count\n  __syncthreads();\n  // each thread will increment the bin for this range\n  if (value >= 10 && value < 20) {\n    atomicAdd(bins + 1, 1);\n  }\n  __syncthreads();\n  if (value >= 20 && value < 30) {\n    atomicAdd(bins + 2, 1);\n  }\n  __syncthreads();\n  if (value >= 30 && value < 40) {\n    atomicAdd(bins + 3, 1);\n  }\n  __syncthreads();\n  if (value >= 40 && value < 50) {\n    atomicAdd(bins + 4, 1);\n  }\n  __syncthreads();\n  if (value >= 50 && value < 60) {\n    atomicAdd(bins + 5, 1);\n  }\n  __syncthreads();\n  if (value >= 60 && value < 70) {\n    atomicAdd(bins + 6, 1);\n  }\n  __syncthreads();\n  if (value >= 70 && value < 80) {\n    atomicAdd(bins + 7, 1);\n  }\n  __syncthreads();\n  if (value >= 80 && value < 90) {\n    atomicAdd(bins + 8, 1);\n  }\n  __syncthreads();\n  if (value >= 90 && value <= 100) {\n    atomicAdd(bins + 9, 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i >= N) return;\n\n  int j = (x[i] / 10.0) + 0.5;\n  if (j < 0) j = 0;\n  if (j > 9) j = 9;\n  atomicAdd(&bins[j], 1);\n}",
            "__shared__ int partial_bins[10];\n  if (threadIdx.x < 10)\n    partial_bins[threadIdx.x] = 0;\n  __syncthreads();\n  for (int i = threadIdx.x; i < N; i += blockDim.x)\n    atomicAdd(&partial_bins[(size_t)ceil(x[i] / 10.0) - 1], 1);\n  __syncthreads();\n  if (threadIdx.x < 10)\n    atomicAdd(&bins[threadIdx.x], partial_bins[threadIdx.x]);\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n   // count number of elements less than 10\n   if (tid < N) {\n      size_t i = (size_t) (x[tid] / 10);\n      if (i == 10) {\n         i = 9;\n      }\n      atomicAdd(&bins[i], 1);\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  int bin = (int)floor(10 * x[i]); // bin number is value x[i] * 10, rounded to integer\n  if (bin < 10) {\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (index >= N) return;\n  const int val = (int) x[index];\n  atomicAdd(&bins[val / 10], 1);\n}",
            "int tid = threadIdx.x;\n  size_t bin = tid / 10;\n  int remainder = tid % 10;\n  int binStart = bin * 10;\n  int binEnd = (bin + 1) * 10;\n  if (binEnd > N)\n    binEnd = N;\n  __shared__ size_t localBins[10];\n  localBins[remainder] = 0;\n  for (int i = binStart + tid; i < binEnd; i += blockDim.x) {\n    localBins[remainder] += ((x[i] >= bin * 10) && (x[i] < (bin + 1) * 10))? 1 : 0;\n  }\n  __syncthreads();\n  if (tid < 10)\n    atomicAdd(&bins[tid], localBins[tid]);\n}",
            "int id = blockDim.x * blockIdx.x + threadIdx.x;\n  int stride = blockDim.x * gridDim.x;\n  for (int i = id; i < N; i += stride) {\n    size_t bin = (size_t)(x[i] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t bin = 0;\n    if (x[i] >= 0 && x[i] < 10) {\n      bin = 1;\n    } else if (x[i] >= 10 && x[i] < 20) {\n      bin = 2;\n    } else if (x[i] >= 20 && x[i] < 30) {\n      bin = 3;\n    } else if (x[i] >= 30 && x[i] < 40) {\n      bin = 4;\n    } else if (x[i] >= 40 && x[i] < 50) {\n      bin = 5;\n    } else if (x[i] >= 50 && x[i] < 60) {\n      bin = 6;\n    } else if (x[i] >= 60 && x[i] < 70) {\n      bin = 7;\n    } else if (x[i] >= 70 && x[i] < 80) {\n      bin = 8;\n    } else if (x[i] >= 80 && x[i] < 90) {\n      bin = 9;\n    } else if (x[i] >= 90 && x[i] < 100) {\n      bin = 10;\n    }\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "__shared__ double buffer[blockDim.x];\n\n    int tid = threadIdx.x;\n\n    buffer[tid] = 0;\n\n    for (size_t i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] < 10) {\n            atomicAdd(&(buffer[0]), 1);\n        } else if (x[i] < 20) {\n            atomicAdd(&(buffer[1]), 1);\n        } else if (x[i] < 30) {\n            atomicAdd(&(buffer[2]), 1);\n        } else if (x[i] < 40) {\n            atomicAdd(&(buffer[3]), 1);\n        } else if (x[i] < 50) {\n            atomicAdd(&(buffer[4]), 1);\n        } else if (x[i] < 60) {\n            atomicAdd(&(buffer[5]), 1);\n        } else if (x[i] < 70) {\n            atomicAdd(&(buffer[6]), 1);\n        } else if (x[i] < 80) {\n            atomicAdd(&(buffer[7]), 1);\n        } else if (x[i] < 90) {\n            atomicAdd(&(buffer[8]), 1);\n        } else if (x[i] < 100) {\n            atomicAdd(&(buffer[9]), 1);\n        }\n    }\n\n    __syncthreads();\n\n    for (int i = tid; i < 10; i += blockDim.x) {\n        atomicAdd(&(bins[i]), buffer[i]);\n    }\n}",
            "__shared__ size_t bins_shared[10];\n  __shared__ double x_shared[1024];\n\n  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  double val = 0;\n\n  if (tid < N) {\n    val = x[tid];\n  }\n\n  x_shared[threadIdx.x] = val;\n  __syncthreads();\n  size_t i = 0;\n  while (i < 10) {\n    if (threadIdx.x < 10) {\n      if (i * 10 <= tid && tid < (i + 1) * 10) {\n        atomicAdd(&bins_shared[threadIdx.x], 1);\n      }\n    }\n    i++;\n  }\n  __syncthreads();\n\n  if (threadIdx.x < 10) {\n    bins[threadIdx.x] = bins_shared[threadIdx.x];\n  }\n}",
            "double thread_val = x[blockIdx.x];\n  // If the value in x is between 0 and 10, increment the counter in bins\n  if (thread_val >= 0 && thread_val < 10) {\n    atomicAdd(&bins[thread_val], 1);\n  }\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int nthreads = blockDim.x * gridDim.x;\n    for (int i = tid; i < N; i += nthreads) {\n        int bin = (int) (x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const double base = 0.0;\n  const double step = 10.0;\n  const size_t blockDim = 1024;\n  const size_t gridDim = (N + blockDim - 1) / blockDim;\n  size_t start = blockDim * blockIdx.x + threadIdx.x;\n  size_t end = blockDim * (blockIdx.x + 1) + threadIdx.x;\n\n  __shared__ size_t smem[blockDim];\n\n  size_t idx = threadIdx.x;\n  smem[idx] = 0;\n\n  // initialize bins to zero\n  if (idx < 10) {\n    bins[idx] = 0;\n  }\n\n  for (size_t i = start; i < N; i += gridDim * blockDim) {\n    size_t bin = static_cast<size_t>((x[i] - base) / step);\n    atomicAdd(&smem[bin], 1);\n  }\n  __syncthreads();\n\n  // copy local bins to global\n  if (idx < 10) {\n    bins[idx] = smem[idx];\n  }\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        const int bin = (int)floor((x[i] / 10.0) + 0.0001);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    const double f = x[i];\n    const size_t bin = (size_t)(f / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tid = hipThreadIdx_x;\n  int bid = hipBlockIdx_x;\n  int gridSize = hipBlockDim_x;\n  __shared__ double xshared[1024];\n\n  xshared[tid] = x[bid * gridSize + tid];\n\n  __syncthreads();\n\n  if (tid == 0) {\n    for (int i = 0; i < N; i++) {\n      int idx = i * gridSize + bid;\n      int val = (int) xshared[i];\n      if (val < 10) {\n        bins[val] += 1;\n      } else {\n        break;\n      }\n    }\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  for (size_t i = tid; i < N; i += stride) {\n    bins[static_cast<int>((x[i] / 10.0))] += 1;\n  }\n}",
            "// Compute the number of threads in each block, and the thread ID\n  // within a block\n  const unsigned int threadId = threadIdx.x;\n  const unsigned int blockSize = blockDim.x;\n  const unsigned int blockId = blockIdx.x;\n\n  // Initialize bins to 0\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n\n  // Compute the number of bins\n  const unsigned int numBins = (N + blockSize - 1) / blockSize;\n\n  // Compute the start and end of this block's data in x\n  const unsigned int start = blockId * blockSize;\n  const unsigned int end = min(start + blockSize, N);\n\n  // Compute the block's data\n  for (unsigned int i = start; i < end; i++) {\n    // Compute which bin the value belongs to\n    const unsigned int bin = (x[i] + 10.0) / 10.0;\n\n    // Increment the bin count\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n    size_t gid = tid * N;\n    size_t bin = 10;\n\n    if (tid < N) {\n        bin = (size_t)x[gid] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n    __syncthreads();\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    double v = x[tid];\n    if (v >= 0 && v < 10)\n      atomicAdd(&bins[static_cast<int>(v)], 1);\n  }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (i < N) {\n        bins[x[i] / 10] += 1;\n    }\n}",
            "// initialize the thread id\n  size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // get the number of threads that are allocated\n  size_t numThreads = blockDim.x * gridDim.x;\n\n  // compute the start and end index in x\n  size_t start = tid * (N - 1) / numThreads;\n  size_t end = (tid + 1) * (N - 1) / numThreads;\n\n  size_t binsSize = 10;\n\n  // compute the bins\n  for (size_t i = start; i < end; i++) {\n    double v = x[i];\n\n    if (v < 0) {\n      continue;\n    }\n\n    if (v < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (v < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (v < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (v < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (v < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (v < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (v < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (v < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (v < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (v < 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    // Each thread has an independent copy of the kernel, and therefore a copy of the\n    // array of bins. Therefore, each thread has an independent copy of the\n    // thread's partial answer.\n    binsBy10Count(x, N, bins, tid);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int bin = x[tid] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Compute the thread number that will compute the 10 bins.\n  int threadNo = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Determine the block number that will compute the 10 bins.\n  int blockNo = blockIdx.x;\n\n  // The threads in the block count the values in [0,10), [10, 20), [20, 30),...\n  if (threadNo < 10) {\n    int count = 0;\n    for (size_t i = blockNo * 100; i < N; i += gridDim.x * 100) {\n      if (x[i] < 10 * (threadNo + 1)) {\n        count++;\n      }\n    }\n    bins[threadNo] = count;\n  }\n}",
            "size_t bin = (int)(x[threadIdx.x] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n  if (idx >= N) return;\n\n  const size_t bin = (size_t)(x[idx] / 10);\n  atomicAdd(bins + bin, 1);\n}",
            "// TODO: Implement\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n\n  // the number of elements per thread\n  size_t nPerThread = N / blockDim.x;\n  // the number of threads per bin\n  size_t nPerBin = (nPerThread + 9) / 10;\n  // the number of bins that have less than nPerBin values\n  size_t nLess = (nPerThread + nPerBin - 1) / nPerBin;\n  // the number of values that do not fit in a bin\n  size_t nRemain = nPerThread - nPerBin * nLess;\n  // the index in x of the first value in this thread's bin\n  size_t start = tid * nPerBin;\n\n  // count the number of values that are in this bin\n  for (size_t i = 0; i < nPerBin; ++i) {\n    // add one to bin i if x is in [i * 10, (i + 1) * 10)\n    bins[i] += (i < nLess) * ((x[start + i] >= i * 10) & (x[start + i] < (i + 1) * 10));\n  }\n\n  // add the number of values that do not fit in a bin\n  if (tid < nRemain) {\n    bins[nLess] += (x[start + nLess] >= nLess * 10);\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  __shared__ double sdata[1024];\n  // Read x into shared memory\n  if (tid < N) {\n    sdata[threadIdx.x] = x[tid];\n  }\n  __syncthreads();\n  // Compute partial sum\n  size_t k = blockDim.x;\n  size_t tid1 = threadIdx.x;\n  size_t stride = k / 2;\n  for (; stride > 0; stride /= 2) {\n    if (tid1 < stride) {\n      sdata[tid1 + stride] += sdata[tid1];\n    }\n    __syncthreads();\n  }\n  // Write results to bins\n  if (tid1 == 0) {\n    for (size_t i = 0; i < 10; i++) {\n      size_t t = i * N / 10;\n      if (t < N) {\n        bins[i] = (size_t) sdata[t];\n      }\n    }\n  }\n}",
            "// Calculate block number and thread number in the 1-dimensional block of\n  // threads. The 1-dimensional block spans the entire array of threads in this\n  // kernel.\n  int bx = blockIdx.x, tx = threadIdx.x;\n\n  // Each thread computes an element of the histogram.\n  if (tx < 10) {\n    // Initialize the histogram to zero.\n    bins[tx] = 0;\n\n    // Compute the histogram.\n    for (size_t i = bx * blockDim.x + tx; i < N; i += blockDim.x * gridDim.x) {\n      // Compute the index of the bin.\n      int bin = int((x[i] / 10));\n\n      // Increment the bin.\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t bin = (x[tid] / 10);\n    if (bin < 10) {\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// Each thread initializes its own value\n  size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int bin = ((int)x[tid]) / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + tid;\n  size_t bin = 10 * (i / 100);\n  if (i < N) {\n    size_t k = (size_t)floor((x[i] - bin) / 10);\n    atomicAdd(&bins[bin + k], 1);\n  }\n}",
            "__shared__ size_t localBins[10];\n  size_t tid = hipThreadIdx_x;\n  size_t bin = tid / 10;\n  localBins[bin] = 0;\n  for (size_t i = tid; i < N; i += hipBlockDim_x) {\n    if (x[i] < bin * 10) {\n      localBins[bin] += 1;\n    }\n  }\n  __syncthreads();\n  atomicAdd(&bins[bin], localBins[bin]);\n}",
            "size_t block = blockIdx.x;\n    size_t thread = threadIdx.x;\n    size_t tid = block * blockDim.x + thread;\n\n    // only compute if thread is in range\n    if (tid < N) {\n        size_t index = (size_t)(x[tid] / 10.0);\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (tid < N) {\n      // size_t idx = static_cast<size_t>(std::round(x[tid] / 10));\n      size_t idx = static_cast<size_t>(x[tid] / 10);\n      atomicAdd(&bins[idx], 1);\n   }\n}",
            "int tid = hipThreadIdx_x;\n  int blockId = hipBlockIdx_x;\n  double blockSum = 0;\n  for (int i = tid; i < N; i += blockDim.x) {\n    double xi = x[i];\n    blockSum += xi / 10.0;\n  }\n  // sum reduction across blocks\n  __shared__ double sdata[1024];\n  sdata[tid] = blockSum;\n  __syncthreads();\n  if (tid < 1023) {\n    sdata[tid] += sdata[tid + 1024];\n  }\n  if (tid == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = (int)sdata[i * 1024];\n    }\n  }\n}",
            "int i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  int bin = 0;\n  while (i < N) {\n    if (x[i] < 10) {\n      bin = 0;\n    } else if (x[i] < 20) {\n      bin = 1;\n    } else if (x[i] < 30) {\n      bin = 2;\n    } else if (x[i] < 40) {\n      bin = 3;\n    } else if (x[i] < 50) {\n      bin = 4;\n    } else if (x[i] < 60) {\n      bin = 5;\n    } else if (x[i] < 70) {\n      bin = 6;\n    } else if (x[i] < 80) {\n      bin = 7;\n    } else if (x[i] < 90) {\n      bin = 8;\n    } else if (x[i] < 100) {\n      bin = 9;\n    }\n    atomicAdd(&bins[bin], 1);\n    i += hipBlockDim_x * hipGridDim_x;\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        int bin = (int)(x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    int bin = x[i] / 10;\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t bin = tid / 10;\n  if (bin < 10) {\n    bins[bin] = 0;\n  }\n  for (; tid < N; tid += blockDim.x * gridDim.x) {\n    bins[tid / 10] += (x[tid] < 10);\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i < N) {\n        size_t bin = (size_t)floor(x[i]/10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int idx = threadIdx.x;\n  // Create an array of 10 bins\n  __shared__ size_t sbins[10];\n  if (idx < 10) {\n    sbins[idx] = 0;\n  }\n  __syncthreads();\n\n  // Find the first bin to increment and the upper bound on the bin value\n  int bin = (int)floor(x[idx] / 10);\n  double binUpperBound = bin * 10.0 + 10.0;\n\n  // Each thread processes a single value\n  for (int i = idx; i < N; i += blockDim.x) {\n    if (x[i] < binUpperBound) {\n      atomicAdd(&sbins[bin], 1);\n    }\n  }\n  __syncthreads();\n\n  // Copy results to global memory\n  if (idx < 10) {\n    atomicAdd(&bins[idx], sbins[idx]);\n  }\n}",
            "// Initialize threadId in each thread\n  int threadId = hipThreadIdx_x;\n  // Initialize globalId in each thread\n  int globalId = hipBlockIdx_x * hipBlockDim_x + threadId;\n\n  if (globalId >= N)\n    return;\n\n  int binId = (int)(x[globalId] / 10.0);\n  atomicAdd(&bins[binId], 1);\n}",
            "int tid = hipThreadIdx_x;\n    int idx = hipBlockIdx_x*hipBlockDim_x+tid;\n    int numThreads = hipBlockDim_x*hipGridDim_x;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    bins[4] = 0;\n    bins[5] = 0;\n    bins[6] = 0;\n    bins[7] = 0;\n    bins[8] = 0;\n    bins[9] = 0;\n    for (size_t i = idx; i < N; i += numThreads) {\n        bins[(int) (x[i] / 10)]++;\n    }\n}",
            "// get id of the thread\n\tint tid = blockIdx.x * blockDim.x + threadIdx.x;\n\t// check if thread has work to do\n\tif (tid >= N) return;\n\n\t// compute the bin index\n\tint bin = x[tid] / 10;\n\n\t// add 1 to the count\n\tatomicAdd(&bins[bin], 1);\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    bins[(size_t)((x[i] / 10) + 0.5)] += 1;\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    int bin = int((x[tid] + 10) / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    double value = x[tid];\n    int bin = (value / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "__shared__ size_t temp[10];\n  size_t thread_id = threadIdx.x;\n  size_t block_id = blockIdx.x;\n  size_t local_id = threadIdx.x;\n  size_t bin_id = block_id * blockDim.x + thread_id;\n\n  // Each thread computes one bin\n  // If there are fewer than 10 values, bin_id will be out of range\n  // and threads can safely exit\n  if (bin_id < 10) {\n    temp[local_id] = 0;\n  }\n  __syncthreads();\n\n  // Each thread reads one value\n  // If there are fewer than N values, bin_id will be out of range\n  // and threads can safely exit\n  if (bin_id < N) {\n    double value = x[bin_id];\n    if (value < 10.0) {\n      temp[0] += 1;\n    } else if (value < 20.0) {\n      temp[1] += 1;\n    } else if (value < 30.0) {\n      temp[2] += 1;\n    } else if (value < 40.0) {\n      temp[3] += 1;\n    } else if (value < 50.0) {\n      temp[4] += 1;\n    } else if (value < 60.0) {\n      temp[5] += 1;\n    } else if (value < 70.0) {\n      temp[6] += 1;\n    } else if (value < 80.0) {\n      temp[7] += 1;\n    } else if (value < 90.0) {\n      temp[8] += 1;\n    } else {\n      temp[9] += 1;\n    }\n  }\n  __syncthreads();\n\n  // Each thread writes to one location in bins\n  if (local_id < 10) {\n    bins[local_id] = temp[local_id];\n  }\n}",
            "// Initialize bins with 0\n  for (int i = threadIdx.x; i < 10; i += blockDim.x)\n    bins[i] = 0;\n  __syncthreads();\n\n  // Determine which bin each value is in and increment the correct bin\n  // Each thread handles one element of x.\n  size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  while (i < N) {\n    atomicAdd(&bins[floor(x[i] / 10)], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    // bins = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n    double val = x[tid];\n    if (val >= 0 && val < 10) {\n      // bins = [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n      atomicAdd(&bins[static_cast<int>(val)], 1);\n    }\n  }\n}",
            "size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (id >= N)\n        return;\n    // size_t i = floor(x[id] / 10);\n    // atomicAdd(&bins[i], 1);\n    // The next line is faster:\n    size_t i = __ffs(size_t(x[id] / 10));\n    atomicAdd(&bins[i-1], 1);\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (tid < N) {\n\t\tconst size_t bin = (size_t) (x[tid] / 10);\n\t\tatomicAdd(&bins[bin], 1);\n\t}\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int bin = (int)floor(x[i] / 10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Initialize the bin counts to zero\n  size_t bin[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  // Get the bin index for each value in x. This will divide the values into 10 bins.\n  for (int i = 0; i < N; i++) {\n    bin[(size_t)(x[i] / 10)]++;\n  }\n  // Accumulate the bin counts in parallel. Each thread handles 1 bin count.\n  for (int i = 0; i < 10; i++) {\n    atomicAdd(&bins[i], bin[i]);\n  }\n}",
            "int tid = hipThreadIdx_x;\n    int gid = hipBlockIdx_x * hipBlockDim_x + tid;\n    if (gid < N) {\n        int bin = (int)(x[gid] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int block = blockIdx.x; // index of the block in the grid\n    int thread = threadIdx.x; // index of the thread in the block\n    int stride = blockDim.x; // number of threads in the block\n    int i = block * stride + thread; // global thread index\n\n    if (i < N) {\n        // Determine which bin the value of x[i] belongs to, and increment the count in that bin\n        atomicAdd(&(bins[(int)(x[i] / 10.0)]), 1);\n    }\n}",
            "size_t thread = blockDim.x * blockIdx.x + threadIdx.x;\n    if (thread < N) {\n        size_t bin = size_t(x[thread] / 10);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "const size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t j = tid;\n  if (j < 10) {\n    size_t count = 0;\n    while (i < N) {\n      double val = x[i];\n      if (val < j * 10 + 10) {\n        count++;\n        i += blockDim.x;\n      } else {\n        break;\n      }\n    }\n    bins[j] = count;\n  }\n}",
            "__shared__ size_t s_bins[10];\n    if(threadIdx.x < 10) {\n        s_bins[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    // compute the bin number of each element of the input array\n    size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n    if(index < N) {\n        size_t bin = (size_t) (x[index] / 10.0);\n        s_bins[bin]++;\n    }\n    __syncthreads();\n\n    // update the bins\n    if(threadIdx.x < 10) {\n        atomicAdd(&bins[threadIdx.x], s_bins[threadIdx.x]);\n    }\n}",
            "// Initialize bins by zero\n  for (int i=0; i<10; i++) {\n    bins[i] = 0;\n  }\n  // Compute bin number for each element in x, store in bins\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    size_t bin = x[i] / 10;\n    if (bin == 10) bin = 9;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tx = hipThreadIdx_x;\n  int bx = hipBlockIdx_x;\n  double start = bx * 10.0;\n  double end = start + 10.0;\n  size_t bin[10] = {0};\n  for (size_t i = tx; i < N; i += hipBlockDim_x) {\n    if (x[i] >= start && x[i] < end) {\n      ++bin[x[i] - start];\n    }\n  }\n  __syncthreads();\n  for (int i = tx; i < 10; i += hipBlockDim_x) {\n    atomicAdd(&bins[i], bin[i]);\n  }\n}",
            "int tid = hipThreadIdx_x;\n    int bid = hipBlockIdx_x;\n\n    // For every element in the array compute the value in the bin.\n    for (size_t i = bid * BLOCKSIZE + tid; i < N; i += BLOCKSIZE * gridDim.x) {\n        bins[x[i] / 10] += 1;\n    }\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    int idx = min(9, static_cast<int>(x[i] / 10));\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "__shared__ double x_shared[1000];\n  __shared__ int counters[10];\n  int tid = threadIdx.x;\n  int block_size = blockDim.x;\n  int block_id = blockIdx.x;\n  int num_blocks = gridDim.x;\n  int block_end = min(N, (block_id + 1) * block_size);\n  if (block_id == 0) {\n    for (int i = 0; i < 10; ++i) {\n      counters[i] = 0;\n    }\n  }\n  __syncthreads();\n  for (int i = block_id * block_size + tid; i < block_end; i += block_size * num_blocks) {\n    int i_mod_10 = (int)(i % 10);\n    x_shared[tid] = x[i];\n    __syncthreads();\n    atomicAdd(&counters[i_mod_10], 1);\n    __syncthreads();\n  }\n  for (int i = tid; i < 10; i += block_size) {\n    atomicAdd(&bins[i], counters[i]);\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int bin = tid / 10;\n  int digit = tid % 10;\n  if (bin < 10 && digit < N) {\n    double value = x[digit];\n    if (value < 10 * bin && value >= 10 * bin + 10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      size_t bin = (size_t)(x[tid] / 10);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t id = hipBlockIdx_x * hipBlockDim_x + tid;\n    if (id < N) {\n        size_t idx = (size_t)x[id];\n        if (idx < 10)\n            atomicAdd(&bins[idx], 1);\n    }\n}",
            "size_t index = blockIdx.x*blockDim.x + threadIdx.x;\n   size_t stride = blockDim.x*gridDim.x;\n   for(size_t i = index; i < N; i += stride) {\n      int bin = (int)floor(x[i]/10);\n      if(bin < 0 || bin > 9)\n         printf(\"value out of range\\n\");\n      atomicAdd(bins+bin, 1);\n   }\n}",
            "int t = threadIdx.x + blockIdx.x * blockDim.x;\n    if (t < N) {\n        int i = floor(x[t] / 10);\n        atomicAdd(&bins[i], 1);\n    }\n}",
            "for (int i = threadIdx.x + blockDim.x * blockIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    int bin = (int) (x[i] / 10.0);\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n\n  size_t i = blockIdx.x * blockDim.x + tid;\n\n  // Calculate which bin to put x[i] in.\n  // See https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#parallel-loop-execution\n  size_t bin = (x[i] / 10);\n\n  // Each thread works on a single bin.\n  // See https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy\n  if (i < N) {\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      bins[(int)x[tid] / 10] += 1;\n   }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "double start, end;\n  MPI_Barrier(MPI_COMM_WORLD);\n  start = MPI_Wtime();\n\n  const size_t size = x.size();\n  const int rank = 0;\n  const int p = 0;\n  const int size_p = size / p;\n  int *counts = new int[10];\n  int *displs = new int[10];\n  double *x_p = new double[size_p];\n\n  for (size_t i = 0; i < size; i++) {\n    x_p[i] = x[i];\n  }\n\n  for (size_t i = 0; i < 10; i++) {\n    counts[i] = 0;\n  }\n\n  for (size_t i = 0; i < size_p; i++) {\n    for (size_t j = 0; j < 10; j++) {\n      if (x_p[i] >= j * 10 && x_p[i] < (j + 1) * 10) {\n        counts[j]++;\n      }\n    }\n  }\n\n  MPI_Gatherv(counts, 10, MPI_INT, counts, counts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n  MPI_Reduce(&counts[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; i++) {\n      bins[i] = counts[i] + bins[i - 1];\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  end = MPI_Wtime();\n  MPI_Finalize();\n  std::cout << \"Execution time: \" << end - start << std::endl;\n}",
            "// TODO\n}",
            "size_t n = x.size();\n    // TODO\n}",
            "// Your code here\n}",
            "int world_size = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int world_rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Calculate number of values per rank.\n  int count = x.size() / world_size;\n  // Calculate number of values in last rank.\n  int remainder = x.size() % world_size;\n  // Assign values.\n  if (world_rank == world_size - 1) {\n    // Last rank.\n    count += remainder;\n  }\n\n  // Create local vector.\n  std::vector<double> local_x(count);\n\n  // Get values for local vector.\n  for (int i = 0; i < count; ++i) {\n    local_x[i] = x[world_rank * count + i];\n  }\n\n  // Create local vector of counters.\n  std::array<size_t, 10> local_bins{0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  for (double val : local_x) {\n    // Get value in range [0,10)\n    int bin = static_cast<int>((val / 10));\n    ++local_bins[bin];\n  }\n\n  // Get the sums of the local bins.\n  std::array<size_t, 10> local_sum = local_bins;\n  MPI_Reduce(local_sum.data(), bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t n = x.size();\n    const int my_rank = getRank();\n\n    // Each rank will have at least one value, but not all will have a value in\n    // every bin. Each rank will have to count its values separately.\n\n    // First, determine the number of values in each bin by doing a prefix sum\n    // with MPI_Scan\n    std::vector<size_t> bin_counts(10, 0);\n    MPI_Scan(&n, bin_counts.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // For example, if rank 0 has 5 values in bin 0, rank 1 has 3 values in bin\n    // 0, and rank 2 has 7 values in bin 0, then rank 0 will have\n    // bin_counts[0] = 5, bin_counts[1] = 5 + 3 = 8, bin_counts[2] = 8 + 7 = 15,\n    // and bin_counts[3] = 15 + 0 = 15\n\n    // Every rank now knows how many values it has in each bin.\n\n    // Next, each rank will count its own values.\n    std::array<size_t, 10> my_bins = {};\n    // std::fill(my_bins.begin(), my_bins.end(), 0);\n    for (size_t i = 0; i < n; ++i) {\n        size_t bin = x[i] / 10;\n        if (bin == 10) {\n            ++bin;\n        }\n        ++my_bins[bin];\n    }\n\n    // Finally, each rank will add its results to the corresponding bins of\n    // bin_counts using MPI_Scan\n    MPI_Scan(my_bins.data(), bin_counts.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // For example, if rank 0 has 12 values in bins 0, 1, and 2, and rank 1 has\n    // 11 values in bins 0 and 3, rank 2 has 8 values in bins 0 and 1, and\n    // rank 3 has 5 values in bin 2, then rank 0 will have\n    // bin_counts[0] = 12, bin_counts[1] = 12, bin_counts[2] = 12 + 11 = 23,\n    // bin_counts[3] = 23 + 11 = 34, and bin_counts[4] = 34 + 8 = 42.\n\n    // Every rank now has the full result. The result is on rank 0 only.\n    if (my_rank == 0) {\n        bins = bin_counts;\n    }\n}",
            "// TODO: compute bins here.\n  MPI_Comm comm = MPI_COMM_WORLD;\n  int rank, world_size;\n  MPI_Comm_size(comm, &world_size);\n  MPI_Comm_rank(comm, &rank);\n\n  std::vector<size_t> counts(world_size, 0);\n  for (size_t i = 0; i < x.size(); ++i) {\n    int temp = static_cast<int>(x[i] / 10);\n    int index = temp + 1;\n    if (0 <= index && index < static_cast<int>(counts.size())) {\n      ++counts[index];\n    }\n  }\n\n  MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, 0, comm);\n}",
            "double min = 0.0, max = 100.0;\n  size_t num_bins = 10;\n\n  // TODO: Implement this function.\n}",
            "size_t n = x.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t local_n = n / MPI_COMM_WORLD_SIZE;\n  size_t start = rank * local_n;\n  size_t end = std::min(start + local_n, n);\n  std::array<int, 10> local_bins{};\n  std::for_each(x.begin() + start, x.begin() + end, [&](double v){\n    int bin = (v / 10);\n    if (bin == 0) {\n      ++local_bins[0];\n    }\n    else if (bin == 1) {\n      ++local_bins[1];\n    }\n    else if (bin == 2) {\n      ++local_bins[2];\n    }\n    else if (bin == 3) {\n      ++local_bins[3];\n    }\n    else if (bin == 4) {\n      ++local_bins[4];\n    }\n    else if (bin == 5) {\n      ++local_bins[5];\n    }\n    else if (bin == 6) {\n      ++local_bins[6];\n    }\n    else if (bin == 7) {\n      ++local_bins[7];\n    }\n    else if (bin == 8) {\n      ++local_bins[8];\n    }\n    else if (bin == 9) {\n      ++local_bins[9];\n    }\n  });\n\n  std::array<int, 10> local_recv{};\n  MPI_Allreduce(local_bins.data(), local_recv.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  std::copy(local_recv.begin(), local_recv.end(), bins.begin());\n}",
            "double min, max;\n    MPI_Allreduce(&x[0], &min, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&x[0], &max, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n    double diff = max - min;\n    int count = x.size();\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int *temp = new int[count];\n    int *start = temp;\n    int *end = temp + count;\n\n    int bins_per_rank = (count + size - 1) / size;\n\n    int start_rank = rank * bins_per_rank;\n    int end_rank = std::min((rank + 1) * bins_per_rank, count);\n\n    int start_bin = start_rank;\n    int end_bin = std::min(end_rank, count);\n\n    int send_size = end_bin - start_bin;\n    int recv_size = end_rank - start_rank;\n\n    for (int i = 0; i < count; i++) {\n        *(temp + (i - start_bin)) = (int)(10 * (x[i] - min) / diff);\n    }\n\n    int *send_counts = new int[size];\n    int *recv_counts = new int[size];\n    int *send_displs = new int[size];\n    int *recv_displs = new int[size];\n\n    MPI_Scatter(temp, send_size, MPI_INT, temp, recv_size, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(temp, send_size, MPI_INT, send_counts, send_size, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(temp, send_size, MPI_INT, send_displs, send_size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int *recv_data = new int[recv_size];\n\n    MPI_Gatherv(temp, recv_size, MPI_INT, recv_data, recv_counts, recv_displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < recv_size; i++) {\n        bins[recv_data[i]] += 1;\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < size; i++) {\n            bins[i] += send_counts[i];\n        }\n    }\n\n    delete[] start;\n    delete[] end;\n    delete[] temp;\n    delete[] send_counts;\n    delete[] recv_counts;\n    delete[] send_displs;\n    delete[] recv_displs;\n    delete[] recv_data;\n}",
            "// TODO\n}",
            "// TODO: implement\n    return;\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // size of x on each rank\n  int n = x.size() / size;\n  // start and end index of rank\n  int start = rank * n;\n  int end = start + n;\n\n  // store the partial result in bins\n  int partial_bins[10]{};\n  for (auto i = start; i < end; ++i) {\n    for (int j = 0; j < 10; ++j) {\n      if (x[i] >= j * 10 && x[i] < (j + 1) * 10) {\n        ++partial_bins[j];\n      }\n    }\n  }\n\n  std::vector<int> counts(10);\n  MPI_Gather(&partial_bins, 10, MPI_INT, counts.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] = counts[i];\n    }\n  }\n}",
            "/*\n    The idea is to compute each count in parallel and then gather the counts on rank 0.\n    Each process will do the same computation by scanning through its x vector.\n    For example, a process with x = [32, 95, 12, 39] will add the following\n    counts in bins: [0, 1, 0, 0, 1]\n    After summing up the counts, each process will write its count to the\n    corresponding bin in `bins` on rank 0.\n    The following code is an attempt to parallelize the computation in a\n    serial way.\n  */\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t j = std::round(x[i]/10.0);\n    bins[j]++;\n  }\n\n  /*\n    The following code is an attempt to parallelize the computation using MPI.\n    The idea is to create a communication topology where all processes compute\n    the counts of the bins for their part of x and communicate the results to the\n    process on rank 0. After that, process 0 will compute the sums of each bin.\n  */\n  int num_proc;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t chunk = x.size() / num_proc;\n\n  std::vector<size_t> local_bins(10);\n  if (rank == 0) {\n    for (int i = 0; i < num_proc; i++) {\n      for (size_t j = 0; j < chunk; j++) {\n        size_t k = (i*chunk)+j;\n        size_t j1 = std::round(x[k]/10.0);\n        local_bins[j1]++;\n      }\n    }\n  }\n\n  MPI_Scatter(local_bins.data(), 10, MPI_INT,\n              bins.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::vector<size_t> sum(10);\n    for (int i = 1; i < num_proc; i++) {\n      std::vector<size_t> remote_bins(10);\n      MPI_Recv(remote_bins.data(), 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < 10; j++) {\n        sum[j] += remote_bins[j];\n      }\n    }\n\n    for (size_t j = 0; j < 10; j++) {\n      bins[j] += sum[j];\n    }\n  } else {\n    MPI_Send(local_bins.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your code here\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        for (size_t i = 0; i < x.size(); i++) {\n            bins[static_cast<size_t>(std::floor(x[i] / 10))] += 1;\n        }\n\n        for (size_t i = 1; i < size; i++) {\n            std::vector<size_t> buffer(10);\n            MPI_Recv(buffer.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 10; j++) {\n                bins[j] += buffer[j];\n            }\n        }\n    } else {\n        std::vector<size_t> buffer(10);\n        for (size_t i = 0; i < x.size(); i++) {\n            buffer[static_cast<size_t>(std::floor(x[i] / 10))] += 1;\n        }\n        MPI_Send(buffer.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // TODO: implement\n}",
            "// Fill in here.\n}",
            "int num_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_bins = 10;\n  int num_per_rank = x.size() / num_ranks;\n  int num_left = x.size() % num_ranks;\n  int num_to_send = 0;\n\n  std::vector<double> local_data;\n  if (rank == 0) {\n    local_data = x;\n  }\n\n  if (rank < num_left) {\n    num_to_send = num_per_rank + 1;\n    local_data.resize(num_per_rank + 1);\n  }\n  else {\n    num_to_send = num_per_rank;\n    local_data.resize(num_per_rank);\n  }\n\n  MPI_Scatter(&local_data[0], num_to_send, MPI_DOUBLE, &local_data[0], num_to_send,\n              MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::vector<int> local_result(num_bins, 0);\n  for (int i = 0; i < num_to_send; i++) {\n    int bin = local_data[i] / 10;\n    if (bin < num_bins) {\n      local_result[bin]++;\n    }\n  }\n\n  std::vector<int> global_result(num_bins, 0);\n  MPI_Reduce(&local_result[0], &global_result[0], num_bins, MPI_INT, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < num_bins; i++) {\n      bins[i] = global_result[i];\n    }\n  }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<size_t> counts;\n  std::vector<double> partial_x;\n  if (rank == 0) {\n    counts = std::vector<size_t>(10, 0);\n  }\n  // TODO: Implement me\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "std::vector<size_t> local_bins;\n  if (x.size() > 0) {\n    size_t size = x.size() / 10;\n    local_bins = std::vector<size_t>(size, 0);\n    for (size_t i = 0; i < size; i++) {\n      for (size_t j = 0; j < 10; j++) {\n        if (x[10 * i + j] >= 10 * j && x[10 * i + j] < 10 * (j + 1)) {\n          local_bins[i] += 1;\n        }\n      }\n    }\n  }\n  std::vector<size_t> global_bins(10);\n  MPI_Allreduce(&local_bins[0], &global_bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n  for (size_t i = 0; i < global_bins.size(); i++) {\n    bins[i] = global_bins[i];\n  }\n}",
            "// Your code here\n\n  // get the total number of elements in x\n  int size = x.size();\n\n  // get the id of this process\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // calculate the interval size\n  double interval_size = 10.0 / size;\n\n  // store the local result in a local array\n  std::array<size_t, 10> local_bins;\n\n  // start from the beginning of x for each process\n  int start = rank * x.size() / size;\n\n  // iterate over the local elements\n  for (size_t i = start; i < x.size(); ++i) {\n    // find the bin\n    int bin = x[i] / interval_size;\n    // increase the count in the corresponding bin\n    ++local_bins[bin];\n  }\n\n  // gather the result from each process\n  MPI_Gather(local_bins.data(),\n             local_bins.size(),\n             MPI_INT,\n             bins.data(),\n             local_bins.size(),\n             MPI_INT,\n             0,\n             MPI_COMM_WORLD);\n\n  // rank 0 calculates the sum of counts\n  if (rank == 0) {\n    // sum each local bin\n    for (size_t i = 1; i < bins.size(); ++i) {\n      bins[i] += bins[i-1];\n    }\n  }\n}",
            "// Your code here...\n}",
            "// TODO: implement me!\n}",
            "int worldSize;\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  int worldRank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n  int worldRankMod10 = worldRank % 10;\n  int chunkSize = (int)x.size() / worldSize;\n  int start = worldRank * chunkSize;\n  int end = (worldRank + 1) * chunkSize;\n  std::vector<double> xChunk(x.begin() + start, x.begin() + end);\n  std::array<size_t, 10> binsLocal;\n  binsLocal.fill(0);\n  for (double x : xChunk) {\n    if (x < 10) {\n      binsLocal[worldRankMod10]++;\n    } else if (x < 20) {\n      binsLocal[(worldRankMod10 + 1) % 10]++;\n    } else if (x < 30) {\n      binsLocal[(worldRankMod10 + 2) % 10]++;\n    } else if (x < 40) {\n      binsLocal[(worldRankMod10 + 3) % 10]++;\n    } else if (x < 50) {\n      binsLocal[(worldRankMod10 + 4) % 10]++;\n    } else if (x < 60) {\n      binsLocal[(worldRankMod10 + 5) % 10]++;\n    } else if (x < 70) {\n      binsLocal[(worldRankMod10 + 6) % 10]++;\n    } else if (x < 80) {\n      binsLocal[(worldRankMod10 + 7) % 10]++;\n    } else if (x < 90) {\n      binsLocal[(worldRankMod10 + 8) % 10]++;\n    } else {\n      binsLocal[(worldRankMod10 + 9) % 10]++;\n    }\n  }\n  std::array<size_t, 10> binsLocalSum;\n  MPI_Allreduce(binsLocal.data(), binsLocalSum.data(), 10, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  if (worldRank == 0) {\n    for (size_t i = 0; i < 10; i++) {\n      bins[i] = binsLocalSum[i];\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<int> bins_local(10, 0);\n\n    /*\n    * First, each process finds the min and max values in the vector x.\n    *\n    * We divide the values in x into chunks, where the size of each chunk\n    * is divisible by the number of processes.\n    *\n    * We want each process to calculate the min and max values of each chunk.\n    * Each process will calculate the min and max values of the chunk\n    * (e.g. the 1st process will calculate the min and max values of the\n    * chunk of size 10, the 2nd process will calculate the min and max values\n    * of the chunk of size 10, and so on).\n    *\n    * Now, we want to send the min and max values of each chunk to\n    * the process with rank = 0 (0, 1, 2, 3,...). We use a simple\n    * approach, where we send each process the min value of the chunk\n    * to the rank = 0 process and the max value of the chunk to the\n    * rank = n - 1 process (n = number of processes).\n    *\n    * Once the min and max values of each chunk are received,\n    * we can get the min and max values of the entire vector x,\n    * by using the min value of the first chunk and the max value\n    * of the last chunk.\n    *\n    * Once we get the min and max values of the entire vector x,\n    * we can calculate the min and max values of each chunk.\n    * For example, if the min value of x is 10 and the max value of x is 90,\n    * the min value of the chunk is 11 and the max value of the chunk is 89.\n    *\n    * Now, we want to send the min and max values of each chunk\n    * to the process with rank = 0 (0, 1, 2, 3,...). We use the same\n    * approach as in the previous step, where we send each process\n    * the min value of the chunk to the rank = 0 process and the\n    * max value of the chunk to the rank = n - 1 process.\n    *\n    * Once we receive the min and max values of each chunk,\n    * we can calculate the min and max values of each bin.\n    *\n    * For example, if the min value of the chunk is 11 and the max value of\n    * the chunk is 89, the number of values in the bin is 8.\n    *\n    * To do this, each process needs to know the min value of the\n    * chunk and the max value of the chunk.\n    *\n    * To do this, we divide the values in x into chunks.\n    * For example, if the vector x contains values 1, 2, 3,..., 100,\n    * and the number of processes is 10, then each process will have\n    * a chunk of size 10.\n    *\n    * Then, we use MPI_Scatter to get the min and max values of each chunk.\n    * For example, the min value of the chunk of process with rank = 0\n    * is 10, the max value of the chunk of process with rank = 0 is 19,\n    * the min value of the chunk of process with rank = 1 is 20,\n    * the max value of the chunk of process with rank = 1 is 29,\n    * and so on.\n    *\n    * To do this, we divide the values in x into chunks.\n    * For example, if the vector x contains values 1, 2, 3,..., 100,\n    * and the number of processes is 10, then each process will have\n    * a chunk of size 10.\n    *\n    * Then, we use MPI_Scatter to get the min and max values of each chunk.\n    * For example, the min value of the chunk of process with rank = 0\n    * is 10, the max value of the chunk of process with rank = 0 is 19,\n    * the min value of the chunk of process with rank = 1 is 20,\n    * the max value of the chunk of process with rank = 1 is 29,\n    * and so on.\n    *\n    * Next, we use MPI_Gather to send the min and max values of each chunk\n    * to the process with rank = 0 (0, 1, 2,",
            "// TODO: Implement me!\n}",
            "// TODO: Implement me\n}",
            "size_t len = x.size();\n  if (len == 0) {\n    return;\n  }\n  double *send_counts = new double[len];\n  double *send_offsets = new double[len];\n  double *recv_counts = new double[len];\n  double *recv_offsets = new double[len];\n\n  // \u8ba1\u7b97\u6bcf\u4e2a\u6570\u7ec4\u7684\u504f\u79fb\u91cf\n  for (int i = 0; i < len; i++) {\n    send_counts[i] = x[i] / 10;\n    send_offsets[i] = i;\n  }\n\n  MPI_Alltoall(send_counts, 1, MPI_DOUBLE, recv_counts, 1, MPI_DOUBLE, MPI_COMM_WORLD);\n\n  for (int i = 1; i < len; i++) {\n    recv_offsets[i] = recv_offsets[i-1] + recv_counts[i-1];\n  }\n  recv_offsets[len - 1] = recv_offsets[len - 2] + recv_counts[len - 2];\n\n  MPI_Alltoallv(send_offsets, send_counts, send_offsets, MPI_DOUBLE,\n                recv_offsets, recv_counts, MPI_DOUBLE, MPI_COMM_WORLD);\n\n  int total_bins = 0;\n  for (int i = 0; i < len; i++) {\n    total_bins += recv_offsets[i];\n  }\n  bins.fill(0);\n  int idx = 0;\n  for (int i = 0; i < len; i++) {\n    for (int j = recv_offsets[i]; j < recv_offsets[i] + recv_counts[i]; j++) {\n      bins[idx++]++;\n    }\n  }\n\n  delete[] send_counts;\n  delete[] send_offsets;\n  delete[] recv_counts;\n  delete[] recv_offsets;\n}",
            "int rank, world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int n = x.size();\n  std::vector<size_t> localBins(10, 0);\n\n  int s = n / world_size;\n  int r = n % world_size;\n\n  if (rank < r)\n    s++;\n\n  for (int i = rank * s; i < std::min(rank * s + s, n); i++) {\n    localBins[std::min(static_cast<int>(x[i] / 10), 9)]++;\n  }\n\n  std::vector<size_t> globalBins(10);\n  MPI_Reduce(localBins.data(), globalBins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++)\n      bins[i] = globalBins[i];\n  }\n}",
            "//TODO: implement\n}",
            "// TODO: implement me!\n  //\n  // Hint:\n  // - You can use MPI to divide the task into sub-tasks, and then use\n  //   `MPI_Scatterv` to gather the result.\n  // - The algorithm is:\n  //   1. Create a vector of integers and initialize it with zeros.\n  //   2. Use `MPI_Scatter` to scatter the number of elements in each bin to each\n  //      process.\n  //   3. Use `MPI_Scatterv` to scatter the data to each process.\n  //   4. For each bin, count the number of values in that bin.\n  //   5. Use `MPI_Gather` to gather the result.\n\n  std::vector<size_t> counts(10, 0);\n  std::vector<double> input;\n  MPI_Scatter(x.data(), 10, MPI_DOUBLE, input.data(), 10, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(counts.data(), 10, MPI_INT, counts.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n  for (size_t i = 0; i < input.size(); i++) {\n    bins[static_cast<int>(input[i] / 10)]++;\n  }\n  MPI_Gather(counts.data(), 10, MPI_INT, counts.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// your code goes here\n}",
            "// Your code here\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int min_size = x.size() / world_size + 1;\n  int max_size = min_size * (world_rank + 1);\n  std::vector<double> local_x;\n\n  if (world_rank == 0) {\n    for (int i = 0; i < world_size - 1; i++) {\n      MPI_Send(&x[i * min_size], min_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  }\n  MPI_Bcast(&min_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  local_x.resize(min_size);\n  MPI_Scatter(x.data(), min_size, MPI_DOUBLE, local_x.data(), min_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 10> local_bins{};\n  for (int i = 0; i < min_size; i++) {\n    int index = std::floor(local_x[i] / 10);\n    local_bins[index]++;\n  }\n\n  MPI_Gather(local_bins.data(), 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    for (int i = 1; i < world_size; i++) {\n      MPI_Status status;\n      MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n      int count;\n      MPI_Get_count(&status, MPI_UNSIGNED_LONG, &count);\n      MPI_Recv(bins.data(), count, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  }\n}",
            "// MPI_Init(0, 0);\n    MPI_Comm_size(MPI_COMM_WORLD, &rank);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_bins = 10;\n    int num_elements = x.size();\n    int chunk_size = num_elements / rank;\n    int num_remaining = num_elements % rank;\n    int end_index = chunk_size + num_remaining;\n    int begin_index = rank * chunk_size;\n\n    if (rank == 0) {\n        for (int i = 0; i < num_bins; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    for (int i = begin_index; i < end_index; i++) {\n        int bin = (int)floor(x[i] / 10);\n        bins[bin] = bins[bin] + 1;\n    }\n\n    // Compute the number of bins for each process\n    std::array<int, num_bins> bins_counts;\n    MPI_Allreduce(bins.data(), bins_counts.data(), num_bins, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n    // Calculate the begin and end indices for each bin on the rank 0 process\n    // bins_indices[i] contains the start and end index for bin i\n    std::array<std::pair<size_t, size_t>, num_bins> bins_indices;\n    size_t total_bins = std::accumulate(bins_counts.begin(), bins_counts.end(), 0);\n    if (rank == 0) {\n        bins_indices[0].first = 0;\n        for (int i = 0; i < num_bins - 1; i++) {\n            bins_indices[i].second = bins_indices[i].first + bins_counts[i];\n            bins_indices[i + 1].first = bins_indices[i].second;\n        }\n        bins_indices[num_bins - 1].second = total_bins;\n    }\n\n    std::array<size_t, num_bins> local_bins;\n    MPI_Scatter(bins.data(), bins_counts.data(), MPI_INT, local_bins.data(), bins_counts.data(), MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gatherv(local_bins.data(), bins_counts.data(), MPI_INT, bins.data(), bins_counts.data(), bins_indices.data(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "/* WRITE YOUR CODE HERE */\n}",
            "int num_procs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<size_t> local_bins = bins;\n    //std::vector<size_t> local_bins(10, 0);\n\n    for (size_t i = 0; i < x.size(); i++) {\n        int val = std::min((int)(10 * x[i]), 9);\n        local_bins[val]++;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype MPI_DOUBLE_ARRAY = MPI_DATATYPE_NULL;\n  MPI_Type_contiguous(x.size(), MPI_DOUBLE, &MPI_DOUBLE_ARRAY);\n  MPI_Type_commit(&MPI_DOUBLE_ARRAY);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // split up the values into chunks\n  std::vector<double> values;\n  std::vector<double> lowerBounds(size);\n  std::vector<double> upperBounds(size);\n\n  // initialize the lower and upper bounds\n  for(int i = 0; i < size; ++i) {\n    lowerBounds[i] = (double)i * x.size() / size;\n    upperBounds[i] = (double)(i + 1) * x.size() / size;\n  }\n\n  // copy the values to rank 0 to get the total number of values\n  if(rank == 0) {\n    for(int i = 0; i < size; ++i) {\n      values.insert(values.end(), x.begin() + lowerBounds[i], x.begin() + upperBounds[i]);\n    }\n  }\n\n  int lower = lowerBounds[rank];\n  int upper = upperBounds[rank];\n\n  // count the values in rank\n  if(rank == 0) {\n    MPI_Reduce(values.data(), bins.data(), bins.size(), MPI_DOUBLE_ARRAY, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(x.data() + lower, bins.data(), bins.size(), MPI_DOUBLE_ARRAY, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  MPI_Type_free(&MPI_DOUBLE_ARRAY);\n}",
            "// TODO: Replace this comment with your solution\n    //\n    // Remember that this function is called on every rank, so you should\n    // only perform the reduction on rank 0.\n    //\n    // The solution is to have a variable nBins that is the number of bins.\n    // In the for loop, you should compute the bin number and the count.\n    // For example, if bin number 2 is for [10, 20), you should use\n    // bins[2] += count.\n    //\n    // You might find std::count_if() and std::accumulate() useful.\n    //\n    // You should only update the counts for the elements that you own.\n    // Remember that the number of elements in x is not necessarily divisible\n    // by the number of ranks. You have to use MPI_Scatter() to get your\n    // elements to rank 0.\n    //\n    // You should not call any MPI_Gather() functions. The result is already\n    // in the vector bins on rank 0.\n    //\n\n    // number of bins\n    size_t nBins = bins.size();\n    // number of elements\n    size_t n = x.size();\n    if (n % nBins!= 0)\n    {\n        std::cout << \"Not divisible\" << std::endl;\n        return;\n    }\n    size_t nPerRank = n/nBins;\n\n    // each rank gets its own vector\n    std::vector<double> rankX(nPerRank);\n    MPI_Scatter(&x[0], nPerRank, MPI_DOUBLE, &rankX[0], nPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < nPerRank; i++) {\n        size_t binNum = (size_t)((rankX[i]/10.0)*nBins);\n        if (binNum < nBins) {\n            bins[binNum] += 1;\n        }\n    }\n    // MPI reduction\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t length = x.size();\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t localLength = length / size;\n    if (rank == 0) {\n        size_t start = 0;\n        for (int i = 1; i < size; i++) {\n            double data;\n            MPI_Status status;\n            MPI_Recv(&data, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n            x.push_back(data);\n            localLength = length / size;\n        }\n    }\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    double *localX = new double[localLength];\n    MPI_Scatter(x.data(), localLength, MPI_DOUBLE, localX, localLength, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (size_t i = 0; i < localLength; i++) {\n        if (localX[i] < 10) {\n            bins[0]++;\n        } else if (localX[i] < 20) {\n            bins[1]++;\n        } else if (localX[i] < 30) {\n            bins[2]++;\n        } else if (localX[i] < 40) {\n            bins[3]++;\n        } else if (localX[i] < 50) {\n            bins[4]++;\n        } else if (localX[i] < 60) {\n            bins[5]++;\n        } else if (localX[i] < 70) {\n            bins[6]++;\n        } else if (localX[i] < 80) {\n            bins[7]++;\n        } else if (localX[i] < 90) {\n            bins[8]++;\n        } else if (localX[i] < 100) {\n            bins[9]++;\n        }\n    }\n    double *localBins = new double[10];\n    MPI_Gather(bins.data(), 10, MPI_INT, localBins, 10, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (size_t i = 0; i < 10; i++) {\n            bins[i] = localBins[i];\n        }\n    }\n    delete[] localX;\n    delete[] localBins;\n}",
            "// TODO: your code here\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t len = x.size();\n    size_t cnt = len / size;\n    int start = rank * cnt;\n    if (rank == size - 1)\n        cnt = len - (size - 1) * cnt;\n    std::vector<double> temp(x.begin() + start, x.begin() + start + cnt);\n    std::array<int, 10> localBins = binsBy10(temp);\n    MPI_Reduce(localBins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::vector<size_t> local_bins(10, 0);\n\n  /* get the size of this process */\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  /* get the rank of this process */\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  /* split the work across all ranks */\n  int size = x.size();\n  int chunk = size / world_size;\n  int start = world_rank * chunk;\n  int end = std::min(size, start + chunk);\n\n  /* do the work */\n  for (int i = start; i < end; ++i) {\n    /* increment the bin corresponding to the value of x[i] */\n    local_bins[x[i]/10]++;\n  }\n\n  /* gather all the bins onto rank 0 */\n  std::array<size_t, 10> bins_on_rank0;\n  MPI_Gather(local_bins.data(), 10, MPI_INT, bins_on_rank0.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  /* copy the bins from rank 0 */\n  if (world_rank == 0) {\n    bins = bins_on_rank0;\n  }\n}",
            "int size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Status status;\n\n\tsize_t n = x.size();\n\tsize_t chunkSize = n / size;\n\tsize_t remainder = n % size;\n\n\tint start = rank * chunkSize;\n\tint end = (rank == size - 1? n : rank * chunkSize + chunkSize);\n\tend += rank < remainder? 1 : 0;\n\n\tstd::vector<int> count = std::vector<int>(10, 0);\n\n\tfor (int i = start; i < end; ++i) {\n\t\tint tmp = x[i] / 10;\n\t\tif (tmp < 10) {\n\t\t\tcount[tmp] += 1;\n\t\t}\n\t}\n\tint sum = 0;\n\tfor (auto i : count) {\n\t\tsum += i;\n\t}\n\tMPI_Reduce(count.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < 10; ++i) {\n\t\t\tbins[i] = bins[i] / size;\n\t\t}\n\t}\n}",
            "// TODO: Compute bins using parallelism\n}",
            "// TODO: Fill this in\n}",
            "if (x.size() == 0)\n\t\treturn;\n\n\t// TODO: compute a partition of the range of x across all ranks\n\n\t// compute partition\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tstd::vector<double> x_copy(x.size());\n\tMPI_Scatter(x.data(), x.size(), MPI_DOUBLE, x_copy.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\t// partition into groups of 10\n\tstd::vector<size_t> counts(size);\n\tstd::vector<size_t> displs(size);\n\tstd::vector<double> temp;\n\tfor (int i = 0; i < size; ++i) {\n\t\tcounts[i] = (x_copy.size() + (size - i - 1)) / size;\n\t\tdispls[i] = (x_copy.size() - counts[i]) * i;\n\t}\n\tMPI_Gatherv(x_copy.data(), counts[rank], MPI_DOUBLE,\n\t\ttemp.data(), counts.data(), displs.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tbins.fill(0);\n\t\tfor (size_t i = 0; i < temp.size(); ++i) {\n\t\t\tfor (size_t j = i * 10; j < (i + 1) * 10; ++j) {\n\t\t\t\tif (j < temp.size())\n\t\t\t\t\tbins[j - i * 10] += 1;\n\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: compute local bins\n}",
            "size_t N = x.size();\n\n  // TODO: Your code here.\n\n  // MPI_Barrier(MPI_COMM_WORLD); // if MPI_Barrier is called before every single MPI call, this program will work properly.\n\n  // TODO: This is not necessary, just a good habit to get into.\n  // Do not forget to release memory resources.\n  MPI_Finalize();\n}",
            "//...\n}",
            "size_t size = x.size();\n    int rank, n_ranks;\n\n    // get rank and number of ranks\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    // split x among the ranks\n    std::vector<size_t> counts(n_ranks, 0);\n    std::partial_sum(x.begin(), x.end(), counts.begin() + 1);\n    size_t lower_limit = std::min(counts[rank], size);\n    size_t upper_limit = std::min(counts[rank + 1], size);\n\n    // compute the number of values in [0,10), [10, 20), [20, 30),...\n    for (size_t i = lower_limit; i < upper_limit; ++i) {\n        if ((x[i] >= 0) && (x[i] < 10)) {\n            ++bins[static_cast<size_t>(x[i])];\n        }\n    }\n}",
            "int rank, nprocs;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int count = x.size();\n    std::vector<double> localX = x;\n    MPI_Bcast(&count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(localX.data(), count, MPI_DOUBLE, localX.data(), count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int numBins = 10;\n    int chunkSize = count / nprocs;\n    int start = rank * chunkSize;\n    int end = rank == nprocs - 1? count : start + chunkSize;\n\n    std::vector<int> localBins(numBins);\n    for (int i = start; i < end; i++) {\n        int localBin = std::floor(localX[i] / 10);\n        localBins[localBin]++;\n    }\n    MPI_Gather(localBins.data(), numBins, MPI_INT, bins.data(), numBins, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int div = x.size() / size;\n  std::vector<double> myX = std::vector<double>(x.begin() + div * rank, x.begin() + div * (rank + 1));\n\n  std::array<size_t, 10> myBins;\n  for (int i = 0; i < myX.size(); ++i) {\n    for (int j = 0; j < 10; ++j) {\n      if (myX[i] <= 10 + j) {\n        myBins[j] += 1;\n      }\n    }\n  }\n  std::vector<size_t> sendCounts(size);\n  std::vector<size_t> recvCounts(size);\n  std::vector<size_t> displs(size);\n  for (int i = 0; i < size; ++i) {\n    sendCounts[i] = 10;\n    recvCounts[i] = 10;\n    displs[i] = 10 * i;\n  }\n  MPI_Scatterv(myBins.data(), sendCounts.data(), displs.data(), MPI_UNSIGNED_LONG,\n               bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  MPI_Gatherv(bins.data(), 10, MPI_UNSIGNED_LONG, bins.data(), recvCounts.data(), displs.data(), MPI_UNSIGNED_LONG,\n             0, MPI_COMM_WORLD);\n}",
            "int numRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  if (my_rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n\n  int num_tasks = (int)x.size();\n  int task_per_rank = num_tasks / numRanks;\n  int remain = num_tasks % numRanks;\n\n  int task_start = my_rank * (task_per_rank + remain);\n  int task_end = task_start + task_per_rank + (my_rank < remain);\n\n  double threshold = my_rank * 10;\n  for (int t = task_start; t < task_end; t++) {\n    if (x[t] >= threshold && x[t] < threshold + 10) {\n      bins[x[t] / 10]++;\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  std::array<size_t, 10> local_bins;\n  MPI_Reduce(bins.data(), local_bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (my_rank == 0) {\n    bins = local_bins;\n  }\n}",
            "size_t count = x.size() / 10;\n\n    std::vector<double> vec_x(x.begin(), x.begin() + count);\n\n    std::vector<double> vec_bins(10);\n    for (int i = 0; i < 10; i++)\n        vec_bins[i] = 0;\n\n    std::vector<double> vec_result;\n\n    for (int i = 0; i < 10; i++) {\n        vec_result = sendReceive(vec_x, vec_bins, i, 0);\n        std::swap(vec_bins, vec_result);\n    }\n\n    if (0 == MPI::COMM_WORLD.Get_rank()) {\n        for (int i = 0; i < 10; i++)\n            bins[i] = vec_bins[i];\n    }\n}",
            "std::vector<int> localBins(10);\n    MPI_Scatter(x.data(), 10, MPI_DOUBLE, localBins.data(), 10, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < 10; ++i) {\n        bins[i] += localBins[i];\n    }\n}",
            "size_t n = x.size();\n    size_t start = 0, end = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    double my_sum = 0.0;\n\n    if (n < size) {\n        if (rank == 0) {\n            for (size_t i = 0; i < n; i++) {\n                int bin = (int) std::floor(x[i] / 10.0);\n                my_sum += bin;\n            }\n        }\n        MPI_Bcast(&my_sum, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        bins[0] = my_sum;\n    } else {\n        int stride = n / size;\n        if (rank == 0) {\n            start = 0;\n            end = stride;\n            for (int i = 1; i < size; i++) {\n                MPI_Send(&stride, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n                start += stride;\n                end += stride;\n            }\n        } else {\n            MPI_Recv(&stride, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&start, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            MPI_Recv(&end, 1, MPI_INT, 0, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        }\n\n        std::vector<double> my_x(end - start);\n        std::copy(x.begin() + start, x.begin() + end, my_x.begin());\n        my_sum = 0.0;\n        for (size_t i = 0; i < my_x.size(); i++) {\n            int bin = (int) std::floor(my_x[i] / 10.0);\n            my_sum += bin;\n        }\n        MPI_Reduce(&my_sum, &bins[0], 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "int myRank;\n  int numProcesses;\n  MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n  /* Send and receive counts and displacements of each rank's slice. */\n  std::vector<size_t> sliceCounts(numProcesses);\n  std::vector<size_t> sliceDisplacements(numProcesses);\n  MPI_Scatter(x.size(), 1, MPI_UNSIGNED_LONG, sliceCounts.data(), 1,\n              MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n  sliceDisplacements[0] = 0;\n  for (int i = 1; i < numProcesses; ++i) {\n    sliceDisplacements[i] = sliceDisplacements[i-1] + sliceCounts[i-1];\n  }\n\n  /* Compute bins on each rank. */\n  std::vector<size_t> rankBins(10, 0);\n  for (size_t i = 0; i < sliceCounts[myRank]; ++i) {\n    if (x[sliceDisplacements[myRank] + i] < 10.0) {\n      ++rankBins[0];\n    } else if (x[sliceDisplacements[myRank] + i] < 20.0) {\n      ++rankBins[1];\n    } else if (x[sliceDisplacements[myRank] + i] < 30.0) {\n      ++rankBins[2];\n    } else if (x[sliceDisplacements[myRank] + i] < 40.0) {\n      ++rankBins[3];\n    } else if (x[sliceDisplacements[myRank] + i] < 50.0) {\n      ++rankBins[4];\n    } else if (x[sliceDisplacements[myRank] + i] < 60.0) {\n      ++rankBins[5];\n    } else if (x[sliceDisplacements[myRank] + i] < 70.0) {\n      ++rankBins[6];\n    } else if (x[sliceDisplacements[myRank] + i] < 80.0) {\n      ++rankBins[7];\n    } else if (x[sliceDisplacements[myRank] + i] < 90.0) {\n      ++rankBins[8];\n    } else {\n      ++rankBins[9];\n    }\n  }\n\n  /* Gather the counts from all ranks. */\n  std::vector<size_t> allBins(10, 0);\n  MPI_Gatherv(rankBins.data(), rankBins.size(), MPI_UNSIGNED_LONG,\n              allBins.data(), sliceCounts.data(), sliceDisplacements.data(),\n              MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (myRank == 0) {\n    bins = std::array<size_t, 10>(allBins.begin(), allBins.begin() + 10);\n  }\n}",
            "size_t n = x.size();\n\tsize_t chunkSize = n / size;\n\n\tint rank = 0;\n\tint size = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tsize_t myStart = chunkSize * rank;\n\tsize_t myEnd = chunkSize * (rank + 1);\n\tif (rank == size - 1) myEnd = n;\n\n\tstd::vector<size_t> localBins;\n\tlocalBins.resize(10);\n\n\tfor (size_t i = myStart; i < myEnd; i++) {\n\t\tint j = (int)(x[i] / 10);\n\t\tif (j >= 0 && j < 10) {\n\t\t\tlocalBins[j]++;\n\t\t}\n\t}\n\n\t// sum up the local counts\n\tstd::vector<size_t> localCounts(localBins.size());\n\tMPI_Allreduce(localBins.data(), localCounts.data(), localBins.size(),\n\t\tMPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n\n\t// store the results\n\tif (rank == 0) {\n\t\tbins = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };\n\t\tfor (size_t i = 0; i < 10; i++) {\n\t\t\tbins[i] = localCounts[i];\n\t\t}\n\t}\n}",
            "int myRank, numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    // find lower and upper bound for this rank\n    double lower = 10 * myRank;\n    double upper = 10 * (myRank + 1);\n\n    // count number of values in [lower, upper)\n    bins.fill(0);\n    for (double val : x) {\n        if (lower <= val && val < upper) {\n            ++bins[static_cast<size_t>(val / 10.0 - 0.0000001)];\n        }\n    }\n\n    // sum counts\n    std::array<size_t, 10> localSums, globalSums;\n    MPI_Reduce(&bins, &localSums, bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&localSums, &globalSums, bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (myRank == 0) {\n        bins = globalSums;\n    }\n}",
            "// TODO\n  int n = x.size();\n  // create a vector to store the counts\n  int *local_bins = new int[10];\n  // get the number of processes\n  int comm_sz;\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n  // get the rank of the process\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // get the number of values in each process\n  int local_n = n / comm_sz;\n  int extra = n % comm_sz;\n\n  std::vector<double> local_x;\n  if (rank < extra) {\n    // this process has extra values, so add 1 to local_n and put them in the front\n    local_n += 1;\n    local_x.push_back(x[rank * (local_n - 1)]);\n  } else {\n    // this process has fewer values, so just put them in the front\n    local_x.push_back(x[(rank - extra) * (local_n - 1)]);\n  }\n\n  for (int i = 1; i < local_n; i++) {\n    local_x.push_back(x[(rank * (local_n - 1)) + i]);\n  }\n  // sort the data\n  std::sort(local_x.begin(), local_x.end());\n\n  // count\n  for (int i = 0; i < local_n - 1; i++) {\n    for (int j = 0; j < 10; j++) {\n      if (local_x[i] <= j * 10 + 10 && local_x[i] > j * 10) {\n        local_bins[j] += 1;\n      }\n    }\n  }\n\n  // gather the result to rank 0\n  int *global_bins = new int[10];\n  MPI_Gather(local_bins, 10, MPI_INT, global_bins, 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // copy the result\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++) {\n      bins[i] = global_bins[i];\n    }\n  }\n\n  delete[] local_bins;\n  delete[] global_bins;\n}",
            "// your code goes here\n}",
            "auto const size = x.size();\n  if (size > 0) {\n    auto const myRank = MPI::COMM_WORLD.Get_rank();\n    std::array<size_t, 10> localBins;\n    size_t const localSize = x.size() / MPI::COMM_WORLD.Get_size();\n    auto const firstLocalElement = myRank * localSize;\n    auto const lastLocalElement = firstLocalElement + localSize;\n    for (size_t i = 0; i < localBins.size(); ++i) {\n      localBins[i] = 0;\n    }\n    for (size_t i = firstLocalElement; i < lastLocalElement; ++i) {\n      int const bucket = std::floor(x[i] / 10.0);\n      localBins[bucket]++;\n    }\n    std::array<size_t, 10> localBinsTotal{};\n    MPI::COMM_WORLD.Reduce(localBins.data(), localBinsTotal.data(), 10, MPI::UINT, MPI::SUM, 0);\n    if (myRank == 0) {\n      bins = localBinsTotal;\n    }\n  }\n}",
            "// TODO: implement me\n\n}",
            "if (x.empty()) return;\n\n  // Divide x into 10 parts.\n  std::vector<std::vector<double>> x_split(10);\n  size_t n = x.size();\n  size_t s = n / 10;\n  for (size_t i = 0; i < 10; ++i) {\n    x_split[i].assign(x.begin() + s * i, x.begin() + s * (i + 1));\n  }\n\n  // Compute the number of values in [0,10).\n  std::array<size_t, 10> bins_local{};\n  for (size_t i = 0; i < 10; ++i) {\n    auto const& x_local = x_split[i];\n    bins_local[i] = std::count_if(x_local.begin(), x_local.end(),\n        [&](double x) { return x <= 10; });\n  }\n\n  // Reduce.\n  MPI_Reduce(bins_local.data(), bins.data(), 10,\n      MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t local_bins[10];\n  for (size_t i=0; i<10; ++i)\n    local_bins[i] = 0;\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank==0) {\n    for (size_t i=0; i<x.size(); ++i)\n      ++local_bins[static_cast<size_t>(std::floor(x[i]/10.0))];\n    bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  }\n  MPI_Bcast(local_bins, 10, MPI_LONG, 0, MPI_COMM_WORLD);\n  MPI_Reduce(local_bins, bins.data(), 10, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code goes here!\n}",
            "int rank, worldSize;\n  MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t localSize = x.size() / worldSize;\n  size_t offset = localSize * rank;\n\n  std::vector<double> localX(x.begin() + offset, x.begin() + offset + localSize);\n  std::array<size_t, 10> localBins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  for (size_t i = 0; i < localSize; ++i) {\n    if (localX[i] >= 0 && localX[i] < 10) {\n      ++localBins[static_cast<size_t>(localX[i])];\n    }\n  }\n\n  MPI_Allreduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "const int num_ranks = MPI_COMM_WORLD.Get_size();\n  const int rank = MPI_COMM_WORLD.Get_rank();\n\n  if (rank == 0) {\n    std::fill(bins.begin(), bins.end(), 0);\n  }\n\n  int local_size = x.size() / num_ranks;\n  int local_start = local_size * rank;\n\n  if (rank == num_ranks - 1) {\n    local_size = x.size() - (local_size * (num_ranks - 1));\n  }\n\n  std::vector<double> local_x(local_size);\n\n  MPI_Scatter(x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 10> local_bins;\n\n  for (size_t i = 0; i < local_size; i++) {\n    local_bins[static_cast<int>(x[i])]++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "size_t n = x.size();\n  // TODO: Implement this function.\n}",
            "/* TODO: implement this function. */\n}",
            "// your code here\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // local variables\n  std::array<size_t, 10> local_bins;\n  int total = 0;\n\n  for (size_t i = 0; i < x.size(); i++) {\n    // assign bin\n    if (x[i] >= 0 && x[i] <= 10) {\n      local_bins[x[i]]++;\n    } else {\n      local_bins[10]++;\n    }\n    total++;\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // check the result\n  if (rank == 0) {\n    std::vector<size_t> correct_bins{1, 2, 0, 3, 0, 0, 1, 2, 0, 1};\n    for (size_t i = 0; i < 10; i++) {\n      assert(correct_bins[i] == bins[i]);\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: implement this function.\n}",
            "// YOUR CODE HERE\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  int block_size = (n + size - 1) / size;\n  int start = block_size * rank;\n  int end = std::min(n, block_size * (rank + 1));\n\n  std::array<size_t, 10> local_bins{};\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  for (int i = start; i < end; i++) {\n    double value = x[i];\n    int idx = (int)value / 10;\n    local_bins[idx]++;\n  }\n\n  std::vector<size_t> global_bins(10, 0);\n  MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  bins = global_bins;\n}",
            "// Your code here.\n  size_t n = x.size();\n  int nproc;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  int nperproc = n / nproc;\n  int rem = n % nproc;\n\n  std::vector<double> local_x(nperproc);\n  std::array<size_t, 10> local_bins{};\n\n  // Send and receive data.\n  MPI_Scatter(x.data(), nperproc, MPI_DOUBLE, local_x.data(), nperproc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Gather(&local_bins, 10, MPI_UNSIGNED_LONG_LONG, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  if (0 == rank) {\n    // Merge.\n    size_t i = 0;\n    size_t j = nperproc;\n    for (size_t k = 0; k < nproc; ++k) {\n      // Copy current bin values.\n      size_t const* src = &bins[k * 10];\n      size_t const* src_end = src + 10;\n      size_t* dest = &local_bins[k * 10];\n      size_t* dest_end = dest + 10;\n\n      while (src!= src_end) {\n        if (j == n) {\n          // We're at the end of the data.\n          std::copy(src, src_end, dest);\n          break;\n        }\n        if (i == nperproc) {\n          // We're at the end of the local array.\n          std::copy(src, src_end, dest);\n          break;\n        }\n\n        if (*src <= x[j]) {\n          *dest += 1;\n          ++dest;\n          ++src;\n        }\n        ++j;\n      }\n    }\n\n    // Merge bins on this rank.\n    i = 0;\n    j = 0;\n    for (size_t k = 0; k < 10; ++k) {\n      if (i == nperproc) {\n        local_bins[k] += rem;\n        break;\n      }\n      if (j == nproc) {\n        break;\n      }\n      if (local_bins[k] <= local_bins[k * 10]) {\n        local_bins[k] += local_bins[k * 10];\n        ++i;\n      } else {\n        local_bins[k] += local_bins[k * 10];\n        ++j;\n      }\n    }\n  }\n\n  MPI_Gather(&local_bins, 10, MPI_UNSIGNED_LONG_LONG, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// TODO: Implement this function\n    return;\n}",
            "// Your code here\n}",
            "// 1. Compute the number of elements in each bin\n  std::vector<size_t> num_in_bins(10, 0);\n  for (double value : x) {\n    if (value < 10) {\n      num_in_bins[value] += 1;\n    }\n  }\n  // 2. Collect number of elements from all processes\n  int nproc;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  std::vector<size_t> all_num_in_bins(10 * nproc);\n  MPI_Allgather(num_in_bins.data(), 10, MPI_UNSIGNED_LONG, all_num_in_bins.data(), 10, MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n  // 3. Sum values in all_num_in_bins\n  size_t sum = 0;\n  for (size_t value : all_num_in_bins) {\n    sum += value;\n  }\n  // 4. Compute number of elements in each bin\n  for (size_t i = 0; i < 10; i++) {\n    bins[i] = all_num_in_bins[i] / sum * x.size();\n  }\n}",
            "// number of data items, for each rank\n  const size_t num_items = x.size();\n\n  // split number of items into all ranks\n  std::vector<size_t> local_sizes(MPI_COMM_SIZE);\n  MPI_Allgather(&num_items, 1, MPI_UNSIGNED_LONG_LONG,\n                local_sizes.data(), 1, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n  // calculate the starting indices\n  std::vector<size_t> local_starts;\n  local_starts.push_back(0);\n  for (int i = 0; i < MPI_COMM_SIZE; ++i) {\n    local_starts.push_back(local_starts.at(i) + local_sizes.at(i));\n  }\n\n  // get data items for each rank\n  std::vector<double> local_data;\n  local_data.resize(local_sizes.at(rank));\n  MPI_Scatterv(x.data(), local_sizes.data(), local_starts.data(), MPI_DOUBLE,\n               local_data.data(), local_sizes.at(rank), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // compute local bin counts\n  std::array<size_t, 10> local_bins;\n  for (size_t i = 0; i < local_data.size(); ++i) {\n    const int value = local_data.at(i);\n    if (value < 10) {\n      ++local_bins.at(0);\n    } else if (value < 20) {\n      ++local_bins.at(1);\n    } else if (value < 30) {\n      ++local_bins.at(2);\n    } else if (value < 40) {\n      ++local_bins.at(3);\n    } else if (value < 50) {\n      ++local_bins.at(4);\n    } else if (value < 60) {\n      ++local_bins.at(5);\n    } else if (value < 70) {\n      ++local_bins.at(6);\n    } else if (value < 80) {\n      ++local_bins.at(7);\n    } else if (value < 90) {\n      ++local_bins.at(8);\n    } else {\n      ++local_bins.at(9);\n    }\n  }\n\n  // gather bin counts from all ranks\n  MPI_Gather(local_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG,\n             bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double min = 0, max = 100;\n  std::vector<double> local_x;\n  int my_count = 0;\n  if (rank == 0) {\n    local_x = x;\n  }\n  MPI_Bcast(&min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank!= 0) {\n    double my_max = min + 10;\n    for (int i = 0; i < size; i++) {\n      if (my_max > max) {\n        my_max = max;\n      }\n      for (double i = min; i < my_max; i++) {\n        if (i >= min && i < my_max && i < max) {\n          local_x.push_back(i);\n        }\n      }\n      min = my_max;\n      my_max += 10;\n    }\n  }\n\n  for (double i : local_x) {\n    if (i >= 0 && i <= 10) {\n      my_count++;\n    }\n  }\n\n  MPI_Reduce(&my_count, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n    // divide work among ranks\n    size_t n_local = n / mpi_size;\n    // remainder\n    size_t remainder = n % mpi_size;\n    // local histogram\n    std::array<size_t, 10> local_bins;\n    // compute local histogram\n    for (size_t i = 0; i < n_local; ++i) {\n        if (x[i] < 10) {\n            ++local_bins[0];\n        } else if (x[i] < 20) {\n            ++local_bins[1];\n        } else if (x[i] < 30) {\n            ++local_bins[2];\n        } else if (x[i] < 40) {\n            ++local_bins[3];\n        } else if (x[i] < 50) {\n            ++local_bins[4];\n        } else if (x[i] < 60) {\n            ++local_bins[5];\n        } else if (x[i] < 70) {\n            ++local_bins[6];\n        } else if (x[i] < 80) {\n            ++local_bins[7];\n        } else if (x[i] < 90) {\n            ++local_bins[8];\n        } else {\n            ++local_bins[9];\n        }\n    }\n    // add remainder to local histogram\n    for (size_t i = 0; i < remainder; ++i) {\n        if (x[n_local + i] < 10) {\n            ++local_bins[0];\n        } else if (x[n_local + i] < 20) {\n            ++local_bins[1];\n        } else if (x[n_local + i] < 30) {\n            ++local_bins[2];\n        } else if (x[n_local + i] < 40) {\n            ++local_bins[3];\n        } else if (x[n_local + i] < 50) {\n            ++local_bins[4];\n        } else if (x[n_local + i] < 60) {\n            ++local_bins[5];\n        } else if (x[n_local + i] < 70) {\n            ++local_bins[6];\n        } else if (x[n_local + i] < 80) {\n            ++local_bins[7];\n        } else if (x[n_local + i] < 90) {\n            ++local_bins[8];\n        } else {\n            ++local_bins[9];\n        }\n    }\n    // reduce all histograms to one on rank 0\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO(you): implement\n\n  // The main idea is to first sort x by values. Then assign x[i] to the rank which\n  // is responsible for the range of [x[i] - 0.1, x[i]]. This is equivalent to\n  // splitting the range into 10 parts.\n  // We can use std::equal_range to get the begin and end iterators of each rank's range.\n  // Note that the begin and end iterators do not point to the first element of x.\n  // The value x[0] belongs to the range [0, 10).\n  // We can use std::distance to get the size of each rank's range.\n  // Finally, we can use MPI_Reduce to get the total count of each rank.\n\n  // The code below is provided as a reference.\n\n  // auto begin = x.begin();\n  // auto end = x.end();\n  // std::sort(begin, end);\n\n  // // Assign x[i] to the rank which is responsible for the range\n  // // [x[i] - 0.1, x[i]].\n  // // Each rank has the same range.\n  // // For example, rank 0 gets x[0], x[10],..., x[90].\n  // // rank 1 gets x[1], x[11],..., x[91].\n  // // rank 2 gets x[2], x[12],..., x[92].\n  // //...\n  // // rank 9 gets x[9], x[19],..., x[99].\n  // auto range_begin = begin + rank * (x.size() / size);\n  // auto range_end = begin + (rank + 1) * (x.size() / size);\n\n  // for (auto itr = range_begin; itr!= range_end; ++itr) {\n  //   auto num = static_cast<size_t>(std::floor(*itr / 10) * 10);\n  //   bins[num] += 1;\n  // }\n\n  // // Each rank has a complete copy of x.\n  // // Each rank has a partial count of x.\n  // // Each rank has a partial sum.\n  // // The total number of counts is obtained by adding them together.\n  // std::vector<size_t> sum(size);\n  // MPI_Reduce(bins.data(), sum.data(), size, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // if (rank == 0) {\n  //   for (int i = 0; i < size; ++i) {\n  //     for (int j = 0; j < 10; ++j) {\n  //       bins[j] += sum[i];\n  //     }\n  //   }\n  // }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "double max = 100;\n  double min = 0;\n\n  // number of values in [0, 10), [10, 20), [20, 30)...\n  std::array<int, 10> counts;\n  int sum = 0;\n\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int chunk_size = x.size() / world_size;\n  int chunk_start = world_rank * chunk_size;\n  int chunk_end = (world_rank == world_size - 1)? x.size() : chunk_start + chunk_size;\n  int local_size = chunk_end - chunk_start;\n\n  // local sum\n  for (int i = 0; i < local_size; ++i) {\n    if (x[i + chunk_start] >= min && x[i + chunk_start] < max) {\n      counts[static_cast<int>((x[i + chunk_start] - min) / 10)] += 1;\n      sum += 1;\n    }\n  }\n\n  // sum across all ranks\n  std::array<int, 10> all_sums;\n  MPI_Reduce(counts.data(), all_sums.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] = static_cast<size_t>(all_sums[i]);\n    }\n  }\n}",
            "double chunk_size = (double)x.size() / (double)MPI_COMM_WORLD->size;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<double> chunk(x.begin() + rank * chunk_size, x.begin() + (rank + 1) * chunk_size);\n\n  std::vector<double> bins_local;\n  bins_local.resize(10, 0);\n\n  std::cout << \"Rank: \" << rank << \" Size: \" << size << std::endl;\n\n  for (double value : chunk) {\n    if (value >= 0.0 && value < 10.0) {\n      bins_local[0] += 1.0;\n    } else if (value >= 10.0 && value < 20.0) {\n      bins_local[1] += 1.0;\n    } else if (value >= 20.0 && value < 30.0) {\n      bins_local[2] += 1.0;\n    } else if (value >= 30.0 && value < 40.0) {\n      bins_local[3] += 1.0;\n    } else if (value >= 40.0 && value < 50.0) {\n      bins_local[4] += 1.0;\n    } else if (value >= 50.0 && value < 60.0) {\n      bins_local[5] += 1.0;\n    } else if (value >= 60.0 && value < 70.0) {\n      bins_local[6] += 1.0;\n    } else if (value >= 70.0 && value < 80.0) {\n      bins_local[7] += 1.0;\n    } else if (value >= 80.0 && value < 90.0) {\n      bins_local[8] += 1.0;\n    } else if (value >= 90.0 && value <= 100.0) {\n      bins_local[9] += 1.0;\n    }\n  }\n\n  std::vector<size_t> bins_mpi(10, 0);\n  MPI_Reduce(bins_local.data(), bins_mpi.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; i++) {\n      bins[i] = bins_mpi[i];\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "double lower = 0;\n  double upper = 10;\n  int numBins = 10;\n  // Do your work here...\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO\n\n}",
            "// TODO\n}",
            "// TODO: compute `bins`\n}",
            "// Fill bins here.  See code template in parallel-histogram.cc.\n}",
            "// TODO: implement me!\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    std::vector<double> x_local(x);\n    if (world_rank == 0) {\n        size_t global_size = x.size();\n        MPI_Bcast(&global_size, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        x_local.resize(global_size);\n        MPI_Scatter(x.data(), global_size, MPI_DOUBLE, x_local.data(), global_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Scatter(x.data(), x.size(), MPI_DOUBLE, x_local.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n\n    std::array<size_t, 10> bins_local{};\n    int my_count{};\n    for (double v : x_local) {\n        if (v >= 0 && v <= 100) {\n            bins_local[static_cast<size_t>((v / 10))]++;\n            my_count++;\n        }\n    }\n\n    if (world_rank == 0) {\n        for (int i = 1; i < world_size; ++i) {\n            size_t count;\n            MPI_Status status{};\n            MPI_Recv(&count, 1, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 10; ++j) {\n                bins[j] += count;\n            }\n        }\n    } else {\n        MPI_Send(&my_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        MPI_Send(bins_local.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// FIXME: fill in this function!\n  int rank;\n  int size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double start = 0;\n  double end = 100;\n  double interval = (end - start) / size;\n  double min = start + rank * interval;\n  double max = min + interval;\n\n  int my_count = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= min && x[i] < max) {\n      my_count++;\n    }\n  }\n\n  int recv_counts[size];\n  MPI_Gather(&my_count, 1, MPI_INT, recv_counts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < recv_counts.size(); ++i) {\n      bins[i] = recv_counts[i];\n    }\n  }\n}",
            "// Your code here\n    size_t len = x.size();\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    std::vector<double> local_data(len);\n    if (rank == 0) {\n        // divide the vector\n        size_t quotient = len / num_ranks;\n        size_t remainder = len % num_ranks;\n        std::vector<double> sub_vec(quotient);\n        for (int i = 0; i < num_ranks; ++i) {\n            for (size_t j = 0; j < quotient; ++j) {\n                sub_vec[j] = x[i * quotient + j];\n            }\n            local_data = sub_vec;\n            if (i < remainder) {\n                local_data.push_back(x[num_ranks * quotient + i]);\n            }\n            if (i == num_ranks - 1) {\n                local_data.resize(len);\n            }\n            MPI_Send(&local_data[0], local_data.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&local_data[0], len, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // calculate\n    size_t start = (rank * 10);\n    size_t end = ((rank + 1) * 10);\n    for (size_t i = start; i < end; ++i) {\n        bins[i % 10] += std::count_if(local_data.begin(), local_data.end(), [=](double n){\n            return n >= i && n < (i + 10);\n        });\n    }\n\n    // gather results\n    if (rank == 0) {\n        for (int i = 1; i < num_ranks; ++i) {\n            std::vector<double> recv_vec(10);\n            MPI_Status status;\n            MPI_Recv(&recv_vec[0], 10, MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n            for (int j = 0; j < 10; ++j) {\n                bins[j] += recv_vec[j];\n            }\n        }\n    }\n    else {\n        MPI_Send(&bins[0], 10, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int num_per_rank = x.size() / size;\n    int start = rank * num_per_rank;\n    int end = (rank + 1) * num_per_rank;\n    std::vector<double> rank_x(x.begin() + start, x.begin() + end);\n    std::array<size_t, 10> rank_bins{};\n    for (double i : rank_x) {\n        int index = std::min(i / 10, 9);\n        rank_bins[index]++;\n    }\n    std::array<size_t, 10> reduced_bins{};\n    MPI_Reduce(rank_bins.data(), reduced_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins = reduced_bins;\n    }\n}",
            "/* Your code here */\n}",
            "// TODO: implement this function\n\n  return;\n\n}",
            "// TODO\n}",
            "// Do not edit this function\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t N = x.size();\n  int const P = 4;  // number of processors\n  int const R = N % P == 0? N / P : N / P + 1;  // number of rows\n  int const C = 10;  // number of columns\n  int const W = 1;  // number of rows for each processor\n  int const W1 = N % P == 0? N / P : N / P + 1;  // number of columns for each processor\n  int const r0 = rank * R; // the first row index for this processor\n  int const r1 = r0 + R; // the last row index for this processor\n  int const c0 = rank * W1; // the first column index for this processor\n  int const c1 = c0 + W1; // the last column index for this processor\n  std::vector<double> x_ = std::vector<double>(x.begin() + c0, x.begin() + c1);\n  std::array<size_t, 10> bins_;\n  for (size_t i = 0; i < 10; ++i) {\n    size_t count = 0;\n    for (size_t j = 0; j < x_.size(); ++j) {\n      double value = x_[j];\n      if (value >= i * 10 && value < (i + 1) * 10) {\n        ++count;\n      }\n    }\n    bins_[i] = count;\n  }\n  MPI_Reduce(bins_.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  const int numprocs = get_num_procs(MPI_COMM_WORLD);\n  const size_t numvalues = x.size();\n  const size_t chunksize = numvalues / numprocs;\n\n  std::array<size_t, 10> local_bins{};\n\n  for (int i = 0; i < 10; ++i) {\n    local_bins[i] = 0;\n  }\n\n  for (size_t i = rank*chunksize; i < (rank+1)*chunksize; ++i) {\n    if (x[i] < 10.0) {\n      ++local_bins[static_cast<int>(x[i])];\n    }\n  }\n\n  // TODO: Add your solution code here.\n}",
            "// write your solution here\n}",
            "// TODO:\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<double> x_local;\n  if (rank == 0) {\n    x_local = x;\n  }\n  int recvcount = x_local.size() / size;\n  int displs = recvcount * rank;\n  MPI_Scatter(x_local.data(), recvcount, MPI_DOUBLE, x_local.data(), recvcount, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<size_t> bins_local(10, 0);\n  for (auto& elem : x_local) {\n    bins_local[int(elem / 10)] += 1;\n  }\n  MPI_Gather(bins_local.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // compute my portion of the data\n    size_t length = x.size() / size;\n    std::vector<double> local_data = {x.begin() + rank * length, x.begin() + rank * length + length};\n\n    // compute my count for each bin\n    std::array<size_t, 10> local_bins;\n    for (auto const& x : local_data) {\n        if (x < 10) {\n            local_bins[0]++;\n        } else if (x < 20) {\n            local_bins[1]++;\n        } else if (x < 30) {\n            local_bins[2]++;\n        } else if (x < 40) {\n            local_bins[3]++;\n        } else if (x < 50) {\n            local_bins[4]++;\n        } else if (x < 60) {\n            local_bins[5]++;\n        } else if (x < 70) {\n            local_bins[6]++;\n        } else if (x < 80) {\n            local_bins[7]++;\n        } else if (x < 90) {\n            local_bins[8]++;\n        } else {\n            local_bins[9]++;\n        }\n    }\n\n    // sum the counts of each bin\n    std::array<size_t, 10> counts;\n    MPI_Reduce(&local_bins, &counts, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // save the result\n    if (rank == 0) {\n        bins = counts;\n    }\n}",
            "size_t const n = x.size();\n  assert(n >= 0);\n  std::vector<size_t> counts(n, 0);\n  assert(counts.size() >= 0);\n  // Each process has its own copy of x.\n  std::vector<double> local_x(x);\n  // Each process has a complete copy of bins.\n  std::array<size_t, 10> local_bins = {};\n\n  // Compute a histogram of local_x.\n  for (size_t i = 0; i < n; ++i) {\n    ++local_bins[(size_t)(local_x[i] / 10.0)];\n  }\n\n  // Combine the histograms in parallel.\n  MPI_Reduce(counts.data(), local_bins.data(), counts.size(),\n             MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Send bins back to rank 0.\n  MPI_Bcast(local_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  if (0 == rank()) {\n    bins = local_bins;\n  }\n}",
            "int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        std::fill(bins.begin(), bins.end(), 0);\n    }\n    MPI_Scatter(x.data(), x.size(), MPI_DOUBLE, bins.data(), bins.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    auto local_bins = bins;\n    MPI_Gather(local_bins.data(), bins.size(), MPI_DOUBLE, bins.data(), bins.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    size_t size = x.size();\n\n    int left = size / world_size * world_rank;\n    int right = left + size / world_size;\n\n    if (world_rank == world_size - 1)\n        right = size;\n\n    int local_count = 0;\n    for (int i = left; i < right; i++) {\n        if (x[i] < 10)\n            local_count++;\n    }\n\n    std::vector<int> local_bins(10);\n    for (int i = 0; i < 10; i++)\n        local_bins[i] = 0;\n\n    MPI_Reduce(&local_count, local_bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = local_bins[i];\n        }\n    }\n}",
            "// TODO\n    MPI_Bcast(&bins, 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement the missing code below\n\n    // MPI_Datatype is a data type used for messages. It can be composed of several types of other datatypes.\n    // See https://www.mpi-forum.org/docs/mpi-2.2/mpi22-report/node229.htm\n    // The first type is the type of the data.\n    // The second type is the number of items to be received.\n    // The third type is the number of items to be sent.\n    MPI_Datatype type;\n    // TODO: fill in the code for the first argument. It is the type of the first argument to MPI_Type_vector.\n    MPI_Type_contiguous(x.size(), MPI_DOUBLE, &type);\n    // TODO: fill in the code for the second and third arguments. They are the types of the second and third arguments to MPI_Type_vector.\n    MPI_Type_commit(&type);\n\n    // TODO: fill in the code for the first argument. It is the type of the first argument to MPI_Scatter.\n    MPI_Scatter(x.data(), 1, type, bins.data(), 1, type, 0, MPI_COMM_WORLD);\n\n    // TODO: fill in the code for the first argument. It is the type of the first argument to MPI_Bcast.\n    MPI_Bcast(bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // TODO: fill in the code for the first argument. It is the type of the first argument to MPI_Type_free.\n    MPI_Type_free(&type);\n}",
            "std::vector<size_t> local_bins(10, 0);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n_local = x.size() / size;\n  double min_local = 0;\n  double max_local = 100;\n  if (rank == 0) {\n    if (x.size() % size!= 0) {\n      n_local++;\n    } else {\n      min_local = x[0];\n      max_local = x[x.size() - n_local];\n    }\n  }\n  MPI_Bcast(&min_local, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&max_local, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<double> x_local(n_local);\n  MPI_Scatter(&x[0], n_local, MPI_DOUBLE, &x_local[0], n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  for (size_t i = 0; i < n_local; i++) {\n    if (x_local[i] >= min_local && x_local[i] < max_local) {\n      size_t index = (x_local[i] - min_local) / 10;\n      local_bins[index]++;\n    }\n  }\n  std::vector<size_t> global_bins(10);\n  MPI_Reduce(&local_bins[0], &global_bins[0], 10, MPI_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&global_bins[0], 10, MPI_LONG_LONG, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; i++) {\n      bins[i] = global_bins[i];\n    }\n  }\n}",
            "std::array<int, 10> count;\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Bcast(count.data(), count.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank!= 0) {\n    MPI_Bcast(bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  } else {\n    for (int i = 0; i < x.size(); ++i) {\n      int index = x[i] / 10.0;\n      if (0 <= index && index < count.size()) {\n        ++count[index];\n      }\n    }\n    for (int i = 1; i < size; ++i) {\n      MPI_Send(count.data(), count.size(), MPI_INT, i, 0, MPI_COMM_WORLD);\n    }\n    for (int i = 0; i < count.size(); ++i) {\n      bins[i] = count[i];\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int left = rank == 0? 0 : rank * x.size() / size;\n    int right = rank == size - 1? x.size() : (rank + 1) * x.size() / size;\n    int size_local = right - left;\n\n    std::vector<double> x_local(size_local);\n    std::copy(x.begin() + left, x.begin() + right, x_local.begin());\n\n    std::array<size_t, 10> bins_local;\n    for (auto const& x : x_local) {\n        if (x < 10)\n            ++bins_local[0];\n        else if (x < 20)\n            ++bins_local[1];\n        else if (x < 30)\n            ++bins_local[2];\n        else if (x < 40)\n            ++bins_local[3];\n        else if (x < 50)\n            ++bins_local[4];\n        else if (x < 60)\n            ++bins_local[5];\n        else if (x < 70)\n            ++bins_local[6];\n        else if (x < 80)\n            ++bins_local[7];\n        else if (x < 90)\n            ++bins_local[8];\n        else\n            ++bins_local[9];\n    }\n\n    MPI_Gather(&bins_local, 10, MPI_UNSIGNED_LONG_LONG, &bins, 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n\n  // Divide the work\n  int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int local_n = n / size;\n  int remainder = n - local_n * size;\n  int offset = local_n * my_rank + std::min(my_rank, remainder);\n\n  // Create a vector for this rank's work\n  std::vector<double> local_x;\n  local_x.assign(x.begin() + offset, x.begin() + offset + local_n);\n\n  // Create a histogram for this rank's work\n  std::array<size_t, 10> local_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  // Do the work\n  for (double value : local_x) {\n    if (value < 10) {\n      ++local_bins[0];\n    } else if (value < 20) {\n      ++local_bins[1];\n    } else if (value < 30) {\n      ++local_bins[2];\n    } else if (value < 40) {\n      ++local_bins[3];\n    } else if (value < 50) {\n      ++local_bins[4];\n    } else if (value < 60) {\n      ++local_bins[5];\n    } else if (value < 70) {\n      ++local_bins[6];\n    } else if (value < 80) {\n      ++local_bins[7];\n    } else if (value < 90) {\n      ++local_bins[8];\n    } else {\n      ++local_bins[9];\n    }\n  }\n\n  // Gather the results\n  MPI_Gather(local_bins.data(), 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // TODO: Implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // This will be the size of the subvector I'm responsible for\n  int sliceSize = x.size() / size;\n\n  // This will be the index in x that marks the start of the subvector I'm\n  // responsible for\n  int startIdx = rank * sliceSize;\n\n  std::vector<double> mySlice(x.begin() + startIdx,\n                              x.begin() + startIdx + sliceSize);\n\n  std::vector<size_t> localBins(bins);\n\n  std::fill(localBins.begin(), localBins.end(), 0);\n\n  for (size_t i = 0; i < mySlice.size(); i++) {\n    size_t idx = mySlice[i] / 10;\n    localBins[idx]++;\n  }\n\n  MPI_Reduce(localBins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n}",
            "/* TODO */\n}",
            "size_t n = x.size();\n  if(n > 0) {\n    std::vector<double> local_x;\n    size_t n_local = n / MPI_Size;\n    for(int i=0; i<MPI_Rank; ++i) {\n      local_x.insert(local_x.end(), x.begin() + n_local*i, x.begin() + n_local*(i+1));\n    }\n    local_x.insert(local_x.end(), x.begin() + n_local*MPI_Rank, x.end());\n    bins.fill(0);\n    for(auto const& i : local_x) {\n      if (i < 10) {\n        ++bins[static_cast<int>(i)];\n      }\n    }\n    int ierr;\n    ierr = MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(ierr!= MPI_SUCCESS) {\n      throw std::runtime_error(\"MPI error\");\n    }\n  }\n  int ierr;\n  ierr = MPI_Barrier(MPI_COMM_WORLD);\n  if(ierr!= MPI_SUCCESS) {\n    throw std::runtime_error(\"MPI error\");\n  }\n}",
            "// TODO: implement me\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int len_x = x.size();\n    int len_bins = bins.size();\n    int s = len_x / size;\n    int res = len_x % size;\n\n    int local_start = rank * s;\n    if (rank < res) {\n        local_start += rank;\n    }\n    int local_end = local_start + s + 1;\n    if (rank == size - 1) {\n        local_end += res;\n    }\n    int local_len = local_end - local_start;\n\n    std::vector<double> local_x;\n    for (int i = local_start; i < local_end; i++) {\n        local_x.push_back(x[i]);\n    }\n    std::vector<int> local_bin;\n    int local_len_bin = 0;\n    for (int i = 0; i < local_len; i++) {\n        if (local_x[i] < 10) {\n            local_bin.push_back(0);\n        } else if (local_x[i] < 20) {\n            local_bin.push_back(1);\n            local_len_bin++;\n        } else if (local_x[i] < 30) {\n            local_bin[local_len_bin - 1]++;\n        } else if (local_x[i] < 40) {\n            local_bin.push_back(2);\n            local_len_bin++;\n        } else if (local_x[i] < 50) {\n            local_bin[local_len_bin - 1]++;\n        } else if (local_x[i] < 60) {\n            local_bin.push_back(3);\n            local_len_bin++;\n        } else if (local_x[i] < 70) {\n            local_bin[local_len_bin - 1]++;\n        } else if (local_x[i] < 80) {\n            local_bin.push_back(4);\n            local_len_bin++;\n        } else if (local_x[i] < 90) {\n            local_bin[local_len_bin - 1]++;\n        } else if (local_x[i] < 100) {\n            local_bin.push_back(5);\n            local_len_bin++;\n        }\n    }\n    // all ranks have the same number of bins\n    std::vector<int> global_bin(len_bins * size, 0);\n    MPI_Allgather(&local_bin[0], local_len_bin, MPI_INT, global_bin.data(), local_len_bin, MPI_INT, MPI_COMM_WORLD);\n    std::vector<int> rank_bin(len_bins, 0);\n    for (int i = 0; i < len_bins; i++) {\n        for (int j = 0; j < size; j++) {\n            rank_bin[i] += global_bin[i + j * len_bins];\n        }\n    }\n    for (int i = 0; i < len_bins; i++) {\n        bins[i] = rank_bin[i];\n    }\n}",
            "int my_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  size_t my_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &my_size);\n\n  // local variables\n  size_t i = 0;\n  size_t sum = 0;\n  size_t start = 10*my_rank;\n  size_t end = start + 10;\n\n  while (i < x.size()) {\n    if (x[i] >= start && x[i] < end) {\n      sum++;\n      i++;\n    }\n    else {\n      i++;\n    }\n  }\n\n  MPI_Reduce(&sum, &bins[0], 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  std::vector<double> x_local(x);\n  int num_points = x_local.size() / world_size;\n  if (rank == 0) {\n    x_local.erase(x_local.begin(), x_local.begin() + num_points);\n  } else {\n    x_local.erase(x_local.begin() + num_points, x_local.end());\n  }\n\n  std::array<size_t, 10> bins_local;\n  std::fill(bins_local.begin(), bins_local.end(), 0);\n  for (size_t i = 0; i < x_local.size(); ++i) {\n    if (x_local[i] >= 0 && x_local[i] < 10) {\n      ++bins_local[x_local[i]];\n    }\n  }\n\n  MPI_Reduce(bins_local.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype MPI_DOUBLE;\n  MPI_Type_contiguous(1, MPI_DOUBLE, &MPI_DOUBLE);\n  MPI_Type_commit(&MPI_DOUBLE);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t N = x.size();\n  std::vector<double> x_local(N);\n  MPI_Scatter(&x[0], N, MPI_DOUBLE, &x_local[0], N, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 10> bins_local;\n  for (size_t i = 0; i < N; ++i) {\n    int idx = static_cast<int>((x_local[i] / 10.0) * 10);\n    if (idx >= 0 && idx < 10) {\n      bins_local[idx]++;\n    }\n  }\n\n  MPI_Reduce(&bins_local[0], &bins[0], bins_local.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  MPI_Type_free(&MPI_DOUBLE);\n}",
            "int rank = 0, size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<int> counts(size, 0);\n  std::vector<int> displs(size, 0);\n\n  double min = *std::min_element(x.cbegin(), x.cend());\n  double max = *std::max_element(x.cbegin(), x.cend());\n\n  double partitionSize = (max - min) / size;\n\n  // Find bin of each element in x\n  for (int i = 0; i < x.size(); i++) {\n    int bin = (x[i] - min) / partitionSize;\n    counts[bin] += 1;\n  }\n\n  // Sum up counts from each rank\n  MPI_Alltoall(counts.data(), 1, MPI_INT, counts.data(), 1, MPI_INT,\n               MPI_COMM_WORLD);\n\n  // Get displacements\n  displs[0] = 0;\n  for (int i = 1; i < size; i++) {\n    displs[i] = displs[i - 1] + counts[i - 1];\n  }\n\n  // Get bins\n  MPI_Gatherv(counts.data(), size, MPI_INT, bins.data(), counts.data(),\n              displs.data(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Only rank 0 should set bins to 0 for bins with no counts\n  if (rank == 0) {\n    for (int i = 0; i < displs[size - 1]; i++) {\n      if (bins[i] == 0) {\n        bins[i] = 0;\n      }\n    }\n  }\n}",
            "double lowerLimit = 0;\n    double upperLimit = 10;\n    std::fill(bins.begin(), bins.end(), 0);\n\n    size_t local_size = x.size();\n    std::vector<double> my_x(local_size);\n    std::vector<size_t> local_bins(10);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t rem = local_size % size;\n    size_t size_local = (rem == 0)? local_size / size : local_size / size + 1;\n    size_t start = rank * size_local;\n    size_t end = start + size_local;\n\n    std::copy(x.begin() + start, x.begin() + end, my_x.begin());\n\n    for (double value : my_x) {\n        int bin = (value < upperLimit && value >= lowerLimit)? floor(value / 10) : 10;\n        local_bins[bin] += 1;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO: Write a parallel version of this function.\n  //HINT: use MPI_Scatter() to distribute x to each rank.\n  //HINT: use MPI_Reduce() to combine each rank's counts into bins on rank 0.\n\n  // Initialize the bins array to 0.\n  for (auto &i : bins) {\n    i = 0;\n  }\n\n  // Get the rank and the number of processes\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int nprocs;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  // Get the length of x for this process\n  int nlocal = x.size() / nprocs;\n  int nrem = x.size() % nprocs;\n  int first = nlocal * rank;\n  int last = first + nlocal + (rank < nrem? 1 : 0);\n\n  // Allocate vector x for each process\n  std::vector<double> xlocal(x.begin() + first, x.begin() + last);\n\n  // Scatter values of x onto each process\n  MPI_Scatter(xlocal.data(), nlocal, MPI_DOUBLE, x.data(), nlocal, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Calculate the number of values in each bin\n  for (size_t i = 0; i < x.size(); i++) {\n    int bin = x[i] / 10;\n    bins[bin]++;\n  }\n\n  // Collect the result from each process\n  MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// rank 0 computes\n    if (x.size() < 10) {\n        // not enough data for any rank\n        bins = std::array<size_t, 10>{0};\n        return;\n    }\n    std::array<size_t, 10> local_bins{};\n    for (size_t i = 0; i < 10; i++) {\n        local_bins[i] = std::count_if(x.begin(), x.end(), [i](double x) {\n            return x >= i*10 && x < (i+1)*10;\n        });\n    }\n    bins = local_bins;\n}",
            "// TODO: implement this method\n    int nRanks, myRank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    size_t size = x.size();\n    size_t sliceSize = size / nRanks;\n    std::vector<size_t> slice(sliceSize);\n    if (myRank == 0) {\n        for (int i = 1; i < nRanks; i++) {\n            std::vector<size_t> recv_counts(10, 0);\n            MPI_Status status;\n            MPI_Recv(recv_counts.data(), 10, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n            for (size_t j = 0; j < 10; ++j) {\n                bins[j] += recv_counts[j];\n            }\n        }\n    }\n    if (size % nRanks == 0) {\n        std::vector<size_t> counts(10, 0);\n        for (int i = 0; i < sliceSize; i++) {\n            counts[(x[i] / 10)]++;\n        }\n        MPI_Send(counts.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n    else {\n        std::vector<size_t> counts(10, 0);\n        for (int i = 0; i < sliceSize; i++) {\n            counts[(x[i] / 10)]++;\n        }\n        MPI_Send(counts.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n        size_t sendCounts = size - sliceSize * (nRanks - 1);\n        for (int i = 0; i < sendCounts; i++) {\n            counts[(x[sliceSize + i] / 10)]++;\n        }\n        MPI_Send(counts.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "size_t N = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // TODO: implement\n}",
            "if (x.empty()) {\n    return;\n  }\n\n  const int rank = 0;\n  const int size = 0;\n\n  std::vector<double> x_ranks(x.size());\n  MPI_Scatter(x.data(), x.size(), MPI_DOUBLE, x_ranks.data(), x.size(), MPI_DOUBLE, rank, MPI_COMM_WORLD);\n\n  std::vector<size_t> bins_ranks(bins.size());\n  for (auto& bin : bins_ranks) {\n    bin = 0;\n  }\n\n  for (auto& x_rank : x_ranks) {\n    if (x_rank < 10) {\n      bins_ranks[x_rank]++;\n    }\n  }\n\n  MPI_Gather(bins_ranks.data(), bins_ranks.size(), MPI_UNSIGNED_LONG, bins.data(), bins_ranks.size(), MPI_UNSIGNED_LONG, rank, MPI_COMM_WORLD);\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // get size of x on current rank\n    size_t size = x.size() / world_size;\n    if (world_rank == world_size - 1) {\n        size += x.size() % world_size;\n    }\n\n    std::vector<size_t> local_bins(10, 0);\n\n    // calculate how many elements will be processed by each rank\n    int n_local = size / 10;\n    if (world_rank == world_size - 1) {\n        n_local += size % 10;\n    }\n\n    // calculate the number of elements to skip\n    int n_skip = size - n_local;\n\n    // calculate first index of the current rank\n    int first_idx = n_skip + (n_local * world_rank);\n\n    // compute local bins\n    for (int i = 0; i < n_local; i++) {\n        local_bins[x[i + first_idx] / 10] += 1;\n    }\n\n    // reduce\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype vector_type, bins_type;\n  MPI_Type_vector(10, 1, 10, MPI_DOUBLE, &vector_type);\n  MPI_Type_contiguous(10, MPI_UNSIGNED_LONG, &bins_type);\n  MPI_Type_commit(&vector_type);\n  MPI_Type_commit(&bins_type);\n\n  // Set up data structures\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n  MPI_Aint lb, extent;\n  MPI_Type_get_extent(MPI_DOUBLE, &lb, &extent);\n  MPI_Aint offset;\n  MPI_Type_get_true_extent(MPI_DOUBLE, &lb, &extent);\n  MPI_Type_get_extent(bins_type, &lb, &extent);\n  MPI_Type_get_extent(vector_type, &lb, &extent);\n  MPI_Type_get_true_extent(vector_type, &lb, &extent);\n  MPI_Type_get_true_extent(bins_type, &lb, &extent);\n  if (comm_rank == 0) {\n    offset = extent - lb;\n  } else {\n    offset = 0;\n  }\n  std::vector<double> x_local(10);\n  std::array<unsigned long, 10> bins_local;\n  std::vector<double> x_all(10 * comm_size);\n  std::array<unsigned long, 10 * comm_size> bins_all;\n  MPI_Datatype vector_type_all, bins_type_all;\n  MPI_Type_create_resized(vector_type, offset, 0, &vector_type_all);\n  MPI_Type_commit(&vector_type_all);\n  MPI_Type_create_resized(bins_type, offset, 0, &bins_type_all);\n  MPI_Type_commit(&bins_type_all);\n  MPI_Scatter(x.data(), 10, vector_type, x_local.data(), 10, vector_type_all, 0, MPI_COMM_WORLD);\n  MPI_Scatter(bins.data(), 10, bins_type, bins_local.data(), 10, bins_type_all, 0, MPI_COMM_WORLD);\n  MPI_Gather(x_local.data(), 10, vector_type, x_all.data(), 10, vector_type_all, 0, MPI_COMM_WORLD);\n  MPI_Gather(bins_local.data(), 10, bins_type, bins_all.data(), 10, bins_type_all, 0, MPI_COMM_WORLD);\n\n  // Find values in the range [0,10) and count them\n  for (double i = 0; i < 10; i++) {\n    bins_all[i] = std::count_if(x_all.begin() + i, x_all.end(), [i](double x) { return x < i + 10; });\n  }\n\n  // Send results back to rank 0\n  MPI_Scatter(bins_all.data(), 10, bins_type_all, bins.data(), 10, bins_type, 0, MPI_COMM_WORLD);\n  MPI_Type_free(&vector_type_all);\n  MPI_Type_free(&bins_type_all);\n  MPI_Type_free(&vector_type);\n  MPI_Type_free(&bins_type);\n}",
            "double range = 100;\n    std::vector<int> count(10, 0);\n\n    // Your code here.\n    double myCount = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] <= 10 * range && x[i] >= 0) {\n            myCount += 1;\n        }\n    }\n    for (int i = 0; i < 10; i++) {\n        bins[i] = myCount;\n    }\n    MPI_Reduce(&myCount, &bins[0], 10, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n    // End of your code.\n\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> local_x = x;\n    int local_size = local_x.size();\n\n    MPI_Scatter(&local_size, 1, MPI_INT, &local_size, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatter(local_x.data(), local_size, MPI_DOUBLE, local_x.data(), local_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 10> local_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n    for (auto v : local_x) {\n        ++local_bins[(int)(v / 10.0)];\n    }\n\n    MPI_Gather(local_bins.data(), 10, MPI_INT, bins.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n    std::vector<size_t> binsLocal(10);\n\n    // compute the number of values in each bin\n    for (size_t i = 0; i < n; i++) {\n        size_t idx = std::floor(x[i]/10);\n        if (idx >= 10) {\n            idx = 9;\n        }\n        binsLocal[idx]++;\n    }\n\n    // send the data to the master\n    MPI_Reduce(binsLocal.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "const size_t size = x.size();\n  std::vector<double> x_local(size);\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  MPI_Scatter(&x[0], size / size, MPI_DOUBLE,\n              &x_local[0], size / size, MPI_DOUBLE,\n              0, MPI_COMM_WORLD);\n\n  // local result\n  std::array<size_t, 10> local_bins;\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  for (double const& value : x_local) {\n    if (value < 10.0) {\n      ++local_bins[0];\n    }\n    else if (value < 20.0) {\n      ++local_bins[1];\n    }\n    else if (value < 30.0) {\n      ++local_bins[2];\n    }\n    else if (value < 40.0) {\n      ++local_bins[3];\n    }\n    else if (value < 50.0) {\n      ++local_bins[4];\n    }\n    else if (value < 60.0) {\n      ++local_bins[5];\n    }\n    else if (value < 70.0) {\n      ++local_bins[6];\n    }\n    else if (value < 80.0) {\n      ++local_bins[7];\n    }\n    else if (value < 90.0) {\n      ++local_bins[8];\n    }\n    else {\n      ++local_bins[9];\n    }\n  }\n\n  // MPI reduction\n  MPI_Reduce(\n    &local_bins[0], &bins[0],\n    local_bins.size(), MPI_UNSIGNED_LONG_LONG,\n    MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t size = x.size();\n    size_t rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Each rank has a complete copy of x\n    std::vector<double> local_x = x;\n\n    // Divide up the values into equal bins\n    size_t bin_size = size / 10;\n    std::vector<size_t> local_counts(10);\n    for (size_t i = 0; i < bin_size; ++i) {\n        for (size_t j = 0; j < 10; ++j) {\n            size_t index = j * bin_size + i;\n            if (index < size) {\n                local_counts[j]++;\n            }\n        }\n    }\n    // Collect local results\n    std::vector<size_t> counts(10);\n    MPI_Reduce(local_counts.data(), counts.data(), 10, MPI_UNSIGNED_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // Store the results in the correct bins\n    for (size_t i = 0; i < bin_size; ++i) {\n        for (size_t j = 0; j < 10; ++j) {\n            size_t index = j * bin_size + i;\n            if (index < size) {\n                bins[j]++;\n            }\n        }\n    }\n    // Collect all results from ranks into the correct bins\n    if (rank == 0) {\n        for (size_t i = 1; i < size; ++i) {\n            std::vector<size_t> temp(10);\n            MPI_Recv(temp.data(), 10, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < 10; ++j) {\n                bins[j] += temp[j];\n            }\n        }\n    } else {\n        MPI_Send(counts.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Write this function!\n\n}",
            "int world_rank, world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // TODO: Your code goes here!\n}",
            "//TODO: compute bins, assume `x` has been divied by the number of processes\n    //      and that `bins` has been initialized to all zeros\n}",
            "// Compute local counts\n  std::array<size_t, 10> counts;\n  for (auto const& value : x) {\n    size_t index = value / 10;\n    counts[index]++;\n  }\n\n  // Add local counts to global counts\n  MPI_Reduce(counts.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "std::cout << \"not implemented yet\" << std::endl;\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins.size());\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::array<size_t, 10> counts{0};\n\n  if (rank == 0) {\n    for (int i = 0; i < size; ++i) {\n      MPI_Status status;\n      MPI_Recv(&counts[0], counts.size(), MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    for (auto value : x) {\n      if (value < 10) {\n        ++counts[value];\n      }\n    }\n    MPI_Send(&counts[0], counts.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, commSize;\n    MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int *counts = new int[commSize];\n    int *displs = new int[commSize];\n\n    if(rank == 0) {\n        std::vector<double> local_x(x.size()/commSize);\n        std::copy(x.begin(), x.begin() + x.size()/commSize, local_x.begin());\n        MPI_Scatter(x.data(), x.size()/commSize, MPI_DOUBLE, local_x.data(), x.size()/commSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        std::sort(local_x.begin(), local_x.end());\n        counts[0] = std::count_if(local_x.begin(), local_x.end(), [](double i) { return i < 10; });\n        displs[0] = 0;\n        int i = 1;\n        for(; i < commSize; i++) {\n            counts[i] = std::count_if(local_x.begin() + displs[i-1], local_x.begin() + displs[i-1] + x.size()/commSize, [](double i) { return i < 10; });\n            displs[i] = displs[i-1] + x.size()/commSize;\n        }\n    } else {\n        MPI_Scatter(x.data(), x.size()/commSize, MPI_DOUBLE, NULL, x.size()/commSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n    MPI_Gatherv(counts, 1, MPI_INT, counts, counts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n    if(rank == 0) {\n        for(int i = 0; i < 10; i++) {\n            bins[i] = 0;\n            for(int j = 0; j < commSize; j++) {\n                bins[i] += counts[j];\n            }\n        }\n    }\n    delete[] counts;\n    delete[] displs;\n}",
            "// Your code here.\n}",
            "MPI_Datatype MPI_DOUBLE_TYPE;\n    MPI_Type_contiguous(sizeof(double), MPI_BYTE, &MPI_DOUBLE_TYPE);\n    MPI_Type_commit(&MPI_DOUBLE_TYPE);\n\n    size_t local_size = x.size();\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // calculate the size of each local array\n    size_t local_first = (x.size() / num_ranks) * rank;\n    size_t local_last = local_first + (x.size() / num_ranks);\n    if (rank == num_ranks - 1) {\n        local_last = x.size();\n    }\n\n    // get the counts of each local array\n    std::vector<int> local_counts(local_last - local_first);\n    MPI_Gather(&local_size, 1, MPI_INT, local_counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // get the total number of values\n    int global_size;\n    MPI_Reduce(&local_size, &global_size, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // get the counts of the values in [0,10), [10, 20), [20, 30),...\n    std::vector<int> local_bins(10, 0);\n    for (size_t i = 0; i < local_last - local_first; i++) {\n        for (size_t j = 0; j < local_counts[i]; j++) {\n            if (x[local_first + i] < 10) {\n                local_bins[0]++;\n            } else if (x[local_first + i] < 20) {\n                local_bins[1]++;\n            } else if (x[local_first + i] < 30) {\n                local_bins[2]++;\n            } else if (x[local_first + i] < 40) {\n                local_bins[3]++;\n            } else if (x[local_first + i] < 50) {\n                local_bins[4]++;\n            } else if (x[local_first + i] < 60) {\n                local_bins[5]++;\n            } else if (x[local_first + i] < 70) {\n                local_bins[6]++;\n            } else if (x[local_first + i] < 80) {\n                local_bins[7]++;\n            } else if (x[local_first + i] < 90) {\n                local_bins[8]++;\n            } else {\n                local_bins[9]++;\n            }\n        }\n    }\n\n    // get the counts of the values in [0,10), [10, 20), [20, 30),... from all ranks\n    std::vector<int> global_bins(10, 0);\n    MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // get the sum of all the values\n    double global_sum = 0;\n    if (rank == 0) {\n        for (size_t i = 0; i < 10; i++) {\n            global_sum += global_bins[i];\n        }\n    }\n\n    // save the result\n    if (rank == 0) {\n        bins = std::array<size_t, 10>();\n        for (size_t i = 0; i < 10; i++) {\n            bins[i] = global_bins[i] / global_sum * global_size;\n        }\n    }\n\n    MPI_Type_free(&MPI_DOUBLE_TYPE);\n}",
            "// TODO\n}",
            "// TODO: Your code goes here.\n    // Get the number of processes and the rank of the current process\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // Find the start and end index for the current process\n    size_t start = rank * (x.size() / size);\n    size_t end = (rank + 1) * (x.size() / size);\n    // For each element in the array x, find the corresponding bin\n    for (size_t i = start; i < end; i++) {\n        // Hint: Find the integer portion of x[i]/10.\n        bins[static_cast<size_t>(x[i] / 10)]++;\n    }\n}",
            "// TODO\n  int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  std::vector<size_t> local_bins(10, 0);\n  int chunk = x.size() / num_procs;\n  int remainder = x.size() % num_procs;\n\n  int start_rank = 0;\n  int end_rank = num_procs;\n\n  if (rank < remainder) {\n    for (int i = 0; i < remainder; i++) {\n      if (rank == i) {\n        for (size_t j = 0; j < x.size(); j++) {\n          if (x[j] >= 0.0 && x[j] < 10.0) {\n            local_bins[std::floor(x[j]) / 10] += 1;\n          }\n        }\n      }\n      rank++;\n    }\n  } else {\n    rank -= remainder;\n    start_rank += remainder;\n    end_rank += remainder;\n  }\n\n  int chunk_start = rank * chunk;\n  int chunk_end = rank == (num_procs - 1)? x.size() : (rank + 1) * chunk;\n\n  for (int i = chunk_start; i < chunk_end; i++) {\n    if (x[i] >= 0.0 && x[i] < 10.0) {\n      local_bins[std::floor(x[i]) / 10] += 1;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t n = x.size();\n    const size_t n_procs = MPI::COMM_WORLD.Get_size();\n    const size_t rank = MPI::COMM_WORLD.Get_rank();\n    const size_t local_n = n / n_procs;\n    const size_t local_start = rank * local_n;\n    const size_t local_end = local_start + local_n;\n    // 1) Every process calculates the number of values between 0 and 10\n    //    in its local_n values.\n    std::array<size_t, 10> local_counts;\n    for (size_t i = 0; i < local_n; i++) {\n        size_t bin = std::floor((x[local_start + i] / 10.0));\n        local_counts[bin]++;\n    }\n\n    // 2) Every process adds the local_counts together\n    size_t total_count = 0;\n    for (size_t i = 0; i < 10; i++) {\n        total_count += local_counts[i];\n    }\n    // 3) Every process broadcasts the total_count to all the processes\n    std::vector<int> total_count_vec(n_procs);\n    MPI::COMM_WORLD.Gather(&total_count, 1, MPI::INT, &total_count_vec[0], 1,\n                           MPI::INT, 0);\n    // 4) Every process can now calculate the indices of each rank's\n    //    contribution to the array of counts.\n    std::vector<size_t> start_idx(n_procs, 0);\n    if (rank > 0) {\n        for (size_t i = 0; i < rank; i++) {\n            start_idx[i] = total_count_vec[i];\n        }\n    }\n    // 5) Every process can now update its local_counts to hold the correct\n    //    indices\n    for (size_t i = 0; i < 10; i++) {\n        local_counts[i] += start_idx[rank];\n    }\n\n    // 6) Every process can now use the correct indices to fill in its\n    //    portion of the array of counts\n    if (rank == 0) {\n        for (size_t i = 1; i < n_procs; i++) {\n            for (size_t j = 0; j < 10; j++) {\n                local_counts[j] += total_count_vec[i];\n            }\n        }\n    }\n\n    // 7) Every process broadcasts the local_counts to the root process\n    std::vector<int> local_counts_vec(10 * n_procs);\n    MPI::COMM_WORLD.Gather(&local_counts[0], 10, MPI::INT, &local_counts_vec[0],\n                           10, MPI::INT, 0);\n    // 8) Every process can now update its local_counts to hold the correct\n    //    counts\n    for (size_t i = 0; i < 10; i++) {\n        local_counts[i] = local_counts_vec[i * n_procs + rank];\n    }\n\n    // 9) Every process can now add the local_counts to the global counts\n    if (rank == 0) {\n        for (size_t i = 1; i < n_procs; i++) {\n            for (size_t j = 0; j < 10; j++) {\n                bins[j] += local_counts_vec[i * 10 + j];\n            }\n        }\n    } else {\n        MPI::COMM_WORLD.Scatter(&local_counts_vec[0], 10, MPI::INT,\n                                &bins[0], 10, MPI::INT, 0);\n    }\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // 1. scatter x to all processes\n    double* x_local = nullptr;\n    if (world_rank == 0) {\n        x_local = new double[x.size()];\n        memcpy(x_local, x.data(), x.size() * sizeof(double));\n    }\n    MPI_Scatter(x_local, x.size() / world_size, MPI_DOUBLE, x_local, x.size() / world_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // 2. sort local x, i.e., x_local\n    if (world_rank == 0) {\n        // sort x_local\n    }\n    MPI_Bcast(&x_local, x.size() / world_size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // 3. count values in [0, 10), [10, 20), [20, 30),...\n    std::vector<size_t> local_bins;\n    for (size_t i = 0; i < x.size() / world_size; i++) {\n        if (x_local[i] >= 0.0 && x_local[i] < 10.0) {\n            local_bins.push_back(1);\n        }\n        else if (x_local[i] >= 10.0 && x_local[i] < 20.0) {\n            local_bins[1] += 1;\n        }\n        else if (x_local[i] >= 20.0 && x_local[i] < 30.0) {\n            local_bins[3] += 1;\n        }\n        //...\n        else if (x_local[i] >= 90.0 && x_local[i] < 100.0) {\n            local_bins[9] += 1;\n        }\n    }\n\n    // 4. gather results\n    if (world_rank == 0) {\n        // sum local_bins\n        for (size_t i = 1; i < world_size; i++) {\n            std::vector<size_t> local_bins_tmp(world_size);\n            MPI_Recv(local_bins_tmp.data(), world_size, MPI_UNSIGNED_LONG_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < world_size; j++) {\n                local_bins[j] += local_bins_tmp[j];\n            }\n        }\n        // set bins\n        memcpy(bins.data(), local_bins.data(), bins.size() * sizeof(size_t));\n        delete[] x_local;\n    }\n    else {\n        MPI_Send(local_bins.data(), world_size, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// 1. Broadcast x\n    // 2. Each process counts elements of x in 10 bins, and stores the result in bins.\n    // 3. Reduce the bins to the root process.\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (size < 2) {\n        return;\n    }\n\n    // 1. Broadcast x\n    std::vector<double> x_bcast(x.size());\n    if (rank == 0) {\n        x_bcast = x;\n    }\n    MPI_Bcast(x_bcast.data(), x_bcast.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // 2. Each process counts elements of x in 10 bins\n    std::array<size_t, 10> local_bins{};\n    for (size_t i = 0; i < x_bcast.size(); ++i) {\n        size_t bin_idx = (size_t) std::floor(x_bcast[i] / 10.0);\n        local_bins[bin_idx] += 1;\n    }\n\n    // 3. Reduce the bins to the root process\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// get MPI ranks\n  int rank, ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n  // get the number of elements\n  int count = x.size();\n\n  // divide the elements among ranks\n  int div = count / ranks;\n  int rem = count % ranks;\n\n  // get the first index for this rank\n  int first = rank * div;\n\n  // get the last index for this rank\n  int last = first + div;\n\n  // if this rank does not have any remainder, add one to include it\n  if (rank < rem)\n    last++;\n\n  // get this rank's count\n  int local_count = last - first;\n\n  // get the last element in this rank\n  double last_element = x.at(local_count - 1);\n\n  // get the bin index for the last element\n  int last_index = static_cast<int>(floor(last_element / 10.0));\n\n  // the count of values in this rank that fall into each bin\n  std::array<int, 10> local_bins{};\n\n  // count the values in each bin and store in local_bins\n  for (auto element : x) {\n    // get the bin index for this element\n    int index = static_cast<int>(floor(element / 10.0));\n\n    // if the element is in the range [0,10)\n    if (index < 10)\n      local_bins.at(index) += 1;\n  }\n\n  // create local vector of results\n  std::vector<int> local_results(last_index + 1);\n\n  // copy the local bins into the local results\n  for (int i = 0; i < 10; i++)\n    local_results.at(i) = local_bins.at(i);\n\n  // gather the results across all ranks\n  std::vector<int> results;\n  MPI_Gather(local_results.data(), last_index + 1, MPI_INT, results.data(),\n             last_index + 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // fill in the bins vector\n  if (rank == 0) {\n    for (int i = 0; i < 10; i++)\n      bins.at(i) = results.at(i);\n  }\n}",
            "/* TODO */\n}",
            "int num_procs, my_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  // local counts\n  std::array<size_t, 10> my_counts;\n  for (int i = 0; i < 10; i++)\n    my_counts[i] = 0;\n\n  // send data from one rank to another\n  for (size_t i = 0; i < x.size(); i++) {\n    int dest_rank = std::floor((x[i] / 10.0) * num_procs);\n    MPI_Send(x.data() + i, 1, MPI_DOUBLE, dest_rank, 0, MPI_COMM_WORLD);\n  }\n\n  // get the counts from other processes\n  for (int i = 0; i < 10; i++)\n    MPI_Recv(my_counts.data() + i, 1, MPI_UNSIGNED_LONG_LONG, i, 0,\n             MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n  // get the result from rank 0 and distribute it to other ranks\n  if (my_rank == 0) {\n    std::array<size_t, 10> global_counts;\n    for (int i = 0; i < num_procs; i++) {\n      MPI_Recv(global_counts.data() + i, 10, MPI_UNSIGNED_LONG_LONG, i, 0,\n               MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (int i = 0; i < 10; i++) {\n      bins[i] = global_counts[i];\n    }\n  } else {\n    MPI_Send(my_counts.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, 0,\n             MPI_COMM_WORLD);\n  }\n}",
            "// TODO: compute bins\n}",
            "if(x.size() % 10!= 0) {\n        throw std::runtime_error(\"x.size() is not divisible by 10\");\n    }\n\n    double const *x_ptr = x.data();\n    size_t const n = x.size() / 10;\n    size_t my_count[10];\n\n    for (size_t i=0; i<10; ++i) {\n        my_count[i] = 0;\n    }\n\n    for (size_t i=0; i<n; ++i) {\n        size_t k = (size_t)(x_ptr[i * 10] / 10);\n        if(k < 10) {\n            my_count[k] += 1;\n        } else {\n            std::cerr << \"x[\" << (i * 10) << \"] = \" << x_ptr[i * 10] << \"; index out of range\\n\";\n        }\n    }\n\n    for (size_t i=0; i<10; ++i) {\n        MPI_Reduce(&my_count[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: implement this function\n\n  size_t n = x.size();\n  size_t chunk = n/10;\n  size_t rem = n%10;\n\n  for(int i=0;i<10;i++){\n      bins[i]=0;\n  }\n\n  std::vector<size_t> local_bins(10, 0);\n  for (size_t i = 0; i < n; i++) {\n      if (x[i] >= i * chunk * 10 && x[i] < (i+1)*chunk * 10)\n          local_bins[(size_t)((x[i]-i*chunk*10)/10)]++;\n  }\n\n  std::vector<size_t> all_bins(10, 0);\n  MPI_Reduce(&local_bins[0], &all_bins[0], 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n      for(int i=0;i<10;i++){\n          if (i*chunk+chunk>n && i*chunk+chunk-n<=rem)\n              bins[i] = all_bins[i] -1;\n          else\n              bins[i] = all_bins[i];\n      }\n  }\n}",
            "/* TODO: implement me */\n  size_t count;\n  size_t size;\n  std::vector<double> x_temp(x.begin(),x.end());\n  double min,max;\n  double my_min,my_max;\n  MPI_Status status;\n  int my_rank, n_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  MPI_Reduce(&x_temp[0], &min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&x_temp[0], &max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&min, &my_min, 1, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&max, &my_max, 1, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&my_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&my_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  if(my_rank == 0)\n  {\n    int temp_count;\n    double temp_min,temp_max;\n    int i;\n    for (i = 0; i < 10; i++)\n    {\n      temp_min = i*10;\n      temp_max = (i+1)*10;\n      MPI_Reduce(&x_temp[0], &temp_count, 1, MPI_INT, MPI_SUM, i, MPI_COMM_WORLD);\n      MPI_Reduce(&temp_count, &count, 1, MPI_INT, MPI_SUM, i, MPI_COMM_WORLD);\n      bins[i] = count;\n    }\n  }\n  else\n  {\n    double my_count;\n    MPI_Reduce(&x_temp[0], &my_count, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&my_count, &count, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    if (rank == 0) {\n        bins.fill(0);\n    }\n    int start, end;\n    int len = x.size() / size;\n    if (rank == 0) {\n        for (int i = 1; i < size; ++i) {\n            MPI_Send(x.data() + i*len, len, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    MPI_Status status;\n    MPI_Recv(bins.data(), bins.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, &status);\n    MPI_Send(bins.data(), bins.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code goes here.\n}",
            "size_t n = x.size();\n  double start = 0;\n  double end = 100;\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n_local = n/size;\n  int i_local = rank*n_local;\n  int n_left = i_local;\n  int n_right = n - i_local;\n  // for each rank do the calculations\n  double start_local = start;\n  double end_local = end;\n  if (rank == 0){\n    if (n_left > 0){\n      start_local = x[i_local];\n    }\n    if (n_right > 0){\n      end_local = x[i_local + n_right];\n    }\n  }\n  MPI_Bcast(&start_local, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&end_local, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  // split up the bins according to the start and end of the input vector\n  std::vector<int> bins_local(10, 0);\n  for (size_t i = 0; i < n_local; ++i){\n    double val = x[i + i_local];\n    if (val >= start_local && val <= end_local){\n      bins_local[(val-start_local)/10]++;\n    }\n  }\n  MPI_Reduce(bins_local.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "double n = x.size();\n  size_t start = 10*myRank;\n  size_t end = 10*myRank + 10;\n  for (size_t i = start; i < end; ++i) {\n    for (size_t j = 0; j < n; ++j) {\n      if (x[j] >= i && x[j] < (i + 10)) {\n        ++bins[i];\n      }\n    }\n  }\n}",
            "int rank, num_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n\t// Each rank gets a block of the data to process\n\tstd::vector<double> rank_x(x.begin() + rank * (x.size() / num_ranks),\n\t\tx.begin() + (rank + 1) * (x.size() / num_ranks));\n\n\t// Each rank processes it's portion of the data\n\tbins.fill(0);\n\tfor (size_t i = 0; i < rank_x.size(); ++i) {\n\t\tif (rank_x[i] >= 0 && rank_x[i] < 10) {\n\t\t\t++bins[int(rank_x[i])];\n\t\t}\n\t}\n\n\t// Sum the counts across all ranks\n\tstd::array<int, 10> bins_sums;\n\tMPI_Allreduce(bins.data(), bins_sums.data(), bins.size(),\n\t\tMPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tstd::copy(bins_sums.cbegin(), bins_sums.cend(), bins.begin());\n\n\treturn;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (x.size() % size!= 0) {\n    std::cerr << \"x.size() % size!= 0\" << std::endl;\n    abort();\n  }\n\n  // Number of values in the interval\n  size_t localCount = x.size() / size;\n\n  // Number of values less than the beginning of the interval\n  size_t lowerBound = (rank == 0? 0 : x[rank - 1]);\n\n  // Number of values greater than the end of the interval\n  size_t upperBound = (rank == size - 1? 100 : x[rank + localCount]);\n\n  // Count the values in [lowerBound, upperBound) and store them in bins\n  size_t localBinCount = 0;\n  for (auto v : x) {\n    if (v >= lowerBound && v < upperBound) {\n      localBinCount++;\n    }\n  }\n\n  std::array<size_t, 10> localBins;\n  for (size_t i = 0; i < localBinCount; ++i) {\n    localBins[localBinCount]++;\n  }\n\n  std::array<size_t, 10> localReducedBins;\n  MPI_Reduce(localBins.data(), localReducedBins.data(), localBins.size(),\n      MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = localReducedBins;\n  }\n}",
            "// TODO: Implement this function.\n}",
            "std::vector<double> x_local = x;\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t start = 10 * rank;\n    size_t end = 10 * (rank + 1);\n    for (size_t i = 0; i < x_local.size(); ++i) {\n        if (x_local[i] >= start && x_local[i] < end) {\n            ++bins[x_local[i] / 10];\n        }\n    }\n}",
            "// TODO: implement this function\n  //\n  // HINTS:\n  //\n  // - Use the MPI_Comm_size and MPI_Comm_rank functions to determine the number\n  //   of ranks and the rank of the current process.\n  //\n  // - The number of elements in `x` should be divisible by the number of\n  //   ranks. This means that the size of the local data is `size = x.size() /\n  //   ranks`.\n  //\n  // - You can determine which elements of x are in this rank's local data by\n  //   determining which elements have indices in the range [i * size, (i+1)\n  //   * size). For example, if `size = 6` and `rank = 1`, then the local data\n  //   consists of the elements with indices [6, 7, 8, 9, 10, 11].\n  //\n  // - To divide the elements in the local data into 10 bins, use\n  //   `std::find_if` and `std::distance`.\n  //\n  // - To send a message from one rank to another, use the `MPI_Send`\n  //   function.\n  //\n  // - To receive a message from another rank, use the `MPI_Recv` function.\n  //\n  // - To receive a message from any rank, use the `MPI_ANY_SOURCE` constant\n  //   as the source for the `MPI_Recv` call.\n  //\n  // - To check whether the received message is valid, use the `MPI_Status`\n  //   object that is passed into the `MPI_Recv` function. The `MPI_Status`\n  //   object's `MPI_ERROR` member can be used to check whether there was an\n  //   error. For example, if an invalid message was received, the error code\n  //   would be set to `MPI_ERR_TRUNCATE`.\n  //\n  // - To broadcast a value from one rank to all the others, use the\n  //   `MPI_Bcast` function. The `MPI_Bcast` function has three parameters.\n  //   The first parameter is a pointer to the value to broadcast. The second\n  //   parameter is the number of bytes to send. The third parameter is the\n  //   data type of the value.\n  //\n  // - To get a copy of the `MPI_COMM_WORLD` communicator, use the\n  //   `MPI_COMM_WORLD` constant.\n  //\n  // - To determine whether a rank is the root process, use the `MPI_Comm_rank`\n  //   function with `MPI_COMM_WORLD` as the first parameter.\n  //\n  // - To get the rank of the root process, use the `MPI_ROOT` constant.\n  //\n  // - To reduce values from all ranks, use the `MPI_Reduce` function. The\n  //   `MPI_Reduce` function has four parameters. The first parameter is a\n  //   pointer to the input value. The second parameter is a pointer to the\n  //   output value. The third parameter is the number of bytes to send. The\n  //   fourth parameter is the MPI reduction operator.\n  //\n  // - To sum values from all ranks, use `MPI_SUM`.\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int slice_size = x.size() / world_size;\n  int left_over = x.size() % world_size;\n  int start = slice_size * world_rank + std::min(world_rank, left_over);\n  int end = start + slice_size + (world_rank < left_over);\n\n  std::vector<double> local_x;\n  if (world_rank == 0) {\n    local_x = std::vector<double>(x.begin() + start, x.begin() + end);\n  } else {\n    local_x = std::vector<double>(x.begin() + start, x.begin() + start + slice_size);\n  }\n\n  // Your code goes here.\n  int counts[10] = {};\n  for (auto& x : local_x) {\n    ++counts[(int)x / 10];\n  }\n\n  // Collect counts from all processes to process 0.\n  if (world_rank == 0) {\n    for (int i = 1; i < world_size; ++i) {\n      MPI_Status status;\n      MPI_Recv(&counts[10 * i], 10, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    MPI_Send(&counts[10 * world_rank], 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // Combine counts from all processes into the final result.\n  if (world_rank == 0) {\n    for (int i = 1; i < world_size; ++i) {\n      for (int j = 0; j < 10; ++j) {\n        counts[j] += counts[j + 10 * i];\n      }\n    }\n  }\n\n  bins = std::array<size_t, 10>(counts);\n}",
            "int numProc;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t length = x.size();\n  std::vector<double> localX(length);\n  std::vector<size_t> localBins(10);\n\n  int numPerProc = length / numProc;\n\n  // each process gets the x values it needs and the bins array to fill\n  MPI_Scatter(&x[0], numPerProc, MPI_DOUBLE, &localX[0], numPerProc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  MPI_Scatter(&bins[0], 10, MPI_UNSIGNED, &localBins[0], 10, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n\n  // for every local x value, check if its value is between 0 and 10\n  // if it is, add 1 to the corresponding bin\n  for (auto x : localX) {\n    if (x < 10) {\n      localBins[0]++;\n    } else if (x < 20) {\n      localBins[1]++;\n    } else if (x < 30) {\n      localBins[2]++;\n    } else if (x < 40) {\n      localBins[3]++;\n    } else if (x < 50) {\n      localBins[4]++;\n    } else if (x < 60) {\n      localBins[5]++;\n    } else if (x < 70) {\n      localBins[6]++;\n    } else if (x < 80) {\n      localBins[7]++;\n    } else if (x < 90) {\n      localBins[8]++;\n    } else {\n      localBins[9]++;\n    }\n  }\n\n  // gather the bins array back to all the ranks\n  MPI_Gather(&localBins[0], 10, MPI_UNSIGNED, &bins[0], 10, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "MPI_Datatype datatype;\n    MPI_Type_contiguous(10, MPI_DOUBLE, &datatype);\n    MPI_Type_commit(&datatype);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    MPI_Info info;\n    MPI_Info_create(&info);\n    MPI_Info_set(info, \"no_locks\", \"true\");\n\n    int counts[10];\n    int offsets[10];\n\n    double *sendcounts = new double[world_size];\n    double *recvcounts = new double[world_size];\n    double *sdispls = new double[world_size];\n    double *rdispls = new double[world_size];\n\n    for (int i = 0; i < world_size; i++) {\n        sendcounts[i] = 10.0;\n        recvcounts[i] = 10.0;\n        sdispls[i] = 10.0;\n        rdispls[i] = 10.0;\n    }\n\n    MPI_Scatterv(sendcounts, recvcounts, rdispls, datatype, counts, 10, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Scatterv(sendcounts, recvcounts, rdispls, datatype, offsets, 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n    MPI_Scatterv(const_cast<double*>(x.data()), recvcounts, rdispls, MPI_DOUBLE, counts, 10, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    int local_counts[10];\n    for (int i = 0; i < 10; i++) {\n        local_counts[i] = 0;\n    }\n\n    for (int i = 0; i < 100; i++) {\n        for (int j = 0; j < counts[i / 10]; j++) {\n            if (i >= offsets[i / 10] && i < offsets[i / 10] + counts[i / 10]) {\n                local_counts[i / 10]++;\n            }\n        }\n    }\n\n    MPI_Gatherv(local_counts, 10, MPI_INT, bins.data(), recvcounts, rdispls, MPI_INT, 0, MPI_COMM_WORLD);\n\n    MPI_Type_free(&datatype);\n    MPI_Info_free(&info);\n\n    delete[] sendcounts;\n    delete[] recvcounts;\n    delete[] sdispls;\n    delete[] rdispls;\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if (x.size() == 0) {\n        return;\n    }\n\n    int elementsCount = x.size() / size;\n    int remained = x.size() % size;\n    if (rank < remained) {\n        ++elementsCount;\n    }\n\n    std::vector<double> localData(elementsCount);\n    std::vector<size_t> localBins(10);\n\n    int i = 0;\n    int begin = elementsCount * rank + std::min(rank, remained);\n    int end = elementsCount * (rank + 1) + std::min(rank + 1, remained);\n    std::copy(x.begin() + begin, x.begin() + end, localData.begin());\n    for (auto const& item : localData) {\n        int bin = item / 10;\n        ++localBins[bin];\n    }\n\n    std::vector<size_t> allBins(10);\n    MPI_Reduce(localBins.data(), allBins.data(), allBins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::copy(allBins.begin(), allBins.begin() + 10, bins.begin());\n    }\n}",
            "// TODO: Implement.\n    //\n    // You will need to send values from x to other ranks, receive results from\n    // other ranks, and update bins appropriately.\n    //\n    // You can assume that the number of values in x is divisible by the number\n    // of ranks. You will also need to handle the case when the number of values\n    // is not divisible by the number of ranks. In this case, you will need to\n    // pad x with the appropriate number of 0s to make it divisible.\n}",
            "double step = 10.0 / x.size();\n    for (size_t i = 0; i < 10; i++) {\n        double start = step * i;\n        double end = step * (i + 1);\n        bins[i] = 0;\n        for (auto j : x)\n            if (j >= start && j < end)\n                bins[i]++;\n    }\n}",
            "// TODO: implement\n  int nproc, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = x.size() / nproc;\n  double* x_local = new double[n];\n  for(int i = 0; i < n; i++){\n    x_local[i] = x[rank * n + i];\n  }\n\n  std::array<int, 10> local_bins;\n  for(int i = 0; i < 10; i++){\n    local_bins[i] = 0;\n  }\n\n  // get rank of process\n  int my_rank = rank;\n\n  // get rank of process\n  int nproc = 10;\n\n  // get number of elements per process\n  int n = 10;\n\n  // get the global rank of each element in the array\n  int g_ranks[10] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n\n  // count local values\n  for(int i = 0; i < 10; i++){\n    for(int j = 0; j < n; j++){\n      if(g_ranks[j] >= i*n && g_ranks[j] < (i+1)*n){\n        local_bins[i] += 1;\n      }\n    }\n  }\n\n  // create vector for all local_bins values\n  std::vector<std::array<int, 10>> local_bins_vec(nproc);\n  for(int i = 0; i < nproc; i++){\n    local_bins_vec[i] = local_bins;\n  }\n\n  // get the global rank of each element in the array\n  int g_ranks[10] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n\n  // get the size of each process\n  int n_local[10] = {10, 10, 10, 10, 10, 10, 10, 10, 10, 10};\n\n  int counts[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  int displs[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  MPI_Scatterv(local_bins_vec.data(), n_local, displs, MPI_INT, counts, 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // get rank of process\n  int my_rank = rank;\n\n  // get number of elements per process\n  int n = 10;\n\n  // get the global rank of each element in the array\n  int g_ranks[10] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n\n  // count local values\n  for(int i = 0; i < 10; i++){\n    for(int j = 0; j < n; j++){\n      if(g_ranks[j] >= i*n && g_ranks[j] < (i+1)*n){\n        local_bins[i] += 1;\n      }\n    }\n  }\n\n  // get the global rank of each element in the array\n  int g_ranks[10] = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9};\n\n  // get the size of each process\n  int n_local[10] = {10, 10, 10, 10, 10, 10, 10, 10, 10, 10};\n\n  int counts[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  int displs[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  MPI_Scatterv(local_bins_vec.data(), n_local, displs, MPI_INT, counts, 10, MPI_INT, 0, MPI_COMM_WORLD);\n\n  MPI",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    // Rank 0 will be responsible for creating the bins and for\n    // collecting the results from all the other ranks.\n    bins.fill(0);\n\n    // Send each of the input vectors to the rest of the ranks.\n    for (int i = 1; i < size; ++i) {\n      MPI_Send(&x[0], x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n\n    // Collect the results from the other ranks\n    for (int i = 1; i < size; ++i) {\n      std::array<size_t, 10> temp;\n      MPI_Recv(&temp[0], 10, MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      for (int j = 0; j < 10; ++j) {\n        bins[j] += temp[j];\n      }\n    }\n  } else {\n    // All other ranks will perform the count\n    std::array<size_t, 10> temp;\n    temp.fill(0);\n\n    for (size_t i = 0; i < x.size(); ++i) {\n      ++temp[static_cast<int>(x[i]) / 10];\n    }\n\n    MPI_Send(&temp[0], 10, MPI_UNSIGNED, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// MPI_Init();\n    //\n    // TODO: implement this function\n    //\n    // MPI_Finalize();\n}",
            "// get size of array\n    auto size = x.size();\n    // get number of processes\n    int size_world;\n    MPI_Comm_size(MPI_COMM_WORLD, &size_world);\n    // get the rank of the current process\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // get the max value of the array\n    auto max = *std::max_element(x.begin(), x.end());\n    // get the min value of the array\n    auto min = *std::min_element(x.begin(), x.end());\n    // get number of elements per rank\n    auto elementsPerRank = size / size_world;\n    // get the start and end index of the current rank\n    auto start = elementsPerRank * rank;\n    auto end = (rank == size_world-1)? size : start + elementsPerRank;\n    // local array\n    std::array<size_t, 10> local_bins;\n    // loop through all the elements of the current rank\n    for(auto i = start; i < end; ++i){\n        // get the current element value\n        auto value = x.at(i);\n        // get the index of the current bin\n        auto index = (value - min) / (max - min) * 10;\n        // increment the current bin\n        ++local_bins.at(index);\n    }\n    // get the sum of all the bins\n    std::array<size_t, 10> total_bins;\n    MPI_Reduce(local_bins.data(), total_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if(rank == 0)\n        bins = total_bins;\n}",
            "}",
            "size_t count = x.size();\n\n    //TODO: Implement me!\n\n    int world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int chunk_size = count / world_size;\n    int last_chunk_size = count % world_size;\n    int chunk_start = chunk_size * world_rank;\n    int chunk_end = chunk_size * (world_rank + 1);\n    if (world_rank == world_size - 1) {\n        chunk_end = chunk_end + last_chunk_size;\n    }\n    std::array<double, 10> chunk_bins = {0};\n    for (size_t i = chunk_start; i < chunk_end; ++i) {\n        if (x[i] < 10) {\n            chunk_bins[0] = chunk_bins[0] + 1;\n        }\n        if (x[i] >= 10 && x[i] < 20) {\n            chunk_bins[1] = chunk_bins[1] + 1;\n        }\n        if (x[i] >= 20 && x[i] < 30) {\n            chunk_bins[2] = chunk_bins[2] + 1;\n        }\n        if (x[i] >= 30 && x[i] < 40) {\n            chunk_bins[3] = chunk_bins[3] + 1;\n        }\n        if (x[i] >= 40 && x[i] < 50) {\n            chunk_bins[4] = chunk_bins[4] + 1;\n        }\n        if (x[i] >= 50 && x[i] < 60) {\n            chunk_bins[5] = chunk_bins[5] + 1;\n        }\n        if (x[i] >= 60 && x[i] < 70) {\n            chunk_bins[6] = chunk_bins[6] + 1;\n        }\n        if (x[i] >= 70 && x[i] < 80) {\n            chunk_bins[7] = chunk_bins[7] + 1;\n        }\n        if (x[i] >= 80 && x[i] < 90) {\n            chunk_bins[8] = chunk_bins[8] + 1;\n        }\n        if (x[i] >= 90 && x[i] < 100) {\n            chunk_bins[9] = chunk_bins[9] + 1;\n        }\n    }\n\n    std::array<double, 10> all_bins = {0};\n    MPI_Reduce(chunk_bins.data(), all_bins.data(), 10, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        bins = all_bins;\n    }\n}",
            "// TODO: your code goes here\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  size_t count = x.size() / world_size;\n\n  // TODO: compute the number of elements in the remainder\n  std::vector<double> local_x(x.begin() + world_rank * count,\n                               x.begin() + world_rank * count + count);\n\n  // TODO: compute the number of elements in the remainder\n  std::array<size_t, 10> local_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  // TODO: iterate over x\n  for (auto el : local_x) {\n    int bin_id = std::floor(el / 10);\n    local_bins[bin_id]++;\n  }\n\n  // TODO: sum the bins together\n  std::array<size_t, 10> global_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    bins = global_bins;\n  }\n}",
            "double min = *std::min_element(x.begin(), x.end());\n  double max = *std::max_element(x.begin(), x.end());\n  double binSize = (max - min) / 10;\n  double rank = 0;\n  double size = 10;\n  if (size < 1) {\n    return;\n  }\n  std::vector<double> localBins(10);\n  std::vector<double> localCount(10);\n  int rank_id, comm_sz;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank_id);\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n  for (auto &val : x) {\n    int idx = static_cast<int>((val - min) / binSize);\n    localBins[idx] += 1;\n  }\n  for (size_t i = 0; i < 10; ++i) {\n    localCount[i] = localBins[i] + localBins[i - 1];\n  }\n  MPI_Reduce(localBins.data(), bins.data(), 10, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(localCount.data(), bins.data(), 10, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Replace this with your code\n  bins = {};\n  return;\n}",
            "const size_t n = x.size();\n  const size_t np = 8;\n  const size_t m = n / np + (n % np!= 0? 1 : 0);\n  std::vector<double> local(m, 0.0);\n  std::vector<double> global(np, 0.0);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    for (size_t i = 0; i < np; i++) {\n      size_t lb = i * m;\n      size_t ub = lb + m;\n      std::copy(x.begin() + lb, x.begin() + ub, local.begin());\n      local[m - 1] = 100.0;\n      MPI_Scatter(local.data(), m, MPI_DOUBLE, global.data() + i, m, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Scatter(local.data(), m, MPI_DOUBLE, global.data(), m, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  std::array<int, 11> locbins;\n  for (size_t i = 0; i < m; i++) {\n    int val = (int)(global[i] / 10.0);\n    locbins[val]++;\n  }\n  MPI_Gather(locbins.data(), 11, MPI_INT, bins.data(), 11, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "std::vector<size_t> localBins = std::array<size_t, 10> {};\n  int comm_size;\n  int comm_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n  if (comm_rank == 0) {\n    for (int i = 0; i < x.size(); i++) {\n      int bin_idx = static_cast<int>(x[i] / 10);\n      if (bin_idx < 10) {\n        localBins[bin_idx] += 1;\n      }\n    }\n  }\n  MPI_Scatter(localBins.data(), 10, MPI_INT, bins.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n  if (comm_rank == 0) {\n    for (int i = 1; i < comm_size; i++) {\n      std::vector<size_t> sendData(10);\n      MPI_Recv(sendData.data(), 10, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (int j = 0; j < 10; j++) {\n        bins[j] += sendData[j];\n      }\n    }\n  } else {\n    MPI_Send(localBins.data(), 10, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<size_t> counts(10, 0);\n  int num_elements_per_rank = x.size() / size;\n  int num_elements_left_over = x.size() % size;\n\n  // Rank 0 has a complete copy of x\n  if (rank == 0) {\n    for (int i = 0; i < num_elements_per_rank * rank + num_elements_left_over;\n         i++) {\n      if (x[i] >= 0 && x[i] < 10) {\n        counts[static_cast<int>(x[i])] += 1;\n      }\n    }\n  } else {\n    for (int i = 0; i < num_elements_per_rank; i++) {\n      if (x[num_elements_per_rank * rank + i] >= 0 &&\n          x[num_elements_per_rank * rank + i] < 10) {\n        counts[static_cast<int>(x[num_elements_per_rank * rank + i])] += 1;\n      }\n    }\n  }\n\n  MPI_Reduce(&counts[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n}",
            "// Your code here.\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int localCount = 0;\n  int nextRank = (rank + 1) % size;\n  for (size_t i = rank; i < x.size(); i += size) {\n    if (x[i] >= 0.0 && x[i] < 10.0) {\n      localCount++;\n    }\n    if ((i + 1) % size == 0) {\n      MPI_Send(&localCount, 1, MPI_INT, nextRank, 0, MPI_COMM_WORLD);\n      localCount = 0;\n    }\n  }\n\n  std::array<int, 10> localBins{};\n  MPI_Status status;\n  for (int i = 0; i < size; ++i) {\n    MPI_Recv(localBins.data(), 10, MPI_INT, MPI_ANY_SOURCE, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n    if (rank == 0) {\n      for (int j = 0; j < 10; ++j) {\n        bins[j] += localBins[j];\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "const int myRank{0};\n  const int numRanks{1};\n  const int n{x.size()};\n  std::vector<size_t> localBins(10, 0);\n  const int sizeOfLocalBin{1};\n\n  const int range = (n / numRanks) + 1;\n  const int start = range * myRank;\n  const int end = (range * (myRank + 1)) - 1;\n\n  // Fill local bins\n  for (int i{start}; i <= end; ++i) {\n    ++localBins[static_cast<int>(x[i] / 10)];\n  }\n\n  // Reduce local bins to global bins\n  MPI_Reduce(localBins.data(), bins.data(), sizeOfLocalBin * 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: compute the bins\n}",
            "}",
            "std::vector<size_t> counts(10, 0);\n\n  double sum = 0;\n  for (size_t i = 0; i < x.size(); i++) {\n    sum += x[i];\n  }\n\n  double average = sum / x.size();\n\n  for (size_t i = 0; i < x.size(); i++) {\n    counts[(size_t)(x[i] / average)]++;\n  }\n\n  for (size_t i = 1; i < 10; i++) {\n    counts[i] += counts[i - 1];\n  }\n\n  // copy first n counts to bins, where n = size of vector\n  // the reason for doing this is that it is easier to do this in parallel\n  MPI_Bcast(&counts[0], x.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < x.size(); i++) {\n    bins[counts[(size_t)(x[i] / average)] - 1]++;\n  }\n}",
            "size_t n = x.size();\n  if (n == 0) {\n    return;\n  }\n  std::vector<int> counts(10, 0);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // distribute n elements to each rank\n  size_t n_per_rank = (n + size - 1) / size;\n  // offset of the first element assigned to the current rank\n  size_t offset = n_per_rank * rank;\n  // number of elements assigned to the current rank\n  size_t local_n = std::min(n_per_rank, n - offset);\n  std::vector<double> local_x(local_n);\n  std::copy(x.begin() + offset, x.begin() + offset + local_n, local_x.begin());\n\n  // compute local counts\n  for (double val : local_x) {\n    if (val < 10) {\n      ++counts[0];\n    } else if (val < 20) {\n      ++counts[1];\n    } else if (val < 30) {\n      ++counts[2];\n    } else if (val < 40) {\n      ++counts[3];\n    } else if (val < 50) {\n      ++counts[4];\n    } else if (val < 60) {\n      ++counts[5];\n    } else if (val < 70) {\n      ++counts[6];\n    } else if (val < 80) {\n      ++counts[7];\n    } else if (val < 90) {\n      ++counts[8];\n    } else if (val < 100) {\n      ++counts[9];\n    } else {\n      std::cout << \"Error: value out of bounds: \" << val << std::endl;\n      return;\n    }\n  }\n\n  // gather results\n  std::vector<int> recvcounts(counts.size(), 0);\n  MPI_Gather(&counts[0], counts.size(), MPI_INT, recvcounts.data(), counts.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  // rank 0 computes final counts\n  if (rank == 0) {\n    for (size_t i = 0; i < counts.size(); ++i) {\n      bins[i] = recvcounts[i];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "cuda",
        "prompt": "/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is initialized with at least as many threads as values in x.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\n__global__ void binsBy10Count(const double *x, size_t N, size_t[10] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Get global thread ID.\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // Compute the block's total sum.\n    if (tid < N) {\n        // Compute the bin index for this thread.\n        int bin = int((x[tid] / 10) + 0.5);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// Your code here\n  // (Note: use 2D blockIdx and threadIdx to access the array bins)\n  // (Note: use CUDA's atomicAdd to add to bins)\n  // (Note: use 1D blockIdx to determine which chunk of the input to process)\n\n  // (Note: You may have noticed that you didn't need to declare or allocate\n  // any device-side variables.  All variables are implicitly thread-local.)\n\n  // (Note: There is a \"solution\" branch in the repo.\n  //  You can compare your implementation to this solution if you get stuck.)\n}",
            "const size_t blockId = blockIdx.x;\n    const size_t threadId = threadIdx.x;\n    size_t i = blockId * blockDim.x + threadId;\n\n    if (i < N) {\n        int binIdx = int((x[i] + 0.05) / 10);\n        atomicAdd(&bins[binIdx], 1);\n    }\n}",
            "const int id = blockDim.x * blockIdx.x + threadIdx.x;\n  const int stride = blockDim.x * gridDim.x;\n\n  int sum = 0;\n  for (int i = id; i < N; i += stride) {\n    sum += (x[i] < 10)? 1 : 0;\n  }\n\n  atomicAdd(&bins[9], sum);\n}",
            "// TODO: Implement this function\n    double t = 0.0;\n    int i = blockIdx.x;\n    if(i*blockDim.x+threadIdx.x<N)\n        t = x[i*blockDim.x+threadIdx.x]/10.0;\n    __syncthreads();\n    atomicAdd(&bins[int(t)],1);\n}",
            "size_t tid = threadIdx.x; // thread index\n\n    // initialize bins\n    for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        int bin = (int)(x[i] / 10); // compute bin by dividing by 10\n        if (bin >= 0 && bin < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x;\n  int blockId = blockIdx.x;\n\n  int threadCount = min(N - blockId * BLOCK_SIZE, BLOCK_SIZE);\n  int gridSize = (threadCount + BLOCK_SIZE - 1) / BLOCK_SIZE;\n\n  __shared__ double cache[BLOCK_SIZE];\n\n  int i = tid + blockId * BLOCK_SIZE;\n\n  int bin_idx = 0;\n  int last_bin_idx = 0;\n  for (int j = 0; j < 10; j++) {\n    bins[j] = 0;\n  }\n\n  for (int j = 0; j < threadCount; j++) {\n    if (x[i + j] < 10) {\n      bins[x[i + j]] += 1;\n    }\n  }\n}",
            "__shared__ size_t s_bins[10];\n    size_t tid = threadIdx.x;\n    if (tid < 10) {\n        s_bins[tid] = 0;\n    }\n    __syncthreads();\n    for (int i = tid; i < N; i += blockDim.x) {\n        size_t t = (size_t)(x[i]);\n        if (t < 10) {\n            s_bins[t] += 1;\n        }\n    }\n    __syncthreads();\n    if (tid < 10) {\n        atomicAdd(&bins[tid], s_bins[tid]);\n    }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if(idx < N) {\n        int b = x[idx] / 10;\n        atomicAdd(&bins[b], 1);\n    }\n}",
            "// YOUR CODE HERE\n}",
            "size_t t = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t step = gridDim.x * blockDim.x;\n    while (t < N) {\n        int value = (int)x[t];\n        if (value < 0) {\n            value = 0;\n        }\n        if (value >= 100) {\n            value = 99;\n        }\n        int bin = value / 10;\n        atomicAdd(&bins[bin], 1);\n        t += step;\n    }\n}",
            "// your code here\n}",
            "// TODO: Your implementation goes here\n}",
            "// Compute the number of values in [0,10), [10, 20), [20, 30),...\n  // The thread with the same index as x[i] contains value x[i]\n\n  int tid = threadIdx.x;\n  __shared__ int counts[10];\n  __shared__ int values[10];\n\n  if (tid < 10) {\n    counts[tid] = 0;\n    values[tid] = tid * 10;\n  }\n  __syncthreads();\n\n  int block_start = blockIdx.x * blockDim.x;\n  int block_end = block_start + blockDim.x;\n  int i = block_start + tid;\n\n  while (i < N) {\n    if (x[i] < values[9]) {\n      counts[0] += (x[i] >= values[0]);\n    } else if (x[i] >= values[9]) {\n      counts[9] += (x[i] < values[10]);\n    } else {\n      counts[9] += (x[i] >= values[9]);\n    }\n    i += blockDim.x * gridDim.x;\n  }\n\n  __syncthreads();\n\n  if (tid < 10) {\n    atomicAdd(&bins[tid], counts[tid]);\n  }\n}",
            "// TODO: Your code goes here\n}",
            "const int tid = blockDim.x * blockIdx.x + threadIdx.x;\n    if (tid < N) {\n        int bin = (int)(x[tid] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n\n    size_t i = blockIdx.x;\n    size_t stride = gridDim.x;\n    for (; i < N; i += stride) {\n        size_t bucket = size_t((x[i] / 10));\n        atomicAdd(&(bins[bucket]), 1);\n    }\n}",
            "// Write your code here\n   int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   double i = bid * 10 + tid;\n   double val = x[i];\n   if (i < N) {\n      if (val < 10)\n         atomicAdd(&bins[0], 1);\n      else if (val < 20)\n         atomicAdd(&bins[1], 1);\n      else if (val < 30)\n         atomicAdd(&bins[2], 1);\n      else if (val < 40)\n         atomicAdd(&bins[3], 1);\n      else if (val < 50)\n         atomicAdd(&bins[4], 1);\n      else if (val < 60)\n         atomicAdd(&bins[5], 1);\n      else if (val < 70)\n         atomicAdd(&bins[6], 1);\n      else if (val < 80)\n         atomicAdd(&bins[7], 1);\n      else if (val < 90)\n         atomicAdd(&bins[8], 1);\n      else if (val < 100)\n         atomicAdd(&bins[9], 1);\n   }\n}",
            "// TODO: Your code here\n  __syncthreads();\n}",
            "// each thread will increment a value in bins\n   size_t bin = threadIdx.x + blockIdx.x * blockDim.x;\n   if (bin < 10) {\n      bins[bin] = 0;\n   }\n\n   // each thread will read a value from x\n   size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      size_t val = static_cast<size_t>(x[i] / 10); // value in [0,10)\n      atomicAdd(&bins[val], 1);\n   }\n}",
            "__shared__ size_t s_bins[10];\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    s_bins[i] = 0;\n  }\n  __syncthreads();\n  for (int i = blockIdx.x*blockDim.x + threadIdx.x; i < N; i += blockDim.x*gridDim.x) {\n    int idx = x[i]/10;\n    atomicAdd(&s_bins[idx], 1);\n  }\n  __syncthreads();\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    atomicAdd(&bins[i], s_bins[i]);\n  }\n}",
            "// TODO\n}",
            "int bin = (int)round(10 * (x[blockIdx.x * blockDim.x + threadIdx.x] - 0.0) / (100.0 - 0.0));\n    atomicAdd(&bins[bin], 1);\n}",
            "size_t bin = (blockIdx.x * blockDim.x) + threadIdx.x;\n\tif (bin >= 10) return;\n\tsize_t start = bin * (N / 10);\n\tsize_t end = (bin+1) * (N / 10);\n\tif (end > N) end = N;\n\tbins[bin] = 0;\n\tfor (size_t i=start; i<end; i++) {\n\t\tif (x[i] >= bin * 10 && x[i] < (bin + 1) * 10) {\n\t\t\tbins[bin] += 1;\n\t\t}\n\t}\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n\n  // Your code here\n  if (i < N) {\n    if (x[i] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "__shared__ double s_x[1024];\n\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    s_x[threadIdx.x] = tid < N? x[tid] : 0;\n\n    __syncthreads();\n\n    if (tid < N) {\n        size_t bin_num = floor((x[tid] + 9) / 10.0);\n        atomicAdd(&bins[bin_num], 1);\n    }\n}",
            "// TODO: Fill in the body of this function\n    const int tid = threadIdx.x;\n    const int gid = threadIdx.x + blockIdx.x * blockDim.x;\n    const int Nthreads = blockDim.x * gridDim.x;\n    const double step = 10.0;\n    const double start = 0.0;\n\n    // TODO: Add appropriate number of threads for the kernel launch\n    // Use 10 threads for this kernel\n\n    int tmp;\n    if(gid < N)\n    {\n        tmp = (int) (x[gid]/step);\n        if(tmp >= 10)\n        {\n            atomicAdd(&bins[9], 1);\n        }\n        else\n        {\n            atomicAdd(&bins[tmp], 1);\n        }\n    }\n}",
            "// Initialize bins to zero\n    for (int i = 0; i < 10; i++) {\n        atomicAdd(&bins[i], 0);\n    }\n    // Increment bins\n    int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    for (int i = idx; i < N; i += stride) {\n        atomicAdd(&bins[static_cast<int>(x[i] / 10)], 1);\n    }\n}",
            "const int bin = threadIdx.x;\n    const double start = bin * 10;\n    const double end = (bin + 1) * 10;\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n        if (x[i] >= start && x[i] < end) {\n            atomicAdd(&bins[bin], 1);\n        }\n    }\n}",
            "double const dx = 10.0 / N;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    bins[int(dx * x[i])]++;\n}",
            "const int tid = threadIdx.x;\n  const size_t idx = blockIdx.x * blockDim.x + tid;\n  size_t myBin[10] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n  for (int i = idx; i < N; i += blockDim.x * gridDim.x) {\n    myBin[int(x[i]) / 10]++;\n  }\n\n  atomicAdd(&bins[0], myBin[0]);\n  atomicAdd(&bins[1], myBin[1]);\n  atomicAdd(&bins[2], myBin[2]);\n  atomicAdd(&bins[3], myBin[3]);\n  atomicAdd(&bins[4], myBin[4]);\n  atomicAdd(&bins[5], myBin[5]);\n  atomicAdd(&bins[6], myBin[6]);\n  atomicAdd(&bins[7], myBin[7]);\n  atomicAdd(&bins[8], myBin[8]);\n  atomicAdd(&bins[9], myBin[9]);\n}",
            "size_t bin = int(x[threadIdx.x] / 10.0);\n    atomicAdd(&bins[bin], 1);\n}",
            "__shared__ double s_x[1024];\n\tsize_t i = blockIdx.x * blockDim.x + threadIdx.x;\n\tint tid = threadIdx.x;\n\ts_x[tid] = 0;\n\tfor (size_t k = 0; k < N; k += 1024) {\n\t\tif (i < N) {\n\t\t\tif (x[i] >= k && x[i] < k + 10) {\n\t\t\t\ts_x[tid] = s_x[tid] + 1;\n\t\t\t}\n\t\t}\n\t}\n\t__syncthreads();\n\tif (tid < 10) {\n\t\tatomicAdd(bins + tid, s_x[tid]);\n\t}\n}",
            "size_t bin;\n\n   // the value of each thread\n   bin = floor(x[blockIdx.x] / 10);\n\n   // only the first thread of each block should do something\n   if(threadIdx.x == 0) {\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "__shared__ double x_shared[256];\n  const size_t tid = threadIdx.x;\n  const size_t bid = blockIdx.x;\n\n  x_shared[tid] = x[bid*256 + tid];\n  __syncthreads();\n  if(tid < 10)\n    bins[tid] = countInRange(x_shared, tid*10, (tid+1)*10);\n}",
            "// TODO: Implement me!\n}",
            "// Fill your code here\n    if (threadIdx.x < 10)\n        atomicAdd(&bins[threadIdx.x], 0);\n    __syncthreads();\n    if (blockIdx.x < N / 10) {\n        for (int i = 0; i < 10; i++)\n            atomicAdd(&bins[i], (x[blockIdx.x * 10 + i] >= i * 10) * (x[blockIdx.x * 10 + i] < (i + 1) * 10));\n    }\n    __syncthreads();\n    if (threadIdx.x < 10)\n        atomicAdd(&bins[threadIdx.x], 0);\n}",
            "size_t bin = (threadIdx.x * 10 / N) + 1;\n  __syncthreads();\n  atomicAdd(&bins[bin], 1);\n}",
            "unsigned int threadID = blockDim.x * blockIdx.x + threadIdx.x;\n  if (threadID < N) {\n    double tmp = (x[threadID] / 10.0);\n    unsigned int binID = floor(tmp);\n    atomicAdd(&bins[binID], 1);\n  }\n}",
            "size_t id = threadIdx.x + blockIdx.x * blockDim.x;\n    if(id < N) {\n        size_t bin = floor(x[id] / 10.0);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    double value = x[idx];\n    if (value < 0) value = 0;\n    if (value < 10) bins[0]++;\n    if (value >= 10 && value < 20) bins[1]++;\n    if (value >= 20 && value < 30) bins[2]++;\n    if (value >= 30 && value < 40) bins[3]++;\n    if (value >= 40 && value < 50) bins[4]++;\n    if (value >= 50 && value < 60) bins[5]++;\n    if (value >= 60 && value < 70) bins[6]++;\n    if (value >= 70 && value < 80) bins[7]++;\n    if (value >= 80 && value < 90) bins[8]++;\n    if (value >= 90 && value < 100) bins[9]++;\n}",
            "// TODO\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int bid = threadIdx.x;\n    while (tid < N) {\n        bins[int(x[tid] / 10.0)] += 1;\n        tid += blockDim.x * gridDim.x;\n    }\n}",
            "unsigned int thread_id = blockDim.x * blockIdx.x + threadIdx.x;\n    unsigned int num_threads = blockDim.x * gridDim.x;\n\n    // each thread adds to its bin\n    for (unsigned int tid = thread_id; tid < N; tid += num_threads) {\n        double xx = x[tid];\n        if (xx <= 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (xx <= 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (xx <= 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (xx <= 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (xx <= 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (xx <= 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (xx <= 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (xx <= 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (xx <= 90) {\n            atomicAdd(&bins[8], 1);\n        } else if (xx <= 100) {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "// Each thread computes the index of one value in x\n  size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N) return;\n  size_t bin = (size_t)(x[index] / 10);\n  atomicAdd(&bins[bin], 1);\n}",
            "extern __shared__ double shared[];\n\tsize_t blockStart = blockIdx.x * blockDim.x;\n\tsize_t i = threadIdx.x + blockStart;\n\tsize_t j = threadIdx.y;\n\tdouble value;\n\n\tvalue = (i < N)? x[i] : 0;\n\n\t// calculate each bin of x\n\tif (j < 10) {\n\t\tif (value < j * 10) {\n\t\t\tatomicAdd(&bins[j], 1);\n\t\t}\n\t}\n\n\t// use this thread to compute one element in each bin\n\t__syncthreads();\n\tif (j < 10) {\n\t\tif (value >= j * 10 && value < (j + 1) * 10) {\n\t\t\tatomicAdd(&bins[j], 1);\n\t\t}\n\t}\n\t__syncthreads();\n\n\tif (value >= 100) {\n\t\tatomicAdd(&bins[10], 1);\n\t}\n}",
            "// Initialize bins by 0\n   for (int i = 0; i < 10; i++) {\n      bins[i] = 0;\n   }\n\n   // compute index in input array of the element to be processed\n   int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      // compute index in output array of the bin to which element belongs\n      // int b = 9;\n      // if (x[tid] >= 0 && x[tid] < 10) {\n      //    b = 0;\n      // } else if (x[tid] >= 10 && x[tid] < 20) {\n      //    b = 1;\n      // } else if (x[tid] >= 20 && x[tid] < 30) {\n      //    b = 2;\n      // } else if (x[tid] >= 30 && x[tid] < 40) {\n      //    b = 3;\n      // } else if (x[tid] >= 40 && x[tid] < 50) {\n      //    b = 4;\n      // } else if (x[tid] >= 50 && x[tid] < 60) {\n      //    b = 5;\n      // } else if (x[tid] >= 60 && x[tid] < 70) {\n      //    b = 6;\n      // } else if (x[tid] >= 70 && x[tid] < 80) {\n      //    b = 7;\n      // } else if (x[tid] >= 80 && x[tid] < 90) {\n      //    b = 8;\n      // } else if (x[tid] >= 90 && x[tid] < 100) {\n      //    b = 9;\n      // }\n      // atomicAdd(&bins[b], 1);\n      atomicAdd(&bins[floor(x[tid] / 10)], 1);\n   }\n}",
            "double min_val = 0.0;\n  double max_val = 100.0;\n  double delta = max_val - min_val;\n\n  size_t x_i = threadIdx.x + blockDim.x * blockIdx.x;\n  if (x_i >= N) return;\n\n  double val = x[x_i];\n  int bin = (val - min_val) / delta * 10;\n\n  atomicAdd(&bins[bin], 1);\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    if (idx < N) {\n        int bin = floor(x[idx] / 10);\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "int threadId = threadIdx.x + blockDim.x * blockIdx.x;\n    int blockSize = blockDim.x * gridDim.x;\n    // Initialize the counter to 0\n    size_t count = 0;\n    for (int i = threadId; i < N; i += blockSize) {\n        if ((x[i] >= 0) && (x[i] < 10))\n            atomicAdd(bins + (size_t)x[i], 1);\n    }\n    // Make sure that all threads have executed\n    __syncthreads();\n}",
            "int idx = blockDim.x * blockIdx.x + threadIdx.x;\n    if (idx < N) {\n        if (x[idx] < 10.0) bins[0] += 1;\n        else if (x[idx] < 20.0) bins[1] += 1;\n        else if (x[idx] < 30.0) bins[2] += 1;\n        else if (x[idx] < 40.0) bins[3] += 1;\n        else if (x[idx] < 50.0) bins[4] += 1;\n        else if (x[idx] < 60.0) bins[5] += 1;\n        else if (x[idx] < 70.0) bins[6] += 1;\n        else if (x[idx] < 80.0) bins[7] += 1;\n        else if (x[idx] < 90.0) bins[8] += 1;\n        else if (x[idx] < 100.0) bins[9] += 1;\n    }\n}",
            "// The index of this thread in the thread block\n    const int threadIdx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadIdx < 10)\n        atomicAdd(&bins[threadIdx], __double2int_rn(x[threadIdx]));\n}",
            "__shared__ size_t binCounts[10];\n  // Initialize the bin counts to zero.\n  if (threadIdx.x < 10) {\n    binCounts[threadIdx.x] = 0;\n  }\n  __syncthreads();\n\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    double d = x[idx];\n    size_t i = (size_t)floor(d / 10);\n    atomicAdd(&binCounts[i], 1);\n  }\n  __syncthreads();\n\n  // Accumulate counts from threads in the block.\n  if (threadIdx.x < 10) {\n    atomicAdd(&bins[threadIdx.x], binCounts[threadIdx.x]);\n  }\n}",
            "const int tid = threadIdx.x;\n  const int num_threads = blockDim.x;\n\n  // The number of elements each thread processes.\n  const size_t N_per_thread = (N - 1) / num_threads + 1;\n\n  // Calculate the start and end indices of the subarray each thread processes.\n  const int start = tid * N_per_thread;\n  const int end = (tid + 1) * N_per_thread;\n\n  for (int i = start; i < end; i++) {\n    if (x[i] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "int tid = threadIdx.x;\n  int bin = tid / 10;\n  int value = tid % 10;\n  double val = value * 10;\n  for (int i = tid; i < N; i += blockDim.x) {\n    if (x[i] < val) {\n      atomicAdd(bins + bin, 1);\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t gid = blockIdx.x;\n\n    if (gid >= N)\n        return;\n\n    double value = x[gid];\n    if (value < 0)\n        value = 0;\n    if (value >= 100)\n        value = 99;\n    size_t index = value / 10;\n\n    atomicAdd(&bins[index], 1);\n}",
            "//TODO\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n    double *xs = x + bid * N;\n    for (size_t i = tid; i < N; i += 100) {\n        size_t bin = (size_t)xs[i] / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int thread = threadIdx.x + blockIdx.x * blockDim.x;\n\tif (thread >= N) {\n\t\treturn;\n\t}\n\tbins[(int) (x[thread] / 10)] += 1;\n}",
            "int myid = threadIdx.x + blockIdx.x * blockDim.x;\n    int step = blockDim.x * gridDim.x;\n    for (int i = myid; i < N; i += step) {\n        if (x[i] >= 0 && x[i] < 10) {\n            atomicAdd(&bins[int(x[i])], 1);\n        }\n    }\n}",
            "// Your code here.\n}",
            "__shared__ int s_bins[10];\n\n  // initialize shared memory\n  if (threadIdx.x < 10) {\n    s_bins[threadIdx.x] = 0;\n  }\n\n  // compute bin count\n  __syncthreads();\n  int bin = (int)floor(x[blockIdx.x] / 10.0);\n  if (bin < 10) {\n    atomicAdd(&s_bins[bin], 1);\n  }\n  __syncthreads();\n\n  // copy from shared to global memory\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    bins[i] += s_bins[i];\n  }\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t stride = blockDim.x * gridDim.x;\n    size_t i = threadId;\n    while(i < N) {\n        size_t j = (size_t)x[i];\n        if(0 <= j && j < 10) {\n            atomicAdd(&bins[j], 1);\n        }\n        i += stride;\n    }\n}",
            "// YOUR CODE HERE\n    return;\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n\tif (tid < N) {\n\t\tint bin = (int)(x[tid]/10.0);\n\t\tatomicAdd(&bins[bin], 1);\n\t}\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid >= N) return;\n\n  bins[(int) ((x[tid] / 10) - 0.00001)] += 1;\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    if (idx >= N) {\n        return;\n    }\n    size_t bin = (size_t)(x[idx] / 10.0);\n    atomicAdd(&bins[bin], 1);\n}",
            "int t = threadIdx.x;\n    double bin = floor(x[t] / 10);\n    if (t < N) {\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t bin;\n\n  while (tid < N) {\n    bin = x[tid] / 10;\n    bins[bin]++;\n    tid += blockDim.x * gridDim.x;\n  }\n}",
            "// TODO\n  // Replace the following __shared__ declaration with a __shared__ array of\n  // length N\n  __shared__ double smem[N];\n\n  // Copy value from global memory to shared memory\n  smem[threadIdx.x] = x[threadIdx.x];\n\n  // Wait for all threads to finish copying to shared memory.\n  // This step is necessary because x is a __constant__ variable in the kernel.\n  __syncthreads();\n\n  // Compute the number of values in [0,10) by using a single thread.\n  // Use the smem[threadIdx.x] value copied from global to shared memory.\n  // Hint: use a conditional branch.\n  int count = 0;\n  if (smem[threadIdx.x] < 10) {\n    count++;\n  }\n\n  // Each thread has a block of threads with the same value of threadIdx.x.\n  // Threads with threadIdx.x = 0 will be responsible for adding their count\n  // to the shared array at index 0, threads with threadIdx.x = 1 will be\n  // responsible for adding their count to the shared array at index 1, and so\n  // on. To accomplish this, threadIdx.x can be used to calculate the index\n  // into the shared array.\n  //\n  // To calculate the index into the shared array, use the following formula:\n  //\n  //   int i = blockDim.x * blockIdx.x + threadIdx.x;\n  //\n  // This formula combines the thread index with the block index to compute a\n  // unique index into the shared array.\n  //\n  // Hint: use blockDim.x and blockIdx.x.\n\n  // TODO\n  int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // TODO\n  if (i < N) {\n    if (smem[i] < 10) {\n      count++;\n    }\n  }\n\n  // TODO\n  // Add the count to the shared array at the index calculated above.\n  // Hint: use a conditional branch.\n  if (threadIdx.x == 0) {\n    bins[0] = count;\n  }\n\n  // Wait for all threads to finish adding the count to the shared array.\n  __syncthreads();\n\n  // Compute the number of values in [10, 20) by using a single thread.\n  // Use the smem[threadIdx.x] value copied from global to shared memory.\n  // Hint: use a conditional branch.\n  count = 0;\n  if (smem[threadIdx.x] >= 10 && smem[threadIdx.x] < 20) {\n    count++;\n  }\n\n  // TODO\n  // Add the count to the shared array at the index calculated above.\n  // Hint: use a conditional branch.\n  if (threadIdx.x == 0) {\n    bins[1] = count;\n  }\n\n  // Wait for all threads to finish adding the count to the shared array.\n  __syncthreads();\n\n  // Compute the number of values in [20, 30) by using a single thread.\n  // Use the smem[threadIdx.x] value copied from global to shared memory.\n  // Hint: use a conditional branch.\n  count = 0;\n  if (smem[threadIdx.x] >= 20 && smem[threadIdx.x] < 30) {\n    count++;\n  }\n\n  // TODO\n  // Add the count to the shared array at the index calculated above.\n  // Hint: use a conditional branch.\n  if (threadIdx.x == 0) {\n    bins[2] = count;\n  }\n\n  // Wait for all threads to finish adding the count to the shared array.\n  __syncthreads();\n\n  // Compute the number of values in [30, 40) by using a single thread.\n  // Use the smem[threadIdx.x] value copied from global to shared memory.\n  // Hint: use a conditional branch.\n  count = 0;\n  if (smem[threadIdx.x] >= 30 && smem[threadIdx.x] < 40) {\n    count++;\n  }\n\n  // TODO\n  // Add the count to the shared array at the index calculated above.\n  // Hint: use a conditional branch.\n  if (threadIdx.x == 0) {\n    bins[3] = count;\n  }\n\n  // Wait for all threads to finish adding the count to the shared array.\n  __syncthreads();\n\n  // Compute the number of values in [40, 50) by using a single thread.\n  // Use the smem[threadIdx.x] value copied from global to shared memory",
            "// YOUR CODE HERE\n    int idx = threadIdx.x + blockIdx.x*blockDim.x;\n    if(idx >= N)\n        return;\n\n    int binIdx = (int) (x[idx] / 10);\n    if(binIdx == 10)\n        binIdx = 9;\n    atomicAdd(&(bins[binIdx]), 1);\n}",
            "// TODO: Fill this in.\n}",
            "// TODO: replace the following dummy code with your implementation\n\n    // get thread id and number of threads\n    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t num_threads = blockDim.x * gridDim.x;\n\n    // loop over elements in x\n    for (size_t i = tid; i < N; i += num_threads) {\n        // TODO: replace the following dummy code with your implementation\n    }\n}",
            "size_t bin = 0;\n   if (threadIdx.x < N) {\n      bin = (size_t) floor(x[threadIdx.x] / 10.0);\n   }\n   atomicAdd(&bins[bin], 1);\n}",
            "// TODO: Your code here\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    size_t bin = (size_t)(10 * x[i]) % 10;\n    atomicAdd(&(bins[bin]), 1);\n}",
            "const size_t tid = threadIdx.x;\n  size_t binid = (tid + 1) / 2;\n  size_t value = (tid % 2 == 0)? (x[tid] / 10) : ((x[tid] - 10) / 10);\n  if (value < 10) {\n    atomicAdd(&bins[value], (size_t)1);\n  }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n    for (size_t i = idx; i < N; i += gridDim.x * blockDim.x) {\n        bins[(size_t)floor(x[i] / 10)] += 1;\n    }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        double v = x[i];\n        size_t bin = (size_t) (v/10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: implement this function\n    size_t id = threadIdx.x;\n    if(id < N)\n    {\n        double value = x[id];\n        int digit = value / 10;\n        atomicAdd(&bins[digit], 1);\n    }\n}",
            "size_t tid = threadIdx.x;\n    size_t i = blockIdx.x * blockDim.x + tid;\n\n    if (i < N) {\n        bins[x[i] / 10] += 1;\n    }\n}",
            "//TODO: Your code goes here\n}",
            "const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    if (x[tid] < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[tid] < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[tid] < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[tid] < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[tid] < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[tid] < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[tid] < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[tid] < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[tid] < 90) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[tid] < 100) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// TODO: Your implementation should go here.\n}",
            "int threadId = blockDim.x * blockIdx.x + threadIdx.x;\n  int N_per_block = blockDim.x * gridDim.x;\n  for (size_t i = threadId; i < N; i += N_per_block) {\n    double value = x[i];\n    double bucket = value / 10.0;\n    int bin_id = floor(bucket);\n    atomicAdd(&bins[bin_id], 1);\n  }\n}",
            "// Initialize the bins array\n  int i = threadIdx.x;\n  if (i < 10) {\n    bins[i] = 0;\n  }\n  __syncthreads();\n\n  // Compute the bins\n  int j = blockIdx.x * blockDim.x + threadIdx.x;\n  if (j < N) {\n    double value = x[j];\n    int index = value / 10;\n    atomicAdd(&(bins[index]), 1);\n  }\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t bin = 0;\n\n    while (index < N) {\n        if (x[index] >= 0 && x[index] < 10) {\n            atomicAdd(&bins[bin], 1);\n        }\n        index += blockDim.x * gridDim.x;\n    }\n}",
            "int idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    int bin = 1 + (int) x[idx] / 10;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t value = threadId < N? (size_t)floor(x[threadId] / 10) : 0;\n  size_t bin = value < 10? value : 9;\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\tif (tid < N) {\n\t\tsize_t bin = x[tid] / 10;\n\t\tatomicAdd(bins + bin, 1);\n\t}\n}",
            "__shared__ size_t _bins[10];\n    int tid = threadIdx.x;\n    int block_size = blockDim.x;\n    int block_id = blockIdx.x;\n    _bins[tid] = 0;\n    for (int i = 0; i < (N - block_size); i += block_size) {\n        if (x[i + tid] <= 10)\n            _bins[0] += 1;\n        else if (x[i + tid] <= 20)\n            _bins[1] += 1;\n        else if (x[i + tid] <= 30)\n            _bins[2] += 1;\n        else if (x[i + tid] <= 40)\n            _bins[3] += 1;\n        else if (x[i + tid] <= 50)\n            _bins[4] += 1;\n        else if (x[i + tid] <= 60)\n            _bins[5] += 1;\n        else if (x[i + tid] <= 70)\n            _bins[6] += 1;\n        else if (x[i + tid] <= 80)\n            _bins[7] += 1;\n        else if (x[i + tid] <= 90)\n            _bins[8] += 1;\n        else\n            _bins[9] += 1;\n    }\n    for (int i = (N - block_size); i < N; i++) {\n        if (x[i] <= 10)\n            _bins[0] += 1;\n        else if (x[i] <= 20)\n            _bins[1] += 1;\n        else if (x[i] <= 30)\n            _bins[2] += 1;\n        else if (x[i] <= 40)\n            _bins[3] += 1;\n        else if (x[i] <= 50)\n            _bins[4] += 1;\n        else if (x[i] <= 60)\n            _bins[5] += 1;\n        else if (x[i] <= 70)\n            _bins[6] += 1;\n        else if (x[i] <= 80)\n            _bins[7] += 1;\n        else if (x[i] <= 90)\n            _bins[8] += 1;\n        else\n            _bins[9] += 1;\n    }\n    __syncthreads();\n\n    // merge result of each block\n    for (int i = block_size / 2; i > 0; i >>= 1) {\n        if (tid < i) {\n            _bins[tid] += _bins[tid + i];\n        }\n        __syncthreads();\n    }\n    if (tid == 0) {\n        bins[0] = _bins[0];\n        bins[1] = _bins[1];\n        bins[2] = _bins[2];\n        bins[3] = _bins[3];\n        bins[4] = _bins[4];\n        bins[5] = _bins[5];\n        bins[6] = _bins[6];\n        bins[7] = _bins[7];\n        bins[8] = _bins[8];\n        bins[9] = _bins[9];\n    }\n}",
            "size_t tid = threadIdx.x;\n\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    size_t value = x[i] / 10;\n    bins[value] = bins[value] + 1;\n  }\n}",
            "extern __shared__ size_t local_bins[];\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    local_bins[i] = 0;\n  }\n\n  __syncthreads();\n\n  size_t chunkSize = (N + blockDim.x - 1) / blockDim.x;\n  size_t start = chunkSize * blockIdx.x;\n  size_t end = min(start + chunkSize, N);\n  for (size_t j = start + threadIdx.x; j < end; j += blockDim.x) {\n    double val = x[j];\n    int bin = (int)val / 10;\n    atomicAdd(&local_bins[bin], 1);\n  }\n\n  __syncthreads();\n\n  for (int i = threadIdx.x; i < 10; i += blockDim.x) {\n    atomicAdd(&bins[i], local_bins[i]);\n  }\n}",
            "size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (n < N) {\n        double v = x[n];\n        if (v < 10) {\n            atomicAdd(&bins[0], 1);\n        } else if (v < 20) {\n            atomicAdd(&bins[1], 1);\n        } else if (v < 30) {\n            atomicAdd(&bins[2], 1);\n        } else if (v < 40) {\n            atomicAdd(&bins[3], 1);\n        } else if (v < 50) {\n            atomicAdd(&bins[4], 1);\n        } else if (v < 60) {\n            atomicAdd(&bins[5], 1);\n        } else if (v < 70) {\n            atomicAdd(&bins[6], 1);\n        } else if (v < 80) {\n            atomicAdd(&bins[7], 1);\n        } else if (v < 90) {\n            atomicAdd(&bins[8], 1);\n        } else {\n            atomicAdd(&bins[9], 1);\n        }\n    }\n}",
            "size_t idx = blockIdx.x*blockDim.x+threadIdx.x;\n  if (idx >= N)\n    return;\n  double y = x[idx];\n  if (y <= 10)\n    atomicAdd(&bins[0], 1);\n  if (y <= 20)\n    atomicAdd(&bins[1], 1);\n  if (y <= 30)\n    atomicAdd(&bins[2], 1);\n  if (y <= 40)\n    atomicAdd(&bins[3], 1);\n  if (y <= 50)\n    atomicAdd(&bins[4], 1);\n  if (y <= 60)\n    atomicAdd(&bins[5], 1);\n  if (y <= 70)\n    atomicAdd(&bins[6], 1);\n  if (y <= 80)\n    atomicAdd(&bins[7], 1);\n  if (y <= 90)\n    atomicAdd(&bins[8], 1);\n  if (y <= 100)\n    atomicAdd(&bins[9], 1);\n}",
            "// Compute thread's unique id (from 0 to N-1)\n    const size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N)\n        return;\n    // Compute bin number\n    const int bin = (int)(x[i]/10);\n    // Store result\n    if (bin < 10)\n        atomicAdd(&(bins[bin]), 1);\n}",
            "__shared__ size_t smem[100];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n\n    for (int i = 0; i < N; i += 100) {\n        if (i + tid < N) {\n            double value = x[i + tid];\n            int index = (value / 10);\n            smem[tid] = index;\n        }\n        __syncthreads();\n        if (i + tid < N) {\n            int index = smem[tid];\n            atomicAdd(&bins[index], 1);\n        }\n    }\n}",
            "extern __shared__ size_t smem[];\n  size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  double val = 0.0;\n\n  while (i < N) {\n    val = x[i];\n    if (val < 10) {\n      atomicAdd(&smem[0], 1);\n    }\n    else if (val < 20) {\n      atomicAdd(&smem[1], 1);\n    }\n    else if (val < 30) {\n      atomicAdd(&smem[2], 1);\n    }\n    else if (val < 40) {\n      atomicAdd(&smem[3], 1);\n    }\n    else if (val < 50) {\n      atomicAdd(&smem[4], 1);\n    }\n    else if (val < 60) {\n      atomicAdd(&smem[5], 1);\n    }\n    else if (val < 70) {\n      atomicAdd(&smem[6], 1);\n    }\n    else if (val < 80) {\n      atomicAdd(&smem[7], 1);\n    }\n    else if (val < 90) {\n      atomicAdd(&smem[8], 1);\n    }\n    else if (val < 100) {\n      atomicAdd(&smem[9], 1);\n    }\n    i += blockDim.x * gridDim.x;\n  }\n  // Make sure all threads have finished writing before copying to global memory\n  __syncthreads();\n  if (tid < 10) {\n    bins[tid] = smem[tid];\n  }\n}",
            "// TODO: Your code here\n}",
            "__shared__ size_t binsBy10[10];\n\n    // 0 <= tid < blockDim.x\n    int tid = threadIdx.x;\n    // 0 <= i < N\n    int i = blockIdx.x * blockDim.x + tid;\n    int bin = 0;\n\n    while (i < N) {\n        bin = floor(x[i] / 10.0);\n        atomicAdd(binsBy10 + bin, 1);\n        i += blockDim.x * gridDim.x;\n    }\n\n    __syncthreads();\n\n    // The binsBy10[i] should be the same as bins[i]\n    if (tid == 0) {\n        for (int i = 0; i < 10; i++) {\n            atomicAdd(bins + i, binsBy10[i]);\n        }\n    }\n}",
            "// TODO: Implement this method\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x;\n    int tid = threadIdx.x;\n\n    if (j < N) {\n        double val = x[j];\n        int bin = (int) (val / 10);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "// INSERT YOUR CODE HERE\n    __syncthreads();\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    if (x[i] < 10.0) {\n      atomicAdd(&bins[0], 1);\n    } else if (x[i] < 20.0) {\n      atomicAdd(&bins[1], 1);\n    } else if (x[i] < 30.0) {\n      atomicAdd(&bins[2], 1);\n    } else if (x[i] < 40.0) {\n      atomicAdd(&bins[3], 1);\n    } else if (x[i] < 50.0) {\n      atomicAdd(&bins[4], 1);\n    } else if (x[i] < 60.0) {\n      atomicAdd(&bins[5], 1);\n    } else if (x[i] < 70.0) {\n      atomicAdd(&bins[6], 1);\n    } else if (x[i] < 80.0) {\n      atomicAdd(&bins[7], 1);\n    } else if (x[i] < 90.0) {\n      atomicAdd(&bins[8], 1);\n    } else if (x[i] < 100.0) {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "int index = threadIdx.x + blockDim.x * blockIdx.x;\n  int tid = threadIdx.x;\n  int block_size = blockDim.x;\n  int grid_size = gridDim.x;\n  if (index >= N)\n    return;\n  for (int i = tid; i < 10; i += block_size)\n    bins[i] = 0;\n  __syncthreads();\n  for (int i = index; i < N; i += grid_size) {\n    int bin = (int)(10 * x[i] / 100);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Fill in your code here.\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tint j = (int) (x[i] / 10);\n\t\tatomicAdd(&bins[j], 1);\n\t}\n}",
            "// Fill this in!\n}",
            "// TODO: Your code goes here\n}",
            "int tid = threadIdx.x;\n  int i = blockIdx.x*blockDim.x + tid;\n  if(i < N) {\n    double xi = x[i];\n    if (xi < 10) {\n      atomicAdd(&bins[0], 1);\n    } else if (xi < 20) {\n      atomicAdd(&bins[1], 1);\n    } else if (xi < 30) {\n      atomicAdd(&bins[2], 1);\n    } else if (xi < 40) {\n      atomicAdd(&bins[3], 1);\n    } else if (xi < 50) {\n      atomicAdd(&bins[4], 1);\n    } else if (xi < 60) {\n      atomicAdd(&bins[5], 1);\n    } else if (xi < 70) {\n      atomicAdd(&bins[6], 1);\n    } else if (xi < 80) {\n      atomicAdd(&bins[7], 1);\n    } else if (xi < 90) {\n      atomicAdd(&bins[8], 1);\n    } else {\n      atomicAdd(&bins[9], 1);\n    }\n  }\n}",
            "// get the thread ID\n   int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // return if i is more than N\n   if(i >= N)\n      return;\n\n   // use the thread ID as an index into x to get the value\n   double value = x[i];\n\n   // return if the value is outside the range of bins\n   if(value < 0 || value >= 100)\n      return;\n\n   // use the thread ID to set the count in the correct bin\n   // (the +1 is to make the bin index 0-indexed)\n   bins[int(value/10) + 1]++;\n}",
            "// Each thread will process one element\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (idx >= N) {\n        return;\n    }\n\n    // Compute the index of the bin to which this element belongs\n    int binIndex = min(int(x[idx] / 10), 9);\n\n    // Compute the total number of elements in each bin.\n    // Store in global memory.\n    __shared__ int count[10];\n    atomicAdd(&count[binIndex], 1);\n\n    // Wait for all threads to complete\n    __syncthreads();\n\n    // Add the counts from each bin to global memory\n    atomicAdd(&bins[binIndex], count[binIndex]);\n}",
            "const size_t t_idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const size_t t_idy = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (t_idx < N) {\n        const int i = (int)floor(x[t_idx]);\n\n        const int start_x = t_idx % 10;\n        const int start_y = t_idy % 10;\n\n        if (i == start_x) {\n            atomicAdd(&bins[start_x + start_y * 10], 1);\n        }\n    }\n}",
            "// Compute the index of this thread in the array.\n  // Every thread will index into x.\n  int index = threadIdx.x;\n  // Each thread processes at most 10 values, but may process fewer.\n  for (int i = 0; i < N / 10; i++) {\n    // Every 10th value will be greater than or equal to 10, so each thread will only\n    // process 10 values if its index is less than 10.\n    if (index < 10) {\n      if (x[i * 10 + index] >= 0 && x[i * 10 + index] < 10) {\n        // x[i * 10 + index] is in [0,10). Increment the bin corresponding to this\n        // value.\n        atomicAdd(&bins[index], 1);\n      }\n    }\n  }\n}",
            "// YOUR CODE GOES HERE\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n   if(i >= N) return;\n   size_t bin = (size_t)(x[i]/10);\n   atomicAdd(&bins[bin], 1);\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int blockID = blockIdx.x;\n  if (tid >= N) {\n    return;\n  }\n  // Initialize the bin values to 0\n  for (int i = 0; i < 10; i++) {\n    bins[i] = 0;\n  }\n  // Compute bin counts\n  int bin = (int)(x[tid] / 10);\n  atomicAdd(&(bins[bin]), 1);\n}",
            "int tid = threadIdx.x;\n    int gid = blockIdx.x * blockDim.x + threadIdx.x;\n    int stride = gridDim.x * blockDim.x;\n\n    // Initialize bins to zero.\n    for (int i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n\n    for (int i = gid; i < N; i += stride) {\n        // Get the index of the bin.\n        int index = floor(x[i] / 10);\n        if (index < 0) {\n            index = 0;\n        } else if (index > 9) {\n            index = 9;\n        }\n\n        // Increment the bin.\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "// Your code goes here.\n\t// Use 10 bins:\n\t// bins[0] counts for values in [0, 10)\n\t// bins[1] counts for values in [10, 20)\n\t//...\n\n\t// Each thread will add 1 to one of the 10 bins.\n\t// The value of each thread is the value between [start, end].\n\t// The end value is a multiple of 10 and may not be equal to start + 10.\n\t// When start + 10 > end, this thread will not do anything.\n\n\t// You can assume that `bins` is initialized to zeros on the host before the kernel is called.\n\t// You can assume that start + 10 > end on the host before the kernel is called.\n\t// The kernel should be launched with at least as many threads as values in `x`.\n\t// You may assume that `x` is a pointer to an array of `N` values of type `double`.\n\t// You may assume that N is a multiple of 10.\n\t// You may assume that the values in `x` will be within the range [0, 100], inclusive.\n\n\t// To calculate the correct index in `bins` use the formula:\n\t// bin = ((x - start) / (end - start)) * bins.size()\n\t// where start = 10 * (threadIdx.x % 10) and end = 10 * (threadIdx.x % 10) + 10\n\t// For example, if threadIdx.x is 16, then start = 160 and end = 170.\n\t// You can use a loop to do this, but you should not have to.\n\n\t// You can assume that the values in `bins` will be between [0, N].\n\t// You can use an atomic operation to update `bins`.\n\t// You can assume that your kernel is called with a single block of 1024 threads.\n}",
            "// TODO: Your code here\n  const int id = threadIdx.x + blockIdx.x * blockDim.x;\n  const int binID = id / 10;\n  const double start = (binID == 0)? 0 : 10 * (binID - 1);\n  const double end = (10 * binID) + 10;\n  if (start <= x[id] && x[id] < end) {\n    atomicAdd(bins + binID, 1);\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    int bin = (int) floor(x[tid] / 10);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int i = threadIdx.x;\n    int j = blockIdx.x;\n\n    if (i < N) {\n        int bin = (int) (x[i] / 10.0);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  unsigned int i = tid;\n\n  while (i < N) {\n    double value = x[i];\n    if (value >= 0 && value < 10)\n      atomicAdd(&bins[value], 1);\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        size_t index = (size_t) ((x[i] / 10) + 0.5);\n        atomicAdd(&bins[index], 1);\n    }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) return;\n   int bin_index = int((x[idx] / 10.0) + 0.5);\n   atomicAdd(&bins[bin_index], 1);\n}",
            "__shared__ size_t localBins[10];\n   int idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (idx < N) {\n      double val = x[idx];\n      if (val >= 0 && val < 10) {\n         atomicAdd(&localBins[int(val)], 1);\n      }\n   }\n\n   __syncthreads();\n\n   if (threadIdx.x < 10) {\n      atomicAdd(&bins[threadIdx.x], localBins[threadIdx.x]);\n   }\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x;\n   size_t blockSize = blockDim.x;\n   size_t gridSize = blockDim.x * gridDim.x;\n   size_t bin = 0;\n   for (size_t i = blockIdx.x * blockSize + tid; i < N; i += gridSize) {\n      double value = x[i];\n      bin = (size_t) (value / 10.0);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// Initialize all threads with their index, and let one thread process\n    // each value.\n    const size_t index = threadIdx.x;\n    const size_t stride = blockDim.x;\n    size_t[10] bin_count = {0};\n    for (size_t i = index; i < N; i += stride) {\n        const int val = int(x[i]);\n        // Convert the double value to an integer value between 0 and 9.\n        const int val_bin = val / 10;\n        bin_count[val_bin]++;\n    }\n\n    // Add the count of the bins to each bin.\n    for (size_t i = 1; i < 10; i++) {\n        bin_count[i] += bin_count[i-1];\n    }\n\n    // Store the count of values in each bin in the global memory.\n    if (index < 10) {\n        bins[index] = bin_count[index];\n    }\n}",
            "__shared__ size_t partialBins[10];\n  size_t localIdx = threadIdx.x;\n  partialBins[0] = 0;\n  for (size_t i = 0; i < N; i += gridDim.x) {\n    double val = x[i + blockIdx.x];\n    if (val < 10) {\n      atomicAdd(&partialBins[0], 1);\n    } else if (val < 20) {\n      atomicAdd(&partialBins[1], 1);\n    } else if (val < 30) {\n      atomicAdd(&partialBins[2], 1);\n    } else if (val < 40) {\n      atomicAdd(&partialBins[3], 1);\n    } else if (val < 50) {\n      atomicAdd(&partialBins[4], 1);\n    } else if (val < 60) {\n      atomicAdd(&partialBins[5], 1);\n    } else if (val < 70) {\n      atomicAdd(&partialBins[6], 1);\n    } else if (val < 80) {\n      atomicAdd(&partialBins[7], 1);\n    } else if (val < 90) {\n      atomicAdd(&partialBins[8], 1);\n    } else {\n      atomicAdd(&partialBins[9], 1);\n    }\n  }\n\n  __syncthreads();\n\n  for (int i = 1; i < 10; i++) {\n    atomicAdd(&partialBins[0], partialBins[i]);\n  }\n\n  if (localIdx < 10) {\n    atomicAdd(&bins[localIdx], partialBins[localIdx]);\n  }\n}",
            "unsigned int tid = threadIdx.x;\n\tunsigned int i = blockIdx.x * blockDim.x + tid;\n\t__shared__ size_t thread_bins[10];\n\n\tif (i < N) {\n\t\tthread_bins[x[i] / 10] += 1;\n\t}\n\t__syncthreads();\n\n\tif (tid < 10) {\n\t\tatomicAdd(&bins[tid], thread_bins[tid]);\n\t}\n}",
            "extern __shared__ double shared[];\n    size_t tid = threadIdx.x;\n\n    int bin = tid / 10;\n    int offset = (tid % 10) * N;\n\n    double *local_x = (double*)(&shared[offset]);\n    double *local_bins = (double*)(&shared[offset + N]);\n\n    // Copy data to local memory\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        local_x[i] = x[i];\n    }\n\n    __syncthreads();\n\n    // Compute bin count\n    size_t local_count = 0;\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        if (local_x[i] >= bin * 10 && local_x[i] < (bin + 1) * 10) {\n            local_bins[bin] += 1;\n            local_count += 1;\n        }\n    }\n\n    __syncthreads();\n\n    // Reduce local bins to global bins\n    if (tid == 0) {\n        for (size_t i = 0; i < 10; i++) {\n            atomicAdd(&bins[i], local_bins[i]);\n        }\n    }\n}",
            "//TODO: implement this function\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t threadId = tid % 10;\n\n    if(threadId == 0) {\n        bins[threadId] = 0;\n    }\n\n    for(int i = tid; i < N; i += blockDim.x * gridDim.x) {\n        int idx = x[i] / 10;\n        atomicAdd(&bins[idx], 1);\n    }\n}",
            "size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t end = N;\n  for (size_t i = start; i < end; i += blockDim.x * gridDim.x) {\n    bins[floor(x[i] / 10)]++;\n  }\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n    if (tid < N) {\n        int bucket = static_cast<int>(floor((x[tid] + 10) / 10));\n        atomicAdd(&bins[bucket], 1);\n    }\n}",
            "__shared__ size_t shared_bins[10];\n\n  size_t idx = threadIdx.x;\n  if(idx >= 10)\n    return;\n\n  double start = (idx * 10 + 0) * 10.0;\n  double end = (idx * 10 + 10) * 10.0;\n\n  size_t count = 0;\n  for(size_t i = 0; i < N; ++i)\n    if(x[i] >= start && x[i] < end)\n      count++;\n\n  shared_bins[idx] = count;\n\n  __syncthreads();\n\n  // The thread with the lowest index adds up the counts for each bin.\n  if(idx == 0) {\n    for(int i = 1; i < 10; ++i) {\n      shared_bins[0] += shared_bins[i];\n    }\n\n    atomicAdd(&bins[0], shared_bins[0]);\n    atomicAdd(&bins[1], shared_bins[1]);\n    atomicAdd(&bins[2], shared_bins[2]);\n    atomicAdd(&bins[3], shared_bins[3]);\n    atomicAdd(&bins[4], shared_bins[4]);\n    atomicAdd(&bins[5], shared_bins[5]);\n    atomicAdd(&bins[6], shared_bins[6]);\n    atomicAdd(&bins[7], shared_bins[7]);\n    atomicAdd(&bins[8], shared_bins[8]);\n    atomicAdd(&bins[9], shared_bins[9]);\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t blockId = blockIdx.x;\n    // load all data to register\n    double data[10];\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        data[i % 10] = x[i];\n    }\n    __syncthreads();\n    // accumulate\n    for (size_t i = tid; i < 10; i += blockDim.x) {\n        bins[i] += __popcll(data[i] * 100.0);\n    }\n}",
            "__shared__ int count[10]; // Thread-private count array.\n\n    // Initialize the count array to zero on the first pass through the loop.\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 10; i++) {\n            count[i] = 0;\n        }\n    }\n\n    __syncthreads(); // Make sure all threads have initialized the count array.\n\n    int bin = (int)(10.0 * x[blockIdx.x] / 100.0); // Compute the bin index.\n\n    if (bin < 0) bin = 0;\n    if (bin > 9) bin = 9;\n\n    atomicAdd(&count[bin], 1); // Increment count for this bin.\n\n    // Store the count array to global memory.\n    if (threadIdx.x == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] = count[i];\n        }\n    }\n}",
            "const int idx = blockIdx.x * blockDim.x + threadIdx.x;\n    const int stride = blockDim.x * gridDim.x;\n    for (int i = idx; i < N; i += stride) {\n        int bin = (int) x[i] / 10;\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "size_t bin = threadIdx.x / N;\n    size_t i = threadIdx.x % N;\n    if (bin >= 10) return;\n    bins[bin] = (x[i] >= bin * 10 && x[i] < (bin + 1) * 10)? bins[bin] + 1 : bins[bin];\n}",
            "// TODO: implement this function\n  // __shared__ double x_shared[N];\n  // int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  // if (tid < N) x_shared[tid] = x[tid];\n  // __syncthreads();\n  // if (tid < N) {\n  //   double x_value = x_shared[tid];\n  //   if (x_value < 10) ++bins[0];\n  //   else if (x_value < 20) ++bins[1];\n  //   else if (x_value < 30) ++bins[2];\n  //   else if (x_value < 40) ++bins[3];\n  //   else if (x_value < 50) ++bins[4];\n  //   else if (x_value < 60) ++bins[5];\n  //   else if (x_value < 70) ++bins[6];\n  //   else if (x_value < 80) ++bins[7];\n  //   else if (x_value < 90) ++bins[8];\n  //   else ++bins[9];\n  // }\n  // __syncthreads();\n  // if (tid < 10) atomicAdd(&bins[tid], bins[tid]);\n  // __syncthreads();\n  // if (tid < 10) {\n  //   if (tid == 0) {\n  //     for (int i = 1; i < 10; ++i) bins[0] += bins[i];\n  //   } else {\n  //     bins[tid] += bins[tid-1];\n  //   }\n  // }\n}",
            "// initialize index in the array of bins\n\tconst int i = threadIdx.x + blockIdx.x * blockDim.x;\n\t// check if we need to update this bin\n\tif (i < N) {\n\t\tif (x[i] >= 0 && x[i] < 10) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t}\n\t\tif (x[i] >= 10 && x[i] < 20) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t}\n\t\tif (x[i] >= 20 && x[i] < 30) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t}\n\t\tif (x[i] >= 30 && x[i] < 40) {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t\tif (x[i] >= 40 && x[i] < 50) {\n\t\t\tatomicAdd(&bins[4], 1);\n\t\t}\n\t\tif (x[i] >= 50 && x[i] < 60) {\n\t\t\tatomicAdd(&bins[5], 1);\n\t\t}\n\t\tif (x[i] >= 60 && x[i] < 70) {\n\t\t\tatomicAdd(&bins[6], 1);\n\t\t}\n\t\tif (x[i] >= 70 && x[i] < 80) {\n\t\t\tatomicAdd(&bins[7], 1);\n\t\t}\n\t\tif (x[i] >= 80 && x[i] < 90) {\n\t\t\tatomicAdd(&bins[8], 1);\n\t\t}\n\t\tif (x[i] >= 90 && x[i] < 100) {\n\t\t\tatomicAdd(&bins[9], 1);\n\t\t}\n\t}\n}",
            "int tid = threadIdx.x;\n  int blockId = blockIdx.x;\n  int numThreads = blockDim.x;\n  int bin = (int) ((blockId * numThreads + tid) * 10 / N);\n  if (bin >= 10) {\n    return;\n  }\n  double x_sum = 0;\n  for (int i = blockId * numThreads + tid; i < N; i += numThreads * gridDim.x) {\n    x_sum += x[i];\n  }\n  double x_mean = x_sum / N;\n  atomicAdd(&bins[bin], (int) (x_sum / (x_mean + 1e-12) / 10 + 1e-12));\n}",
            "int tid = threadIdx.x; // thread id\n  int bid = blockIdx.x;  // block id\n  double l = bid * 10;   // lower bound\n  double h = l + 10;     // upper bound\n  int count = 0;\n  for (int i = 0; i < N; i++) {\n    if (x[i] >= l && x[i] < h) {\n      count += 1;\n    }\n  }\n  atomicAdd(&bins[tid], count);\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid >= N) return;\n    if (x[tid] < 10) {\n        atomicAdd(&bins[0], 1);\n    } else if (x[tid] < 20) {\n        atomicAdd(&bins[1], 1);\n    } else if (x[tid] < 30) {\n        atomicAdd(&bins[2], 1);\n    } else if (x[tid] < 40) {\n        atomicAdd(&bins[3], 1);\n    } else if (x[tid] < 50) {\n        atomicAdd(&bins[4], 1);\n    } else if (x[tid] < 60) {\n        atomicAdd(&bins[5], 1);\n    } else if (x[tid] < 70) {\n        atomicAdd(&bins[6], 1);\n    } else if (x[tid] < 80) {\n        atomicAdd(&bins[7], 1);\n    } else if (x[tid] < 90) {\n        atomicAdd(&bins[8], 1);\n    } else {\n        atomicAdd(&bins[9], 1);\n    }\n}",
            "const int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        int bin = (int) (x[idx] / 10);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "/* YOUR CODE HERE */\n    const int i = threadIdx.x;\n    double sum = 0;\n    for(int j=i;j<N;j+=blockDim.x){\n        if((x[j]>=0.0)&&(x[j]<10.0)){\n            sum += 1.0;\n        }\n    }\n    atomicAdd(&bins[0],sum);\n}",
            "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  int bin;\n  for (int i=gid; i<N; i+=gridDim.x*blockDim.x) {\n    bin = x[i] / 10;\n    if (bin >= 10) {\n      printf(\"ERROR: bin = %d\\n\", bin);\n      return;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      int bin = int(x[tid] / 10.0);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "size_t bin = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n  size_t value, digit;\n  for (size_t i = bin; i < N; i += stride) {\n    value = x[i];\n    digit = (size_t) floor(value / 10.0);\n    atomicAdd(&bins[digit], 1);\n  }\n}",
            "unsigned int tid = threadIdx.x;\n  unsigned int block_size = blockDim.x;\n\n  size_t block_start = tid + blockIdx.x * block_size;\n  size_t block_end = block_start + block_size;\n\n  unsigned int block_bin = 0;\n\n  for (size_t i = block_start; i < block_end; i++) {\n    size_t bin = floor(x[i] / 10);\n    if (bin < 10) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "// Compute the index of the thread in the block\n    int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // For each thread, initialize the bin to 0\n    int i = 0;\n\n    // Iterate over the array and compute the sum of all numbers in [0,10),\n    // [10, 20), [20, 30),...\n    for (i = 0; i < N; i++) {\n        // The condition checks if the index is in the corresponding bin\n        if (x[i] >= i * 10 && x[i] < (i + 1) * 10)\n            atomicAdd(&bins[i], 1);\n    }\n}",
            "__shared__ double sdata[10];\n  unsigned tid = threadIdx.x;\n  unsigned i = blockIdx.x*blockDim.x+threadIdx.x;\n  unsigned gridSize = blockDim.x*gridDim.x;\n  double mybin = 0;\n  for (; i<N; i+=gridSize) {\n    mybin = floor(x[i]/10);\n    atomicAdd(&bins[mybin],1);\n  }\n  sdata[tid] = mybin;\n  __syncthreads();\n\n  // Reduce the bins by 10\n  if (tid < 9) {\n    atomicAdd(&bins[sdata[tid]+1], sdata[tid+1]);\n  }\n}",
            "int i = threadIdx.x;\n    int stride = blockDim.x;\n    int block = blockIdx.x;\n    int start = block * stride;\n    for (int j = i + start; j < N; j += stride * gridDim.x) {\n        int b = (int) (x[j] / 10);\n        atomicAdd(bins + b, 1);\n    }\n}",
            "// Your code here\n}",
            "// Your code here\n   //...\n}",
            "int t = threadIdx.x;\n  int i = blockIdx.x * blockDim.x + t;\n  bins[t] = 0;\n  __syncthreads();\n  if (i < N) {\n    int bin = floor(x[i]/10.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int blockSize = blockDim.x;\n\n  __shared__ double smem[1024];\n  double blockMin[10], blockMax[10];\n  double x_val;\n\n  // Initialize blockMin and blockMax\n  for (int i = tid; i < 10; i += blockSize) {\n    blockMin[i] = blockMax[i] = x[bid*10+i];\n  }\n\n  __syncthreads();\n\n  // Each thread works on one value\n  for (int i = tid; i < N; i += blockSize) {\n    x_val = x[bid*10 + i%10];\n    int bin = (x_val - blockMin[i%10])/10.0;\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t threadID = threadIdx.x;\n  size_t blockSize = blockDim.x;\n  size_t gridSize = blockSize * gridDim.x;\n  size_t i = blockIdx.x * blockSize + threadID;\n  size_t start = 10 * (i / 10);\n  size_t end = start + 10;\n\n  while (i < N) {\n    if (start <= x[i] && x[i] < end) {\n      atomicAdd(&bins[i % 10], 1);\n    }\n    i += gridSize;\n  }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n    for (int i = tid; i < N; i += gridDim.x * blockDim.x) {\n        if (x[i] < 0) continue;\n        else if (x[i] < 10) bins[0]++;\n        else if (x[i] < 20) bins[1]++;\n        else if (x[i] < 30) bins[2]++;\n        else if (x[i] < 40) bins[3]++;\n        else if (x[i] < 50) bins[4]++;\n        else if (x[i] < 60) bins[5]++;\n        else if (x[i] < 70) bins[6]++;\n        else if (x[i] < 80) bins[7]++;\n        else if (x[i] < 90) bins[8]++;\n        else if (x[i] < 100) bins[9]++;\n    }\n}",
            "unsigned int idx = threadIdx.x + blockIdx.x*blockDim.x;\n    if (idx >= N) return;\n\n    double value = x[idx];\n    int bin = (int)(value / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (x[i] < 10)\n      atomicAdd(&bins[0], 1);\n    else if (x[i] < 20)\n      atomicAdd(&bins[1], 1);\n    else if (x[i] < 30)\n      atomicAdd(&bins[2], 1);\n    else if (x[i] < 40)\n      atomicAdd(&bins[3], 1);\n    else if (x[i] < 50)\n      atomicAdd(&bins[4], 1);\n    else if (x[i] < 60)\n      atomicAdd(&bins[5], 1);\n    else if (x[i] < 70)\n      atomicAdd(&bins[6], 1);\n    else if (x[i] < 80)\n      atomicAdd(&bins[7], 1);\n    else if (x[i] < 90)\n      atomicAdd(&bins[8], 1);\n    else\n      atomicAdd(&bins[9], 1);\n  }\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    // round down\n    int k = int(x[i] / 10);\n    atomicAdd(&bins[k], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t gid = blockIdx.x;\n    size_t bin = tid * 10 + gid;\n    size_t N_per_thread = (N + blockDim.x - 1) / blockDim.x;\n    size_t i = N_per_thread * gid;\n    if (i >= N) return;\n    if (i + tid < N) {\n        if (bin < 10) {\n            if (x[i + tid] < 10.0) {\n                atomicAdd(bins + bin, 1);\n            }\n        }\n    }\n}",
            "size_t tid = threadIdx.x;\n  // shared mem is thread-local\n  extern __shared__ int cache[];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] < 10.0)? 1 : 0;\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 10.0 && x[tid] < 20.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 20.0 && x[tid] < 30.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 30.0 && x[tid] < 40.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 40.0 && x[tid] < 50.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 50.0 && x[tid] < 60.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 60.0 && x[tid] < 70.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 70.0 && x[tid] < 80.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 80.0 && x[tid] < 90.0)? cache[tid] + 1 : cache[tid];\n  // each thread loads a value from global mem\n  cache[tid] = (x[tid] >= 90.0 && x[tid] < 100.0)? cache[tid] + 1 : cache[tid];\n\n  // each thread writes to its own bin\n  atomicAdd(&(bins[cache[tid] - 1]), 1);\n}",
            "const double inc = 10.0;\n    const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    const size_t binIdx = tid / (N / 10);\n    const size_t idx = tid % (N / 10);\n    if (tid < N) {\n        atomicAdd(&(bins[binIdx]), (x[idx] >= 0 && x[idx] < inc)? 1 : 0);\n    }\n}",
            "__shared__ double s_x[100];\n\n  int thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (thread_id < N) {\n    s_x[threadIdx.x] = x[thread_id];\n  }\n  __syncthreads();\n\n  if (threadIdx.x < 10) {\n    int bin = 0;\n    for (int i = threadIdx.x; i < N; i += 10) {\n      if ((s_x[i] >= bin * 10) && (s_x[i] < (bin + 1) * 10)) {\n        atomicAdd(&bins[bin], 1);\n      }\n    }\n  }\n}",
            "// Each thread will be given a separate piece of the input array\n    size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n    // Check if the index is out of bounds. If so, do nothing.\n    if (idx >= N) return;\n\n    size_t bin_idx = (size_t)((x[idx] / 10) + 0.5);\n\n    if (bin_idx >= 10) bin_idx = 9;\n    atomicAdd(&bins[bin_idx], 1);\n}",
            "unsigned int tid = threadIdx.x;\n\n  double lo = 0;\n  double hi = 100;\n  size_t bin = 0;\n\n  for(size_t i=tid; i<N; i+=gridDim.x) {\n    if(x[i] >= lo && x[i] < hi) {\n      bins[bin]++;\n    }\n    if(x[i] >= hi) {\n      lo = hi;\n      bin++;\n      hi += 10;\n    }\n  }\n}",
            "size_t x0 = 10;\n  size_t x1 = 20;\n  size_t x2 = 30;\n  size_t x3 = 40;\n  size_t x4 = 50;\n  size_t x5 = 60;\n  size_t x6 = 70;\n  size_t x7 = 80;\n  size_t x8 = 90;\n  size_t x9 = 100;\n  //...\n\n  // TODO: Implement the kernel\n}",
            "unsigned thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  unsigned block_id = blockIdx.x;\n  unsigned block_dim = blockDim.x;\n\n  // Compute the number of values in [0,10), [10, 20), [20, 30),...\n  __shared__ size_t shared[10];\n\n  // Each thread initializes its own counter to 0\n  if (thread_id < 10) {\n    shared[thread_id] = 0;\n  }\n  __syncthreads();\n\n  // Count values in [0, 10), [10, 20), [20, 30),...\n  while (thread_id < N) {\n    unsigned bin = (unsigned)floor(x[thread_id] / 10.0);\n    atomicAdd(&shared[bin], 1);\n    thread_id += block_dim;\n  }\n  __syncthreads();\n\n  // Aggregate results\n  if (thread_id < 10) {\n    atomicAdd(&bins[thread_id], shared[thread_id]);\n  }\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    if (i < N) {\n      // Convert i to a bin number from 0 to 9\n      // 7 / 10 = 0, 32 / 10 = 3, 95 / 10 = 9,...\n      int bin = (int)floor(x[i] / 10);\n      bins[bin] += 1;\n    }\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t bin = tid / 10;\n    if (bin < 10) {\n        bins[bin] += N - tid;\n    }\n}",
            "const int bin_index = threadIdx.x;\n\n  // Each thread gets its own copy of the array and adds its contribution to the\n  // proper bin\n  double local_bins[10];\n  for (size_t i = 0; i < 10; i++) {\n    local_bins[i] = 0;\n  }\n\n  // Iterate through the array adding each value to the right bin\n  for (int i = threadIdx.x; i < N; i += blockDim.x) {\n    int bin = (int)x[i] / 10;\n    local_bins[bin] += 1;\n  }\n\n  // Sum the values from each thread\n  for (int i = bin_index; i < 10; i += blockDim.x) {\n    atomicAdd(&(bins[i]), local_bins[i]);\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t nthreads = blockDim.x * gridDim.x;\n\n    // initialize bins\n    for (size_t i = tid; i < 10; i += nthreads) {\n        bins[i] = 0;\n    }\n    __syncthreads();\n\n    // compute bins\n    for (size_t i = gid; i < N; i += nthreads*gridDim.x) {\n        int bin = int(x[i] / 10);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "//TODO: Implement this kernel.\n    //Hint: You might want to look at the other example codes.\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n\n    while (i < N) {\n        int bin = int(x[i]) / 10;\n        atomicAdd(&bins[bin], 1);\n        i += blockDim.x * gridDim.x;\n    }\n}",
            "__shared__ size_t blockSums[10];\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (i < N) {\n    int bin = (int)(x[i] / 10);\n    atomicAdd(&blockSums[bin], 1);\n  }\n\n  __syncthreads();\n\n  for (int i = 0; i < 10; i++)\n    atomicAdd(&bins[i], blockSums[i]);\n}",
            "const size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        if (x[idx] >= 0 && x[idx] < 10) {\n            bins[0]++;\n        } else if (x[idx] >= 10 && x[idx] < 20) {\n            bins[1]++;\n        } else if (x[idx] >= 20 && x[idx] < 30) {\n            bins[2]++;\n        } else if (x[idx] >= 30 && x[idx] < 40) {\n            bins[3]++;\n        } else if (x[idx] >= 40 && x[idx] < 50) {\n            bins[4]++;\n        } else if (x[idx] >= 50 && x[idx] < 60) {\n            bins[5]++;\n        } else if (x[idx] >= 60 && x[idx] < 70) {\n            bins[6]++;\n        } else if (x[idx] >= 70 && x[idx] < 80) {\n            bins[7]++;\n        } else if (x[idx] >= 80 && x[idx] < 90) {\n            bins[8]++;\n        } else if (x[idx] >= 90 && x[idx] < 100) {\n            bins[9]++;\n        }\n    }\n}",
            "int i = threadIdx.x;\n\tint j = blockIdx.x;\n\n\tif (i < N) {\n\t\tint k = (int)(x[i] / 10);\n\t\tatomicAdd(&bins[k], 1);\n\t}\n}",
            "// TODO: Implement this function\n  // Hints:\n  // - Use __syncthreads() to make sure all threads have finished before\n  //   reading and writing to `bins`\n\n  // Do not change the following line!\n  __shared__ double x_shared[MAX_THREADS_PER_BLOCK];\n\n  if (blockIdx.x * blockDim.x + threadIdx.x < N) {\n    x_shared[threadIdx.x] = x[blockIdx.x * blockDim.x + threadIdx.x];\n  }\n  __syncthreads();\n  int tid = threadIdx.x;\n\n  if (x_shared[tid] >= 0 && x_shared[tid] < 10)\n    atomicAdd(&(bins[x_shared[tid]]), 1);\n}",
            "// Get our global thread ID.\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    // Each thread processes one value.\n    if (i < N) {\n        // Add 1 to the bin corresponding to the value in x[i].\n        atomicAdd(&bins[(int)x[i] / 10], 1);\n    }\n}",
            "//TODO: Implement\n\t__syncthreads();\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int bin = x[tid]/10;\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (tid >= N)\n\t\treturn;\n\tint idx = floor((x[tid]-0)/10);\n\tatomicAdd(&bins[idx], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    if (tid >= N) return;\n    int bin = (int) (x[tid] / 10.0);\n    atomicAdd(&bins[bin], 1);\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if(i < N) {\n      int bin = int(x[i]/10);\n      if(bin == 10) bin = 9;\n      atomicAdd(&(bins[bin]), 1);\n   }\n}",
            "__shared__ double xshared[256];\n  size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n  xshared[threadIdx.x] = x[thread_id];\n  __syncthreads();\n\n  for (size_t i = threadIdx.x; i < 10; i += blockDim.x) {\n    if (i * 10 <= N && thread_id < N) {\n      if (xshared[thread_id] >= (i * 10) && xshared[thread_id] < (i + 1) * 10)\n        atomicAdd(&bins[i], 1);\n    }\n  }\n}",
            "size_t tid = threadIdx.x;\n   size_t start = blockIdx.x * blockDim.x + tid;\n   size_t stride = blockDim.x * gridDim.x;\n   size_t bin = 0;\n   for (size_t i = start; i < N; i += stride) {\n      if (x[i] >= bin * 10 && x[i] < bin * 10 + 10) {\n         atomicAdd(bins + bin, 1);\n      }\n      ++bin;\n   }\n}",
            "double v = x[threadIdx.x];\n    int bin = floor(v / 10);\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO\n  // Your code goes here!\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (tid < N) {\n    // determine which bin value belongs in\n    int bin = static_cast<int>(x[tid] / 10);\n    // add 1 to bin if x[tid] in [bin * 10, (bin + 1) * 10)\n    atomicAdd(&bins[bin], (x[tid] % 10 >= 0 && x[tid] % 10 < 10));\n  }\n}",
            "// TODO: Fill in your kernel code here\n    // This will run once for each item in `x`\n    // You need to use atomic functions to add up the counts of all the items\n    // in each bin.\n    // Remember that the bins are numbered from 0 to 9.\n    // Note: This kernel is very similar to the \"histogram\" kernel\n    //   from last week's lab.\n    // The only change is that you are adding up the counts to `bins`\n    //   instead of storing them in the output.\n    for (int i = 0; i < N; i++) {\n        double num = x[i];\n        int bin = (int) num / 10;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// YOUR CODE HERE\n    __shared__ double cache[2000];\n    size_t cacheIndex = threadIdx.x + blockDim.x * blockIdx.x;\n    if (cacheIndex < N)\n        cache[cacheIndex] = x[cacheIndex];\n    __syncthreads();\n    if (threadIdx.x < 10) {\n        size_t start = (int)(threadIdx.x * 10 / 100 * N);\n        size_t end = (int)((threadIdx.x + 1) * 10 / 100 * N);\n        size_t count = 0;\n        for (size_t i = start; i < end; ++i)\n            if (cache[i] < threadIdx.x * 10 + 10)\n                ++count;\n        bins[threadIdx.x] = count;\n    }\n}",
            "size_t i = threadIdx.x + blockDim.x * blockIdx.x;\n  if(i < N) {\n    int n = (int)(x[i] / 10);\n    atomicAdd(&(bins[n]), 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n    size_t bid = blockIdx.x;\n    size_t stride = blockDim.x;\n    size_t start = bid * stride;\n    size_t end = (start + stride) > N? N : (start + stride);\n    if (start >= end) { return; }\n    const double *x_ptr = x + start;\n    size_t bin;\n    for (size_t i = start + tid; i < end; i += stride) {\n        double x_value = x_ptr[i];\n        bin = (size_t)x_value / 10;\n        if (bin >= 10) {\n            bin = 9;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i >= N)\n\t\treturn;\n\n\tint bin = (int)(x[i]/10);\n\tatomicAdd(&bins[bin], 1);\n}",
            "__shared__ size_t sBins[10];\n    if (threadIdx.x < 10) {\n        sBins[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n    for (; i < N; i += gridDim.x * blockDim.x) {\n        size_t n = static_cast<size_t>(x[i]);\n        sBins[n / 10] += 1;\n    }\n    __syncthreads();\n    if (threadIdx.x < 10) {\n        bins[threadIdx.x] += sBins[threadIdx.x];\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "17_histogram_bin_0-100",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Vector x contains values between 0 and 100, inclusive. Count the number of\n   values in [0,10), [10, 20), [20, 30), ... and store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Example:\n\n   input: [7, 32, 95, 12, 39, 32, 11, 71, 70, 66]\n   output: [1, 2, 0, 3, 0, 0, 1, 2, 0, 1]\n*/\nvoid binsBy10Count(std::vector<double> const& x, std::array<size_t, 10> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "size_t numValues = x.size();\n    if (numValues < 100) {\n        std::cerr << \"Number of values in vector x is less than 100.\\n\";\n        exit(1);\n    }\n\n    size_t numBins = 10;\n    bins.fill(0);\n\n#pragma omp parallel\n    {\n        size_t numThreads = omp_get_num_threads();\n        size_t threadId = omp_get_thread_num();\n        size_t chunkSize = numValues / numThreads;\n        size_t start = threadId * chunkSize;\n        size_t end = start + chunkSize;\n\n        if (threadId == numThreads - 1) {\n            end = numValues;\n        }\n\n        for (size_t i = start; i < end; ++i) {\n            size_t bin = std::floor(x[i] / 10);\n            bins[bin] += 1;\n        }\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG,\n               MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 1; i < numBins; ++i) {\n            bins[i] += bins[i-1];\n        }\n    }\n}",
            "// your code here\n  int world_size;\n  int world_rank;\n  int num_threads = 1;\n  int num_tasks = 1;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n  num_threads = omp_get_max_threads();\n  num_tasks = world_size / num_threads;\n\n  // compute the number of elements to be divided by 10\n  size_t num_values = x.size();\n  // compute the elements per task\n  size_t elements_per_task = num_values / num_tasks;\n  size_t start_pos = world_rank * elements_per_task;\n  size_t end_pos = elements_per_task * (world_rank + 1);\n  if (world_rank == num_tasks - 1) {\n    end_pos = num_values;\n  }\n  int local_bins[10];\n  for (size_t i = 0; i < 10; i++) {\n    local_bins[i] = 0;\n  }\n#pragma omp parallel for\n  for (size_t i = start_pos; i < end_pos; i++) {\n    int element = x[i];\n    if (element < 10) {\n      local_bins[element]++;\n    }\n  }\n  if (world_rank == 0) {\n    for (int i = 1; i < num_tasks; i++) {\n      MPI_Recv(local_bins, 10, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n    for (size_t i = 0; i < 10; i++) {\n      bins[i] = local_bins[i];\n    }\n  }\n  else {\n    MPI_Send(local_bins, 10, MPI_INT, 0, 1, MPI_COMM_WORLD);\n  }\n}",
            "// Write your parallel code here.\n}",
            "// TODO\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int sum = 0;\n    int local_sum = 0;\n    int local_num = 0;\n    if (rank == 0) {\n        int len = x.size();\n        double tmp = len / (double)size;\n        int rest = len % size;\n        int last = 0;\n        for (int i = 0; i < size; i++) {\n            if (i == size - 1) {\n                local_num = rest;\n            } else {\n                local_num = (int)tmp;\n            }\n\n            if (i == 0) {\n                last = 0;\n            } else {\n                last = (int)tmp * (i - 1);\n            }\n            sum += local_num;\n            double l_min = x[last];\n            double l_max = x[last + local_num - 1];\n            if ((l_min >= 0.0) && (l_max <= 10.0)) {\n                for (int j = 0; j < local_num; j++) {\n                    if (x[last + j] < 10.0) {\n                        local_sum++;\n                    }\n                }\n            }\n            last = last + local_num;\n        }\n    }\n\n    MPI_Reduce(&local_sum, &bins[0], 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&local_num, &bins[10], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&sum, &bins[11], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code goes here.\n}",
            "std::vector<size_t> partial_bins(10);\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        size_t bin_index = std::floor(x[i]/10);\n        partial_bins[bin_index]++;\n    }\n\n#pragma omp parallel\n    {\n        auto& bins = partial_bins;\n        size_t total_bins = 0;\n\n        for (int i = 0; i < 10; i++) {\n#pragma omp atomic\n            total_bins += bins[i];\n        }\n\n        if (omp_get_thread_num() == 0) {\n            std::copy(bins.begin(), bins.end(), bins.begin()+1);\n            bins[0] = total_bins;\n        }\n    }\n\n    if (omp_get_thread_num() == 0) {\n        bins[0] = 0;\n    }\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "int numRanks, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> localX(x.size() / numRanks);\n    std::vector<size_t> localBins(10);\n\n    MPI_Scatter(&x[0], x.size() / numRanks, MPI_DOUBLE,\n                &localX[0], x.size() / numRanks, MPI_DOUBLE,\n                0, MPI_COMM_WORLD);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < localX.size(); i++) {\n        size_t bin = (size_t) std::floor(localX[i] / 10);\n        if (bin < 10) localBins[bin]++;\n    }\n\n    MPI_Gather(&localBins[0], 10, MPI_UNSIGNED_LONG,\n               &bins[0], 10, MPI_UNSIGNED_LONG,\n               0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// write your solution here\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Get number of values per rank\n  int numberOfValuesPerRank = x.size() / size;\n\n  // Distribute values\n  std::vector<double> values;\n  values.resize(numberOfValuesPerRank);\n  MPI_Scatter(&x[0], numberOfValuesPerRank, MPI_DOUBLE, &values[0], numberOfValuesPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Compute in parallel\n  std::array<size_t, 10> tmp_bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  #pragma omp parallel num_threads(size)\n  {\n    #pragma omp for schedule(static)\n    for (int i = 0; i < values.size(); ++i) {\n      int index = (int) (values[i] / 10);\n      tmp_bins[index] += 1;\n    }\n  }\n\n  // Sum up the bins\n  MPI_Reduce(&tmp_bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n}",
            "size_t const n = x.size();\n\n  // TODO: implement this function\n\n}",
            "double const lowerLimit = 0, upperLimit = 10;\n  std::vector<size_t> localBins(10);\n  size_t localCount = 0;\n  for(double val : x) {\n    if(val >= lowerLimit && val < upperLimit) {\n      localBins[val - lowerLimit] += 1;\n      localCount++;\n    }\n  }\n  MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(0 == MPI::COMM_WORLD.Get_rank()) {\n    for(auto &bin : bins) {\n      bin = bin/localCount;\n    }\n  }\n}",
            "// Get the number of MPI ranks.\n  int ranks = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n  // Get the rank of this process.\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  // Every rank has a complete copy of x.\n  std::vector<double> x_loc;\n  if (rank == 0) {\n    x_loc.resize(x.size());\n    for (size_t i = 0; i < x.size(); ++i) {\n      x_loc[i] = x[i];\n    }\n  }\n  // Broadcast the complete copy of x to every rank.\n  MPI_Bcast(x_loc.data(), x_loc.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  // Each rank has `x.size()` work to do.\n  // Each work is an iteration of the following loop:\n  //   1. Find the index i of the first value in the subinterval [i*10, (i+1)*10]\n  //   2. Find the index j of the last value in the subinterval [i*10, (i+1)*10]\n  //   3. Increment the counter `bins[i]` for every value k such that x[k] falls\n  //      in the subinterval [i*10, (i+1)*10].\n  //   4. Decrement the counter `bins[i+1]` for every value k such that x[k]\n  //      falls in the subinterval [(i+1)*10, (i+2)*10].\n  //   5. Repeat step 1-4 for all possible values of i.\n  // The sum of all the counters should be equal to `x.size()`.\n  //\n  // The following code is not optimal.\n  // It is a good demonstration of how OpenMP can be used with MPI.\n  //\n  // However, since the problem is so simple and the values in x are\n  // known, an optimal implementation is possible, which is provided\n  // in the `binsBy10CountOpt` function below.\n  size_t local_size = x_loc.size();\n  size_t min_count = 0;\n  size_t max_count = 0;\n  size_t local_bins[10] = {0};\n  for (int i = 0; i < ranks; ++i) {\n    // Skip all the work of the other ranks.\n    if (i!= rank) {\n      continue;\n    }\n    // Compute `bins[i]`\n    for (size_t k = 0; k < local_size; ++k) {\n      // Find the index of the subinterval.\n      // The subinterval is of size 10.\n      int subinterval_index = std::floor(x_loc[k]/10);\n      // If the subinterval index is `i`, then increment the counter.\n      if (subinterval_index == i) {\n        ++local_bins[subinterval_index];\n      }\n      // Otherwise, decrement the counter of the next subinterval.\n      else {\n        --local_bins[subinterval_index+1];\n      }\n    }\n    // Compute `min_count` and `max_count`.\n    min_count = std::min(min_count, local_bins[i]);\n    max_count = std::max(max_count, local_bins[i]);\n  }\n  // Collect the results of the work on all ranks into `bins`\n  MPI_Reduce(local_bins, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // Reduce the results from each rank to rank 0.\n  // This reduces the number of messages sent from `ranks` to 1.\n  // The time complexity of this reduction is O(log(ranks))\n  // versus O(ranks) for a single message.\n  MPI_Bcast(&min_count, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  MPI_Bcast(&max_count, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  // `min_count` should equal the number of elements in [0,10).\n  // `max_count` should equal the number of elements in [90,100].\n  // `min_count` should equal the number of elements in [0,10) on rank 0.\n  // `max_count` should equal the number of elements in [90",
            "/* YOUR CODE HERE */\n  int rank, size;\n  double *x_arr;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Allocate memory for x_arr on rank 0\n  if (rank == 0) {\n    x_arr = new double[x.size()];\n  }\n\n  MPI_Bcast(x.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Distribute x to ranks\n  MPI_Scatter(x.data(), x.size() / size, MPI_DOUBLE, x_arr, x.size() / size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Create a bin for each rank\n  std::array<size_t, 10> localBins;\n  for (auto i = 0; i < localBins.size(); i++) {\n    localBins[i] = 0;\n  }\n\n  // Count values in the bins\n  for (auto i = 0; i < x.size(); i++) {\n    int bin = (int)(x_arr[i] / 10);\n    localBins[bin]++;\n  }\n\n  // Sum the local bins into a single bin on rank 0\n  if (rank == 0) {\n    for (auto i = 1; i < size; i++) {\n      MPI_Recv(localBins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (auto j = 0; j < 10; j++) {\n        bins[j] += localBins[j];\n      }\n    }\n  } else {\n    MPI_Send(localBins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  // Gather results on rank 0\n  MPI_Gather(bins.data(), 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    delete[] x_arr;\n  }\n}",
            "// TODO\n    int world_size, world_rank, name_len;\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Get_processor_name(processor_name, &name_len);\n\n    std::vector<double> my_data = x;\n    std::vector<double> data(x.size());\n    std::vector<double> partial_result(10, 0);\n\n    int chunk_size = 100 / world_size;\n    int last_chunk_size = 100 % world_size;\n\n    MPI_Scatter(my_data.data(), chunk_size * sizeof(double), MPI_BYTE, data.data(),\n                chunk_size * sizeof(double), MPI_BYTE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(x.data() + chunk_size * world_rank, last_chunk_size * sizeof(double),\n                MPI_BYTE, data.data() + chunk_size, last_chunk_size * sizeof(double),\n                MPI_BYTE, 0, MPI_COMM_WORLD);\n\n    int offset = 0;\n    int chunk_count = chunk_size / 10;\n    int last_chunk_count = chunk_size % 10;\n\n    omp_set_num_threads(4);\n    #pragma omp parallel default(shared)\n    {\n        #pragma omp for\n        for (int i = 0; i < chunk_count; i++) {\n            for (int j = 0; j < 10; j++) {\n                partial_result[j] += data[offset + j];\n            }\n            offset += 10;\n        }\n        #pragma omp single\n        {\n            for (int i = 0; i < last_chunk_count; i++) {\n                partial_result[i] += data[offset + i];\n            }\n        }\n    }\n\n    MPI_Gather(partial_result.data(), partial_result.size() * sizeof(double), MPI_BYTE, bins.data(),\n               partial_result.size() * sizeof(double), MPI_BYTE, 0, MPI_COMM_WORLD);\n    if (world_rank == 0) {\n        for (int i = 0; i < 10; i++) {\n            bins[i] += last_chunk_size / 10 * last_chunk_count;\n        }\n    }\n}",
            "if (x.size()!= 100)\n\t\tthrow std::invalid_argument(\"wrong vector size\");\n\t// TODO: your code goes here\n}",
            "// TODO\n  // Hint:\n  //   Each rank will have the same values in `x`, but each rank may have different\n  //   values in `bins`. We can use MPI_Scatter to get the rank-th value in `x`\n  //   to each rank.\n  //   MPI_Scatterv can help to scatter `bins` to each rank.\n  //   Each rank can use a different thread in OpenMP to compute counts.\n  //   Finally, MPI_Gatherv can help to gather `bins` from each rank back to rank 0.\n  size_t num_threads = omp_get_max_threads();\n  bins.fill(0);\n  size_t rank = 0;\n  size_t size = 10;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int *bin_p = bins.data();\n  int *bins_scatter = new int[size];\n  int *bins_gather = new int[size * num_threads];\n  MPI_Scatter(bin_p, size, MPI_INT, bins_scatter, size, MPI_INT, 0, MPI_COMM_WORLD);\n  double start = omp_get_wtime();\n#pragma omp parallel num_threads(num_threads)\n  {\n    int bin = 0;\n#pragma omp for schedule(static)\n    for (int i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0 && x[i] < 10) {\n        bins_scatter[bin]++;\n      }\n      bin = (bin + 1) % size;\n    }\n  }\n  double end = omp_get_wtime();\n  MPI_Gather(bins_scatter, size, MPI_INT, bins_gather, size, MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 0; i < num_threads; ++i) {\n      for (int j = 0; j < 10; ++j) {\n        bins[j] += bins_gather[i * 10 + j];\n      }\n    }\n    double time = end - start;\n    std::cout << \"OpenMP \" << num_threads << \" threads time: \" << time << std::endl;\n  }\n}",
            "// TODO: implement\n}",
            "// Your code here.\n  int num_threads = omp_get_max_threads();\n  int n = x.size();\n  std::vector<int> counts(num_threads, 0);\n  size_t sum = 0;\n  for(size_t i = 0; i < num_threads; ++i) {\n    std::vector<double> thread_x;\n    size_t step = (n + num_threads - 1) / num_threads;\n    size_t start = i * step;\n    size_t end = (i + 1) * step;\n    thread_x.reserve(end - start);\n    for(auto j = start; j < end; ++j) {\n      thread_x.push_back(x[j]);\n    }\n    #pragma omp parallel for num_threads(num_threads)\n    for(size_t j = 0; j < thread_x.size(); ++j) {\n      if(thread_x[j] <= 10) {\n        counts[i]++;\n      }\n    }\n  }\n  for(auto &i : counts) {\n    sum += i;\n  }\n  if(MPI_Rank() == 0) {\n    std::copy(counts.begin(), counts.end(), bins.begin());\n    bins[0] += n - sum;\n  }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins[0]);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t local_size = x.size() / bins[0];\n    size_t my_start = local_size * rank;\n    size_t my_end = local_size * (rank + 1);\n\n    int max_threads = omp_get_max_threads();\n    std::vector<int> thread_start(max_threads);\n    std::vector<int> thread_end(max_threads);\n\n    for (int i = 0; i < max_threads; ++i) {\n        thread_start[i] = my_start + i * local_size / max_threads;\n        thread_end[i] = my_start + (i + 1) * local_size / max_threads;\n    }\n\n    thread_end[max_threads - 1] = my_end;\n\n    std::vector<int> thread_count(max_threads, 0);\n\n    #pragma omp parallel\n    {\n        int id = omp_get_thread_num();\n        for (int i = thread_start[id]; i < thread_end[id]; ++i) {\n            int v = std::floor(x[i]);\n            if (v < 10) {\n                ++thread_count[id];\n            }\n        }\n    }\n\n    // Use MPI to get thread_count from each thread.\n    std::vector<int> thread_count_local(max_threads);\n    MPI_Gather(&thread_count[0], max_threads, MPI_INT, thread_count_local.data(), max_threads, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::vector<int> global_count(bins[0]);\n        // Sum the local counts\n        for (int i = 0; i < bins[0]; ++i) {\n            global_count[i] = 0;\n            for (int j = 0; j < bins[0]; ++j) {\n                global_count[i] += thread_count_local[j * bins[0] + i];\n            }\n        }\n        bins = global_count;\n    }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int const blockSize = x.size() / numRanks;\n\n  if (rank == 0) {\n    bins = std::array<size_t, 10>{};\n  }\n\n  double const localMin = rank * blockSize;\n  double const localMax = (rank + 1) * blockSize - 1;\n\n  size_t localCount = std::count_if(x.cbegin() + std::floor(localMin),\n                                    x.cbegin() + std::ceil(localMax),\n                                    [](double const& d) { return d < 10; });\n\n  MPI_Reduce(&localCount,\n             bins.data() + rank,\n             1,\n             MPI_UNSIGNED_LONG,\n             MPI_SUM,\n             0,\n             MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::partial_sum(bins.cbegin(),\n                     bins.cend(),\n                     bins.begin());\n  }\n}",
            "size_t n = x.size();\n\n  // TODO: implement\n\n  // Hint:\n  // * Each rank will compute a portion of the histogram.\n  // * To avoid having the histogram computed twice on rank 0,\n  //   a rank can send its partial histogram to rank 0 before\n  //   computing its own.\n  // * For OpenMP, the first loop in the function is serial\n  //   and should not be parallelized.\n}",
            "// TODO: compute number of elements in x\n    int n = x.size();\n    // TODO: determine my rank and the total number of ranks\n    int myRank, nRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n    // TODO: compute the number of elements each rank will have\n    int elementsPerRank = n/nRanks;\n    // TODO: compute the elements assigned to this rank\n    int elementsBegin = myRank * elementsPerRank;\n    int elementsEnd = elementsBegin + elementsPerRank;\n    // TODO: compute the number of bins each rank will have\n    int binsPerRank = bins.size()/nRanks;\n    // TODO: compute the bins assigned to this rank\n    int binsBegin = myRank * binsPerRank;\n    int binsEnd = binsBegin + binsPerRank;\n    // TODO: allocate bins for this rank\n    std::array<size_t, 10> localBins;\n    for (size_t i = binsBegin; i < binsEnd; ++i) {\n        localBins[i] = 0;\n    }\n    // TODO: compute the bins assigned to this rank\n    for (size_t i = elementsBegin; i < elementsEnd; ++i) {\n        // TODO: determine which bin each value should be counted in\n        int bin = x[i]/10;\n        // TODO: increment the bin value by one\n        ++localBins[bin];\n    }\n    // TODO: reduce the number of elements and bins assigned to each rank\n    MPI_Allreduce(localBins.data(), bins.data(), localBins.size(), MPI_UNSIGNED_LONG_LONG,\n                  MPI_SUM, MPI_COMM_WORLD);\n}",
            "size_t const n = x.size();\n    bins.fill(0);\n#pragma omp parallel for\n    for (size_t i=0; i<n; ++i) {\n        if (x[i]>=10) {\n            ++bins[9];\n        } else {\n            ++bins[x[i]/10];\n        }\n    }\n}",
            "// TODO: implement\n\n}",
            "size_t const N = x.size();\n  bins.fill(0);\n#pragma omp parallel\n  {\n    std::array<size_t, 10> bins_local;\n    bins_local.fill(0);\n#pragma omp for\n    for (size_t i = 0; i < N; ++i) {\n      if (x[i] < 10) {\n        bins_local[static_cast<size_t>(x[i])] += 1;\n      }\n    }\n#pragma omp critical\n    {\n      for (size_t i = 0; i < 10; ++i) {\n        bins[i] += bins_local[i];\n      }\n    }\n  }\n}",
            "// TODO\n\n}",
            "const size_t size = x.size();\n    const int rank = 0;\n    const int num_procs = 2;\n\n    double min_val, max_val;\n    MPI_Allreduce( &x[0], &min_val, size, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce( &x[0], &max_val, size, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n    const double delta = max_val - min_val;\n    const double width = delta / 10;\n\n    std::array<size_t, 10> local_bins;\n    // std::fill(local_bins.begin(), local_bins.end(), 0);\n    local_bins.fill(0);\n\n    #pragma omp parallel num_threads(num_procs)\n    {\n        const int thread_id = omp_get_thread_num();\n        const int num_threads = omp_get_num_threads();\n        std::array<double, 2> range{0.0, 10.0};\n        range[0] = (thread_id + 1) * width;\n        range[1] = range[0] + width;\n\n        const size_t begin = thread_id * size / num_threads;\n        const size_t end = (thread_id + 1) * size / num_threads;\n\n        #pragma omp for schedule(static)\n        for (size_t i = begin; i < end; ++i)\n        {\n            if ((x[i] >= range[0]) && (x[i] < range[1]))\n                ++local_bins[int((x[i] - range[0]) / width)];\n        }\n    }\n    // MPI_Reduce(&local_bins, &bins, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t xLength = x.size();\n  size_t chunk = xLength / size;\n  size_t remainder = xLength % size;\n\n  // Every rank has a complete copy of x\n  std::vector<double> local_x(x.begin() + rank * chunk, x.begin() + (rank + 1) * chunk);\n  if (remainder > 0) {\n    local_x.insert(local_x.end(), x.end() - remainder + rank, x.end());\n  }\n\n  // Calculate the bins by 10 using OpenMP\n  #pragma omp parallel for schedule(static)\n  for (int i = 0; i < 10; i++) {\n    int start = i * 10;\n    int end = start + 10;\n    bins[i] = std::count_if(local_x.begin(), local_x.end(),\n      [start, end](double x) { return (x >= start) && (x < end); });\n  }\n\n  // MPI Reduce\n  int sendcounts[10];\n  for (int i = 0; i < 10; i++) {\n    sendcounts[i] = bins[i];\n  }\n  MPI_Reduce(sendcounts, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (x.size() % size!= 0) {\n    throw std::runtime_error(\"x.size() % size!= 0\");\n  }\n\n  std::vector<double> x_l(x.begin() + rank * x.size() / size, x.begin() + (rank + 1) * x.size() / size);\n\n  std::array<size_t, 10> bins_l{};\n\n  // TODO\n\n  bins = bins_l;\n}",
            "const size_t n = x.size();\n  std::vector<size_t> local_bins(bins.size());\n  std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel\n#pragma omp single\n#pragma omp taskloop\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] < 10) {\n      local_bins[0] += 1;\n    } else if (x[i] < 20) {\n      local_bins[1] += 1;\n    } else if (x[i] < 30) {\n      local_bins[2] += 1;\n    } else if (x[i] < 40) {\n      local_bins[3] += 1;\n    } else if (x[i] < 50) {\n      local_bins[4] += 1;\n    } else if (x[i] < 60) {\n      local_bins[5] += 1;\n    } else if (x[i] < 70) {\n      local_bins[6] += 1;\n    } else if (x[i] < 80) {\n      local_bins[7] += 1;\n    } else if (x[i] < 90) {\n      local_bins[8] += 1;\n    } else {\n      local_bins[9] += 1;\n    }\n  }\n\n#pragma omp single\n#pragma omp taskloop\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = local_bins[i];\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Gather(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG,\n             bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG,\n             0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code goes here\n  const int num_procs = MPI_COMM_SIZE;\n  const int rank = MPI_COMM_RANK;\n  const int n = x.size();\n  const int chunk = n/num_procs;\n  const int start = chunk * rank;\n  const int end = start + chunk;\n  std::vector<size_t> temp_bins(10, 0);\n\n  for (int i = start; i < end; ++i) {\n    const double xi = x[i];\n    const size_t idx = std::floor(xi / 10);\n    if (xi >= 0 && xi < 100) {\n      temp_bins[idx]++;\n    }\n  }\n\n  MPI_Reduce(temp_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n  size_t N = x.size();\n  size_t localCount[10];\n  size_t globalCount[10];\n\n  for(int i = 0; i < 10; i++){\n    localCount[i] = 0;\n  }\n  for(int i = 0; i < N; i++){\n    int bin = x[i] / 10;\n    localCount[bin]++;\n  }\n  for(int i = 0; i < 10; i++){\n    localCount[i] = localCount[i] / N;\n  }\n  MPI_Reduce(localCount, globalCount, 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank == 0){\n    bins = std::array<size_t, 10>{globalCount[0], globalCount[1], globalCount[2], globalCount[3], globalCount[4], globalCount[5], globalCount[6], globalCount[7], globalCount[8], globalCount[9]};\n  }\n}",
            "/* YOUR CODE HERE */\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  /*\n   * Each rank gets a portion of the data to compute.\n   * Each rank will process a portion of the data and store it in `local_bins`.\n   * The sum of `local_bins` from all the ranks will be the result.\n   */\n  std::array<size_t, 10> local_bins = {};\n  size_t data_size = x.size();\n  size_t chunk_size = data_size / world_size;\n  size_t start_index = chunk_size * world_rank;\n  size_t end_index = chunk_size * (world_rank + 1);\n  if (world_rank == world_size - 1)\n    end_index = data_size;\n  std::cout << \"Rank: \" << world_rank << \", Start: \" << start_index\n            << \", End: \" << end_index << std::endl;\n\n  for (size_t i = start_index; i < end_index; ++i) {\n    double value = x[i];\n    if (value >= 0 && value <= 10) {\n      size_t bin_id = static_cast<size_t>(value);\n      local_bins[bin_id]++;\n    }\n  }\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n  std::cout << \"Bins: \" << bins.data()[0] << \", \" << bins.data()[1] << \", \"\n            << bins.data()[2] << \", \" << bins.data()[3] << \", \"\n            << bins.data()[4] << \", \" << bins.data()[5] << \", \"\n            << bins.data()[6] << \", \" << bins.data()[7] << \", \"\n            << bins.data()[8] << \", \" << bins.data()[9] << std::endl;\n}",
            "size_t n = x.size();\n  // Your code here\n}",
            "size_t count_size = x.size();\n  size_t chunk_size = count_size / MPI_Comm_size(MPI_COMM_WORLD);\n\n  int rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n_threads = 0;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_threads);\n\n  std::vector<double> chunk(chunk_size);\n  if (rank == 0) {\n    auto it_end = x.begin() + count_size / MPI_Comm_size(MPI_COMM_WORLD);\n    auto it_beg = x.begin();\n    for (int i = 0; i < MPI_Comm_size(MPI_COMM_WORLD); i++) {\n      std::copy(it_beg, it_end, chunk.begin());\n      it_beg = it_end;\n      it_end = it_end + chunk_size;\n      MPI_Send(chunk.data(), chunk_size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  } else {\n    MPI_Status status;\n    MPI_Recv(chunk.data(), chunk_size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n  }\n\n  int n = count_size / n_threads;\n  int remainder = count_size % n_threads;\n\n  std::vector<double> thread_x(n + remainder);\n  std::vector<size_t> thread_bins(n + remainder);\n\n#pragma omp parallel num_threads(n_threads)\n  {\n    int id = omp_get_thread_num();\n    int i = 0;\n\n    if (id < remainder) {\n      i = id;\n    } else {\n      i = id - remainder;\n    }\n\n    size_t start = n * i;\n    size_t end = n * (i + 1) - 1;\n\n    if (id < remainder) {\n      end += 1;\n    }\n\n    std::copy(chunk.begin() + start, chunk.begin() + end + 1, thread_x.begin());\n\n    binsBy10Count(thread_x, thread_bins);\n\n    if (id < remainder) {\n      bins[thread_bins[i]] += 1;\n    } else {\n      bins[thread_bins[i]] += 1;\n    }\n  }\n\n  if (rank == 0) {\n    auto it_end = bins.begin() + 10;\n    auto it_beg = bins.begin();\n    for (int i = 1; i < MPI_Comm_size(MPI_COMM_WORLD); i++) {\n      MPI_Status status;\n      MPI_Recv(it_beg, 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n      it_beg = it_end;\n      it_end = it_end + 10;\n    }\n  } else {\n    MPI_Send(bins.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "/* TODO: your code here */\n}",
            "size_t const num_bins = bins.size();\n  size_t const num_elements = x.size();\n  size_t num_threads = omp_get_max_threads();\n\n  size_t chunk_size = (num_elements + num_threads - 1) / num_threads;\n\n  std::vector<size_t> chunk_counts(num_threads);\n  std::vector<size_t> counts_per_bin(num_bins);\n  std::vector<size_t> chunk_offsets(num_threads + 1);\n\n  // omp for loop\n# pragma omp parallel num_threads(num_threads)\n  {\n    size_t const thread_id = omp_get_thread_num();\n    size_t const thread_offset = thread_id * chunk_size;\n    size_t const this_chunk_size = std::min(chunk_size, num_elements - thread_offset);\n    size_t const this_chunk_end = thread_offset + this_chunk_size;\n\n    // omp for loop\n# pragma omp for schedule(dynamic)\n    for (size_t i = thread_offset; i < this_chunk_end; ++i) {\n      for (size_t j = 0; j < num_bins; ++j) {\n        if (x[i] >= 10.0 * j && x[i] < 10.0 * (j + 1)) {\n          ++counts_per_bin[j];\n          break;\n        }\n      }\n    }\n\n    chunk_counts[thread_id] = this_chunk_size;\n  }\n\n  MPI_Reduce(chunk_counts.data(), chunk_offsets.data() + 1, num_threads + 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  size_t const num_total_elements = chunk_offsets[num_threads];\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    chunk_offsets[0] = 0;\n    counts_per_bin = std::vector<size_t>(counts_per_bin.size(), 0);\n  }\n\n  MPI_Reduce_scatter(counts_per_bin.data(), bins.data(), num_bins, MPI_UNSIGNED_LONG, MPI_SUM, chunk_offsets.data(), MPI_COMM_WORLD);\n\n  if (MPI::COMM_WORLD.Get_rank() == 0) {\n    for (size_t i = 0; i < num_bins; ++i) {\n      bins[i] = static_cast<double>(bins[i]) / static_cast<double>(num_total_elements);\n    }\n  }\n}",
            "// TODO: Fill in this function\n}",
            "// Your code here\n}",
            "bins.fill(0);\n    size_t num_elements = x.size();\n    size_t num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int chunk_size = num_elements/num_ranks;\n\n    #pragma omp parallel default(none) shared(bins, x, chunk_size, rank)\n    {\n        int thread_id = omp_get_thread_num();\n        int max_thread_id = omp_get_max_threads();\n\n        for (size_t chunk_id = 0; chunk_id < num_elements/chunk_size; chunk_id++) {\n            size_t start = chunk_id * chunk_size + rank * chunk_size;\n            size_t end = start + chunk_size;\n\n            for (size_t i = start; i < end; i++) {\n                if (x[i] >= thread_id * 10.0 && x[i] < (thread_id + 1) * 10.0) {\n                    bins[thread_id]++;\n                }\n            }\n        }\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (size_t i = 0; i < bins.size(); i++) {\n            std::cout << bins[i] << \" \";\n        }\n        std::cout << std::endl;\n    }\n}",
            "// Your code here\n    size_t total_count = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int interval_size = (x.size() - 1) / size;\n    int remainder = (x.size() - 1) % size;\n    int start_index, end_index;\n    if(rank < remainder) {\n        start_index = rank * (interval_size + 1);\n        end_index = (rank + 1) * (interval_size + 1);\n    } else {\n        start_index = rank * interval_size + remainder;\n        end_index = (rank + 1) * interval_size + remainder;\n    }\n\n    std::vector<int> local_count(10);\n    for(int i = start_index; i < end_index; i++) {\n        local_count[(int)(x[i] / 10)]++;\n    }\n\n    MPI_Allreduce(local_count.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "const size_t num_bins = 10;\n    size_t i;\n    size_t n = x.size();\n    std::vector<int> binsByThread(num_bins);\n\n    /* parallel code here */\n\n    bins[0] = binsByThread[0];\n    for(i = 1; i < num_bins; ++i) {\n        bins[i] = binsByThread[i] + bins[i-1];\n    }\n}",
            "const int rank = 0;\n  const int size = 0;\n  double min, max;\n\n  MPI_Reduce(&x[0], &min, 1, MPI_DOUBLE, MPI_MIN, rank, MPI_COMM_WORLD);\n  MPI_Reduce(&x[0], &max, 1, MPI_DOUBLE, MPI_MAX, rank, MPI_COMM_WORLD);\n\n  int n = x.size();\n  if (rank == 0) {\n    std::cout << \"min = \" << min << std::endl;\n    std::cout << \"max = \" << max << std::endl;\n  }\n\n  int local_n = n / size;\n  int remainder = n % size;\n\n  int offset = 0;\n  int r = rank;\n  if (rank < remainder) {\n    r = rank;\n    offset = local_n + 1;\n  }\n  else {\n    r = rank - remainder;\n    offset = local_n;\n  }\n\n  std::vector<double> local_x(local_n);\n  std::vector<double> local_bins(10);\n\n  MPI_Scatter(&x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n\n  #pragma omp parallel num_threads(8)\n  {\n    #pragma omp for\n    for (int i = 0; i < local_n; ++i) {\n      int b = std::floor(local_x[i] / 10.0);\n      ++local_bins[b];\n    }\n  }\n\n  std::vector<double> local_sums(10);\n\n  MPI_Allreduce(&local_bins[0], &local_sums[0], 10, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n  std::vector<double> global_sums(10);\n\n  MPI_Gather(&local_sums[0], 10, MPI_DOUBLE, &global_sums[0], 10, MPI_DOUBLE, rank, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] = static_cast<size_t>(global_sums[i]);\n    }\n  }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    for (int j = 0; j < 10; j++) {\n      if (x[i] <= j * 10) {\n        bins[j]++;\n        break;\n      }\n    }\n  }\n}",
            "// TODO: implement\n  size_t n = x.size();\n  std::vector<double> y(x);\n  std::sort(y.begin(), y.end());\n  size_t i = 0, j = 0;\n  while (i < n) {\n    while (j < n && y[j] < i * 10)\n      ++j;\n    bins[i] = j - i;\n    i++;\n  }\n  MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "double count = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] < 10.0) {\n            count++;\n        }\n    }\n\n    MPI_Reduce(&count, &bins[0], 10, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (omp_get_thread_num() == 0) {\n        size_t sum = 0;\n        for (int i = 0; i < bins.size(); i++) {\n            sum += bins[i];\n        }\n    }\n}",
            "}",
            "std::vector<double> x_sub(x);\n    std::array<size_t, 10> bins_sub = { 0 };\n\n    MPI_Datatype MPI_DOUBLE;\n    MPI_Type_contiguous(10, MPI_DOUBLE, &MPI_DOUBLE);\n    MPI_Type_commit(&MPI_DOUBLE);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t chunkSize = x.size() / MPI_SIZE;\n    size_t start = chunkSize * rank;\n    size_t end = (rank == MPI_SIZE - 1)? x.size() : start + chunkSize;\n    std::vector<double> x_rank(x.begin() + start, x.begin() + end);\n\n    MPI_Scatter(x_rank.data(), x_rank.size(), MPI_DOUBLE, x_sub.data(), x_rank.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        #pragma omp for schedule(static)\n        for (size_t i = 0; i < x_sub.size(); i++) {\n            if (x_sub[i] >= 0 && x_sub[i] < 10) {\n                bins_sub[x_sub[i]]++;\n            }\n        }\n    }\n\n    MPI_Reduce(bins_sub.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Type_free(&MPI_DOUBLE);\n}",
            "// Compute the number of elements to be sent to each rank.\n    size_t const n = x.size();\n    size_t const n_per_rank = n / MPI_COMM_SIZE;\n    size_t const remainder = n % MPI_COMM_SIZE;\n\n    // Set the number of OpenMP threads equal to the number of ranks,\n    // if not already set.\n    // int const threads = omp_get_max_threads();\n    // if (threads!= MPI_COMM_SIZE) {\n    //     omp_set_num_threads(MPI_COMM_SIZE);\n    // }\n\n    // Use OpenMP to compute the sum of each rank's elements.\n    // std::vector<size_t> local_bins(10, 0);\n#pragma omp parallel for\n    for (int i = 0; i < n_per_rank; ++i) {\n        // int const rank = omp_get_thread_num();\n        int const rank = omp_get_thread_num();\n\n        // Find the index of the first value in this rank's range.\n        size_t const first = n_per_rank * rank;\n\n        // Find the index of the first value in the next rank's range.\n        size_t const last = first + n_per_rank + (rank < remainder? 1 : 0);\n\n        // Count the number of values in this rank's range.\n        for (size_t j = first; j < last; ++j) {\n            // Find the first multiple of 10 greater than or equal to the value.\n            // std::ceil is the equivalent of std::round in the C++11 standard.\n            int const bin = static_cast<int>(std::ceil(10.0 * x[j]));\n\n            // Ensure that the value falls within the range [0, 10].\n            if (0 <= bin && bin < 10) {\n                // Update the bin count.\n                ++bins[bin];\n            }\n        }\n    }\n\n    // Wait for all ranks to finish.\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // Compute the sum of each rank's elements.\n    // std::vector<size_t> local_bins(10, 0);\n    // MPI_Reduce(\n    //     bins.data(),\n    //     local_bins.data(),\n    //     10,\n    //     MPI_UNSIGNED,\n    //     MPI_SUM,\n    //     0,\n    //     MPI_COMM_WORLD);\n\n    // Store the result on rank 0.\n    // MPI_Gather(\n    //     local_bins.data(),\n    //     10,\n    //     MPI_UNSIGNED,\n    //     bins.data(),\n    //     10,\n    //     MPI_UNSIGNED,\n    //     0,\n    //     MPI_COMM_WORLD);\n}",
            "int rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  size_t n = x.size();\n  size_t block_size = n / num_procs;\n  size_t rem = n % num_procs;\n\n  // each rank gets a block to process\n  std::vector<double> rank_x;\n  rank_x.reserve(block_size + (rank < rem? 1 : 0));\n\n  // initialize all the bins\n  bins = std::array<size_t, 10>{};\n\n  if (rank == 0) {\n    rank_x.insert(rank_x.end(), x.begin(), x.end());\n  } else {\n    rank_x.insert(rank_x.end(), x.begin() + (rank - 1) * block_size, x.begin() + rank * block_size);\n    if (rank < rem) {\n      rank_x.insert(rank_x.end(), x.end() - rem + rank, x.end());\n    }\n  }\n\n  // each rank has a complete copy of x\n  std::vector<size_t> rank_bins = bins;\n\n  #pragma omp parallel for schedule(dynamic)\n  for (size_t i = 0; i < rank_x.size(); i++) {\n    int val = rank_x[i];\n    if (val < 10) {\n      rank_bins[val]++;\n    }\n  }\n\n  MPI_Reduce(rank_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    // TODO: Your code here\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int bin_size = x.size() / size;\n    int remain = x.size() % size;\n    int start_rank = remain / bin_size;\n\n    double *local_bins;\n    int *send_bin_size, *recv_bin_size, *send_bin_count, *recv_bin_count;\n    double *send_data, *recv_data;\n    int *send_count, *recv_count;\n    int *send_displ, *recv_displ;\n\n    local_bins = new double[10];\n    send_bin_size = new int[size];\n    recv_bin_size = new int[size];\n    send_bin_count = new int[size];\n    recv_bin_count = new int[size];\n    send_data = new double[bin_size];\n    recv_data = new double[bin_size];\n    send_count = new int[size];\n    recv_count = new int[size];\n    send_displ = new int[size];\n    recv_displ = new int[size];\n\n    for (int i = 0; i < bin_size; i++) {\n        send_data[i] = x[start_rank + i * size];\n        send_count[i] = 1;\n        send_displ[i] = start_rank + i * size;\n    }\n    send_bin_count[0] = bin_size;\n    send_bin_size[0] = bin_size;\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            send_bin_count[i] = 1;\n            send_bin_size[i] = bin_size;\n            send_data[i] = x[start_rank + i * bin_size];\n            send_count[i] = 1;\n            send_displ[i] = start_rank + i * bin_size;\n        }\n    }\n\n    MPI_Alltoall(send_bin_count, 1, MPI_INT, recv_bin_count, 1, MPI_INT, MPI_COMM_WORLD);\n    MPI_Alltoall(send_bin_size, 1, MPI_INT, recv_bin_size, 1, MPI_INT, MPI_COMM_WORLD);\n\n    MPI_Alltoallv(send_data, send_bin_size, send_displ, MPI_DOUBLE, recv_data, recv_bin_size, recv_displ, MPI_DOUBLE, MPI_COMM_WORLD);\n    MPI_Alltoallv(send_count, send_bin_size, send_displ, MPI_INT, recv_count, recv_bin_size, recv_displ, MPI_INT, MPI_COMM_WORLD);\n\n    for (int i = 0; i < bin_size; i++) {\n        if (recv_data[i] >= 0 && recv_data[i] < 10) {\n            local_bins[static_cast<int>(recv_data[i])] += recv_count[i];\n        }\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    delete[] local_bins;\n    delete[] send_bin_size;\n    delete[] recv_bin_size;\n    delete[] send_bin_count;\n    delete[] recv_bin_count;\n    delete[] send_data;\n    delete[] recv_data;\n    delete[] send_count;\n    delete[] recv_count;\n    delete[] send_displ;\n    delete[] recv_displ;\n}",
            "}",
            "// YOUR CODE HERE\n    // throw std::runtime_error(\"TODO\");\n    bins = std::array<size_t, 10> {};\n    std::fill(bins.begin(), bins.end(), 0);\n    int n = x.size();\n    int chunk = n / size;\n\n    #pragma omp parallel for\n    for (int i = 0; i < size; i++) {\n        std::vector<double> v = std::vector<double>(x.begin() + i * chunk, x.begin() + (i + 1) * chunk);\n        for (int j = 0; j < chunk; j++) {\n            if (v[j] >= 0 && v[j] < 10) bins[int(v[j])] += 1;\n            if (v[j] >= 10 && v[j] < 20) bins[10] += 1;\n            if (v[j] >= 20 && v[j] < 30) bins[11] += 1;\n            if (v[j] >= 30 && v[j] < 40) bins[12] += 1;\n            if (v[j] >= 40 && v[j] < 50) bins[13] += 1;\n            if (v[j] >= 50 && v[j] < 60) bins[14] += 1;\n            if (v[j] >= 60 && v[j] < 70) bins[15] += 1;\n            if (v[j] >= 70 && v[j] < 80) bins[16] += 1;\n            if (v[j] >= 80 && v[j] < 90) bins[17] += 1;\n            if (v[j] >= 90 && v[j] <= 100) bins[18] += 1;\n        }\n    }\n}",
            "std::vector<size_t> bins_per_rank;\n\tstd::vector<double> x_per_rank;\n\tsize_t rank = 0;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tint size = 0;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tif (rank == 0) {\n\t\tbins_per_rank.resize(size);\n\t\tx_per_rank.resize(x.size());\n\t}\n\tMPI_Scatter(x.data(), x.size(), MPI_DOUBLE, x_per_rank.data(), x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\tstd::function<void(double, double)> f = [](double x, double y) {\n\t\tif (x < 10) {\n\t\t\t++y;\n\t\t} else if (x < 20) {\n\t\t\t++y;\n\t\t} else if (x < 30) {\n\t\t\t++y;\n\t\t} else if (x < 40) {\n\t\t\t++y;\n\t\t} else if (x < 50) {\n\t\t\t++y;\n\t\t} else if (x < 60) {\n\t\t\t++y;\n\t\t} else if (x < 70) {\n\t\t\t++y;\n\t\t} else if (x < 80) {\n\t\t\t++y;\n\t\t} else if (x < 90) {\n\t\t\t++y;\n\t\t} else {\n\t\t\t++y;\n\t\t}\n\t\treturn y;\n\t};\n\tbins_per_rank[rank] = std::accumulate(x_per_rank.begin(), x_per_rank.end(), 0, f);\n\tMPI_Gather(bins_per_rank.data(), bins_per_rank.size(), MPI_UNSIGNED, bins.data(), bins_per_rank.size(), MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "// Compute how many elements to compute in each thread\n  size_t chunk = x.size() / omp_get_max_threads();\n\n  // Compute the bin counts in parallel\n  #pragma omp parallel for schedule(dynamic)\n  for (size_t i = 0; i < x.size(); i += chunk) {\n    int rank = omp_get_thread_num();\n    int threadCount = omp_get_num_threads();\n\n    // Do the computation for this thread\n    int bin = x[i] / 10;\n    int low = bin * 10;\n    int high = low + 10;\n\n    #pragma omp atomic\n    ++bins[bin];\n  }\n\n  // Now, gather the results\n  MPI_Gather(bins.data(), 10, MPI_UNSIGNED_LONG, bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  // Now, sum the counts on rank 0\n  if (MPI_PROC_NULL == MPI_COMM_WORLD.rank()) {\n    #pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < bins.size(); i++) {\n      int threadCount = omp_get_num_threads();\n\n      #pragma omp atomic\n      bins[i] += bins[i];\n    }\n  }\n}",
            "// TODO: Compute the counts and store them in bins.\n  // You are free to use MPI and OpenMP.\n\n  int num_bins = 10;\n  double range_size = 10;\n  double range_start = 0;\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  int local_size = x.size() / world_size;\n  int left_over = x.size() % world_size;\n\n  if(world_rank == 0){\n    std::vector<size_t> temp(num_bins);\n    std::vector<double> local_x = std::vector<double>(local_size);\n\n    for(int i = 1; i < world_size; ++i){\n      std::vector<size_t> temp2(num_bins);\n      MPI_Recv(temp2.data(), num_bins, MPI_INT, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int j = 0; j < num_bins; ++j){\n        temp[j] += temp2[j];\n      }\n    }\n\n    for(int i = 0; i < local_size; ++i){\n      local_x[i] = x[i];\n    }\n\n    if(left_over!= 0){\n      int rank = left_over;\n      std::vector<size_t> temp2(num_bins);\n      MPI_Recv(temp2.data(), num_bins, MPI_INT, rank, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for(int j = 0; j < num_bins; ++j){\n        temp[j] += temp2[j];\n      }\n    }\n\n    for(int i = 0; i < num_bins; ++i){\n      bins[i] = temp[i];\n    }\n\n  } else {\n    int rank = world_rank;\n\n    int local_bins[num_bins];\n    for(int i = 0; i < num_bins; ++i){\n      local_bins[i] = 0;\n    }\n\n    for(int i = rank * local_size; i < (rank + 1) * local_size; ++i){\n      if(local_x[i] >= range_start && local_x[i] < (range_start + range_size)){\n        local_bins[(local_x[i] - range_start) / range_size] += 1;\n      }\n    }\n\n    MPI_Send(local_bins, num_bins, MPI_INT, 0, 1, MPI_COMM_WORLD);\n  }\n}",
            "// Compute the number of elements in each bin\n  for (auto &bin : bins) {\n    bin = 0;\n  }\n\n  // Compute the sum in each bin\n  #pragma omp parallel for schedule(static, 1)\n  for (size_t i = 0; i < x.size(); ++i) {\n    auto const value = x[i];\n    bins[std::floor(value / 10)] += 1;\n  }\n}",
            "const size_t rank = 0, n = x.size();\n    size_t myBins[10] = {0};\n\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < n; ++i) {\n            ++myBins[static_cast<size_t>(x[i]/10)];\n        }\n    }\n\n    bins = myBins;\n    MPI_Reduce(myBins, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n}",
            "int rank;\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int const num_threads = 2;\n    omp_set_num_threads(num_threads);\n\n    // Partition data into local vectors\n    std::vector<double> local_x(x.size() / world_size);\n    std::vector<size_t> local_bins(10);\n\n    // Scatter the x values\n    MPI_Scatter(x.data(), local_x.size(), MPI_DOUBLE, local_x.data(), local_x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Use OpenMP to compute in parallel\n    #pragma omp parallel for\n    for (int i = 0; i < local_x.size(); ++i) {\n        int bin = local_x[i] / 10;\n        if (bin < 10) {\n            ++local_bins[bin];\n        }\n    }\n\n    // Reduce the results from every thread on rank 0\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n}",
            "bins.fill(0);\n    int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    int blocksize = x.size()/nproc;\n\n    std::vector<double> tmp;\n    if (rank == 0)\n        tmp = x;\n    MPI_Scatter(tmp.data(), blocksize, MPI_DOUBLE, x.data(), blocksize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    size_t low = rank * blocksize, high = (rank + 1) * blocksize;\n    omp_set_num_threads(1);\n#pragma omp parallel for\n    for (int i = low; i < high; i++) {\n        if (x[i] < 10) bins[0]++;\n        else if (x[i] < 20) bins[1]++;\n        else if (x[i] < 30) bins[2]++;\n        else if (x[i] < 40) bins[3]++;\n        else if (x[i] < 50) bins[4]++;\n        else if (x[i] < 60) bins[5]++;\n        else if (x[i] < 70) bins[6]++;\n        else if (x[i] < 80) bins[7]++;\n        else if (x[i] < 90) bins[8]++;\n        else if (x[i] <= 100) bins[9]++;\n    }\n\n    std::array<size_t, 10> localbins;\n    MPI_Reduce(bins.data(), localbins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0)\n        bins = localbins;\n}",
            "// TODO: your code here\n}",
            "// TODO: implement me!\n}",
            "const int rank = 0;\n  const int commSize = 1;\n  const int omp_thread_count = 8;\n  int world_size = 1;\n  int world_rank = 0;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  std::vector<double> x_local(x.size() / commSize);\n  int j = 0;\n  for (int i = world_rank; i < x.size(); i += commSize) {\n    x_local[j] = x[i];\n    j++;\n  }\n\n  std::vector<size_t> bins_local(10);\n  #pragma omp parallel for num_threads(omp_thread_count)\n  for (int i = 0; i < x_local.size(); i++) {\n    if (x_local[i] >= 0 && x_local[i] < 10) {\n      bins_local[static_cast<int>(x_local[i])]++;\n    }\n  }\n\n  MPI_Gather(&bins_local[0], bins_local.size(), MPI_INT, &bins[0], bins_local.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (int i = 1; i < world_size; i++) {\n      for (int j = 0; j < 10; j++) {\n        bins[j] += bins[j + 10];\n      }\n    }\n  }\n}",
            "if (x.size() % size!= 0)\n        throw std::invalid_argument(\"x has to be divisible by size\");\n\n    size_t n = x.size() / size; // number of elements in each process\n    std::vector<double> local_x(x.begin(), x.begin() + n);\n    std::array<size_t, 10> local_bins;\n    local_bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        if (local_x[i] < 10) local_bins[0]++;\n        else if (local_x[i] < 20) local_bins[1]++;\n        else if (local_x[i] < 30) local_bins[2]++;\n        else if (local_x[i] < 40) local_bins[3]++;\n        else if (local_x[i] < 50) local_bins[4]++;\n        else if (local_x[i] < 60) local_bins[5]++;\n        else if (local_x[i] < 70) local_bins[6]++;\n        else if (local_x[i] < 80) local_bins[7]++;\n        else if (local_x[i] < 90) local_bins[8]++;\n        else local_bins[9]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for schedule(static, 1)\n    for (size_t i = 0; i < x.size(); ++i) {\n        bins[static_cast<size_t>(std::floor(x[i]/10))]++;\n    }\n}",
            "int myrank, nproc;\n  MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n  MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n\n  // Determine the number of elements each rank will process\n  int my_size = x.size() / nproc;\n\n  // If this is not the last rank, then we must make sure we don't process past the end of x\n  if (myrank!= nproc-1) {\n    my_size = (x.size() - myrank * (x.size()/nproc));\n  }\n\n  // Create a vector of data to store the number of elements in each bin\n  std::array<size_t, 10> local_bins;\n  for (auto& bin : local_bins) {\n    bin = 0;\n  }\n\n  // Compute the number of elements in each bin\n  for (int i=0; i<my_size; ++i) {\n    if ((x[myrank*my_size+i] >= 0) && (x[myrank*my_size+i] < 10)) {\n      ++local_bins[static_cast<size_t>(x[myrank*my_size+i])];\n    }\n  }\n\n  // Reduce the local_bins across the ranks\n  MPI_Reduce(&local_bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // If this rank is zero, print out the number of elements in each bin\n  if (myrank == 0) {\n    for (int i=0; i<10; ++i) {\n      std::cout << bins[i] <<'';\n    }\n    std::cout << std::endl;\n  }\n\n  return;\n}",
            "int comm_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n    std::vector<double> x_rank(x.size() / comm_size);\n\n    MPI_Scatter(x.data(), x_rank.size(), MPI_DOUBLE, x_rank.data(), x_rank.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::array<size_t, 10> bins_rank{};\n\n    for(size_t i = 0; i < x_rank.size(); i++)\n    {\n        if(x_rank[i] < 10) {\n            bins_rank[x_rank[i]]++;\n        } else if (x_rank[i] < 20) {\n            bins_rank[10]++;\n        } else {\n            bins_rank[20]++;\n        }\n    }\n\n    MPI_Reduce(bins_rank.data(), bins.data(), bins.size(), MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // We have to initialize each entry in the array to 0, because we're going to\n  // add to them, and OpenMP isn't thread safe\n  bins = std::array<size_t, 10>{0};\n\n  // Compute the number of threads per rank\n  int num_threads_per_rank = omp_get_max_threads() / num_ranks;\n  if (rank == num_ranks - 1) {\n    num_threads_per_rank += omp_get_max_threads() % num_ranks;\n  }\n\n  #pragma omp parallel for num_threads(num_threads_per_rank)\n  for (size_t i = 0; i < x.size(); ++i) {\n    double val = x[i];\n    int bin = val / 10;\n    if (bin >= 0 && bin < bins.size()) {\n      bins[bin] += 1;\n    }\n  }\n}",
            "size_t n = x.size();\n    std::vector<double> local_bins(bins.size());\n\n    #pragma omp parallel\n    {\n        // partition\n        double const* x_loc = x.data() + omp_get_thread_num() * (n / omp_get_num_threads());\n        double* bins_loc = local_bins.data() + omp_get_thread_num();\n\n        // count\n        #pragma omp for\n        for(size_t i=0; i<n/omp_get_num_threads(); ++i) {\n            size_t bin = x_loc[i] / 10;\n            ++bins_loc[bin];\n        }\n    }\n\n    // reduction\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n    size_t myBins[10] = {};\n\n    int nthread = omp_get_max_threads();\n    int nthread_local = nthread / 2;\n    int nthread_remote = nthread - nthread_local;\n\n    #pragma omp parallel num_threads(nthread) shared(myBins, nthread_local, nthread_remote)\n    {\n        int rank, size;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n        #pragma omp single\n        {\n            std::vector<std::vector<double>> local_data(size, std::vector<double>());\n            int id = 0;\n            for(int i = 0; i < nthread_local; ++i) {\n                local_data[id].push_back(x[i]);\n            }\n            for(int i = nthread_local; i < nthread; ++i) {\n                local_data[id].push_back(x[i]);\n                ++id;\n            }\n            for(int i = 0; i < size; ++i) {\n                MPI_Send(local_data[i].data(), local_data[i].size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            }\n        }\n\n        #pragma omp barrier\n        #pragma omp single\n        {\n            for(int i = 0; i < size; ++i) {\n                MPI_Status status;\n                MPI_Recv(myBins, 10, MPI_LONG_LONG, i, 0, MPI_COMM_WORLD, &status);\n            }\n        }\n    }\n    bins = std::array<size_t, 10> {};\n    for(int i = 0; i < 10; ++i) {\n        bins[i] = myBins[i];\n    }\n}",
            "int n_procs = -1;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n\n  // Each rank gets a vector of the size of the local array.\n  std::vector<double> local_x(x.size()/n_procs);\n\n  // Assign the local array to each rank.\n  MPI_Scatter(x.data(), x.size()/n_procs, MPI_DOUBLE, local_x.data(), local_x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 10> local_bins{};\n\n  // Calculate the number of values in each bin in parallel\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for(int i = 0; i < 10; i++){\n      local_bins[i] = 0;\n    }\n    #pragma omp for schedule(dynamic, 10)\n    for(size_t i = 0; i < local_x.size(); i++){\n      if(local_x[i] < 10){\n        local_bins[0] += 1;\n      } else if(local_x[i] < 20) {\n        local_bins[1] += 1;\n      } else if(local_x[i] < 30) {\n        local_bins[2] += 1;\n      } else if(local_x[i] < 40) {\n        local_bins[3] += 1;\n      } else if(local_x[i] < 50) {\n        local_bins[4] += 1;\n      } else if(local_x[i] < 60) {\n        local_bins[5] += 1;\n      } else if(local_x[i] < 70) {\n        local_bins[6] += 1;\n      } else if(local_x[i] < 80) {\n        local_bins[7] += 1;\n      } else if(local_x[i] < 90) {\n        local_bins[8] += 1;\n      } else {\n        local_bins[9] += 1;\n      }\n    }\n  }\n\n  // Gather the results from all ranks into a single array on rank 0.\n  MPI_Gather(local_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, bins.data(), local_bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n}",
            "// Your code here\n\n}",
            "// TODO: implement this method\n}",
            "// 1. Set up bins array as zeroes\n  size_t n = x.size();\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins[i] = 0;\n  }\n\n  // 2. Iterate over data and count\n  // 2a. OpenMP is used to distribute work among threads\n  // 2b. For each value `x[i]`, determine which bin it falls into, i.e., determine `bin = x[i] / 10`.\n  // 2c. Increment the bin counter.\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    int bin = x[i] / 10;\n    bins[bin]++;\n  }\n\n  // 3. Reduce bins array\n  // 3a. Use MPI to communicate values of bins array between ranks.\n  // 3b. Use MPI to broadcast values of bins array from rank 0 to all other ranks.\n  // 3c. Each rank stores a copy of the reduced bins array. Rank 0 stores the final result.\n\n  // 3a. Initialize bins array\n  std::array<size_t, 10> bins_sum = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n  // 3b. Broadcast final result\n  MPI_Bcast(bins_sum.data(), bins_sum.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  // 3c. Each rank stores a copy of the reduced bins array. Rank 0 stores the final result.\n  // 3c. Use OpenMP to distribute work among threads.\n  #pragma omp parallel for\n  for (size_t i = 0; i < bins.size(); ++i) {\n    bins_sum[i] += bins[i];\n  }\n\n  // 3d. MPI reduces bins_sum. Result is stored on rank 0.\n  MPI_Reduce(bins_sum.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "double const bin_size = 10.0;\n    int const n = x.size();\n\n    /* Distribute values evenly */\n    std::vector<size_t> counts(n);\n    for (int i = 0; i < n; i++) {\n        counts[i] = std::floor(x[i] / bin_size);\n    }\n\n    /* Reduce the counts across all ranks */\n    std::vector<size_t> global_counts(10);\n    MPI_Reduce(counts.data(), global_counts.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    /* Sum counts on each thread */\n    std::vector<size_t> partial_counts(10);\n    #pragma omp parallel for\n    for (int i = 0; i < 10; i++) {\n        partial_counts[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n; i++) {\n        partial_counts[counts[i]]++;\n    }\n\n    /* Reduce counts in each thread */\n    std::vector<size_t> thread_counts(10);\n    #pragma omp parallel for\n    for (int i = 0; i < 10; i++) {\n        thread_counts[i] = partial_counts[i];\n    }\n    std::vector<size_t> global_thread_counts(10);\n    MPI_Reduce(thread_counts.data(), global_thread_counts.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = global_counts;\n        for (int i = 0; i < 10; i++) {\n            bins[i] -= global_thread_counts[i];\n        }\n    }\n}",
            "// FIXME: implement me\n}",
            "}",
            "// TODO: Compute the values in bins by parallel computing\n  // the bins by 10\n}",
            "/* TODO: your code here */\n  /* MPI_Comm_size: https://www.open-mpi.org/doc/current/man3/MPI_Comm_size.3.php */\n  int num_procs;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  /* MPI_Comm_rank: https://www.open-mpi.org/doc/current/man3/MPI_Comm_rank.3.php */\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  /* MPI_Scatter: https://www.open-mpi.org/doc/current/man3/MPI_Scatter.3.php */\n  /* x: the input data */\n  /* bins: the buffer to store the result */\n  MPI_Scatter(&x[0], x.size(), MPI_DOUBLE, &bins[0], bins.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  /* omp_get_num_procs: https://www.openmp.org/spec-html/5.0/openmpsu117.html#x169-1620002.11.1 */\n  int num_threads = omp_get_num_procs();\n  omp_set_num_threads(num_threads);\n  #pragma omp parallel for\n  for (int i = 0; i < num_procs; ++i) {\n    if (rank == i) {\n      for (size_t j = 0; j < 10; ++j) {\n        bins[j] = 0;\n      }\n      for (size_t j = 0; j < x.size(); ++j) {\n        if (x[j] < j * 10) {\n          ++bins[j / 10];\n        }\n      }\n    }\n    #pragma omp barrier\n  }\n  if (rank == 0) {\n    std::array<size_t, 10> bins_tmp;\n    MPI_Reduce(&bins[0], &bins_tmp[0], bins.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = bins_tmp;\n  }\n}",
            "// YOUR CODE HERE\n  size_t rank;\n  int nthreads;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nthreads);\n\n#pragma omp parallel for\n  for (int i = 0; i < nthreads; ++i) {\n    if (rank == i) {\n      int thread_id = omp_get_thread_num();\n      std::cout << \"thread \" << thread_id << \" started\\n\";\n\n      for (size_t i = 0; i < x.size(); ++i) {\n        int bin = x[i] / 10;\n        if (bin >= 0 && bin < 10)\n          ++bins[bin];\n      }\n      std::cout << \"thread \" << thread_id << \" finished\\n\";\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n  }\n\n  MPI_Reduce(&bins, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<int> localBins;\n  // Add 1 for rank 0\n  localBins.resize(10, 0);\n  for (auto const& value : x) {\n    if (value >= rank * 10 && value < (rank + 1) * 10) {\n      ++localBins[value / 10 - rank * 10];\n    }\n  }\n  // Allreduce\n  std::vector<int> globalBins(10 * size);\n  MPI_Allgather(localBins.data(), 10, MPI_INT, globalBins.data(), 10, MPI_INT, MPI_COMM_WORLD);\n  // Rank 0 stores result in `bins`\n  if (rank == 0) {\n    bins.fill(0);\n    for (auto i = 0; i < size; ++i) {\n      for (auto j = 0; j < 10; ++j) {\n        bins[j] += globalBins[i * 10 + j];\n      }\n    }\n  }\n}",
            "size_t const n = x.size();\n\n    size_t const NUM_THREADS = 8;\n\n    size_t const NUM_RANKS = omp_get_max_threads();\n\n    std::vector<size_t> myBins(NUM_THREADS, 0);\n    std::vector<size_t> globalBins(NUM_THREADS, 0);\n\n    for (size_t i = 0; i < n; ++i) {\n        // Each rank will assign each bin to a different thread\n        size_t const rank = i / n * NUM_RANKS;\n\n        // TODO: Your code here\n\n        // End of your code\n    }\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t num_threads = omp_get_max_threads();\n  std::array<size_t, 10> bins_local;\n  bins_local.fill(0);\n\n  /* This section is not required, but it is a nice way to check the correctness\n     of your implementation. */\n  // double start_time = MPI_Wtime();\n  if (rank == 0) {\n    for (size_t i = 0; i < num_threads; ++i) {\n      std::vector<double> x_copy(x);\n      std::vector<size_t> bins_copy(bins_local.begin(), bins_local.end());\n      omp_set_num_threads(1);\n      #pragma omp parallel for\n      for (size_t j = 0; j < x.size(); ++j) {\n        if (x[j] >= 0 && x[j] < 10) {\n          bins_copy[static_cast<size_t>(x[j])] += 1;\n        }\n      }\n      for (size_t j = 0; j < 10; ++j) {\n        bins[j] += bins_copy[j];\n      }\n    }\n  } else {\n    for (size_t j = 0; j < x.size(); ++j) {\n      if (x[j] >= 0 && x[j] < 10) {\n        bins_local[static_cast<size_t>(x[j])] += 1;\n      }\n    }\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  // double end_time = MPI_Wtime();\n  // std::cout << \"Total Time Taken: \" << end_time - start_time << \"\\n\";\n\n  MPI_Reduce(bins_local.data(), bins.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n  int n = x.size() / ranks;\n\n  std::vector<double> x_local(n);\n  std::vector<size_t> bins_local(10);\n\n  // Every process will have a copy of x\n  MPI_Scatter(x.data(), n, MPI_DOUBLE, x_local.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (x_local[i] < 10.0) {\n      bins_local[0] += 1;\n    }\n    if (x_local[i] >= 10.0 && x_local[i] < 20.0) {\n      bins_local[1] += 1;\n    }\n    if (x_local[i] >= 20.0 && x_local[i] < 30.0) {\n      bins_local[2] += 1;\n    }\n    if (x_local[i] >= 30.0 && x_local[i] < 40.0) {\n      bins_local[3] += 1;\n    }\n    if (x_local[i] >= 40.0 && x_local[i] < 50.0) {\n      bins_local[4] += 1;\n    }\n    if (x_local[i] >= 50.0 && x_local[i] < 60.0) {\n      bins_local[5] += 1;\n    }\n    if (x_local[i] >= 60.0 && x_local[i] < 70.0) {\n      bins_local[6] += 1;\n    }\n    if (x_local[i] >= 70.0 && x_local[i] < 80.0) {\n      bins_local[7] += 1;\n    }\n    if (x_local[i] >= 80.0 && x_local[i] < 90.0) {\n      bins_local[8] += 1;\n    }\n    if (x_local[i] >= 90.0 && x_local[i] < 100.0) {\n      bins_local[9] += 1;\n    }\n  }\n\n  // All ranks will have the correct bins\n  MPI_Gather(bins_local.data(), bins_local.size(), MPI_SIZE_T, bins.data(), bins_local.size(), MPI_SIZE_T, 0, MPI_COMM_WORLD);\n\n}",
            "bins = std::array<size_t, 10> {};\n\n  // TODO: Fill in code here\n  //\n  //\n  //\n  //\n}",
            "int rank, nranks;\n\n    // TODO: implement this function\n    //...\n    //...\n    //...\n    //...\n}",
            "size_t i = 0;\n\twhile (i < x.size()) {\n\t\tint my_rank;\n\t\tMPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n\t\t#pragma omp parallel for num_threads(2)\n\t\tfor (size_t j = 0; j < omp_get_num_threads(); j++) {\n\t\t\t// omp_get_thread_num() returns the thread number\n\t\t\tdouble value = x[i];\n\t\t\tint low = static_cast<int>(value / 10 * (1 + omp_get_thread_num()));\n\t\t\tint high = static_cast<int>(value / 10 * (1 + omp_get_thread_num() + 1));\n\t\t\tint count = 0;\n\n\t\t\tfor (size_t k = i; k < x.size() && x[k] <= high; k++) {\n\t\t\t\tif (x[k] >= low) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// MPI_Reduce_scatter() combines results across all processes into `bins`\n\t\t\t// MPI_Reduce_scatter() requires a buffer to be passed in so we can't pass bins\n\t\t\t// directly. The results are spread across bins in a round robin fashion\n\t\t\tMPI_Reduce_scatter(&count, bins.data(), 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\t\t}\n\n\t\t// Advance loop\n\t\ti = i + omp_get_max_threads();\n\t}\n}",
            "int rank = 0;\n    int size = 0;\n    int prev_rank = 0;\n    int next_rank = 0;\n\n    // TODO: initialize MPI and OpenMP here\n    // TODO: initialize rank and size\n    // TODO: initialize prev_rank and next_rank\n\n    size_t n = x.size();\n    if (n == 0) {\n        return;\n    }\n    // TODO: initialize bins by rank 0\n    // TODO: broadcast bins from rank 0 to all other ranks\n\n    for (int i = 0; i < n; i++) {\n        // TODO: determine bin and send the value to the appropriate rank\n        // TODO: receive bin from appropriate rank and increment it\n        // TODO: determine next rank and receive bin from next rank\n        // TODO: add one to the bin on next_rank\n    }\n    // TODO: gather all bins into rank 0\n\n    // TODO: clean up MPI and OpenMP here\n}",
            "size_t n = x.size();\n\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Compute the number of elements each rank should work with\n  int elements_per_rank = n / world_size;\n  int elements_remainder = n % world_size;\n\n  // Partition each rank's input vector\n  std::vector<double> x_local(elements_per_rank);\n  std::vector<double> x_local_remainder(elements_remainder);\n\n  if (world_rank == 0) {\n    std::copy(x.begin(), x.begin() + elements_per_rank * world_size, x_local.begin());\n    std::copy(x.begin() + elements_per_rank * world_size, x.end(), x_local_remainder.begin());\n  }\n\n  // Divide the partition by 10\n  std::vector<size_t> bins_local(10);\n  std::vector<size_t> bins_local_remainder(10);\n\n#pragma omp parallel sections num_threads(1)\n  {\n    // Threads in the same section are synchronized\n#pragma omp section\n    {\n      for (int i = 0; i < 10; ++i) {\n        bins_local[i] = std::count_if(x_local.begin(), x_local.end(), [i](double x) {return x >= i * 10 && x < i * 10 + 10;});\n      }\n    }\n\n#pragma omp section\n    {\n      for (int i = 0; i < 10; ++i) {\n        bins_local_remainder[i] = std::count_if(x_local_remainder.begin(), x_local_remainder.end(), [i](double x) {return x >= i * 10 && x < i * 10 + 10;});\n      }\n    }\n  }\n\n  // Add the local bins to the global bins\n  if (world_rank == 0) {\n    std::copy(bins_local.begin(), bins_local.end(), bins.begin());\n\n    for (int i = 1; i < world_size; ++i) {\n      MPI_Recv(bins.data(), 10, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n  } else {\n    MPI_Send(bins_local.data(), 10, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n\n  if (world_rank == 0) {\n    for (int i = 0; i < elements_remainder; ++i) {\n      if (x[elements_per_rank * world_size + i] >= 90) {\n        bins[9]++;\n      } else {\n        bins[x[elements_per_rank * world_size + i] / 10]++;\n      }\n    }\n  }\n\n  MPI_Reduce(bins_local_remainder.data(), bins_local.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Add the local remainder bins to the global bins\n  if (world_rank == 0) {\n    for (int i = 0; i < 10; ++i) {\n      bins[i] += bins_local[i];\n    }\n  }\n}",
            "// initialize bins to zero\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // compute the number of elements to process\n  size_t n = x.size();\n\n  // number of threads in each team\n  int num_threads = omp_get_num_threads();\n  // number of teams\n  int num_teams = omp_get_num_teams();\n  // thread number within the team\n  int thread_num = omp_get_thread_num();\n\n  // size of each team\n  int n_per_team = n/num_teams;\n  // local thread id\n  int local_id = thread_num%num_threads;\n  // global thread id\n  int global_id = local_id + n_per_team*thread_num;\n\n  // loop over local elements\n  for (size_t i=global_id; i<n; i+=num_threads*num_teams) {\n    // get the value\n    double val = x[i];\n\n    // increment the correct bin\n    if (val < 10) {\n      bins[0] += 1;\n    } else if (val < 20) {\n      bins[1] += 1;\n    } else if (val < 30) {\n      bins[2] += 1;\n    } else if (val < 40) {\n      bins[3] += 1;\n    } else if (val < 50) {\n      bins[4] += 1;\n    } else if (val < 60) {\n      bins[5] += 1;\n    } else if (val < 70) {\n      bins[6] += 1;\n    } else if (val < 80) {\n      bins[7] += 1;\n    } else if (val < 90) {\n      bins[8] += 1;\n    } else {\n      bins[9] += 1;\n    }\n  }\n}",
            "const int num_bins = bins.size();\n  const int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n  const int num_ranks = MPI_Comm_size(MPI_COMM_WORLD);\n  const int chunk_size = x.size() / num_ranks;\n\n  auto* start = x.data() + rank * chunk_size;\n  auto* end = x.data() + rank * chunk_size + chunk_size;\n  if (rank == num_ranks - 1)\n    end = x.data() + x.size();\n\n  /* You can use parallel for for loops with OpenMP. */\n  #pragma omp parallel for\n  for (int i = 0; i < num_bins; i++) {\n    size_t count = 0;\n    for (auto *ptr = start; ptr < end; ptr++) {\n      if (*ptr >= i * 10 && *ptr < (i + 1) * 10) {\n        count++;\n      }\n    }\n    /* Only the first rank should write to the result. */\n    if (rank == 0) {\n      bins[i] = count;\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "const int n = x.size();\n    const int rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    const int size = MPI_Comm_size(MPI_COMM_WORLD);\n\n    std::vector<size_t> localBins(10);\n#pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        const int index = x[i] / 10;\n        if (index >= 0 && index < 10) {\n            localBins[index]++;\n        }\n    }\n\n    const int nLocalBins = std::accumulate(localBins.begin(), localBins.end(), 0);\n\n    // TODO: exchange data for bins in parallel\n}",
            "std::array<size_t, 10> localBins;\n    std::fill(localBins.begin(), localBins.end(), 0);\n\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 10.0)\n            localBins[static_cast<int>(x[i])] += 1;\n    }\n    // now add localBins to bins\n}",
            "size_t n = x.size();\n    size_t nThreads = omp_get_max_threads();\n    size_t nBins = bins.size();\n    std::vector<size_t> tempBins(nBins);\n    double const oneHundred = 100.0;\n\n    /* Split the work to the threads */\n    size_t threadWork = n / nThreads;\n    size_t leftover = n % nThreads;\n    std::vector<size_t> threadBins(nBins);\n\n    /* Compute bins for each thread */\n    #pragma omp parallel num_threads(nThreads)\n    {\n        size_t myID = omp_get_thread_num();\n        size_t lowBound = threadWork * myID;\n        size_t highBound = lowBound + threadWork;\n        if (myID < leftover) {\n            highBound += 1;\n        }\n        std::vector<size_t> myBins(nBins);\n        #pragma omp for\n        for (size_t i = lowBound; i < highBound; ++i) {\n            size_t j = static_cast<size_t>(x[i] * nBins / oneHundred);\n            ++myBins[j];\n        }\n        #pragma omp critical\n        {\n            std::copy(myBins.begin(), myBins.end(), threadBins.begin());\n        }\n    }\n\n    /* Combine results from threads into bins */\n    std::vector<size_t> recvcounts(nThreads * nBins);\n    std::vector<size_t> displs(nThreads * nBins);\n    MPI_Gather(&threadBins[0], nBins, MPI_UNSIGNED_LONG,\n        recvcounts.data(), nBins, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (myRank == 0) {\n        displs[0] = 0;\n        for (size_t i = 1; i < nThreads * nBins; ++i) {\n            displs[i] = displs[i-1] + recvcounts[i-1];\n        }\n        std::vector<size_t> allBins(recvcounts[0]);\n        MPI_Gatherv(&threadBins[0], nBins, MPI_UNSIGNED_LONG,\n            allBins.data(), recvcounts.data(), displs.data(),\n            MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n        std::copy(allBins.begin(), allBins.end(), bins.begin());\n    }\n}",
            "int n = 10;\n    MPI_Comm comm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n    int rank, size;\n    MPI_Comm_rank(comm, &rank);\n    MPI_Comm_size(comm, &size);\n    int num_threads = 8;\n    omp_set_num_threads(num_threads);\n\n    int chunk = x.size() / size;\n    std::vector<double> part(x.begin() + rank*chunk, x.begin() + rank*chunk + chunk);\n    int left_over = x.size() - chunk * size;\n    if (rank == size-1)\n        part.resize(part.size() + left_over);\n\n    std::vector<size_t> local_bins(n);\n#pragma omp parallel for\n    for (size_t i=0; i<part.size(); i++) {\n        double part_i = part[i];\n        int bin = static_cast<int>(part_i / 10);\n        if (part_i < 10)\n            bin = 0;\n        else if (part_i > 90)\n            bin = 9;\n        local_bins[bin] += 1;\n    }\n    std::vector<size_t> local_bins_temp(n);\n    MPI_Allreduce(local_bins.data(), local_bins_temp.data(), n, MPI_UNSIGNED_LONG, MPI_SUM, comm);\n\n    if (rank == 0) {\n        for (size_t i=0; i<n; i++)\n            bins[i] = local_bins_temp[i];\n    }\n\n    MPI_Comm_free(&comm);\n}",
            "size_t n = x.size();\n\n  /* Implement me */\n\n  //#if 1\n  double *x_local;\n  double *bins_local;\n  int rank, size, namelen;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Status status;\n  MPI_Get_processor_name(omp_get_thread_num(), NULL, &namelen);\n\n  size_t n_local = n / size;\n  if(rank == 0)\n    bins_local = new double[size * 10];\n  if(rank == 0)\n    x_local = new double[size * n_local];\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local, n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  int i = 0;\n#pragma omp parallel for num_threads(size)\n  for (size_t j = 0; j < n_local; j++) {\n    if (x_local[j] >= i * 10 && x_local[j] < (i + 1) * 10) {\n      bins_local[i]++;\n    }\n  }\n  if(rank == 0) {\n    for (size_t j = 0; j < size; j++) {\n      MPI_Recv(bins_local + j * 10, 10, MPI_DOUBLE, j, 1, MPI_COMM_WORLD, &status);\n    }\n    for (size_t j = 0; j < 10; j++) {\n      bins[j] = 0;\n      for (size_t k = 0; k < size; k++) {\n        bins[j] += bins_local[k * 10 + j];\n      }\n    }\n  }\n  else {\n    MPI_Send(bins_local, 10, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);\n  }\n  if(rank == 0) {\n    delete[] bins_local;\n    delete[] x_local;\n  }\n}",
            "// TODO: implement me!\n}",
            "// TODO: Write your code here\n\n  // bins[0] = 1;\n  // bins[1] = 2;\n  //...\n  // bins[9] = 1;\n  // bins[10] = 0;\n\n  // std::sort(x.begin(), x.end());\n\n  // #pragma omp parallel for num_threads(4)\n  // for (size_t i = 0; i < x.size(); i++) {\n  //   for (size_t j = 0; j < 10; j++) {\n  //     if (x[i] < 10 * (j + 1)) {\n  //       #pragma omp atomic\n  //       bins[j] += 1;\n  //     }\n  //   }\n  // }\n\n  #pragma omp parallel for num_threads(4)\n  for (size_t i = 0; i < x.size(); i++) {\n    size_t idx = (size_t) x[i] / 10;\n    #pragma omp atomic\n    bins[idx] += 1;\n  }\n  // #pragma omp parallel for num_threads(4)\n  // for (size_t i = 0; i < x.size(); i++) {\n  //   size_t idx = (size_t) x[i] / 10;\n  //   bins[idx] += 1;\n  // }\n\n  // for (size_t i = 0; i < x.size(); i++) {\n  //   size_t idx = (size_t) x[i] / 10;\n  //   bins[idx] += 1;\n  // }\n\n  // #pragma omp parallel for num_threads(4)\n  // for (size_t i = 0; i < x.size(); i++) {\n  //   bins[((size_t) x[i] / 10)] += 1;\n  // }\n}",
            "//...\n}",
            "const size_t size = x.size();\n    const int rank = MPI_COMM_WORLD.rank();\n    const int num_ranks = MPI_COMM_WORLD.size();\n\n    size_t chunk_size = size / num_ranks;\n    if (rank < (size % num_ranks)) {\n        chunk_size++;\n    }\n\n    std::vector<double> local_x(chunk_size);\n    std::vector<size_t> local_bins(10);\n\n    if (rank == 0) {\n        local_x.assign(x.begin(), x.end());\n    }\n\n    MPI_Scatter(&local_x[0], local_x.size(), MPI_DOUBLE, &local_x[0], local_x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    auto local_size = local_x.size();\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < local_size; i++) {\n        double n = local_x[i];\n        int bin = (int) floor((n / 10.0));\n        local_bins[bin]++;\n    }\n\n    MPI_Gather(&local_bins[0], local_bins.size(), MPI_INT, &bins[0], local_bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        auto sum = std::accumulate(bins.begin(), bins.end(), 0);\n        for (int i = 0; i < 10; i++) {\n            bins[i] = bins[i] / sum;\n        }\n    }\n}",
            "int num_threads = 8;\n  int rank, size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<double> local_x(x.size());\n  std::vector<int> local_bins(10);\n\n  // Partition the workload\n  size_t chunk = x.size() / size;\n\n  // Broadcast the chunk to each process\n  MPI_Bcast(&chunk, 1, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  // Create the subarray for each process\n  MPI_Scatter(&x[0], chunk, MPI_DOUBLE, &local_x[0], chunk, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Set the number of threads used for OpenMP\n  omp_set_num_threads(num_threads);\n\n  // Compute the number of values in [0,10), [10, 20),...\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n\n    if (local_x[i] >= 0.0 && local_x[i] < 10.0) {\n      local_bins[static_cast<int>(local_x[i])] += 1;\n    }\n  }\n\n  // Sum up the values in each bin\n  MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Compute the bins by a process with rank 0.\n    // Then, broadcast the result to all processes.\n    // Write your code here.\n}",
            "// TODO: implement me!\n}",
            "// TODO: implement this function\n}",
            "/* To be completed */\n}",
            "size_t size = x.size();\n\n    // number of tasks\n    int num_tasks = 10;\n\n    // each task will process `size / num_tasks` elements\n    int size_task = (size / num_tasks) + (size % num_tasks > 0);\n\n    // the element of x we are working on\n    int i = 0;\n\n    // the values in [0,10)\n    int count = 0;\n\n    #pragma omp parallel shared(x, bins, count)\n    {\n        // each task gets a thread\n        #pragma omp single\n        {\n            for (int task = 0; task < num_tasks; task++)\n            {\n                // task 0 will start on element 0\n                i = task * size_task;\n\n                // task 9 will process the remainder of the vector\n                if (task == (num_tasks-1))\n                {\n                    size_task = size - i;\n                }\n\n                // count values in [0,10) for this task\n                for (; i < i + size_task; i++)\n                {\n                    if ((x[i] >= 0) && (x[i] < 10))\n                    {\n                        count++;\n                    }\n                }\n\n                // store the count in the corresponding bin\n                bins[task] = count;\n            }\n        }\n    }\n}",
            "std::array<double, 10> ranks;\n  for (int i = 0; i < 10; ++i) {\n    ranks[i] = i;\n  }\n  std::vector<double> ranks_vec(ranks.begin(), ranks.end());\n\n  std::array<size_t, 10> local_bins;\n  for (int i = 0; i < 10; ++i) {\n    local_bins[i] = 0;\n  }\n\n  /* compute */\n\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    int bin = 0;\n    double val = x[i];\n    for (int j = 0; j < 10; ++j) {\n      if (ranks_vec[j] >= val) {\n        bin = j;\n        break;\n      }\n    }\n    ++local_bins[bin];\n  }\n  int total_bins;\n  MPI_Reduce(local_bins.data(), &total_bins, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (total_bins > 0) {\n    MPI_Gather(local_bins.data(), 10, MPI_INT, bins.data(), 10, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n}",
            "// your code goes here\n}",
            "// TODO: fill in this function\n    // hint: use omp for and mpi reduce\n\n}",
            "size_t const size = x.size();\n   if (omp_get_thread_num() == 0) {\n      bins = std::array<size_t, 10>{};\n   }\n\n   #pragma omp for\n   for (size_t i = 0; i < size; ++i) {\n      if (x[i] < 10) {\n         ++bins[0];\n      }\n      else if (x[i] < 20) {\n         ++bins[1];\n      }\n      else if (x[i] < 30) {\n         ++bins[2];\n      }\n      else if (x[i] < 40) {\n         ++bins[3];\n      }\n      else if (x[i] < 50) {\n         ++bins[4];\n      }\n      else if (x[i] < 60) {\n         ++bins[5];\n      }\n      else if (x[i] < 70) {\n         ++bins[6];\n      }\n      else if (x[i] < 80) {\n         ++bins[7];\n      }\n      else if (x[i] < 90) {\n         ++bins[8];\n      }\n      else {\n         ++bins[9];\n      }\n   }\n\n   if (omp_get_thread_num() == 0) {\n      MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n}",
            "int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int proc_size = x.size() / world_size;\n\n   std::vector<double> local_x(x.begin() + proc_size * world_rank, x.begin() + proc_size * (world_rank + 1));\n\n   std::array<size_t, 10> local_bins{};\n#pragma omp parallel for\n   for (size_t i = 0; i < local_x.size(); i++) {\n      if (local_x[i] <= 10) {\n         ++local_bins[0];\n      } else if (local_x[i] <= 20) {\n         ++local_bins[1];\n      } else if (local_x[i] <= 30) {\n         ++local_bins[2];\n      } else if (local_x[i] <= 40) {\n         ++local_bins[3];\n      } else if (local_x[i] <= 50) {\n         ++local_bins[4];\n      } else if (local_x[i] <= 60) {\n         ++local_bins[5];\n      } else if (local_x[i] <= 70) {\n         ++local_bins[6];\n      } else if (local_x[i] <= 80) {\n         ++local_bins[7];\n      } else if (local_x[i] <= 90) {\n         ++local_bins[8];\n      } else if (local_x[i] <= 100) {\n         ++local_bins[9];\n      }\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double chunkSize = 100 / size;\n  double begin = chunkSize * rank;\n  double end = chunkSize * (rank + 1);\n\n  std::vector<double> localCount(10, 0);\n\n  #pragma omp parallel num_threads(omp_get_max_threads())\n  {\n    std::vector<double> threadCount(10, 0);\n\n    #pragma omp for schedule(dynamic, 1)\n    for (auto i = 0u; i < x.size(); i++) {\n      auto value = x[i];\n      if (value >= begin && value < end) {\n        int index = static_cast<int>(value / chunkSize);\n        threadCount[index]++;\n      }\n    }\n\n    #pragma omp critical\n    for (auto i = 0; i < 10; i++) {\n      localCount[i] += threadCount[i];\n    }\n  }\n\n  MPI_Reduce(localCount.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::vector<int> bins_count(10, 0);\n\n  // TODO: implement this\n  // Hint: use the partition scheme described in the lecture:\n  //   https://github.com/UCSB-CS56-F17/lectureNotes_10.09/blob/master/lec12_MPI/lec12_MPI.pdf\n  // In this scheme, the \"low\" rank does the calculations for values in [0, 10)\n  //   and sends its partial results to the \"high\" rank. The high rank does the\n  //   same for values in [10, 20) and sends the results to the low rank, etc.\n  //   The \"high\" rank always sends results to rank 0, which stores the results.\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    bins[static_cast<size_t>(x[i]/10)]++;\n  }\n}",
            "int rank, size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tsize_t numValues = x.size();\n\tint numBins = 10;\n\n\t// create a vector to keep a copy of the input data on each rank\n\tstd::vector<double> x_rank(numValues);\n\tMPI_Scatter(x.data(), numValues, MPI_DOUBLE, x_rank.data(), numValues, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n\t// Compute the number of values in each bin in parallel on each rank\n\t// Each rank computes a sub-set of the values\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp for\n\t\tfor (int bin = 0; bin < numBins; bin++) {\n\t\t\tsize_t count = 0;\n\t\t\t// Loop over each value in x_rank\n\t\t\tfor (size_t i = 0; i < numValues; i++) {\n\t\t\t\t// Only values in the current bin are counted\n\t\t\t\tif (x_rank[i] >= bin * 10 && x_rank[i] < (bin + 1) * 10) {\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Store the result of the computation for each bin on rank 0\n\t\t\tif (rank == 0) {\n\t\t\t\tbins[bin] = count;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Broadcast the result to all ranks\n\tMPI_Bcast(bins.data(), numBins, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "const size_t n = x.size();\n    const size_t chunk = n / omp_get_max_threads(); // Each thread gets a chunk of 20\n    const size_t remainder = n % omp_get_max_threads(); // We have to account for the remainder\n\n    bins = std::array<size_t, 10>();\n    std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel\n    {\n        const int rank = omp_get_thread_num(); // rank = 0, 1, 2,...\n        const int nthreads = omp_get_num_threads(); // nthreads = 1, 2, 3,...\n        const size_t chunksize = chunk + (rank < remainder); // rank < remainder: 1, rank >= remainder: 20\n        const size_t start = rank * chunksize; // start = 0, 20, 40,...\n        const size_t end = (rank + 1) * chunksize; // end = 20, 40, 60,...\n        if (rank < remainder) end += 1; // end = 21, 41, 61,...\n\n        #pragma omp for\n        for (size_t i = start; i < end; i++) {\n            size_t bin = static_cast<size_t>(x[i] / 10);\n            if (x[i] >= 10) bin = 9;\n            bins[bin]++;\n        }\n    }\n\n    std::array<size_t, 10> bins_allreduce;\n    MPI_Reduce(bins.data(), bins_allreduce.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) bins = bins_allreduce;\n}",
            "int n = x.size();\n  int rank, n_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  const int local_n = n / n_ranks;\n  const int remainder = n - local_n * n_ranks;\n  const size_t n_bins = bins.size();\n  const int start = rank * local_n + std::min(rank, remainder);\n  const int end = start + local_n + (rank < remainder? 1 : 0);\n\n  std::vector<double> local_x(local_n);\n  if (rank < remainder) {\n    std::copy(x.begin() + start, x.begin() + end, local_x.begin());\n  } else {\n    std::copy(x.begin() + start, x.end(), local_x.begin());\n  }\n  size_t* local_bins = bins.data();\n  int i = 0;\n#pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    size_t* local_bins_tid = local_bins + tid * n_bins;\n    const int size = local_x.size();\n    int i_start = size * tid / omp_get_num_threads();\n    int i_end = size * (tid + 1) / omp_get_num_threads();\n    for (int i = i_start; i < i_end; ++i) {\n      local_bins_tid[local_x[i] / 10] += 1;\n    }\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Reduce(bins.data(), bins.data(), n_bins, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const size_t N = x.size();\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; i++) {\n\t\t// get local data and do computations\n\t\tauto value = x[i];\n\t\tauto bin = (int)(value / 10);\n\t\tbins[bin]++;\n\t}\n\n\t// sum up the values in bins from all ranks\n\tMPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t// each rank has a complete copy of x, so no need to exchange data\n\tif (omp_get_thread_num() == 0) {\n\t\t// each rank puts the values in its own bin in the same spot\n\t\tfor (size_t i = 0; i < N; i++) {\n\t\t\tauto value = x[i];\n\t\t\tauto bin = (int)(value / 10);\n\t\t\tbins[bin]--;\n\t\t}\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "//...\n\n  // write your solution here...\n\n  MPI_Finalize();\n}",
            "// TODO: implement here\n}",
            "if (x.size() < 10) {\n\t\tthrow std::invalid_argument(\"x must have at least 10 elements\");\n\t}\n\n\tstd::array<size_t, 10> local_bins;\n\tlocal_bins.fill(0);\n\tconst double upper_bound = 10;\n\tconst int num_threads = 4;\n\tconst int num_ranks = omp_get_max_threads();\n\tconst int chunk_size = x.size() / num_ranks;\n\tconst int remainder = x.size() % num_ranks;\n\tdouble temp_range_start = 0;\n\tdouble temp_range_end = 0;\n\tint count = 0;\n\n\t#pragma omp parallel num_threads(num_threads)\n\t{\n\t\t// Each rank has its own copy of x\n\t\tauto local_x = x;\n\t\tconst int rank = omp_get_thread_num();\n\t\tconst int num_threads = omp_get_num_threads();\n\t\tconst int chunk_size = local_x.size() / num_ranks;\n\t\tconst int remainder = local_x.size() % num_ranks;\n\n\t\tif (rank == 0) {\n\t\t\t// Each thread adds the same number of values\n\t\t\tfor (int i = 0; i < num_threads; ++i) {\n\t\t\t\ttemp_range_start = (i * chunk_size) + ((i < remainder)? i : remainder);\n\t\t\t\ttemp_range_end = ((i + 1) * chunk_size) + ((i + 1 < remainder)? i + 1 : remainder);\n\n\t\t\t\t#pragma omp task\n\t\t\t\tfor (int j = temp_range_start; j < temp_range_end; ++j) {\n\t\t\t\t\tif (local_x[j] <= upper_bound) {\n\t\t\t\t\t\t++local_bins[(int)(local_x[j])];\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse {\n\t\t\t#pragma omp task\n\t\t\tfor (int i = rank * chunk_size + ((rank < remainder)? rank : remainder); i < (rank + 1) * chunk_size + ((rank + 1 < remainder)? rank + 1 : remainder); ++i) {\n\t\t\t\tif (local_x[i] <= upper_bound) {\n\t\t\t\t\t++local_bins[(int)(local_x[i])];\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t#pragma omp taskwait\n\t}\n\n\t#pragma omp parallel for reduction(+:count)\n\tfor (int i = 0; i < 10; ++i) {\n\t\tcount += local_bins[i];\n\t\tbins[i] = count;\n\t}\n}",
            "size_t n = x.size();\n  size_t start = 0;\n  int rank, n_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  size_t size_per_rank = n / n_ranks;\n  if (rank == n_ranks - 1) {\n    size_per_rank += n % n_ranks;\n  }\n  bins.fill(0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < size_per_rank; ++i) {\n    size_t rank_index = start + i;\n    if (rank_index < n && x[rank_index] < 10) {\n      bins[size_t(x[rank_index])] += 1;\n    }\n  }\n  int sendcounts[n_ranks];\n  int recvcounts[n_ranks];\n  int displs[n_ranks];\n  MPI_Gather(&size_per_rank, 1, MPI_INT, recvcounts, 1, MPI_INT, 0, MPI_COMM_WORLD);\n  displs[0] = 0;\n  for (size_t i = 1; i < n_ranks; ++i) {\n    displs[i] = displs[i-1] + recvcounts[i-1];\n  }\n  MPI_Gatherv(&bins, size_per_rank, MPI_INT, &bins, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int num_threads = omp_get_max_threads();\n  int num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  std::vector<double> partial_bin_counts(num_threads);\n  std::vector<size_t> bin_counts(num_threads);\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_num = omp_get_thread_num();\n\n    size_t partial_count = 0;\n    for(size_t i = 0; i < x.size(); i++) {\n      double val = x[i];\n      int bin = (int) val / 10;\n      if (val < 10.0) {\n        partial_count++;\n      } else if (val < 20.0) {\n        partial_count += bin_counts[thread_num];\n      } else if (val < 30.0) {\n        partial_count += bin_counts[thread_num] + bin_counts[thread_num+1];\n      } else {\n        partial_count += bin_counts[thread_num] + bin_counts[thread_num+1] + bin_counts[thread_num+2];\n      }\n    }\n\n    partial_bin_counts[thread_num] = partial_count;\n\n    #pragma omp barrier\n\n    #pragma omp single\n    {\n      for(size_t i = 0; i < num_threads; i++) {\n        bin_counts[i] = partial_bin_counts[i];\n      }\n    }\n  }\n\n  std::vector<size_t> all_bin_counts(10, 0);\n  MPI_Reduce(bin_counts.data(), all_bin_counts.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (num_ranks == 1) {\n    bins = all_bin_counts;\n  } else {\n    MPI_Gather(all_bin_counts.data(), 10, MPI_UNSIGNED_LONG_LONG, bins.data(), 10, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Compute the total number of values\n   size_t num_values = x.size();\n   // Compute the bin size\n   size_t bin_size = num_values / bins.size();\n   // Find the total number of bins\n   size_t num_bins = 0;\n   for (size_t i = 0; i < bins.size(); i++) {\n      num_bins += bins[i] * bin_size;\n   }\n   // Find the remainder\n   size_t remainder = num_values - (num_bins * bins.size());\n\n   size_t start = 0;\n   size_t end = num_bins;\n   #pragma omp parallel for schedule(dynamic)\n   for (size_t i = 0; i < bins.size(); i++) {\n      // This is the amount of values in bin i\n      size_t bin_vals = bin_size;\n      // Check if this bin has a remainder\n      if (i < remainder) {\n         // There is a remainder so add 1 to the amount of values\n         bin_vals++;\n      }\n      // Compute the value of the start of bin i\n      size_t val_start = start;\n      // Compute the value of the end of bin i\n      size_t val_end = val_start + bin_vals;\n      // Count the values in this bin\n      bins[i] = std::count_if(x.begin() + val_start, x.begin() + val_end, [val_start, val_end](double value) {\n            return (value >= val_start) && (value < val_end);\n         });\n      // Increment the starting point of bin i\n      start = end;\n      // Increment the end of bin i\n      end = end + bins[i];\n   }\n}",
            "// TODO: compute counts of values in [0,10), [10, 20),...\n    std::fill(bins.begin(), bins.end(), 0);\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: compute the number of elements in each bin\n\n  // TODO: compute the sum of the number of elements in each bin on each rank\n\n  // TODO: sum the values of the bins (on rank 0)\n\n  // TODO: broadcast the result to all ranks\n}",
            "// TODO: implement this function\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (rank == 0) {\n\t\tbins.fill(0);\n\t}\n\tsize_t local_count = 0;\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < x.size(); i++) {\n\t\t\tlocal_count += ((x[i] >= 0) && (x[i] < 10));\n\t\t}\n\t}\n\tsize_t local_bins[10] = { 0 };\n\n\tMPI_Gather(&local_count, 1, MPI_INT, local_bins, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n\tMPI_Reduce(&local_bins, &bins[0], 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\tint nthreads = omp_get_max_threads();\n\tstd::vector<size_t> local_bins_by_thread(nthreads);\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tint thread_id = omp_get_thread_num();\n\t\tif (x[i] >= 0 && x[i] < 10) {\n\t\t\tlocal_bins_by_thread[thread_id] += 1;\n\t\t}\n\t}\n\tif (rank == 0) {\n\t\tfor (int i = 1; i < nthreads; i++) {\n\t\t\tbins[i] += bins[i - 1] + local_bins_by_thread[i];\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < 10; i++) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        bins[std::floor(x[i] / 10.0)]++;\n    }\n}",
            "const size_t n = x.size();\n\n#pragma omp parallel default(none)\n  {\n    std::array<size_t, 10> local_bins;\n    std::fill(local_bins.begin(), local_bins.end(), 0);\n\n#pragma omp for\n    for (int i = 0; i < n; ++i) {\n      size_t bin = static_cast<size_t>(x[i] / 10);\n      local_bins[bin] += 1;\n    }\n\n    // Merge all local_bins with bins\n#pragma omp critical\n    {\n      for (int i = 0; i < 10; ++i) {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n}",
            "int nprocs, rank, n, chunk, offset;\n    double const chunk_size = 10.0;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    n = x.size();\n    chunk = (n + nprocs - 1) / nprocs;\n    offset = rank * chunk;\n    std::vector<double> x_local(chunk, 0.0);\n    std::vector<size_t> local_bins(10, 0);\n#pragma omp parallel num_threads(8)\n    {\n        int tid = omp_get_thread_num();\n        size_t begin = tid * (chunk / 8);\n        size_t end = std::min((tid + 1) * (chunk / 8), chunk);\n        std::vector<double> local_x(end - begin, 0.0);\n        std::array<size_t, 10> local_bins(10, 0);\n        for (int i = begin; i < end; ++i) {\n            local_x[i - begin] = x[i + offset];\n        }\n        for (size_t i = 0; i < x.size(); ++i) {\n            double val = local_x[i];\n            if (val < 10.0) {\n                local_bins[val] += 1;\n            }\n        }\n        for (int i = 0; i < 10; ++i) {\n            local_bins[i] += local_bins[i - 1];\n        }\n        for (int i = 0; i < 10; ++i) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    bins[static_cast<size_t>(x[i] / 10.0)] += 1;\n  }\n}",
            "size_t num_threads = omp_get_max_threads();\n    size_t num_elements = x.size();\n    size_t num_bins = 10;\n\n    std::vector<size_t> histogram(num_bins);\n\n#pragma omp parallel for\n    for (size_t i = 0; i < num_elements; ++i) {\n        int bucket = static_cast<int>((x[i] / 10.0) * static_cast<double>(num_bins));\n        if (bucket == num_bins) bucket = num_bins - 1;\n        ++histogram[bucket];\n    }\n\n    // Reduce histograms across ranks\n    MPI_Reduce(histogram.data(), bins.data(), num_bins, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t const num_bins = 10;\n   // TODO\n}",
            "/* YOUR CODE HERE */\n}",
            "// TODO\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t n = x.size();\n  std::vector<size_t> local_bins(10);\n\n  size_t start = n * rank / size;\n  size_t end = n * (rank + 1) / size;\n\n  for (size_t i = start; i < end; i++) {\n    int bin = x[i] / 10;\n    local_bins[bin]++;\n  }\n\n  std::vector<size_t> global_bins(10);\n  MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n\n    for (size_t i = 0; i < 10; i++) {\n      for (int j = 0; j < size; j++) {\n        size_t start = n * j / size;\n        size_t end = n * (j + 1) / size;\n        bins[i] += global_bins[i * size + j] * (end - start) / n;\n      }\n    }\n  }\n}",
            "if (x.empty()) {\n    return;\n  }\n  int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  auto local_begin = x.begin() + world_rank * x.size() / world_size;\n  auto local_end = x.begin() + (world_rank + 1) * x.size() / world_size;\n\n  std::vector<size_t> local_bins(10);\n  for (auto v = local_begin; v!= local_end; ++v) {\n    if (*v < 10.0) {\n      local_bins[static_cast<size_t>(*v)] += 1;\n    }\n  }\n\n  std::array<size_t, 10> local_result;\n  MPI_Reduce(local_bins.data(), local_result.data(), local_bins.size(),\n             MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (world_rank == 0) {\n    for (auto i = 0; i < local_result.size(); ++i) {\n      bins[i] = local_result[i];\n    }\n  }\n}",
            "std::array<size_t, 10> localBins;\n\n  // your code here\n  // TODO: you may want to try implementing this function yourself!\n  // If you do, you might find it helpful to refer to the MPI example code in lab 02.\n  // You will probably need to modify the loop over `x` so that it is parallel.\n  // This might require restructuring the for loop and the computation\n  // of `localBins`.\n}",
            "size_t n = x.size();\n    int rank;\n    int nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Each processor has a chunk of x.\n    size_t chunk = n/nproc;\n\n    // Each processor starts at a different index.\n    size_t start = rank * chunk;\n    size_t end = (rank + 1) * chunk - 1;\n\n    if (rank == nproc - 1) {\n        end = n - 1;\n    }\n\n    std::vector<size_t> local_bins(10, 0);\n\n#pragma omp parallel for\n    for (size_t i = start; i < end; i++) {\n        // compute which bin the current value is in\n        int bin = static_cast<int>(x[i]/10);\n        if (bin == 10) {\n            bin = 0;\n        }\n\n        // increment the bin count\n        local_bins[bin]++;\n    }\n\n    // gather results from all processors\n    MPI_Gather(local_bins.data(), local_bins.size(), MPI_INT,\n               bins.data(), local_bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "if (x.size() % 2!= 0) {\n      std::cerr << \"Error: Array size must be even.\" << std::endl;\n      return;\n   }\n\n   // number of elements in each subarray\n   int subSize = x.size() / 2;\n\n   std::vector<double> subVec1(x.begin(), x.begin() + subSize);\n   std::vector<double> subVec2(x.begin() + subSize, x.end());\n\n   std::vector<size_t> localBins(10, 0);\n\n#pragma omp parallel for\n   for (int i = 0; i < subSize; i++) {\n      if (subVec1[i] >= 0 && subVec1[i] < 10) {\n         localBins[static_cast<int>(subVec1[i])]++;\n      }\n   }\n\n#pragma omp parallel for\n   for (int i = 0; i < subSize; i++) {\n      if (subVec2[i] >= 10 && subVec2[i] < 20) {\n         localBins[static_cast<int>(subVec2[i]) - 10]++;\n      }\n   }\n\n   // sum all local bins into global bins\n   MPI_Reduce(localBins.data(), bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n   bins.fill(0);\n\n   // TODO: Fill in the code to compute the number of values in [0,10), [10, 20),\n   // [20, 30),... and store the counts in `bins`.\n\n   // TODO: Replace the following dummy code with your actual code.\n   // NOTE: Each rank has a complete copy of x.  Therefore, you must use a single\n   //       MPI rank to perform the calculation and then broadcast the result\n   //       to the other ranks.\n\n   // Start timing\n   auto t1 = std::chrono::steady_clock::now();\n\n   // NOTE: You must add code to compute the number of values in [0,10), [10, 20),\n   // [20, 30),... and store the counts in `bins`.\n\n   // End timing\n   auto t2 = std::chrono::steady_clock::now();\n\n   // Display elapsed time\n   std::chrono::duration<double> elapsed_seconds = t2 - t1;\n   std::cout << \"Elapsed time: \" << elapsed_seconds.count() << \"s\\n\";\n}",
            "// TODO: implement this function\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins.size());\n  std::vector<size_t> localBins(bins.size());\n  int size = x.size();\n  #pragma omp parallel for\n  for (int i = 0; i < size; i++) {\n    double value = x[i];\n    int bin = (value / 10) + 1;\n    if (bin > 0) {\n      bin--;\n    }\n    localBins[bin]++;\n  }\n  // MPI_Reduce(localBins, bins, bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(localBins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: Your code here\n\n}",
            "// TODO\n}",
            "const int MPI_COMM_WORLD = 0;\n  const int N = x.size();\n\n  // Your code here.\n}",
            "size_t const numRanks = 4;\n  double const binSize = 10;\n  size_t const numBins = 10;\n  size_t const numValues = x.size();\n\n  // TODO(you): Fill in this function\n}",
            "size_t n = x.size();\n  // TODO: Your code goes here.\n}",
            "// TODO: implement me\n}",
            "int nprocs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  const size_t length = x.size() / nprocs;\n  const size_t rem = x.size() % nprocs;\n\n  std::vector<double> local_x(length);\n  std::vector<double> local_bins(10, 0);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < length; ++i) {\n      local_x[i] = x[i];\n    }\n    for (size_t i = length * (rank + 1) - rem; i < length * (rank + 1); ++i) {\n      local_x[i - (length * (rank + 1) - rem)] = x[i];\n    }\n  }\n\n  MPI_Scatter(local_x.data(), length, MPI_DOUBLE, local_x.data(), length, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  auto time1 = MPI_Wtime();\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < length; ++i) {\n    for (size_t j = 0; j < local_bins.size(); ++j) {\n      if (local_x[i] >= (j * 10) && local_x[i] < ((j + 1) * 10)) {\n        local_bins[j]++;\n      }\n    }\n  }\n\n  auto time2 = MPI_Wtime();\n\n  std::vector<double> global_bins(10);\n\n  MPI_Gather(local_bins.data(), 10, MPI_DOUBLE, global_bins.data(), 10, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    std::cout << \"Time: \" << time2 - time1 << std::endl;\n    for (size_t i = 0; i < global_bins.size(); ++i) {\n      std::cout << global_bins[i] << \" \";\n    }\n    std::cout << std::endl;\n  }\n\n  if (rank == 0) {\n    for (size_t i = 0; i < 10; ++i) {\n      bins[i] = (size_t)global_bins[i];\n    }\n  }\n}",
            "/* Your code goes here */\n}",
            "// TODO: Replace 0 with your own MPI rank\n    int rank = 0;\n    // TODO: Replace 0 with the number of OpenMP threads to use\n    int nthreads = 0;\n    // TODO: Replace 0 with the number of MPI processes\n    int nprocs = 0;\n    int n = x.size();\n    // TODO: Compute the starting and ending indices in x assigned to this rank\n    int start = 0;\n    int end = 0;\n\n    #pragma omp parallel num_threads(nthreads)\n    {\n        int tid = omp_get_thread_num();\n        #pragma omp for\n        for (int i = start; i < end; ++i) {\n            // TODO: Compute the index of the bin for x[i]\n            int bin = 0;\n            bins[bin] += 1;\n        }\n    }\n\n    // TODO: Combine the bin counts for all ranks into the final result on rank 0\n}",
            "size_t n = x.size();\n  if (n == 0) {\n    return;\n  }\n  bins.fill(0);\n  size_t nThreads = 4;\n  omp_set_num_threads(nThreads);\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    double const x_i = x[i];\n    if (x_i < 0.0 || x_i >= 100.0) {\n      continue;\n    }\n    size_t const bin = static_cast<size_t>(x_i / 10.0);\n    #pragma omp critical\n    {\n      bins[bin] += 1;\n    }\n  }\n}",
            "}",
            "// TODO: implement\n}",
            "// Write your code here.\n}",
            "int rank, numranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &numranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    bins = std::array<size_t, 10>();\n  }\n\n  int chunksize = x.size() / numranks;\n  double const* data = x.data() + rank * chunksize;\n\n  std::array<size_t, 10> local_bins = std::array<size_t, 10>();\n\n  #pragma omp parallel for reduction(+:local_bins[0:10])\n  for (int i = 0; i < chunksize; ++i) {\n    // TODO: this does not give the same result as the solution\n    if (data[i] >= 0 && data[i] < 10) {\n      local_bins[data[i]] += 1;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel\n  {\n    size_t thread_bins[10] = {0};\n\n    // #pragma omp for nowait // uncomment to remove the need for synchronization\n    // this loop is not a critical section because each iteration does not\n    // affect the results of other iterations. The only exception to this\n    // is the last iteration, which does not affect the results of other\n    // threads, because each thread iterates over a different chunk of\n    // the vector.\n    // #pragma omp for ordered // uncomment to add a synchronization barrier\n    #pragma omp for schedule(static)\n    for (int i = 0; i < x.size(); i++) {\n      // loop body starts here\n      int bin = (int)x[i]/10;\n      thread_bins[bin]++;\n      // loop body ends here\n    }\n\n    // now add thread_bins to bins\n    #pragma omp critical\n    for (int i = 0; i < 10; i++) {\n      bins[i] += thread_bins[i];\n    }\n  } // pragma omp parallel\n}",
            "size_t const size = x.size();\n  bins.fill(0);\n  size_t const step = size / 10;\n\n  #pragma omp parallel for\n  for (int i = 0; i < 10; i++) {\n    size_t const beg = i * step;\n    size_t const end = beg + step;\n    for (int j = beg; j < end; j++) {\n      if (x[j] < 10)\n        bins[x[j]]++;\n    }\n  }\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    size_t offset = 0;\n    #pragma omp parallel for reduction(+:offset)\n    for (int i = 1; i < 10; i++)\n      offset += bins[i - 1];\n    for (int i = 0; i < 10; i++)\n      bins[i] += offset;\n  }\n}",
            "std::array<size_t, 10> bins_local;\n  size_t sum_local;\n#pragma omp parallel for num_threads(omp_get_max_threads())\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 10.0) {\n      bins_local[static_cast<int>(x[i])]++;\n    }\n  }\n#pragma omp parallel for num_threads(omp_get_max_threads()) reduction(+:sum_local)\n  for (size_t i = 0; i < 10; i++) {\n    sum_local += bins_local[i];\n  }\n\n  MPI_Reduce(bins_local.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (sum_local > 0) {\n    MPI_Reduce(&sum_local, &bins[10], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, n_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n  int start = 10*rank / n_ranks;\n  int end = 10*(rank + 1) / n_ranks;\n  int local_size = end - start;\n\n  std::vector<double> local(x.begin() + start, x.begin() + end);\n  std::array<size_t, 10> local_bins = std::array<size_t, 10>();\n\n  #pragma omp parallel num_threads(n_ranks)\n  {\n    #pragma omp for\n    for (size_t i = 0; i < local_size; i++) {\n      for (int j = 0; j < 10; j++) {\n        if ((local[i] / 10.0) == j) {\n          local_bins[j]++;\n        }\n      }\n    }\n  }\n\n  std::vector<std::array<size_t, 10>> all_bins(n_ranks);\n  MPI_Gather(&local_bins, 10, MPI_UNSIGNED_LONG, all_bins.data(), 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int i = 0; i < n_ranks; i++) {\n      for (int j = 0; j < 10; j++) {\n        bins[j] += all_bins[i][j];\n      }\n    }\n  }\n}",
            "// TODO\n  // Get the rank of this process and the total number of processes\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO\n  // Each process needs to partition the values in x and send the partitions to the appropriate process\n  // Use a vector of pairs containing the start and end indices for each process.\n  std::vector<std::pair<int, int>> localPartition;\n\n  // Get the size of the local partition\n  int localSize = x.size() / size;\n\n  // Get the start and end indices for this process\n  int start = rank * localSize;\n  int end = (rank + 1) * localSize;\n\n  // Only the last process may have a different size of the partition\n  if(rank == size - 1) end = x.size();\n\n  // Send the start and end indices to the appropriate process\n  MPI_Bcast(&start, 1, MPI_INT, rank, MPI_COMM_WORLD);\n  MPI_Bcast(&end, 1, MPI_INT, rank, MPI_COMM_WORLD);\n\n  // Create the local partition for this process\n  for(int i = start; i < end; i++) {\n    localPartition.push_back({i, i+1});\n  }\n\n  // TODO\n  // Each process then loops through the indices in localPartition and sends the values in x\n  // to the appropriate process. The result is a vector of pairs containing the indices and\n  // values for each process.\n  std::vector<std::pair<int, int>> partition(localPartition.size());\n  MPI_Scatter(&localPartition[0], localPartition.size(), MPI_INT, &partition[0], localPartition.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Create the partition\n  std::vector<double> localX(partition.size());\n  std::vector<double> remoteX(partition.size());\n  for(int i = 0; i < localX.size(); i++) {\n    localX[i] = x[partition[i].first];\n  }\n\n  // TODO\n  // Each process then loops through the indices in partition and receives the values from the\n  // appropriate process. The result is a vector of pairs containing the indices and values\n  // for each process.\n  MPI_Scatter(&localX[0], partition.size(), MPI_DOUBLE, &remoteX[0], partition.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // TODO\n  // Each process then computes the indices and values in the local partition.\n  std::vector<int> localIndices(localPartition.size());\n  std::vector<int> localValues(localPartition.size());\n  for(int i = 0; i < localIndices.size(); i++) {\n    localIndices[i] = partition[i].first;\n    localValues[i] = remoteX[i];\n  }\n\n  // Create the result vector\n  std::vector<int> result(localIndices.size());\n\n  // TODO\n  // Each process then loops through the indices in localIndices and computes the number of values\n  // in the local partition that fall within each bin.\n  for(int i = 0; i < localIndices.size(); i++) {\n    int index = localIndices[i];\n    double value = localValues[i];\n\n    if (value >= 0 && value < 10) {\n      result[i] = 0;\n    }\n    else if (value >= 10 && value < 20) {\n      result[i] = 1;\n    }\n    else if (value >= 20 && value < 30) {\n      result[i] = 2;\n    }\n    else if (value >= 30 && value < 40) {\n      result[i] = 3;\n    }\n    else if (value >= 40 && value < 50) {\n      result[i] = 4;\n    }\n    else if (value >= 50 && value < 60) {\n      result[i] = 5;\n    }\n    else if (value >= 60 && value < 70) {\n      result[i] = 6;\n    }\n    else if (value >= 70 && value < 80) {\n      result[i] = 7;\n    }\n    else if (value >= 80 && value < 90) {\n      result[i] = 8;\n    }\n    else if (value >= 90 && value <= 100) {\n      result[i] = 9;\n    }\n    else {\n      result[i",
            "// Get rank, size, and start and end indices\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  size_t start = (rank * x.size()) / size;\n  size_t end = ((rank + 1) * x.size()) / size;\n  std::cout << \"Rank \" << rank << \" with indices [\" << start << \",\" << end << \"]\" << std::endl;\n\n  // Compute local histogram for current rank\n  std::array<size_t, 10> local_bins{};\n  for (size_t i = start; i < end; i++) {\n    auto index = std::floor(x[i] / 10);\n    local_bins[index]++;\n  }\n\n  // Reduce to get final histogram\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int n = x.size();\n    double step = 10;\n\n    // divide the values evenly between processes\n    int n_per_process = n / size;\n    // the first process has the extra value (modulo division)\n    if (rank == 0) n_per_process += n % size;\n    // the last process has no values\n    if (rank == size - 1) n_per_process = 0;\n    std::vector<double> x_local(n_per_process);\n    MPI_Scatter(x.data(), n_per_process, MPI_DOUBLE, x_local.data(), n_per_process, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    // distribute n_per_process values to each thread in a round-robin fashion\n    omp_set_num_threads(n_per_process);\n#pragma omp parallel\n    {\n        int n_local = omp_get_num_threads();\n        int rank_local = omp_get_thread_num();\n        std::vector<int> bins_local(10);\n#pragma omp for schedule(static)\n        for (int i = 0; i < n_local; ++i) {\n            // determine which bin the i'th value falls into\n            bins_local[std::floor(x_local[i] / step)] += 1;\n        }\n        // sum bins from all threads into bins\n        for (int i = 1; i < 10; i++) {\n            bins_local[i] += bins_local[i - 1];\n        }\n        MPI_Reduce(bins_local.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// You may want to add more bins here.\n  std::array<size_t, 11> bins11 = {0};\n\n  size_t numBins = 0;\n  size_t numX = x.size();\n  std::vector<double> myX(numX);\n  for(int i = 0; i < numX; ++i){\n    myX[i] = (x[i] / 10);\n    if(myX[i] > numBins){\n      numBins = myX[i];\n    }\n  }\n\n  // MPI\n  double mySum = 0;\n  MPI_Reduce(myX.data(), bins11.data(), numX, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  // OpenMP\n  #pragma omp parallel for reduction(+:mySum)\n  for(int i = 0; i < numX; ++i){\n    mySum += (bins11[myX[i]] - 1);\n  }\n\n  // MPI\n  MPI_Reduce(&mySum, &bins11[numBins], 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  // OpenMP\n  #pragma omp parallel for reduction(+:bins11[numBins])\n  for(int i = 0; i < numX; ++i){\n    if(myX[i] == numBins){\n      bins11[numBins] += 1;\n    }\n  }\n\n  // MPI\n  std::array<size_t, 11> bins11Sum = {0};\n  MPI_Reduce(bins11.data(), bins11Sum.data(), numBins+1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // OpenMP\n  #pragma omp parallel for reduction(+:bins11Sum)\n  for(int i = 0; i < numBins; ++i){\n    bins11Sum[i] += bins11[i];\n  }\n\n  // MPI\n  std::array<size_t, 10> bins10Sum = {0};\n  MPI_Reduce(bins11Sum.data(), bins10Sum.data(), numBins+1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // OpenMP\n  #pragma omp parallel for reduction(+:bins10Sum)\n  for(int i = 0; i < numBins; ++i){\n    bins10Sum[i] += bins11Sum[i+1];\n  }\n\n  if(omp_get_thread_num() == 0){\n    bins = bins10Sum;\n  }\n}",
            "size_t num_threads = omp_get_max_threads();\n   std::vector<size_t> local_bins(10, 0);\n   std::vector<size_t> global_bins(10, 0);\n   for (size_t i = 0; i < num_threads; i++) {\n      for (size_t j = 10 * i; j < 10 * (i + 1); j++) {\n         if (x[j] < 10) {\n            local_bins[x[j]]++;\n         }\n      }\n   }\n   MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (global_bins[0] > 0) {\n      for (size_t i = 1; i < global_bins.size(); i++) {\n         global_bins[i] += global_bins[i - 1];\n      }\n   }\n   bins = global_bins;\n}",
            "// get number of elements\n  size_t n = x.size();\n\n  // initialize bins to 0\n  std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    // add one to corresponding bin\n    bins[static_cast<size_t>(x[i] / 10.0)]++;\n  }\n\n  // each rank sends result to rank 0\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "const int nRanks = 1;\n  const int nThreads = 2;\n\n  std::vector<double> local_x(nRanks * x.size());\n  std::copy(x.begin(), x.end(), local_x.begin());\n\n  // TODO: copy to device\n\n  // TODO: assign to bins array\n\n  // TODO: copy from device\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement binsBy10Count\n}",
            "size_t n = x.size();\n  // TODO\n  //...\n}",
            "// TODO: implement this function\n}",
            "int rank, num_ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\tint chunk_size = x.size() / num_ranks;\n\tint remainder = x.size() % num_ranks;\n\n\tint start_index = rank * chunk_size + (rank < remainder? rank : remainder);\n\tint end_index = rank * chunk_size + chunk_size + (rank < remainder? rank : remainder);\n\n\tstd::vector<double> partial_vector(x.begin() + start_index, x.begin() + end_index);\n\n\t// TODO: Compute number of values in [0,10), [10, 20),... and store in bins.\n\t// You may use MPI and OpenMP to compute in parallel.\n\n\tint chunk_size2 = partial_vector.size() / num_ranks;\n\tint remainder2 = partial_vector.size() % num_ranks;\n\n\tint start_index2 = rank * chunk_size2 + (rank < remainder2? rank : remainder2);\n\tint end_index2 = rank * chunk_size2 + chunk_size2 + (rank < remainder2? rank : remainder2);\n\n\t//std::cout << \"rank: \" << rank << \" start: \" << start_index << \" end: \" << end_index << std::endl;\n\t//std::cout << \"rank: \" << rank << \" start: \" << start_index2 << \" end: \" << end_index2 << std::endl;\n\n\tstd::vector<double> partial_vector2(partial_vector.begin() + start_index2, partial_vector.begin() + end_index2);\n\n#pragma omp parallel for\n\tfor (int i = 0; i < partial_vector2.size(); i++) {\n\t\tfor (int j = 0; j < 10; j++) {\n\t\t\tif (partial_vector2[i] >= j*10 && partial_vector2[i] < (j+1)*10) {\n\t\t\t\tbins[j] += 1;\n\t\t\t}\n\t\t}\n\t}\n\n\t//MPI_Barrier(MPI_COMM_WORLD);\n\n\tMPI_Reduce(bins.data(), bins.data(), 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  size_t localSize = x.size() / size;\n\n  if (rank == 0) {\n    for (int i = 0; i < size; i++) {\n      MPI_Send(x.data() + i * localSize, localSize, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  std::array<size_t, 10> localBins{};\n  std::fill(localBins.begin(), localBins.end(), 0);\n  std::vector<double> localX(localSize);\n  MPI_Status status;\n  MPI_Recv(localX.data(), localSize, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n\n  for (size_t i = 0; i < localSize; i++) {\n    localBins[static_cast<size_t>((localX[i] / 10))] += 1;\n  }\n\n  MPI_Reduce(localBins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: write code to count values in parallel\n}",
            "// TODO: Replace the following with your own code to compute the histogram.\n  bins.fill(0);\n  for (int i = 0; i < x.size(); i++) {\n    int bin = (x[i] / 10);\n    if (bin < 0) {\n      bins[0]++;\n    } else if (bin > 9) {\n      bins[9]++;\n    } else {\n      bins[bin]++;\n    }\n  }\n}",
            "// TODO: implement me\n}",
            "//...\n}",
            "// TODO\n    // you must use OpenMP and MPI to compute the counts.\n    // assume MPI has already been initialized.\n    // Every rank has a complete copy of x.\n    // The result is stored in bins on rank 0.\n    // You may want to use a reduction.\n    // You may also want to use a map or a reduce.\n\n    // TODO\n    // you should fill in the missing parts\n    //\n    // the code here is just an example.\n    // you should remove this line and replace it\n    // with your own implementation.\n    std::array<size_t, 10> localBins;\n    for (size_t i = 0; i < x.size(); i++) {\n        // find the bin\n        // i.e. figure out which index in localBins this value should be added\n        // and increment the bin at that index.\n    }\n\n    // compute the global bins\n    // TODO\n    // this is the part that should be implemented with MPI and OpenMP.\n    //\n    // you should use a reduction to do the reduction.\n    //\n    // hint:\n    // https://stackoverflow.com/questions/10242072/mpi-reduce-on-an-array-of-structs\n    //\n    // to make the code compile, you will need to add the proper headers.\n    // you might want to add more includes as well.\n    //\n    // you may find this helpful:\n    // https://stackoverflow.com/questions/2314397/writing-a-reduction-routine-for-mpi\n    MPI_Reduce(\n        localBins.data(),\n        bins.data(),\n        10,\n        MPI_UNSIGNED_LONG_LONG,\n        MPI_SUM,\n        0,\n        MPI_COMM_WORLD);\n}",
            "double const chunkSize = x.size() / static_cast<double>(omp_get_num_procs());\n    auto const myChunkBegin = static_cast<size_t>(std::floor(chunkSize * static_cast<double>(omp_get_thread_num())));\n    auto const myChunkEnd = static_cast<size_t>(std::floor(chunkSize * static_cast<double>(omp_get_thread_num() + 1)));\n    auto const myChunkSize = myChunkEnd - myChunkBegin;\n    auto const chunkBegin = x.begin() + myChunkBegin;\n    auto const chunkEnd = x.begin() + myChunkEnd;\n\n    #pragma omp parallel for\n    for (auto it = chunkBegin; it!= chunkEnd; ++it) {\n        auto const val = *it;\n        if (val >= 0.0 && val < 10.0) {\n            bins[static_cast<size_t>(val)]++;\n        }\n    }\n\n    // Combine the partial bins into bins on rank 0\n    if (omp_get_thread_num() == 0) {\n        for (size_t i = 1; i < omp_get_num_threads(); ++i) {\n            auto const offset = i * 10;\n            for (size_t j = 0; j < 10; ++j) {\n                bins[j + offset] += bins[j];\n            }\n        }\n    }\n\n    #pragma omp barrier\n    // Reduce bins by 10\n    if (omp_get_thread_num() == 0) {\n        for (size_t i = 0; i < 10; ++i) {\n            for (size_t j = 1; j < omp_get_num_threads(); ++j) {\n                bins[i] += bins[i + j * 10];\n            }\n        }\n    }\n}",
            "double sum;\n  int rank;\n  int comm_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int thread_count = omp_get_num_threads();\n  std::vector<double> thread_x(x.size()/thread_count);\n\n#pragma omp parallel for default(shared) reduction(+:sum)\n  for (int thread = 0; thread < thread_count; thread++) {\n    int start = thread * thread_x.size();\n    int end = start + thread_x.size();\n    for (int i = start; i < end; i++) {\n      thread_x[i] = x[i];\n    }\n  }\n  MPI_Scatter(&thread_x[0], thread_x.size(), MPI_DOUBLE, &x[0], thread_x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < x.size(); i++) {\n    int index = x[i] / 10;\n    bins[index]++;\n  }\n\n  MPI_Reduce(&bins[0], &bins[0], 10, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Add your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  omp_set_num_threads(size);\n  if (rank == 0) {\n    bins.fill(0);\n  }\n  std::vector<size_t> local_bins(10);\n  for (size_t i = 0; i < x.size(); i++) {\n    if ((x[i] >= 0) && (x[i] < 10)) {\n      local_bins[x[i]]++;\n    }\n  }\n  MPI_Gather(&local_bins[0], 10, MPI_UNSIGNED_LONG, &bins[0], 10, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n}",
            "// Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int N = x.size();\n\n  // Compute the number of elements to be distributed among ranks\n  double n_per_rank = static_cast<double>(N) / size;\n\n  // Split x into local_x and x_extra\n  std::vector<double> local_x(n_per_rank);\n  std::vector<double> x_extra(N - n_per_rank * size);\n\n  // Divide x into local_x and x_extra\n  std::copy(x.begin(), x.begin() + n_per_rank * size, local_x.begin());\n  std::copy(x.begin() + n_per_rank * size, x.end(), x_extra.begin());\n\n  // Split local_x and x_extra into chunks of 10 values\n  std::vector<double> local_x_10(n_per_rank * 10);\n  std::vector<double> x_extra_10(x_extra.size() / 10);\n\n  // Divide local_x into chunks of 10 values\n  for (size_t i = 0; i < n_per_rank; ++i) {\n    std::copy(local_x.begin() + i * 10, local_x.begin() + (i + 1) * 10, local_x_10.begin() + i * 100);\n  }\n\n  // Divide x_extra into chunks of 10 values\n  for (size_t i = 0; i < x_extra.size() / 10; ++i) {\n    std::copy(x_extra.begin() + i * 10, x_extra.begin() + (i + 1) * 10, x_extra_10.begin() + i * 100);\n  }\n\n  // Find number of extra values per rank, and determine whether extra values are last or not\n  std::vector<int> extra_per_rank(size, 0);\n  std::vector<int> extra_per_rank_copy(size, 0);\n  std::vector<int> extra_per_rank_sum(size, 0);\n  std::vector<int> extra_per_rank_last(size, 0);\n  std::vector<double> local_x_10_extra_last(n_per_rank, 0);\n  std::vector<double> x_extra_10_extra_last(x_extra.size() / 10, 0);\n  for (size_t i = 0; i < x_extra.size() / 10; ++i) {\n    extra_per_rank[i % size]++;\n    extra_per_rank_copy[i % size]++;\n  }\n  for (size_t i = 0; i < size; ++i) {\n    MPI_Reduce(&extra_per_rank[i], &extra_per_rank_sum[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n      extra_per_rank_last[i] = extra_per_rank_sum[i] - extra_per_rank_sum[0];\n    }\n    MPI_Bcast(&extra_per_rank_last[i], 1, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n\n  // Determine if extra values are last\n  for (size_t i = 0; i < n_per_rank; ++i) {\n    if (extra_per_rank_last[i % size] == 1) {\n      local_x_10_extra_last[i] = local_x_10[i * 10 + extra_per_rank_copy[i % size] - 1];\n    }\n  }\n\n  for (size_t i = 0; i < x_extra.size() / 10; ++i) {\n    if (extra_per_rank_last[i % size] == 1) {\n      x_extra_10_extra_last[i] = x_extra_10[i * 10 + extra_per_rank_copy[i % size] - 1];\n    }\n  }\n\n  // Combine local_x_10_extra_last and x_extra_10_extra_last into x_",
            "// determine number of threads and MPI ranks\n    int const numThreads = omp_get_max_threads();\n    int const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n    int const numRanks = MPI_Comm_size(MPI_COMM_WORLD);\n\n    // get the size of the input data\n    size_t const n = x.size();\n\n    // determine the chunk size\n    size_t const chunkSize = n / numRanks;\n\n    // create an array of the appropriate size to store results\n    std::array<size_t, 10> partialBins;\n\n    // loop over chunks of the input array, counting the number of elements in each chunk\n    // use OpenMP to parallelize the loop\n    size_t begin = rank * chunkSize;\n    size_t end = begin + chunkSize;\n\n#pragma omp parallel for num_threads(numThreads) schedule(static, chunkSize) reduction(+:partialBins[0:10])\n    for (size_t i = begin; i < end; ++i) {\n        double const value = x[i];\n        size_t const bin = static_cast<size_t>(std::round(value / 10.0));\n        partialBins[bin] += 1;\n    }\n\n    // compute the partial sums over partialBins\n    std::partial_sum(std::begin(partialBins), std::end(partialBins), std::begin(partialBins));\n\n    // gather the counts from all ranks to rank 0\n    MPI_Reduce(MPI_IN_PLACE, partialBins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // determine the start of each bin\n        std::vector<size_t> starts(bins.size());\n        starts[0] = 0;\n        std::partial_sum(std::begin(starts) + 1, std::end(starts), std::begin(starts));\n\n        // compute the result by subtracting the start of each bin from the end of the bin\n        std::transform(std::begin(bins), std::end(bins), std::begin(starts), std::begin(bins),\n                       std::minus<size_t>());\n    }\n}",
            "// compute the number of elements that each rank will process\n  size_t num_el = x.size() / omp_get_max_threads();\n\n  // calculate the starting element and ending element of each rank\n  int rank, num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  size_t start = rank * num_el;\n  size_t end = start + num_el;\n\n  // local counter for each rank\n  std::array<size_t, 10> local_bins;\n  for (size_t i = 0; i < 10; i++) {\n    local_bins[i] = 0;\n  }\n\n  // perform the computations of each thread\n  #pragma omp parallel for\n  for (size_t i = start; i < end; i++) {\n    for (size_t j = 0; j < 10; j++) {\n      if (x[i] >= j * 10 && x[i] < (j + 1) * 10) {\n        local_bins[j]++;\n      }\n    }\n  }\n\n  // sum up the local counters\n  MPI_Reduce(local_bins.data(), bins.data(), 10, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "int n = x.size();\n  int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  std::vector<size_t> local_bins(10);\n  std::fill(local_bins.begin(), local_bins.end(), 0);\n\n  int n_per_proc = (n + nprocs - 1) / nprocs;\n  int first = n_per_proc * rank;\n  int last = n_per_proc * (rank + 1);\n  if (rank == nprocs - 1) last = n;\n\n  // Each thread operates on a contiguous chunk of data\n  // First, compute local bins\n  #pragma omp parallel for\n  for (int i = first; i < last; ++i) {\n    int j = x[i];\n    if (j < 10) {\n      ++local_bins[j];\n    }\n  }\n\n  // Reduce bins to master\n  std::vector<size_t> global_bins(10, 0);\n  MPI_Reduce(local_bins.data(), global_bins.data(), 10, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Copy global bins to bins\n  if (rank == 0) {\n    std::copy(global_bins.begin(), global_bins.end(), bins.begin());\n  }\n}",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use Kokkos to compute in parallel. Assume that Kokkos has already been initialized.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(Kokkos::View<const double*> const& x, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "Kokkos::TeamPolicy<Kokkos::TeamSingle> policy(x.extent(0));\n    policy.set_scratch_size(0, Kokkos::PerTeam(1024*sizeof(size_t)));\n    Kokkos::parallel_for(\"Count Quartiles\", policy, [=] (const Kokkos::TeamPolicy<Kokkos::TeamSingle> &policy, const Kokkos::ThreadVectorRange<Kokkos::TeamSingle> &r) {\n      Kokkos::parallel_for(Kokkos::TeamThreadRange(policy, 0, bins.extent(0)), [&] (const int i) {\n        bins(i) = 0;\n      });\n\n      size_t n = x.extent(0);\n      for (size_t i=r.league_rank()*r.team_size(); i<n; i+=r.league_size()*r.team_size()) {\n        double y = x(i);\n        if (y < 0.25 || y >= 0.75) continue;\n        if (y < 0.5) bins(0)++;\n        else if (y < 0.75) bins(1)++;\n        else bins(2)++;\n        bins(3)++;\n      }\n    });\n}",
            "// TODO\n  //...\n\n}",
            "Kokkos::parallel_for(4, KOKKOS_LAMBDA(size_t i) {\n    double sum = 0.0;\n    for (size_t j = 0; j < x.extent(0); j++) {\n      if (x(j) >= i * 0.25 && x(j) < (i + 1) * 0.25) {\n        sum += 1;\n      }\n    }\n    bins(i) = sum;\n  });\n}",
            "}",
            "// TODO\n}",
            "}",
            "// TODO: replace `bins` with a Kokkos::View<size_t>\n  Kokkos::deep_copy(bins, Kokkos::View<size_t[4]>(\"bins\", {0, 0, 0, 0}));\n  // TODO: fill in the following function call\n  auto x_h = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_h, x);\n  size_t n = x.extent(0);\n  size_t n_q = n / 4;\n  for (size_t i = 0; i < n; i++) {\n    if (x_h(i) >= n_q && x_h(i) < n_q * 2) {\n      Kokkos::atomic_fetch_add(&bins(0), 1);\n    } else if (x_h(i) >= n_q * 2 && x_h(i) < n_q * 3) {\n      Kokkos::atomic_fetch_add(&bins(1), 1);\n    } else if (x_h(i) >= n_q * 3 && x_h(i) < n_q * 4) {\n      Kokkos::atomic_fetch_add(&bins(2), 1);\n    } else {\n      Kokkos::atomic_fetch_add(&bins(3), 1);\n    }\n  }\n}",
            "Kokkos::parallel_for(\"count-quartiles\", x.extent(0), KOKKOS_LAMBDA(int i) {\n      double x_i = x(i);\n\n      // check for which quartile\n      // 0.25 is a magic number\n      if (x_i < 0.25) {\n         bins(0) += 1;\n      } else if (x_i >= 0.25 && x_i < 0.50) {\n         bins(1) += 1;\n      } else if (x_i >= 0.50 && x_i < 0.75) {\n         bins(2) += 1;\n      } else if (x_i >= 0.75) {\n         bins(3) += 1;\n      }\n   });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, x.extent(0)),\n    KOKKOS_LAMBDA(size_t i) {\n      double xi = x(i);\n      if (xi >= 0 && xi <= 0.25) {\n        ++bins(0);\n      } else if (xi > 0.25 && xi <= 0.5) {\n        ++bins(1);\n      } else if (xi > 0.5 && xi <= 0.75) {\n        ++bins(2);\n      } else {\n        ++bins(3);\n      }\n    }\n  );\n}",
            "// TODO: implement\n}",
            "// TODO: fill in this function\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>>(0, x.extent(0)), KOKKOS_LAMBDA(size_t i) {\n    double x_i = x(i);\n    if (x_i < 0 || x_i > 10) {\n      // ignore out of bounds values\n      return;\n    }\n    size_t quartile_idx = size_t(4 * x_i);\n    bins(quartile_idx) += 1;\n  });\n  Kokkos::fence();\n  Kokkos::deep_copy(bins, bins);\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Your code goes here.\n}",
            "// Implement the algorithm here.\n\n}",
            "// YOUR CODE HERE\n  Kokkos::parallel_for(\"count_quartiles\", x.extent(0), KOKKOS_LAMBDA (const int i) {\n    if (x(i) > 0 && x(i) < 0.25) {\n      bins(0) += 1;\n    } else if (x(i) >= 0.25 && x(i) < 0.5) {\n      bins(1) += 1;\n    } else if (x(i) >= 0.5 && x(i) < 0.75) {\n      bins(2) += 1;\n    } else if (x(i) >= 0.75) {\n      bins(3) += 1;\n    }\n  });\n  Kokkos::fence();\n}",
            "// TODO: implement the function\n}",
            "Kokkos::View<size_t> counts(\"counts\", 4);\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)),\n            KOKKOS_LAMBDA(const int i) {\n                double val = x(i);\n                if (val < 0.25) {\n                    ++counts(0);\n                } else if (val < 0.5) {\n                    ++counts(1);\n                } else if (val < 0.75) {\n                    ++counts(2);\n                } else {\n                    ++counts(3);\n                }\n            });\n\n    Kokkos::deep_copy(counts, counts);\n    Kokkos::deep_copy(bins, counts);\n}",
            "// Insert your code here.\n}",
            "// TODO: implement this function\n   // You can modify this function to make it count the number of doubles\n   // in the input vector that have a fractional part in a given interval.\n   // For example, if the interval is [0.25, 0.5), count the number of doubles\n   // in [0.25, 0.5) that are in the input vector.  Then increment the corresponding\n   // bin in `bins` to count the total number of doubles in the given interval.\n   // You may assume that the input vector `x` contains at least one element, and\n   // all elements of the input vector are between 0 and 100.\n   //\n   // Your code should call the Kokkos parallel_reduce() function. You do not need\n   // to implement any sort of concurrency control.\n}",
            "// TODO: Implement this function using Kokkos\n}",
            "auto N = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, N),\n    [=](int i) {\n      double xi = x(i);\n      if (xi < 0) {\n        bins(0) += 1;\n      } else if (xi < 0.25) {\n        bins(1) += 1;\n      } else if (xi < 0.5) {\n        bins(2) += 1;\n      } else if (xi < 0.75) {\n        bins(3) += 1;\n      }\n    }\n  );\n}",
            "// TODO\n  // Your code goes here\n  // Hint: you should look up the Kokkos reductions\n}",
            "// TODO: implement\n}",
            "// TODO: implement\n\n}",
            "// Compute the number of doubles in each quartile in parallel\n\t// TODO: Your code goes here\n}",
            "auto x_host = Kokkos::create_mirror_view(x);\n    Kokkos::deep_copy(x_host, x);\n    // TODO: fill in code to find the number of x that are in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    // Hint: Use Kokkos::parallel_reduce\n    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n            Kokkos::Impl::FunctorValueReduce<Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>, double*, size_t[4]>(\n                [&x_host](size_t i, size_t& count){\n                    if(x_host(i) >= 0.75 && x_host(i) <= 1) {\n                        count[3]++;\n                    } else if(x_host(i) >= 0.5 && x_host(i) < 0.75) {\n                        count[2]++;\n                    } else if(x_host(i) >= 0.25 && x_host(i) < 0.5) {\n                        count[1]++;\n                    } else if(x_host(i) >= 0 && x_host(i) < 0.25) {\n                        count[0]++;\n                    } else {\n                        printf(\"ERROR\\n\");\n                    }\n                }, bins\n            ), Kokkos::Sum<size_t[4]>());\n}",
            "Kokkos::parallel_for(\"Quartile Count\", x.extent(0), KOKKOS_LAMBDA(size_t i) {\n      double v = x(i);\n      if (v >= 0 && v < 0.25) {\n        bins(0) += 1;\n      }\n      else if (v >= 0.25 && v < 0.5) {\n        bins(1) += 1;\n      }\n      else if (v >= 0.5 && v < 0.75) {\n        bins(2) += 1;\n      }\n      else {\n        bins(3) += 1;\n      }\n    });\n}",
            "// TODO\n\n}",
            "// TODO: fill in this function\n}",
            "// TODO: implement this function\n\n  // Hint: see class notes\n\n}",
            "// TODO: implement me!\n}",
            "double fracs[4] = { 0.25, 0.5, 0.75, 1 };\n\n  // TODO: implement the function\n}",
            "// TODO: Your code here\n}",
            "// your code goes here\n\n}",
            "// YOUR CODE HERE\n    throw std::runtime_error(\"not implemented\");\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: You fill in here\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0));\n  Kokkos::parallel_for(policy, [&](size_t i) {\n    double xval = x(i);\n    double q025 = 0.25 * xval;\n    double q050 = 0.5 * xval;\n    double q075 = 0.75 * xval;\n    int quartile = 0;\n    if (xval < q025) quartile = 0;\n    else if (xval < q050) quartile = 1;\n    else if (xval < q075) quartile = 2;\n    else quartile = 3;\n    bins(quartile) += 1;\n  });\n  Kokkos::deep_copy(bins, bins);\n}",
            "// TODO: replace these with actual code.\n  size_t x_size = x.extent(0);\n  size_t quartile_size = x_size / 4;\n  bins(0) = quartile_size;\n  bins(1) = quartile_size;\n  bins(2) = quartile_size;\n  bins(3) = quartile_size;\n\n  return;\n}",
            "// You don't need to modify anything in this function\n}",
            "auto policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0));\n  Kokkos::parallel_for(\"Quartiles\", policy, KOKKOS_LAMBDA (const int i) {\n    double value = x(i);\n    if (value < 0.25) {\n      bins(0) += 1;\n    } else if (value < 0.5) {\n      bins(1) += 1;\n    } else if (value < 0.75) {\n      bins(2) += 1;\n    } else {\n      bins(3) += 1;\n    }\n  });\n}",
            "// TODO: Implement this function\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// TODO: Fill in code here.\n}",
            "// TODO: Implement this function\n}",
            "// TODO: compute the solution\n\n}",
            "// TODO: implement me\n\n}",
            "// TODO: your code here\n  // See https://github.com/parthenon/kokkos_kernels/wiki/Tutorials-for-Kokkos-Kernels-1.x\n  // for examples of how to use the Kokkos Kernels library.\n}",
            "size_t n = x.extent(0);\n    size_t b1 = 0, b2 = 0, b3 = 0, b4 = 0;\n\n    // TODO: replace the following code with parallel code using Kokkos\n    for (size_t i=0; i<n; i++) {\n        double xi = x(i);\n        double a = xi - floor(xi);\n\n        if (a < 0.25) {\n            b1++;\n        } else if (a < 0.5) {\n            b2++;\n        } else if (a < 0.75) {\n            b3++;\n        } else {\n            b4++;\n        }\n    }\n\n    bins(0) = b1;\n    bins(1) = b2;\n    bins(2) = b3;\n    bins(3) = b4;\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<>(0, x.extent(0)), [=](const int i) {\n        auto y = x(i);\n        if (y >= 0 && y <= 0.25) {\n            ++bins(0);\n        } else if (y > 0.25 && y <= 0.5) {\n            ++bins(1);\n        } else if (y > 0.5 && y <= 0.75) {\n            ++bins(2);\n        } else if (y > 0.75 && y <= 1) {\n            ++bins(3);\n        }\n    });\n    Kokkos::fence();\n}",
            "// YOUR CODE HERE\n  throw std::runtime_error(\"countQuartiles not implemented\");\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n\t\tKOKKOS_LAMBDA (const int i) {\n\t\t\tdouble xx = x(i);\n\t\t\tif (xx >= 0 && xx < 0.25) {\n\t\t\t\t++bins(0);\n\t\t\t}\n\t\t\telse if (xx >= 0.25 && xx < 0.5) {\n\t\t\t\t++bins(1);\n\t\t\t}\n\t\t\telse if (xx >= 0.5 && xx < 0.75) {\n\t\t\t\t++bins(2);\n\t\t\t}\n\t\t\telse if (xx >= 0.75 && xx <= 1) {\n\t\t\t\t++bins(3);\n\t\t\t}\n\t});\n}",
            "// Your code here\n\n  Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace> range_policy(0, x.extent(0));\n  Kokkos::parallel_for(range_policy, KOKKOS_LAMBDA(int i) {\n    double value = x(i);\n    if (value <= 0.25) {\n      bins(0) += 1;\n    } else if (value <= 0.5) {\n      bins(1) += 1;\n    } else if (value <= 0.75) {\n      bins(2) += 1;\n    } else {\n      bins(3) += 1;\n    }\n  });\n}",
            "// your code here\n}",
            "// TODO: Implement this function\n\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, x.extent(0)), [&] (size_t i) {\n    if(x(i) < 0 || x(i) > 10) {\n      bins(0) += 1;\n      bins(3) += 1;\n    } else {\n      double remainder = x(i) - floor(x(i));\n      if(remainder <= 0.25) {\n        bins(0) += 1;\n      } else if(remainder <= 0.5) {\n        bins(1) += 1;\n      } else if(remainder <= 0.75) {\n        bins(2) += 1;\n      } else {\n        bins(3) += 1;\n      }\n    }\n  });\n}",
            "/* TODO: Implement this function */\n    size_t n = x.extent(0);\n    Kokkos::View<size_t*> temp(\"temp\", n);\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int i){\n        if (x(i) >= 0 && x(i) < 0.25) temp(i) = 0;\n        else if (x(i) >= 0.25 && x(i) < 0.5) temp(i) = 1;\n        else if (x(i) >= 0.5 && x(i) < 0.75) temp(i) = 2;\n        else temp(i) = 3;\n    });\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA(const int i){\n        Kokkos::atomic_fetch_add(&bins(temp(i)), 1);\n    });\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n}",
            "// Your code goes here.\n}",
            "// YOUR CODE HERE\n    Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int, const int, size_t i){\n        if (i < bins.extent(0) && (x(i) < 0.25 || x(i) > 0.75)) {\n            ++bins(i);\n        }\n    });\n}",
            "const size_t n = x.extent(0);\n  const size_t num_blocks = 200;\n\n  const size_t block_size = n / num_blocks;\n  const size_t remaining = n % num_blocks;\n\n  Kokkos::View<double*[4]> bins_per_block(\"Bins per block\", num_blocks);\n  Kokkos::parallel_for(num_blocks, KOKKOS_LAMBDA(const size_t block) {\n      const size_t start = block_size * block;\n      const size_t end = start + block_size;\n      const size_t block_size = end - start;\n      if (block < remaining) {\n        block_size += 1;\n      }\n\n      const double* block_data = x.data() + start;\n\n      double[4] block_bins;\n      size_t count_less_half = 0;\n      size_t count_half_to_three_quarters = 0;\n      size_t count_three_quarters_to_half = 0;\n      size_t count_greater_half = 0;\n\n      for (size_t i = 0; i < block_size; ++i) {\n        if (block_data[i] < 0.25) {\n          count_less_half += 1;\n        } else if (block_data[i] < 0.5) {\n          count_half_to_three_quarters += 1;\n        } else if (block_data[i] < 0.75) {\n          count_three_quarters_to_half += 1;\n        } else {\n          count_greater_half += 1;\n        }\n      }\n\n      block_bins[0] = count_less_half;\n      block_bins[1] = count_half_to_three_quarters;\n      block_bins[2] = count_three_quarters_to_half;\n      block_bins[3] = count_greater_half;\n\n      bins_per_block(block, 0) = count_less_half;\n      bins_per_block(block, 1) = count_half_to_three_quarters;\n      bins_per_block(block, 2) = count_three_quarters_to_half;\n      bins_per_block(block, 3) = count_greater_half;\n    });\n\n  // Calculate the final bins\n  size_t total_count_less_half = 0;\n  size_t total_count_half_to_three_quarters = 0;\n  size_t total_count_three_quarters_to_half = 0;\n  size_t total_count_greater_half = 0;\n\n  Kokkos::View<double*[4]> final_bins(\"Final bins\", 1);\n  Kokkos::parallel_reduce(num_blocks, KOKKOS_LAMBDA(const size_t block, size_t& final_bins_val) {\n      const size_t start = block_size * block;\n      const size_t end = start + block_size;\n      const size_t block_size = end - start;\n      if (block < remaining) {\n        block_size += 1;\n      }\n      double[4] block_bins;\n      size_t count_less_half = 0;\n      size_t count_half_to_three_quarters = 0;\n      size_t count_three_quarters_to_half = 0;\n      size_t count_greater_half = 0;\n\n      const double* block_data = x.data() + start;\n      for (size_t i = 0; i < block_size; ++i) {\n        if (block_data[i] < 0.25) {\n          count_less_half += 1;\n        } else if (block_data[i] < 0.5) {\n          count_half_to_three_quarters += 1;\n        } else if (block_data[i] < 0.75) {\n          count_three_quarters_to_half += 1;\n        } else {\n          count_greater_half += 1;\n        }\n      }\n\n      block_bins[0] = count_less_half;\n      block_bins[1] = count_half_to_three_quarters;\n      block_bins[2] = count_three_quarters_to_half;\n      block_bins[3]",
            "/* Fill in your implementation here. You may have to add additional arguments\n       to the function signature if the defaults (e.g. the value of the `reducer`)\n       are not sufficient. */\n}",
            "//TODO: Your code here!\n}",
            "size_t n = x.extent(0);\n    Kokkos::parallel_for(\"countQuartiles\", n, KOKKOS_LAMBDA(size_t i) {\n        double xi = x(i);\n        if (xi >= 0.0 && xi < 0.25) {\n            bins(0)++;\n        }\n        else if (xi >= 0.25 && xi < 0.5) {\n            bins(1)++;\n        }\n        else if (xi >= 0.5 && xi < 0.75) {\n            bins(2)++;\n        }\n        else {\n            bins(3)++;\n        }\n    });\n}",
            "Kokkos::parallel_for(1, KOKKOS_LAMBDA (const int) {\n\n  });\n}",
            "}",
            "// TODO\n}",
            "// TODO: fill in this function using Kokkos\n}",
            "// TODO: implement this function\n  Kokkos::fill_n(bins, 4, 0);\n  Kokkos::parallel_for(100, KOKKOS_LAMBDA(const int) {\n    if ((x(0) <= 0.25*x(0)) && (x(0) < 0.5)) {\n      bins(0) += 1;\n    }\n    if ((x(1) <= 0.25*x(1)) && (x(1) < 0.5)) {\n      bins(1) += 1;\n    }\n    if ((x(2) <= 0.25*x(2)) && (x(2) < 0.5)) {\n      bins(2) += 1;\n    }\n    if ((x(3) <= 0.25*x(3)) && (x(3) < 0.5)) {\n      bins(3) += 1;\n    }\n  });\n  Kokkos::fence();\n}",
            "const size_t num_elements = x.extent(0);\n\n  Kokkos::parallel_for(num_elements, KOKKOS_LAMBDA(const size_t& i) {\n    double value = x(i);\n    if (value >= 0.0 && value < 0.25) {\n      ++bins[0];\n    } else if (value >= 0.25 && value < 0.5) {\n      ++bins[1];\n    } else if (value >= 0.5 && value < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  });\n\n}",
            "}",
            "/* TODO */\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n\t\t[=] (const size_t i) {\n\t\t\tconst double val = x(i);\n\t\t\tif (val < 0.25) {\n\t\t\t\tbins(0) += 1;\n\t\t\t} else if (val < 0.5) {\n\t\t\t\tbins(1) += 1;\n\t\t\t} else if (val < 0.75) {\n\t\t\t\tbins(2) += 1;\n\t\t\t} else {\n\t\t\t\tbins(3) += 1;\n\t\t\t}\n\t\t}\n\t);\n}",
            "// TODO\n\n}",
            "// TODO: Fill this in\n}",
            "// your code goes here\n   return;\n}",
            "// YOUR CODE HERE\n   size_t n = x.extent(0);\n   size_t* x_host = Kokkos::create_mirror_view(x);\n   Kokkos::deep_copy(x_host, x);\n\n   // Compute the range of values in the vector, using the Kokkos\n   // parallel_reduce to find min and max values at the same time.\n   // Store the minimum value in min_val, and the maximum value\n   // in max_val.\n   double min_val, max_val;\n   Kokkos::parallel_reduce(\"RangeReduction\",\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n      KOKKOS_LAMBDA(const int i, double& min, double& max) {\n         min = std::min(min, x_host[i]);\n         max = std::max(max, x_host[i]);\n      }, Kokkos::Min<double>(min_val), Kokkos::Max<double>(max_val));\n\n   // Compute the number of values in each bin, using Kokkos\n   // parallel_for to count the number of values in each bin.\n   // Store the counts in `bins`.\n\n   // YOUR CODE HERE\n\n   Kokkos::deep_copy(bins, bin_host);\n   Kokkos::deallocate(x_host);\n}",
            "// TODO: Implement this function\n}",
            "// TODO\n    int N = x.extent(0);\n    double sum;\n    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, N), [&](int i, double &total) {\n        if(x(i) >= 0 && x(i) < 0.25) {\n            bins(0) += 1;\n            total += x(i);\n        }\n        else if(x(i) >= 0.25 && x(i) < 0.5) {\n            bins(1) += 1;\n            total += x(i);\n        }\n        else if(x(i) >= 0.5 && x(i) < 0.75) {\n            bins(2) += 1;\n            total += x(i);\n        }\n        else {\n            bins(3) += 1;\n            total += x(i);\n        }\n    }, sum);\n    std::cout << sum << std::endl;\n}",
            "// TODO: Implement this function\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: your code here\n  size_t num_doubles = x.extent(0);\n  size_t low_count = 0;\n  size_t mid_count = 0;\n  size_t high_count = 0;\n  size_t very_high_count = 0;\n  Kokkos::RangePolicy<Kokkos::HostSpace> policy(0, num_doubles);\n  Kokkos::parallel_reduce(\"countQuartiles\", policy, KOKKOS_LAMBDA(const int& i, size_t& sum) {\n    if (x(i) <= 0.25) {\n      low_count++;\n    } else if (x(i) <= 0.5) {\n      mid_count++;\n    } else if (x(i) <= 0.75) {\n      high_count++;\n    } else {\n      very_high_count++;\n    }\n  }, Kokkos::Sum<size_t>(sum));\n  bins(0) = low_count;\n  bins(1) = mid_count;\n  bins(2) = high_count;\n  bins(3) = very_high_count;\n  Kokkos::fence();\n}",
            "// TODO: Implement this\n}",
            "// Fill your solution here.\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA(size_t i) {\n      auto val = x(i);\n      if (val >= 0.0 && val < 0.25) {\n        ++bins(0);\n      } else if (val >= 0.25 && val < 0.5) {\n        ++bins(1);\n      } else if (val >= 0.5 && val < 0.75) {\n        ++bins(2);\n      } else if (val >= 0.75 && val < 1.0) {\n        ++bins(3);\n      }\n    });\n\n  Kokkos::fence();\n}",
            "/* YOUR CODE HERE */\n}",
            "Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(size_t i) {\n      // TODO: replace this with your code\n      bins(0) = i;\n      bins(1) = i;\n      bins(2) = i;\n      bins(3) = i;\n   });\n}",
            "// TODO: Implement this method\n}",
            "/* YOUR CODE HERE */\n\n}",
            "// TODO\n\n}",
            "// TODO\n}",
            "size_t n = x.extent(0);\n\n  // Your code goes here...\n}",
            "Kokkos::View<size_t*> bins_h(\"bins_h\", 4);\n   Kokkos::deep_copy(bins_h, bins);\n\n   auto policy = Kokkos::TeamPolicy<>::team_policy(x.size(), Kokkos::AUTO);\n   policy.team_size_max(4);\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const Kokkos::TeamPolicy<>::member_type& team) {\n      const size_t id = team.league_rank();\n      size_t count = 0;\n      for(auto i = team.team_begin(); i < x.size(); i += team.team_size()) {\n         if(x(i) < 0 || x(i) > 1) {\n            team.team_barrier();\n            continue;\n         }\n         double fractional_part = x(i) - floor(x(i));\n         if(fractional_part >= 0.0 && fractional_part < 0.25) {\n            count++;\n         } else if(fractional_part >= 0.25 && fractional_part < 0.50) {\n            team.team_barrier();\n            count += Kokkos::atomic_fetch_add(&bins_h(1), 1);\n         } else if(fractional_part >= 0.5 && fractional_part < 0.75) {\n            team.team_barrier();\n            count += Kokkos::atomic_fetch_add(&bins_h(2), 1);\n         } else if(fractional_part >= 0.75 && fractional_part < 1.0) {\n            team.team_barrier();\n            count += Kokkos::atomic_fetch_add(&bins_h(3), 1);\n         }\n      }\n      Kokkos::atomic_fetch_add(&bins_h(0), count);\n   });\n\n   Kokkos::deep_copy(bins, bins_h);\n}",
            "// TODO: implement countQuartiles\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&] (int i) {\n    // 7.8 4.2 9.1 7.6 0.27 1.5 3.8\n    //    ^  ^  ^\n    //  0.25 0.5 0.75\n    if (x(i) < 0.25) {\n      bins(0) += 1;\n    } else if (x(i) < 0.5) {\n      bins(1) += 1;\n    } else if (x(i) < 0.75) {\n      bins(2) += 1;\n    } else {\n      bins(3) += 1;\n    }\n  });\n}",
            "// Your code here\n  // Hint: You can use Kokkos::parallel_for and Kokkos::atomic_fetch_add.\n\n  size_t n = x.extent(0);\n  Kokkos::View<size_t, Kokkos::HostSpace> counts(\"counts\", 4);\n\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, n),\n    KOKKOS_LAMBDA (const size_t& i) {\n      double x_i = x(i);\n      if (x_i < 0.25) {\n        Kokkos::atomic_fetch_add(&counts(0), 1);\n      } else if (x_i < 0.5) {\n        Kokkos::atomic_fetch_add(&counts(1), 1);\n      } else if (x_i < 0.75) {\n        Kokkos::atomic_fetch_add(&counts(2), 1);\n      } else {\n        Kokkos::atomic_fetch_add(&counts(3), 1);\n      }\n  });\n  Kokkos::deep_copy(counts, bins);\n}",
            "}",
            "// TODO: You fill in here...\n\n  size_t n = x.extent(0);\n\n  // Allocate the bins view and initialize it to zero\n  Kokkos::View<size_t[4]> bins_ = Kokkos::View<size_t[4]>(\"bins\", 4);\n  Kokkos::deep_copy(bins_, 0);\n\n  // Compute the number of doubles that have a fractional part in each bin\n  double lower_bounds[] = {0.0, 0.25, 0.5, 0.75};\n  for (size_t i = 0; i < 4; ++i) {\n    Kokkos::View<double*> x_sub(\"x_sub\", n);\n    Kokkos::deep_copy(x_sub, x);\n    Kokkos::View<double*> x_sub_frac(\"x_sub_frac\", n);\n    Kokkos::deep_copy(x_sub_frac, x);\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n      KOKKOS_LAMBDA (size_t i) {\n        x_sub(i) = x(i) - lower_bounds[i];\n        x_sub_frac(i) = x_sub(i) - std::floor(x_sub(i));\n      }\n    );\n\n    Kokkos::View<size_t*> count_sub(\"count_sub\", n);\n    Kokkos::deep_copy(count_sub, 0);\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n      KOKKOS_LAMBDA (size_t i) {\n        if (x_sub_frac(i) >= 0 && x_sub_frac(i) < 0.25) {\n          Kokkos::atomic_fetch_add(&(count_sub(i)), 1);\n        }\n      }\n    );\n\n    Kokkos::View<size_t*> count(\"count\", 1);\n    Kokkos::deep_copy(count, 0);\n\n    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n      KOKKOS_LAMBDA (size_t i, size_t& sum) {\n        sum += count_sub(i);\n      }, count);\n\n    Kokkos::deep_copy(bins_(i), count());\n  }\n\n  Kokkos::deep_copy(bins, bins_);\n}",
            "const size_t N = x.size();\n  size_t* b = bins.data();\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA (int i) {\n    if (x(i) < 0.25) b[0]++;\n    else if (x(i) < 0.5) b[1]++;\n    else if (x(i) < 0.75) b[2]++;\n    else b[3]++;\n  });\n}",
            "// YOUR CODE HERE\n}",
            "auto x_host = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_host, x);\n  auto x_host_ptr = x_host.data();\n  auto bins_host = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(bins_host, bins);\n  auto bins_host_ptr = bins_host.data();\n  Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.size()),\n      KOKKOS_LAMBDA(int i) {\n        size_t i_bin;\n        double x_i = x_host_ptr[i];\n        if (x_i >= 0.0 && x_i < 0.25) {\n          i_bin = 0;\n        } else if (x_i >= 0.25 && x_i < 0.5) {\n          i_bin = 1;\n        } else if (x_i >= 0.5 && x_i < 0.75) {\n          i_bin = 2;\n        } else {\n          i_bin = 3;\n        }\n        Kokkos::atomic_fetch_add(&bins_host_ptr[i_bin], 1);\n      });\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO: Fill in code to count the quartiles.\n}",
            "// TODO\n}",
            "// *** TO BE IMPLEMENTED ***\n}",
            "// TODO\n    size_t N = x.extent(0);\n    size_t nbins = 4;\n    size_t quotient = N / nbins;\n    size_t remainder = N % nbins;\n\n    // TODO\n    for (size_t i = 0; i < quotient; i++) {\n        for (size_t j = 0; j < nbins; j++) {\n            size_t idx = i * nbins + j;\n            double fraction = x(idx) - floor(x(idx));\n            if (fraction >= 0.25 && fraction < 0.5) {\n                bins(j) += 1;\n            } else if (fraction >= 0.5 && fraction < 0.75) {\n                bins(j + 1) += 1;\n            } else if (fraction >= 0.75 && fraction < 1.0) {\n                bins(j + 2) += 1;\n            } else {\n                bins(j + 3) += 1;\n            }\n        }\n    }\n\n    // TODO\n    for (size_t j = 0; j < remainder; j++) {\n        size_t idx = quotient * nbins + j;\n        double fraction = x(idx) - floor(x(idx));\n        if (fraction >= 0.25 && fraction < 0.5) {\n            bins(j) += 1;\n        } else if (fraction >= 0.5 && fraction < 0.75) {\n            bins(j + 1) += 1;\n        } else if (fraction >= 0.75 && fraction < 1.0) {\n            bins(j + 2) += 1;\n        } else {\n            bins(j + 3) += 1;\n        }\n    }\n}",
            "size_t N = x.extent(0);\n  Kokkos::parallel_for(N, KOKKOS_LAMBDA(size_t i) {\n    double val = x(i);\n    if (val < 0.25) {\n      bins(0)++;\n    } else if (val < 0.5) {\n      bins(1)++;\n    } else if (val < 0.75) {\n      bins(2)++;\n    } else {\n      bins(3)++;\n    }\n  });\n}",
            "// TODO: your code here\n  double frac[4] = {0.25, 0.5, 0.75, 1.0};\n  size_t n = x.extent(0);\n\n  auto x_h = Kokkos::create_mirror_view(x);\n  Kokkos::deep_copy(x_h, x);\n\n  auto bins_h = Kokkos::create_mirror_view(bins);\n\n  size_t count[4] = {0};\n  for(size_t i = 0; i < n; i++) {\n    if(x_h(i) < frac[0]) {\n      count[0] += 1;\n    } else if (x_h(i) >= frac[0] && x_h(i) < frac[1]) {\n      count[1] += 1;\n    } else if (x_h(i) >= frac[1] && x_h(i) < frac[2]) {\n      count[2] += 1;\n    } else if (x_h(i) >= frac[2] && x_h(i) < frac[3]) {\n      count[3] += 1;\n    } else if (x_h(i) >= frac[3]) {\n      count[0] += 1;\n      count[1] += 1;\n      count[2] += 1;\n      count[3] += 1;\n    }\n  }\n\n  bins_h(0) = count[0];\n  bins_h(1) = count[1];\n  bins_h(2) = count[2];\n  bins_h(3) = count[3];\n\n  Kokkos::deep_copy(bins, bins_h);\n\n  return;\n}",
            "size_t num_doubles = x.extent(0);\n\tsize_t num_threads = Kokkos::TeamPolicy<>::team_size_recommended(Kokkos::ParallelForTag());\n\tKokkos::TeamPolicy<>::member_type team_member = Kokkos::TeamPolicy<>::team_policy_t(num_threads, 1, 0).team_member();\n\tKokkos::parallel_for(\"countQuartiles\", num_doubles, KOKKOS_LAMBDA(const size_t i) {\n\t\tdouble fraction = x(i) - floor(x(i));\n\t\tswitch (fraction) {\n\t\t\tcase 0:\n\t\t\t\tbins(0) += 1;\n\t\t\t\tbreak;\n\t\t\tcase 0.25:\n\t\t\t\tbins(1) += 1;\n\t\t\t\tbreak;\n\t\t\tcase 0.5:\n\t\t\t\tbins(2) += 1;\n\t\t\t\tbreak;\n\t\t\tcase 0.75:\n\t\t\t\tbins(3) += 1;\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbreak;\n\t\t}\n\t});\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "// Complete this function.\n  // TODO\n}",
            "// TODO: implement this function\n  // hint: use Kokkos::parallel_reduce with two lambda expressions: one for counting and one for filling\n\n  auto x_host = Kokkos::create_mirror_view(x);\n  auto bins_host = Kokkos::create_mirror_view(bins);\n\n  Kokkos::deep_copy(x_host, x);\n  Kokkos::deep_copy(bins_host, bins);\n\n  double quartiles[4] = {0.0, 0.25, 0.5, 0.75};\n  size_t counts[4] = {0, 0, 0, 0};\n  double frac;\n\n  for (size_t i = 0; i < x.extent(0); i++) {\n    frac = x_host(i) - floor(x_host(i));\n    for (int j = 0; j < 4; j++) {\n      if (frac >= quartiles[j] && frac < quartiles[j+1]) {\n        counts[j]++;\n      }\n    }\n  }\n\n  Kokkos::deep_copy(bins, counts);\n\n  return;\n}",
            "// TODO: your code here\n}",
            "// TODO\n}",
            "double const cutoffs[4] = {0.25, 0.5, 0.75, 1.0};\n    bins = Kokkos::View<size_t[4]>(\"bins\", 4);\n    size_t n = x.extent(0);\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Serial>(0, n), KOKKOS_LAMBDA(const int i) {\n        double x_i = x(i);\n        double x_i_abs = std::fabs(x_i);\n        if (x_i_abs < cutoffs[0]) {\n            bins(0) += 1;\n        } else if (x_i_abs < cutoffs[1]) {\n            bins(1) += 1;\n        } else if (x_i_abs < cutoffs[2]) {\n            bins(2) += 1;\n        } else {\n            bins(3) += 1;\n        }\n    });\n    Kokkos::fence();\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO:\n  // 1. Initialize the output view (array)\n  // 2. Fill it with the number of elements in each of the four quartiles\n\n  auto h_bins = Kokkos::create_mirror_view(bins);\n  Kokkos::deep_copy(h_bins, bins);\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(\"countQuartiles\", x.extent(0), KOKKOS_LAMBDA(size_t i) {\n    if (x(i) < 0.25) {\n      bins(0) += 1;\n    } else if (x(i) < 0.5) {\n      bins(1) += 1;\n    } else if (x(i) < 0.75) {\n      bins(2) += 1;\n    } else {\n      bins(3) += 1;\n    }\n  });\n}",
            "// 1. Allocate and initialize bins\n    //...\n\n    // 2. Count up the number of values that fall in each bin\n    //...\n\n    // 3. Compute the final answer\n    //...\n}",
            "// TODO:\n  //...\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: your code goes here\n}",
            "// Your code goes here!\n\n}",
            "// Kokkos parallel_for implementation\n  // Hint: Use Kokkos::MDRangePolicy\n  // Hint: Use Kokkos::TeamPolicy\n  // Hint: Use Kokkos::TeamThreadRange\n}",
            "// TODO: Compute the bin counts here.\n    Kokkos::parallel_for(10, KOKKOS_LAMBDA (const int i) {\n    });\n}",
            "// TODO\n}",
            "// TODO: Replace the following line with your implementation\n  size_t size = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, size),\n                       [=](const int i) {\n                         double value = x(i);\n                         // TODO: Fill in the rest of this function.\n                       });\n}",
            "// TODO: insert code here\n\n}",
            "// TODO: Compute the quartiles using Kokkos. \n  // Use the Kokkos parallel_for to parallelize the computation.\n}",
            "// TODO\n  return;\n}",
            "}",
            "size_t n = x.extent(0);\n  Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n),\n    KOKKOS_LAMBDA(const int i) {\n      double xi = x(i);\n      // put this in a if statement for floating-point error?\n      if (xi <= 0.25)\n        ++bins(0);\n      else if (xi <= 0.5)\n        ++bins(1);\n      else if (xi <= 0.75)\n        ++bins(2);\n      else\n        ++bins(3);\n  });\n  Kokkos::fence();\n}",
            "// Fill in your code here\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n      KOKKOS_LAMBDA (size_t i) {\n         if (x(i) < 0.25)\n            bins(0) += 1;\n         else if (x(i) < 0.5)\n            bins(1) += 1;\n         else if (x(i) < 0.75)\n            bins(2) += 1;\n         else\n            bins(3) += 1;\n      }\n   );\n}",
            "}",
            "// TODO: Your code here\n}",
            "// YOUR CODE HERE\n\n}",
            "// YOUR CODE HERE\n  Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(x.extent(0));\n  Kokkos::parallel_for(\"countQuartiles\", policy, KOKKOS_LAMBDA(const int i) {\n    double x_i = x(i);\n    if (x_i < 0.25) {\n      bins(0) += 1;\n    } else if (x_i < 0.5) {\n      bins(1) += 1;\n    } else if (x_i < 0.75) {\n      bins(2) += 1;\n    } else {\n      bins(3) += 1;\n    }\n  });\n  Kokkos::fence();\n}",
            "Kokkos::parallel_for(1, KOKKOS_LAMBDA(size_t i) {\n        double quartile = (x(i) - floor(x(i))) / 0.25;\n        if (quartile <= 0.25) {\n            bins(0) += 1;\n        }\n        else if (quartile <= 0.5) {\n            bins(1) += 1;\n        }\n        else if (quartile <= 0.75) {\n            bins(2) += 1;\n        }\n        else {\n            bins(3) += 1;\n        }\n    });\n}",
            "// TODO: Finish this function\n}",
            "//TODO: Your code goes here\n}",
            "double lower = 0.0;\n  double upper = 0.25;\n  double step = (upper-lower)/3;\n\n  double val = 0.0;\n  Kokkos::View<size_t*> bins_i(\"bins_i\");\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(int) {\n    for (int i = 0; i < 4; i++) {\n      bins_i(i) = 0;\n    }\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(x.size(), KOKKOS_LAMBDA(int i) {\n    val = x(i);\n    for (int j = 0; j < 4; j++) {\n      if ((val >= lower) && (val < upper)) {\n        bins_i(j) += 1;\n      }\n      upper += step;\n      lower += step;\n    }\n  });\n  Kokkos::fence();\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(int) {\n    bins(0) = bins_i(0);\n    bins(1) = bins_i(1);\n    bins(2) = bins_i(2);\n    bins(3) = bins_i(3);\n  });\n  Kokkos::fence();\n}",
            "// TODO\n    Kokkos::View<size_t[4]> temp(\"temp\", 4);\n    Kokkos::parallel_for(\"Count quartiles\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)),\n    KOKKOS_LAMBDA (const int i) {\n        double value = x(i);\n        if (0.0 <= value && value < 0.25) {\n            Kokkos::atomic_fetch_add(&(temp(0)), 1);\n        } else if (0.25 <= value && value < 0.5) {\n            Kokkos::atomic_fetch_add(&(temp(1)), 1);\n        } else if (0.5 <= value && value < 0.75) {\n            Kokkos::atomic_fetch_add(&(temp(2)), 1);\n        } else if (0.75 <= value && value <= 1.0) {\n            Kokkos::atomic_fetch_add(&(temp(3)), 1);\n        }\n    });\n    Kokkos::deep_copy(bins, temp);\n}",
            "// TODO: implement this function\n\n}",
            "// TODO: Implement this function\n  size_t bin_count = 0;\n  // sort vector\n  Kokkos::View<double*> x_sorted(\"x_sorted\", x.extent(0));\n  Kokkos::View<size_t*> sort_indices(\"sort_indices\", x.extent(0));\n  Kokkos::View<size_t*> reverse_indices(\"reverse_indices\", x.extent(0));\n  Kokkos::parallel_for(\"fill_sort_indices\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)), KOKKOS_LAMBDA(const int i) {\n    sort_indices(i) = i;\n    reverse_indices(i) = i;\n    x_sorted(i) = x(i);\n  });\n  Kokkos::fence();\n  Kokkos::sort(Kokkos::HostSpace, x_sorted, sort_indices);\n  Kokkos::parallel_for(\"fill_reverse_indices\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)), KOKKOS_LAMBDA(const int i) {\n    reverse_indices(sort_indices(i)) = i;\n  });\n  Kokkos::fence();\n  // calculate fractions of each value in x_sorted\n  Kokkos::View<size_t*> bins_host(\"bins_host\", 4);\n  Kokkos::deep_copy(bins_host, bins);\n  Kokkos::parallel_for(\"calculate fractions\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)), KOKKOS_LAMBDA(const int i) {\n    // check which bin\n    if (x_sorted(i) < 0.25) {\n      bins_host(0) += 1;\n    } else if (x_sorted(i) < 0.5) {\n      bins_host(1) += 1;\n    } else if (x_sorted(i) < 0.75) {\n      bins_host(2) += 1;\n    } else {\n      bins_host(3) += 1;\n    }\n  });\n  Kokkos::fence();\n  Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO: Your code here\n    // TODO: You may assume that x.extent(0) == bins.extent(0)\n    // TODO: Do not modify the Kokkos code above this comment\n    // TODO: Do not modify this function.\n    // TODO: Do not assume that Kokkos is initialized.\n    // TODO: Do not assume that `x` is sorted.\n    // TODO: Do not assume that `x` has duplicate values.\n    // TODO: Do not use any parallelism.\n    // TODO: You may assume that Kokkos is initialized.\n    // TODO: You may assume that `x` is sorted.\n    // TODO: You may assume that `x` has duplicate values.\n    // TODO: You may assume that `bins` is initialized.\n\n    // TODO: Count the number of doubles in the vector `x` that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: complete the function\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.extent(0)), [&] (size_t i) {\n        double a = x(i);\n        if (a > 0.25) {\n            if (a < 0.5)\n                ++bins(1);\n            else\n                ++bins(2);\n        }\n        else if (a > 0)\n            ++bins(0);\n        else\n            ++bins(3);\n    });\n    Kokkos::fence();\n}",
            "// TODO: Implement this function\n}",
            "// Fill the bins array with zeroes.\n    // This is not necessary, but it helps the compiler with some type inference.\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 4),\n        KOKKOS_LAMBDA(int bin) {\n            bins(bin) = 0;\n        });\n    Kokkos::fence();\n\n    // Compute the number of values in each bin.\n    const double q0 = 0.0;\n    const double q1 = 0.25;\n    const double q2 = 0.5;\n    const double q3 = 0.75;\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n        KOKKOS_LAMBDA(int i) {\n            if (x(i) >= q0 && x(i) < q1) {\n                ++bins(0);\n            } else if (x(i) >= q1 && x(i) < q2) {\n                ++bins(1);\n            } else if (x(i) >= q2 && x(i) < q3) {\n                ++bins(2);\n            } else if (x(i) >= q3) {\n                ++bins(3);\n            }\n        });\n    Kokkos::fence();\n\n    // Print the results.\n    // Printing should not be inside parallel_for.\n    // You can only print from one thread at a time.\n    std::cout << \"quartile 0, 1, 2, 3: \" << bins(0) << \", \" << bins(1) << \", \" << bins(2) << \", \" << bins(3) << std::endl;\n}",
            "// TO DO: Add your code here\n}",
            "// YOUR CODE HERE\n  //\n  // Hint: you can use Kokkos::TeamPolicy to define a team of threads to work\n  // on the input vector. You can then use Kokkos::parallel_for to parallelize\n  // the work and access data using thread local views. You may find the \n  // following links helpful:\n  //\n  // http://kokkos.github.io/kokkos-tutorials/\n  // http://kokkos.github.io/kokkos-tutorials/tutorial_02_parallel_for/\n  // http://kokkos.github.io/kokkos-tutorials/tutorial_03_layout_patterns/\n  // https://github.com/kokkos/kokkos-tutorials/blob/master/tutorial/01-vectors/Kokkos_Tutorial_01_Vectors.cpp\n  //\n  // You can access the data in a view using the `data()` function. For example:\n  //\n  // size_t num_elements = x.extent(0);\n  // double *x_data = x.data();\n  // for (size_t i = 0; i < num_elements; ++i) {\n  //   // do something with x_data[i]\n  // }\n  //\n  //\n  // HINT: remember to set the number of threads to use with Kokkos. For example:\n  //\n  // Kokkos::set_num_threads(4);\n  //\n\n  // TODO: Your code here\n  Kokkos::parallel_for(\n    \"countQuartiles\",\n    Kokkos::TeamPolicy<Kokkos::TeamDynamic<Kokkos::DefaultExecutionSpace::execution_space::static_pool_size()>>(\n      x.extent(0), Kokkos::AUTO),\n    KOKKOS_LAMBDA(const Kokkos::TeamMember &member, const size_t i) {\n      size_t bin = 0;\n      if (x(i) < 0.25)\n        bin = 0;\n      else if (x(i) < 0.5)\n        bin = 1;\n      else if (x(i) < 0.75)\n        bin = 2;\n      else if (x(i) < 1)\n        bin = 3;\n\n      member.team_barrier();\n      bins(bin) += 1;\n    });\n}",
            "const int n = x.size();\n  const double one_fourth = 1.0 / 4.0;\n  const double two_third = 2.0 / 3.0;\n  const double three_fourth = 3.0 / 4.0;\n  const double four_third = 4.0 / 3.0;\n  Kokkos::parallel_for(n, KOKKOS_LAMBDA(int i) {\n      const double x_i = x(i);\n      if ((x_i >= 0.0 && x_i < 0.25) || (x_i >= 1.0 && x_i < 1.25)) {\n        bins(0) += 1;\n      } else if ((x_i >= 0.25 && x_i < 0.5) || (x_i >= 1.25 && x_i < 1.5)) {\n        bins(1) += 1;\n      } else if ((x_i >= 0.5 && x_i < 0.75) || (x_i >= 1.5 && x_i < 1.75)) {\n        bins(2) += 1;\n      } else {\n        bins(3) += 1;\n      }\n    });\n  Kokkos::fence();\n\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(int i) {\n      bins(i) += bins(i-1);\n    });\n  Kokkos::fence();\n\n  Kokkos::parallel_for(4, KOKKOS_LAMBDA(int i) {\n      bins(i) = (i * 2) - 1;\n    });\n  Kokkos::fence();\n\n  Kokkos::parallel_for(1, KOKKOS_LAMBDA(int i) {\n      bins(i) -= bins(0);\n    });\n  Kokkos::fence();\n}",
            "// TODO\n}",
            "// TODO\n    // You may need to declare more local variables, or define additional helper functions.\n    //\n    // You may also need to add a Kokkos reduction, possibly with a custom operator.\n    // See the Kokkos docs for how to do this.\n}",
            "//TODO: complete this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: fill in this function\n}",
            "// TODO: Define a policy to perform parallel reductions and then execute it.\n  // The policy should execute with at most one reducer and vector of length 4.\n  // The reducer should be initialized with the number of elements in the array.\n  // The functor should sum the values in the vector (or use the standard C++ sum function)\n  // and store the result in the output array.\n  // Hint: Look at the Kokkos documentation for reducers:\n  //   https://github.com/kokkos/kokkos/wiki/Reducers\n  // Hint: Look at the Kokkos documentation for policies and functors:\n  //   https://github.com/kokkos/kokkos/wiki/Policies-and-Futures\n  // Hint: Look at the Kokkos documentation for reductions:\n  //   https://github.com/kokkos/kokkos/wiki/Reductions\n  // Hint: You may find the kokkos::parallel_reduce function useful.\n  // Hint: You may find the Kokkos::subview function useful.\n\n  // TODO: Define the functor that is passed to parallel_reduce\n  // Hint: Look at the Kokkos documentation for functors.\n  // Hint: You may find the Kokkos::subview function useful.\n\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, x.extent(0)), [&] (size_t i) {\n    double fraction = x(i) - floor(x(i));\n    if (fraction > 0.75)\n      bins[3]++;\n    else if (fraction > 0.5)\n      bins[2]++;\n    else if (fraction > 0.25)\n      bins[1]++;\n    else\n      bins[0]++;\n  });\n}",
            "// TODO: Implement this function\n}",
            "//...\n}",
            "size_t n = x.extent(0);\n  auto bin0 = bins(0);\n  auto bin1 = bins(1);\n  auto bin2 = bins(2);\n  auto bin3 = bins(3);\n\n  Kokkos::View<size_t*> bin_map(\"bin_map\", n);\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA(const size_t i){\n    double xi = x(i);\n    double xi_fraction = xi - floor(xi);\n    if (xi_fraction < 0.25) {\n      bin0 += 1;\n    }\n    else if (xi_fraction < 0.5) {\n      bin1 += 1;\n    }\n    else if (xi_fraction < 0.75) {\n      bin2 += 1;\n    }\n    else {\n      bin3 += 1;\n    }\n    bin_map(i) = (xi_fraction > 0.75);\n  });\n  Kokkos::fence();\n\n  Kokkos::View<size_t*> tmp_bin0(\"tmp_bin0\", bin0);\n  Kokkos::View<size_t*> tmp_bin1(\"tmp_bin1\", bin1);\n  Kokkos::View<size_t*> tmp_bin2(\"tmp_bin2\", bin2);\n  Kokkos::View<size_t*> tmp_bin3(\"tmp_bin3\", bin3);\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA(const size_t i){\n    size_t index = bin_map(i);\n    if (index == 0) {\n      tmp_bin0(bin0) = i;\n      bin0 += 1;\n    }\n    else if (index == 1) {\n      tmp_bin1(bin1) = i;\n      bin1 += 1;\n    }\n    else if (index == 2) {\n      tmp_bin2(bin2) = i;\n      bin2 += 1;\n    }\n    else {\n      tmp_bin3(bin3) = i;\n      bin3 += 1;\n    }\n  });\n  Kokkos::fence();\n\n  Kokkos::View<size_t*> tmp_bin0_sorted(\"tmp_bin0_sorted\", bin0);\n  Kokkos::View<size_t*> tmp_bin1_sorted(\"tmp_bin1_sorted\", bin1);\n  Kokkos::View<size_t*> tmp_bin2_sorted(\"tmp_bin2_sorted\", bin2);\n  Kokkos::View<size_t*> tmp_bin3_sorted(\"tmp_bin3_sorted\", bin3);\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, bin0), KOKKOS_LAMBDA(const size_t i){\n    tmp_bin0_sorted(i) = tmp_bin0(i);\n  });\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, bin1), KOKKOS_LAMBDA(const size_t i){\n    tmp_bin1_sorted(i) = tmp_bin1(i);\n  });\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, bin2), KOKKOS_LAMBDA(const size_t i){\n    tmp_bin2_sorted(i) = tmp_bin2(i);\n  });\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, bin3), KOKKOS_LAMBDA(const size_t i){\n    tmp_bin3_sorted(i) = tmp_bin3(i);\n  });\n  Kokkos::fence();\n\n  Kokkos::View<size_t*> sorted_bins(\"sorted_bins\", n);\n  Kokkos::parallel_for(Kokkos::RangePolicy<>(0, n), KOKKOS_LAMBDA(const size_t i){\n    size_t xi = x(i);\n    size_t index = bin_map(i);\n    if (index == 0) {\n      sorted_bins(tmp_bin0_sorted(xi)) = i;\n    }\n    else if (index == 1) {\n      sorted_bins(tmp_bin1_sorted(xi)) = i;\n    }\n    else if (index == 2) {\n      sorted_bins(tmp_bin2_sorted(xi)) = i;\n    }\n    else {\n      sorted_bins(tmp_bin3_sorted(xi)) = i;\n    }",
            "// TODO: You fill in this function\n}",
            "// TODO\n}",
            "}",
            "// TODO: your code here\n}",
            "// TODO: implement me!\n    // You should assume that the number of elements in x is divisible by 4.\n    // TODO: you may find std::lower_bound helpful\n}",
            "// YOUR CODE HERE\n    // You can use Kokkos' parallel_for to do the same computation in parallel\n}",
            "// TODO\n  bins = 0;\n}",
            "auto bins_d = Kokkos::create_mirror_view(bins);\n  for (size_t i = 0; i < x.extent(0); ++i) {\n    double fractional_part = std::fmod(x(i), 0.5);\n    if (fractional_part < 0.25) {\n      bins_d(0) += 1;\n    } else if (fractional_part < 0.5) {\n      bins_d(1) += 1;\n    } else if (fractional_part < 0.75) {\n      bins_d(2) += 1;\n    } else {\n      bins_d(3) += 1;\n    }\n  }\n  Kokkos::deep_copy(bins, bins_d);\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: insert code here\n}",
            "// YOUR CODE HERE\n\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n    // You may assume that the size of x is a power of 2.\n    // Note that the output is an array of four integers (i.e., a C-style array).\n    // Hint: The Kokkos::parallel_for() macro can be used to run parallel for loops.\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, x.size()),\n        KOKKOS_LAMBDA (int i) {\n            // TODO\n        }\n    );\n}",
            "// TODO\n}",
            "/* TODO: implement this function */\n}",
            "/* You need to write your Kokkos code here */\n   Kokkos::View<size_t> block_offsets(\"Block Offsets\", 1);\n   Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int& idx) {\n      block_offsets(0) = idx;\n   });\n   Kokkos::fence();\n   Kokkos::View<size_t*> block_offsets_ptr(\"Block Offsets Ptr\", 1);\n   Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int& idx) {\n      block_offsets_ptr(0) = &block_offsets(0);\n   });\n   Kokkos::fence();\n   Kokkos::View<size_t> block_sizes(\"Block Sizes\", 1);\n   Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int& idx) {\n      block_sizes(0) = x.extent(0);\n   });\n   Kokkos::fence();\n   Kokkos::View<size_t*> block_sizes_ptr(\"Block Sizes Ptr\", 1);\n   Kokkos::parallel_for(1, KOKKOS_LAMBDA(const int& idx) {\n      block_sizes_ptr(0) = &block_sizes(0);\n   });\n   Kokkos::fence();\n   Kokkos::View<size_t*> bin_ptr(\"Bins Ptr\", 4);\n   Kokkos::parallel_for(4, KOKKOS_LAMBDA(const int& idx) {\n      bin_ptr(idx) = &bins(idx);\n   });\n   Kokkos::fence();\n   Kokkos::parallel_for(x.extent(0), KOKKOS_LAMBDA(const int& i) {\n      int bin = -1;\n      if (x(i) < 0.25) {\n         bin = 0;\n      } else if (x(i) < 0.5) {\n         bin = 1;\n      } else if (x(i) < 0.75) {\n         bin = 2;\n      } else {\n         bin = 3;\n      }\n      Kokkos::atomic_fetch_add(&(bin_ptr(bin)[0]), 1);\n   });\n   Kokkos::fence();\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: implement the function body\n}",
            "// TODO: add your code here\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, Kokkos::Schedule<Kokkos::Dynamic, Kokkos::Dynamic>>(0, x.extent(0)), KOKKOS_LAMBDA(const int& i) {\n        if (x(i) < 0.25)\n            bins(0) += 1;\n        else if (x(i) < 0.5)\n            bins(1) += 1;\n        else if (x(i) < 0.75)\n            bins(2) += 1;\n        else\n            bins(3) += 1;\n    });\n}",
            "// TODO: Fill this in with your code\n    size_t n = x.extent(0);\n    double tmp;\n    int i;\n    Kokkos::parallel_for(n, KOKKOS_LAMBDA (const int& i) {\n        if (i == 0) {\n            tmp = x(0);\n        }\n        else {\n            tmp = (tmp + x(i)) / 2;\n        }\n        if (tmp < 0.25) {\n            i = 0;\n        }\n        else if (tmp < 0.5) {\n            i = 1;\n        }\n        else if (tmp < 0.75) {\n            i = 2;\n        }\n        else {\n            i = 3;\n        }\n        Kokkos::atomic_fetch_add(&(bins(i)), 1);\n    });\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use OpenMP to compute in parallel.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// write your code here\n  // 1. sort the array using a parallel algorithm\n  omp_set_num_threads(10);\n#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    for (int j = 0; j < x.size() - 1; j++) {\n      if (x[j] > x[j + 1]) {\n        double tmp = x[j];\n        x[j] = x[j + 1];\n        x[j + 1] = tmp;\n      }\n    }\n  }\n  // 2. divide the array into 4 groups\n  // 3. count the number of elements in each group\n\n  int q1 = x.size() / 4;\n  int q2 = x.size() / 2;\n  int q3 = 3 * x.size() / 4;\n\n  for (int i = 0; i < x.size(); i++) {\n    if (i < q1) {\n      bins[0]++;\n    } else if (i < q2) {\n      bins[1]++;\n    } else if (i < q3) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "size_t n = x.size();\n    bins = std::array<size_t, 4>(0, 0, 0, 0);\n    if (n == 0) {\n        return;\n    }\n\n#pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < n; i++) {\n        double fractional_part = std::fmod(x[i], 1);\n        if (fractional_part < 0.25) {\n            bins[0] += 1;\n        } else if (fractional_part < 0.5) {\n            bins[1] += 1;\n        } else if (fractional_part < 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// your code goes here\n}",
            "// TODO: Your code here\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // for(auto i = 0; i < x.size(); i++){\n    //     if(x[i] < 0.25){\n    //         bins[0] = bins[0] + 1;\n    //     }else if(x[i] > 0.25 && x[i] < 0.5){\n    //         bins[1] = bins[1] + 1;\n    //     }else if(x[i] > 0.5 && x[i] < 0.75){\n    //         bins[2] = bins[2] + 1;\n    //     }else if(x[i] > 0.75){\n    //         bins[3] = bins[3] + 1;\n    //     }\n    // }\n    size_t n = x.size();\n    size_t k = (int)n/4;\n    size_t m = 0;\n\n    #pragma omp parallel shared(x, bins, n, k) private(m)\n    {\n        m = omp_get_thread_num();\n        size_t i = k*m;\n        size_t j = i + k;\n        \n        for(; i < j; i++){\n            if(x[i] < 0.25){\n                bins[0] = bins[0] + 1;\n            }else if(x[i] > 0.25 && x[i] < 0.5){\n                bins[1] = bins[1] + 1;\n            }else if(x[i] > 0.5 && x[i] < 0.75){\n                bins[2] = bins[2] + 1;\n            }else if(x[i] > 0.75){\n                bins[3] = bins[3] + 1;\n            }\n        }\n    }\n}",
            "size_t n = x.size();\n\tif(n < 1) {\n\t\tbins = {0, 0, 0, 0};\n\t\treturn;\n\t}\n\tif(n < 4) {\n\t\tbins = {0, 0, 0, 0};\n\t\treturn;\n\t}\n\n\t// initialize to 0\n\tbins = {0, 0, 0, 0};\n\n\t#pragma omp parallel for\n\tfor(size_t i = 0; i < n; ++i) {\n\t\tif(x[i] >= 0 && x[i] < 0.25) {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[0];\n\t\t} else if(x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[1];\n\t\t} else if(x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[2];\n\t\t} else {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[3];\n\t\t}\n\t}\n}",
            "size_t i;\n\tsize_t numThreads = 0;\n\t#pragma omp parallel\n\t{\n\t\tnumThreads = omp_get_num_threads();\n\t}\n\n\t#pragma omp parallel for\n\tfor (i = 0; i < x.size(); i++) {\n\t\tif (x[i] >= 0 && x[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x[i] >= 0.75 && x[i] < 1) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// Your code goes here.\n}",
            "/* YOUR CODE HERE */\n}",
            "// Your code here\n    int size = x.size();\n    int num_threads = omp_get_max_threads();\n    int range = size / num_threads;\n    int *temp = new int[num_threads];\n    int i = 0;\n    int temp_sum = 0;\n    for(i = 0; i < num_threads - 1; ++i) {\n        temp[i] = std::count_if(x.begin() + i * range, x.begin() + (i + 1) * range, [](double x) {\n            return x >= 0.25 && x < 0.5;\n        });\n        temp_sum += temp[i];\n    }\n    temp[num_threads - 1] = std::count_if(x.begin() + (num_threads - 1) * range, x.end(), [](double x) {\n        return x >= 0.25 && x < 0.5;\n    });\n    temp_sum += temp[num_threads - 1];\n    for(i = 0; i < num_threads; ++i)\n        bins[i] = temp[i];\n    delete[] temp;\n}",
            "// Compute the number of threads available\n    int nthreads = omp_get_max_threads();\n\n    // Each thread gets its own copy of the vector\n    std::vector<double> x_thread(x);\n\n    // Partition the data using the number of threads available\n    omp_set_num_threads(nthreads);\n    #pragma omp parallel num_threads(nthreads)\n    {\n        // Determine the start and end index of the range\n        // assigned to the current thread\n        size_t start = omp_get_thread_num() * x.size() / nthreads;\n        size_t end = (omp_get_thread_num() + 1) * x.size() / nthreads;\n\n        // Perform the count\n        for (size_t i = start; i < end; ++i) {\n            if (x_thread[i] >= 0.0 && x_thread[i] < 0.25) {\n                ++bins[0];\n            } else if (x_thread[i] >= 0.25 && x_thread[i] < 0.5) {\n                ++bins[1];\n            } else if (x_thread[i] >= 0.5 && x_thread[i] < 0.75) {\n                ++bins[2];\n            } else {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "std::vector<size_t> bin_sums(4, 0);\n  size_t num_elements = x.size();\n#pragma omp parallel\n  {\n#pragma omp for reduction(+:bin_sums)\n    for(int i=0; i<num_elements; i++){\n      double value = x[i];\n      size_t index = 0;\n      if(value > 0.75){\n        index = 3;\n      }\n      else if(value > 0.5){\n        index = 2;\n      }\n      else if(value > 0.25){\n        index = 1;\n      }\n      else{\n        index = 0;\n      }\n      bin_sums[index] += 1;\n    }\n  }\n  bins = bin_sums;\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if ((x[i] < 0.25) || (x[i] >= 0.75)) {\n      bins[0]++;\n    } else if ((x[i] >= 0.25) && (x[i] < 0.5)) {\n      bins[1]++;\n    } else if ((x[i] >= 0.5) && (x[i] < 0.75)) {\n      bins[2]++;\n    } else if (x[i] >= 0.75) {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Implement this function\n    bins.fill(0);\n\n    int n = x.size();\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        double num = x[i];\n        if (num > 0 && num < 0.25) {\n            bins[0]++;\n        } else if (num >= 0.25 && num < 0.5) {\n            bins[1]++;\n        } else if (num >= 0.5 && num < 0.75) {\n            bins[2]++;\n        } else if (num >= 0.75 && num <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    if ((x[i] > 0.75) && (x[i] < 1)) {\n      bins[3]++;\n    } else if ((x[i] > 0.5) && (x[i] < 0.75)) {\n      bins[2]++;\n    } else if ((x[i] > 0.25) && (x[i] < 0.5)) {\n      bins[1]++;\n    } else {\n      bins[0]++;\n    }\n  }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for (auto const & i: x) {\n        if(i >= 0.0 && i < 0.25) {\n            bins[0] += 1;\n        }\n        else if(i >= 0.25 && i < 0.5) {\n            bins[1] += 1;\n        }\n        else if(i >= 0.5 && i < 0.75) {\n            bins[2] += 1;\n        }\n        else if(i >= 0.75 && i <= 1.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "int numThreads = omp_get_max_threads();\n  omp_set_num_threads(4);\n  \n  size_t start = 0, end = x.size() / numThreads;\n  #pragma omp parallel \n  {\n    int id = omp_get_thread_num();\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n    for (size_t i = start; i < end; i++) {\n      if (x[i] >= 0.0 && x[i] < 0.25) localBins[0]++;\n      else if (x[i] >= 0.25 && x[i] < 0.50) localBins[1]++;\n      else if (x[i] >= 0.50 && x[i] < 0.75) localBins[2]++;\n      else if (x[i] >= 0.75 && x[i] < 1.00) localBins[3]++;\n    }\n    bins[0] += localBins[0];\n    bins[1] += localBins[1];\n    bins[2] += localBins[2];\n    bins[3] += localBins[3];\n  }\n  omp_set_num_threads(numThreads);\n}",
            "// TODO: Fill in this function.\n\n}",
            "size_t N = x.size();\n  // std::cout << N << std::endl;\n  bins.fill(0);\n\n  // #pragma omp parallel for\n  // for(int i=0; i<N; i++){\n  //   if(x[i]>=0.75 && x[i]<=1){\n  //     bins[3]+=1;\n  //   }\n  //   else if(x[i]>=0.5 && x[i]<=0.75){\n  //     bins[2]+=1;\n  //   }\n  //   else if(x[i]>=0.25 && x[i]<=0.5){\n  //     bins[1]+=1;\n  //   }\n  //   else if(x[i]>=0 && x[i]<=0.25){\n  //     bins[0]+=1;\n  //   }\n  // }\n\n  #pragma omp parallel for\n  for(int i=0; i<N; i++){\n    if((x[i] >= 0.75) && (x[i] <= 1)){\n      bins[3]++;\n    }\n    else if((x[i] >= 0.5) && (x[i] <= 0.75)){\n      bins[2]++;\n    }\n    else if((x[i] >= 0.25) && (x[i] <= 0.5)){\n      bins[1]++;\n    }\n    else if((x[i] >= 0) && (x[i] <= 0.25)){\n      bins[0]++;\n    }\n  }\n\n  // #pragma omp parallel for\n  // for(int i=0; i<N; i++){\n  //   if(x[i]>=0.75){\n  //     bins[3]+=1;\n  //   }\n  //   else if(x[i]>=0.5 && x[i]<0.75){\n  //     bins[2]+=1;\n  //   }\n  //   else if(x[i]>=0.25 && x[i]<0.5){\n  //     bins[1]+=1;\n  //   }\n  //   else if(x[i]>=0 && x[i]<0.25){\n  //     bins[0]+=1;\n  //   }\n  // }\n}",
            "bins = {0, 0, 0, 0};\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double value = x[i];\n        if (value >= 0 && value < 0.25) {\n            bins[0]++;\n        } else if (value >= 0.25 && value < 0.5) {\n            bins[1]++;\n        } else if (value >= 0.5 && value < 0.75) {\n            bins[2]++;\n        } else if (value >= 0.75 && value <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double tmp = x[i];\n    if (tmp >= 0 && tmp < 0.25)\n      ++bins[0];\n    else if (tmp >= 0.25 && tmp < 0.5)\n      ++bins[1];\n    else if (tmp >= 0.5 && tmp < 0.75)\n      ++bins[2];\n    else if (tmp >= 0.75 && tmp < 1)\n      ++bins[3];\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tdouble val = x[i];\n\t\tif (val < 0.25) {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[0];\n\t\t} else if (val < 0.5) {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[1];\n\t\t} else if (val < 0.75) {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[2];\n\t\t} else {\n\t\t\t#pragma omp atomic\n\t\t\t++bins[3];\n\t\t}\n\t}\n}",
            "// 1. Implement the function\n  size_t len = x.size();\n  size_t q1_start = (int)(0.25 * len);\n  size_t q2_start = (int)(0.5 * len);\n  size_t q3_start = (int)(0.75 * len);\n  size_t q1_end = q2_start;\n  size_t q2_end = q3_start;\n  size_t q3_end = len;\n  int counter;\n  omp_set_num_threads(4);\n  #pragma omp parallel\n  {\n    int tid = omp_get_thread_num();\n    counter = 0;\n    #pragma omp for\n    for (size_t i = q1_start; i < q1_end; i++)\n    {\n      if ((x[i] >= 0.0) && (x[i] < 0.25))\n      {\n        counter++;\n      }\n    }\n    #pragma omp critical\n    bins[0] += counter;\n    \n    counter = 0;\n    #pragma omp for\n    for (size_t i = q2_start; i < q2_end; i++)\n    {\n      if ((x[i] >= 0.25) && (x[i] < 0.5))\n      {\n        counter++;\n      }\n    }\n    #pragma omp critical\n    bins[1] += counter;\n\n    counter = 0;\n    #pragma omp for\n    for (size_t i = q3_start; i < q3_end; i++)\n    {\n      if ((x[i] >= 0.5) && (x[i] < 0.75))\n      {\n        counter++;\n      }\n    }\n    #pragma omp critical\n    bins[2] += counter;\n\n    counter = 0;\n    #pragma omp for\n    for (size_t i = q3_end; i < len; i++)\n    {\n      if ((x[i] >= 0.75) && (x[i] <= 1.0))\n      {\n        counter++;\n      }\n    }\n    #pragma omp critical\n    bins[3] += counter;\n  }\n}",
            "/* Your code here */\n    std::fill(bins.begin(), bins.end(), 0);\n    size_t n = x.size();\n    #pragma omp parallel\n    {\n        int th = omp_get_thread_num();\n        #pragma omp for schedule(static, 10)\n        for (size_t i = 0; i < n; ++i) {\n            double a = x[i];\n            if (a >= 0.0 && a < 0.25)\n                ++bins[0];\n            else if (a >= 0.25 && a < 0.50)\n                ++bins[1];\n            else if (a >= 0.50 && a < 0.75)\n                ++bins[2];\n            else if (a >= 0.75 && a < 1.00)\n                ++bins[3];\n        }\n    }\n    /* End of your code */\n}",
            "// TODO: Your code here\n  size_t N = x.size();\n  bins = {0,0,0,0};\n  std::sort(x.begin(), x.end());\n  for(int i=0;i<N;i++){\n    double frac = x[i] - std::floor(x[i]);\n    if(frac>=0.0 && frac<0.25) bins[0]++;\n    else if(frac>=0.25 && frac<0.5) bins[1]++;\n    else if(frac>=0.5 && frac<0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "// TODO: Your code here\n\n  size_t length = x.size();\n  size_t size = sizeof(double);\n  size_t nthreads;\n  int tid;\n  double quartiles[4];\n  double *y, *q;\n\n  q = (double *)malloc(size*nthreads);\n\n  #pragma omp parallel default(none) shared(nthreads, y, q)\n  {\n    tid = omp_get_thread_num();\n    nthreads = omp_get_num_threads();\n    y = (double *)malloc(size*nthreads);\n\n    quartiles[0] = 0.0;\n    quartiles[1] = 0.25;\n    quartiles[2] = 0.5;\n    quartiles[3] = 0.75;\n\n    #pragma omp for schedule(static)\n    for (int i=0; i < length; i++) {\n      for (int j = 0; j < nthreads; j++) {\n        y[j] = x[i] - quartiles[j];\n      }\n\n      for (int j = 0; j < nthreads; j++) {\n        q[j] += (y[j] >= 0.0) && (y[j] < 0.25);\n      }\n    }\n    #pragma omp critical\n    {\n      for (int j = 0; j < nthreads; j++) {\n        bins[j] += q[j];\n      }\n    }\n  }\n\n  free(q);\n  free(y);\n}",
            "// write your parallel code here\n    size_t N = x.size();\n\tint num_threads = omp_get_max_threads();\n\tsize_t part = N / num_threads;\n\tint t = omp_get_thread_num();\n\tsize_t start = t * part;\n\tsize_t end = start + part;\n\tif (t == num_threads - 1) end = N;\n    for (size_t i = start; i < end; i++) {\n        double x_i = x[i];\n        if (x_i >= 0 && x_i < 0.25) {\n            bins[0]++;\n        } else if (x_i >= 0.25 && x_i < 0.5) {\n            bins[1]++;\n        } else if (x_i >= 0.5 && x_i < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: implement\n  bins[0]=0;\n  bins[1]=0;\n  bins[2]=0;\n  bins[3]=0;\n  //int len=x.size();\n  //int num_thrd=omp_get_num_procs();\n  //int rem_thrd=len%num_thrd;\n  //int thrd_chunk=len/num_thrd;\n  //double step=1.0/4.0;\n  //double lower=0.0;\n  //double upper=0.0;\n  //double val=0.0;\n  //size_t count=0;\n  //size_t idx=0;\n\n  //#pragma omp parallel default(none) shared(x,bins) private(count,idx,val,lower,upper)\n  //for (int i=0; i<num_thrd; i++){\n  //#pragma omp for\n  //for (int j=0; j<thrd_chunk; j++){\n  //count=0;\n  //idx=i*thrd_chunk+j;\n  //val=x[idx];\n  //for (int k=0; k<4; k++){\n  //upper=lower+step;\n  //if (val>lower && val<upper){\n  //count++;\n  //}\n  //lower=upper;\n  //}\n  //bins[count]+=1;\n  //}\n  //}\n  //#pragma omp for\n  //for (int j=rem_thrd; j<thrd_chunk; j++){\n  //count=0;\n  //idx=j;\n  //val=x[idx];\n  //for (int k=0; k<4; k++){\n  //upper=lower+step;\n  //if (val>lower && val<upper){\n  //count++;\n  //}\n  //lower=upper;\n  //}\n  //bins[count]+=1;\n  //}\n  //bins[0]+=1;\n\n}",
            "bins = std::array<size_t, 4>{0, 0, 0, 0};\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); i++) {\n        double x_i = x[i];\n        if (x_i >= 0 && x_i < 0.25) {\n            bins[0] += 1;\n        } else if (x_i >= 0.25 && x_i < 0.5) {\n            bins[1] += 1;\n        } else if (x_i >= 0.5 && x_i < 0.75) {\n            bins[2] += 1;\n        } else if (x_i >= 0.75 && x_i <= 1) {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n    const size_t N = x.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        double xi = x[i];\n        // Your code goes here\n    }\n}",
            "bins.fill(0);\n    const int N = x.size();\n#pragma omp parallel for\n    for (int i=0; i<N; i++) {\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] >= 0.75 && x[i] < 1.0) {\n            bins[3]++;\n        }\n    }\n}",
            "// Your code here\n}",
            "// TODO: Your code here\n    // 1. compute the size of the vector\n    size_t n = x.size();\n\n    // 2. compute the number of threads that will be used\n    int num_threads = 4;\n    omp_set_num_threads(num_threads);\n\n    // 3. distribute work to each thread\n    double chunk_size = (double) n / (double) num_threads;\n\n    // 4. create a chunk array that will be used to assign a chunk of values to each thread\n    size_t chunk_arr[num_threads];\n\n    for (int i = 0; i < num_threads; i++) {\n        chunk_arr[i] = i * chunk_size;\n    }\n    // 5. create a sum variable for each thread\n    size_t sum[num_threads];\n    // 6. perform the sum\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        for (int i = chunk_arr[tid]; i < chunk_arr[tid] + chunk_size; i++) {\n            sum[tid] += (x[i] >= 0.0 && x[i] <= 0.25) + (x[i] > 0.25 && x[i] <= 0.5) + (x[i] > 0.5 && x[i] <= 0.75) + (x[i] > 0.75 && x[i] <= 1);\n        }\n    }\n\n    // 7. sum the total\n    bins[0] = sum[0];\n    for (int i = 1; i < num_threads; i++) {\n        bins[0] += sum[i];\n    }\n    // 8. set the remaining bins to the total\n    for (int i = 1; i < 4; i++) {\n        bins[i] = bins[0];\n    }\n}",
            "omp_set_num_threads(omp_get_max_threads());\n    const double quartiles[] = {0.25, 0.5, 0.75};\n    const size_t n = x.size();\n    const double step = 1.0 / 4.0;\n    bins.fill(0);\n\n    #pragma omp parallel for schedule(static)\n    for(size_t i = 0; i < n; ++i) {\n        double x_i = x[i];\n        for(int j = 0; j < 4; ++j) {\n            if(x_i >= (j + 1) * step && x_i < j * step) {\n                bins[j] += 1;\n            }\n        }\n    }\n}",
            "int num_threads = 10;\n  #pragma omp parallel num_threads(num_threads)\n  {\n    auto thread_num = omp_get_thread_num();\n    auto thread_bins = bins;\n\n    auto start = x.begin() + thread_num * x.size() / num_threads;\n    auto end = x.begin() + (thread_num + 1) * x.size() / num_threads;\n    for(auto it = start; it!= end; ++it) {\n      double f = *it - floor(*it);\n      if(f < 0.25) {\n        ++thread_bins[0];\n      } else if(f < 0.5) {\n        ++thread_bins[1];\n      } else if(f < 0.75) {\n        ++thread_bins[2];\n      } else {\n        ++thread_bins[3];\n      }\n    }\n\n    #pragma omp critical\n    {\n      for(size_t i = 0; i < thread_bins.size(); ++i) {\n        bins[i] += thread_bins[i];\n      }\n    }\n  }\n}",
            "size_t const n = x.size();\n\tbins = std::array<size_t, 4>{};\n\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < n; ++i) {\n\t\tdouble const q = std::fmod(x[i], 1.0);\n\t\tif (q < 0.25) {\n\t\t\t++bins[0];\n\t\t} else if (q < 0.5) {\n\t\t\t++bins[1];\n\t\t} else if (q < 0.75) {\n\t\t\t++bins[2];\n\t\t} else {\n\t\t\t++bins[3];\n\t\t}\n\t}\n}",
            "#pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n    for (size_t i = 0; i < x.size(); i++) {\n        double fractional = x[i] - std::floor(x[i]);\n        if (fractional < 0.25) {\n            bins[0]++;\n        } else if (fractional < 0.5) {\n            bins[1]++;\n        } else if (fractional < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// bins.fill(0);\n  #pragma omp parallel for schedule(dynamic, 1)\n  for (size_t i = 0; i < x.size(); ++i) {\n    double z = x[i];\n    if (z < 0.25) ++bins[0];\n    else if (z < 0.50) ++bins[1];\n    else if (z < 0.75) ++bins[2];\n    else if (z <= 1.00) ++bins[3];\n  }\n}",
            "// TODO: your code goes here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  const size_t num_elements = x.size();\n#pragma omp parallel for\n  for (size_t i = 0; i < num_elements; ++i) {\n    if (x[i] >= 0 && x[i] <= 0.25) {\n      bins[0]++;\n    } else if (x[i] > 0.25 && x[i] <= 0.5) {\n      bins[1]++;\n    } else if (x[i] > 0.5 && x[i] <= 0.75) {\n      bins[2]++;\n    } else if (x[i] > 0.75 && x[i] <= 1) {\n      bins[3]++;\n    }\n  }\n}",
            "// Initialize bins to zero\n  std::fill(bins.begin(), bins.end(), 0);\n  \n  // Set up the number of threads in OpenMP\n  // Set the default number of threads to the maximum number of threads available.\n  // Use a #pragma to set the number of threads to 1.\n#pragma omp parallel num_threads(1)\n  {\n    // TODO:\n    // Implement the parallel for loop using OpenMP\n    // Use a reduction variable to add the value of x to bins.\n#pragma omp for reduction(+:bins)\n    for (size_t i = 0; i < x.size(); i++) {\n      if (x[i] >= 0 && x[i] < 0.25) {\n        bins[0] += 1;\n      } else if (x[i] >= 0.25 && x[i] < 0.5) {\n        bins[1] += 1;\n      } else if (x[i] >= 0.5 && x[i] < 0.75) {\n        bins[2] += 1;\n      } else if (x[i] >= 0.75 && x[i] < 1) {\n        bins[3] += 1;\n      } else {\n        // Error\n      }\n    }\n  }\n}",
            "// TO DO: your code here\n}",
            "#pragma omp parallel sections\n    {\n        #pragma omp section\n        {\n            size_t i = 0;\n            for (auto it = x.begin(); it!= x.end(); ++it) {\n                if (*it >= 0.0 && *it < 0.25) {\n                    #pragma omp critical\n                    {\n                        bins[0] += 1;\n                    }\n                }\n                ++i;\n            }\n        }\n\n        #pragma omp section\n        {\n            size_t i = 0;\n            for (auto it = x.begin(); it!= x.end(); ++it) {\n                if (*it >= 0.25 && *it < 0.5) {\n                    #pragma omp critical\n                    {\n                        bins[1] += 1;\n                    }\n                }\n                ++i;\n            }\n        }\n\n        #pragma omp section\n        {\n            size_t i = 0;\n            for (auto it = x.begin(); it!= x.end(); ++it) {\n                if (*it >= 0.5 && *it < 0.75) {\n                    #pragma omp critical\n                    {\n                        bins[2] += 1;\n                    }\n                }\n                ++i;\n            }\n        }\n\n        #pragma omp section\n        {\n            size_t i = 0;\n            for (auto it = x.begin(); it!= x.end(); ++it) {\n                if (*it >= 0.75 && *it <= 1.0) {\n                    #pragma omp critical\n                    {\n                        bins[3] += 1;\n                    }\n                }\n                ++i;\n            }\n        }\n    }\n}",
            "size_t n = x.size();\n  std::array<size_t, 4> binsLocal;\n#pragma omp parallel\n  {\n    binsLocal = {0, 0, 0, 0};\n#pragma omp for\n    for (size_t i = 0; i < n; ++i) {\n      double const y = x[i];\n      if (y <= 0.25) {\n        ++binsLocal[0];\n      }\n      else if (y <= 0.5) {\n        ++binsLocal[1];\n      }\n      else if (y <= 0.75) {\n        ++binsLocal[2];\n      }\n      else {\n        ++binsLocal[3];\n      }\n    }\n  }\n  for (int i = 0; i < 4; ++i) {\n    bins[i] = binsLocal[i];\n  }\n}",
            "// your code here\n}",
            "// Your code here!\n}",
            "// TODO\n    size_t length = x.size();\n    int n_threads = omp_get_num_procs();\n    double quartiles[4] = {0, 0.25, 0.5, 0.75};\n    double quartile;\n    size_t n_quartile;\n\n    std::vector<size_t> bins_parallel(4, 0);\n\n    for (int i = 0; i < length; i++) {\n        quartile = x[i];\n        for (int j = 0; j < 4; j++) {\n            if (quartile >= quartiles[j]) {\n                n_quartile = j;\n                bins_parallel[n_quartile] += 1;\n                break;\n            }\n        }\n    }\n\n    bins = bins_parallel;\n}",
            "// Fill the counts vector with zeros\n  bins.fill(0);\n\n  // Compute the quartiles in parallel\n  #pragma omp parallel for\n  for(size_t i = 0; i < x.size(); i++) {\n    double v = x[i];\n    if(v <= 0.25)\n      bins[0] += 1;\n    else if(v <= 0.5)\n      bins[1] += 1;\n    else if(v <= 0.75)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n}",
            "// TODO: compute the number of values in each bin, and put the results in `bins`\n    size_t i,j;\n    for(i=0; i<4; i++){\n        bins[i]=0;\n    }\n    omp_set_num_threads(16);\n#pragma omp parallel for\n    for(i=0; i<x.size(); i++){\n        if(x[i]>=0 && x[i]<=0.25){\n            bins[0]++;\n        }\n        else if(x[i]>=0.25 && x[i]<=0.5){\n            bins[1]++;\n        }\n        else if(x[i]>=0.5 && x[i]<=0.75){\n            bins[2]++;\n        }\n        else{\n            bins[3]++;\n        }\n    }\n}",
            "size_t len = x.size();\n    bins.fill(0);\n\n#pragma omp parallel for\n    for (size_t i = 0; i < len; i++) {\n        double frac = x[i] - floor(x[i]);\n        if (frac < 0.25) {\n            bins[0]++;\n        } else if (frac < 0.5) {\n            bins[1]++;\n        } else if (frac < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "const double binWidth = 1/4.0;\n    const double min = 0.0;\n    const double max = 1.0;\n    \n    int nthreads = 4;\n#pragma omp parallel for\n    for (int t=0; t < nthreads; ++t)\n    {\n        for (size_t i = 0; i < x.size(); ++i)\n        {\n            double val = x[i];\n            if (val >= min && val < max)\n            {\n                int idx = static_cast<int>((val - min) / binWidth);\n                bins[idx] = bins[idx] + 1;\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "#pragma omp parallel\n  {\n    // your code here\n  }\n}",
            "bins.fill(0);\n  #pragma omp parallel for\n  for (auto i=0; i<x.size(); ++i) {\n    auto d = x[i];\n    auto quantile = d/4;\n    if (d >= 3*quantile) {\n      ++bins[3];\n    } else if (d >= 2*quantile) {\n      ++bins[2];\n    } else if (d >= quantile) {\n      ++bins[1];\n    } else {\n      ++bins[0];\n    }\n  }\n}",
            "// TODO\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i) {\n        const double value = x[i];\n        if(value < 0 || value >= 1) {\n            bins[0] += 1;\n        } else if(value < 0.25) {\n            bins[1] += 1;\n        } else if(value < 0.5) {\n            bins[2] += 1;\n        } else if(value < 0.75) {\n            bins[3] += 1;\n        } else {\n            bins[4] += 1;\n        }\n    }\n}",
            "// TODO: write your code here\n}",
            "bins.fill(0);\n    \n    // TODO: implement me\n}",
            "#pragma omp parallel for num_threads(omp_get_max_threads())\n  for (size_t i=0; i<x.size(); i++) {\n    // Your code goes here\n  }\n}",
            "/* TODO: your code goes here */\n  int numThreads = 0;\n  int numThr = omp_get_max_threads();\n  int threadId = 0;\n  int chunk = 0;\n  int min = 0;\n  int max = 0;\n  size_t index = 0;\n  double value = 0;\n\n  omp_set_dynamic(0);\n  omp_set_num_threads(numThr);\n  #pragma omp parallel num_threads(numThr) private(threadId) private(chunk) private(min) private(max) private(index) private(value)\n  {\n\n    threadId = omp_get_thread_num();\n    chunk = x.size() / numThr;\n    min = threadId * chunk;\n    max = (threadId+1) * chunk;\n    for(int i = min; i < max; i++){\n      value = x[i];\n      if(value >= 0 && value < 0.25){\n        bins[0]++;\n      } else if(value >= 0.25 && value < 0.50){\n        bins[1]++;\n      } else if(value >= 0.50 && value < 0.75){\n        bins[2]++;\n      } else if(value >= 0.75 && value <= 1.00){\n        bins[3]++;\n      } else {\n        std::cout << \"ERROR! Value is out of range. Should be in [0, 1]\" << std::endl;\n        exit(1);\n      }\n    }\n  }\n}",
            "// TODO: Your code here\n    // omp_set_num_threads(8);\n    // #pragma omp parallel for\n    // for (int i = 0; i < x.size(); i++) {\n    //     if (x[i] < 0.25) {\n    //         bins[0] += 1;\n    //     } else if (x[i] >= 0.25 && x[i] < 0.5) {\n    //         bins[1] += 1;\n    //     } else if (x[i] >= 0.5 && x[i] < 0.75) {\n    //         bins[2] += 1;\n    //     } else if (x[i] >= 0.75 && x[i] <= 1) {\n    //         bins[3] += 1;\n    //     }\n    // }\n    #pragma omp parallel for reduction(+ : bins[0], bins[1], bins[2], bins[3])\n    for (int i = 0; i < x.size(); i++) {\n        int id = omp_get_thread_num();\n        if (x[i] < 0.25) {\n            bins[0] += 1;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins[1] += 1;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins[2] += 1;\n        } else if (x[i] >= 0.75 && x[i] <= 1) {\n            bins[3] += 1;\n        }\n    }\n}",
            "int nthreads = 4;\n    int chunksize = x.size() / nthreads;\n\n    // TODO: compute your solution here\n}",
            "size_t const n = x.size();\n  size_t const block_size = 40;\n\n  size_t start = 0;\n  size_t end = n;\n  size_t n_blocks = n / block_size;\n\n  #pragma omp parallel\n  {\n    int const n_threads = omp_get_num_threads();\n    int const my_id = omp_get_thread_num();\n\n    for (int i = 0; i < n_blocks; ++i) {\n      size_t const begin_block = start + block_size * i;\n      size_t const end_block = start + block_size * (i+1);\n\n      std::vector<double> block;\n      block.assign(x.begin()+begin_block, x.begin()+end_block);\n\n      size_t block_count[4] = {0};\n      #pragma omp parallel for\n      for (size_t j = 0; j < block.size(); ++j) {\n        double const value = block[j];\n        if (value >= 0 && value < 0.25) {\n          #pragma omp atomic\n          ++block_count[0];\n        }\n        else if (value >= 0.25 && value < 0.5) {\n          #pragma omp atomic\n          ++block_count[1];\n        }\n        else if (value >= 0.5 && value < 0.75) {\n          #pragma omp atomic\n          ++block_count[2];\n        }\n        else {\n          #pragma omp atomic\n          ++block_count[3];\n        }\n      }\n\n      #pragma omp critical\n      {\n        for (size_t j = 0; j < 4; ++j) {\n          #pragma omp atomic\n          bins[j] += block_count[j];\n        }\n      }\n    }\n  }\n\n  if (n % block_size > 0) {\n    size_t const block_size = n % block_size;\n    size_t const start = n - block_size;\n    size_t const end = n;\n\n    std::vector<double> block;\n    block.assign(x.begin()+start, x.begin()+end);\n\n    size_t block_count[4] = {0};\n    for (size_t j = 0; j < block.size(); ++j) {\n      double const value = block[j];\n      if (value >= 0 && value < 0.25) {\n        ++block_count[0];\n      }\n      else if (value >= 0.25 && value < 0.5) {\n        ++block_count[1];\n      }\n      else if (value >= 0.5 && value < 0.75) {\n        ++block_count[2];\n      }\n      else {\n        ++block_count[3];\n      }\n    }\n\n    for (size_t j = 0; j < 4; ++j) {\n      bins[j] += block_count[j];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        } else if (x[i] < 0.5) {\n            bins[1]++;\n        } else if (x[i] < 0.75) {\n            bins[2]++;\n        } else if (x[i] < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "size_t const len = x.size();\n\tsize_t i = 0;\n\t#pragma omp parallel for\n\tfor (i = 0; i < len; i++) {\n\t\tdouble val = x[i];\n\t\tif (val >= 0.0 && val <= 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (val > 0.25 && val <= 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if (val > 0.5 && val <= 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if (val > 0.75 && val <= 1.0) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO: Implement this function.\n}",
            "// *** INSERT YOUR CODE HERE ***\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] < 0.25)\n         ++bins[0];\n      else if (x[i] < 0.5)\n         ++bins[1];\n      else if (x[i] < 0.75)\n         ++bins[2];\n      else if (x[i] < 1.0)\n         ++bins[3];\n   }\n}",
            "// YOUR CODE HERE\n  size_t n = x.size();\n  bins.fill(0);\n  if(n < 4) {\n    return;\n  }\n  std::vector<size_t> num_threads(1);\n#pragma omp parallel\n  {\n#pragma omp single\n    {\n#pragma omp taskloop\n      for(size_t i = 0; i < n; i++) {\n        if(x[i] < 0.25) {\n          bins[0]++;\n        } else if(x[i] < 0.5) {\n          bins[1]++;\n        } else if(x[i] < 0.75) {\n          bins[2]++;\n        } else {\n          bins[3]++;\n        }\n      }\n    }\n#pragma omp atomic\n    num_threads[0]++;\n  }\n  for(size_t i = 1; i < num_threads[0]; i++) {\n    std::array<size_t, 4> temp;\n#pragma omp taskwait\n#pragma omp task firstprivate(i)\n    {\n#pragma omp atomic read\n      temp[0] = bins[0];\n#pragma omp atomic\n      bins[0] += temp[i];\n    }\n#pragma omp task firstprivate(i)\n    {\n#pragma omp atomic read\n      temp[1] = bins[1];\n#pragma omp atomic\n      bins[1] += temp[i];\n    }\n#pragma omp task firstprivate(i)\n    {\n#pragma omp atomic read\n      temp[2] = bins[2];\n#pragma omp atomic\n      bins[2] += temp[i];\n    }\n#pragma omp task firstprivate(i)\n    {\n#pragma omp atomic read\n      temp[3] = bins[3];\n#pragma omp atomic\n      bins[3] += temp[i];\n    }\n  }\n}",
            "// TODO: your code goes here\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double a = std::abs(x[i] - std::floor(x[i]));\n    if (a < 0.25) {\n      bins[0]++;\n    } else if (a < 0.50) {\n      bins[1]++;\n    } else if (a < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "size_t n = x.size();\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        double bin = x[i] / 0.25;\n        size_t bin_idx = (size_t)bin;\n        if (bin_idx == 3) {\n            bins[bin_idx]++;\n        } else {\n            bins[bin_idx]++;\n            bins[bin_idx + 1]++;\n        }\n    }\n}",
            "bins.fill(0);\n\n  // Enter your code here\n\n}",
            "// TODO: count the number of elements in x in each of the 4 quartiles.\n    // Hint: Use a for-loop and 4 parallel regions to parallelize the operation.\n    // Hint: Use atomic operations to update bins.\n}",
            "bins.fill(0);\n  int count = 0;\n# pragma omp parallel for reduction(+:count)\n  for (size_t i = 0; i < x.size(); ++i) {\n    double x_i = x[i];\n    double fraction = x_i - std::floor(x_i);\n    if (0.0 <= fraction && fraction < 0.25) ++bins[0];\n    else if (0.25 <= fraction && fraction < 0.5) ++bins[1];\n    else if (0.5 <= fraction && fraction < 0.75) ++bins[2];\n    else if (0.75 <= fraction && fraction <= 1.0) ++bins[3];\n    ++count;\n  }\n  std::cout << \"Using OpenMP, finished \" << count << \" iterations.\\n\";\n}",
            "bins.fill(0);\n#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (x[i] < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if (x[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n  for(auto i=0; i < x.size(); ++i) {\n    if(x[i] < 0.25) {\n      ++bins[0];\n    } else if(x[i] < 0.5) {\n      ++bins[1];\n    } else if(x[i] < 0.75) {\n      ++bins[2];\n    } else if(x[i] < 1) {\n      ++bins[3];\n    }\n  }\n}",
            "size_t const n = x.size();\n\n  // YOUR CODE HERE\n}",
            "// TODO\n    bins = {};\n}",
            "// Your code here\n}",
            "// Your code here\n    #pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        double value = x[i];\n        if (value <= 0.25) {\n            bins[0]++;\n        } else if (value > 0.25 && value <= 0.5) {\n            bins[1]++;\n        } else if (value > 0.5 && value <= 0.75) {\n            bins[2]++;\n        } else if (value > 0.75 && value <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Implement this function.\n}",
            "bins.fill(0);\n    #pragma omp parallel for reduction(+ : bins)\n    for (int i = 0; i < x.size(); i++) {\n        double value = x[i];\n        if (value >= 0 && value < 0.25) {\n            bins[0]++;\n        } else if (value >= 0.25 && value < 0.5) {\n            bins[1]++;\n        } else if (value >= 0.5 && value < 0.75) {\n            bins[2]++;\n        } else if (value >= 0.75 && value <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: implement this function\n}",
            "size_t n = x.size();\n\n#pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        auto f = x[i] - std::floor(x[i]);\n        if (f < 0.25) {\n            ++bins[0];\n        } else if (f < 0.5) {\n            ++bins[1];\n        } else if (f < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "bins.fill(0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    auto v = x[i];\n    if (v < 0) {\n      v = -v;\n    }\n    if (v < 0.25) {\n      bins[0]++;\n    } else if (v < 0.5) {\n      bins[1]++;\n    } else if (v < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "size_t n = x.size();\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    double frac = x[i] - floor(x[i]);\n    if (frac > 0.25 && frac <= 0.5) {\n      bins[0] = bins[0] + 1;\n    } else if (frac > 0.5 && frac <= 0.75) {\n      bins[1] = bins[1] + 1;\n    } else if (frac > 0.75 && frac <= 1.0) {\n      bins[2] = bins[2] + 1;\n    } else {\n      bins[3] = bins[3] + 1;\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < x.size(); ++i) {\n      double x_i = x[i];\n      double q;\n      if (x_i < 0.25)\n         q = 0;\n      else if (x_i < 0.5)\n         q = 1;\n      else if (x_i < 0.75)\n         q = 2;\n      else\n         q = 3;\n      #pragma omp atomic\n      bins[q] += 1;\n   }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (x[i] < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (x[i] < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
            "// compute the number of threads available\n  int num_threads = omp_get_max_threads();\n\n  // initialize the thread private variables\n  bins.fill(0);\n  size_t const n = x.size();\n  size_t const step = n / num_threads;\n\n  // #pragma omp parallel for\n  #pragma omp parallel default(none) shared(x, bins, step)\n  {\n    int tid = omp_get_thread_num();\n    size_t const start = tid * step;\n    size_t const end = std::min((tid + 1) * step, n);\n    for (size_t i = start; i < end; ++i) {\n      double const y = x[i];\n      double const quartile = y / 4;\n      size_t const index = quartile * 4;\n      bins[index] += 1;\n    }\n  }\n}",
            "int n = x.size();\n\tint N = omp_get_num_procs();\n\tint M = (int)std::ceil((double)n / N);\n\tint id = omp_get_thread_num();\n\tint start = id * M;\n\tint end = std::min((id + 1) * M, n);\n\tbins = { 0, 0, 0, 0 };\n\tint i;\n\t#pragma omp for\n\tfor(i = start; i < end; i++) {\n\t\tdouble xx = x[i];\n\t\tif(xx > 0.75) {\n\t\t\tbins[3]++;\n\t\t} else if(xx > 0.5) {\n\t\t\tbins[2]++;\n\t\t} else if(xx > 0.25) {\n\t\t\tbins[1]++;\n\t\t} else {\n\t\t\tbins[0]++;\n\t\t}\n\t}\n\t#pragma omp barrier\n\t#pragma omp single\n\t{\n\t\tfor(int i = 0; i < N; i++) {\n\t\t\tint start = i * M;\n\t\t\tint end = std::min((i + 1) * M, n);\n\t\t\tfor(int j = 0; j < 4; j++) {\n\t\t\t\tbins[j] += omp_get_num_threads() * (end - start);\n\t\t\t}\n\t\t}\n\t}\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double x_i = x[i];\n    if (x_i <= 0.25) {\n      bins[0] += 1;\n    }\n    else if (x_i <= 0.5) {\n      bins[1] += 1;\n    }\n    else if (x_i <= 0.75) {\n      bins[2] += 1;\n    }\n    else {\n      bins[3] += 1;\n    }\n  }\n}",
            "// Write your code here.\n\n  size_t n = x.size();\n  size_t blocksize = (n+1)/2;\n\n  size_t i,j,k;\n  int tmp;\n  int sum;\n\n  std::vector<int> sum_of_block;\n  std::vector<int> index;\n\n  sum_of_block.resize(blocksize, 0);\n  index.resize(blocksize, 0);\n\n  omp_set_num_threads(4);\n  omp_set_dynamic(0);\n\n#pragma omp parallel private(i,j,k,sum,tmp) firstprivate(n)\n  {\n    int thread_id = omp_get_thread_num();\n    int num_thread = omp_get_num_threads();\n\n#pragma omp for schedule(static)\n    for(i=0; i<n; i++){\n      int block_id = i/blocksize;\n      if(i%blocksize == 0){\n        sum = 0;\n      }\n      sum += (int)x[i];\n      if(i%blocksize == blocksize-1){\n        sum_of_block[block_id] = sum;\n        index[block_id] = i-blocksize+1;\n      }\n    }\n  }\n\n  for(i=0;i<blocksize;i++){\n    tmp = index[i];\n    sum = sum_of_block[i];\n    j = (i+1)*blocksize-1;\n    if(tmp <= j){\n      if(tmp <= j && tmp >= j-blocksize+1){\n        bins[0] += sum;\n      }else if(tmp >= j-blocksize+1 && tmp < j-blocksize+1+blocksize/2){\n        bins[1] += sum;\n      }else if(tmp >= j-blocksize+1+blocksize/2 && tmp < j-blocksize+1+blocksize){\n        bins[2] += sum;\n      }else if(tmp >= j-blocksize+1+blocksize && tmp <= j){\n        bins[3] += sum;\n      }\n    }\n  }\n\n}",
            "// TODO\n}",
            "#pragma omp parallel\n\t{\n\t\tsize_t n = x.size();\n\t\tsize_t count1, count2, count3, count4;\n\t\tint threads_number = omp_get_num_threads();\n\t\tint thread_id = omp_get_thread_num();\n\n\t\t#pragma omp for\n\t\tfor (int i = 0; i < n; ++i) {\n\t\t\tif (x[i] >= 0 && x[i] < 0.25) {\n\t\t\t\tcount1++;\n\t\t\t}\n\t\t\telse if (x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\t\tcount2++;\n\t\t\t}\n\t\t\telse if (x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\t\tcount3++;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tcount4++;\n\t\t\t}\n\t\t}\n\n\t\t// reduction\n\t\t#pragma omp critical\n\t\t{\n\t\t\tbins[0] += count1;\n\t\t\tbins[1] += count2;\n\t\t\tbins[2] += count3;\n\t\t\tbins[3] += count4;\n\t\t}\n\t}\n}",
            "// TODO: implement this function using OpenMP\n}",
            "#pragma omp parallel\n    {\n        size_t nthreads = omp_get_num_threads();\n        size_t tid = omp_get_thread_num();\n        size_t n = x.size();\n        size_t step = n/nthreads;\n        size_t start = tid*step;\n        size_t end = start+step;\n        if (tid == nthreads-1) {\n            end = n;\n        }\n\n        std::array<size_t, 4> counts;\n        counts.fill(0);\n        #pragma omp for\n        for (size_t i = start; i < end; i++) {\n            double val = x[i];\n            if (val >= 0 && val < 0.25) {\n                counts[0] += 1;\n            } else if (val >= 0.25 && val < 0.5) {\n                counts[1] += 1;\n            } else if (val >= 0.5 && val < 0.75) {\n                counts[2] += 1;\n            } else if (val >= 0.75 && val < 1) {\n                counts[3] += 1;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < counts.size(); i++) {\n                bins[i] += counts[i];\n            }\n        }\n    }\n}",
            "#pragma omp parallel sections num_threads(4)\n    {\n        #pragma omp section\n        {\n            size_t count = 0;\n            for (size_t i = 0; i < x.size(); i++) {\n                if (x[i] < 0.25) {\n                    count++;\n                }\n            }\n            bins[0] = count;\n        }\n        #pragma omp section\n        {\n            size_t count = 0;\n            for (size_t i = 0; i < x.size(); i++) {\n                if (x[i] >= 0.25 && x[i] < 0.5) {\n                    count++;\n                }\n            }\n            bins[1] = count;\n        }\n        #pragma omp section\n        {\n            size_t count = 0;\n            for (size_t i = 0; i < x.size(); i++) {\n                if (x[i] >= 0.5 && x[i] < 0.75) {\n                    count++;\n                }\n            }\n            bins[2] = count;\n        }\n        #pragma omp section\n        {\n            size_t count = 0;\n            for (size_t i = 0; i < x.size(); i++) {\n                if (x[i] >= 0.75) {\n                    count++;\n                }\n            }\n            bins[3] = count;\n        }\n    }\n}",
            "const size_t numThreads = 8;\n\tconst size_t chunkSize = x.size() / numThreads;\n\tstd::vector<size_t> localBins(bins.size());\n\n#pragma omp parallel num_threads(numThreads)\n\t{\n\t\tsize_t threadId = omp_get_thread_num();\n\n\t\tsize_t from = threadId * chunkSize;\n\t\tsize_t to = (threadId + 1) * chunkSize;\n\n\t\tfor (size_t i = from; i < to; i++) {\n\t\t\tdouble fractionalPart = x[i] - std::floor(x[i]);\n\t\t\tif (fractionalPart < 0.25)\n\t\t\t\tlocalBins[0]++;\n\t\t\telse if (fractionalPart < 0.5)\n\t\t\t\tlocalBins[1]++;\n\t\t\telse if (fractionalPart < 0.75)\n\t\t\t\tlocalBins[2]++;\n\t\t\telse\n\t\t\t\tlocalBins[3]++;\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < localBins.size(); i++) {\n\t\tbins[i] += localBins[i];\n\t}\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for schedule(dynamic)\n  for (auto i = 0; i < x.size(); ++i) {\n    double p = x[i];\n    if (p >= 0 && p < 0.25) {\n      #pragma omp atomic\n      ++bins[0];\n    } else if (p >= 0.25 && p < 0.5) {\n      #pragma omp atomic\n      ++bins[1];\n    } else if (p >= 0.5 && p < 0.75) {\n      #pragma omp atomic\n      ++bins[2];\n    } else if (p >= 0.75 && p <= 1) {\n      #pragma omp atomic\n      ++bins[3];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) {\n            bins[0]++;\n        }\n        else if (x[i] < 0.5) {\n            bins[1]++;\n        }\n        else if (x[i] < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double val = x[i];\n    if (val < 0.25) {\n      ++bins[0];\n    } else if (val < 0.5) {\n      ++bins[1];\n    } else if (val < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "bins = { 0, 0, 0, 0 };\n#pragma omp parallel\n    {\n#pragma omp single\n        {\n            size_t n = x.size();\n            std::vector<int> tempBins(4);\n            size_t i = 0;\n            int end = n / 4;\n#pragma omp task\n            {\n                for (; i < end; ++i) {\n                    double value = x[i];\n                    double frac = value - std::floor(value);\n                    int bin = std::floor(frac * 4);\n                    tempBins[bin]++;\n                }\n            }\n#pragma omp task\n            {\n                end = n / 2;\n                for (; i < end; ++i) {\n                    double value = x[i];\n                    double frac = value - std::floor(value);\n                    int bin = std::floor(frac * 4) + 2;\n                    tempBins[bin]++;\n                }\n            }\n#pragma omp task\n            {\n                end = n * 3 / 4;\n                for (; i < end; ++i) {\n                    double value = x[i];\n                    double frac = value - std::floor(value);\n                    int bin = std::floor(frac * 4) + 1;\n                    tempBins[bin]++;\n                }\n            }\n#pragma omp task\n            {\n                end = n;\n                for (; i < end; ++i) {\n                    double value = x[i];\n                    double frac = value - std::floor(value);\n                    int bin = std::floor(frac * 4) + 3;\n                    tempBins[bin]++;\n                }\n            }\n#pragma omp taskwait\n\n            for (int i = 0; i < 4; i++) {\n#pragma omp atomic\n                bins[i] += tempBins[i];\n            }\n        }\n    }\n}",
            "// TODO: implement\n    int nthreads = omp_get_max_threads();\n    bins.fill(nthreads);\n    omp_set_num_threads(nthreads);\n    #pragma omp parallel for\n    for(int i=0; i<x.size(); ++i){\n        int bin = (int)(x[i]*4);\n        ++bins[bin];\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   size_t N = x.size();\n\n#pragma omp parallel for\n   for (size_t i = 0; i < N; i++) {\n      double val = x[i];\n      if (val >= 0.0 && val < 0.25) {\n         bins[0]++;\n      } else if (val >= 0.25 && val < 0.5) {\n         bins[1]++;\n      } else if (val >= 0.5 && val < 0.75) {\n         bins[2]++;\n      } else if (val >= 0.75 && val <= 1.0) {\n         bins[3]++;\n      }\n   }\n}",
            "size_t n = x.size();\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n#pragma omp parallel for\n    for (size_t i = 0; i < n; ++i) {\n        if (x[i] >= 0 && x[i] < 0.25) {\n            ++bins[0];\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            ++bins[1];\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            ++bins[2];\n        } else if (x[i] >= 0.75 && x[i] <= 1) {\n            ++bins[3];\n        }\n    }\n}",
            "bins.fill(0);\n    // your code here\n    //...\n}",
            "size_t size = x.size();\n    double *data = new double[size];\n\n    for (int i = 0; i < size; i++) {\n        data[i] = (double)x[i];\n    }\n\n    #pragma omp parallel\n    {\n        int threadNum = omp_get_thread_num();\n        int nThreads = omp_get_num_threads();\n\n        int step = size / nThreads;\n        int start = threadNum * step;\n        int end = (threadNum == nThreads - 1)? size : (threadNum + 1) * step;\n        int threadSize = end - start;\n        int threadBins[4];\n\n        double *threadData = new double[threadSize];\n\n        for (int i = 0; i < threadSize; i++) {\n            threadData[i] = data[start + i];\n        }\n\n        delete [] data;\n        data = threadData;\n\n        for (int i = 0; i < 4; i++) {\n            threadBins[i] = 0;\n        }\n\n        for (int i = 0; i < threadSize; i++) {\n            double fraction = data[i] - floor(data[i]);\n\n            if (fraction > 0.75) {\n                threadBins[3]++;\n            } else if (fraction > 0.5) {\n                threadBins[2]++;\n            } else if (fraction > 0.25) {\n                threadBins[1]++;\n            } else {\n                threadBins[0]++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            for (int i = 0; i < 4; i++) {\n                bins[i] += threadBins[i];\n            }\n        }\n    }\n}",
            "// TO BE IMPLEMENTED\n}",
            "#pragma omp parallel\n  {\n    size_t numThreads = omp_get_num_threads();\n    size_t threadID = omp_get_thread_num();\n    size_t n = x.size();\n    size_t chunkSize = n / numThreads;\n    size_t start = threadID * chunkSize;\n    size_t end = (threadID + 1 == numThreads)? n : (threadID + 1) * chunkSize;\n    std::vector<size_t> threadBins(4);\n\n    size_t numBins = 4;\n    double binWidth = 1.0 / numBins;\n    for (size_t i = start; i < end; i++) {\n      double fraction = x[i] / binWidth;\n      if (fraction < 0.25) {\n        threadBins[0]++;\n      }\n      else if (fraction < 0.5) {\n        threadBins[1]++;\n      }\n      else if (fraction < 0.75) {\n        threadBins[2]++;\n      }\n      else {\n        threadBins[3]++;\n      }\n    }\n\n    for (size_t i = 0; i < numBins; i++) {\n      bins[i] += threadBins[i];\n    }\n  }\n}",
            "// YOUR CODE HERE\n    // #pragma omp parallel for schedule(static, 4)\n}",
            "int chunk_size = x.size() / omp_get_max_threads();\n  #pragma omp parallel for\n  for (int i = 0; i < omp_get_max_threads(); i++) {\n    int chunk_start = i * chunk_size;\n    int chunk_end = std::min((i + 1) * chunk_size, (int) x.size());\n    // your code here\n  }\n}",
            "#pragma omp parallel default(shared)\n  {\n    #pragma omp for\n    for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] < 0.25)\n        ++bins[0];\n      else if (x[i] < 0.5)\n        ++bins[1];\n      else if (x[i] < 0.75)\n        ++bins[2];\n      else\n        ++bins[3];\n    }\n  }\n}",
            "// TODO: Count the number of doubles in the vector x that have a fractional part\n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    // Use OpenMP to compute in parallel.\n\n    // initialize bins\n    for (auto &bin : bins) {\n        bin = 0;\n    }\n\n    int size = x.size();\n    // get min and max\n    double min = x[0];\n    double max = x[0];\n    for (int i = 1; i < size; i++) {\n        if (x[i] < min) {\n            min = x[i];\n        }\n        if (x[i] > max) {\n            max = x[i];\n        }\n    }\n\n    // split range into 4 parts and count numbers in each part\n    // e.g. [min, 0.25)\n    #pragma omp parallel\n    {\n        double lower = min;\n        double upper = min + (max - min) / 4.0;\n\n        int counter = 0;\n        #pragma omp for\n        for (int i = 0; i < size; i++) {\n            if (x[i] > lower && x[i] <= upper) {\n                counter++;\n            }\n        }\n        bins[0] = counter;\n    }\n\n    // e.g. [0.25, 0.5)\n    #pragma omp parallel\n    {\n        double lower = min + (max - min) / 4.0;\n        double upper = min + (max - min) / 2.0;\n\n        int counter = 0;\n        #pragma omp for\n        for (int i = 0; i < size; i++) {\n            if (x[i] > lower && x[i] <= upper) {\n                counter++;\n            }\n        }\n        bins[1] = counter;\n    }\n\n    // e.g. [0.5, 0.75)\n    #pragma omp parallel\n    {\n        double lower = min + (max - min) / 2.0;\n        double upper = min + (max - min) / 4.0 * 3.0;\n\n        int counter = 0;\n        #pragma omp for\n        for (int i = 0; i < size; i++) {\n            if (x[i] > lower && x[i] <= upper) {\n                counter++;\n            }\n        }\n        bins[2] = counter;\n    }\n\n    // e.g. [0.75, max]\n    #pragma omp parallel\n    {\n        double lower = min + (max - min) / 4.0 * 3.0;\n\n        int counter = 0;\n        #pragma omp for\n        for (int i = 0; i < size; i++) {\n            if (x[i] > lower) {\n                counter++;\n            }\n        }\n        bins[3] = counter;\n    }\n}",
            "#pragma omp parallel\n\t{\n\t\t#pragma omp for schedule(static, 4)\n\t\tfor (size_t i = 0; i < x.size(); i++) {\n\t\t\tif (x[i] < 0.25) {\n\t\t\t\tbins[0]++;\n\t\t\t} else if (x[i] < 0.5) {\n\t\t\t\tbins[1]++;\n\t\t\t} else if (x[i] < 0.75) {\n\t\t\t\tbins[2]++;\n\t\t\t} else {\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// Add your code here\n}",
            "omp_set_num_threads(4);\n  size_t N = x.size();\n  bins.fill(0);\n#pragma omp parallel for reduction(+:bins)\n  for (size_t i = 0; i < N; ++i) {\n    double x_i = x[i];\n    if (x_i >= 0 && x_i < 0.25) {\n      bins[0] += 1;\n    } else if (x_i >= 0.25 && x_i < 0.5) {\n      bins[1] += 1;\n    } else if (x_i >= 0.5 && x_i < 0.75) {\n      bins[2] += 1;\n    } else if (x_i >= 0.75 && x_i < 1) {\n      bins[3] += 1;\n    } else {\n      assert(false);\n    }\n  }\n}",
            "}",
            "// TODO: replace this with your implementation\n\tbins.fill(0);\n\tdouble sum = 0;\n\tfor (double i : x) {\n\t\tsum += i;\n\t}\n\tint threads = omp_get_max_threads();\n\tfor (int i = 0; i < threads; i++) {\n\t\tint id = i + 1;\n\t\tdouble start = (double)id / (double)threads;\n\t\tdouble end = (double)(id + 1) / (double)threads;\n\t\tdouble min = sum / (double)threads * start;\n\t\tdouble max = sum / (double)threads * end;\n\t\tfor (double j : x) {\n\t\t\tif (j > min && j < max)\n\t\t\t\tbins[id - 1] += 1;\n\t\t}\n\t}\n}",
            "size_t n = x.size();\n  size_t threshold = 0.25 * n;\n  size_t first_half = n / 2;\n  size_t second_half = (n + 1) / 2;\n\n  bins.fill(0);\n#pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n  for (size_t i = 0; i < n; ++i) {\n    if (i < threshold)\n      ++bins[0];\n    else if (i < first_half)\n      ++bins[1];\n    else if (i < second_half)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "omp_set_num_threads(2);\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double value = x[i];\n        if (value < 0.25) {\n            bins[0]++;\n        }\n        else if (value < 0.50) {\n            bins[1]++;\n        }\n        else if (value < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: replace this comment with your code\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  #pragma omp parallel for num_threads(4)\n  for (int i = 0; i < x.size(); i++) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      #pragma omp atomic\n      bins[0]++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      #pragma omp atomic\n      bins[1]++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      #pragma omp atomic\n      bins[2]++;\n    } else if (x[i] >= 0.75 && x[i] <= 1) {\n      #pragma omp atomic\n      bins[3]++;\n    }\n  }\n  return;\n}",
            "// TODO: Implement this function\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); ++i) {\n    double x_i = x[i];\n    if (x_i >= 0.25 && x_i < 0.5) {\n      bins[0]++;\n    }\n    else if (x_i >= 0.5 && x_i < 0.75) {\n      bins[1]++;\n    }\n    else if (x_i >= 0.75 && x_i <= 1) {\n      bins[2]++;\n    }\n    else if (x_i >= 0 && x_i < 0.25) {\n      bins[3]++;\n    }\n  }\n}",
            "//...\n}",
            "// TODO: implement\n  size_t num_threads = 0;\n  #pragma omp parallel\n  {\n    num_threads = omp_get_num_threads();\n  }\n  omp_set_num_threads(4);\n  #pragma omp parallel for\n  for (int i = 0; i < x.size(); i++) {\n    int bin = 0;\n    if (x[i] >= 0 && x[i] <= 0.25) bin = 0;\n    else if (x[i] > 0.25 && x[i] <= 0.5) bin = 1;\n    else if (x[i] > 0.5 && x[i] <= 0.75) bin = 2;\n    else bin = 3;\n\n    #pragma omp critical\n    {\n      bins[bin] += 1;\n    }\n  }\n  omp_set_num_threads(num_threads);\n}",
            "// TODO: Implement this function.\n}",
            "size_t size = x.size();\n    bins.fill(0);\n    double bin_size = size / 4.0;\n\n    for (double val : x) {\n        int bin_idx = std::min(std::max(0, (int)std::round((val - 0.0) / bin_size)), 3);\n        bins[bin_idx]++;\n    }\n}",
            "bins.fill(0);\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < x.size(); i++) {\n        if (x[i] < 0.25) {\n            #pragma omp atomic\n            bins[0]++;\n        }\n        else if (x[i] < 0.5) {\n            #pragma omp atomic\n            bins[1]++;\n        }\n        else if (x[i] < 0.75) {\n            #pragma omp atomic\n            bins[2]++;\n        }\n        else {\n            #pragma omp atomic\n            bins[3]++;\n        }\n    }\n}",
            "bins = {};\n\n  size_t N = x.size();\n  size_t q1 = N / 4;\n  size_t q2 = N / 2;\n  size_t q3 = 3 * N / 4;\n\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < N; i++) {\n    double xi = x[i];\n    if (xi < q1 * 0.25) {\n      bins[0]++;\n    } else if (xi < q2 * 0.25) {\n      bins[1]++;\n    } else if (xi < q3 * 0.25) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "bins = {0, 0, 0, 0};\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tif (x[i] < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (x[i] < 0.50) {\n\t\t\tbins[1]++;\n\t\t} else if (x[i] < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n    #pragma omp parallel for reduction(+: bins[0], bins[1], bins[2], bins[3])\n    for (int i = 0; i < x.size(); i++) {\n        double a = x[i] * 4;\n        double b = a - 2;\n        double c = a - 1;\n        double d = a - 0.5;\n        if (a - b < 0.25 && b - c < 0.25 && c - d < 0.25) {\n            bins[0]++;\n        } else if (a - c < 0.25 && c - d < 0.25 && d - b < 0.25) {\n            bins[1]++;\n        } else if (a - d < 0.25 && d - c < 0.25 && c - b < 0.25) {\n            bins[2]++;\n        } else if (a - b < 0.25 && b - c < 0.25 && c - d < 0.25 && d - a < 0.25) {\n            bins[3]++;\n        }\n    }\n}",
            "const size_t N = x.size();\n  std::array<size_t, 4> tmpBins = {0, 0, 0, 0};\n\n  double tmpFraction = 0;\n  int tmpIndex = 0;\n  //std::cout << \"hello\" << std::endl;\n  #pragma omp parallel for reduction(+: tmpBins[0], tmpBins[1], tmpBins[2], tmpBins[3])\n  for (int i = 0; i < N; i++) {\n    if (x[i] < 0 || x[i] > 10) {\n      tmpBins[3] += 1;\n    } else {\n      tmpFraction = x[i] - floor(x[i]);\n      tmpIndex = floor(tmpFraction * 4);\n      tmpBins[tmpIndex] += 1;\n    }\n  }\n\n  for (int i = 0; i < 4; i++) {\n    bins[i] = tmpBins[i];\n  }\n}",
            "// Initialize bins\n  for (auto &b : bins)\n    b = 0;\n\n#pragma omp parallel\n  {\n#pragma omp for schedule(static) reduction(+:bins[:])\n    for (int i = 0; i < x.size(); ++i) {\n      double const &xi = x[i];\n\n      // 0.25 <= xi < 0.5\n      if (0.25 <= xi && xi < 0.5) {\n        // Thread-safe, see http://stackoverflow.com/a/2122752/854133\n        #pragma omp atomic\n        bins[0] += 1;\n      }\n      // 0.5 <= xi < 0.75\n      else if (0.5 <= xi && xi < 0.75) {\n        #pragma omp atomic\n        bins[1] += 1;\n      }\n      // 0.75 <= xi < 1\n      else if (0.75 <= xi && xi < 1) {\n        #pragma omp atomic\n        bins[2] += 1;\n      }\n      // 0 <= xi < 0.25\n      else if (0 <= xi && xi < 0.25) {\n        #pragma omp atomic\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            __sync_fetch_and_add(&bins[0], 1);\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            __sync_fetch_and_add(&bins[1], 1);\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            __sync_fetch_and_add(&bins[2], 1);\n        } else if (x[i] >= 0.75 && x[i] < 1.0) {\n            __sync_fetch_and_add(&bins[3], 1);\n        }\n    }\n}",
            "// YOUR CODE HERE\n    // The following code gives you a starting point. You should add parallelism.\n    // We use a partitioned parallelism for counting the bins.\n    \n    const size_t n = x.size();\n\n    const double q0 = 0;\n    const double q1 = 0.25;\n    const double q2 = 0.5;\n    const double q3 = 0.75;\n\n    double threshold0 = q0;\n    double threshold1 = q1;\n    double threshold2 = q2;\n    double threshold3 = q3;\n\n    int i0 = 0;\n    int i1 = 0;\n    int i2 = 0;\n    int i3 = 0;\n\n    int j0 = 0;\n    int j1 = 0;\n    int j2 = 0;\n    int j3 = 0;\n\n    // Partitioning x into chunks of approximately equal size.\n    // We use a balanced partitioning, which means we partition the chunks of approximately equal size.\n    // This is a partitioning that is optimal for the OpenMP implementation.\n\n    // You can also use a non-balanced partitioning. For instance, one can partition by size,\n    // and use the same chunk size for all the partitions.\n    // This is a partitioning that is not optimal for the OpenMP implementation,\n    // because the overhead for parallelism in OpenMP is not well balanced.\n\n    const size_t n0 = n / 4;\n    const size_t n1 = n / 4;\n    const size_t n2 = n / 2;\n    const size_t n3 = n / 4;\n\n    auto a0 = std::vector<double>(x.begin(), x.begin() + n0);\n    auto a1 = std::vector<double>(x.begin() + n0, x.begin() + n0 + n1);\n    auto a2 = std::vector<double>(x.begin() + n0 + n1, x.begin() + n0 + n1 + n2);\n    auto a3 = std::vector<double>(x.begin() + n0 + n1 + n2, x.begin() + n0 + n1 + n2 + n3);\n\n    int thread_count = omp_get_max_threads();\n\n    std::vector<int> i0_vec(thread_count);\n    std::vector<int> i1_vec(thread_count);\n    std::vector<int> i2_vec(thread_count);\n    std::vector<int> i3_vec(thread_count);\n\n    std::vector<int> j0_vec(thread_count);\n    std::vector<int> j1_vec(thread_count);\n    std::vector<int> j2_vec(thread_count);\n    std::vector<int> j3_vec(thread_count);\n\n    int j;\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int thread_count = omp_get_num_threads();\n\n        #pragma omp for\n        for (j = 0; j < thread_count; j++) {\n            if (j < thread_count - 1) {\n                i0_vec[j] = 0;\n                i1_vec[j] = 0;\n                i2_vec[j] = 0;\n                i3_vec[j] = 0;\n                j0_vec[j] = 0;\n                j1_vec[j] = 0;\n                j2_vec[j] = 0;\n                j3_vec[j] = 0;\n            } else {\n                i0_vec[j] = 0;\n                i1_vec[j] = 0;\n                i2_vec[j] = 0;\n                i3_vec[j] = 0;\n                j0_vec[j] = 0;\n                j1_vec[j] = 0;\n                j2_vec[j] = 0;\n                j3_vec[j] = 0;\n            }\n        }\n\n        #pragma omp for schedule(dynamic, 1)\n        for (j = 0; j < a0.size(); j++) {\n            if (a0[j] >= threshold0 && a0[j] < threshold1) {\n                i0_vec[tid]++;\n                j0_vec[tid]++;\n            } else if (a0[j] >= threshold1 && a0[j] < threshold2) {\n                i1_vec[tid]++;\n                j1_vec[tid]++;\n            } else if (a0[j] >= threshold2 &&",
            "std::array<size_t, 4> bins_temp;\n  bins_temp.fill(0);\n#pragma omp parallel\n  {\n    size_t i;\n#pragma omp for\n    for (i = 0; i < x.size(); i++) {\n      if (x[i] >= 0 && x[i] < 0.25) {\n        bins_temp[0]++;\n      } else if (x[i] >= 0.25 && x[i] < 0.50) {\n        bins_temp[1]++;\n      } else if (x[i] >= 0.50 && x[i] < 0.75) {\n        bins_temp[2]++;\n      } else if (x[i] >= 0.75 && x[i] < 1) {\n        bins_temp[3]++;\n      }\n    }\n  }\n  bins.swap(bins_temp);\n}",
            "bins.fill(0);\n\n    double quartile0 = 0.25, quartile1 = 0.5, quartile2 = 0.75;\n\n    #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n    for (auto xi : x) {\n        if (xi < quartile0) bins[0]++;\n        else if (xi < quartile1) bins[1]++;\n        else if (xi < quartile2) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); i++) {\n        double y = x[i];\n        if (y < 0.25) {\n            #pragma omp atomic\n            ++bins[0];\n        } else if (y < 0.5) {\n            #pragma omp atomic\n            ++bins[1];\n        } else if (y < 0.75) {\n            #pragma omp atomic\n            ++bins[2];\n        } else {\n            #pragma omp atomic\n            ++bins[3];\n        }\n    }\n}",
            "// Compute the number of threads to use\n  const int nThreads = omp_get_max_threads();\n\n  // Initialize the bins\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n\n  // Iterate over the elements of the vector in parallel\n  size_t blockSize = (x.size() + nThreads - 1) / nThreads;\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    // Compute the index of the current thread\n    int threadIdx = omp_get_thread_num();\n\n    // Compute the number of elements per block\n    size_t blockStart = threadIdx * blockSize;\n    size_t blockEnd = std::min(blockStart + blockSize, x.size());\n\n    // Compute the lower and upper bound of the current thread\n    double lower = 0;\n    double upper = 0.5;\n    if (threadIdx == 0) {\n      lower = 0;\n    } else {\n      lower = upper;\n    }\n    upper = 1;\n\n    // Count the number of elements in the current block\n    for (size_t j = blockStart; j < blockEnd; j++) {\n      if (x[j] >= lower && x[j] < upper) {\n        bins[threadIdx] += 1;\n      }\n    }\n  }\n}",
            "#pragma omp parallel num_threads(8)\n    {\n        std::array<size_t, 4> local_bins;\n        #pragma omp for schedule(static) nowait\n        for(size_t i = 0; i < x.size(); i++) {\n            double val = x[i];\n            if(val < 0.25) local_bins[0]++;\n            else if(val < 0.5) local_bins[1]++;\n            else if(val < 0.75) local_bins[2]++;\n            else local_bins[3]++;\n        }\n\n        #pragma omp critical\n        {\n            bins[0] += local_bins[0];\n            bins[1] += local_bins[1];\n            bins[2] += local_bins[2];\n            bins[3] += local_bins[3];\n        }\n    }\n}",
            "bins.fill(0);\n\n   #pragma omp parallel for\n   for (int i=0; i < x.size(); ++i) {\n      if (x[i] <= 0.25) {\n         bins[0] = bins[0] + 1;\n      }\n      if ((x[i] > 0.25) && (x[i] <= 0.5)) {\n         bins[1] = bins[1] + 1;\n      }\n      if ((x[i] > 0.5) && (x[i] <= 0.75)) {\n         bins[2] = bins[2] + 1;\n      }\n      if ((x[i] > 0.75) && (x[i] <= 1.0)) {\n         bins[3] = bins[3] + 1;\n      }\n   }\n}",
            "#pragma omp parallel for reduction(+:bins[0:4])\n\tfor (size_t i=0; i<x.size(); ++i) {\n\t\tdouble x_i = x[i];\n\t\tif (x_i < 0.25) {\n\t\t\tbins[0] += 1;\n\t\t} else if (x_i < 0.5) {\n\t\t\tbins[1] += 1;\n\t\t} else if (x_i < 0.75) {\n\t\t\tbins[2] += 1;\n\t\t} else {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "size_t length = x.size();\n  bins = std::array<size_t, 4>();\n  size_t lower_quarter = 0;\n  size_t middle_quarter = 0;\n  size_t upper_quarter = 0;\n  double midpoint = 0.5;\n  double quarter_width = 0.25;\n\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (size_t i = 0; i < length; i++) {\n      if (x[i] >= lower_quarter * quarter_width && x[i] < midpoint * quarter_width) {\n        lower_quarter++;\n      } else if (x[i] >= midpoint * quarter_width && x[i] < (1 - upper_quarter) * quarter_width) {\n        middle_quarter++;\n      } else {\n        upper_quarter++;\n      }\n    }\n  }\n  bins[0] = lower_quarter;\n  bins[1] = middle_quarter;\n  bins[2] = upper_quarter;\n  bins[3] = length - (lower_quarter + middle_quarter + upper_quarter);\n}",
            "size_t n = x.size();\n\n    // Add your code here!\n    bins.fill(0);\n    omp_set_num_threads(16);\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++)\n    {\n        double value = x[i];\n        if (value >= 0.0 && value < 0.25)\n        {\n            bins[0]++;\n        }\n        else if (value >= 0.25 && value < 0.5)\n        {\n            bins[1]++;\n        }\n        else if (value >= 0.5 && value < 0.75)\n        {\n            bins[2]++;\n        }\n        else\n        {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n  omp_set_num_threads(8);\n  #pragma omp parallel\n  {\n  #pragma omp for\n  for (int i = 0; i < x.size(); ++i) {\n    auto const x_i = x.at(i);\n    if (x_i < 0.25) {\n      ++bins[0];\n    } else if (x_i < 0.5) {\n      ++bins[1];\n    } else if (x_i < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n  }\n}",
            "// INSERT YOUR CODE HERE\n\tstd::fill(bins.begin(), bins.end(), 0);\n\tconst double quartiles[4] = {0, 0.25, 0.5, 0.75};\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tdouble current = x[i];\n\t\tfor (size_t j = 0; j < 4; j++) {\n\t\t\tdouble value = current * quartiles[j];\n\t\t\tint index = int(value);\n\t\t\tdouble remainder = value - double(index);\n\t\t\tif (remainder < 0.25)\n\t\t\t\tbins[j] += 1;\n\t\t}\n\t}\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double val = x[i];\n    if (val >= 0 && val <= 0.25) {\n      bins[0]++;\n    } else if (val > 0.25 && val <= 0.5) {\n      bins[1]++;\n    } else if (val > 0.5 && val <= 0.75) {\n      bins[2]++;\n    } else if (val > 0.75 && val <= 1) {\n      bins[3]++;\n    }\n  }\n}",
            "omp_set_num_threads(4);\n\n  for (auto i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      ++bins[0];\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    } else if (x[i] >= 0.75 && x[i] <= 1) {\n      ++bins[3];\n    }\n  }\n}",
            "#pragma omp parallel for\n    for(size_t i=0; i<x.size(); ++i) {\n        if(x[i] < 0.25)\n            bins[0]++;\n        else if(x[i] < 0.5)\n            bins[1]++;\n        else if(x[i] < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "if (x.size()!= 4) {\n    throw std::invalid_argument(\"x must have exactly 4 elements\");\n  }\n  bins.fill(0);\n\n  #pragma omp parallel default(shared) num_threads(4)\n  {\n    #pragma omp single\n    {\n      size_t i = 0;\n      #pragma omp task default(shared) firstprivate(i)\n      {\n        for (i = 0; i < x.size(); ++i) {\n          if (x[i] < 0.25) {\n            bins[0]++;\n          } else if (x[i] < 0.5) {\n            bins[1]++;\n          } else if (x[i] < 0.75) {\n            bins[2]++;\n          } else {\n            bins[3]++;\n          }\n        }\n      }\n    }\n  }\n}",
            "// Fill in your code here.\n}",
            "// TODO\n}",
            "std::array<double, 4> bins_d;\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0.0 && x[i] < 0.25) {\n      bins_d[0] += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins_d[1] += 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins_d[2] += 1;\n    } else if (x[i] >= 0.75 && x[i] < 1.0) {\n      bins_d[3] += 1;\n    }\n  }\n\n#pragma omp critical\n  for (size_t i = 0; i < 4; ++i) {\n    bins[i] = static_cast<size_t>(bins_d[i]);\n  }\n}",
            "bins = {0, 0, 0, 0};\n    // TODO: implement this function\n    \n    for (double elem : x)\n    {\n        double quartile = elem / 4;\n        if (elem < quartile)\n        {\n            bins[0]++;\n        }\n        else if (elem < 2*quartile)\n        {\n            bins[1]++;\n        }\n        else if (elem < 3*quartile)\n        {\n            bins[2]++;\n        }\n        else\n        {\n            bins[3]++;\n        }\n    }\n}",
            "// Initialize the bins to zero.\n   std::fill(bins.begin(), bins.end(), 0);\n   // Count in parallel\n   #pragma omp parallel for\n   for (size_t i = 0; i < x.size(); i++) {\n      // Get the fractional part of x[i]\n      double frac = std::fmod(x[i], 1.0);\n      // Count for each bin\n      if (frac < 0.25)\n         bins[0]++;\n      else if (frac < 0.5)\n         bins[1]++;\n      else if (frac < 0.75)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "size_t N = x.size();\n    size_t n = N / 4;\n    size_t m = N % 4;\n\n    bins.fill(0);\n    int *arr = new int[N];\n    std::vector<int> count(N);\n    int i = 0;\n\n    #pragma omp parallel num_threads(omp_get_num_procs())\n    {\n        #pragma omp for\n        for (int i = 0; i < N; i++) {\n            arr[i] = x[i] * 4;\n        }\n\n        #pragma omp for\n        for (int i = 0; i < N; i++) {\n            int index = arr[i];\n            if (index >= 0 && index < N) {\n                count[index] += 1;\n            }\n        }\n    }\n\n    for (int i = 0; i < n; i++) {\n        bins[0] += count[i];\n    }\n\n    for (int i = n; i < 2*n; i++) {\n        bins[1] += count[i];\n    }\n\n    for (int i = 2*n; i < 3*n; i++) {\n        bins[2] += count[i];\n    }\n\n    for (int i = 3*n; i < 4*n; i++) {\n        bins[3] += count[i];\n    }\n\n    for (int i = 0; i < m; i++) {\n        if (x[4*n + i] * 4 >= 0 && x[4*n + i] * 4 < N) {\n            bins[0] += 1;\n        }\n    }\n    for (int i = 0; i < m; i++) {\n        if (x[4*n + i] * 4 >= n && x[4*n + i] * 4 < 2*n) {\n            bins[1] += 1;\n        }\n    }\n    for (int i = 0; i < m; i++) {\n        if (x[4*n + i] * 4 >= 2*n && x[4*n + i] * 4 < 3*n) {\n            bins[2] += 1;\n        }\n    }\n    for (int i = 0; i < m; i++) {\n        if (x[4*n + i] * 4 >= 3*n && x[4*n + i] * 4 < 4*n) {\n            bins[3] += 1;\n        }\n    }\n\n    delete [] arr;\n}",
            "#pragma omp parallel for default(shared) schedule(static)\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25)\n      bins[0]++;\n    else if (x[i] < 0.5)\n      bins[1]++;\n    else if (x[i] < 0.75)\n      bins[2]++;\n    else\n      bins[3]++;\n  }\n}",
            "size_t const N = x.size();\n    std::fill(bins.begin(), bins.end(), 0);\n\n    /* TO DO: implement */\n    #pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < N; ++i) {\n        if (x[i] < 0.25) {\n            ++bins[0];\n        }\n        else if (x[i] < 0.5) {\n            ++bins[1];\n        }\n        else if (x[i] < 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n    // TODO\n    // 2.5.2.1 - #pragma omp parallel for schedule(static)\n    // 2.5.2.2 - #pragma omp parallel for schedule(static)\n}",
            "// Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n    // Use OpenMP to compute in parallel.\n\n    double low = 0;\n    double high = 1;\n    double step = (high - low)/4;\n\n    // initialize bins\n    for (auto &i : bins)\n        i = 0;\n\n    #pragma omp parallel for schedule(dynamic)\n    for (auto i = 0; i < x.size(); ++i) {\n        double q = (x[i] - low)/step;\n\n        if (q >= 0 && q < 1)\n            bins[0] += 1;\n        else if (q >= 1 && q < 2)\n            bins[1] += 1;\n        else if (q >= 2 && q < 3)\n            bins[2] += 1;\n        else if (q >= 3 && q < 4)\n            bins[3] += 1;\n    }\n}",
            "size_t i, num_threads;\n\n#pragma omp parallel\n  {\n#pragma omp single\n    {\n      num_threads = omp_get_num_threads();\n      std::fill(bins.begin(), bins.end(), 0);\n    }\n\n#pragma omp for\n    for (i = 0; i < x.size(); i++) {\n      if (x[i] >= 0 && x[i] <= 0.25)\n        bins[0]++;\n      else if (x[i] > 0.25 && x[i] <= 0.5)\n        bins[1]++;\n      else if (x[i] > 0.5 && x[i] <= 0.75)\n        bins[2]++;\n      else if (x[i] > 0.75 && x[i] <= 1)\n        bins[3]++;\n    }\n  }\n\n  for (i = 0; i < num_threads; i++) {\n    printf(\"thread %zu: bins[0] = %zu\\tbins[1] = %zu\\tbins[2] = %zu\\tbins[3] = %zu\\n\", i, bins[0], bins[1], bins[2], bins[3]);\n  }\n}",
            "size_t n = x.size();\n  double threshold = 0.25;\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    double xi = x[i];\n    if (xi < threshold) {\n      bins[0] += 1;\n    }\n    if (xi >= threshold && xi < 2*threshold) {\n      bins[1] += 1;\n    }\n    if (xi >= 2*threshold && xi < 3*threshold) {\n      bins[2] += 1;\n    }\n    if (xi >= 3*threshold) {\n      bins[3] += 1;\n    }\n  }\n}",
            "}",
            "const size_t size = x.size();\n  bins = {0, 0, 0, 0};\n  #pragma omp parallel for\n  for(size_t i = 0; i < size; ++i){\n    double value = x[i];\n    if(value >= 0 && value < 0.25){\n      ++bins[0];\n    }\n    else if(value >= 0.25 && value < 0.50){\n      ++bins[1];\n    }\n    else if(value >= 0.50 && value < 0.75){\n      ++bins[2];\n    }\n    else{\n      ++bins[3];\n    }\n  }\n}",
            "//TODO\n}",
            "// TODO: Your code here.\n    bins.fill(0);\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (int i = 0; i < x.size(); ++i) {\n            if (x[i] < 0.25) {\n                ++bins[0];\n            } else if (x[i] < 0.5) {\n                ++bins[1];\n            } else if (x[i] < 0.75) {\n                ++bins[2];\n            } else {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0.0 && x[i] <= 0.25) {\n            ++bins[0];\n        } else if (x[i] > 0.25 && x[i] <= 0.5) {\n            ++bins[1];\n        } else if (x[i] > 0.5 && x[i] <= 0.75) {\n            ++bins[2];\n        } else if (x[i] > 0.75) {\n            ++bins[3];\n        }\n    }\n}",
            "const size_t n = x.size();\n   const double q = 0.25; // the fractional part\n   bins = {0, 0, 0, 0}; // all counters start at 0\n   \n   // TODO: Use OpenMP to compute in parallel\n}",
            "// 1. compute how many elements in x there are\n    size_t N = x.size();\n\n    // 2. initialize bins to 0\n    // 3. add a `pragma omp parallel` and an `omp for`\n    // 4. for each element x[i], find out which bin it belongs to (and add 1)\n    // 5. add a `pragma omp barrier` after the parallel for loop\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i)\n\t{\n\t\tif (x[i] < 0.25)\n\t\t{\n\t\t\tbins[0] += 1;\n\t\t}\n\t\telse if (x[i] < 0.5)\n\t\t{\n\t\t\tbins[1] += 1;\n\t\t}\n\t\telse if (x[i] < 0.75)\n\t\t{\n\t\t\tbins[2] += 1;\n\t\t}\n\t\telse if (x[i] <= 1.0)\n\t\t{\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n    // 6. add a `pragma omp single` to print the result\n    #pragma omp single\n    {\n        std::cout << \"Quartiles: \" << bins[0] <<'' << bins[1] <<'' << bins[2] <<'' << bins[3] << std::endl;\n    }\n}",
            "size_t length = x.size();\n  double lower_bound;\n  double upper_bound;\n  size_t numThreads;\n\n  // TODO: set the appropriate number of threads\n  //numThreads = omp_get_num_procs();\n  numThreads = 2;\n  omp_set_num_threads(numThreads);\n\n  #pragma omp parallel num_threads(numThreads)\n  {\n    int id = omp_get_thread_num();\n    int bin_id;\n    size_t lower = (length * id) / numThreads;\n    size_t upper = (length * (id + 1)) / numThreads;\n\n    for (int i = lower; i < upper; ++i) {\n      double value = x[i];\n      if (value > 0.0 && value < 0.25) {\n        bin_id = 0;\n      } else if (value >= 0.25 && value < 0.5) {\n        bin_id = 1;\n      } else if (value >= 0.5 && value < 0.75) {\n        bin_id = 2;\n      } else if (value >= 0.75 && value <= 1.0) {\n        bin_id = 3;\n      } else {\n        bin_id = -1;\n      }\n\n      #pragma omp atomic\n      bins[bin_id] += 1;\n    }\n  }\n}",
            "#pragma omp parallel for\n    for (int i = 0; i < (int)x.size(); i++) {\n        double tmp = x[i];\n        if (tmp <= 0.25) {\n            bins[0] += 1;\n        } else if (tmp <= 0.5) {\n            bins[1] += 1;\n        } else if (tmp <= 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// You will need to add your code here\n}",
            "}",
            "size_t n = x.size();\n\n    #pragma omp parallel for \n    for (int i=0; i < n; i++) {\n\n        double xi = x[i];\n        if (xi < 0) xi = 0; // Ignore negative values\n        if (xi > 100) xi = 100; // Ignore values greater than 100\n\n        if (xi < 25) {\n            bins[0] += 1;\n        } else if (xi < 50) {\n            bins[1] += 1;\n        } else if (xi < 75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "double bin_size = 0.25;\n\tbins.fill(0);\n#pragma omp parallel\n\t{\n#pragma omp for\n\t\tfor (size_t i = 0; i < x.size(); i++) {\n\t\t\tdouble val = x[i];\n\t\t\tif (val < bin_size) {\n\t\t\t\tbins[0]++;\n\t\t\t}\n\t\t\telse if (val < (bin_size * 2)) {\n\t\t\t\tbins[1]++;\n\t\t\t}\n\t\t\telse if (val < (bin_size * 3)) {\n\t\t\t\tbins[2]++;\n\t\t\t}\n\t\t\telse if (val < (bin_size * 4)) {\n\t\t\t\tbins[3]++;\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: Your code here\n    std::fill(bins.begin(), bins.end(), 0);\n    double fraction = 0;\n    #pragma omp parallel for schedule(static) reduction(+:fraction)\n    for(int i = 0; i < x.size(); i++){\n        fraction = x[i] - std::floor(x[i]);\n        if(fraction >= 0 && fraction < 0.25){\n            #pragma omp atomic\n            bins[0] += 1;\n        }\n        else if(fraction >= 0.25 && fraction < 0.5){\n            #pragma omp atomic\n            bins[1] += 1;\n        }\n        else if(fraction >= 0.5 && fraction < 0.75){\n            #pragma omp atomic\n            bins[2] += 1;\n        }\n        else{\n            #pragma omp atomic\n            bins[3] += 1;\n        }\n    }\n}",
            "const double chunk = 0.25;\n  const size_t n = x.size();\n  std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel\n  {\n    std::vector<size_t> bins_private(bins);\n\n    #pragma omp for schedule(static)\n    for (int i=0; i<n; i++) {\n      double x_i = x[i];\n      if (x_i >= 0 && x_i <= chunk) {\n        bins_private[0]++;\n      } else if (x_i > chunk && x_i <= chunk*2) {\n        bins_private[1]++;\n      } else if (x_i > chunk*2 && x_i <= chunk*3) {\n        bins_private[2]++;\n      } else if (x_i > chunk*3) {\n        bins_private[3]++;\n      }\n    }\n\n    #pragma omp critical\n    for (int i=0; i<bins.size(); i++) {\n      bins[i] += bins_private[i];\n    }\n  }\n}",
            "if (x.size()!= bins.size()) {\n    throw std::invalid_argument(\"x and bins must have the same size\");\n  }\n\n  // TODO: compute the number of doubles in each bin\n\n  // TODO: print bins\n}",
            "bins = {};\n\tsize_t n = x.size();\n\tif (n > 0) {\n\t\t// TODO\n\t}\n}",
            "size_t const n = x.size();\n  size_t const num_thread = omp_get_max_threads();\n\n#pragma omp parallel for num_threads(num_thread)\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] < 0.25) {\n      bins[0] += 1;\n    } else if (x[i] < 0.5) {\n      bins[1] += 1;\n    } else if (x[i] < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "bins.fill(0);\n#pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      #pragma omp atomic\n      ++bins[0];\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      #pragma omp atomic\n      ++bins[1];\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      #pragma omp atomic\n      ++bins[2];\n    } else if (x[i] >= 0.75 && x[i] < 1) {\n      #pragma omp atomic\n      ++bins[3];\n    }\n  }\n}",
            "bins.fill(0);\n    #pragma omp parallel for\n    for(size_t i = 0; i < x.size(); ++i){\n        if (x[i] <= 0.25) {\n            ++bins[0];\n        } else if (x[i] <= 0.5) {\n            ++bins[1];\n        } else if (x[i] <= 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n    std::cout << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n}",
            "size_t const n = x.size();\n\n#pragma omp parallel for\n    for(int i=0; i<n; i++) {\n        double const xi = x[i];\n        double const q = xi * 4;\n        int const index = static_cast<int>(std::round(q));\n        int const index_mod_4 = index % 4;\n        bins[index_mod_4]++;\n    }\n}",
            "// TODO: Your code here\n\n    // Number of bins\n    int n = 4;\n\n    // Counts per bin\n    size_t counts[n];\n\n    // Initialize counts\n    for (int i = 0; i < n; i++) {\n        counts[i] = 0;\n    }\n\n    // Start parallel region\n#pragma omp parallel for\n    for (int i = 0; i < x.size(); i++) {\n        // Get bin of current element\n        int bin = floor(x[i] / 0.25);\n        // Increase count of this bin\n        counts[bin] += 1;\n    }\n\n    // Copy counts into output array\n    for (int i = 0; i < n; i++) {\n        bins[i] = counts[i];\n    }\n\n    // End parallel region\n}",
            "// TODO: Your code here.\n}",
            "// TODO: Replace this code with a parallel implementation.\n  // Hint: Use omp_get_thread_num(), omp_get_num_threads(), \n  // and omp_get_num_procs(). You can use a for loop to partition the vector \n  // across threads.\n  //\n  // Note: if you're using the sequential version of this program, you'll \n  // need to compile with -foffload-static-libomp to work around a bug. \n  // You should also see a significant speedup (e.g., 100x) on larger \n  // datasets.\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    double d = x[i];\n    if (d >= 0.0 && d < 0.25) ++bins[0];\n    if (d >= 0.25 && d < 0.5) ++bins[1];\n    if (d >= 0.5 && d < 0.75) ++bins[2];\n    if (d >= 0.75 && d <= 1.0) ++bins[3];\n  }\n\n}",
            "// YOUR CODE HERE\n\n  // omp_set_num_threads(4);\n\n  //#pragma omp parallel for\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n  \n  //#pragma omp parallel for schedule(dynamic, 1)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n  \n  //#pragma omp parallel for schedule(guided)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n\n  //#pragma omp parallel for schedule(auto)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n\n  //#pragma omp parallel for schedule(static)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n\n  //#pragma omp parallel for schedule(static, 2)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n\n  //#pragma omp parallel for schedule(static)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n\n  //#pragma omp parallel for schedule(static)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins[0];\n  //   else if(x[i] < 0.5)\n  //     ++bins[1];\n  //   else if(x[i] < 0.75)\n  //     ++bins[2];\n  //   else\n  //     ++bins[3];\n  // }\n\n  //#pragma omp parallel for schedule(static)\n  // for(int i = 0; i < x.size(); ++i) {\n  //   if(x[i] < 0.25)\n  //     ++bins",
            "#pragma omp parallel for default(none) schedule(dynamic, 10)\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      ++bins[0];\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    } else if (x[i] >= 0.75 && x[i] <= 1) {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "size_t numElements = x.size();\n\n  std::vector<double> sub_array;\n  std::vector<size_t> tempBins(4);\n\n  // Partition the array into 4 sub-arrays\n  size_t sub_size = numElements / 4;\n  for (size_t i = 0; i < 4; i++) {\n    sub_array.assign(x.begin() + i * sub_size, x.begin() + (i + 1) * sub_size);\n  }\n\n  // Initialize bins\n  std::fill(bins.begin(), bins.end(), 0);\n\n  // OpenMP parallel region\n# pragma omp parallel shared(sub_array, bins)\n  {\n    size_t tid = omp_get_thread_num();\n\n    double sub_array_min = *(std::min_element(sub_array.begin(), sub_array.end()));\n    double sub_array_max = *(std::max_element(sub_array.begin(), sub_array.end()));\n    double sub_array_delta = sub_array_max - sub_array_min;\n    double sub_array_mid = sub_array_min + sub_array_delta / 2;\n\n    // Determine how many elements each thread will process\n    size_t numElems = sub_array.size();\n    size_t thread_chunk = numElems / 4;\n\n    // Process only the elements that belong to this thread\n    std::vector<double>::iterator beg = sub_array.begin() + tid * thread_chunk;\n    std::vector<double>::iterator end = sub_array.begin() + (tid + 1) * thread_chunk;\n\n    // Initialize bins\n    std::fill(tempBins.begin(), tempBins.end(), 0);\n\n    // Count the number of elements in this thread's sub-array\n    for (auto it = beg; it!= end; it++) {\n      if (*it >= sub_array_min && *it < sub_array_mid) {\n        tempBins[0]++;\n      } else if (*it >= sub_array_mid && *it < sub_array_max) {\n        tempBins[1]++;\n      } else if (*it >= sub_array_max && *it < sub_array_max + sub_array_delta / 4) {\n        tempBins[2]++;\n      } else {\n        tempBins[3]++;\n      }\n    }\n\n    // Update bins in master thread\n    if (tid == 0) {\n      for (size_t i = 0; i < 4; i++) {\n        bins[i] += tempBins[i];\n      }\n    }\n  }\n\n  // Copy bins from master thread\n  std::copy(bins.begin(), bins.end(), tempBins.begin());\n}",
            "size_t const N = x.size();\n  bins = std::array<size_t, 4>{0, 0, 0, 0};\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    double const x_i = x[i];\n    if (x_i < 0.25)\n      ++bins[0];\n    else if (x_i < 0.5)\n      ++bins[1];\n    else if (x_i < 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "//TODO: Implement this function\n    bins.fill(0);\n    size_t n = x.size();\n    #pragma omp parallel for\n    for(size_t i = 0; i < n; ++i)\n    {\n        double cur = x[i];\n        if(cur >= 0 && cur < 0.25)\n        {\n            bins[0]++;\n        }\n        else if(cur >= 0.25 && cur < 0.5)\n        {\n            bins[1]++;\n        }\n        else if(cur >= 0.5 && cur < 0.75)\n        {\n            bins[2]++;\n        }\n        else\n        {\n            bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0.0 && x[i] < 0.25) {\n      ++bins[0];\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      ++bins[1];\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      ++bins[2];\n    } else if (x[i] >= 0.75 && x[i] < 1.0) {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO\n}",
            "// Fill bins with the appropriate values\n  bins = std::array<size_t, 4>{};\n\n  // Your code here\n  size_t const xSize = x.size();\n\n  size_t const num_threads = 4;\n  #pragma omp parallel for num_threads(num_threads)\n  for (int i = 0; i < xSize; i++) {\n    double const curr_value = x[i];\n    if (curr_value >= 0 && curr_value <= 0.25) {\n      bins[0]++;\n    } else if (curr_value > 0.25 && curr_value <= 0.5) {\n      bins[1]++;\n    } else if (curr_value > 0.5 && curr_value <= 0.75) {\n      bins[2]++;\n    } else if (curr_value > 0.75 && curr_value <= 1) {\n      bins[3]++;\n    }\n  }\n}",
            "const size_t N = x.size();\n    std::array<size_t, 4> bins_local;\n    bins_local.fill(0);\n    size_t nthreads = 2;\n    if(N >= 10000){\n        nthreads = omp_get_max_threads();\n    }\n    #pragma omp parallel for num_threads(nthreads)\n    for(int i=0;i<N;i++){\n        if(x[i] >= 0.0 && x[i] < 0.25){\n            #pragma omp atomic\n            bins_local[0]++;\n        }\n        else if(x[i] >= 0.25 && x[i] < 0.5){\n            #pragma omp atomic\n            bins_local[1]++;\n        }\n        else if(x[i] >= 0.5 && x[i] < 0.75){\n            #pragma omp atomic\n            bins_local[2]++;\n        }\n        else if(x[i] >= 0.75 && x[i] <= 1.0){\n            #pragma omp atomic\n            bins_local[3]++;\n        }\n    }\n    for(int i=0;i<4;i++){\n        bins[i] = bins_local[i];\n    }\n}",
            "const size_t n = x.size();\n  bins.fill(0);\n  //std::vector<size_t> bins(4, 0);\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < n; ++i) {\n    //if (x[i] >= 0.0 && x[i] < 0.25) {\n    //if (x[i] >= 0.0 && x[i] < 0.25) {\n    if (x[i] >= 0.25 && x[i] < 0.50) {\n      #pragma omp atomic\n      ++bins[0];\n    }\n    //if (x[i] >= 0.25 && x[i] < 0.50) {\n    //if (x[i] >= 0.50 && x[i] < 0.75) {\n    if (x[i] >= 0.50 && x[i] < 0.75) {\n      #pragma omp atomic\n      ++bins[1];\n    }\n    //if (x[i] >= 0.50 && x[i] < 0.75) {\n    //if (x[i] >= 0.75 && x[i] < 1.00) {\n    if (x[i] >= 0.75 && x[i] < 1.00) {\n      #pragma omp atomic\n      ++bins[2];\n    }\n    //if (x[i] >= 0.75 && x[i] < 1.00) {\n    //if (x[i] >= 0.0 && x[i] < 0.25) {\n    if (x[i] >= 0.0 && x[i] < 0.25) {\n      #pragma omp atomic\n      ++bins[3];\n    }\n  }\n}\n\n\nint main() {\n  const size_t n = 10000000;\n  std::vector<double> x(n);\n\n  // initialize x with random numbers\n  std::random_device rd;\n  std::mt19937 gen(rd());\n  std::uniform_real_distribution<> dis(0.0, 1.0);\n  for (size_t i = 0; i < n; ++i) {\n    x[i] = dis(gen);\n  }\n\n  std::array<size_t, 4> bins;\n  //omp_set_num_threads(8);\n  //omp_set_num_threads(2);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(omp_get_max_threads());\n  //omp_set_num_threads(omp_get_num_procs());\n  //omp_set_num_threads(omp_get_num_procs() / 2);\n  omp_set_num_threads(4);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(40);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(100);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(400);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(200);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(300);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(400);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(500);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(600);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(700);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(800);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(900);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(1000);\n  //omp_set_num_threads(1);\n  //omp_set_num_threads(2000);\n  //omp_set_num_threads(1);\n  //omp_set",
            "// TODO\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "serial",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "bins = std::array<size_t, 4>{0, 0, 0, 0};\n   for (auto &v : x) {\n      if (v >= 0 && v < 0.25) {\n         ++bins[0];\n      } else if (v >= 0.25 && v < 0.5) {\n         ++bins[1];\n      } else if (v >= 0.5 && v < 0.75) {\n         ++bins[2];\n      } else if (v >= 0.75 && v <= 1) {\n         ++bins[3];\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n\n    // TODO: implement this function.\n}",
            "size_t n = x.size();\n  std::vector<double> sorted_x(x);\n  std::sort(sorted_x.begin(), sorted_x.end());\n  for (int i = 0; i < n; i++) {\n    if (sorted_x[i] < 0.25) {\n      bins[0]++;\n    } else if (sorted_x[i] < 0.50) {\n      bins[1]++;\n    } else if (sorted_x[i] < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "for (auto value: x) {\n    if (0 <= value && value < 0.25) {\n      ++bins[0];\n    }\n    else if (0.25 <= value && value < 0.5) {\n      ++bins[1];\n    }\n    else if (0.5 <= value && value < 0.75) {\n      ++bins[2];\n    }\n    else if (0.75 <= value && value <= 1) {\n      ++bins[3];\n    }\n  }\n}",
            "bins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\tfor (double i : x) {\n\t\tif (i >= 0.0 && i < 0.25) {\n\t\t\tbins[0] += 1;\n\t\t}\n\t\telse if (i >= 0.25 && i < 0.50) {\n\t\t\tbins[1] += 1;\n\t\t}\n\t\telse if (i >= 0.50 && i < 0.75) {\n\t\t\tbins[2] += 1;\n\t\t}\n\t\telse if (i >= 0.75 && i < 1.00) {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "size_t n = x.size();\n  size_t n_1 = n - 1;\n  double *x_data = x.data();\n  size_t *bins_data = bins.data();\n  for (size_t i = 0; i < n; i++) {\n    double d = x_data[i];\n    if (d <= 0.25) {\n      bins_data[0]++;\n    } else if (d <= 0.5) {\n      bins_data[1]++;\n    } else if (d <= 0.75) {\n      bins_data[2]++;\n    } else if (d <= 1) {\n      bins_data[3]++;\n    } else {\n      throw std::runtime_error(\"Invalid value\");\n    }\n  }\n}",
            "std::array<double, 4> thresholds{0.0, 0.25, 0.5, 0.75};\n  for (auto t : thresholds) {\n    for (auto xi : x) {\n      if (xi >= t && xi < t + 0.25)\n        bins[0]++;\n      else if (xi >= t + 0.25 && xi < t + 0.5)\n        bins[1]++;\n      else if (xi >= t + 0.5 && xi < t + 0.75)\n        bins[2]++;\n      else if (xi >= t + 0.75 && xi <= t + 1.0)\n        bins[3]++;\n    }\n  }\n}",
            "bins.fill(0);\n  for (auto d : x) {\n    if (d < 0.25) {\n      ++bins[0];\n    } else if (d < 0.5) {\n      ++bins[1];\n    } else if (d < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// FIXME: Implement this function\n}",
            "std::array<double, 4> bounds = {0.0, 0.25, 0.5, 0.75};\n  for (double const& el : x) {\n    for (size_t i = 0; i < bounds.size(); i++) {\n      if (el > bounds[i] && el <= bounds[i+1]) {\n        bins[i]++;\n      }\n    }\n  }\n}",
            "double q1 = x.front();\n    double q3 = x.back();\n\n    for (double d : x) {\n        if (d < q1) {\n            q1 = d;\n        }\n        if (d > q3) {\n            q3 = d;\n        }\n    }\n    std::sort(x.begin(), x.end());\n    size_t N = x.size();\n\n    // First quartile\n    double p = 0.25;\n    size_t i = 0;\n    while (p < 1 && i < N) {\n        if (x[i] < q1) {\n            ++i;\n        }\n        else {\n            p = (i + 1) / static_cast<double>(N);\n        }\n    }\n    bins[0] = i;\n\n    // Second quartile\n    p = 0.5;\n    i = 0;\n    while (p < 1 && i < N) {\n        if (x[i] < q1) {\n            ++i;\n        }\n        else {\n            p = (i + 1) / static_cast<double>(N);\n        }\n    }\n    bins[1] = i;\n\n    // Third quartile\n    p = 0.75;\n    i = 0;\n    while (p < 1 && i < N) {\n        if (x[i] < q3) {\n            ++i;\n        }\n        else {\n            p = (i + 1) / static_cast<double>(N);\n        }\n    }\n    bins[2] = i;\n\n    // Fourth quartile\n    p = 1;\n    i = 0;\n    while (p < 1 && i < N) {\n        if (x[i] < q3) {\n            ++i;\n        }\n        else {\n            p = (i + 1) / static_cast<double>(N);\n        }\n    }\n    bins[3] = i;\n}",
            "bins[0] = 0; // Count of values that are less than 0.25\n    bins[1] = 0; // Count of values that are less than 0.5\n    bins[2] = 0; // Count of values that are less than 0.75\n    bins[3] = 0; // Count of values that are less than 1.0\n    for (auto val : x) {\n        if (val < 0.25) {\n            bins[0]++;\n        } else if (val < 0.5) {\n            bins[1]++;\n        } else if (val < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto value : x) {\n        double quartile = value * 4;\n        if (quartile >= 0 && quartile < 2) {\n            bins[0]++;\n        } else if (quartile >= 2 && quartile < 4) {\n            bins[1]++;\n        } else if (quartile >= 4 && quartile < 6) {\n            bins[2]++;\n        } else if (quartile >= 6 && quartile < 8) {\n            bins[3]++;\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n   for (auto const& v : x) {\n      double const q = v * 4;\n      if (q < 0.25) {\n         ++bins[0];\n      } else if (q < 0.5) {\n         ++bins[1];\n      } else if (q < 0.75) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "size_t const n = x.size();\n  double const eps = 0.25;\n\n  // Initialize the vector with all 0s.\n  bins.fill(0);\n\n  for (size_t i = 0; i < n; i++) {\n    double xi = x[i];\n    if ((xi >= 0) && (xi < 0.25)) {\n      bins[0] += 1;\n    } else if ((xi >= 0.25) && (xi < 0.5)) {\n      bins[1] += 1;\n    } else if ((xi >= 0.5) && (xi < 0.75)) {\n      bins[2] += 1;\n    } else if ((xi >= 0.75) && (xi <= 1)) {\n      bins[3] += 1;\n    }\n  }\n}",
            "std::transform(x.begin(), x.end(), bins.begin(), [&] (double val) { \n        if (val > 0.75) { return 4; }\n        if (val > 0.5) { return 3; }\n        if (val > 0.25) { return 2; }\n        return 1; \n    });\n}",
            "int n = x.size();\n   int n2 = n/2;\n   int n4 = n/4;\n   int n6 = n2*2;\n\n   std::vector<double> q;\n   for (int i = 0; i < n; ++i) {\n      q.push_back(x[i]);\n   }\n\n   // sort in increasing order\n   std::sort(q.begin(), q.end());\n\n   // count number of elements that have a fractional part in [0, 0.25)\n   int num_1 = 0;\n   for (int i = 0; i < n4; ++i) {\n      if (q[i] < 0.25) {\n         num_1++;\n      }\n   }\n\n   // count number of elements that have a fractional part in [0.25, 0.5)\n   int num_2 = 0;\n   for (int i = n4; i < n2; ++i) {\n      if ((q[i] >= 0.25) && (q[i] < 0.5)) {\n         num_2++;\n      }\n   }\n\n   // count number of elements that have a fractional part in [0.5, 0.75)\n   int num_3 = 0;\n   for (int i = n2; i < n6; ++i) {\n      if ((q[i] >= 0.5) && (q[i] < 0.75)) {\n         num_3++;\n      }\n   }\n\n   // count number of elements that have a fractional part in [0.75, 1)\n   int num_4 = 0;\n   for (int i = n6; i < n; ++i) {\n      if (q[i] >= 0.75) {\n         num_4++;\n      }\n   }\n\n   bins[0] = num_1;\n   bins[1] = num_2;\n   bins[2] = num_3;\n   bins[3] = num_4;\n}",
            "/* To be completed */\n}",
            "// This function will not be implemented\n}",
            "size_t N = x.size();\n\n    // Count values in the first quartile bin.\n    for (size_t i = 0; i < N; i++) {\n        double x_i = x[i];\n        if (x_i >= 0.0 && x_i < 0.25) {\n            bins[0] += 1;\n        }\n    }\n\n    // Count values in the second quartile bin.\n    for (size_t i = 0; i < N; i++) {\n        double x_i = x[i];\n        if (x_i >= 0.25 && x_i < 0.5) {\n            bins[1] += 1;\n        }\n    }\n\n    // Count values in the third quartile bin.\n    for (size_t i = 0; i < N; i++) {\n        double x_i = x[i];\n        if (x_i >= 0.5 && x_i < 0.75) {\n            bins[2] += 1;\n        }\n    }\n\n    // Count values in the fourth quartile bin.\n    for (size_t i = 0; i < N; i++) {\n        double x_i = x[i];\n        if (x_i >= 0.75 && x_i <= 1.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "for (double v : x) {\n    if (v < 0.25) ++bins[0];\n    else if (v < 0.5) ++bins[1];\n    else if (v < 0.75) ++bins[2];\n    else ++bins[3];\n  }\n}",
            "bins = std::array<size_t, 4>();\n  for (double val : x) {\n    if (val <= 0.25) {\n      bins[0] += 1;\n    } else if (val <= 0.5) {\n      bins[1] += 1;\n    } else if (val <= 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "/* WRITE YOUR CODE HERE */\n}",
            "std::sort(x.begin(), x.end());\n  size_t n = x.size();\n  bins[0] = 0;\n  for (size_t i = 1; i < n; ++i) {\n    if (x[i] < x[0] + 0.25 * (x[n-1] - x[0]))\n      ++bins[0];\n    else if (x[i] < x[0] + 0.5 * (x[n-1] - x[0]))\n      ++bins[1];\n    else if (x[i] < x[0] + 0.75 * (x[n-1] - x[0]))\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "for(double v: x) {\n    if(v < 0 || v > 1) {\n      throw std::invalid_argument(\"value out of range\");\n    }\n\n    if(v >= 0 && v < 0.25) {\n      bins[0]++;\n    }\n    else if(v >= 0.25 && v < 0.5) {\n      bins[1]++;\n    }\n    else if(v >= 0.5 && v < 0.75) {\n      bins[2]++;\n    }\n    else if(v >= 0.75 && v < 1) {\n      bins[3]++;\n    }\n  }\n}",
            "bins.fill(0);\n   for (auto y : x) {\n      if (y <= 0.25) {\n         ++bins[0];\n      }\n      else if (y <= 0.5) {\n         ++bins[1];\n      }\n      else if (y <= 0.75) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "assert(x.size() == bins.size());\n    // YOUR CODE HERE\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for(int i=0;i<x.size();i++){\n        if(x[i]>=0 && x[i]<=0.25){\n            bins[0] += 1;\n        }\n        else if(x[i]>0.25 && x[i]<=0.5){\n            bins[1] += 1;\n        }\n        else if(x[i]>0.5 && x[i]<=0.75){\n            bins[2] += 1;\n        }\n        else if(x[i]>0.75){\n            bins[3] += 1;\n        }\n    }\n}",
            "auto middle = x.begin() + x.size() / 2;\n  auto first_half = x.begin();\n  auto second_half = middle;\n  auto first_quart = x.begin();\n  auto third_quart = middle;\n  auto fourth_quart = x.end();\n\n  for (auto it = first_half; it!= middle; ++it) {\n    if (*it < 0.25 * *first_quart)\n      ++bins[0];\n    else if (*it < 0.5 * *first_quart)\n      ++bins[1];\n    else if (*it < 0.75 * *first_quart)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n  for (auto it = second_half; it!= fourth_quart; ++it) {\n    if (*it < 0.25 * *third_quart)\n      ++bins[0];\n    else if (*it < 0.5 * *third_quart)\n      ++bins[1];\n    else if (*it < 0.75 * *third_quart)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "for (auto const& i : x) {\n    if (i < 0.25) {\n      bins[0] += 1;\n    }\n    else if (i < 0.5) {\n      bins[1] += 1;\n    }\n    else if (i < 0.75) {\n      bins[2] += 1;\n    }\n    else {\n      bins[3] += 1;\n    }\n  }\n}",
            "std::array<size_t, 4> count;\n\tfor (auto const &a : x) {\n\t\tif (a < 0.25) {\n\t\t\t++count[0];\n\t\t}\n\t\telse if (a < 0.5) {\n\t\t\t++count[1];\n\t\t}\n\t\telse if (a < 0.75) {\n\t\t\t++count[2];\n\t\t}\n\t\telse {\n\t\t\t++count[3];\n\t\t}\n\t}\n\tbins = count;\n}",
            "// TODO: Fill out this function\n    bins[0]=0;\n    bins[1]=0;\n    bins[2]=0;\n    bins[3]=0;\n    for(auto it=x.begin();it!=x.end();it++)\n    {\n        double r=*it;\n        if(r<0.25)\n            bins[0]++;\n        else if(r>=0.25 && r<0.5)\n            bins[1]++;\n        else if(r>=0.5 && r<0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "bins = {0, 0, 0, 0};\n\n  for (double elem : x) {\n    if (elem < 0.25) {\n      ++bins[0];\n    } else if (elem < 0.5) {\n      ++bins[1];\n    } else if (elem < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n\n}",
            "for (double y : x) {\n    if (y < 0.25)\n      ++bins[0];\n    else if (y < 0.5)\n      ++bins[1];\n    else if (y < 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "std::array<double, 4> q = {0.25, 0.5, 0.75, 1};\n    bins = std::array<size_t, 4>{0, 0, 0, 0};\n    for (auto xi : x) {\n        for (size_t i = 0; i < q.size(); i++) {\n            if (xi < q[i]) {\n                bins[i]++;\n                break;\n            }\n        }\n    }\n}",
            "for(double const& val : x) {\n    if(val < 0.25) {\n      bins[0]++;\n    }\n    else if(val < 0.5) {\n      bins[1]++;\n    }\n    else if(val < 0.75) {\n      bins[2]++;\n    }\n    else {\n      bins[3]++;\n    }\n  }\n}",
            "// Initialize bins to 0\n  bins = {0, 0, 0, 0};\n\n  // TODO: Implement the function\n}",
            "size_t N = x.size();\n    bins.fill(0);\n    for (auto const& xi : x) {\n        if (xi < 0.25 * N) {\n            ++bins[0];\n        } else if (xi < 0.5 * N) {\n            ++bins[1];\n        } else if (xi < 0.75 * N) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO: Your code here\n  std::vector<double> quartiles = {-0.25, -0.25, 0.25, 0.25};\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= quartiles[0] && x[i] <= quartiles[1]) {\n      ++bins[0];\n    } else if (x[i] >= quartiles[2] && x[i] <= quartiles[3]) {\n      ++bins[1];\n    } else {\n      ++bins[2];\n    }\n  }\n}",
            "for (double const& item : x) {\n    if (item < 0.25) {\n      bins[0] += 1;\n    } else if (item < 0.5) {\n      bins[1] += 1;\n    } else if (item < 0.75) {\n      bins[2] += 1;\n    } else if (item < 1) {\n      bins[3] += 1;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "if (x.size() <= 0) {\n\t\treturn;\n\t}\n\n\tstd::sort(x.begin(), x.end());\n\tsize_t len = x.size();\n\tsize_t middle = len/2;\n\tsize_t lower = middle/2;\n\tsize_t upper = middle + lower;\n\n\t// if odd number of elements, set middle as the average of the two middle elements\n\tif (len % 2 == 1) {\n\t\tmiddle = (x[middle] + x[middle+1]) / 2;\n\t\tupper = middle + lower;\n\t}\n\n\tdouble lowerBound = 0;\n\tdouble upperBound = 0.25;\n\tbins[0] = 0;\n\tfor (size_t i = 0; i < len; i++) {\n\t\tif (x[i] > lowerBound && x[i] <= upperBound) {\n\t\t\tbins[0]++;\n\t\t}\n\t\tlowerBound = upperBound;\n\t\tupperBound = upperBound + 0.25;\n\t}\n\n\tlowerBound = 0.25;\n\tupperBound = 0.5;\n\tbins[1] = 0;\n\tfor (size_t i = 0; i < len; i++) {\n\t\tif (x[i] > lowerBound && x[i] <= upperBound) {\n\t\t\tbins[1]++;\n\t\t}\n\t\tlowerBound = upperBound;\n\t\tupperBound = upperBound + 0.25;\n\t}\n\n\tlowerBound = 0.5;\n\tupperBound = 0.75;\n\tbins[2] = 0;\n\tfor (size_t i = 0; i < len; i++) {\n\t\tif (x[i] > lowerBound && x[i] <= upperBound) {\n\t\t\tbins[2]++;\n\t\t}\n\t\tlowerBound = upperBound;\n\t\tupperBound = upperBound + 0.25;\n\t}\n\n\tlowerBound = 0.75;\n\tupperBound = 1;\n\tbins[3] = 0;\n\tfor (size_t i = 0; i < len; i++) {\n\t\tif (x[i] > lowerBound && x[i] <= upperBound) {\n\t\t\tbins[3]++;\n\t\t}\n\t\tlowerBound = upperBound;\n\t\tupperBound = upperBound + 0.25;\n\t}\n}",
            "if (x.empty()) {\n        return;\n    }\n\n    for (double xi : x) {\n        if (xi >= 0 && xi <= 1) {\n            bins[0]++;\n        } else if (xi > 1 && xi <= 2) {\n            bins[1]++;\n        } else if (xi > 2 && xi <= 3) {\n            bins[2]++;\n        } else if (xi > 3 && xi <= 4) {\n            bins[3]++;\n        } else {\n            std::cout << \"Error: Quartile out of range. Quartiles are in [0, 1], [1, 2], [2, 3], and [3, 4].\" << std::endl;\n        }\n    }\n}",
            "// your code goes here\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    size_t i;\n    for(i = 0; i < x.size(); i++){\n        if((x[i] >= 0.0 && x[i] < 0.25) || (x[i] >= 0.25 && x[i] < 0.50) || (x[i] >= 0.50 && x[i] < 0.75) || (x[i] >= 0.75 && x[i] <= 1.0)){\n            bins[x[i]*4]++;\n        }\n    }\n}",
            "// your code goes here\n}",
            "// TODO: YOUR CODE HERE\n\n    // Your code here\n}",
            "// YOUR CODE HERE\n}",
            "for (auto val : x) {\n        double tmp = (val - std::floor(val));\n        if (tmp < 0.25) {\n            ++bins[0];\n        } else if (tmp < 0.5) {\n            ++bins[1];\n        } else if (tmp < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "//TODO: Implement this function\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (double xi : x) {\n    if (xi < 0.25)\n      bins[0]++;\n    else if (xi < 0.5)\n      bins[1]++;\n    else if (xi < 0.75)\n      bins[2]++;\n    else\n      bins[3]++;\n  }\n}",
            "/* TODO */\n    std::sort(x.begin(), x.end());\n    for (auto &i : bins)\n        i = 0;\n    size_t low = 0;\n    size_t high = x.size() - 1;\n    size_t mid;\n    for (auto &i : bins) {\n        mid = (high + low) / 2;\n        if (x[mid] <= 0.25) {\n            low = mid + 1;\n        } else if (x[mid] <= 0.5) {\n            low = mid;\n        } else if (x[mid] <= 0.75) {\n            high = mid;\n        } else {\n            high = mid - 1;\n        }\n        i++;\n    }\n}",
            "for (double n : x) {\n        if (n >= 0 && n < 0.25) {\n            bins[0]++;\n        }\n        else if (n >= 0.25 && n < 0.5) {\n            bins[1]++;\n        }\n        else if (n >= 0.5 && n < 0.75) {\n            bins[2]++;\n        }\n        else if (n >= 0.75 && n <= 1) {\n            bins[3]++;\n        }\n    }\n}",
            "// your code here\n  for(size_t i=0; i<x.size(); ++i)\n    {\n      if(x[i] < 0.25)\n\t++bins[0];\n      else if(x[i] < 0.5)\n\t++bins[1];\n      else if(x[i] < 0.75)\n\t++bins[2];\n      else\n\t++bins[3];\n    }\n}",
            "for (auto i = 0; i < x.size(); i++) {\n    if (x[i] >= 0.0 && x[i] < 0.25) {\n      bins[0] += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.50) {\n      bins[1] += 1;\n    } else if (x[i] >= 0.50 && x[i] < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "auto n = x.size();\n  std::sort(x.begin(), x.end());\n  double quarter;\n  for (size_t i = 0; i < n; ++i) {\n    quarter = 1.0 * (i+1) / (n+1);\n    if (quarter < 0.25)\n      ++bins[0];\n    else if (quarter < 0.5)\n      ++bins[1];\n    else if (quarter < 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "for (double element : x) {\n        if (element < 0.25) {\n            bins[0] += 1;\n        } else if (element < 0.5) {\n            bins[1] += 1;\n        } else if (element < 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// TODO: your code here\n}",
            "// TODO: implement this function\n    //...\n    double count = 0;\n    for (auto i : x) {\n        if (i >= 0 && i < 0.25)\n            ++count;\n        else if (i >= 0.25 && i < 0.5)\n            ++count;\n        else if (i >= 0.5 && i < 0.75)\n            ++count;\n        else if (i >= 0.75 && i <= 1)\n            ++count;\n    }\n    std::cout << \"count = \" << count << std::endl;\n    bins[0] = count;\n    bins[1] = count;\n    bins[2] = count;\n    bins[3] = count;\n    return;\n}",
            "// INSERT YOUR CODE HERE\n  // bins = [2, 1, 2, 2]\n\n  // Count the number of elements in each interval\n  for (const auto& e : x) {\n    if (e < 0.25) {\n      ++bins[0];\n    } else if (e < 0.5) {\n      ++bins[1];\n    } else if (e < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// YOUR CODE HERE\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for (double i : x) {\n    if (i <= 0.25) {\n      bins[0] += 1;\n    } else if (i <= 0.5) {\n      bins[1] += 1;\n    } else if (i <= 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "}",
            "// Fill in this function.\n}",
            "size_t n = x.size();\n\n    std::vector<double> x_sorted = x;\n    sort(x_sorted.begin(), x_sorted.end());\n\n    size_t count = 0;\n    double fractional_part = 0.0;\n\n    for (auto val : x_sorted) {\n        fractional_part = std::modf(val, &count);\n        if (fractional_part < 0.25)\n            ++bins[0];\n        else if (fractional_part < 0.50)\n            ++bins[1];\n        else if (fractional_part < 0.75)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "for (auto xi : x) {\n        double v = xi / 0.25;\n        size_t bin = 0;\n        if (v < 0.75) {\n            if (v < 0.5) {\n                bin = 0;\n            } else if (v < 0.75) {\n                bin = 1;\n            } else {\n                bin = 3;\n            }\n        } else {\n            bin = 2;\n        }\n        bins[bin] += 1;\n    }\n}",
            "bins = {0, 0, 0, 0};\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double frac = x[i] - std::floor(x[i]);\n    if (frac <= 0.25)\n      ++bins[0];\n    else if (frac <= 0.5)\n      ++bins[1];\n    else if (frac <= 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "// Your code goes here\n}",
            "// TODO: Implement this function.\n}",
            "// TODO: Implement this function.\n}",
            "// Your code goes here\n}",
            "for (double v : x) {\n      size_t i = v / 0.25; // the bin for double v\n      if (v >= 0.5)\n         ++i;\n      if (v >= 0.75)\n         ++i;\n      ++bins[i];\n   }\n}",
            "// YOUR CODE HERE\n    bins[0] = std::count_if(x.begin(), x.end(), [&x](double val) {return val >= 0.0 && val < 0.25;});\n    bins[1] = std::count_if(x.begin(), x.end(), [&x](double val) {return val >= 0.25 && val < 0.5;});\n    bins[2] = std::count_if(x.begin(), x.end(), [&x](double val) {return val >= 0.5 && val < 0.75;});\n    bins[3] = std::count_if(x.begin(), x.end(), [&x](double val) {return val >= 0.75 && val < 1.0;});\n}",
            "for (double d : x) {\n    if (d >= 0.75) {\n      ++bins[3];\n    } else if (d >= 0.5) {\n      ++bins[2];\n    } else if (d >= 0.25) {\n      ++bins[1];\n    } else {\n      ++bins[0];\n    }\n  }\n}",
            "size_t length = x.size();\n    bins.fill(0);\n    for (size_t i = 0; i < length; i++) {\n        if (x[i] < 0.25) {\n            bins[0] += 1;\n        }\n        else if (x[i] < 0.5) {\n            bins[1] += 1;\n        }\n        else if (x[i] < 0.75) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "assert(bins.size() == 4);\n\n  bins.fill(0);\n  // Count the number of values in each bin.\n  for (double v : x) {\n    int bin = (v < 0.5? 0 : v < 0.75? 1 : v < 1.0? 2 : 3);\n    ++bins[bin];\n  }\n}",
            "std::vector<double> xs = x;\n  std::sort(xs.begin(), xs.end());\n\n  for (size_t i = 0; i < xs.size(); i++) {\n    double val = xs[i];\n    if (val < 0.25) {\n      bins[0]++;\n    } else if (val < 0.5) {\n      bins[1]++;\n    } else if (val < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "size_t n = x.size();\n  if(n == 0) {\n    throw std::invalid_argument(\"x is empty\");\n  }\n  bins.fill(0);\n  for(size_t i = 0; i < n; ++i) {\n    if(x[i] < 0.0 || x[i] >= 1.0) {\n      throw std::invalid_argument(\"x contains a value outside of [0, 1)\");\n    }\n    double d = x[i];\n    if(d < 0.25) {\n      ++bins[0];\n    } else if(d < 0.50) {\n      ++bins[1];\n    } else if(d < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "double const cutoffs[4] = { 0.25, 0.5, 0.75, 1 };\n    bins = std::array<size_t, 4>();\n    for (double const& value : x) {\n        if (value >= 0) {\n            auto const index = std::upper_bound(std::begin(cutoffs), std::end(cutoffs), value / 4.0) - std::begin(cutoffs);\n            bins[index]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n\n  return;\n}",
            "int n = x.size();\n   if (n < 4) {\n       std::cout << \"Error! There are less than four elements in the array\" << std::endl;\n       std::cout << \"n: \" << n << std::endl;\n       return;\n   }\n\n   // Sort the vector x.\n   std::sort(x.begin(), x.end());\n\n   // Compute the value of the quartile given by the index j\n   // in the array x. For example, if j is 1, then the value\n   // of the first quartile is the median.\n   auto getQuartile = [&x](size_t j) {\n       size_t index = static_cast<size_t>((j + 1) * (x.size() + 1) / 4);\n       if (index == x.size() + 1) {\n           index -= 1;\n       }\n       return x[index];\n   };\n\n   // Count the number of elements that have a fractional part\n   // in the range [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (int j = 0; j < n; ++j) {\n       // Find which quartile the element j belongs to.\n       double element = x[j];\n       size_t quartileIndex;\n       if (element < getQuartile(0)) {\n           quartileIndex = 0;\n       }\n       else if (element < getQuartile(1)) {\n           quartileIndex = 1;\n       }\n       else if (element < getQuartile(2)) {\n           quartileIndex = 2;\n       }\n       else {\n           quartileIndex = 3;\n       }\n\n       // Increment the count of the corresponding quartile.\n       ++bins[quartileIndex];\n   }\n}",
            "size_t i;\n    for (i = 0; i < x.size(); i++) {\n        if (x[i] < 0 || x[i] > 1) {\n            throw std::invalid_argument(\"All elements of x must be between 0 and 1.\");\n        }\n        if (x[i] <= 0.25) {\n            bins[0]++;\n        } else if (x[i] <= 0.5) {\n            bins[1]++;\n        } else if (x[i] <= 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "assert(x.size() > 0);\n    bins[0] = 0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        double current_value = x[i];\n        if (current_value < 0.25) {\n            ++bins[0];\n        }\n        else if (current_value < 0.5) {\n            ++bins[1];\n        }\n        else if (current_value < 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "// FIXME: implement me!\n}",
            "double const quartileSize = 0.25;\n    double const quartileOne = quartileSize;\n    double const quartileTwo = 2 * quartileSize;\n    double const quartileThree = 3 * quartileSize;\n    size_t const length = x.size();\n    for (size_t i = 0; i < length; ++i) {\n        if (x[i] < quartileOne) {\n            ++bins[0];\n        } else if (x[i] < quartileTwo) {\n            ++bins[1];\n        } else if (x[i] < quartileThree) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "size_t low, high, i;\n    low = 0; high = x.size() - 1;\n    for (i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.25 * (x[high] - x[low]) + x[low]) {\n            low = i;\n            if (x[high] - x[low] < 0.25) {\n                high = i;\n                break;\n            }\n        }\n    }\n    bins[0] = high - low + 1;\n\n    low = 0; high = x.size() - 1;\n    for (i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.5 * (x[high] - x[low]) + x[low]) {\n            low = i;\n            if (x[high] - x[low] < 0.5) {\n                high = i;\n                break;\n            }\n        }\n    }\n    bins[1] = high - low + 1;\n\n    low = 0; high = x.size() - 1;\n    for (i = 0; i < x.size(); i++) {\n        if (x[i] >= 0.75 * (x[high] - x[low]) + x[low]) {\n            low = i;\n            if (x[high] - x[low] < 0.75) {\n                high = i;\n                break;\n            }\n        }\n    }\n    bins[2] = high - low + 1;\n\n    low = 0; high = x.size() - 1;\n    for (i = 0; i < x.size(); i++) {\n        if (x[i] >= 1.0 * (x[high] - x[low]) + x[low]) {\n            low = i;\n            if (x[high] - x[low] < 1.0) {\n                high = i;\n                break;\n            }\n        }\n    }\n    bins[3] = high - low + 1;\n}",
            "// TODO: implement this function\n}",
            "}",
            "bins = {};\n  double const n = x.size();\n\n  for (auto const& xi : x) {\n    if (xi < 0 || xi > 1) {\n      continue;\n    }\n    size_t const q = static_cast<size_t>(floor((xi * 4.0)));\n    bins[q]++;\n  }\n}",
            "assert(x.size() > 0);\n    assert(x.size() < 1000000);\n    std::array<double, 4> thresholds = {0.25, 0.5, 0.75, 1};\n    for (size_t i = 0; i < x.size(); ++i) {\n        for (size_t j = 0; j < thresholds.size(); ++j) {\n            if (x[i] >= thresholds[j] * x[i]) {\n                bins[j] += 1;\n            }\n        }\n    }\n}",
            "// TODO: Your code goes here\n}",
            "/*\n  YOUR CODE HERE\n  */\n}",
            "assert(bins.size() == 4);\n  for (auto const &e : x) {\n    if (e < 0.25) {\n      bins[0]++;\n    } else if (e < 0.50) {\n      bins[1]++;\n    } else if (e < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Compute the number of elements in each bin. Store the results in `bins`.\n  std::vector<double> vec = x;\n  std::sort(vec.begin(), vec.end());\n  for(size_t i = 0; i < vec.size(); i++){\n    if(vec[i] < 0.25)\n      bins[0]++;\n    else if(vec[i] >= 0.25 && vec[i] < 0.5)\n      bins[1]++;\n    else if(vec[i] >= 0.5 && vec[i] < 0.75)\n      bins[2]++;\n    else\n      bins[3]++;\n  }\n}",
            "for (double value: x) {\n    if (value <= 0.25) {\n      ++bins[0];\n    } else if (value <= 0.5) {\n      ++bins[1];\n    } else if (value <= 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// Fill in your code here.\n}",
            "bins.fill(0);\n  for (auto const& i : x) {\n    if (i < 0.25) {\n      ++bins[0];\n    } else if (i < 0.5) {\n      ++bins[1];\n    } else if (i < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "std::size_t count = 0;\n  for (double const &item : x) {\n    if (0 < item && item < 0.25) {\n      ++count;\n    } else if (0.25 <= item && item < 0.5) {\n      ++count;\n    } else if (0.5 <= item && item < 0.75) {\n      ++count;\n    } else if (0.75 <= item && item <= 1) {\n      ++count;\n    }\n  }\n  bins[0] = count;\n\n  count = 0;\n  for (double const &item : x) {\n    if (0 < item && item < 0.5) {\n      ++count;\n    } else if (0.5 <= item && item < 0.75) {\n      ++count;\n    } else if (0.75 <= item && item <= 1) {\n      ++count;\n    }\n  }\n  bins[1] = count;\n\n  count = 0;\n  for (double const &item : x) {\n    if (0 < item && item < 0.75) {\n      ++count;\n    } else if (0.75 <= item && item <= 1) {\n      ++count;\n    }\n  }\n  bins[2] = count;\n\n  count = 0;\n  for (double const &item : x) {\n    if (item <= 1) {\n      ++count;\n    }\n  }\n  bins[3] = count;\n}",
            "// TODO: write implementation here\n}",
            "// Your code here\n  double i;\n  for(i = 0; i < 4; i++) {\n    bins[i] = 0;\n  }\n\n  for(auto element : x) {\n    if(element >= 0 && element < 0.25) {\n      bins[0]++;\n    } else if(element >= 0.25 && element < 0.5) {\n      bins[1]++;\n    } else if(element >= 0.5 && element < 0.75) {\n      bins[2]++;\n    } else if(element >= 0.75 && element <= 1) {\n      bins[3]++;\n    }\n  }\n\n  for(auto element : bins) {\n    std::cout << element << \" \";\n  }\n\n  std::cout << \"\\n\";\n}",
            "// YOUR CODE HERE\n  bins.fill(0);\n  size_t N = x.size();\n  for (auto i = 0; i < N; i++) {\n    if (x[i] < 0.25 * N) {\n      bins[0] = bins[0] + 1;\n    } else if (x[i] < 0.5 * N) {\n      bins[1] = bins[1] + 1;\n    } else if (x[i] < 0.75 * N) {\n      bins[2] = bins[2] + 1;\n    } else {\n      bins[3] = bins[3] + 1;\n    }\n  }\n}",
            "bins.fill(0); // Reset to zeros for each call\n  double const cutoffs[] = {0.0, 0.25, 0.5, 0.75, 1.0};\n\n  for (double value : x) {\n    for (size_t i = 0; i < 5; i++) {\n      if (value <= cutoffs[i]) {\n        bins[i]++;\n        break;\n      }\n    }\n  }\n}",
            "/* Enter your code here */\n  std::sort(x.begin(), x.end());\n  std::vector<double>::const_iterator it = x.begin();\n  size_t counter = 0;\n  size_t max = 0;\n  while(it!= x.end()){\n    if((*it) >= 0 && (*it) < 0.25){\n      bins[0] += 1;\n      max = std::max(bins[0], max);\n    } else if((*it) >= 0.25 && (*it) < 0.5){\n      bins[1] += 1;\n      max = std::max(bins[1], max);\n    } else if((*it) >= 0.5 && (*it) < 0.75){\n      bins[2] += 1;\n      max = std::max(bins[2], max);\n    } else if((*it) >= 0.75 && (*it) < 1){\n      bins[3] += 1;\n      max = std::max(bins[3], max);\n    } else {\n      std::cout << \"Error\" << std::endl;\n    }\n    it++;\n  }\n  for(int i = 0; i < 4; i++){\n    std::cout << bins[i] << \" \";\n  }\n  std::cout << std::endl;\n  if(max > 0) {\n    for(int i = 0; i < 4; i++){\n      std::cout << ((double)(bins[i])/max) << \" \";\n    }\n    std::cout << std::endl;\n  }\n}",
            "double const step = 0.25;\n  size_t const n = x.size();\n  for (size_t i = 0; i < n; ++i) {\n    double const d = x[i] - (int)x[i];\n    if (d <= step) {\n      ++bins[0];\n    } else if (d <= 2*step) {\n      ++bins[1];\n    } else if (d <= 3*step) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO: replace the following dummy code with your implementation\n\n  size_t n = x.size();\n\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] < 0.25) {\n      bins[0] += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1] += 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n\n  return;\n}",
            "double const thresh1 = 0.25;\n   double const thresh2 = 0.5;\n   double const thresh3 = 0.75;\n\n   for (double elem : x) {\n      double fractional = std::fmod(elem, 1);\n      if (fractional < thresh1)\n         bins[0]++;\n      else if (fractional >= thresh1 && fractional < thresh2)\n         bins[1]++;\n      else if (fractional >= thresh2 && fractional < thresh3)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "for (double const &value : x) {\n\t\tif (value > 0 && value <= 0.25) {\n\t\t\t++bins[0];\n\t\t} else if (value > 0.25 && value <= 0.5) {\n\t\t\t++bins[1];\n\t\t} else if (value > 0.5 && value <= 0.75) {\n\t\t\t++bins[2];\n\t\t} else {\n\t\t\t++bins[3];\n\t\t}\n\t}\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: implement this function\n}",
            "double q1 = 0.25;\n    double q2 = 0.5;\n    double q3 = 0.75;\n\n    for (double value : x) {\n        if (value < q1) {\n            bins[0]++;\n        }\n        else if (value < q2) {\n            bins[1]++;\n        }\n        else if (value < q3) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement the function\n}",
            "for (double i : x) {\n\t\tif (0 <= i && i < 0.25)\n\t\t\tbins[0]++;\n\t\telse if (0.25 <= i && i < 0.5)\n\t\t\tbins[1]++;\n\t\telse if (0.5 <= i && i < 0.75)\n\t\t\tbins[2]++;\n\t\telse if (0.75 <= i && i <= 1)\n\t\t\tbins[3]++;\n\t}\n}",
            "bins = {0, 0, 0, 0};\n  double step = 1.0 / 4.0;\n  double min = std::min_element(x.begin(), x.end()) - x.begin();\n  double max = std::max_element(x.begin(), x.end()) - x.begin();\n  for (double y : x) {\n    int i = std::floor((y - min) / (max - min) * 4);\n    if (0 <= i && i <= 3) {\n      if (y < (i + 1) * step)\n        ++bins[i];\n    }\n  }\n}",
            "bins = std::array<size_t, 4>{0,0,0,0};\n   for (double xi : x) {\n      size_t bin = std::lround(xi * 4.0);\n      if (bin < 4)\n         bins[bin] += 1;\n   }\n}",
            "// YOUR CODE HERE\n    throw std::runtime_error(\"TODO: implement this function\");\n}",
            "for (auto const &xx: x) {\n    if (xx < 0.25) {\n      ++bins[0];\n    } else if (xx < 0.5) {\n      ++bins[1];\n    } else if (xx < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n  size_t n = x.size();\n  for(int i=0; i<n; i++)\n  {\n    if(x[i] < 0.25)\n    {\n      bins[0]++;\n    }\n    else if(x[i] < 0.5)\n    {\n      bins[1]++;\n    }\n    else if(x[i] < 0.75)\n    {\n      bins[2]++;\n    }\n    else\n    {\n      bins[3]++;\n    }\n  }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for(double xi : x){\n        if(xi <= 0.25){\n            bins[0] += 1;\n        }\n        if(xi <= 0.5){\n            bins[1] += 1;\n        }\n        if(xi <= 0.75){\n            bins[2] += 1;\n        }\n        if(xi <= 1){\n            bins[3] += 1;\n        }\n    }\n}",
            "// TODO: Write your code here\n    size_t N = x.size();\n    double Q1 = 0;\n    double Q2 = 0;\n    double Q3 = 0;\n    double Q4 = 0;\n    std::sort(x.begin(), x.end());\n    // If we have an even number of elements, \n    // we can simply take the average of two middle elements \n    // to find the median. \n    if (N % 2 == 0) {\n        Q2 = (x[N / 2 - 1] + x[N / 2]) / 2;\n    }\n    else {\n        Q2 = x[N / 2];\n    }\n\n    // If we have an odd number of elements, \n    // we take the middle element to be the median\n    if (N % 4 == 0) {\n        Q1 = x[(N / 4 - 1) / 2];\n        Q3 = x[(N / 4 + N / 2) / 2];\n        Q4 = x[(N / 4 + N / 2 + N / 4) / 2];\n    }\n    else if (N % 4 == 1) {\n        Q1 = x[N / 4 - 1];\n        Q3 = x[N / 2];\n        Q4 = x[N / 4 + N / 2];\n    }\n    else if (N % 4 == 2) {\n        Q1 = x[(N / 4 - 1) / 2];\n        Q3 = x[N / 2];\n        Q4 = x[(N / 4 + N / 2) / 2];\n    }\n    else {\n        Q1 = x[N / 4 - 1];\n        Q3 = x[N / 2];\n        Q4 = x[(N / 4 + N / 2) / 2];\n    }\n\n    size_t i = 0;\n    for (i = 0; i < N; i++) {\n        if ((x[i] >= Q1) && (x[i] < Q2)) {\n            bins[0] += 1;\n        }\n        else if ((x[i] >= Q2) && (x[i] < Q3)) {\n            bins[1] += 1;\n        }\n        else if ((x[i] >= Q3) && (x[i] <= Q4)) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n\n}",
            "bins.fill(0);\n    for (double value : x) {\n        if (value < 0.25) {\n            ++bins[0];\n        } else if (value < 0.5) {\n            ++bins[1];\n        } else if (value < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "assert(bins.size() == 4);\n\n  for (double value : x) {\n    if (value < 0 || value > 100) {\n      throw std::runtime_error(\"Error: the value \" + std::to_string(value) + \" is out of range [0, 100]\");\n    }\n  }\n\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0.25) {\n      bins[0] += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      bins[1] += 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      bins[2] += 1;\n    } else if (x[i] >= 0.75) {\n      bins[3] += 1;\n    }\n  }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (auto const& v : x) {\n        auto value = (v - floor(v));\n\n        if (0 <= value && value < 0.25) {\n            bins[0]++;\n        } else if (0.25 <= value && value < 0.5) {\n            bins[1]++;\n        } else if (0.5 <= value && value < 0.75) {\n            bins[2]++;\n        } else if (0.75 <= value && value < 1) {\n            bins[3]++;\n        }\n    }\n}",
            "if (x.size() < 4) {\n\t\tthrow std::length_error(\"Error: x.size() < 4\");\n\t}\n\t\n\tfor (double d: x) {\n\t\tif (d < 0 || d > 1) {\n\t\t\tthrow std::out_of_range(\"Error: d is outside of range [0, 1]\");\n\t\t}\n\t}\n\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tdouble d = x[i];\n\t\tif (d >= 0 && d < 0.25) {\n\t\t\t++bins[0];\n\t\t}\n\t\telse if (d >= 0.25 && d < 0.5) {\n\t\t\t++bins[1];\n\t\t}\n\t\telse if (d >= 0.5 && d < 0.75) {\n\t\t\t++bins[2];\n\t\t}\n\t\telse {\n\t\t\t++bins[3];\n\t\t}\n\t}\n}",
            "bins = {0, 0, 0, 0};\n  for (double val : x) {\n    if (val < 0.25)\n      ++bins[0];\n    else if (val < 0.5)\n      ++bins[1];\n    else if (val < 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "double max = *std::max_element(x.begin(), x.end());\n  double step = max / 4.0;\n\n  for (auto value : x) {\n    if (value < step) {\n      bins[0] += 1;\n    } else if (value < 2 * step) {\n      bins[1] += 1;\n    } else if (value < 3 * step) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "/* Add your code here */\n}",
            "size_t n = x.size();\n    bins.fill(0);\n    for (size_t i = 0; i < n; i++) {\n        double fraction = x[i] - (int)x[i];\n        if (fraction <= 0.25) {\n            bins[0]++;\n        }\n        else if (fraction <= 0.5) {\n            bins[1]++;\n        }\n        else if (fraction <= 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "size_t n = x.size();\n    for (size_t i = 0; i < n; ++i) {\n        double value = x[i];\n        if (value < 0.25) {\n            ++bins[0];\n        } else if (value < 0.5) {\n            ++bins[1];\n        } else if (value < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO: Implement this function\n    bins[0]=0;\n    bins[1]=0;\n    bins[2]=0;\n    bins[3]=0;\n    for (size_t i = 0; i < x.size(); ++i) {\n        if (x[i] >= 0.0 && x[i] <= 0.25) {\n            bins[0]++;\n        } else if (x[i] > 0.25 && x[i] <= 0.5) {\n            bins[1]++;\n        } else if (x[i] > 0.5 && x[i] <= 0.75) {\n            bins[2]++;\n        } else if (x[i] > 0.75 && x[i] <= 1.0) {\n            bins[3]++;\n        }\n    }\n}",
            "//TODO: YOUR CODE HERE\n}",
            "for (auto const& val : x) {\n        double fract = std::fmod(val, 1);\n        if (fract < 0.25) {\n            ++bins[0];\n        } else if (fract < 0.5) {\n            ++bins[1];\n        } else if (fract < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// write code here\n\t// 0.25 <= x < 0.5\n\tsize_t count_0_25 = 0;\n\t// 0.5 <= x < 0.75\n\tsize_t count_0_5 = 0;\n\t// 0.75 <= x <= 1\n\tsize_t count_0_75 = 0;\n\t// 0 <= x < 0.25\n\tsize_t count_0 = 0;\n\t// 1 <= x < 1.25\n\tsize_t count_1 = 0;\n\t// 1.25 <= x < 1.5\n\tsize_t count_1_25 = 0;\n\t// 1.5 <= x < 1.75\n\tsize_t count_1_5 = 0;\n\t// 1.75 <= x < 2\n\tsize_t count_1_75 = 0;\n\t// 2 <= x <= 2.25\n\tsize_t count_2 = 0;\n\t// 2.25 <= x <= 2.5\n\tsize_t count_2_25 = 0;\n\t// 2.5 <= x <= 2.75\n\tsize_t count_2_5 = 0;\n\t// 2.75 <= x <= 3\n\tsize_t count_2_75 = 0;\n\t// 3 <= x <= 3.25\n\tsize_t count_3 = 0;\n\t// 3.25 <= x <= 3.5\n\tsize_t count_3_25 = 0;\n\t// 3.5 <= x <= 3.75\n\tsize_t count_3_5 = 0;\n\t// 3.75 <= x <= 4\n\tsize_t count_3_75 = 0;\n\t// 4 <= x <= 4.25\n\tsize_t count_4 = 0;\n\t// 4.25 <= x <= 4.5\n\tsize_t count_4_25 = 0;\n\t// 4.5 <= x <= 4.75\n\tsize_t count_4_5 = 0;\n\t// 4.75 <= x <= 5\n\tsize_t count_4_75 = 0;\n\t// 5 <= x <= 5.25\n\tsize_t count_5 = 0;\n\t// 5.25 <= x <= 5.5\n\tsize_t count_5_25 = 0;\n\t// 5.5 <= x <= 5.75\n\tsize_t count_5_5 = 0;\n\t// 5.75 <= x <= 6\n\tsize_t count_5_75 = 0;\n\t// 6 <= x <= 6.25\n\tsize_t count_6 = 0;\n\t// 6.25 <= x <= 6.5\n\tsize_t count_6_25 = 0;\n\t// 6.5 <= x <= 6.75\n\tsize_t count_6_5 = 0;\n\t// 6.75 <= x <= 7\n\tsize_t count_6_75 = 0;\n\t// 7 <= x <= 7.25\n\tsize_t count_7 = 0;\n\t// 7.25 <= x <= 7.5\n\tsize_t count_7_25 = 0;\n\t// 7.5 <= x <= 7.75\n\tsize_t count_7_5 = 0;\n\t// 7.75 <= x <= 8\n\tsize_t count_7_75 = 0;\n\t// 8 <= x <= 8.25\n\tsize_t count_8 = 0;\n\t// 8.25 <= x <= 8.5\n\tsize_t count_8_25 = 0;\n\t// 8.5 <= x <= 8.75\n\tsize_t count_8_5 = 0;\n\t// 8.75 <= x <= 9\n\tsize_t count_8_75 = 0;\n\t// 9 <= x <= 9",
            "// TODO\n}",
            "for (double xi : x) {\n        if (xi >= 0.25 && xi < 0.5)\n            ++bins[0];\n        else if (xi >= 0.5 && xi < 0.75)\n            ++bins[1];\n        else if (xi >= 0.75 && xi <= 1)\n            ++bins[2];\n        else\n            ++bins[3];\n    }\n}",
            "// your code here\n  bins[0] = std::count_if(x.begin(), x.end(), [](const double &a) {\n    return a >= 0 && a < 0.25;\n  });\n  bins[1] = std::count_if(x.begin(), x.end(), [](const double &a) {\n    return a >= 0.25 && a < 0.5;\n  });\n  bins[2] = std::count_if(x.begin(), x.end(), [](const double &a) {\n    return a >= 0.5 && a < 0.75;\n  });\n  bins[3] = std::count_if(x.begin(), x.end(), [](const double &a) {\n    return a >= 0.75 && a <= 1;\n  });\n}",
            "/*\n        Fill in this function\n    */\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // Write your code here.\n}",
            "// YOUR CODE HERE\n}",
            "size_t i = 0;\n  for (double num : x) {\n    if (num >= 0 && num <= 0.25) {\n      bins[0]++;\n    }\n    else if (num > 0.25 && num <= 0.5) {\n      bins[1]++;\n    }\n    else if (num > 0.5 && num <= 0.75) {\n      bins[2]++;\n    }\n    else if (num > 0.75 && num <= 1) {\n      bins[3]++;\n    }\n  }\n}",
            "bins.fill(0);\n    for (double val : x) {\n        int quartile = 0;\n        if (val > 0 && val < 0.25) {\n            quartile = 0;\n        }\n        else if (val >= 0.25 && val < 0.5) {\n            quartile = 1;\n        }\n        else if (val >= 0.5 && val < 0.75) {\n            quartile = 2;\n        }\n        else if (val >= 0.75 && val <= 1) {\n            quartile = 3;\n        }\n        bins[quartile]++;\n    }\n}",
            "for (auto xi : x) {\n    double xq = std::floor(xi / 0.25);\n    bins[static_cast<size_t>(xq)]++;\n  }\n}",
            "size_t n = x.size();\n  size_t m = n/4;\n  size_t count = 0;\n  for (size_t i = 0; i < n; ++i) {\n    double val = x[i];\n    if (val >= 0 && val < 0.25) {\n      ++count;\n    } else if (val >= 0.25 && val < 0.50) {\n      ++count;\n    } else if (val >= 0.50 && val < 0.75) {\n      ++count;\n    } else if (val >= 0.75 && val <= 1) {\n      ++count;\n    }\n  }\n  for (size_t i = 0; i < 4; ++i) {\n    bins[i] = count;\n  }\n}",
            "// TODO: implement me!\n}",
            "double sum = 0.0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    double x_i = x[i];\n    if (x_i < 0.0 || x_i > 10.0) {\n      throw std::invalid_argument(\"Error: the value of x_i is not in [0, 10]\");\n    }\n    // Compute the index of the bin.\n    size_t index = static_cast<size_t>(x_i / 0.25);\n    // Increment the count of that bin.\n    bins[index]++;\n    // Sum up the elements of x.\n    sum += x_i;\n  }\n  // Compute the mean of x.\n  double mean = sum / x.size();\n\n  // Compute the difference between each element of x and the mean.\n  for (size_t i = 0; i < x.size(); ++i) {\n    double diff = x[i] - mean;\n    // Compute the index of the bin.\n    size_t index = static_cast<size_t>(diff / 0.25);\n    // Increment the count of that bin.\n    bins[index]++;\n  }\n\n  // Compute the ratio of the count of each bin and the number of elements in x.\n  for (size_t i = 0; i < bins.size(); ++i) {\n    double bin_size = (i + 1) * 0.25;\n    // double bin_size = 0.25 * (i + 1);\n    bins[i] = static_cast<size_t>(bins[i] / (x.size() * bin_size));\n  }\n}",
            "size_t i;\n    for (i = 0; i < x.size(); ++i) {\n        double x_i = x[i];\n        if (x_i < 0.25) {\n            ++bins[0];\n        } else if (x_i < 0.5) {\n            ++bins[1];\n        } else if (x_i < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "bins.fill(0);\n    size_t const N = x.size();\n    for (size_t i = 0; i < N; ++i) {\n        if (x[i] <= 0.25) {\n            ++bins[0];\n        }\n        else if (x[i] <= 0.50) {\n            ++bins[1];\n        }\n        else if (x[i] <= 0.75) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    double step = 0.25;\n    for (double num : x) {\n        if (num <= 0.25) {\n            bins[0] += 1;\n        } else if (num <= 0.5) {\n            bins[1] += 1;\n        } else if (num <= 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n  for (auto const& value: x) {\n    if (value >= 0 && value < 0.25) {\n      bins[0] += 1;\n    } else if (value >= 0.25 && value < 0.5) {\n      bins[1] += 1;\n    } else if (value >= 0.5 && value < 0.75) {\n      bins[2] += 1;\n    } else if (value >= 0.75 && value <= 1) {\n      bins[3] += 1;\n    }\n  }\n}",
            "size_t i = 0;\n    for(auto const& j : x){\n        if(j > 0.25 && j <= 0.5)\n            ++bins[0];\n        else if(j > 0.5 && j <= 0.75)\n            ++bins[1];\n        else if(j > 0.75 && j <= 1)\n            ++bins[2];\n        else if(j > 0 && j < 0.25)\n            ++bins[3];\n        else\n            throw \"Invalid input: elements of x are not in [0, 1]\";\n        ++i;\n    }\n    for(auto& k : bins) {\n        std::cout << k << '\\t';\n    }\n    std::cout << '\\n';\n}",
            "// TODO: implement this function\n  bins.fill(0);\n}",
            "size_t i = 0;\n  while (i < x.size()) {\n    double fraction = x[i] - (int)x[i];\n    if (fraction < 0.25) {\n      bins[0]++;\n    } else if (fraction < 0.5) {\n      bins[1]++;\n    } else if (fraction < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n    i++;\n  }\n}",
            "bins.fill(0);\n  for (auto x_i : x) {\n    if (x_i < 0.25) {\n      bins[0]++;\n    } else if (x_i < 0.5) {\n      bins[1]++;\n    } else if (x_i < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "assert(x.size() > 0);\n  for (size_t i = 0; i < x.size(); i++) {\n    double value = x[i];\n    if (value <= 0.25) {\n      bins[0]++;\n    } else if (value <= 0.5) {\n      bins[1]++;\n    } else if (value <= 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "std::array<double, 4> fracs = {0.25, 0.5, 0.75, 1.0};\n  bins = {0, 0, 0, 0};\n  for (size_t i = 0; i < x.size(); ++i) {\n    for (size_t j = 0; j < fracs.size(); ++j) {\n      if (x[i] < fracs[j]) {\n        bins[j]++;\n        break;\n      }\n    }\n  }\n}",
            "double const h = 0.25;\n    bins = {0, 0, 0, 0};\n    for (double xi : x) {\n        if (0 <= xi && xi < h) {\n            bins[0]++;\n        } else if (h <= xi && xi < 2 * h) {\n            bins[1]++;\n        } else if (2 * h <= xi && xi < 3 * h) {\n            bins[2]++;\n        } else if (3 * h <= xi) {\n            bins[3]++;\n        }\n    }\n}",
            "}",
            "bins[0] = std::count_if(std::begin(x), std::end(x),\n                           [](double v) { return v <= 0.25; });\n   bins[1] = std::count_if(std::begin(x), std::end(x),\n                           [](double v) { return v <= 0.5; });\n   bins[2] = std::count_if(std::begin(x), std::end(x),\n                           [](double v) { return v <= 0.75; });\n   bins[3] = std::count_if(std::begin(x), std::end(x),\n                           [](double v) { return v <= 1; });\n}",
            "double lower_bound = 0.0, upper_bound = 0.0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    lower_bound = x[i] * 0.25;\n    upper_bound = x[i] * 0.75;\n    if (lower_bound == 0) {\n      lower_bound = 0.0;\n    }\n    if (x[i] >= 0.25 && x[i] <= 0.75) {\n      ++bins[1];\n    } else if (lower_bound < 0.25 && upper_bound >= 0.25) {\n      ++bins[0];\n    } else if (lower_bound < 0.5 && upper_bound >= 0.5) {\n      ++bins[1];\n    } else if (lower_bound < 0.75 && upper_bound >= 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO: fill in this function\n    bins.fill(0);\n    for (auto val : x) {\n        if (val < 0.25) {\n            bins[0] += 1;\n        } else if (val >= 0.25 && val < 0.5) {\n            bins[1] += 1;\n        } else if (val >= 0.5 && val < 0.75) {\n            bins[2] += 1;\n        } else if (val >= 0.75 && val < 1) {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n  for (double v : x) {\n    // Insert your code here\n  }\n}",
            "for (auto const &i : x) {\n    if (i <= 0.25) {\n      ++bins[0];\n    } else if (i <= 0.5) {\n      ++bins[1];\n    } else if (i <= 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "bins.fill(0); // initialize array to zeros\n    // write your code here\n}",
            "size_t n = x.size();\n  bins.fill(0);\n\n  for (auto const & element : x) {\n    int index = element * 4;\n    if (index >= 4) {\n      index = 3;\n    }\n    ++bins[index];\n  }\n}",
            "size_t index = 0;\n    for (auto const& element: x) {\n        double value = element / 0.25;\n        if (value >= 0 && value < 1) {\n            bins[index] += 1;\n        }\n        else if (value >= 1 && value < 2) {\n            bins[index + 1] += 1;\n        }\n        else if (value >= 2 && value < 3) {\n            bins[index + 2] += 1;\n        }\n        else {\n            bins[index + 3] += 1;\n        }\n        index += 1;\n    }\n}",
            "for (auto val : x) {\n        if (val <= 0.25) {\n            ++bins[0];\n        } else if (val <= 0.5) {\n            ++bins[1];\n        } else if (val <= 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO\n    double max = 1.0;\n    double min = 0.0;\n    double val;\n    int index = 0;\n    for (int i = 0; i < 4; i++) {\n        bins[i] = 0;\n    }\n    for (std::vector<double>::const_iterator itr = x.begin(); itr!= x.end(); itr++) {\n        val = *itr;\n        if (val >= min && val < 0.25) {\n            index = 0;\n        } else if (val >= 0.25 && val < 0.50) {\n            index = 1;\n        } else if (val >= 0.50 && val < 0.75) {\n            index = 2;\n        } else if (val >= 0.75 && val < 1.0) {\n            index = 3;\n        } else {\n            continue;\n        }\n        bins[index]++;\n    }\n    return;\n}",
            "bins = std::array<size_t, 4>();\n\n  for (double number : x) {\n    if (number < 0 || number > 1) {\n      throw std::invalid_argument(\"Invalid value \" + std::to_string(number) + \". Value should be between 0 and 1.\");\n    }\n\n    if (number < 0.25) {\n      bins[0] += 1;\n    } else if (number < 0.5) {\n      bins[1] += 1;\n    } else if (number < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "std::vector<double> x_fpart = x; // make a copy\n  for (size_t i = 0; i < x.size(); ++i) {\n    if (x[i] >= 0) {\n      x_fpart[i] -= (int) x[i];\n    } else {\n      x_fpart[i] -= (int) x[i] - 1;\n    }\n  }\n  size_t num_elements = x.size();\n\n  // find the index of the first element in the vector x_fpart that is <= 0.25\n  size_t i = 0;\n  while (x_fpart[i] >= 0.25) {\n    i++;\n  }\n\n  // keep track of the number of elements in the current bucket\n  size_t num_in_bucket = 0;\n\n  // keep track of the number of elements in the current bucket\n  size_t bucket = 0;\n\n  // run through the vector x_fpart until you run out of elements\n  while (i < num_elements) {\n    // if the next element is in the current bucket\n    if (x_fpart[i] < 0.5) {\n      num_in_bucket++;\n    } else { // if the next element is not in the current bucket\n      // if there are any elements in the current bucket\n      if (num_in_bucket > 0) {\n        // update the number of elements in the bucket\n        bins[bucket] = num_in_bucket;\n        // increment the bucket counter\n        bucket++;\n      }\n      // set the number of elements in the current bucket to 1\n      num_in_bucket = 1;\n    }\n    i++;\n  }\n}",
            "double const THRESH = 0.25;\n    for (auto const &element : x) {\n        if (element < THRESH) {\n            bins[0] += 1;\n        } else if (element < 0.5 * THRESH) {\n            bins[1] += 1;\n        } else if (element < 0.75 * THRESH) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "for (double value : x) {\n    if (value <= 0.25) {\n      bins[0]++;\n    } else if (value <= 0.5) {\n      bins[1]++;\n    } else if (value <= 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (auto v: x) {\n    if (v < 0.25) {\n      bins[0] += 1;\n    } else if (v >= 0.25 && v < 0.5) {\n      bins[1] += 1;\n    } else if (v >= 0.5 && v < 0.75) {\n      bins[2] += 1;\n    } else if (v >= 0.75 && v <= 1) {\n      bins[3] += 1;\n    }\n  }\n}",
            "bins.fill(0);\n\tfor (double i : x) {\n\t\tif (i >= 0 && i <= 0.25) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (i > 0.25 && i <= 0.5) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (i > 0.5 && i <= 0.75) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (i > 0.75 && i <= 1) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// your code here\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n  bins[0]=0;\n  bins[1]=0;\n  bins[2]=0;\n  bins[3]=0;\n  for(auto const& i: x) {\n    if (i>=0 and i<=0.25)\n      bins[0]++;\n    else if (i>=0.25 and i<=0.50)\n      bins[1]++;\n    else if (i>=0.50 and i<=0.75)\n      bins[2]++;\n    else if (i>=0.75 and i<=1)\n      bins[3]++;\n  }\n}",
            "if (x.size() == 0) {\n\t\treturn;\n\t}\n\n\tbins = {0, 0, 0, 0}; // initialize the bins to all zeros\n\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] >= 0.0 && x[i] < 0.25) {\n\t\t\t++bins[0];\n\t\t}\n\t\telse if (x[i] >= 0.25 && x[i] < 0.5) {\n\t\t\t++bins[1];\n\t\t}\n\t\telse if (x[i] >= 0.5 && x[i] < 0.75) {\n\t\t\t++bins[2];\n\t\t}\n\t\telse if (x[i] >= 0.75 && x[i] <= 1.0) {\n\t\t\t++bins[3];\n\t\t}\n\t}\n\n\treturn;\n}",
            "for (auto const& val: x) {\n    if (val >= 0.75) {\n      ++bins[3];\n    } else if (val >= 0.5) {\n      ++bins[2];\n    } else if (val >= 0.25) {\n      ++bins[1];\n    } else {\n      ++bins[0];\n    }\n  }\n}",
            "// YOUR CODE HERE\n    throw std::runtime_error(\"Not implemented yet\");\n}",
            "bins = std::array<size_t, 4>();\n  for (double element : x) {\n    if (element <= 0.25) {\n      bins[0] += 1;\n    } else if (element <= 0.5) {\n      bins[1] += 1;\n    } else if (element <= 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "/* Fill out code here */\n  bins = {0,0,0,0};\n  double sum_x = 0;\n  for (size_t i = 0; i < x.size(); ++i) {\n    sum_x += x[i];\n    if (x[i] <= 0.25) {\n      ++bins[0];\n    } else if (x[i] <= 0.5) {\n      ++bins[1];\n    } else if (x[i] <= 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n  for (size_t i = 0; i < 4; ++i) {\n    bins[i] = sum_x*bins[i]/x.size();\n  }\n}",
            "for (double const& value: x) {\n        double fraction = value - std::floor(value);\n        if (fraction < 0.25) {\n            bins[0]++;\n        } else if (fraction < 0.5) {\n            bins[1]++;\n        } else if (fraction < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "// your code here\n}",
            "bins = {0, 0, 0, 0};\n\n  for (double xi : x) {\n    if (xi < 0.25) {\n      ++bins[0];\n    } else if (xi < 0.5) {\n      ++bins[1];\n    } else if (xi < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO: Your code here\n   size_t counter = 0;\n   double min = 0;\n   double max = 1;\n   int range = 0;\n   for (double i: x) {\n      if (i >= min && i < (max / 4)) {\n         counter += 1;\n      }\n      else if (i >= (max / 4) && i < (max / 2)) {\n         counter += 2;\n      }\n      else if (i >= (max / 2) && i < ((max * 3) / 4)) {\n         counter += 3;\n      }\n      else {\n         counter += 4;\n      }\n   }\n   bins[0] = counter;\n\n   counter = 0;\n   min = (max / 4);\n   max = (max / 2);\n   for (double i: x) {\n      if (i >= min && i < (max / 4)) {\n         counter += 1;\n      }\n      else if (i >= (max / 4) && i < (max / 2)) {\n         counter += 2;\n      }\n      else if (i >= (max / 2) && i < ((max * 3) / 4)) {\n         counter += 3;\n      }\n      else {\n         counter += 4;\n      }\n   }\n   bins[1] = counter;\n\n   counter = 0;\n   min = (max / 2);\n   max = ((max * 3) / 4);\n   for (double i: x) {\n      if (i >= min && i < (max / 4)) {\n         counter += 1;\n      }\n      else if (i >= (max / 4) && i < (max / 2)) {\n         counter += 2;\n      }\n      else if (i >= (max / 2) && i < ((max * 3) / 4)) {\n         counter += 3;\n      }\n      else {\n         counter += 4;\n      }\n   }\n   bins[2] = counter;\n\n   counter = 0;\n   min = ((max * 3) / 4);\n   max = max;\n   for (double i: x) {\n      if (i >= min && i < (max / 4)) {\n         counter += 1;\n      }\n      else if (i >= (max / 4) && i < (max / 2)) {\n         counter += 2;\n      }\n      else if (i >= (max / 2) && i < ((max * 3) / 4)) {\n         counter += 3;\n      }\n      else {\n         counter += 4;\n      }\n   }\n   bins[3] = counter;\n\n   // for (size_t i: bins) {\n   //    std::cout << i << \" \";\n   // }\n   // std::cout << std::endl;\n\n}",
            "bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (auto &i : x) {\n        if (i < 0.25) {\n            bins[0] = bins[0] + 1;\n        } else if (i < 0.50) {\n            bins[1] = bins[1] + 1;\n        } else if (i < 0.75) {\n            bins[2] = bins[2] + 1;\n        } else {\n            bins[3] = bins[3] + 1;\n        }\n    }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  for (auto xi: x) {\n    if (xi < 0.25) {\n      bins[0] += 1;\n    } else if (xi < 0.5) {\n      bins[1] += 1;\n    } else if (xi < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "for (double const value : x) {\n    if (value >= 0 && value < 0.25) {\n      ++bins[0];\n    } else if (value >= 0.25 && value < 0.5) {\n      ++bins[1];\n    } else if (value >= 0.5 && value < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "for (auto const& i : x) {\n        if (i >= 0.25 && i < 0.5) {\n            bins[1]++;\n        }\n        else if (i >= 0.5 && i < 0.75) {\n            bins[2]++;\n        }\n        else if (i >= 0.75 && i <= 1) {\n            bins[3]++;\n        }\n        else {\n            bins[0]++;\n        }\n    }\n}",
            "for (auto const& xi : x) {\n\t\t// get the fractional part of xi in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n\t\tdouble const quartile = 0.25 * (xi - floor(xi / 0.25));\n\t\tif (quartile < 0.25) {\n\t\t\t++bins[0];\n\t\t} else if (quartile < 0.5) {\n\t\t\t++bins[1];\n\t\t} else if (quartile < 0.75) {\n\t\t\t++bins[2];\n\t\t} else {\n\t\t\t++bins[3];\n\t\t}\n\t}\n}",
            "bins[0] = 0; // count the number of values that are less than or equal to 0.25.\n  bins[1] = 0; // count the number of values that are less than or equal to 0.5.\n  bins[2] = 0; // count the number of values that are less than or equal to 0.75.\n  bins[3] = 0; // count the number of values that are less than or equal to 1.\n\n  for (auto const& elem : x) {\n    if (elem <= 0.25) {\n      bins[0] += 1;\n    } else if (elem <= 0.5) {\n      bins[1] += 1;\n    } else if (elem <= 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] < 0 || x[i] > 100) {\n      std::cout << \"Wrong input value\" << std::endl;\n      return;\n    }\n    double x_val = x[i];\n    double x_remainder = std::fmod(x_val, 1.0);\n\n    if (x_remainder < 0.25) {\n      bins[0]++;\n    } else if (x_remainder < 0.5) {\n      bins[1]++;\n    } else if (x_remainder < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "for (double y: x) {\n        if (y >= 0 && y < 0.25) {\n            bins[0]++;\n        } else if (y >= 0.25 && y < 0.5) {\n            bins[1]++;\n        } else if (y >= 0.5 && y < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "for (auto const& element: x) {\n        if (element <= 0.25)\n            bins[0]++;\n        else if (element <= 0.5)\n            bins[1]++;\n        else if (element <= 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "}",
            "bins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\tsize_t n = x.size();\n\tfor (int i = 0; i < n; i++) {\n\t\tdouble x_i = x[i];\n\t\tif (x_i > 0.25) {\n\t\t\tif (x_i < 0.5) {\n\t\t\t\tbins[0] += 1;\n\t\t\t} else if (x_i < 0.75) {\n\t\t\t\tbins[1] += 1;\n\t\t\t} else {\n\t\t\t\tbins[2] += 1;\n\t\t\t}\n\t\t} else {\n\t\t\tif (x_i > 0.0) {\n\t\t\t\tbins[3] += 1;\n\t\t\t}\n\t\t}\n\t}\n}",
            "for (size_t i = 0; i < x.size(); ++i) {\n    double value = x[i];\n    if (value >= 0 && value <= 0.25) {\n      ++bins[0];\n    }\n    else if (value > 0.25 && value <= 0.5) {\n      ++bins[1];\n    }\n    else if (value > 0.5 && value <= 0.75) {\n      ++bins[2];\n    }\n    else if (value > 0.75 && value <= 1) {\n      ++bins[3];\n    }\n  }\n}",
            "for(auto y : x) {\n        if(y >= 0.0 && y < 0.25) {\n            bins[0]++;\n        }\n        else if(y >= 0.25 && y < 0.5) {\n            bins[1]++;\n        }\n        else if(y >= 0.5 && y < 0.75) {\n            bins[2]++;\n        }\n        else if(y >= 0.75 && y <= 1.0) {\n            bins[3]++;\n        }\n        else {\n            std::cout << \"Invalid quartile value: \" << y << std::endl;\n        }\n    }\n}",
            "// TODO: Your code here\n  int quartile = 0;\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] < 0.25) {\n      ++quartile;\n    } else if (x[i] < 0.5) {\n      ++quartile;\n    } else if (x[i] < 0.75) {\n      ++quartile;\n    } else {\n      ++quartile;\n    }\n  }\n\n  for (int i = 0; i < x.size(); ++i) {\n    if (x[i] < 0.25) {\n      ++bins[0];\n    } else if (x[i] < 0.5) {\n      ++bins[1];\n    } else if (x[i] < 0.75) {\n      ++bins[2];\n    } else {\n      ++bins[3];\n    }\n  }\n}",
            "bins.fill(0);\n   for (size_t i = 0; i < x.size(); ++i) {\n      size_t bin = (size_t) floor(x[i] * 4);\n      bins[bin]++;\n   }\n}",
            "// TODO: Fill this in\n  for (auto d: x){\n    if(d <= 0.25){\n      bins[0] += 1;\n    }\n    else if (d <= 0.5){\n      bins[1] += 1;\n    }\n    else if (d <= 0.75){\n      bins[2] += 1;\n    }\n    else{\n      bins[3] += 1;\n    }\n  }\n}",
            "double quantile = 0.25;\n  double value;\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    value = x[i];\n\n    if (value >= quantile) {\n      quantile += 0.25;\n    } else {\n      ++bins[static_cast<size_t>(value * 4)];\n    }\n  }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "hip",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use AMD HIP to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "double xval = x[hipBlockIdx_x];\n  double v = xval - floor(xval);\n  if (v < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (v < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (v < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t blockid = blockIdx.x;\n  size_t blockSize = blockDim.x;\n\n  // Compute the range of elements in this block\n  size_t start = blockid * blockSize;\n  size_t end = min(start + blockSize, N);\n  start = min(start, N);\n\n  // Compute the number of values in each bin.\n  for (size_t i = start + tid; i < end; i += blockSize) {\n    double xi = x[i];\n    // Compute the value of the bin to which x[i] belongs\n    size_t bin = (xi >= 0.75)? 3 : (xi >= 0.5)? 2 : (xi >= 0.25)? 1 : 0;\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "// Your code goes here\n}",
            "int tid = threadIdx.x;\n  double *smem = (double *)sharedMem;\n\n  // TODO: Initialize the shared memory\n  // Hint: you will need to allocate at least 8 doubles.\n  smem[tid] = 0;\n\n  // TODO: Compute the sum in shared memory\n  // Hint: use __syncthreads() to make sure that all threads have\n  // finished before you read from the shared memory.\n  double sum = 0;\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double val = x[i];\n    if (val >= 0 && val < 0.25) {\n      smem[tid] += val;\n    } else if (val >= 0.25 && val < 0.5) {\n      smem[tid] += val;\n    } else if (val >= 0.5 && val < 0.75) {\n      smem[tid] += val;\n    } else if (val >= 0.75 && val <= 1) {\n      smem[tid] += val;\n    }\n  }\n  __syncthreads();\n\n  // TODO: Compute the sum in shared memory again\n  // Hint: use a double2\n  double2 sum_double2 = make_double2(0, 0);\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double val = x[i];\n    if (val >= 0 && val < 0.25) {\n      sum_double2.x += val;\n    } else if (val >= 0.25 && val < 0.5) {\n      sum_double2.y += val;\n    } else if (val >= 0.5 && val < 0.75) {\n      sum_double2.x += val;\n    } else if (val >= 0.75 && val <= 1) {\n      sum_double2.y += val;\n    }\n  }\n  __syncthreads();\n\n  // TODO: Compute the sum in shared memory again\n  // Hint: use a double4\n  double4 sum_double4 = make_double4(0, 0, 0, 0);\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double val = x[i];\n    if (val >= 0 && val < 0.25) {\n      sum_double4.x += val;\n    } else if (val >= 0.25 && val < 0.5) {\n      sum_double4.y += val;\n    } else if (val >= 0.5 && val < 0.75) {\n      sum_double4.z += val;\n    } else if (val >= 0.75 && val <= 1) {\n      sum_double4.w += val;\n    }\n  }\n  __syncthreads();\n\n  // TODO: Compute the sum in shared memory again\n  // Hint: use a double8\n  double8 sum_double8 = make_double8(0, 0, 0, 0, 0, 0, 0, 0);\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double val = x[i];\n    if (val >= 0 && val < 0.25) {\n      sum_double8.x += val;\n    } else if (val >= 0.25 && val < 0.5) {\n      sum_double8.y += val;\n    } else if (val >= 0.5 && val < 0.75) {\n      sum_double8.z += val;\n    } else if (val >= 0.75 && val <= 1) {\n      sum_double8.w += val;\n    }\n  }\n  __syncthreads();\n\n  // TODO: Compute the sum in shared memory again\n  // Hint: use a double16\n  double16 sum_double16 = make_double16(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0);\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double val = x[i];\n    if (val >= 0 && val < 0.25) {\n      sum_double16.x += val;\n    } else if (val >= 0.25 && val <",
            "int tid = hipThreadIdx_x;\n    // AMD HIP has no shuffle intrinsic, so we'll need to write our own shuffle function\n    // This is the shuffle_xor version from the AMD HIP documentation:\n    __device__ __forceinline__ int shuffle(int a, int b, int c) {\n        uint32_t a32 = __double2uint_rn(a);\n        uint32_t b32 = __double2uint_rn(b);\n        uint32_t c32 = __double2uint_rn(c);\n        uint32_t r32 = __funnelshift_l(a32, b32, 32);\n        return __uint2double_rn(r32 ^ c32);\n    }\n    // Each thread will calculate the number of values that are less than its position\n    // in the sorted vector. This should make it easier to figure out where to start\n    // scanning for the next quartile.\n    if (tid < N) {\n        double x_tid = x[tid];\n        int tid_less = __ballot(x_tid < x_tid);\n        bins[0] += __popc(tid_less & 0xFF);\n        bins[1] += __popc((tid_less >> 8) & 0xFF);\n        bins[2] += __popc((tid_less >> 16) & 0xFF);\n        bins[3] += __popc(tid_less >> 24);\n    }\n    // Wait for all threads to finish before proceeding\n    __syncthreads();\n    if (tid == 0) {\n        int sum = bins[0] + bins[1] + bins[2] + bins[3];\n        bins[0] = sum / 4;\n        bins[1] = (sum - bins[0]) / 3;\n        bins[2] = (sum - bins[0] - bins[1]) / 2;\n        bins[3] = sum - bins[0] - bins[1] - bins[2];\n    }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t quartile = 0;\n    double fraction = 0.0;\n\n    if (tid < N) {\n        fraction = fmod(x[tid], 1.0);\n\n        if (fraction < 0.25) {\n            quartile = 0;\n        } else if (fraction < 0.50) {\n            quartile = 1;\n        } else if (fraction < 0.75) {\n            quartile = 2;\n        } else {\n            quartile = 3;\n        }\n    }\n    __syncthreads();\n\n    atomicAdd(&bins[quartile], 1);\n}",
            "// YOUR CODE HERE\n   //...\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i < N) {\n        double d = x[i];\n        double frac = d - floor(d);\n        // Quartile index is 1 if frac is in [0, 0.25), 2 if in [0.25, 0.5),...\n        int q = 1 + static_cast<int>(frac * 4.0);\n        if (q > 4) {\n            q = 4;\n        }\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "// TODO: Fill in this function\n  return;\n}",
            "size_t bin = (hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x) / 4;\n  size_t start = (hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x) % 4;\n  size_t size = N - hipBlockIdx_x * hipBlockDim_x;\n  bins[bin] = 0;\n  for (size_t i = start; i < size; i += hipBlockDim_x) {\n    double value = x[hipBlockIdx_x * hipBlockDim_x + i];\n    if (0 <= value && value < 0.25) {\n      bins[bin]++;\n    } else if (0.25 <= value && value < 0.5) {\n      bins[bin]++;\n    } else if (0.5 <= value && value < 0.75) {\n      bins[bin]++;\n    } else if (0.75 <= value && value <= 1.0) {\n      bins[bin]++;\n    }\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t stride = hipGridDim_x * hipBlockDim_x;\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    while (tid < N) {\n        double i = x[tid];\n        if (i < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (i < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (i < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n        tid += stride;\n    }\n}",
            "// TODO: Implement this function\n  __shared__ size_t sharedBins[4];\n  // Compute the starting thread index for this block\n  size_t threadIndex = (blockDim.x * blockIdx.x) + threadIdx.x;\n  if (threadIndex < N) {\n    double xFrac = x[threadIndex] - floor(x[threadIndex]);\n    if (xFrac <= 0.25) {\n      atomicAdd(&sharedBins[0], 1);\n    } else if (xFrac <= 0.5) {\n      atomicAdd(&sharedBins[1], 1);\n    } else if (xFrac <= 0.75) {\n      atomicAdd(&sharedBins[2], 1);\n    } else {\n      atomicAdd(&sharedBins[3], 1);\n    }\n  }\n  // Add all the threads' bins to bins\n  if (threadIdx.x < 4) {\n    atomicAdd(&bins[threadIdx.x], sharedBins[threadIdx.x]);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  double *x_in = (double *)(&x[bid * N]);\n  double *x_out = (double *)(&bins[bid * 4]);\n\n  size_t[4] smem_bins;\n  smem_bins[0] = 0;\n  smem_bins[1] = 0;\n  smem_bins[2] = 0;\n  smem_bins[3] = 0;\n\n  size_t num_items = min(N, (bid + 1) * N - bid * N);\n  for (size_t i = tid; i < num_items; i += blockDim.x) {\n    int bin = (int)(100 * x_in[i]) / 25;\n    atomicAdd(&smem_bins[bin], 1);\n  }\n  // Reduce the block data to the global data\n  __syncthreads();\n\n  if (tid == 0) {\n    for (size_t i = 1; i < blockDim.x; i++) {\n      smem_bins[0] += smem_bins[i];\n      smem_bins[1] += smem_bins[i + 1];\n      smem_bins[2] += smem_bins[i + 2];\n      smem_bins[3] += smem_bins[i + 3];\n    }\n    x_out[0] = smem_bins[0];\n    x_out[1] = smem_bins[1];\n    x_out[2] = smem_bins[2];\n    x_out[3] = smem_bins[3];\n  }\n}",
            "size_t index = blockDim.x * blockIdx.x + threadIdx.x;\n  if (index < N) {\n    // Replace this with a real implementation of Quartiles.\n    double f = x[index] - floor(x[index]);\n    if (f <= 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (f <= 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (f <= 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  double4 *x4 = (double4*)x;\n  int i = bid*N/gridDim.x + tid;\n  double4 x4i = x4[i];\n  __shared__ double4 smem[BLOCK_DIM];\n  smem[tid] = x4i;\n  __syncthreads();\n  double *s = (double*)&smem[0];\n  size_t bins0 = (s[tid] >= 0.0 && s[tid] < 0.25)? 1 : 0;\n  size_t bins1 = (s[tid] >= 0.25 && s[tid] < 0.5)? 1 : 0;\n  size_t bins2 = (s[tid] >= 0.5 && s[tid] < 0.75)? 1 : 0;\n  size_t bins3 = (s[tid] >= 0.75 && s[tid] < 1.0)? 1 : 0;\n  // use atomicAdd to avoid conflicts between threads\n  atomicAdd(&bins[0], bins0);\n  atomicAdd(&bins[1], bins1);\n  atomicAdd(&bins[2], bins2);\n  atomicAdd(&bins[3], bins3);\n}",
            "size_t n = blockDim.x * blockIdx.x + threadIdx.x;\n   if (n < N) {\n     if (x[n] < 0.25)\n       atomicAdd(&bins[0], 1);\n     else if (x[n] < 0.5)\n       atomicAdd(&bins[1], 1);\n     else if (x[n] < 0.75)\n       atomicAdd(&bins[2], 1);\n     else\n       atomicAdd(&bins[3], 1);\n   }\n}",
            "int t = blockDim.x * blockIdx.x + threadIdx.x;\n    int n = blockDim.x * gridDim.x;\n    for (size_t i = t; i < N; i += n) {\n        double value = x[i];\n        double f = value - floor(value);\n        if (f >= 0.25 && f < 0.5)\n            atomicAdd(&bins[0], 1);\n        else if (f >= 0.5 && f < 0.75)\n            atomicAdd(&bins[1], 1);\n        else if (f >= 0.75 && f < 1.0)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// Add your implementation here\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    for (size_t i = tid; i < N; i += gridDim.x * blockDim.x) {\n        double bin = x[i] * 4;\n        if (bin < 0) bin = 0;\n        if (bin >= 4) bin = 3;\n        atomicAdd(bins + bin, 1);\n    }\n}",
            "// TODO: implement this function\n}",
            "double x_f;\n  const double a = 0, b = 0.25, c = 0.5, d = 0.75;\n  // TODO: Implement the kernel\n}",
            "const int threadID = blockDim.x * blockIdx.x + threadIdx.x;\n    const double x_i = x[threadID];\n    if (x_i >= 0 && x_i < 0.25) {\n        atomicAdd(&bins[0], 1);\n    }\n    if (x_i >= 0.25 && x_i < 0.5) {\n        atomicAdd(&bins[1], 1);\n    }\n    if (x_i >= 0.5 && x_i < 0.75) {\n        atomicAdd(&bins[2], 1);\n    }\n    if (x_i >= 0.75 && x_i < 1) {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  // TODO\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n}",
            "/* YOUR CODE HERE */\n  size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t num = 0;\n  if (index < N) {\n    double val = x[index];\n    if (val >= 0.0 && val < 0.25)\n      num++;\n    else if (val >= 0.25 && val < 0.5)\n      num++;\n    else if (val >= 0.5 && val < 0.75)\n      num++;\n    else if (val >= 0.75 && val <= 1.0)\n      num++;\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], num);\n  }\n  __syncthreads();\n  if (threadIdx.x == 1) {\n    atomicAdd(&bins[1], num);\n  }\n  __syncthreads();\n  if (threadIdx.x == 2) {\n    atomicAdd(&bins[2], num);\n  }\n  __syncthreads();\n  if (threadIdx.x == 3) {\n    atomicAdd(&bins[3], num);\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    double x_i = x[i];\n    double x_i_25 = floor(x_i * 4.0) / 4.0;\n    double x_i_50 = floor(x_i * 2.0) / 2.0;\n    double x_i_75 = floor((x_i + 1) * 2.0 - 1) / 2.0;\n    if (x_i <= x_i_25) {\n      if (x_i <= x_i_50) {\n        if (x_i <= x_i_75) {\n          atomicAdd(&bins[0], 1);\n        } else {\n          atomicAdd(&bins[3], 1);\n        }\n      } else {\n        atomicAdd(&bins[2], 1);\n      }\n    } else {\n      atomicAdd(&bins[1], 1);\n    }\n  }\n}",
            "// Your code goes here.\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double xi = x[tid];\n    double quartile = (xi + 0.25) / 0.5;\n    int idx = 3;\n    if (quartile <= 1) {\n      idx = 0;\n    } else if (quartile <= 2) {\n      idx = 1;\n    } else if (quartile <= 3) {\n      idx = 2;\n    }\n    atomicAdd(&bins[idx], 1);\n  }\n}",
            "size_t myId = blockDim.x * blockIdx.x + threadIdx.x;\n  double myDouble;\n  __shared__ double shmem[blockDim.x];\n\n  // count the number of elements in each bin\n  if (myId < N) {\n    myDouble = x[myId];\n    if (myDouble < 0.25)\n      shmem[threadIdx.x] = 1;\n    else if (myDouble < 0.5)\n      shmem[threadIdx.x] += 1;\n    else if (myDouble < 0.75)\n      shmem[threadIdx.x] += 2;\n    else\n      shmem[threadIdx.x] += 3;\n  }\n\n  __syncthreads();\n\n  // combine the counts\n  for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n    if (threadIdx.x < stride)\n      shmem[threadIdx.x] += shmem[threadIdx.x + stride];\n    __syncthreads();\n  }\n\n  // write the result\n  if (threadIdx.x == 0) {\n    bins[0] = shmem[0];\n    bins[1] = shmem[1];\n    bins[2] = shmem[2];\n    bins[3] = shmem[3];\n  }\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    const double xx = x[tid];\n    const double bin = xx - floor(xx);\n    if (bin < 0.25) bins[0]++;\n    else if (bin < 0.5) bins[1]++;\n    else if (bin < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "double2 a = {0, 0};\n    double2 b = {0, 0};\n    double2 c = {0, 0};\n    double2 d = {0, 0};\n\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double z = x[i];\n        int j = 2 * (z > 0.25);\n        int k = 2 * (z > 0.5);\n        int l = 2 * (z > 0.75);\n        a.x += j;\n        b.x += k;\n        c.x += l;\n        if (z > 0.25 && z <= 0.5) {\n            a.y += 1;\n            b.y += 1;\n        } else if (z > 0.5 && z <= 0.75) {\n            b.y += 1;\n            c.y += 1;\n        } else if (z > 0.75) {\n            c.y += 1;\n            d.y += 1;\n        } else if (z == 0.25) {\n            a.y += 1;\n        } else if (z == 0.5) {\n            b.y += 1;\n        } else if (z == 0.75) {\n            c.y += 1;\n        }\n    }\n\n    // Reduce the counts in parallel.\n    __syncthreads();\n    for (int i = blockDim.x / 2; i >= 32; i >>= 1) {\n        a.x += __shfl_xor(a.x, i);\n        a.y += __shfl_xor(a.y, i);\n        b.x += __shfl_xor(b.x, i);\n        b.y += __shfl_xor(b.y, i);\n        c.x += __shfl_xor(c.x, i);\n        c.y += __shfl_xor(c.y, i);\n        d.y += __shfl_xor(d.y, i);\n    }\n    if (threadIdx.x < 32) {\n        atomicAdd(&bins[0], a.x);\n        atomicAdd(&bins[1], a.y);\n        atomicAdd(&bins[2], b.x);\n        atomicAdd(&bins[3], b.y);\n        atomicAdd(&bins[4], c.x);\n        atomicAdd(&bins[5], c.y);\n        atomicAdd(&bins[6], d.y);\n    }\n}",
            "// YOUR CODE HERE\n}",
            "double sum = 0;\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int blockSize = blockDim.x;\n  int base = bid * blockSize;\n  int limit = min(base + blockSize, N);\n  int i;\n  for (i = base + tid; i < limit; i += blockSize) {\n    sum += x[i] / 0.25;\n  }\n  sum = blockReduceSum(sum);\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], sum);\n  }\n}",
            "// TODO\n    int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int bid = blockIdx.y;\n    int bdim = blockDim.x * gridDim.x;\n    for (int i = tid; i < N; i += bdim) {\n        double value = x[i];\n        if (value >= 0.0 && value < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (value >= 0.25 && value < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (value >= 0.5 && value < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// INSERT YOUR CODE HERE\n\n    double t = x[threadIdx.x];\n    double f = 0.25;\n\n    if (t <= f)\n    {\n        atomicAdd(&bins[0], 1);\n    }\n    else if ((t > f) && (t <= (2*f)))\n    {\n        atomicAdd(&bins[1], 1);\n    }\n    else if ((t > (2*f)) && (t <= (3*f)))\n    {\n        atomicAdd(&bins[2], 1);\n    }\n    else if ((t > (3*f)) && (t <= 1))\n    {\n        atomicAdd(&bins[3], 1);\n    }\n    else\n        printf(\"Error in counting quartiles, t is %f\\n\", t);\n}",
            "__shared__ size_t localBins[4];\n  size_t threadId = threadIdx.x;\n  double threshold = 0;\n  int binIdx;\n  if (threadId < 4) {\n    localBins[threadId] = 0;\n    binIdx = threadId;\n    threshold = binIdx * 0.25 + 0.25;\n  }\n  __syncthreads();\n\n  for (size_t i = threadId; i < N; i += blockDim.x) {\n    double value = x[i];\n    // We need to check if `value` is between\n    // `threshold` and `threshold + 0.25`.\n    // But, if `value` is close to 1, then\n    // `value > threshold` might not be true.\n    // So, we compare `value >= threshold` and\n    // `value < threshold + 0.25` instead.\n    int bin = (value >= threshold && value < threshold + 0.25)? 0 :\n      (value >= threshold + 0.25 && value < threshold + 0.5)? 1 :\n      (value >= threshold + 0.5 && value < threshold + 0.75)? 2 : 3;\n\n    atomicAdd(&(localBins[bin]), 1);\n  }\n\n  __syncthreads();\n\n  if (threadId < 4) {\n    atomicAdd(&(bins[binIdx]), localBins[threadId]);\n  }\n}",
            "int tid = threadIdx.x;\n    size_t *bin = &bins[tid];\n    __shared__ double xShared[BLOCK_SIZE];\n    int blockOffset = BLOCK_SIZE * blockIdx.x;\n    xShared[tid] = x[blockOffset + tid];\n    __syncthreads();\n    if (tid == 0) {\n        for (int i = 0; i < BLOCK_SIZE; i++) {\n            size_t idx = blockOffset + i;\n            if (x[idx] >= 0 && x[idx] < 0.25) {\n                bin[0]++;\n            } else if (x[idx] >= 0.25 && x[idx] < 0.5) {\n                bin[1]++;\n            } else if (x[idx] >= 0.5 && x[idx] < 0.75) {\n                bin[2]++;\n            } else if (x[idx] >= 0.75 && x[idx] <= 1) {\n                bin[3]++;\n            }\n        }\n    }\n}",
            "// TODO: Replace this with the actual code\n   __syncthreads();\n}",
            "// Your code here.\n}",
            "int tid = hipThreadIdx_x;\n    __shared__ int sh_bins[4];\n    __shared__ double sh_x[1024];\n\n    // Read in 1024 elements of x into the shared array.\n    for (size_t i = tid; i < N; i += hipBlockDim_x) {\n        sh_x[i] = x[i];\n    }\n    __syncthreads();\n\n    // Do the counting.\n    int j = tid / 4;\n    int i = tid % 4;\n    if (j < N) {\n        double val = sh_x[j];\n        sh_bins[i] += (val >= 0 && val < 0.25);\n        sh_bins[i] += (val >= 0.25 && val < 0.5);\n        sh_bins[i] += (val >= 0.5 && val < 0.75);\n        sh_bins[i] += (val >= 0.75 && val < 1);\n    }\n\n    // Reduction.\n    for (int stride = hipBlockDim_x / 2; stride > 0; stride /= 2) {\n        if (tid < stride) {\n            sh_bins[i] += sh_bins[i + stride];\n        }\n        __syncthreads();\n    }\n\n    // Write out the results.\n    if (tid == 0) {\n        bins[0] = sh_bins[0];\n        bins[1] = sh_bins[1];\n        bins[2] = sh_bins[2];\n        bins[3] = sh_bins[3];\n    }\n}",
            "double a = 0.0;\n    size_t b = 0;\n    for (size_t i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        if (x[i] >= 0.0 && x[i] <= 0.25) {\n            a += 1.0;\n            b += 1;\n        } else if (x[i] > 0.25 && x[i] <= 0.5) {\n            a += 1.0;\n            b += 2;\n        } else if (x[i] > 0.5 && x[i] <= 0.75) {\n            a += 1.0;\n            b += 3;\n        } else if (x[i] > 0.75 && x[i] <= 1.0) {\n            a += 1.0;\n            b += 4;\n        }\n    }\n    atomicAdd(&bins[0], a);\n    atomicAdd(&bins[b], a);\n}",
            "size_t idx = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (idx < N) {\n    double x_val = x[idx];\n    size_t bin = hipBlockIdx_x;\n    if (x_val <= 0.25) {\n      bin = 0;\n    } else if (x_val <= 0.5) {\n      bin = 1;\n    } else if (x_val <= 0.75) {\n      bin = 2;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO\n}",
            "__shared__ size_t shBins[4];\n   size_t tid = threadIdx.x;\n   size_t blockN = blockDim.x;\n   size_t i = blockIdx.x * blockN + tid;\n\n   shBins[0] = 0;\n   shBins[1] = 0;\n   shBins[2] = 0;\n   shBins[3] = 0;\n\n   for ( ; i < N; i += blockN) {\n      double val = x[i];\n      double frac = 0.25 * (val - floor(val));\n      if (frac < 0.25) {\n         shBins[0] += 1;\n      } else if (frac < 0.5) {\n         shBins[1] += 1;\n      } else if (frac < 0.75) {\n         shBins[2] += 1;\n      } else {\n         shBins[3] += 1;\n      }\n   }\n   __syncthreads();\n\n   atomicAdd(&bins[0], shBins[0]);\n   atomicAdd(&bins[1], shBins[1]);\n   atomicAdd(&bins[2], shBins[2]);\n   atomicAdd(&bins[3], shBins[3]);\n}",
            "// TODO: Your implementation here\n  return;\n}",
            "/* TODO */\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N) {\n    return;\n  }\n\n  // Get the fractional part of the value in [0, 0.25).\n  // If the result is 0.0 or 0.25, assign 0 to `bin`.\n  double bin = __fdividef(x[i] * 4, 1);\n  bin = __fdividef(bin, 0.25);\n  if (bin <= 0.0 || bin > 0.25) {\n    bin = 0;\n  }\n\n  atomicAdd(&bins[0], (size_t)bin);\n  bin = __fdividef((x[i] * 4 - bin * 0.25), 0.5);\n  if (bin < 0.0 || bin > 0.25) {\n    bin = 0;\n  }\n  atomicAdd(&bins[1], (size_t)bin);\n  bin = __fdividef((x[i] * 4 - bin * 0.5), 0.25);\n  if (bin < 0.0 || bin > 0.25) {\n    bin = 0;\n  }\n  atomicAdd(&bins[2], (size_t)bin);\n  bin = __fdividef((x[i] * 4 - bin * 0.75), 0.25);\n  if (bin < 0.0 || bin > 0.25) {\n    bin = 0;\n  }\n  atomicAdd(&bins[3], (size_t)bin);\n}",
            "// TODO\n}",
            "size_t id = threadIdx.x + blockDim.x * blockIdx.x;\n  // 8 warps are needed for one warp to cover [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1)\n  if (id < N) {\n    if (x[id] >= 0.25) {\n      atomicAdd(&bins[3], 1);\n      if (x[id] >= 0.5) {\n        atomicAdd(&bins[2], 1);\n        if (x[id] >= 0.75) {\n          atomicAdd(&bins[1], 1);\n        } else {\n          atomicAdd(&bins[0], 1);\n        }\n      } else {\n        atomicAdd(&bins[0], 1);\n      }\n    } else {\n      atomicAdd(&bins[0], 1);\n    }\n  }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  double y = x[i];\n  if (y < 0) y += 1.0;  // shift to [0, 1)\n\n  __shared__ size_t temp[4];  // temporary storage for the block\n\n  temp[0] = 0;  // the current number of elements in the 1st bin\n  temp[1] = 0;  // the current number of elements in the 2nd bin\n  temp[2] = 0;  // the current number of elements in the 3rd bin\n  temp[3] = 0;  // the current number of elements in the 4th bin\n\n  while (i < N) {\n    if (y < 0.25)\n      temp[0]++;\n    else if (y < 0.5)\n      temp[1]++;\n    else if (y < 0.75)\n      temp[2]++;\n    else\n      temp[3]++;\n    i += hipGridDim_x * hipBlockDim_x;\n    y = x[i];\n    if (y < 0) y += 1.0;\n  }\n  // reduce in shared memory\n  hip_atomic_add_sizet(temp, bins);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N)\n        return;\n    double x_i = x[i];\n    double q = x_i - floor(x_i);\n    if (q >= 0.25 && q < 0.5)\n        atomicAdd(&bins[0], 1);\n    else if (q >= 0.5 && q < 0.75)\n        atomicAdd(&bins[1], 1);\n    else if (q >= 0.75 && q < 1.0)\n        atomicAdd(&bins[2], 1);\n    else\n        atomicAdd(&bins[3], 1);\n}",
            "// TODO: Your code goes here\n  __shared__ size_t s_bins[4];\n\n  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double bin = x[tid] * 4;\n    if (bin < 0.25) {\n      s_bins[0] = s_bins[0] + 1;\n    } else if (bin < 0.5) {\n      s_bins[1] = s_bins[1] + 1;\n    } else if (bin < 0.75) {\n      s_bins[2] = s_bins[2] + 1;\n    } else {\n      s_bins[3] = s_bins[3] + 1;\n    }\n  }\n  __syncthreads();\n\n  atomicAdd(&bins[0], s_bins[0]);\n  atomicAdd(&bins[1], s_bins[1]);\n  atomicAdd(&bins[2], s_bins[2]);\n  atomicAdd(&bins[3], s_bins[3]);\n}",
            "}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\n\tfor(size_t i = bid * blockDim.x + tid; i < N; i += gridDim.x * blockDim.x) {\n\t\tdouble f = x[i];\n\t\tif(f >= 0.25 && f < 0.5) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t} else if(f >= 0.5 && f < 0.75) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t} else if(f >= 0.75 && f < 1) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t} else {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "__shared__ double buffer[1024];\n  unsigned tid = threadIdx.x;\n  unsigned block = blockIdx.x;\n  unsigned local = block * blockDim.x + threadIdx.x;\n  size_t begin = block * N / gridDim.x;\n  size_t end = (block+1) * N / gridDim.x;\n\n  // Compute quartile locations and store in shared memory.\n  double Q1 = 0.25 * (x[begin] + x[end-1]);\n  double Q2 = 0.5 * (x[begin] + x[end-1]);\n  double Q3 = 0.75 * (x[begin] + x[end-1]);\n  if (local < N) {\n    buffer[tid] = (x[local] > Q1) * 1 + (x[local] > Q2) * 1 + (x[local] > Q3) * 1;\n  }\n  __syncthreads();\n\n  // Reduce in parallel.\n  for (size_t s = blockDim.x / 2; s > 0; s >>= 1) {\n    if (local < N && tid < s) {\n      buffer[tid] += buffer[tid+s];\n    }\n    __syncthreads();\n  }\n\n  // Write results to global memory.\n  if (local < N) {\n    bins[buffer[0]] += 1;\n  }\n}",
            "size_t tid = threadIdx.x;\n  double *my_bins = (double*)&bins;\n  my_bins[tid] = 0;\n\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double xi = x[i];\n    if (xi > 0) {\n      if (xi <= 0.25) {\n        my_bins[0] += 1;\n      } else if (xi <= 0.5) {\n        my_bins[1] += 1;\n      } else if (xi <= 0.75) {\n        my_bins[2] += 1;\n      } else {\n        my_bins[3] += 1;\n      }\n    }\n  }\n\n  __syncthreads();\n\n  for (size_t i = 1; i < blockDim.x; i *= 2) {\n    if (tid % (2 * i) == 0) {\n      my_bins[tid] += my_bins[tid + i];\n    }\n    __syncthreads();\n  }\n  if (tid == 0) {\n    my_bins[0] = my_bins[blockDim.x - 1];\n  }\n}",
            "// YOUR CODE HERE\n}",
            "int tid = threadIdx.x;\n  int bin = tid/4;\n  int block = blockIdx.x;\n  int i = 4*block*blockDim.x + tid;\n  __shared__ double cache[256];\n  cache[tid] = i<N? x[i] : 0.0;\n  __syncthreads();\n\n  if (tid < 4) {\n    for (int j = tid; j < N; j += 32) {\n      double v = cache[j/4];\n      if (v >= (double)bin*0.25 && v < (double)(bin+1)*0.25) {\n        atomicAdd(&bins[bin], 1);\n      }\n    }\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    double p = x[tid];\n    if (p <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (p <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (p <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    double f = x[tid] - floor(x[tid]);\n    int bin = (f >= 0 && f < 0.25) + (f >= 0.25 && f < 0.5) + (f >= 0.5 && f < 0.75) + (f >= 0.75 && f <= 1.0);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: Compute the number of elements in each of the 4 quartiles (0.25, 0.5, 0.75)\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    double absx = fabs(x[idx]);\n    if (absx <= 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (absx <= 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (absx <= 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// FIXME: Implement this function.\n}",
            "// TODO\n}",
            "int tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  if (tid < N) {\n    double v = fabs(x[tid] - floor(x[tid] * 4) / 4);\n    if (v < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (v < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (v < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid >= N) return;\n\n  double d = x[tid];\n  if (d < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (d < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (d < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t blockSize = hipBlockDim_x;\n  size_t gridSize = hipGridDim_x;\n\n  __shared__ size_t shared[4];\n\n  for (size_t block = blockIdx.x; block < N; block += gridSize) {\n    size_t start = block * blockSize + tid;\n    size_t end = start + blockSize;\n\n    size_t sum = 0;\n    for (size_t i = start; i < end; i++) {\n      double value = x[i];\n      if (value >= 0 && value < 0.25)\n        sum++;\n      else if (value >= 0.25 && value < 0.5)\n        sum += 2;\n      else if (value >= 0.5 && value < 0.75)\n        sum += 3;\n      else if (value >= 0.75 && value < 1)\n        sum += 4;\n    }\n\n    __syncthreads();\n    atomicAdd(&shared[sum & 3], 1);\n  }\n\n  __syncthreads();\n  atomicAdd(&bins[0], shared[0]);\n  atomicAdd(&bins[1], shared[1]);\n  atomicAdd(&bins[2], shared[2]);\n  atomicAdd(&bins[3], shared[3]);\n}",
            "// TODO: Your code here\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t stride = blockDim.x * gridDim.x;\n\n    // each thread counts a single bin\n    for (size_t i = tid; i < N; i += stride) {\n        double xi = x[i];\n        size_t bin = (xi >= 0)? 0 : 1;\n        bin += (xi >= 0 && xi < 0.25)? 1 : 0;\n        bin += (xi >= 0.25 && xi < 0.5)? 1 : 0;\n        bin += (xi >= 0.5 && xi < 0.75)? 1 : 0;\n        bin += (xi >= 0.75 && xi < 1)? 1 : 0;\n\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    double temp = x[tid] - floor(x[tid]);\n    // 1/16 ~ 0.0625\n    if (temp <= 0.0625)\n      atomicAdd(&bins[0], 1);\n    // 2/16 ~ 0.125\n    else if (temp > 0.0625 && temp <= 0.125)\n      atomicAdd(&bins[1], 1);\n    // 3/16 ~ 0.1875\n    else if (temp > 0.125 && temp <= 0.1875)\n      atomicAdd(&bins[2], 1);\n    // 4/16 ~ 0.25\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double xi = x[i];\n    double q = floor(xi / 0.25);\n    if (q < 0) q = 0;\n    if (q > 3) q = 3;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "int idx = threadIdx.x + blockIdx.x * blockDim.x;\n\n    if (idx < N) {\n        int bin = int(x[idx] * 4.0);\n        if (bin == 4) bin = 3;\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double y = x[i];\n        if (y < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (y < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (y < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "__shared__ int counts[16];\n\n    // TODO: implement this function\n    //\n    // Hints:\n    //\n    // To compute the count in [0, 0.25), the kernel should return a result\n    // that is 1 if x is between 0 and 0.25 (inclusive), and 0 otherwise.\n    //\n    // Similarly, to compute the count in [0.25, 0.5), the kernel should return\n    // a result that is 1 if x is between 0.25 and 0.5 (inclusive), and 0 otherwise.\n    //\n    // A similar computation is used to compute the count in [0.5, 0.75), and\n    // [0.75, 1).\n    //\n    // When a thread is assigned a value in x, it may have multiple values\n    // for the index. For example, suppose the thread is assigned the value\n    // 7.8, but a thread earlier in the warp assigned the value 9.1.\n    // This is OK, and you can use atomicAdd() to sum the results.\n    //\n    // When two threads have the same value for x, they should both\n    // compute the same result. To avoid this, you can use a __syncwarp()\n    // before each thread computes its own value, so that all threads\n    // within a warp have the same value for x.\n\n    // __syncthreads();\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n    while (tid < N) {\n        double value = x[tid];\n        if (value >= 0.0 && value < 0.25)\n            bins[0]++;\n        else if (value >= 0.25 && value < 0.5)\n            bins[1]++;\n        else if (value >= 0.5 && value < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n        tid += blockDim.x * gridDim.x;\n    }\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your implementation goes here.\n}",
            "__shared__ int sbins[4];\n\n\tint i = threadIdx.x;\n\n\tif(i < N) {\n\t\tconst double xi = x[i];\n\t\tint q = __double2int_rd(xi * 4);\n\t\tsbins[q]++;\n\t}\n\n\t__syncthreads();\n\n\tif(threadIdx.x < 4) {\n\t\tatomicAdd(&bins[threadIdx.x], sbins[threadIdx.x]);\n\t}\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t gid = hipBlockIdx_x * hipBlockDim_x + tid;\n  if (gid < N) {\n    double value = x[gid];\n    size_t i = (value > 0.5) + (value > 0.75);\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "// YOUR CODE HERE\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    double t = x[i] * 4;\n    if (t >= 2)\n      bins[3]++;\n    else if (t >= 1)\n      bins[2]++;\n    else if (t >= 0.75)\n      bins[1]++;\n    else if (t >= 0.5)\n      bins[0]++;\n    else\n      bins[0]++;\n  }\n}",
            "for (size_t i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x; i < N; i += hipGridDim_x * hipBlockDim_x) {\n    double v = x[i];\n    if (v < 0.25)\n      bins[0] += 1;\n    else if (v < 0.5)\n      bins[1] += 1;\n    else if (v < 0.75)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n}",
            "int tid = blockIdx.x*blockDim.x+threadIdx.x;\n   int idx = tid%N;\n   if (idx < N) {\n      int n = static_cast<int>(x[idx]);\n      if (n < 0) {\n         ++bins[0];\n      } else if (n < 25) {\n         ++bins[1];\n      } else if (n < 50) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (i >= N) return;\n    size_t part = ((size_t) (x[i]*4) ) % 4;\n    if (part == 0 || part == 1) bins[part]++;\n    if (part == 2 || part == 3) bins[part+2]++;\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n    if (i < N) {\n        double v = x[i];\n        double v4 = v * 4;\n        if (v4 < 0.25)\n            bins[0]++;\n        else if (v4 < 0.5)\n            bins[1]++;\n        else if (v4 < 0.75)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "// TODO: Your code here\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    size_t digit = 0;\n    if (x[i] >= 0.5) {\n      digit += 1;\n    }\n    if (x[i] >= 0.75) {\n      digit += 1;\n    }\n    atomicAdd(&bins[digit], 1);\n  }\n}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n  double q1 = 0.25, q2 = 0.5, q3 = 0.75;\n  if (idx < N) {\n    double v = x[idx];\n    bins[0] += v < q1;\n    bins[1] += v < q2 && v >= q1;\n    bins[2] += v < q3 && v >= q2;\n    bins[3] += v >= q3;\n  }\n}",
            "// TODO: complete this function\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  const size_t numBlocks = gridDim.x;\n\n  __shared__ size_t s_bins[4];\n\n  if (tid < 4) s_bins[tid] = 0;\n\n  __syncthreads();\n\n  for (size_t i = tid; i < N; i += numBlocks) {\n    double frac = fmod(x[i], 1.0);\n    if (frac < 0.25) s_bins[0]++;\n    else if (frac < 0.5) s_bins[1]++;\n    else if (frac < 0.75) s_bins[2]++;\n    else s_bins[3]++;\n  }\n\n  __syncthreads();\n\n  for (size_t i = tid; i < 4; i += numBlocks) {\n    atomicAdd(bins + i, s_bins[i]);\n  }\n}",
            "// Compute the index of this thread.\n  size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n  // Do work for as long as we're within bounds.\n  while (i < N) {\n    double value = x[i];\n\n    if (value < 0) {\n      value = 0;\n    }\n    else if (value > 1) {\n      value = 1;\n    }\n\n    double remainder = value - floor(value);\n\n    if (remainder < 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    else if (remainder < 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    else if (remainder < 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    else {\n      atomicAdd(&bins[3], 1);\n    }\n\n    // Increment the loop counter.\n    i += blockDim.x * gridDim.x;\n  }\n}",
            "__shared__ double buffer[256];\n\tsize_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (gid >= N) return;\n\tsize_t quartile = floor(x[gid] / 0.25);\n\tbuffer[threadIdx.x] = quartile;\n\t__syncthreads();\n\n\tint step = blockDim.x;\n\tint offset = 0;\n\twhile (step > 1) {\n\t\tint index = 2 * threadIdx.x + offset;\n\t\tint index2 = index + step;\n\t\tif (index < step) {\n\t\t\tif (index2 < step) {\n\t\t\t\tif (buffer[index] > buffer[index2]) {\n\t\t\t\t\tbuffer[index] += step;\n\t\t\t\t\tbuffer[index2] = buffer[index] - step;\n\t\t\t\t} else {\n\t\t\t\t\tbuffer[index2] += step;\n\t\t\t\t\tbuffer[index] = buffer[index2] - step;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tbuffer[index] += step;\n\t\t\t}\n\t\t}\n\t\t__syncthreads();\n\t\tstep = step / 2;\n\t\toffset = offset + step;\n\t}\n\tif (threadIdx.x == 0) {\n\t\tatomicAdd(&bins[buffer[0]], 1);\n\t\tatomicAdd(&bins[buffer[step]], 1);\n\t}\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t gid = hipBlockIdx_x * blockDim.x + tid;\n  size_t stride = hipGridDim_x * blockDim.x;\n  size_t local[4] = {0};\n\n  // Each thread processes 1024 elements\n  for (size_t i = gid; i < N; i += stride) {\n    double v = x[i];\n    if (v >= 0 && v < 0.25) {\n      atomicAdd(&local[0], 1);\n    } else if (v >= 0.25 && v < 0.5) {\n      atomicAdd(&local[1], 1);\n    } else if (v >= 0.5 && v < 0.75) {\n      atomicAdd(&local[2], 1);\n    } else if (v >= 0.75 && v < 1) {\n      atomicAdd(&local[3], 1);\n    }\n  }\n\n  // Sum local counts into global counts\n  for (int i = 0; i < 4; i++) {\n    atomicAdd(&bins[i], local[i]);\n  }\n}",
            "}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    double f = x[tid];\n    // check if the fractional part is within the interval\n    if (f >= 0 && f < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (f >= 0.25 && f < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (f >= 0.5 && f < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "const int tid = hipThreadIdx_x;\n  const int blockDim = hipBlockDim_x;\n  const int bid = hipBlockIdx_x;\n  const int blockSize = blockDim * gridDim;\n  const int binSize = N / 4;\n\n  for (size_t i = bid * binSize; i < (bid + 1) * binSize && i < N; i++) {\n    double value = x[i];\n    int bin = (value < 0.25) + (value < 0.5) * 2 + (value < 0.75) * 3;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// Your code here\n    double bin = (double)(x[blockDim.x * blockIdx.x + threadIdx.x]) / (N);\n    if (bin < 0.25)\n        atomicAdd(&bins[0], 1);\n    else if (bin < 0.5)\n        atomicAdd(&bins[1], 1);\n    else if (bin < 0.75)\n        atomicAdd(&bins[2], 1);\n    else\n        atomicAdd(&bins[3], 1);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (tid < N) {\n    double value = x[tid];\n    int q = 0;\n    if (value < 0.25)\n      q = 0;\n    else if (value < 0.5)\n      q = 1;\n    else if (value < 0.75)\n      q = 2;\n    else if (value <= 1.0)\n      q = 3;\n    atomicAdd(&bins[q], 1);\n  }\n}",
            "// TODO: implement the CUDA kernel that computes the quartiles\n    int tid = hipThreadIdx_x;\n    int blockId = hipBlockIdx_x;\n\n    int start = tid + blockId * blockDim.x;\n    if (start < N) {\n        double x_i = x[start];\n        if (x_i >= 0) {\n            if (x_i < 0.25) {\n                atomicAdd(&bins[0], 1);\n            } else if (x_i < 0.5) {\n                atomicAdd(&bins[1], 1);\n            } else if (x_i < 0.75) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "double d = 0;\n   __shared__ double sd;\n   if (threadIdx.x == 0) {\n      sd = 0;\n   }\n   __syncthreads();\n\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      d = x[i];\n      if (d < 0.25) {\n         sd += 1;\n      } else if (d < 0.5) {\n         sd += 2;\n      } else if (d < 0.75) {\n         sd += 3;\n      } else {\n         sd += 4;\n      }\n   }\n   __syncthreads();\n\n   atomicAdd(&bins[0], sd);\n}",
            "double x_int;\n\n    size_t tid = hipThreadIdx_x;\n    size_t idx = hipBlockIdx_x * hipBlockDim_x + tid;\n\n    if (idx < N) {\n        x_int = floor(x[idx]);\n        if (x[idx] - x_int < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (x[idx] - x_int < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (x[idx] - x_int < 0.75)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    double xi = x[tid];\n    if (xi < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (xi < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (xi < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "int gid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  int stride = hipGridDim_x * hipBlockDim_x;\n  int n_blocks = (N + hipGridDim_x * hipBlockDim_x - 1) / (hipGridDim_x * hipBlockDim_x);\n  int block_sum = 0;\n  for (int i = 0; i < n_blocks; i++) {\n    double v = x[gid + i * stride];\n    block_sum += ((v >= 0.0) && (v < 0.25));\n    block_sum += ((v >= 0.25) && (v < 0.5));\n    block_sum += ((v >= 0.5) && (v < 0.75));\n    block_sum += ((v >= 0.75) && (v <= 1.0));\n  }\n  atomicAdd(&bins[0], block_sum);\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n\t// TODO: your code goes here\n\t__shared__ size_t local_bins[4];\n\n\tif(tid < N){\n\t\tdouble temp = x[tid];\n\t\ttemp = temp * 4;\n\t\ttemp = temp - floor(temp);\n\n\t\tif(temp <= 0.25){\n\t\t\tatomicAdd(&local_bins[0], 1);\n\t\t}else if(temp <= 0.5){\n\t\t\tatomicAdd(&local_bins[1], 1);\n\t\t}else if(temp <= 0.75){\n\t\t\tatomicAdd(&local_bins[2], 1);\n\t\t}else{\n\t\t\tatomicAdd(&local_bins[3], 1);\n\t\t}\n\t}\n\n\t// The shared memory is initialized with 0.\n\t// Wait for all threads to finish counting.\n\t__syncthreads();\n\n\tif(tid == 0){\n\t\tfor(int i=0; i<4; i++){\n\t\t\tatomicAdd(&bins[i], local_bins[i]);\n\t\t}\n\t}\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    double x_i = x[i];\n    size_t bin = (x_i < 0.25)? 0 : ((x_i < 0.5)? 1 : ((x_i < 0.75)? 2 : 3));\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  __shared__ double s[4];\n\n  if (i >= N)\n    return;\n\n  size_t j = floor(x[i] * 4.0);\n  if (j >= 4)\n    j = 3;\n\n  atomicAdd(&bins[j], 1);\n}",
            "int tid = blockIdx.x*blockDim.x+threadIdx.x;\n\n  if (tid < N) {\n    double x_i = x[tid];\n    int bin = 0;\n    if (x_i <= 0.25) {\n      bin = 0;\n    } else if (x_i <= 0.5) {\n      bin = 1;\n    } else if (x_i <= 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// YOUR CODE HERE\n  // Remember that 0.5 belongs to both 0 and 1, so you need to count it twice\n  //\n  // HINT: look at the sample code in Quartiles.cu for some hints.\n\n  // HINT: if you want to store the output in a variable, you must declare\n  // it as a local variable, otherwise it will be stored in global memory\n  // and you won't get the correct result.\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t binIndex = 0;\n   for(size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n      double x_i = x[i];\n      if (x_i < 0.25) {\n         binIndex = 0;\n      } else if (x_i < 0.5) {\n         binIndex = 1;\n      } else if (x_i < 0.75) {\n         binIndex = 2;\n      } else {\n         binIndex = 3;\n      }\n      atomicAdd(&bins[binIndex], 1);\n   }\n}",
            "// TODO\n}",
            "// TODO: Your code here\n}",
            "size_t thread_id = threadIdx.x;\n  size_t block_id = blockIdx.x;\n  __shared__ size_t block_sums[4];\n\n  /* Compute the number of doubles in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and\n     [0.75, 1) in parallel using AMD HIP. */\n  if (thread_id < 4) {\n    size_t i = block_id * blockDim.x + thread_id;\n    double x_i = x[i];\n    size_t num = 0;\n    if (x_i >= 0 && x_i <= 0.25) {\n      num = 1;\n    }\n    else if (x_i > 0.25 && x_i <= 0.5) {\n      num = 2;\n    }\n    else if (x_i > 0.5 && x_i <= 0.75) {\n      num = 3;\n    }\n    else if (x_i > 0.75 && x_i <= 1) {\n      num = 4;\n    }\n    size_t tid = thread_id;\n    while (tid < N) {\n      double x_tid = x[tid];\n      if ((x_tid >= 0 && x_tid < 0.25) || (x_tid > 0.75 && x_tid <= 1)) {\n        atomicAdd(&block_sums[num - 1], 1);\n      }\n      tid += 4;\n    }\n  }\n  /* Add the block-level sums into `bins`. */\n  if (thread_id == 0) {\n    block_sums[0] += block_sums[1];\n    block_sums[2] += block_sums[3];\n    atomicAdd(&bins[0], block_sums[0]);\n    atomicAdd(&bins[1], block_sums[1]);\n    atomicAdd(&bins[2], block_sums[2]);\n    atomicAdd(&bins[3], block_sums[3]);\n  }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    // Compute which bin the value x[tid] falls into\n    size_t bin = 0;\n    if (x[tid] >= 0.0 && x[tid] < 0.25) {\n      bin = 0;\n    } else if (x[tid] >= 0.25 && x[tid] < 0.5) {\n      bin = 1;\n    } else if (x[tid] >= 0.5 && x[tid] < 0.75) {\n      bin = 2;\n    } else {\n      bin = 3;\n    }\n\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int stride = gridDim.x * blockDim.x;\n\n  while (tid < N) {\n    double d = x[tid];\n    int bin = 3;\n    if (d < 0.25) {\n      bin = 0;\n    } else if (d < 0.50) {\n      bin = 1;\n    } else if (d < 0.75) {\n      bin = 2;\n    }\n    atomicAdd(&bins[bin], 1);\n    tid += stride;\n  }\n}",
            "size_t thread_id = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  if (thread_id < N) {\n    double v = x[thread_id];\n    // TODO: Replace this with the fractional part of v\n    size_t i = v >= 0? 0 : 1;\n    if (v > 0.75) i += 2;\n    atomicAdd(bins + i, 1);\n  }\n}",
            "// TODO\n}",
            "size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t stride = gridDim.x * blockDim.x;\n   for(size_t i = start; i < N; i += stride) {\n      double value = x[i];\n      int bin = (value >= 0.25 && value < 0.5)? 1 :\n               (value >= 0.5 && value < 0.75)? 2 :\n               (value >= 0.75 && value <= 1)? 3 : 0;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  // each thread finds its own local bins\n  __shared__ size_t s_bins[4];\n  s_bins[0] = 0;\n  s_bins[1] = 0;\n  s_bins[2] = 0;\n  s_bins[3] = 0;\n\n  for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n    double xi = x[i];\n    // use integer modulus operator to map [0, 1) into [0, 4)\n    size_t bin = (size_t)(xi * 4) % 4;\n    s_bins[bin] += 1;\n  }\n\n  __syncthreads();\n\n  // add up all the local bins, one thread per bin\n  if (threadIdx.x == 0) {\n    for (size_t bin = 0; bin < 4; bin++) {\n      bins[bin] = s_bins[bin];\n    }\n  }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  __shared__ double sdata[1024];\n  int bin = tid / 4;\n  int offset = tid % 4;\n  sdata[tid] = (tid < N)? x[tid] : 0;\n  __syncthreads();\n\n  // Parallel reduction.\n  #pragma unroll\n  for (int stride = blockDim.x / 2; stride > 0; stride >>= 1) {\n    if (tid < stride) {\n      sdata[tid] += sdata[tid + stride];\n    }\n    __syncthreads();\n  }\n\n  if (tid == 0) {\n    atomicAdd(&bins[bin * 4 + offset], sdata[0]);\n  }\n}",
            "__shared__ double xsh[256];\n    __shared__ int idx[256];\n    int bid = blockIdx.x;\n    int tid = threadIdx.x;\n    size_t n = N / gridDim.x;\n    x += bid * n;\n    int i = bid * n + tid;\n    int num_chunks = n / 64;\n    double x_i;\n    int idx_i;\n    if (i < n) {\n        x_i = x[i];\n        if (x_i < 0.25) {\n            idx_i = 0;\n        } else if (x_i < 0.5) {\n            idx_i = 1;\n        } else if (x_i < 0.75) {\n            idx_i = 2;\n        } else {\n            idx_i = 3;\n        }\n        idx[tid] = idx_i;\n        xsh[tid] = x_i;\n    }\n    __syncthreads();\n    for (int chunk = 0; chunk < num_chunks; chunk++) {\n        int i = tid;\n        if (i < 64) {\n            x_i = xsh[64 * chunk + i];\n            idx_i = idx[64 * chunk + i];\n            if (x_i < 0.25) {\n                idx_i = 0;\n            } else if (x_i < 0.5) {\n                idx_i = 1;\n            } else if (x_i < 0.75) {\n                idx_i = 2;\n            } else {\n                idx_i = 3;\n            }\n            idx[i] = idx_i;\n        }\n        __syncthreads();\n    }\n    if (i < n) {\n        int idx_i = idx[tid];\n        atomicAdd(bins + idx_i, 1);\n    }\n}",
            "// TODO\n  const int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  const double *const x_block = x + tid * N;\n  const int offset = tid * 4;\n\n  int k = 0;\n  double quartile;\n  for (int i = 0; i < N; i++) {\n    quartile = x_block[i] * 0.25;\n    if (quartile >= 0 && quartile < 0.25)\n      atomicAdd(&bins[offset + k], 1);\n    else if (quartile >= 0.25 && quartile < 0.5)\n      atomicAdd(&bins[offset + k + 1], 1);\n    else if (quartile >= 0.5 && quartile < 0.75)\n      atomicAdd(&bins[offset + k + 2], 1);\n    else if (quartile >= 0.75)\n      atomicAdd(&bins[offset + k + 3], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t bid = blockIdx.x;\n  size_t gid = bid * blockDim.x + tid;\n  if (gid < N) {\n    double u = x[gid];\n    bins[0] += (u >= 0 && u < 0.25)? 1 : 0;\n    bins[1] += (u >= 0.25 && u < 0.5)? 1 : 0;\n    bins[2] += (u >= 0.5 && u < 0.75)? 1 : 0;\n    bins[3] += (u >= 0.75 && u < 1)? 1 : 0;\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\tsize_t stride = hipGridDim_x * hipBlockDim_x;\n\n\t// Initialize the quartiles to zero.\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\tfor (; tid < N; tid += stride) {\n\t\tdouble val = x[tid];\n\t\tif (val >= 0.0 && val < 0.25) {\n\t\t\tbins[0] += 1;\n\t\t} else if (val >= 0.25 && val < 0.5) {\n\t\t\tbins[1] += 1;\n\t\t} else if (val >= 0.5 && val < 0.75) {\n\t\t\tbins[2] += 1;\n\t\t} else if (val >= 0.75 && val < 1.0) {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "size_t bin = (size_t)threadIdx.x;\n    __shared__ double smem[4];\n    if (bin < 4) {\n        size_t start = (bin * N) / 4;\n        size_t end = ((bin + 1) * N) / 4;\n        smem[bin] = 0;\n        for (size_t i = start; i < end; ++i) {\n            double x = x[i];\n            if (x >= 0 && x < 0.25)\n                smem[bin]++;\n            else if (x >= 0.25 && x < 0.5)\n                smem[bin + 1]++;\n            else if (x >= 0.5 && x < 0.75)\n                smem[bin + 2]++;\n            else if (x >= 0.75)\n                smem[bin + 3]++;\n        }\n    }\n    __syncthreads();\n    if (bin < 4)\n        atomicAdd(&bins[bin], smem[bin]);\n}",
            "size_t thread = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  double fractional;\n\n  for (size_t i = thread; i < N; i += hipGridDim_x * hipBlockDim_x) {\n    fractional = fmod(x[i], 1.0);\n\n    if (fractional <= 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (fractional <= 0.50)\n      atomicAdd(&bins[1], 1);\n    else if (fractional <= 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO\n}",
            "const int tid = hipThreadIdx_x;\n\n  // Divide the vector in 4 parts, compute the counts for each part, \n  // and accumulate the counts into bins.\n  //\n  // TODO: Replace the following code with a more efficient implementation.\n  // Your code must be correct!\n  //\n  // Hint: You can use the value of x[N] as an indicator that the value is \n  //       out of the [0, 1) range.\n  if (tid == 0) {\n    int i = 0;\n    for (; i < N; i++) {\n      if (x[i] >= 0.0 && x[i] < 0.25) {\n        bins[0]++;\n      } else if (x[i] >= 0.25 && x[i] < 0.50) {\n        bins[1]++;\n      } else if (x[i] >= 0.50 && x[i] < 0.75) {\n        bins[2]++;\n      } else if (x[i] >= 0.75 && x[i] <= 1.0) {\n        bins[3]++;\n      } else {\n        // Out of range\n      }\n    }\n  }\n}",
            "// The starting index of this block\n    int blockStart = blockIdx.x * blockDim.x;\n    // The ending index of this block\n    int blockEnd = min(N, blockStart + blockDim.x);\n    // The starting index of this thread (in the block)\n    int threadStart = threadIdx.x;\n    // The ending index of this thread (in the block)\n    int threadEnd = min(blockEnd, threadStart + blockDim.x);\n\n    // Each thread loops over its block.\n    for (int i = threadStart; i < blockEnd; i += blockDim.x) {\n        double value = x[i];\n        if (value >= 0.0 && value < 0.25) {\n            atomicAdd(bins + 0, 1);\n        } else if (value >= 0.25 && value < 0.5) {\n            atomicAdd(bins + 1, 1);\n        } else if (value >= 0.5 && value < 0.75) {\n            atomicAdd(bins + 2, 1);\n        } else if (value >= 0.75 && value <= 1.0) {\n            atomicAdd(bins + 3, 1);\n        }\n    }\n}",
            "size_t gid = threadIdx.x + blockIdx.x * blockDim.x;\n  double quartile_0 = 0.25 * N;\n  double quartile_1 = 0.50 * N;\n  double quartile_2 = 0.75 * N;\n  size_t quartile_3 = N;\n\n  for (; gid < N; gid += gridDim.x * blockDim.x) {\n    double xi = x[gid];\n    // xi is in [0, 1]\n    if (xi < quartile_0)\n      atomicAdd(&bins[0], 1);\n    else if (xi < quartile_1)\n      atomicAdd(&bins[1], 1);\n    else if (xi < quartile_2)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  __shared__ double temp[1024];\n  temp[threadIdx.x] = (i < N)? x[i] : 0;\n  __syncthreads();\n  if (threadIdx.x < 256) {\n    temp[threadIdx.x] += temp[threadIdx.x + 256];\n  }\n  __syncthreads();\n  if (threadIdx.x < 128) {\n    temp[threadIdx.x] += temp[threadIdx.x + 128];\n  }\n  __syncthreads();\n  if (threadIdx.x < 64) {\n    temp[threadIdx.x] += temp[threadIdx.x + 64];\n  }\n  __syncthreads();\n  if (threadIdx.x < 32) {\n    temp[threadIdx.x] += temp[threadIdx.x + 32];\n  }\n  __syncthreads();\n  if (threadIdx.x < 16) {\n    temp[threadIdx.x] += temp[threadIdx.x + 16];\n  }\n  __syncthreads();\n  if (threadIdx.x < 8) {\n    temp[threadIdx.x] += temp[threadIdx.x + 8];\n  }\n  __syncthreads();\n  if (threadIdx.x < 4) {\n    temp[threadIdx.x] += temp[threadIdx.x + 4];\n  }\n  __syncthreads();\n  if (threadIdx.x < 2) {\n    temp[threadIdx.x] += temp[threadIdx.x + 2];\n  }\n  __syncthreads();\n  if (threadIdx.x < 1) {\n    temp[threadIdx.x] += temp[threadIdx.x + 1];\n  }\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    bins[0] = (temp[0] < 0.25)? 1 : 0;\n    bins[1] = (temp[0] < 0.5)? (temp[0] < 0.25) : 0;\n    bins[2] = (temp[0] < 0.75)? (temp[0] < 0.5) : 0;\n    bins[3] = (temp[0] < 1)? (temp[0] < 0.75) : 0;\n  }\n}",
            "double sum = 0;\n  for (size_t i = 0; i < N; i++) {\n    size_t bin = ((size_t)(x[i] / 0.25) + 1) % 4;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t blocks = blockIdx.x;\n  double thread_x = x[blocks * blockDim.x + tid];\n  size_t frac = (size_t)(thread_x * 4);\n  __syncthreads();\n  if (frac < 1)\n    atomicAdd(&bins[0], 1);\n  else if (frac < 2)\n    atomicAdd(&bins[1], 1);\n  else if (frac < 3)\n    atomicAdd(&bins[2], 1);\n  else\n    atomicAdd(&bins[3], 1);\n}",
            "double x_i;\n  size_t index = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  size_t thread_id = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n\n  if (index < N) {\n    x_i = x[index];\n\n    if (x_i >= 0.0 && x_i <= 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x_i > 0.25 && x_i <= 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x_i > 0.5 && x_i <= 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (x_i > 0.75 && x_i <= 1.0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t start = hipThreadIdx_x;\n  size_t end = N;\n\n  // Your code here\n  size_t start_index = start*2;\n  size_t end_index = end*2;\n\n  for (size_t i=start_index; i<end_index; i+=4) {\n    size_t idx = i / 2;\n    double value = x[idx];\n    // 0 <= value < 0.25\n    if (value >= 0 && value < 0.25) {\n      atomicAdd(&bins[0], 1);\n    }\n    // 0.25 <= value < 0.5\n    if (value >= 0.25 && value < 0.5) {\n      atomicAdd(&bins[1], 1);\n    }\n    // 0.5 <= value < 0.75\n    if (value >= 0.5 && value < 0.75) {\n      atomicAdd(&bins[2], 1);\n    }\n    // 0.75 <= value <= 1\n    if (value >= 0.75 && value <= 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "/* Compute the bin that `x[i]` will be placed into. */\n  int bin = int(x[blockIdx.x * blockDim.x + threadIdx.x] * 4.0);\n\n  /* Since x is stored in shared memory, there are 4 copies of the value. Only\n     one of these copies will be updated by the kernel. */\n  double x_copy = x[blockIdx.x * blockDim.x + threadIdx.x];\n  if (bin == 0 || bin == 3) x_copy = 1.0 - x[blockIdx.x * blockDim.x + threadIdx.x];\n\n  /* Atomically increment the appropriate bin. */\n  atomicAdd(&bins[bin], 1);\n}",
            "size_t i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  double xp;\n  size_t bin;\n  if (i < N) {\n    xp = x[i];\n    bin = 0;\n    if (xp >= 0.0 && xp < 0.25) bin = 0;\n    else if (xp >= 0.25 && xp < 0.5) bin = 1;\n    else if (xp >= 0.5 && xp < 0.75) bin = 2;\n    else if (xp >= 0.75 && xp <= 1.0) bin = 3;\n    atomicAdd(bins + bin, 1);\n  }\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t blockDim = hipBlockDim_x;\n   size_t i = hipBlockIdx_x * blockDim + tid;\n\n   if (i < N) {\n      double val = x[i];\n      size_t bin = 0;\n      if (val < 0.25)\n         bin = 0;\n      else if (val < 0.5)\n         bin = 1;\n      else if (val < 0.75)\n         bin = 2;\n      else\n         bin = 3;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  int bin[4];\n  for (int i = 0; i < 4; i++) {\n    bin[i] = 0;\n  }\n\n  if (tid < N) {\n    double x_i = x[tid];\n    for (int i = 0; i < 4; i++) {\n      if (x_i >= (double) i * 0.25 && x_i < (double) (i + 1) * 0.25) {\n        bin[i] += 1;\n      }\n    }\n  }\n\n  for (int i = 0; i < 4; i++) {\n    atomicAdd(&bins[i], bin[i]);\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  __shared__ int cache[256];\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  while (tid < N) {\n    cache[threadIdx.x] = (x[tid] >= 0.0 && x[tid] < 0.25)? 1 : 0;\n    __syncthreads();\n    bins[0] += cache[threadIdx.x];\n\n    cache[threadIdx.x] = (x[tid] >= 0.25 && x[tid] < 0.5)? 1 : 0;\n    __syncthreads();\n    bins[1] += cache[threadIdx.x];\n\n    cache[threadIdx.x] = (x[tid] >= 0.5 && x[tid] < 0.75)? 1 : 0;\n    __syncthreads();\n    bins[2] += cache[threadIdx.x];\n\n    cache[threadIdx.x] = (x[tid] >= 0.75 && x[tid] <= 1.0)? 1 : 0;\n    __syncthreads();\n    bins[3] += cache[threadIdx.x];\n\n    tid += blockDim.x * gridDim.x;\n  }\n}",
            "int tid = threadIdx.x;\n\n  for (size_t i = tid; i < N; i += blockDim.x) {\n    double fraction = x[i] - floor(x[i]);\n    if (fraction < 0.25) {\n      bins[0] += 1;\n    } else if (fraction < 0.5) {\n      bins[1] += 1;\n    } else if (fraction < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "for (size_t i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n    // 1) Compute `i / 4`, i.e. the index in [0, 4) of the sub-bin that `x[i]` falls into.\n    // 2) Compute `1 << i / 4` and `x[i] & (1 << i / 4)`, i.e. the mask for `x[i]` and the\n    //    mask for the sub-bin that `x[i]` falls into.\n    // 3) Perform a `count` if the mask matches both, i.e. if `x[i]` falls into the sub-bin.\n    const size_t index = i >> 2;\n    const size_t mask = 1 << (i & 0x3);\n    if ((x[i] & mask) == mask) {\n      atomicAdd(&bins[index], 1);\n    }\n  }\n}",
            "// TODO: replace this with a parallel reduction\n\t//  (see https://developer.nvidia.com/blog/even-easier-introduction-cuda/)\n\n\t// TODO: replace this with a parallel for-loop over N\n\t//  (see https://developer.nvidia.com/blog/even-easier-introduction-cuda/)\n\n\t// TODO: replace this with a parallel for-loop over 4 blocks\n\t//  (see https://developer.nvidia.com/blog/even-easier-introduction-cuda/)\n\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n}",
            "__shared__ int numThreads;\n  if (threadIdx.x == 0)\n    numThreads = hipBlockDim_x;\n  __syncthreads();\n\n  double threadSum = 0;\n  double threadMin = INFINITY;\n  double threadMax = -INFINITY;\n\n  size_t tid = threadIdx.x + hipBlockIdx_x * hipBlockDim_x;\n  while (tid < N) {\n    double val = x[tid];\n    if (val < threadMin)\n      threadMin = val;\n    if (val > threadMax)\n      threadMax = val;\n    threadSum += val;\n    tid += numThreads;\n  }\n  __syncthreads();\n\n  __shared__ double globalSum;\n  __shared__ double globalMin;\n  __shared__ double globalMax;\n  if (threadIdx.x == 0) {\n    globalSum = 0;\n    globalMin = INFINITY;\n    globalMax = -INFINITY;\n  }\n  __syncthreads();\n\n  threadSum += globalSum;\n  threadMin = fmin(threadMin, globalMin);\n  threadMax = fmax(threadMax, globalMax);\n\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    globalSum = threadSum;\n    globalMin = threadMin;\n    globalMax = threadMax;\n  }\n  __syncthreads();\n\n  threadSum += globalSum;\n  threadMin = fmin(threadMin, globalMin);\n  threadMax = fmax(threadMax, globalMax);\n\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    globalSum = threadSum;\n    globalMin = threadMin;\n    globalMax = threadMax;\n  }\n  __syncthreads();\n\n  threadSum += globalSum;\n  threadMin = fmin(threadMin, globalMin);\n  threadMax = fmax(threadMax, globalMax);\n\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    globalSum = threadSum;\n    globalMin = threadMin;\n    globalMax = threadMax;\n  }\n  __syncthreads();\n\n  int tidInBlock = threadIdx.x;\n\n  // Compute the number of elements in [0, 0.25)\n  // The first 25% are in [globalMin, 0.25 * (globalMin + globalSum)]\n  if (threadSum > 0 && tidInBlock < 0.25 * globalSum)\n    atomicAdd(&bins[0], 1);\n\n  // Compute the number of elements in [0.25, 0.5)\n  // The next 25% are in [0.25 * (globalMin + globalSum), 0.5 * (globalMin + globalSum)]\n  if (threadSum > 0 && tidInBlock < 0.5 * globalSum)\n    atomicAdd(&bins[1], 1);\n\n  // Compute the number of elements in [0.5, 0.75)\n  // The next 25% are in [0.5 * (globalMin + globalSum), 0.75 * (globalMin + globalSum)]\n  if (threadSum > 0 && tidInBlock < 0.75 * globalSum)\n    atomicAdd(&bins[2], 1);\n\n  // Compute the number of elements in [0.75, 1)\n  // The final 25% are in [0.75 * (globalMin + globalSum), globalMax]\n  if (threadSum > 0 && tidInBlock < globalMax)\n    atomicAdd(&bins[3], 1);\n\n}",
            "size_t tid = hipThreadIdx_x;\n  size_t bid = hipBlockIdx_x;\n  size_t numthreads = hipBlockDim_x;\n\n  size_t start = bid * numthreads * 25;\n  if (tid + start >= N)\n    return;\n\n  double xx = x[tid + start];\n  size_t bin = (size_t)((xx - (size_t)xx) / 0.25);\n\n  if (bin < 4)\n    atomicAdd(&bins[bin], 1);\n}",
            "int tid = threadIdx.x;\n  int bin0 = 0;\n  int bin1 = 0;\n  int bin2 = 0;\n  int bin3 = 0;\n\n  // For every 2048th value, check which bin it should be in.\n  for (size_t i = tid; i < N; i += 2048) {\n    double frac = x[i] - floor(x[i]);\n    if (frac >= 0.0 && frac < 0.25) {\n      bin0++;\n    } else if (frac >= 0.25 && frac < 0.5) {\n      bin1++;\n    } else if (frac >= 0.5 && frac < 0.75) {\n      bin2++;\n    } else {\n      bin3++;\n    }\n  }\n\n  // Reduce these counts using shuffle and atomics.\n  atomicAdd(&bins[0], bin0);\n  atomicAdd(&bins[1], bin1);\n  atomicAdd(&bins[2], bin2);\n  atomicAdd(&bins[3], bin3);\n}",
            "}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    int k = (x[i] * 4.0) + 0.5;\n    if (k == 0) ++bins[0];\n    else if (k < 4) ++bins[k - 1];\n  }\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    // TODO: Fill in the missing code to compute the quartile counts.\n    // Do not change this function.\n    double x_i = x[tid];\n    if (x_i <= 0.25) {\n      bins[0] += 1;\n    } else if (x_i <= 0.5) {\n      bins[1] += 1;\n    } else if (x_i <= 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "unsigned int tid = hipThreadIdx_x; // thread id\n    unsigned int bid = hipBlockIdx_x; // block id\n    unsigned int tid_in_block = tid % 32; // thread id in block\n    unsigned int bid_in_grid = tid / 32; // block id in grid\n    unsigned int tid_in_grid = tid_in_block + bid_in_grid * 32; // thread id in grid\n    unsigned int nthreads = blockDim.x * gridDim.x; // total number of threads in the grid\n\n    // number of threads in each block\n    const unsigned int nthreads_in_block = 256;\n    // number of blocks in each grid\n    const unsigned int nblocks = N / nthreads_in_block + 1;\n    // total number of threads in the grid\n    const unsigned int nthreads_in_grid = nblocks * nthreads_in_block;\n    // calculate the global thread id\n    unsigned int global_tid = bid * nthreads_in_block + tid_in_block;\n\n    // each block calculates the count for a single bin\n    int bin = global_tid % 4;\n    // each block calculates the starting position of that bin\n    unsigned int start = global_tid / 4 * (N / 4);\n\n    // each thread counts elements in a single bin\n    // and stores the result in the block-local shared memory\n    __shared__ unsigned int smem_count;\n    if (global_tid < nthreads_in_grid) {\n        smem_count = 0;\n    }\n\n    __syncthreads();\n    // each thread counts elements in a single bin\n    if (global_tid >= start && global_tid < start + N / 4) {\n        if (bin == 0 && x[global_tid] < 0.25) {\n            smem_count++;\n        }\n        else if (bin == 1 && x[global_tid] >= 0.25 && x[global_tid] < 0.5) {\n            smem_count++;\n        }\n        else if (bin == 2 && x[global_tid] >= 0.5 && x[global_tid] < 0.75) {\n            smem_count++;\n        }\n        else if (bin == 3 && x[global_tid] >= 0.75) {\n            smem_count++;\n        }\n    }\n\n    __syncthreads();\n\n    // each thread accumulates the partial results\n    // in the block-local shared memory\n    if (tid_in_block == 0) {\n        unsigned int offset = 4 * bid_in_grid;\n        atomicAdd(&bins[offset + bin], smem_count);\n    }\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid >= N) return;\n\n  __shared__ int bins_shared[4];\n  bins_shared[0] = 0;\n  bins_shared[1] = 0;\n  bins_shared[2] = 0;\n  bins_shared[3] = 0;\n\n  const double a = x[tid];\n  const double b = a - 0.25;\n  const double c = a - 0.5;\n  const double d = a - 0.75;\n  const double f = fabs(b);\n  const double g = fabs(c);\n  const double h = fabs(d);\n  const int f0 = f < 0.25;\n  const int g0 = g < 0.25;\n  const int h0 = h < 0.25;\n  const int f1 = f0 + g0;\n  const int g1 = f0 + g0 + h0;\n  const int f2 = f1 + g1;\n  const int f3 = f2 + h0;\n  const int f4 = f3 + g1;\n  const int f5 = f4 + g0;\n  const int f6 = f5 + g1;\n  const int f7 = f6 + h0;\n  const int f8 = f7 + g0;\n  const int f9 = f8 + g1;\n\n  if (f0) bins_shared[0]++;\n  if (f1) bins_shared[1]++;\n  if (f2) bins_shared[2]++;\n  if (f3) bins_shared[3]++;\n  if (f4) bins_shared[0]++;\n  if (f5) bins_shared[1]++;\n  if (f6) bins_shared[2]++;\n  if (f7) bins_shared[3]++;\n  if (f8) bins_shared[0]++;\n  if (f9) bins_shared[1]++;\n  __syncthreads();\n\n  if (hipThreadIdx_x < 4)\n    bins[hipThreadIdx_x] += bins_shared[hipThreadIdx_x];\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx < N) {\n    double y = x[idx];\n    if (y < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (y < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (y < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// YOUR CODE HERE\n  int idx = threadIdx.x + blockIdx.x * blockDim.x;\n  if (idx < N) {\n    double value = x[idx];\n    if (value >= 0 && value < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (value >= 0.25 && value < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (value >= 0.5 && value < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (value >= 0.75 && value < 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "size_t n = blockDim.x * blockIdx.x + threadIdx.x;\n    if (n < N) {\n        double v = x[n];\n        if (v < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (v < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (v < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t thread = threadIdx.x + blockIdx.x * blockDim.x;\n  if (thread < N) {\n    int bin = 0;\n    double value = x[thread];\n    if (value >= 0.25 && value < 0.5)\n      bin = 1;\n    else if (value >= 0.5 && value < 0.75)\n      bin = 2;\n    else if (value >= 0.75 && value <= 1)\n      bin = 3;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n  size_t binIdx = 0;\n  if (tid < N) {\n    double x_i = x[tid];\n    double frac = x_i - floor(x_i);\n    if (frac < 0.25)\n      binIdx = 0;\n    else if (frac < 0.5)\n      binIdx = 1;\n    else if (frac < 0.75)\n      binIdx = 2;\n    else\n      binIdx = 3;\n    atomicAdd(&bins[binIdx], 1);\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x * blockDim.x + tid;\n  __shared__ size_t sbins[4];\n  size_t idx = tid;\n  sbins[0] = 0;\n  sbins[1] = 0;\n  sbins[2] = 0;\n  sbins[3] = 0;\n  while (idx < N) {\n    double xi = x[idx];\n    if (xi < 0.25)\n      sbins[0]++;\n    else if (xi < 0.5)\n      sbins[1]++;\n    else if (xi < 0.75)\n      sbins[2]++;\n    else\n      sbins[3]++;\n    idx += blockDim.x * gridDim.x;\n  }\n  __syncthreads();\n  atomicAdd(&bins[0], sbins[0]);\n  atomicAdd(&bins[1], sbins[1]);\n  atomicAdd(&bins[2], sbins[2]);\n  atomicAdd(&bins[3], sbins[3]);\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n    if(tid < N) {\n        double v = x[tid];\n        if (v < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (v < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (v < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (tid >= N) return;\n   \n   double v = fabs(x[tid]);\n   if (v < 0.25) bins[0]++;\n   else if (v < 0.5) bins[1]++;\n   else if (v < 0.75) bins[2]++;\n   else bins[3]++;\n}",
            "// Your code here...\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= N) return;\n    double x_i = x[i];\n    // Determine which bin this x_i is in.\n    // We do it using the binary search technique.\n    int bin = 0;\n    double q0 = 0, q1 = 0, q2 = 0, q3 = 0;\n    while (q0 <= x_i && x_i < q3) {\n        double q_tmp = 0.25 * (q0 + q3);\n        if (q0 <= x_i && x_i < q_tmp) {\n            bin = 0;\n            q0 = q_tmp;\n        } else if (q_tmp <= x_i && x_i < q1) {\n            bin = 1;\n            q1 = q_tmp;\n        } else if (q1 <= x_i && x_i < q2) {\n            bin = 2;\n            q2 = q_tmp;\n        } else {\n            bin = 3;\n            q3 = q_tmp;\n        }\n    }\n    atomicAdd(&bins[bin], 1);\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (i >= N)\n    return;\n  double xi = x[i];\n  if (xi <= 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (xi <= 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (xi <= 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: implement a kernel that counts the number of elements of x that have\n  //       fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  //       Store the counts in `bins`.\n}",
            "double xVal;\n    // Your code goes here\n}",
            "// The following code is a simple example of how to use blockIdx and threadIdx\n  // to parallelize. You shouldn't need to change it.\n  const size_t block = blockIdx.x;\n  const size_t thread = threadIdx.x;\n  const size_t stride = blockDim.x;\n  const size_t chunk = (N - 1) / stride + 1;\n  const size_t first = thread + block * stride;\n  const size_t last = min(first + chunk, N);\n  // Initialize all threads to 0\n  for (size_t i = 0; i < 4; i++)\n    bins[i] = 0;\n\n  for (size_t i = first; i < last; i++) {\n    const double xi = x[i];\n    // Count all the elements that are in [0, 0.25)\n    if (xi < 0.25)\n      atomicAdd(&bins[0], 1);\n    // Count all the elements that are in [0.25, 0.5)\n    else if (xi < 0.5)\n      atomicAdd(&bins[1], 1);\n    // Count all the elements that are in [0.5, 0.75)\n    else if (xi < 0.75)\n      atomicAdd(&bins[2], 1);\n    // Count all the elements that are in [0.75, 1)\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "const size_t thread_idx = blockIdx.x * blockDim.x + threadIdx.x;\n  const size_t thread_idy = blockIdx.y * blockDim.y + threadIdx.y;\n\n  if (thread_idx < N && thread_idy == 0) {\n    const double val = x[thread_idx];\n    const double q1 = val >= 0.5? 0.5 : 0.25;\n    const double q2 = val >= 0.75? 0.5 : 0.25;\n    const double q3 = val >= 0.25? 0.75 : 0.5;\n    const double q4 = val >= 0.5? 0.75 : 0.5;\n    const double diff12 = q2 - q1;\n    const double diff34 = q4 - q3;\n    const double diff13 = q3 - q1;\n    const double diff24 = q4 - q2;\n    if (q1 < val && val < q2) {\n      bins[0] += 1;\n    } else if (q3 < val && val < q4) {\n      bins[1] += 1;\n    } else if (diff12 > 0 && q1 < val && val < diff12 + q1) {\n      bins[2] += 1;\n    } else if (diff34 > 0 && diff24 > 0 && q2 < val && val < diff24 + q2) {\n      bins[3] += 1;\n    }\n  }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t count[4];\n  if (i < N) {\n    count[0] = (x[i] < 0.25);\n    count[1] = (0.25 <= x[i]) && (x[i] < 0.5);\n    count[2] = (0.5 <= x[i]) && (x[i] < 0.75);\n    count[3] = (0.75 <= x[i]);\n    bins[0] += count[0];\n    bins[1] += count[1];\n    bins[2] += count[2];\n    bins[3] += count[3];\n  }\n}",
            "int gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    if (gid < N) {\n        int bin = 0;\n        if (x[gid] < 0.25) bin = 0;\n        if (x[gid] >= 0.25 && x[gid] < 0.5) bin = 1;\n        if (x[gid] >= 0.5 && x[gid] < 0.75) bin = 2;\n        if (x[gid] >= 0.75) bin = 3;\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double y = x[tid];\n    int i = (y < 0.5)? 0 : 1;\n    int j = (y < 0.25)? 0 : ((y < 0.5)? 1 : ((y < 0.75)? 2 : 3));\n    atomicAdd(&bins[i], 1);\n    atomicAdd(&bins[j + 4], 1);\n  }\n}",
            "size_t tidx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tidx < N) {\n    double value = x[tidx];\n    size_t bin = 0;\n    if (value >= 0.0 && value < 0.25) bin = 0;\n    else if (value >= 0.25 && value < 0.5) bin = 1;\n    else if (value >= 0.5 && value < 0.75) bin = 2;\n    else if (value >= 0.75 && value <= 1.0) bin = 3;\n    atomicAdd(&(bins[bin]), 1);\n  }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n  if (tid < N) {\n    double temp = x[tid];\n    if (temp >= 0 && temp < 0.25) {\n      atomicAdd(&(bins[0]), 1);\n    } else if (temp >= 0.25 && temp < 0.5) {\n      atomicAdd(&(bins[1]), 1);\n    } else if (temp >= 0.5 && temp < 0.75) {\n      atomicAdd(&(bins[2]), 1);\n    } else {\n      atomicAdd(&(bins[3]), 1);\n    }\n  }\n}",
            "int bin = (threadIdx.x + blockIdx.x * blockDim.x) / 4;\n  if (bin < 4) {\n    // compute quartile bound\n    double lb = bin * 0.25;\n    double ub = (bin + 1) * 0.25;\n    __shared__ double shSum[256];\n    shSum[threadIdx.x] = 0.0;\n    size_t n = 0;\n    for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      double xi = x[i];\n      if (xi >= lb && xi < ub) {\n        shSum[threadIdx.x] += 1;\n      }\n    }\n    __syncthreads();\n    for (int s = 1; s < blockDim.x; s *= 2) {\n      size_t t = threadIdx.x + s;\n      if (t < blockDim.x) {\n        shSum[threadIdx.x] += shSum[t];\n      }\n      __syncthreads();\n    }\n    if (threadIdx.x == 0) {\n      bins[bin] = shSum[0];\n    }\n  }\n}",
            "const size_t t = threadIdx.x;\n\n  // Calculate a partition using a prefix sum with atomicAdd.\n  // `part` is the index of the first element that falls into the current quartile\n  // (i.e. [part, part + 1] contains elements in the current quartile).\n  // For example, if the quartiles are:\n  // [0, 0.25), [0.25, 0.5), [0.5, 0.75), [0.75, 1]\n  // and the vector x is: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  // then the partition would be: [0, 2, 3, 6]\n  size_t part = 0;\n  for (size_t i = t; i < N; i += blockDim.x) {\n    double f = x[i] * 4.0;\n    double frac = f - floor(f);\n    if (frac > 0.75) {\n      part = i + 1;\n    }\n  }\n  size_t total;\n  for (int offset = 16; offset > 0; offset /= 2) {\n    total = __shfl_xor_sync(0xffffffff, part, offset, blockDim.x);\n    part += __shfl_xor_sync(0xffffffff, total, offset, blockDim.x);\n  }\n  if (t == 0) {\n    atomicAdd(&bins[0], part);\n  }\n\n  // Do the same thing, but for the other quartiles.\n  part = N;\n  for (size_t i = t; i < N; i += blockDim.x) {\n    double f = x[i] * 4.0;\n    double frac = f - floor(f);\n    if (frac < 0.25) {\n      part = i;\n    }\n  }\n  for (int offset = 16; offset > 0; offset /= 2) {\n    total = __shfl_xor_sync(0xffffffff, part, offset, blockDim.x);\n    part += __shfl_xor_sync(0xffffffff, total, offset, blockDim.x);\n  }\n  if (t == 0) {\n    atomicAdd(&bins[1], part - N);\n  }\n\n  part = N;\n  for (size_t i = t; i < N; i += blockDim.x) {\n    double f = x[i] * 4.0;\n    double frac = f - floor(f);\n    if (frac < 0.5) {\n      part = i;\n    }\n  }\n  for (int offset = 16; offset > 0; offset /= 2) {\n    total = __shfl_xor_sync(0xffffffff, part, offset, blockDim.x);\n    part += __shfl_xor_sync(0xffffffff, total, offset, blockDim.x);\n  }\n  if (t == 0) {\n    atomicAdd(&bins[2], part - N);\n  }\n\n  part = N;\n  for (size_t i = t; i < N; i += blockDim.x) {\n    double f = x[i] * 4.0;\n    double frac = f - floor(f);\n    if (frac < 0.75) {\n      part = i;\n    }\n  }\n  for (int offset = 16; offset > 0; offset /= 2) {\n    total = __shfl_xor_sync(0xffffffff, part, offset, blockDim.x);\n    part += __shfl_xor_sync(0xffffffff, total, offset, blockDim.x);\n  }\n  if (t == 0) {\n    atomicAdd(&bins[3], part - N);\n  }\n}",
            "for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    double bin = x[i] * 4.0;\n    if (bin < 0.25)\n      bins[0]++;\n    else if (bin < 0.5)\n      bins[1]++;\n    else if (bin < 0.75)\n      bins[2]++;\n    else\n      bins[3]++;\n  }\n}",
            "for (int tid = threadIdx.x + blockDim.x*blockIdx.x; tid < N; tid += blockDim.x * gridDim.x) {\n    // YOUR CODE HERE\n    int index = __clz(tid);\n    int bin = index & 3;\n    int offset = index >> 2;\n    if (x[tid] < offset + 1) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n  int blockDim2 = blockDim.x * blockDim.y;\n  int i = id / blockDim2;\n  int j = id % blockDim2;\n\n  __shared__ double buffer[1024];\n  __shared__ size_t counts[4];\n\n  if (id < 1024) {\n    buffer[id] = 0.0;\n    counts[0] = 0;\n    counts[1] = 0;\n    counts[2] = 0;\n    counts[3] = 0;\n  }\n\n  __syncthreads();\n\n  while (i < N) {\n    double xi = x[i];\n    int index = 0;\n    if (xi <= 0.75)\n      index = 3;\n    else if (xi <= 0.5)\n      index = 2;\n    else if (xi <= 0.25)\n      index = 1;\n    atomicAdd(&counts[index], 1);\n    i += blockDim.x;\n  }\n\n  __syncthreads();\n\n  if (id < 1024)\n    buffer[id] = counts[j];\n\n  __syncthreads();\n\n  if (id == 0) {\n    atomicAdd(&bins[0], buffer[0]);\n    atomicAdd(&bins[1], buffer[1]);\n    atomicAdd(&bins[2], buffer[2]);\n    atomicAdd(&bins[3], buffer[3]);\n  }\n}",
            "size_t threadId = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n  size_t stride = hipBlockDim_x * hipGridDim_x;\n\n  size_t count = 0;\n  for (size_t i = threadId; i < N; i += stride) {\n    double bin = x[i] * 4; // [0, 1] -> [0, 4)\n    if (bin < 2.0)\n      count++;\n    else if (bin < 4.0)\n      atomicAdd(&bins[1], 1);\n    else if (bin < 6.0)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n  atomicAdd(&bins[0], count);\n}",
            "// YOUR CODE HERE\n  // You need to:\n  // 1. Find the thread id, the start and end of the data, and the data index \n  //    corresponding to the start of this thread in x.\n  // 2. For the data indices in the range, increment bins[i] for each value \n  //    of i that satisfies the bins criterion. You will have to use a \n  //    conditional to decide whether x[i] falls into a bin. The bins are\n  //    [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  // 3. Since we're using atomicInc, be sure to synchronize after each \n  //    update to bins.\n  const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    size_t start = tid;\n    size_t end = tid + N;\n    size_t i = tid;\n    while (i < end) {\n      if (x[i] >= 0.0 && x[i] < 0.25) {\n        atomicInc(&bins[0], N);\n      }\n      else if (x[i] >= 0.25 && x[i] < 0.50) {\n        atomicInc(&bins[1], N);\n      }\n      else if (x[i] >= 0.50 && x[i] < 0.75) {\n        atomicInc(&bins[2], N);\n      }\n      else if (x[i] >= 0.75 && x[i] < 1.0) {\n        atomicInc(&bins[3], N);\n      }\n      i += blockDim.x * gridDim.x;\n    }\n  }\n  __syncthreads();\n}",
            "/* Use shared memory to store temporary values. In this example, we only need a single element. */\n  __shared__ double s;\n\n  /* Each thread processes 2 elements. */\n  size_t i = 2 * blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = 2 * gridDim.x * blockDim.x;\n  double v;\n\n  /* Loop over the input vector, using a stride of 2*blockDim.x. */\n  while (i < 2 * N) {\n    v = x[i];\n    /* Compute 0.25 <= v < 0.5 */\n    if (0.25 <= v && v < 0.5) {\n      atomicAdd(&s, 1);\n    }\n    /* Compute 0.5 <= v < 0.75 */\n    if (0.5 <= v && v < 0.75) {\n      atomicAdd(&s, 1);\n    }\n    /* Compute 0.75 <= v < 1.0 */\n    if (0.75 <= v && v < 1.0) {\n      atomicAdd(&s, 1);\n    }\n    i += stride;\n  }\n\n  /* Copy the result from shared memory to global memory. */\n  if (threadIdx.x == 0) {\n    bins[0] = s;\n  }\n}",
            "// TODO: Your code here.\n}",
            "const int tid = threadIdx.x;\n    const int block_size = blockDim.x;\n    const int block_start = blockDim.x * blockIdx.x;\n    const int grid_size = gridDim.x;\n    const int warp_size = 32;\n\n    // Initialize the counters with 0\n    for (int j = tid; j < 4; j += block_size) {\n        bins[j] = 0;\n    }\n    __syncthreads();\n\n    // This kernel is launched with at least N threads\n    if (tid < N) {\n        // Find which interval the value belongs to.\n        int interval = 0;\n        double value = x[tid + block_start];\n        if (value < 0.25) {\n            interval = 0;\n        }\n        else if (value < 0.5) {\n            interval = 1;\n        }\n        else if (value < 0.75) {\n            interval = 2;\n        }\n        else {\n            interval = 3;\n        }\n\n        // Increment the count in the corresponding interval\n        atomicAdd(&bins[interval], 1);\n    }\n    __syncthreads();\n\n    // Reduce the number of counters in the threads that participate in the reduction\n    if (block_size >= warp_size) {\n        if (tid < warp_size) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n\n    if (block_size >= 64) {\n        if (tid < 32) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n\n    if (block_size >= 32) {\n        if (tid < 16) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n\n    if (block_size >= 16) {\n        if (tid < 8) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n\n    if (block_size >= 8) {\n        if (tid < 4) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n\n    if (block_size >= 4) {\n        if (tid < 2) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n\n    if (block_size >= 2) {\n        if (tid < 1) {\n            bins[0] += bins[0 + tid];\n            bins[1] += bins[1 + tid];\n            bins[2] += bins[2 + tid];\n            bins[3] += bins[3 + tid];\n        }\n        __syncthreads();\n    }\n}",
            "// TODO: Your code here\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n  if (i < N) {\n    double q = x[i] - floor(x[i]);\n    if (q < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (q < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (q < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int tid = hipThreadIdx_x;\n  int gid = hipBlockIdx_x * blockDim.x + tid;\n  if (gid >= N) return;\n\n  __shared__ double sum;\n  __shared__ int n;\n\n  // First thread of each block calculates the sum of the values that\n  // should go in each bin and stores the count in `n`.\n  if (tid == 0) {\n    // Create a block-wide array of bins to store the partial sums.\n    int bin[4];\n\n    // Determine the count of each bin.\n    bin[0] = __float2int_rn(x[gid] < 0.25);\n    bin[1] = __float2int_rn(x[gid] >= 0.25 && x[gid] < 0.5);\n    bin[2] = __float2int_rn(x[gid] >= 0.5 && x[gid] < 0.75);\n    bin[3] = __float2int_rn(x[gid] >= 0.75);\n\n    // Sum the bins.\n    sum = 0.0;\n    for (int j = 0; j < 4; j++) {\n      sum += bin[j];\n    }\n\n    // Store the count of values that should go in each bin.\n    n = __float2int_rn(sum);\n  }\n\n  // Wait for all threads to finish writing the first block.\n  __syncthreads();\n\n  // Compute the block-wide index in the array of bins.\n  int bin = tid;\n\n  // Each thread writes its bin's count to the appropriate element of `bins`.\n  if (bin < n) {\n    // Add the block-wide sum of values that should go in each bin to the index\n    // in `bins` to get the index of the appropriate bin.\n    bins[bin] = __float2int_rn(sum) + bin;\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double val = x[tid];\n    if (val < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (val < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (val < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double value = x[tid];\n    size_t index = 0;\n    if (value >= 0.0 && value < 0.25) index = 0;\n    else if (value >= 0.25 && value < 0.5) index = 1;\n    else if (value >= 0.5 && value < 0.75) index = 2;\n    else if (value >= 0.75 && value <= 1.0) index = 3;\n    atomicAdd(&bins[index], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    __shared__ double shared[256];\n    size_t bin[4] = {0, 0, 0, 0};\n    size_t i = tid;\n    if (i < N) {\n        double x_i = x[i];\n        if (x_i > 0.75) {\n            bin[3]++;\n        } else if (x_i > 0.5) {\n            bin[2]++;\n        } else if (x_i > 0.25) {\n            bin[1]++;\n        } else {\n            bin[0]++;\n        }\n    }\n    // TODO: Add code to fill the shared memory, then merge the bins\n    // You will need to add code that computes the sum of the 4 elements\n    // in the shared memory (bin[0], bin[1], bin[2], bin[3])\n\n    // TODO: Add code to merge the bins\n}",
            "// YOUR CODE GOES HERE\n  return;\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i >= N) {\n      return;\n   }\n   double fraction = x[i] - floor(x[i]);\n   if (fraction < 0.25) {\n      atomicAdd(&bins[0], 1);\n   } else if (fraction < 0.5) {\n      atomicAdd(&bins[1], 1);\n   } else if (fraction < 0.75) {\n      atomicAdd(&bins[2], 1);\n   } else {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n  if (tid < N) {\n    double xVal = x[tid];\n    int bin = (xVal < 0.25? 0 : xVal < 0.5? 1 : xVal < 0.75? 2 : 3);\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "/* TODO */\n  __syncthreads();\n}",
            "size_t thid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    size_t numthreads = hipBlockDim_x * hipGridDim_x;\n    for (size_t i = thid; i < N; i += numthreads) {\n        double xi = x[i];\n        int j = (xi >= 0.75) + (xi >= 0.5) + (xi >= 0.25);\n        atomicAdd(&bins[j], 1);\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n\n\t// your code here\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO: your code here\n}",
            "// TODO: replace this with your code\n    size_t size = x.size();\n    bins = std::array<size_t, 4>();\n    // int rank;\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // std::cout << \"Rank: \" << rank << std::endl;\n    // if (rank == 0)\n    // {\n    //     std::cout << \"Size: \" << size << std::endl;\n    // }\n    // double local_bins[4];\n    // local_bins = std::array<double, 4>();\n    // for (int i = 0; i < size; i++)\n    // {\n    //     int index = i%4;\n    //     if (x[i] >= 0.0 && x[i] < 0.25)\n    //     {\n    //         local_bins[index]++;\n    //     }\n    //     else if (x[i] >= 0.25 && x[i] < 0.5)\n    //     {\n    //         local_bins[index+1]++;\n    //     }\n    //     else if (x[i] >= 0.5 && x[i] < 0.75)\n    //     {\n    //         local_bins[index+2]++;\n    //     }\n    //     else if (x[i] >= 0.75 && x[i] < 1.0)\n    //     {\n    //         local_bins[index+3]++;\n    //     }\n    // }\n    // MPI_Reduce(local_bins, bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    // MPI_Barrier(MPI_COMM_WORLD);\n    // if (rank == 0)\n    // {\n    //     for (int i = 0; i < 4; i++)\n    //     {\n    //         std::cout << bins[i] << \" \";\n    //     }\n    //     std::cout << std::endl;\n    // }\n}",
            "double rank = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t n = x.size();\n  int left = n / 4;\n  int right = n * 3 / 4;\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  double min = rank * n / size;\n  double max = (rank + 1) * n / size;\n  size_t result = 0;\n  if (rank!= size - 1) {\n    for (size_t i = min; i < max; i++) {\n      if ((x[i] >= left / n) && (x[i] < right / n)) {\n        result++;\n      }\n    }\n  }\n  MPI_Reduce(&result, &(bins[0]), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; i++) {\n      bins[i] = bins[i] / size;\n    }\n  }\n}",
            "}",
            "if (x.size() <= 0) {\n    throw std::invalid_argument(\"x should not be empty\");\n  }\n  if (x.size() % 4!= 0) {\n    throw std::invalid_argument(\"x should have exactly four elements\");\n  }\n\n  // Initialize all bins to 0.\n  bins.fill(0);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement me!\n\n  // You can use MPI_Reduce to reduce the bins to rank 0.\n\n  // You can use MPI_Scatter to scatter the x to all ranks.\n\n  // You can use MPI_Gather to gather the bins to rank 0.\n}",
            "// The number of MPI ranks in the current world\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // The rank of the calling process in the current world\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // Split the vector x into subvectors for each MPI rank.\n    // The resulting vector contains the same elements as x, but in\n    // a different order.\n    // E.g. if world_size = 2, then\n    //     x_1 = [7.8, 4.2, 0.27, 1.5]\n    //     x_2 = [9.1, 7.6, 1.5, 3.8]\n    std::vector<double> x_1(x.begin(), x.begin() + x.size()/world_size);\n    std::vector<double> x_2(x.begin() + x.size()/world_size, x.end());\n\n    // Compute the count of the number of elements in x_1 that have a fractional part\n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    size_t count_1 = 0;\n    for(auto &d : x_1) {\n        if(d >= 0 && d < 0.25) {\n            count_1++;\n        } else if (d >= 0.25 && d < 0.5) {\n            count_1 += 2;\n        } else if (d >= 0.5 && d < 0.75) {\n            count_1 += 3;\n        } else if (d >= 0.75 && d <= 1) {\n            count_1 += 4;\n        }\n    }\n\n    // Compute the count of the number of elements in x_2 that have a fractional part\n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    size_t count_2 = 0;\n    for(auto &d : x_2) {\n        if(d >= 0 && d < 0.25) {\n            count_2++;\n        } else if (d >= 0.25 && d < 0.5) {\n            count_2 += 2;\n        } else if (d >= 0.5 && d < 0.75) {\n            count_2 += 3;\n        } else if (d >= 0.75 && d <= 1) {\n            count_2 += 4;\n        }\n    }\n\n    // Combine the two results into a single result.\n    if(world_rank == 0) {\n        bins = {count_1, 0, count_2, 0};\n    } else if (world_rank == 1) {\n        bins = {0, count_1, 0, count_2};\n    }\n}",
            "// TO BE IMPLEMENTED\n}",
            "}",
            "// TODO\n}",
            "// TODO: implement me!\n}",
            "double min, max;\n  MPI_Allreduce(MPI_IN_PLACE, &x[0], x.size(), MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(MPI_IN_PLACE, &x[0], x.size(), MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n\n  size_t n = x.size();\n  size_t delta = (size_t)(0.25 * n);\n  min = x[0];\n  max = x[n - 1];\n  for (size_t i = 0; i < n; i++) {\n    double current = x[i];\n    if (current < (min + (max - min) * 0.25)) {\n      bins[0]++;\n    } else if (current < (min + (max - min) * 0.50)) {\n      bins[1]++;\n    } else if (current < (min + (max - min) * 0.75)) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int n = x.size();\n    int i = rank * n / size;\n    int j = (rank + 1) * n / size;\n    for(int k = i; k < j; ++k) {\n        if(x[k] >= 0.25 && x[k] < 0.5) {\n            ++bins[0];\n        } else if(x[k] >= 0.5 && x[k] < 0.75) {\n            ++bins[1];\n        } else if(x[k] >= 0.75 && x[k] < 1) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n    MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n}",
            "std::vector<size_t> counts(4);\n  int rank, size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int chunk = x.size() / size;\n  int remainder = x.size() % size;\n  int begin = rank * (chunk + (rank < remainder? 1 : 0));\n  int end = begin + chunk + (rank < remainder? 1 : 0);\n\n  for (int i = begin; i < end; i++) {\n    double v = x[i];\n    if (v < 0.25)\n      counts[0]++;\n    else if (v < 0.5)\n      counts[1]++;\n    else if (v < 0.75)\n      counts[2]++;\n    else\n      counts[3]++;\n  }\n\n  MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* INSERT YOUR CODE HERE */\n}",
            "int num_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int quotient = num_ranks / 4;\n  int remainder = num_ranks % 4;\n\n  std::vector<double> local_bins(4);\n  local_bins.fill(0);\n\n  // each rank will compute an interval of x\n  int index = rank / quotient;\n  double min = index * 0.25;\n  double max = (index + 1) * 0.25;\n\n  for (double value : x) {\n    if (value >= min && value < max) {\n      local_bins[index]++;\n    }\n  }\n\n  MPI_Gather(local_bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    int total = std::accumulate(bins.begin(), bins.end(), 0);\n    // reassign each bin with its percentage\n    for (int &value : bins) {\n      value = (value * 100) / total;\n    }\n  }\n\n  // each rank will output its interval of x to the correct place\n  MPI_Gather(local_bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, index, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  size_t n = x.size();\n  // round up the division of the number of elements by the number of processes\n  size_t n_per_proc = (n + size - 1) / size;\n  // find the starting index of the elements for this process\n  size_t start_idx = rank * n_per_proc;\n  // find the ending index of the elements for this process\n  size_t end_idx = std::min(n, (rank + 1) * n_per_proc);\n\n  // allocate space to receive results\n  std::vector<size_t> local_bins(4, 0);\n\n  // process the local portion of x\n  for (size_t i = start_idx; i < end_idx; ++i) {\n    double val = x[i];\n    double mod = val - std::floor(val);\n    if (mod > 0.75) {\n      ++local_bins[3];\n    } else if (mod > 0.5) {\n      ++local_bins[2];\n    } else if (mod > 0.25) {\n      ++local_bins[1];\n    } else {\n      ++local_bins[0];\n    }\n  }\n\n  // perform reduction\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function.\n}",
            "if (x.size() < 1) {\n        std::cerr << \"Error: x is empty.\" << std::endl;\n        return;\n    }\n\n    // TODO: replace 0 with the number of MPI ranks.\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: replace 0 with the number of elements in x.\n    size_t n = x.size();\n\n    // TODO: replace 0 with the length of `x` (number of elements in each process).\n    size_t n_local = 0;\n\n    // TODO: replace 0 with the number of elements in the last process.\n    size_t n_last = 0;\n\n    // TODO: replace 0 with the offset of the first element in `x`\n    // that this process owns.\n    size_t first_local_index = 0;\n\n    // TODO: replace 0 with the offset of the first element in the\n    // last process's local slice of `x`.\n    size_t last_local_index = 0;\n\n    // TODO: replace 0 with the number of elements in the first\n    // process's local slice of `x`.\n    size_t first_local_n = 0;\n\n    // TODO: replace 0 with the number of elements in the last\n    // process's local slice of `x`.\n    size_t last_local_n = 0;\n\n    // TODO: replace 0 with the index of the first element in the\n    // last process's local slice of `x`.\n    size_t last_local_first_index = 0;\n\n    // TODO: replace 0 with the offset of the first element in `x`\n    // that the last process owns.\n    size_t last_first_local_index = 0;\n\n    // TODO: replace 0 with the number of elements that the last\n    // process owns.\n    size_t last_n = 0;\n\n    // TODO: replace 0 with the offset of the first element in the\n    // first process's local slice of `x`.\n    size_t first_local_first_index = 0;\n\n    // TODO: replace 0 with the index of the first element in `x`\n    // that this process owns.\n    size_t first_local_first_index_global = 0;\n\n    // TODO: replace 0 with the offset of the first element in the\n    // first process's local slice of `x`.\n    size_t first_local_first_index_local = 0;\n\n    // TODO: replace 0 with the total number of elements that the\n    // first process owns.\n    size_t first_n = 0;\n\n    // TODO: replace 0 with the total number of elements in the\n    // first process's local slice of `x`.\n    size_t first_local_n_total = 0;\n\n    // TODO: replace 0 with the number of elements that the first\n    // process owns.\n    size_t first_local_n_local = 0;\n\n    // TODO: replace 0 with the global index of the first element in the\n    // first process's local slice of `x`.\n    size_t first_local_first_index_local_global = 0;\n\n    // TODO: replace 0 with the global index of the first element in the\n    // last process's local slice of `x`.\n    size_t last_local_first_index_local_global = 0;\n\n    // TODO: replace 0 with the global index of the first element in the\n    // first process's local slice of `x`.\n    size_t first_local_first_index_local_global_global = 0;\n\n    // TODO: replace 0 with the global index of the first element in the\n    // last process's local slice of `x`.\n    size_t last_local_first_index_local_global_global = 0;\n\n    // TODO: replace 0 with the rank of the last process.\n    int last_rank = 0;\n\n    // TODO: replace 0 with the rank of the first process.\n    int first_rank = 0;\n\n    // TODO: replace 0 with the global rank of this process.\n    int my_rank = 0;\n\n    // TODO: replace 0 with the global number of processes.\n    int n_procs = 0;\n\n    // TODO: replace 0 with the number of local elements that the\n    // last process owns.\n    size_t last_local_n_local = 0;\n\n    // TODO: replace 0 with the global index of the first element in the\n    // last process's local slice of `x`.\n    size_t last_local_first_index_local_global",
            "if (x.empty()) {\n    return;\n  }\n\n  // Get the number of processes\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // Get the rank of the process\n  int world_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  size_t n = x.size();\n  size_t k = n / world_size;\n  size_t r = n % world_size;\n  size_t start = k*world_rank + std::min(r, world_rank);\n  size_t end = start + k - 1;\n  if (world_rank == world_size - 1) {\n    end = n - 1;\n  }\n\n  size_t local_bins[4] = {};\n\n  // Count number of elements in each bin\n  for (size_t i = start; i <= end; ++i) {\n    double x_i = x[i];\n    if (x_i < 0.25) {\n      ++local_bins[0];\n    } else if (x_i < 0.5) {\n      ++local_bins[1];\n    } else if (x_i < 0.75) {\n      ++local_bins[2];\n    } else {\n      ++local_bins[3];\n    }\n  }\n\n  // Reduce across processes\n  MPI_Reduce(local_bins, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Divide counts by number of elements\n  for (size_t i = 0; i < 4; ++i) {\n    bins[i] /= n;\n  }\n}",
            "// TODO: Your code here.\n}",
            "size_t myBins[4] = {0};\n  // Each process will compute a portion of the total sum (sum) and sum of squares (sumsq)\n  double sum = 0;\n  double sumsq = 0;\n  size_t size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute my portion of x.\n  double portionOfX = (rank + 1) * x.size() / size;\n  // Compute the number of elements on this process.\n  size_t portionOfXSize = (rank + 1) * x.size() / size - (rank * x.size() / size);\n\n  // Compute a portion of the sum and sum of squares.\n  for (int i = 0; i < portionOfXSize; i++) {\n    sum += x[i];\n    sumsq += x[i] * x[i];\n  }\n\n  // Compute the quartiles.\n  double quartiles[4];\n  quartiles[0] = 0.25;\n  quartiles[1] = 0.5;\n  quartiles[2] = 0.75;\n  quartiles[3] = 1;\n  double totalSum = 0;\n  double totalSumSq = 0;\n  MPI_Reduce(&sum, &totalSum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&sumsq, &totalSumSq, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  double myBinsLocal[4] = {0};\n  // Compute the quartiles for this process.\n  for (int i = 0; i < 4; i++) {\n    double quartile = quartiles[i];\n    double current = 0;\n    double next = totalSum;\n    double nextSumSq = totalSumSq;\n    if (quartile!= 1) {\n      current = 0;\n      next = quartile * totalSum;\n      nextSumSq = quartile * totalSumSq;\n    }\n\n    // Sum all the elements in x that have a fractional part in [current, next)\n    for (int j = 0; j < portionOfXSize; j++) {\n      double fractionalPart = x[j] - floor(x[j]);\n      if (current <= fractionalPart && fractionalPart < next) {\n        myBinsLocal[i]++;\n      }\n    }\n  }\n\n  // Put the local count in the bins array on the root process.\n  MPI_Reduce(myBinsLocal, myBins, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    bins[0] = myBins[0];\n    bins[1] = myBins[1];\n    bins[2] = myBins[2];\n    bins[3] = myBins[3];\n  }\n}",
            "// TODO: Implement me.\n\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// Compute the number of elements in the input\n    size_t n = x.size();\n    // Compute the number of elements in each sub-vector\n    size_t n_025 = n/4;\n    size_t n_050 = n/2;\n    size_t n_075 = 3 * n_025;\n    // Compute the start and end indices of each sub-vector\n    size_t start_025 = 0;\n    size_t end_025 = n_025;\n    size_t start_050 = n_025;\n    size_t end_050 = start_050 + n_050;\n    size_t start_075 = end_050;\n    size_t end_075 = n;\n    // Create a vector that stores the number of elements in each sub-vector\n    std::vector<size_t> sub_n = {n_025, n_050, n_075, n - n_075 - n_050 - n_025};\n    // Create a vector that stores the start and end indices of each sub-vector\n    std::vector<std::array<size_t, 2>> sub_i = {{start_025, end_025}, {start_050, end_050}, {start_075, end_075}};\n    // Create a vector that stores the sub-vectors that have a fractional part\n    std::vector<std::vector<double>> sub_v(4);\n    for (size_t i = 0; i < 4; i++) {\n        for (size_t j = sub_i[i][0]; j < sub_i[i][1]; j++) {\n            if (fmod(x[j], 1) >= 0.25 && fmod(x[j], 1) < 0.5) {\n                sub_v[i].push_back(x[j]);\n            }\n        }\n    }\n\n    // Compute the number of elements in each sub-vector that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    // Each rank will compute the number of elements in sub_v[i]\n    size_t sub_count_025 = sub_v[0].size();\n    size_t sub_count_050 = sub_v[1].size();\n    size_t sub_count_075 = sub_v[2].size();\n    size_t sub_count_100 = sub_v[3].size();\n\n    // Create a vector that stores the number of elements in each sub-vector that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    std::vector<size_t> sub_count = {sub_count_025, sub_count_050, sub_count_075, sub_count_100};\n\n    // Create a vector that stores the number of elements in each sub-vector that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    std::vector<std::array<size_t, 2>> sub_count_i = {{0, sub_count[0]}, {sub_count[0], sub_count[0] + sub_count[1]}, {sub_count[0] + sub_count[1], sub_count[0] + sub_count[1] + sub_count[2]}, {sub_count[0] + sub_count[1] + sub_count[2], sub_count[0] + sub_count[1] + sub_count[2] + sub_count[3]}};\n\n    // Compute the number of elements in each sub-vector that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    // Each rank will compute the number of elements in sub_v[i]\n    size_t sub_count_025",
            "bins = {0, 0, 0, 0};\n\tdouble lowerLimit = 0.0, upperLimit = 0.0;\n\tdouble fractionPart = 0.0;\n\n\tfor(size_t i = 0; i < x.size(); ++i) {\n\t\tfractionPart = x[i] - floor(x[i]);\n\t\tif(fractionPart >= 0 && fractionPart < 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if(fractionPart >= 0.25 && fractionPart < 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if(fractionPart >= 0.5 && fractionPart < 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if(fractionPart >= 0.75 && fractionPart < 1.0) {\n\t\t\tbins[3]++;\n\t\t} else {\n\t\t\t// std::cout << \"Error: fractional part of \" << x[i] << \" not in [0, 0.25), [0.25, 0.5), [0.5, 0.75), or [0.75, 1).\" << std::endl;\n\t\t\treturn;\n\t\t}\n\t}\n}",
            "// TODO: implement this function\n    int size,rank;\n    MPI_Comm_size(MPI_COMM_WORLD,&size);\n    MPI_Comm_rank(MPI_COMM_WORLD,&rank);\n\n    int n = x.size();\n    int chunk=n/size;\n    int remainder=n%size;\n\n    if(rank==0)\n    {\n        std::vector<int> sendcounts(size,0);\n        std::vector<int> displs(size,0);\n        if(size==1)\n        {\n            sendcounts[0]=n;\n        }\n        else\n        {\n            for(int i=0;i<size-1;i++)\n            {\n                sendcounts[i]=chunk;\n            }\n            sendcounts[size-1]=chunk+remainder;\n        }\n        std::partial_sum(sendcounts.begin(),sendcounts.end()-1,displs.begin()+1);\n        std::vector<double> data_send(sendcounts[rank]);\n        for(int i=0;i<sendcounts[rank];i++)\n        {\n            data_send[i]=x[i];\n        }\n        std::vector<double> data_recv(sendcounts[rank]);\n\n        MPI_Scatterv(&data_send[0],&sendcounts[0],&displs[0],MPI_DOUBLE,&data_recv[0],sendcounts[rank],MPI_DOUBLE,0,MPI_COMM_WORLD);\n        //calculate quartiles\n        for(int i=0;i<sendcounts[rank];i++)\n        {\n            if(data_recv[i] >= 0.0 && data_recv[i] < 0.25)\n            {\n                bins[0]++;\n            }\n            else if(data_recv[i] >= 0.25 && data_recv[i] < 0.5)\n            {\n                bins[1]++;\n            }\n            else if(data_recv[i] >= 0.5 && data_recv[i] < 0.75)\n            {\n                bins[2]++;\n            }\n            else if(data_recv[i] >= 0.75 && data_recv[i] <= 1.0)\n            {\n                bins[3]++;\n            }\n        }\n    }\n    else\n    {\n        MPI_Scatterv(x.data(),sendcounts.data(),displs.data(),MPI_DOUBLE,data_recv.data(),sendcounts[rank],MPI_DOUBLE,0,MPI_COMM_WORLD);\n        //calculate quartiles\n        for(int i=0;i<sendcounts[rank];i++)\n        {\n            if(data_recv[i] >= 0.0 && data_recv[i] < 0.25)\n            {\n                bins[0]++;\n            }\n            else if(data_recv[i] >= 0.25 && data_recv[i] < 0.5)\n            {\n                bins[1]++;\n            }\n            else if(data_recv[i] >= 0.5 && data_recv[i] < 0.75)\n            {\n                bins[2]++;\n            }\n            else if(data_recv[i] >= 0.75 && data_recv[i] <= 1.0)\n            {\n                bins[3]++;\n            }\n        }\n    }\n\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    double low, high;\n    size_t lowcount, highcount;\n    low = 0;\n    high = 0.25;\n    lowcount = 0;\n    highcount = 0;\n    // TODO: your code goes here\n}",
            "//TODO: implement this function.\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  double x_part = (double)rank / (double)size;\n  std::vector<double> local_x;\n  int len = (int)(x.size() / (double)size);\n  for (int i = 0; i < len; i++) {\n    local_x.push_back(x[i]);\n  }\n  int rem = (int)x.size() % size;\n  for (int i = len*rank+1; i <= len*rank+rem; i++) {\n    local_x.push_back(x[i]);\n  }\n  int num = local_x.size();\n  std::sort(local_x.begin(), local_x.end());\n  double len_x = (double)local_x.size();\n  int pos1 = std::floor(x_part * len_x);\n  int pos2 = std::ceil(x_part * len_x);\n  if (pos1 == pos2) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n  } else {\n    if (pos1 > 0) {\n      bins[0] = (int)std::count_if(local_x.begin(), local_x.begin() + pos1,\n                                  [](double x) {return x < 0.25;});\n    } else {\n      bins[0] = 0;\n    }\n    if (pos2 < (int)local_x.size()) {\n      bins[1] = (int)std::count_if(local_x.begin() + pos1 + 1, local_x.begin() + pos2,\n                                  [](double x) {return (x >= 0.25) && (x < 0.5);});\n    } else {\n      bins[1] = 0;\n    }\n    bins[2] = (int)std::count_if(local_x.begin() + pos2 + 1, local_x.end(),\n                                [](double x) {return (x >= 0.5) && (x < 0.75);});\n    bins[3] = (int)std::count_if(local_x.begin() + pos2 + 1, local_x.end(),\n                                [](double x) {return x >= 0.75;});\n  }\n  MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n    int numprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_elements = x.size() / numprocs;\n    int extra_elements = x.size() % numprocs;\n    int start = rank * num_elements;\n    int end = rank * num_elements + num_elements;\n    int total_end = start + num_elements;\n\n    if(rank == numprocs - 1)\n        total_end += extra_elements;\n\n    std::vector<double> local_x = std::vector<double>(x.begin() + start, x.begin() + end);\n\n    double fractional_part_lower;\n    double fractional_part_upper;\n\n    size_t sum_lower = 0;\n    size_t sum_upper = 0;\n\n    for (auto val : local_x) {\n        fractional_part_lower = val - floor(val);\n        fractional_part_upper = ceil(val) - val;\n\n        if (fractional_part_lower < 0.25 && fractional_part_lower > 0.0)\n            sum_lower++;\n        else if (fractional_part_lower < 0.5 && fractional_part_lower > 0.25)\n            sum_lower++;\n        else if (fractional_part_lower < 0.75 && fractional_part_lower > 0.5)\n            sum_lower++;\n        else if (fractional_part_lower < 1.0 && fractional_part_lower > 0.75)\n            sum_lower++;\n\n        if (fractional_part_upper < 0.25 && fractional_part_upper > 0.0)\n            sum_upper++;\n        else if (fractional_part_upper < 0.5 && fractional_part_upper > 0.25)\n            sum_upper++;\n        else if (fractional_part_upper < 0.75 && fractional_part_upper > 0.5)\n            sum_upper++;\n        else if (fractional_part_upper < 1.0 && fractional_part_upper > 0.75)\n            sum_upper++;\n    }\n\n    MPI_Reduce(&sum_lower, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&sum_upper, &bins[1], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins[2] = 0;\n        bins[3] = 0;\n        if (x.size() % 2 == 0) {\n            bins[2] = (x.size() - extra_elements) / 2;\n            bins[3] = x.size() - extra_elements - bins[2];\n        }\n        else {\n            bins[2] = (x.size() - extra_elements + 1) / 2;\n            bins[3] = x.size() - extra_elements - bins[2];\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  size_t n = x.size();\n  size_t n_per_rank = (n + size - 1) / size;\n  size_t n_below_quarter = (size + 1) / 4;\n  size_t n_above_quarter = (size - 1) / 4;\n  std::vector<double> x_local(n_per_rank);\n  MPI_Scatter(x.data(), n_per_rank, MPI_DOUBLE, x_local.data(), n_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  std::vector<int> bins_local(4);\n  for (size_t i = 0; i < n_per_rank; i++) {\n    double x = x_local[i];\n    if (x < 0.25) {\n      bins_local[0]++;\n    }\n    else if (x < 0.5) {\n      bins_local[1]++;\n    }\n    else if (x < 0.75) {\n      bins_local[2]++;\n    }\n    else {\n      bins_local[3]++;\n    }\n  }\n  std::array<size_t, 4> bins_all;\n  MPI_Gather(bins_local.data(), 4, MPI_INT, bins_all.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n  bins = bins_all;\n}",
            "// TODO\n}",
            "/*... */\n}",
            "int rank, size;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\tint const N = x.size();\n\tint const chunkSize = N / size;\n\tint const chunkStart = rank * chunkSize;\n\tint const chunkEnd = (rank == size - 1)? N : (rank + 1) * chunkSize;\n\n\t// determine the index of the element in x that would be in the [0, 0.25) interval\n\tsize_t lowerQuartileIndex = (size_t)(0.25 * N);\n\tif (lowerQuartileIndex >= N) {\n\t\tlowerQuartileIndex = N - 1;\n\t}\n\n\t// determine the index of the element in x that would be in the [0.75, 1) interval\n\tsize_t upperQuartileIndex = (size_t)(0.75 * N);\n\tif (upperQuartileIndex >= N) {\n\t\tupperQuartileIndex = N - 1;\n\t}\n\n\t// determine the fractional part of the lower quartile\n\tdouble lowerQuartileFraction = x[lowerQuartileIndex] - floor(x[lowerQuartileIndex]);\n\n\t// determine the fractional part of the upper quartile\n\tdouble upperQuartileFraction = x[upperQuartileIndex] - floor(x[upperQuartileIndex]);\n\n\t// count elements in [0, 0.25)\n\tsize_t countLower = 0;\n\tfor (int i = chunkStart; i < chunkEnd; i++) {\n\t\tif (i == lowerQuartileIndex) {\n\t\t\tcountLower++;\n\t\t} else if (x[i] < floor(x[i])) {\n\t\t\tcountLower++;\n\t\t}\n\t}\n\n\t// count elements in [0.25, 0.5)\n\tsize_t countCenter = 0;\n\tfor (int i = chunkStart; i < chunkEnd; i++) {\n\t\tif (i == lowerQuartileIndex) {\n\t\t\tcountCenter++;\n\t\t} else if (i == upperQuartileIndex) {\n\t\t\tcountCenter++;\n\t\t} else if (x[i] >= floor(x[i]) && x[i] < floor(x[i]) + 0.5) {\n\t\t\tcountCenter++;\n\t\t}\n\t}\n\n\t// count elements in [0.5, 0.75)\n\tsize_t countUpper = 0;\n\tfor (int i = chunkStart; i < chunkEnd; i++) {\n\t\tif (i == upperQuartileIndex) {\n\t\t\tcountUpper++;\n\t\t} else if (x[i] >= floor(x[i]) + 0.5 && x[i] < floor(x[i]) + 0.75) {\n\t\t\tcountUpper++;\n\t\t}\n\t}\n\n\t// count elements in [0.75, 1]\n\tsize_t countOther = 0;\n\tfor (int i = chunkStart; i < chunkEnd; i++) {\n\t\tif (i == upperQuartileIndex) {\n\t\t\tcountOther++;\n\t\t} else if (x[i] >= floor(x[i]) + 0.75) {\n\t\t\tcountOther++;\n\t\t}\n\t}\n\n\t// only rank 0 needs to compute the result\n\tif (rank == 0) {\n\t\tbins[0] = countLower;\n\t\tbins[1] = countCenter;\n\t\tbins[2] = countUpper;\n\t\tbins[3] = countOther;\n\t}\n\n\tMPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n\n    // Find the min and max of x\n    double min = *std::min_element(std::begin(x), std::end(x));\n    double max = *std::max_element(std::begin(x), std::end(x));\n    double binWidth = (max - min) / 4.0;\n\n    // Initialize bins\n    bins.fill(0);\n\n    // Add the number of elements to each bin\n    for (double const& d : x) {\n        if (d >= min && d < min + binWidth) {\n            bins[0]++;\n        } else if (d >= min + binWidth && d < min + 2 * binWidth) {\n            bins[1]++;\n        } else if (d >= min + 2 * binWidth && d < min + 3 * binWidth) {\n            bins[2]++;\n        } else if (d >= min + 3 * binWidth && d <= max) {\n            bins[3]++;\n        }\n    }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int length = x.size();\n    int counts[4];\n    counts[0] = 0;\n    counts[1] = 0;\n    counts[2] = 0;\n    counts[3] = 0;\n    int * sendcounts = new int[size];\n    int * displs = new int[size];\n    int * recvcounts = new int[size];\n    int * recvdispls = new int[size];\n    // Determine number of elements for each rank\n    int slice = length / size;\n    int remainder = length % size;\n    for (int i = 0; i < size; i++) {\n        sendcounts[i] = slice + (i < remainder? 1 : 0);\n        displs[i] = i * slice;\n    }\n    MPI_Scatterv(counts, sendcounts, displs, MPI_INT, recvcounts, size, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Determine local positions\n    int * positions = new int[recvcounts[rank]];\n    int slice_size = slice;\n    for (int i = 0; i < rank; i++) {\n        slice_size += (remainder > 0? 1 : 0);\n        remainder -= 1;\n    }\n    for (int i = 0; i < recvcounts[rank]; i++) {\n        positions[i] = slice_size * rank + i;\n    }\n\n    // Send the elements to each rank and count the elements with a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    double * local_x = new double[recvcounts[rank]];\n    MPI_Scatterv(&x[0], sendcounts, displs, MPI_DOUBLE, local_x, recvcounts[rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < recvcounts[rank]; i++) {\n        int index = positions[i];\n        if (local_x[i] < 0.25) {\n            counts[0]++;\n        } else if (local_x[i] < 0.5) {\n            counts[1]++;\n        } else if (local_x[i] < 0.75) {\n            counts[2]++;\n        } else {\n            counts[3]++;\n        }\n    }\n\n    // Determine the number of elements that each rank will receive.\n    recvcounts[0] = counts[0];\n    recvcounts[1] = counts[1];\n    recvcounts[2] = counts[2];\n    recvcounts[3] = counts[3];\n    recvdispls[0] = 0;\n    recvdispls[1] = recvcounts[0];\n    recvdispls[2] = recvcounts[0] + recvcounts[1];\n    recvdispls[3] = recvcounts[0] + recvcounts[1] + recvcounts[2];\n\n    // Receive the number of elements that each rank will receive from each other rank.\n    MPI_Gatherv(counts, 4, MPI_INT, recvcounts, recvcounts, recvdispls, MPI_INT, 0, MPI_COMM_WORLD);\n    // Receive the number of elements that each rank will receive from each other rank.\n    MPI_Gatherv(counts, 4, MPI_INT, recvcounts, recvcounts, recvdispls, MPI_INT, 0, MPI_COMM_WORLD);\n    // Receive the number of elements that each rank will receive from each other rank.\n    MPI_Gatherv(counts, 4, MPI_INT, recvcounts, recvcounts, recvdispls, MPI_INT, 0, MPI_COMM_WORLD);\n    // Receive the number of elements that each rank will receive from each other rank.\n    MPI_Gatherv(counts, 4, MPI_INT, recvcounts, recvcounts, recvdispls, MPI_INT, 0, MPI_COMM_WORLD);\n    // Determine the local position of the start of each bin.\n    std::array<int, 4> recvdispls_int;\n    recvdispls_int[0] = recvdispls[0];\n    recvdispls_int[1] = recvdispls[1];\n    recvdispls_int[2] = recvdispls[2];\n    recvdispls_int[3] = recv",
            "int my_rank, n_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n  // Calculate length of input vector\n  size_t length = x.size();\n  // Calculate total length of input vector\n  size_t global_length;\n  MPI_Allreduce(&length, &global_length, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  // Calculate total number of elements\n  int n_elements = 2 * global_length;\n\n  // Calculate local start and end positions\n  int start = (n_elements / n_ranks) * my_rank;\n  int end = start + (n_elements / n_ranks);\n\n  // Create vector to hold local values\n  std::vector<double> local_values;\n\n  // Loop through all values in vector\n  for (int i = 0; i < length; ++i) {\n    if (x[i] >= start && x[i] < end) {\n      local_values.push_back(x[i]);\n    }\n  }\n\n  // Calculate count of values in each bin\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (int i = 0; i < local_values.size(); ++i) {\n    // Calculate bin\n    int bin = floor(local_values[i] * 4);\n    // Increment bin count\n    ++bins[bin];\n  }\n\n  // Reduce counts on rank 0\n  if (my_rank == 0) {\n    std::vector<int> recv_counts(n_ranks);\n    std::vector<int> displs(n_ranks);\n\n    // Calculate displacements\n    displs[0] = 0;\n    for (int i = 1; i < n_ranks; ++i) {\n      displs[i] = displs[i - 1] + recv_counts[i - 1];\n    }\n\n    // Calculate receive counts\n    MPI_Gather(&bins, 4, MPI_INT, recv_counts.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Calculate total number of elements\n    int n_total = 2 * global_length;\n\n    // Calculate counts of values in each bin\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (int i = 0; i < recv_counts.size(); ++i) {\n      // Calculate start and end positions of values\n      int start = (n_total / n_ranks) * i;\n      int end = start + (n_total / n_ranks);\n      // Calculate bin\n      int bin = floor((start + end) / 2 * 4);\n      // Increment bin count\n      bins[bin] += recv_counts[i];\n    }\n  } else {\n    MPI_Gather(&bins, 4, MPI_INT, NULL, 4, MPI_INT, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: compute the number of values in each bin\n    // TODO: update the bins in rank 0\n}",
            "size_t n = x.size();\n\n  // Compute the indices of the first, second, third, and fourth quartiles.\n  size_t q1 = n / 4;\n  size_t q2 = n / 2;\n  size_t q3 = 3 * n / 4;\n  size_t q4 = n;\n\n  // Assign the values in x to bins according to the quartile they are in.\n  for (double value : x) {\n    if (value < x[q1])\n      ++bins[0];\n    else if (value < x[q2])\n      ++bins[1];\n    else if (value < x[q3])\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n\n  // Send and receive bins using MPI\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<size_t> binsLocal(bins.begin(), bins.end());\n  std::vector<size_t> binsRec(4);\n\n  // Send and receive counts using MPI\n  MPI_Scatter(binsLocal.data(), 4, MPI_INT, binsRec.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Sum all the counts on rank 0\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; ++i) {\n      MPI_Reduce(MPI_IN_PLACE, &binsRec[i], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n  }\n\n  // Send and receive counts using MPI\n  MPI_Gather(binsRec.data(), 4, MPI_INT, binsLocal.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n  // Copy the result to the output bins if rank is 0\n  if (rank == 0) {\n    for (size_t i = 0; i < 4; ++i) {\n      bins[i] = binsLocal[i];\n    }\n  }\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\n\tbins = {0, 0, 0, 0};\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < x.size(); i++) {\n\t\t\tif (x[i] >= 0 && x[i] <= 0.25) {\n\t\t\t\tbins[0] += 1;\n\t\t\t}\n\t\t\telse if (x[i] > 0.25 && x[i] <= 0.5) {\n\t\t\t\tbins[1] += 1;\n\t\t\t}\n\t\t\telse if (x[i] > 0.5 && x[i] <= 0.75) {\n\t\t\t\tbins[2] += 1;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tbins[3] += 1;\n\t\t\t}\n\t\t}\n\t}\n\n\t// TODO: complete this code\n\n\tint div = x.size() / size;\n\tint mod = x.size() % size;\n\n\tstd::vector<double> v1;\n\tstd::vector<double> v2;\n\tif (rank < mod) {\n\t\tv1 = std::vector<double>(x.begin() + rank * div + rank, x.begin() + rank * div + rank + div);\n\t\tv2 = std::vector<double>(x.begin() + rank * div + rank + div, x.begin() + rank * div + rank + div + mod);\n\t}\n\telse {\n\t\tv1 = std::vector<double>(x.begin() + rank * div + rank, x.begin() + rank * div + rank + div + mod);\n\t\tv2 = std::vector<double>(x.begin() + rank * div + rank + div + mod, x.end());\n\t}\n\tstd::array<size_t, 4> b1;\n\tstd::array<size_t, 4> b2;\n\tcountQuartiles(v1, b1);\n\tcountQuartiles(v2, b2);\n\n\tif (rank == 0) {\n\t\tbins[0] = b1[0] + b2[0];\n\t\tbins[1] = b1[1] + b2[1];\n\t\tbins[2] = b1[2] + b2[2];\n\t\tbins[3] = b1[3] + b2[3];\n\t}\n\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n    int myid, numtasks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n    MPI_Comm_size(MPI_COMM_WORLD, &numtasks);\n\n    std::vector<double> local_data = x;\n    int part_size = local_data.size() / numtasks;\n\n    std::vector<double> sub_data = std::vector<double>(local_data.begin() + part_size * myid, local_data.begin() + part_size * (myid + 1));\n    //printf(\"myid: %d, sub_data.size(): %lu\\n\", myid, sub_data.size());\n\n    int sub_size = sub_data.size();\n\n    std::sort(sub_data.begin(), sub_data.end());\n    bins[0] = std::count_if(sub_data.begin(), sub_data.end(), [=](double x) { return x < 0.25; });\n    bins[1] = std::count_if(sub_data.begin(), sub_data.end(), [=](double x) { return x >= 0.25 && x < 0.5; });\n    bins[2] = std::count_if(sub_data.begin(), sub_data.end(), [=](double x) { return x >= 0.5 && x < 0.75; });\n    bins[3] = std::count_if(sub_data.begin(), sub_data.end(), [=](double x) { return x >= 0.75; });\n\n    //std::vector<size_t> bins = std::array<size_t, 4>(0, 0, 0, 0);\n    if (myid == 0) {\n        std::vector<size_t> sub_bins = std::vector<size_t>(bins.begin(), bins.begin() + sub_size);\n        std::vector<size_t> recv_bins(sub_size * numtasks);\n        //printf(\"myid: %d, recv_bins.size(): %lu\\n\", myid, recv_bins.size());\n        MPI_Gather(&sub_bins[0], sub_bins.size(), MPI_INT, &recv_bins[0], sub_bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        //printf(\"myid: %d, recv_bins.size(): %lu\\n\", myid, recv_bins.size());\n        bins = std::array<size_t, 4>(recv_bins.begin(), recv_bins.begin() + bins.size());\n        //printf(\"myid: %d, recv_bins.size(): %lu\\n\", myid, recv_bins.size());\n        //std::copy(recv_bins.begin(), recv_bins.end(), bins.begin());\n        //printf(\"myid: %d, bins.size(): %lu\\n\", myid, bins.size());\n    }\n    else {\n        MPI_Gather(&bins[0], bins.size(), MPI_INT, nullptr, bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n        //printf(\"myid: %d, bins.size(): %lu\\n\", myid, bins.size());\n    }\n}",
            "// TODO: implement\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int count = x.size() / size;\n  double *sendbuf = new double[count];\n  double *recvbuf = new double[count];\n  double *sendcount = new double[size];\n  double *recvcount = new double[size];\n  double *sendoffset = new double[size];\n  double *recvoffset = new double[size];\n\n  for (int i = 0; i < count; i++) {\n    sendbuf[i] = x[i];\n  }\n\n  for (int i = 0; i < size; i++) {\n    sendcount[i] = count;\n  }\n\n  MPI_Scatterv(sendbuf, sendcount, sendoffset, MPI_DOUBLE, recvbuf, count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  int low = 0;\n  int high = count - 1;\n  int mid = (low + high) / 2;\n  double min = 0;\n  double max = 0;\n  double mean = 0;\n  double sum = 0;\n\n  for (int i = 0; i < count; i++) {\n    if (recvbuf[i] < recvbuf[mid]) {\n      low = i;\n    } else if (recvbuf[i] > recvbuf[mid]) {\n      high = i;\n    }\n    sum += recvbuf[i];\n    if (recvbuf[i] < min) {\n      min = recvbuf[i];\n    } else if (recvbuf[i] > max) {\n      max = recvbuf[i];\n    }\n  }\n\n  mean = sum / count;\n\n  bins[0] = (low * 100) / count;\n  bins[1] = (high * 100) / count;\n  bins[2] = (max * 100) / count;\n  bins[3] = (min * 100) / count;\n\n  MPI_Gatherv(bins.data(), bins.size(), MPI_INT, recvbuf, recvcount, recvoffset, MPI_INT, 0, MPI_COMM_WORLD);\n\n  delete[] sendbuf;\n  delete[] recvbuf;\n  delete[] sendcount;\n  delete[] recvcount;\n  delete[] sendoffset;\n  delete[] recvoffset;\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int num_quartiles = 4;\n  int num_elements = x.size();\n  int chunk_size = num_elements / size;\n  int start = rank * chunk_size;\n  int end = start + chunk_size - 1;\n  if (rank == size - 1) {\n    end = num_elements - 1;\n  }\n\n  std::vector<int> quartile_count(num_quartiles, 0);\n\n  for (int i = start; i <= end; ++i) {\n    double number = x[i];\n    if (number < 0.25) {\n      quartile_count[0] += 1;\n    } else if (number < 0.5) {\n      quartile_count[1] += 1;\n    } else if (number < 0.75) {\n      quartile_count[2] += 1;\n    } else {\n      quartile_count[3] += 1;\n    }\n  }\n\n  std::vector<int> quartile_counts_total(num_quartiles);\n  MPI_Allreduce(quartile_count.data(), quartile_counts_total.data(), num_quartiles, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < num_quartiles; ++i) {\n    bins[i] = quartile_counts_total[i];\n  }\n}",
            "// TODO: implement this function.\n}",
            "//...\n  size_t rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int local_x_size = x.size() / size;\n  int remain_x_size = x.size() % size;\n  int offset = rank * local_x_size;\n\n  std::vector<double> local_x(local_x_size);\n  if (rank < remain_x_size) {\n    std::copy(x.begin() + offset, x.begin() + offset + local_x_size + 1, local_x.begin());\n  } else {\n    std::copy(x.begin() + offset, x.begin() + offset + local_x_size, local_x.begin());\n  }\n\n  int min_index, max_index;\n  size_t min_count, max_count;\n  //...\n}",
            "/* To be written */\n}",
            "// YOUR CODE HERE\n}",
            "size_t n = x.size();\n    std::vector<size_t> counts(4);\n    int myRank, numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    int start = myRank * n / numRanks;\n    int end = (myRank + 1) * n / numRanks;\n\n    for (int i = start; i < end; ++i) {\n        int fraction = (x[i] - std::floor(x[i])) * 4;\n        if (fraction < 0) {\n            fraction = 0;\n        } else if (fraction > 3) {\n            fraction = 3;\n        }\n        ++counts[fraction];\n    }\n\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (myRank == 0) {\n        // find the number of data elements in each quartile\n        for (int i = 0; i < counts.size(); ++i) {\n            bins[i] = (counts[i] + n / 4 - 1) / (n / 4);\n        }\n    }\n}",
            "// TODO: your code here\n  //int rank, size;\n  //MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  //MPI_Comm_size(MPI_COMM_WORLD, &size);\n  //double start_time, end_time;\n  //MPI_Barrier(MPI_COMM_WORLD);\n  //start_time = MPI_Wtime();\n  //std::vector<double> x_local = x;\n  //std::vector<double> x_sub;\n  //size_t n_local = x_local.size();\n  //size_t n_sub = n_local / size;\n  //std::vector<size_t> bins_local(4);\n  //std::vector<size_t> bins_sub(4);\n  //MPI_Scatter(x_local.data(), n_sub, MPI_DOUBLE, x_sub.data(), n_sub, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  //countQuartiles(x_sub, bins_sub);\n  //MPI_Gather(bins_sub.data(), bins_sub.size(), MPI_INT, bins_local.data(), bins_sub.size(), MPI_INT, 0, MPI_COMM_WORLD);\n  //bins = bins_local;\n  //MPI_Barrier(MPI_COMM_WORLD);\n  //end_time = MPI_Wtime();\n  //double elapsed_time = end_time - start_time;\n  //if (rank == 0) {\n  //  std::cout << \"Time: \" << elapsed_time << std::endl;\n  //}\n}",
            "// TODO: Your code goes here!\n\n    return;\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  // check if size is 0\n  assert(size!= 0);\n  // check if size is divisible by number of bins\n  assert(size % bins.size() == 0);\n\n  // total number of elements\n  size_t N = x.size();\n\n  // number of elements per rank\n  size_t N_per_rank = N / size;\n\n  // number of elements that belong to the current rank\n  size_t N_my = N_per_rank;\n  if (rank == size - 1) {\n    N_my = N - N_per_rank * (size - 1);\n  }\n\n  // start and end index of local vector\n  size_t first = N_per_rank * rank;\n  size_t last = first + N_my;\n\n  // compute number of elements in each bin on each rank\n  size_t local_bins[4] = {0, 0, 0, 0};\n  for (size_t i = first; i < last; i++) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      local_bins[0] += 1;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      local_bins[1] += 1;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      local_bins[2] += 1;\n    } else {\n      local_bins[3] += 1;\n    }\n  }\n\n  // sum bins on each rank to obtain global bins\n  MPI_Allreduce(local_bins, bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  return;\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// 1. Divide x into groups of 1/4 of x's size, based on rank.\n  //    Rank 0 gets the first 1/4 of the elements, rank 1 gets the next 1/4, etc.\n  // 2. Compute the fractional part of each element.\n  // 3. Bin the fractional parts into four groups based on the number of elements in each group.\n  // 4. Send the counts to rank 0.\n  // 5. Gather the counts from all ranks to rank 0.\n}",
            "//TODO: compute the quartile bins\n}",
            "// Compute local fractional part of each element\n    // Each rank has a complete copy of x\n    std::vector<double> frac_part(x.size());\n    for (int i = 0; i < x.size(); i++) {\n        frac_part[i] = x[i] - std::floor(x[i]);\n    }\n\n    // Count elements in each bin\n    std::array<size_t, 4> counts = {0, 0, 0, 0};\n    for (double d : frac_part) {\n        if (d < 0.25) {\n            counts[0]++;\n        } else if (d < 0.5) {\n            counts[1]++;\n        } else if (d < 0.75) {\n            counts[2]++;\n        } else {\n            counts[3]++;\n        }\n    }\n\n    // Reduce counts across MPI ranks\n    MPI_Allreduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    double start_time = MPI_Wtime();\n\n    // Divide the data into quartiles: 0.0, 0.25, 0.5, and 0.75\n    std::vector<double> lower_quartile, middle_quartile, upper_quartile;\n    std::vector<double> lower_quartile_counts, middle_quartile_counts, upper_quartile_counts;\n    lower_quartile.reserve(x.size() / 4);\n    middle_quartile.reserve(x.size() / 4);\n    upper_quartile.reserve(x.size() / 4);\n    lower_quartile_counts.reserve(x.size() / 4);\n    middle_quartile_counts.reserve(x.size() / 4);\n    upper_quartile_counts.reserve(x.size() / 4);\n    for (double const &elem : x) {\n        if (elem < 0.25) {\n            lower_quartile.push_back(elem);\n        } else if (elem >= 0.25 && elem < 0.5) {\n            middle_quartile.push_back(elem);\n        } else if (elem >= 0.5 && elem < 0.75) {\n            upper_quartile.push_back(elem);\n        }\n    }\n\n    // Count the number of elements in each quartile\n    for (size_t i = 0; i < x.size() / 4; ++i) {\n        lower_quartile_counts.push_back(lower_quartile.at(i));\n        middle_quartile_counts.push_back(middle_quartile.at(i));\n        upper_quartile_counts.push_back(upper_quartile.at(i));\n    }\n\n    // Compute the number of elements in each quartile\n    std::sort(lower_quartile_counts.begin(), lower_quartile_counts.end());\n    std::sort(middle_quartile_counts.begin(), middle_quartile_counts.end());\n    std::sort(upper_quartile_counts.begin(), upper_quartile_counts.end());\n\n    // Compute the result\n    bins[0] = lower_quartile_counts.size();\n    bins[1] = middle_quartile_counts.size();\n    bins[2] = upper_quartile_counts.size();\n    bins[3] = x.size() / 4 - bins[0] - bins[1] - bins[2];\n\n    double end_time = MPI_Wtime();\n\n    if (rank == 0) {\n        printf(\"Time elapsed: %.6lfs\\n\", end_time - start_time);\n    }\n}",
            "double const quartiles[4] = {0.25, 0.5, 0.75, 1};\n  std::array<double, 4> localBins;\n  for (size_t i = 0; i < 4; i++) {\n    localBins[i] = 0;\n  }\n  for (double const& elem : x) {\n    double const fractional = elem - std::floor(elem);\n    for (size_t i = 0; i < 4; i++) {\n      if (fractional >= quartiles[i] && fractional < quartiles[i + 1]) {\n        localBins[i]++;\n      }\n    }\n  }\n  bins = localBins;\n}",
            "// TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  double const QUARTER = 0.25;\n\n  int const NUM_BINS = 4;\n  int const NUM_BINS_PER_RANK = NUM_BINS / size;\n  int const START_BIN = NUM_BINS_PER_RANK * rank;\n  int const END_BIN = START_BIN + NUM_BINS_PER_RANK;\n\n  std::vector<int> localBins;\n  for (int bin = START_BIN; bin < END_BIN; ++bin) {\n    localBins.push_back(0);\n  }\n\n  for (double value : x) {\n    double remainder = fmod(value, QUARTER);\n    if (remainder <= QUARTER * 0.25) {\n      localBins[0]++;\n    } else if (remainder <= QUARTER * 0.5) {\n      localBins[1]++;\n    } else if (remainder <= QUARTER * 0.75) {\n      localBins[2]++;\n    } else {\n      localBins[3]++;\n    }\n  }\n\n  std::vector<int> localBinsSum = localBins;\n  MPI_Reduce(&localBins[0], &localBinsSum[0], localBins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (int bin = 0; bin < NUM_BINS; ++bin) {\n      bins[bin] = localBinsSum[bin];\n    }\n  }\n}",
            "double quartile = 1/4;\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // First, split the vector into quartiles on each process\n   // Then, each process can do its own computations\n   int chunk = x.size() / size;\n   std::vector<double> quartile_x(x.begin()+chunk*rank, x.begin()+chunk*(rank+1));\n\n   // Sort the quartile_x\n   std::sort(quartile_x.begin(), quartile_x.end());\n\n   // Now we can compute the quartiles\n   size_t first = quartile_x.size() * quartile;\n   size_t second = quartile_x.size() * (quartile*2);\n   size_t third = quartile_x.size() * (quartile*3);\n\n   // First bin\n   if (quartile_x[first] >= 0 && quartile_x[first] < 0.25) {\n      bins[0]++;\n   } else if (quartile_x[first] >= 0.25 && quartile_x[first] < 0.5) {\n      bins[1]++;\n   } else if (quartile_x[first] >= 0.5 && quartile_x[first] < 0.75) {\n      bins[2]++;\n   } else {\n      bins[3]++;\n   }\n\n   // Second bin\n   if (quartile_x[second] >= 0 && quartile_x[second] < 0.25) {\n      bins[0]++;\n   } else if (quartile_x[second] >= 0.25 && quartile_x[second] < 0.5) {\n      bins[1]++;\n   } else if (quartile_x[second] >= 0.5 && quartile_x[second] < 0.75) {\n      bins[2]++;\n   } else {\n      bins[3]++;\n   }\n\n   // Third bin\n   if (quartile_x[third] >= 0 && quartile_x[third] < 0.25) {\n      bins[0]++;\n   } else if (quartile_x[third] >= 0.25 && quartile_x[third] < 0.5) {\n      bins[1]++;\n   } else if (quartile_x[third] >= 0.5 && quartile_x[third] < 0.75) {\n      bins[2]++;\n   } else {\n      bins[3]++;\n   }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    \n    int my_n = x.size() / size; // number of doubles for each rank\n    \n    // each rank finds the quartiles for his own doubles\n    int i;\n    int my_bins[4] = {0, 0, 0, 0};\n    for (i = 0; i < my_n; ++i) {\n        double value = x[rank * my_n + i];\n        if (value < 0.25 * (i+1)) {\n            ++my_bins[0];\n        } else if (value < 0.5 * (i+1)) {\n            ++my_bins[1];\n        } else if (value < 0.75 * (i+1)) {\n            ++my_bins[2];\n        } else {\n            ++my_bins[3];\n        }\n    }\n    \n    // gather the results from all ranks\n    int counts[4] = {0, 0, 0, 0};\n    MPI_Reduce(&my_bins, counts, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    \n    if (rank == 0) {\n        bins[0] = counts[0];\n        bins[1] = counts[1];\n        bins[2] = counts[2];\n        bins[3] = counts[3];\n    }\n}",
            "double local_counts[4] = {0};\n\n  // Calculate local counts\n  for (auto &value : x) {\n    double frac = value - std::floor(value);\n    if (frac < 0.25) {\n      local_counts[0] += 1;\n    } else if (frac < 0.5) {\n      local_counts[1] += 1;\n    } else if (frac < 0.75) {\n      local_counts[2] += 1;\n    } else {\n      local_counts[3] += 1;\n    }\n  }\n\n  // Gather local counts to rank 0\n  double recv_counts[4] = {0};\n  MPI_Gather(&local_counts, 4, MPI_DOUBLE, recv_counts, 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Sum counts across ranks\n  if (rank == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (size_t i = 0; i < nprocs; i++) {\n      bins[0] += recv_counts[i * 4 + 0];\n      bins[1] += recv_counts[i * 4 + 1];\n      bins[2] += recv_counts[i * 4 + 2];\n      bins[3] += recv_counts[i * 4 + 3];\n    }\n  }\n}",
            "size_t const n = x.size();\n  // TODO\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (x.size() % size!= 0) {\n    if (rank == 0) {\n      std::cout << \"x.size() is not a multiple of size\" << std::endl;\n    }\n    return;\n  }\n\n  size_t numElements = x.size() / size;\n\n  int myFirst = rank * numElements;\n  int myLast = myFirst + numElements - 1;\n\n  std::vector<double> localQuartiles = {0, 0.25, 0.5, 0.75};\n\n  int numPoints = x.size();\n\n  for (int i = 0; i < 4; i++) {\n    std::vector<double> localCounts(size);\n    std::vector<double> localPartialSums(size);\n    std::vector<double> localCounts_tmp(size);\n    std::vector<double> localPartialSums_tmp(size);\n\n    MPI_Scatter(&x[myFirst], numElements, MPI_DOUBLE, &localQuartiles[i], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(&numPoints, 1, MPI_INT, &numPoints, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    localCounts[rank] = 0;\n    for (int j = 0; j < numPoints; j++) {\n      if (localQuartiles[i] <= x[j] && x[j] < localQuartiles[i+1]) {\n        localCounts[rank]++;\n      }\n    }\n    MPI_Gather(&localCounts[rank], 1, MPI_DOUBLE, &localCounts_tmp[0], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(&localCounts[rank], 1, MPI_DOUBLE, &localCounts[0], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int j = 0; j < size; j++) {\n      localPartialSums[j] = localCounts[j];\n    }\n\n    for (int j = 1; j < size; j++) {\n      localPartialSums[j] += localPartialSums[j-1];\n    }\n\n    for (int j = 0; j < size; j++) {\n      if (rank == j) {\n        localPartialSums_tmp[rank] = localPartialSums[rank];\n      }\n      MPI_Gather(&localPartialSums[rank], 1, MPI_DOUBLE, &localPartialSums_tmp[0], 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      if (rank == j) {\n        bins[i] = localPartialSums_tmp[rank];\n      }\n    }\n  }\n}",
            "size_t n = x.size();\n    MPI_Datatype type_real = MPI_DOUBLE;\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n    MPI_Comm_rank(MPI_COMM_WORLD, &bins);\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n    MPI_Comm_rank(MPI_COMM_WORLD, &bins);\n\n    if (bins == 0) {\n        MPI_Abort(MPI_COMM_WORLD, 0);\n    }\n\n    size_t n_lo = n/4;\n    size_t n_hi = n*3/4;\n    std::vector<double> x_lo;\n    std::vector<double> x_hi;\n\n    // Count values that are less than the lower quartile\n    for (size_t i = 0; i < n_lo; i++) {\n        if (x[i] < x[n_lo]) {\n            x_lo.push_back(x[i]);\n        }\n    }\n\n    // Count values that are greater than the upper quartile\n    for (size_t i = 0; i < n_hi; i++) {\n        if (x[i] >= x[n_hi]) {\n            x_hi.push_back(x[i]);\n        }\n    }\n\n    // Compute number of values in each quartile\n    MPI_Allreduce(&x_lo.size(), &bins[0], 1, type_real, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Allreduce(&x_hi.size(), &bins[1], 1, type_real, MPI_SUM, MPI_COMM_WORLD);\n\n    // Count values that are within the lower and upper quartiles\n    for (size_t i = n_lo; i < n_hi; i++) {\n        if ((x[i] >= x[n_lo]) && (x[i] < x[n_hi])) {\n            x_hi.push_back(x[i]);\n        }\n    }\n\n    MPI_Allreduce(&x_hi.size(), &bins[3], 1, type_real, MPI_SUM, MPI_COMM_WORLD);\n    MPI_Allreduce(&x.size(), &bins[2], 1, type_real, MPI_SUM, MPI_COMM_WORLD);\n}",
            "size_t numDoubles = x.size();\n  size_t numRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Find the number of doubles each rank will be responsible for\n  // (approximately)\n  size_t numDoublesPerRank = numDoubles / numRanks;\n\n  std::vector<double> rankX;\n  if (rank == 0) {\n    rankX = x;\n  }\n\n  // Divide up the array evenly so all the ranks have the same x\n  MPI_Scatter(rankX.data(), numDoublesPerRank, MPI_DOUBLE,\n      x.data(), numDoublesPerRank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Count the number of doubles in each quartile\n  size_t left = 0;\n  size_t right = 0;\n  size_t quarter = 1;\n  size_t numDoublesInQuartile = 0;\n\n  // For each double in the array\n  for (size_t i = 0; i < x.size(); i++) {\n    // If the double is in the current quartile\n    if (x[i] >= left && x[i] < right) {\n      numDoublesInQuartile++;\n    }\n\n    // If the double is at a quartile boundary\n    if (x[i] == left || x[i] == right) {\n      // Update the quartile boundaries and count\n      quarter++;\n      left = right;\n      right = left + 1.0 / (double)quarter;\n      numDoublesInQuartile = 0;\n    }\n  }\n\n  // Store the number of doubles in the quartiles in the bins array\n  bins[0] = numDoublesInQuartile;\n\n  // Find the total number of doubles in the array\n  size_t numTotalDoubles = 0;\n  MPI_Reduce(&numDoubles, &numTotalDoubles, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // If this is rank 0, divide the number of doubles in the quartiles by the total\n  // number of doubles to get the fraction of doubles in each quartile\n  if (rank == 0) {\n    double fraction = 1.0 / (double)numTotalDoubles;\n    bins[0] *= fraction;\n    bins[1] = (numTotalDoubles - numDoublesInQuartile) * fraction;\n    bins[2] = bins[1];\n    bins[3] = bins[0];\n  }\n}",
            "int world_size;\n  int world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  if (world_size!= 4) {\n    throw std::runtime_error(\"world_size must be 4\");\n  }\n\n  if (world_rank == 0) {\n    std::vector<double> x_local = x;\n    MPI_Scatter(x_local.data(), (int)x_local.size(), MPI_DOUBLE,\n                x.data(), (int)x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  std::vector<size_t> local_bins = {0, 0, 0, 0};\n  std::vector<size_t> recvcounts(4);\n  std::vector<int> displs(4);\n  displs[0] = 0;\n  displs[1] = x.size() / 4;\n  displs[2] = (x.size() * 2) / 4;\n  displs[3] = (x.size() * 3) / 4;\n\n  if (world_rank == 0) {\n    recvcounts[0] = x.size() / 4;\n    recvcounts[1] = x.size() / 2;\n    recvcounts[2] = x.size() / 4;\n    recvcounts[3] = x.size() / 4;\n  } else {\n    recvcounts[0] = x.size() / 4;\n    recvcounts[1] = x.size() / 2;\n    recvcounts[2] = x.size() / 4;\n    recvcounts[3] = x.size() / 4;\n  }\n\n  MPI_Scatterv(local_bins.data(), recvcounts.data(), displs.data(), MPI_INT,\n               bins.data(), (int)bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < x.size(); ++i) {\n    double fractional_part = x[i] - std::floor(x[i]);\n    if (fractional_part >= 0 && fractional_part < 0.25) {\n      bins[0] += 1;\n    } else if (fractional_part >= 0.25 && fractional_part < 0.50) {\n      bins[1] += 1;\n    } else if (fractional_part >= 0.5 && fractional_part < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n\n  if (world_rank == 0) {\n    MPI_Gatherv(bins.data(), (int)bins.size(), MPI_INT,\n                local_bins.data(), recvcounts.data(), displs.data(), MPI_INT,\n                0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gatherv(bins.data(), (int)bins.size(), MPI_INT,\n                local_bins.data(), recvcounts.data(), displs.data(), MPI_INT,\n                0, MPI_COMM_WORLD);\n  }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  /* divide the data equally and round down for the last rank */\n  size_t n = x.size() / size;\n  if (rank == size - 1) {\n    n += x.size() % size;\n  }\n\n  std::vector<double> local_x(x.begin() + rank * n, x.begin() + (rank + 1) * n);\n\n  std::sort(local_x.begin(), local_x.end());\n\n  /* count the elements in each bin */\n  for (size_t i = 0; i < local_x.size(); i++) {\n    double fractional_part = std::fmod(local_x[i], 1);\n    if (fractional_part < 0.25) {\n      bins[0] += 1;\n    } else if (fractional_part < 0.5) {\n      bins[1] += 1;\n    } else if (fractional_part < 0.75) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    size_t n = x.size() / world_size;\n    std::vector<double> x_l(n), x_r(n);\n    for (int i = 0; i < n; i++) x_l[i] = x[i + world_rank * n];\n    MPI_Scatter(x_l.data(), n, MPI_DOUBLE, x_r.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    std::array<size_t, 4> local_bins;\n    for (double x : x_r) {\n        if (x <= 0.25) local_bins[0]++;\n        if (x > 0.25 && x <= 0.5) local_bins[1]++;\n        if (x > 0.5 && x <= 0.75) local_bins[2]++;\n        if (x > 0.75) local_bins[3]++;\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), local_bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t const num_bins = 4;\n    std::vector<size_t> local_bins(num_bins);\n\n    /* Your solution here */\n    double min_x = *min_element(x.begin(), x.end());\n    double max_x = *max_element(x.begin(), x.end());\n    double bin_size = (max_x - min_x) / num_bins;\n\n    for (size_t i = 0; i < x.size(); ++i) {\n        double curr_x = x[i];\n        if (curr_x < min_x) {\n            local_bins[0] += 1;\n        } else if (curr_x >= min_x && curr_x < (min_x + bin_size)) {\n            local_bins[1] += 1;\n        } else if (curr_x >= (min_x + bin_size) && curr_x < (min_x + 2*bin_size)) {\n            local_bins[2] += 1;\n        } else {\n            local_bins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), num_bins, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, nproc;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\tint local_n = x.size() / nproc;\n\tint rem = x.size() % nproc;\n\tstd::vector<double> local(local_n + (rank < rem? 1 : 0));\n\tif (rank < rem) {\n\t\tlocal.assign(x.begin() + rank * local_n, x.begin() + rank * local_n + local_n);\n\t\tlocal.push_back(x[local.size()]);\n\t}\n\telse {\n\t\tlocal.assign(x.begin() + rank * local_n, x.begin() + rank * local_n + local_n + (x.size() % nproc));\n\t}\n\tint num_less_0_25 = 0, num_less_0_50 = 0, num_less_0_75 = 0, num_less_1 = 0;\n\tfor (size_t i = 0; i < local.size(); ++i) {\n\t\tdouble value = local[i];\n\t\tif (value <= 0.25) ++num_less_0_25;\n\t\telse if (value <= 0.50) ++num_less_0_50;\n\t\telse if (value <= 0.75) ++num_less_0_75;\n\t\telse ++num_less_1;\n\t}\n\tstd::array<int, 4> local_count = { num_less_0_25, num_less_0_50, num_less_0_75, num_less_1 };\n\tstd::array<int, 4> global_count;\n\tMPI_Allreduce(local_count.data(), global_count.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tbins = { global_count[0], global_count[1], global_count[2], global_count[3] };\n}",
            "size_t n = x.size();\n  bins = {0, 0, 0, 0};\n  // TODO: Implement\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int subSize = n / size;\n  double *subX = new double[subSize];\n  MPI_Scatter(x.data(), subSize, MPI_DOUBLE, subX, subSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < subSize; i++) {\n    double temp = subX[i] % 1;\n    if (temp >= 0.0 && temp < 0.25)\n      bins[0]++;\n    else if (temp >= 0.25 && temp < 0.5)\n      bins[1]++;\n    else if (temp >= 0.5 && temp < 0.75)\n      bins[2]++;\n    else if (temp >= 0.75 && temp < 1.0)\n      bins[3]++;\n    else\n      std::cout << \"Error in MPI task. Temp is \" << temp << std::endl;\n  }\n\n  MPI_Gather(bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n  delete[] subX;\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<double> local_x;\n  std::array<size_t, 4> local_bins;\n\n  int local_size = (int)x.size() / size;\n  if (rank < (int)x.size() % size) {\n    local_size++;\n  }\n\n  local_x.assign(x.begin() + rank * local_size, x.begin() + (rank + 1) * local_size);\n\n  for (double i : local_x) {\n    if ((i - (int)i) > 0.25) {\n      local_bins[3]++;\n    } else if ((i - (int)i) > 0.5) {\n      local_bins[2]++;\n    } else if ((i - (int)i) > 0.75) {\n      local_bins[1]++;\n    } else {\n      local_bins[0]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n\n    // initialize the bins to zero\n    bins = std::array<size_t, 4>();\n\n    // compute the number of elements in the array\n    const int n = x.size();\n\n    // get the number of processes that are part of this computation\n    int num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // get the rank number\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // create a new vector to store the local elements\n    std::vector<double> local_x(n);\n\n    // scatter the x array to the processes\n    MPI_Scatter(x.data(), n, MPI_DOUBLE, local_x.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // now divide the elements into 4 groups\n    double *q1 = new double[n / 4];\n    double *q2 = new double[n / 4];\n    double *q3 = new double[n / 4];\n    double *q4 = new double[n / 4];\n\n    // find the elements which lie in the groups\n    for(int i = 0; i < n / 4; i++){\n        if(local_x[i] >= 0.0 && local_x[i] < 0.25) {\n            q1[i] = local_x[i];\n        }\n        else if(local_x[i] >= 0.25 && local_x[i] < 0.5) {\n            q2[i] = local_x[i];\n        }\n        else if(local_x[i] >= 0.5 && local_x[i] < 0.75) {\n            q3[i] = local_x[i];\n        }\n        else if(local_x[i] >= 0.75 && local_x[i] < 1.0) {\n            q4[i] = local_x[i];\n        }\n    }\n\n    // create another vector to store the elements from the q1 group\n    std::vector<double> q1_vals(q1, q1 + n / 4);\n\n    // create another vector to store the elements from the q2 group\n    std::vector<double> q2_vals(q2, q2 + n / 4);\n\n    // create another vector to store the elements from the q3 group\n    std::vector<double> q3_vals(q3, q3 + n / 4);\n\n    // create another vector to store the elements from the q4 group\n    std::vector<double> q4_vals(q4, q4 + n / 4);\n\n    // create a vector to store the size of the elements in q1\n    std::vector<double> q1_sizes(n / 4);\n\n    // create a vector to store the size of the elements in q2\n    std::vector<double> q2_sizes(n / 4);\n\n    // create a vector to store the size of the elements in q3\n    std::vector<double> q3_sizes(n / 4);\n\n    // create a vector to store the size of the elements in q4\n    std::vector<double> q4_sizes(n / 4);\n\n    // find the size of each q1 element\n    for(int i = 0; i < q1_vals.size(); i++) {\n        q1_sizes[i] = q1_vals[i];\n    }\n\n    // find the size of each q2 element\n    for(int i = 0; i < q2_vals.size(); i++) {\n        q2_sizes[i] = q2_vals[i];\n    }\n\n    // find the size of each q3 element\n    for(int i = 0; i < q3_vals.size(); i++) {\n        q3_sizes[i] = q3_vals[i];\n    }\n\n    // find the size of each q4 element\n    for(int i = 0; i < q4_vals.size(); i++) {\n        q4_sizes[i] = q4_vals[i];\n    }\n\n    // find the total size of the q1 group\n    double q1_total = std::accumulate(q1_sizes.begin(), q1_sizes.end(), 0);\n\n    // find the total size of the q2 group\n    double q2_total = std::accumulate(q2_sizes.begin(), q2_sizes.end(), 0);\n\n    // find the total size of the q3 group\n    double q3_total =",
            "// TODO: Your code here\n    return;\n}",
            "// TODO: implement\n}",
            "// find the number of elements in x\n  int n = x.size();\n\n  // find the number of elements in each bin\n  int const binWidth = n / 4;\n  bins[0] = std::count_if(x.begin(), x.begin() + binWidth, [](double d) {\n    return d >= 0 && d < 0.25;\n  });\n  bins[1] = std::count_if(x.begin() + binWidth, x.begin() + binWidth * 2, [](double d) {\n    return d >= 0.25 && d < 0.5;\n  });\n  bins[2] = std::count_if(x.begin() + binWidth * 2, x.begin() + binWidth * 3, [](double d) {\n    return d >= 0.5 && d < 0.75;\n  });\n  bins[3] = std::count_if(x.begin() + binWidth * 3, x.end(), [](double d) {\n    return d >= 0.75;\n  });\n\n  // all reduce bins to get the sum of all elements in all bins\n  MPI_Allreduce(MPI_IN_PLACE, &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    size_t n = x.size();\n    int n_per_process = n / size;\n    int n_left = n % size;\n    // First process gets to do some extra work\n    if (rank == 0) {\n        // Initialize `bins`\n        for (int i = 0; i < 4; ++i) {\n            bins[i] = 0;\n        }\n        // Copy first (n_per_process + n_left) elements of x to the first n_per_process elements of y\n        // and the last n_left elements to the last n_per_process elements of y\n        std::vector<double> y(n_per_process + n_left);\n        std::copy(x.begin(), x.begin() + n_per_process, y.begin());\n        std::copy(x.begin() + (n - n_left), x.end(), y.begin() + n_per_process);\n        // Sort y in ascending order\n        std::sort(y.begin(), y.end());\n        // Add elements to bins\n        for (int i = 0; i < n_per_process; ++i) {\n            if (y[i] >= 0 && y[i] < 0.25) {\n                bins[0] += 1;\n            } else if (y[i] >= 0.25 && y[i] < 0.5) {\n                bins[1] += 1;\n            } else if (y[i] >= 0.5 && y[i] < 0.75) {\n                bins[2] += 1;\n            } else if (y[i] >= 0.75 && y[i] <= 1) {\n                bins[3] += 1;\n            }\n        }\n    }\n    // Every process has a complete copy of x, so the first `n_per_process` elements\n    // of `x` are sent to the first `n_per_process` processes, and the last `n_left`\n    // elements of `x` are sent to the last `n_left` processes\n    std::vector<double> x_per_process(n_per_process);\n    MPI_Scatter(x.data(), n_per_process, MPI_DOUBLE, x_per_process.data(), n_per_process, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    // Count the elements in the first `n_per_process` elements of `x_per_process`\n    std::array<size_t, 4> bins_per_process;\n    for (int i = 0; i < n_per_process; ++i) {\n        if (x_per_process[i] >= 0 && x_per_process[i] < 0.25) {\n            bins_per_process[0] += 1;\n        } else if (x_per_process[i] >= 0.25 && x_per_process[i] < 0.5) {\n            bins_per_process[1] += 1;\n        } else if (x_per_process[i] >= 0.5 && x_per_process[i] < 0.75) {\n            bins_per_process[2] += 1;\n        } else if (x_per_process[i] >= 0.75 && x_per_process[i] <= 1) {\n            bins_per_process[3] += 1;\n        }\n    }\n    // Receive the counts of elements in the last `n_left` elements of `x_per_process`\n    MPI_Gather(bins_per_process.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: write code\n}",
            "// TODO: Your code here\n}",
            "int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    size_t n = x.size();\n\n    std::vector<double> x_local(n);\n    std::vector<double> q_local(4);\n    std::vector<int> rank_local(n);\n    std::vector<size_t> send_counts_local(num_procs, 0);\n    std::vector<size_t> send_offsets_local(num_procs, 0);\n    std::vector<size_t> recv_counts_local(num_procs, 0);\n    std::vector<size_t> recv_offsets_local(num_procs, 0);\n\n    int k = 0;\n    size_t x_global_size = 0;\n    size_t q_global_size = 0;\n    for (int i = 0; i < num_procs; ++i) {\n        for (size_t j = k; j < n; ++j) {\n            if ((i == 0 && j == k) || x[j] >= 0.25 * (j - k + 1)) {\n                x_local[x_global_size] = x[j];\n                ++x_global_size;\n            }\n        }\n\n        send_counts_local[i] = x_local.size() - send_offsets_local[i];\n        recv_counts_local[i] = q_local.size() - recv_offsets_local[i];\n\n        k = x_local.size() - 1;\n    }\n\n    MPI_Alltoall(send_counts_local.data(), 1, MPI_UNSIGNED_LONG_LONG, \n                 recv_counts_local.data(), 1, MPI_UNSIGNED_LONG_LONG, MPI_COMM_WORLD);\n\n    size_t x_global_start = 0;\n    size_t q_global_start = 0;\n    for (int i = 1; i < num_procs; ++i) {\n        send_offsets_local[i] = send_offsets_local[i-1] + send_counts_local[i-1];\n        recv_offsets_local[i] = recv_offsets_local[i-1] + recv_counts_local[i-1];\n        x_global_start += send_counts_local[i-1];\n        q_global_start += recv_counts_local[i-1];\n    }\n\n    MPI_Alltoallv(x_local.data(), send_counts_local.data(), send_offsets_local.data(),\n                  MPI_DOUBLE, x_global.data(), recv_counts_local.data(), recv_offsets_local.data(),\n                  MPI_DOUBLE, MPI_COMM_WORLD);\n\n    MPI_Alltoallv(q_local.data(), recv_counts_local.data(), recv_offsets_local.data(),\n                  MPI_INT, bins.data(), recv_counts_local.data(), recv_offsets_local.data(),\n                  MPI_INT, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = std::array<size_t, 4> {0, 0, 0, 0};\n        for (size_t i = 0; i < n; ++i) {\n            ++bins[x_global[i] < 0.25 * (i + 1)? 0 : x_global[i] < 0.5 * (i + 1)? 1 : \n                           x_global[i] < 0.75 * (i + 1)? 2 : 3];\n        }\n    }\n}",
            "double total = x.size();\n  double quart = total / 4;\n  double half = total / 2;\n\n  // your code here\n}",
            "// TODO: Compute the quartiles and store them in bins.\n  size_t size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  std::vector<double> loc_x;\n  loc_x.resize(x.size()/size);\n  std::vector<int> loc_bins(4);\n  if (rank == 0) {\n    MPI_Scatter(x.data(), x.size()/size, MPI_DOUBLE, loc_x.data(), x.size()/size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Scatter(x.data(), x.size()/size, MPI_DOUBLE, loc_x.data(), x.size()/size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n  for (int i = 0; i < loc_x.size(); i++) {\n    if (loc_x[i] < 0.25) {\n      loc_bins[0]++;\n    } else if (loc_x[i] < 0.5) {\n      loc_bins[1]++;\n    } else if (loc_x[i] < 0.75) {\n      loc_bins[2]++;\n    } else {\n      loc_bins[3]++;\n    }\n  }\n  MPI_Gather(loc_bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n    size_t n_2 = n / 2;\n    size_t n_4 = n / 4;\n\n    double m = x[n_2];\n    double m_minus_1 = x[n_2 - 1];\n    double m_plus_1 = x[n_2 + 1];\n\n    double m_minus_2 = x[n_4 - 1];\n    double m_plus_2 = x[n_4 + 1];\n\n    std::vector<double> local_bins{0, 0, 0, 0};\n\n    for (size_t i = 0; i < n; i++) {\n        if (i <= n_2 && m - m_minus_1 > 0.25) {\n            m = m_minus_1;\n            m_minus_1 = x[n_2 - (i + 1)];\n        } else if (i >= n_2 && m_plus_1 - m > 0.25) {\n            m = m_plus_1;\n            m_plus_1 = x[n_2 + (i - n_2)];\n        } else if (i >= n_4 && m_minus_2 - m < 0.25) {\n            m = m_minus_2;\n            m_minus_2 = x[n_4 + (i - n_4)];\n        } else if (i <= n_4 && m_plus_2 - m > 0.25) {\n            m = m_plus_2;\n            m_plus_2 = x[n_4 - (i + 1)];\n        } else if (m - m_minus_1 < 0.25) {\n            local_bins[0]++;\n        } else if (m - m_minus_1 < 0.5) {\n            local_bins[1]++;\n        } else if (m_plus_1 - m < 0.5) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "}",
            "size_t n = x.size();\n    // std::array<double, 4> quartiles = {0.25, 0.5, 0.75, 1};\n    std::array<double, 4> quartiles = {0.25, 0.5, 0.75, 0.9};\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    double chunk = n/size;\n    double start = rank*chunk;\n    double end = rank*chunk + chunk;\n    if (rank == 0) {\n        end = n;\n    }\n    std::vector<double> localBins(quartiles.size());\n    // std::sort(x.begin()+start, x.begin()+end);\n    std::nth_element(x.begin()+start, x.begin()+(start+chunk/2), x.begin()+end);\n    for (int i = 0; i < quartiles.size(); i++) {\n        std::nth_element(x.begin()+start, x.begin()+(start+chunk*(quartiles[i])), x.begin()+end);\n        localBins[i] = start+chunk*(quartiles[i]);\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), quartiles.size(), MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n\n    double *x_copy;\n    if (my_rank == 0) {\n        x_copy = (double *)malloc(n * sizeof(double));\n        memcpy(x_copy, x.data(), n * sizeof(double));\n    }\n\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = 0;\n        }\n    }\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int my_left_bound = n * my_rank / size;\n    int my_right_bound = n * (my_rank + 1) / size;\n\n    MPI_Scatter(x_copy + my_left_bound, my_right_bound - my_left_bound, MPI_DOUBLE,\n                bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (my_rank == 0) {\n        free(x_copy);\n    }\n}",
            "// compute number of values in each group\n   int n = x.size();\n   int n025 = n / 4;\n   int n050 = n / 2;\n   int n075 = n050 + n025;\n   int n100 = n - n025 - n050 - n075;\n   // split data into 4 groups\n   std::array<std::vector<double>, 4> groups;\n   std::array<int, 4> group_sizes;\n   group_sizes[0] = n025;\n   group_sizes[1] = n050;\n   group_sizes[2] = n075;\n   group_sizes[3] = n100;\n   for (int i = 0; i < n; i++) {\n      int group = i / n050;\n      groups[group].push_back(x[i]);\n   }\n\n   // send data to other ranks\n   int n_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n   std::array<int, 4> group_sizes_bcast;\n   std::array<std::vector<double>, 4> groups_bcast;\n   MPI_Bcast(group_sizes.data(), group_sizes.size(), MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Bcast(groups[0].data(), group_sizes[0], MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(groups[1].data(), group_sizes[1], MPI_DOUBLE, 1, MPI_COMM_WORLD);\n   MPI_Bcast(groups[2].data(), group_sizes[2], MPI_DOUBLE, 2, MPI_COMM_WORLD);\n   MPI_Bcast(groups[3].data(), group_sizes[3], MPI_DOUBLE, 3, MPI_COMM_WORLD);\n\n   // count values in each group\n   std::vector<int> counts(4, 0);\n   for (int i = 0; i < 4; i++) {\n      counts[i] = count_if(groups[i].begin(), groups[i].end(), [](double x) {\n         return x > 0.25 && x <= 0.5;\n      });\n   }\n   std::vector<int> counts_bcast;\n   MPI_Reduce(counts.data(), counts_bcast.data(), counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Bcast(counts_bcast.data(), counts_bcast.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n   // save counts to bins\n   for (int i = 0; i < counts_bcast.size(); i++) {\n      bins[i] = counts_bcast[i];\n   }\n}",
            "// TODO: implement this function\n  int rank, num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (x.size() % num_ranks!= 0) {\n    if (rank == 0) {\n      std::cout << \"Error: Number of ranks does not divide the input vector length\" << std::endl;\n      exit(1);\n    }\n  } else {\n    int part_length = x.size() / num_ranks;\n    std::vector<double> part(part_length);\n    if (rank == 0) {\n      for (int i = 1; i < num_ranks; i++) {\n        MPI_Send(x.data() + (i * part_length), part_length, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      }\n    }\n    MPI_Status status;\n    MPI_Recv(part.data(), part_length, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    size_t index = 0;\n    for (size_t i = 0; i < part.size(); i++) {\n      if (part[i] >= 0 && part[i] < 0.25) {\n        bins[0] += 1;\n      } else if (part[i] >= 0.25 && part[i] < 0.5) {\n        bins[1] += 1;\n      } else if (part[i] >= 0.5 && part[i] < 0.75) {\n        bins[2] += 1;\n      } else if (part[i] >= 0.75 && part[i] <= 1) {\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "const size_t n = x.size();\n\n  // TODO\n  // Write your code here\n  double quartile_1 = 0;\n  double quartile_2 = 0;\n  double quartile_3 = 0;\n  double quartile_4 = 0;\n\n  // Use a 4-point partition, but this will result in a skewed distribution.\n  // Could use 10-point partition with a final 10th point to ensure\n  // 100% even distribution\n  double step = 1.0/4;\n\n  if (n == 0) {\n    std::cout << \"Empty vector\";\n    return;\n  }\n\n  if (n == 1) {\n    bins[0] = 1;\n    return;\n  }\n\n  // Count the number of elements in the range 0 to 0.25\n  for (auto i = 0; i < n; i++) {\n    if (x[i] >= 0 && x[i] <= step) {\n      quartile_1++;\n    }\n  }\n\n  // Count the number of elements in the range 0.25 to 0.5\n  for (auto i = 0; i < n; i++) {\n    if (x[i] > step && x[i] <= step*2) {\n      quartile_2++;\n    }\n  }\n\n  // Count the number of elements in the range 0.5 to 0.75\n  for (auto i = 0; i < n; i++) {\n    if (x[i] > step*2 && x[i] <= step*3) {\n      quartile_3++;\n    }\n  }\n\n  // Count the number of elements in the range 0.75 to 1\n  for (auto i = 0; i < n; i++) {\n    if (x[i] > step*3) {\n      quartile_4++;\n    }\n  }\n\n  // Assign the counts to bins[0], bins[1], bins[2], and bins[3]\n  bins[0] = quartile_1;\n  bins[1] = quartile_2;\n  bins[2] = quartile_3;\n  bins[3] = quartile_4;\n\n  return;\n}",
            "/* TODO */\n}",
            "size_t length = x.size();\n  size_t block_size = length / 4;\n  size_t remainder = length % 4;\n  bins[0] = std::count_if(x.begin(), x.begin() + block_size,\n                          [](double d) { return (d > 0.0 && d < 0.25); });\n  bins[1] = std::count_if(x.begin() + block_size, x.begin() + 2 * block_size,\n                          [](double d) { return (d > 0.25 && d < 0.5); });\n  bins[2] = std::count_if(x.begin() + 2 * block_size, x.begin() + 3 * block_size,\n                          [](double d) { return (d > 0.5 && d < 0.75); });\n  bins[3] = std::count_if(x.begin() + 3 * block_size, x.end(),\n                          [](double d) { return (d > 0.75); });\n  if (remainder == 1) {\n    if (x[length - 1] > 0.75) {\n      bins[3] += 1;\n    }\n    else if (x[length - 1] > 0.5) {\n      bins[2] += 1;\n    }\n    else if (x[length - 1] > 0.25) {\n      bins[1] += 1;\n    }\n    else {\n      bins[0] += 1;\n    }\n  }\n  else if (remainder == 2) {\n    if (x[length - 1] > 0.75) {\n      bins[3] += 1;\n    }\n    else if (x[length - 1] > 0.5) {\n      bins[2] += 1;\n    }\n    else if (x[length - 1] > 0.25) {\n      bins[1] += 1;\n    }\n    else {\n      bins[0] += 1;\n    }\n    if (x[length - 2] > 0.75) {\n      bins[3] += 1;\n    }\n    else if (x[length - 2] > 0.5) {\n      bins[2] += 1;\n    }\n    else if (x[length - 2] > 0.25) {\n      bins[1] += 1;\n    }\n    else {\n      bins[0] += 1;\n    }\n  }\n  else if (remainder == 3) {\n    if (x[length - 1] > 0.75) {\n      bins[3] += 1;\n    }\n    else if (x[length - 1] > 0.5) {\n      bins[2] += 1;\n    }\n    else if (x[length - 1] > 0.25) {\n      bins[1] += 1;\n    }\n    else {\n      bins[0] += 1;\n    }\n    if (x[length - 2] > 0.75) {\n      bins[3] += 1;\n    }\n    else if (x[length - 2] > 0.5) {\n      bins[2] += 1;\n    }\n    else if (x[length - 2] > 0.25) {\n      bins[1] += 1;\n    }\n    else {\n      bins[0] += 1;\n    }\n    if (x[length - 3] > 0.75) {\n      bins[3] += 1;\n    }\n    else if (x[length - 3] > 0.5) {\n      bins[2] += 1;\n    }\n    else if (x[length - 3] > 0.25) {\n      bins[1] += 1;\n    }\n    else {\n      bins[0] += 1;\n    }\n  }\n}",
            "double quartile = 0;\n  int size;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  if (rank == 0) {\n    std::vector<double> x_temp(x);\n    std::sort(x_temp.begin(), x_temp.end());\n    quartile = x_temp[(x_temp.size() / size) * rank];\n  }\n  MPI_Bcast(&quartile, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  size_t count = 0;\n  for (double i : x) {\n    if (i >= quartile && i < quartile + 0.25) {\n      count++;\n    } else if (i >= quartile + 0.25 && i < quartile + 0.5) {\n      count++;\n    } else if (i >= quartile + 0.5 && i < quartile + 0.75) {\n      count++;\n    } else if (i >= quartile + 0.75) {\n      count++;\n    }\n  }\n  MPI_Reduce(&count, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// Your implementation goes here!\n}",
            "// size of data on each rank\n    size_t local_size = x.size() / MPI_COMM_WORLD->Get_size();\n    // index of first element in data on this rank\n    size_t start = local_size * MPI_COMM_WORLD->Get_rank();\n    // index of last element in data on this rank\n    size_t end = start + local_size;\n    // local count of data on this rank\n    size_t local_count = end - start;\n\n    // sort x on this rank\n    std::sort(x.begin() + start, x.begin() + end);\n\n    // number of local elements that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    size_t quartiles[4] = {0, 0, 0, 0};\n\n    for (size_t i = 0; i < local_count; ++i) {\n        double fraction = x[start + i] - floor(x[start + i]);\n        if (fraction >= 0 && fraction < 0.25) {\n            quartiles[0]++;\n        }\n        if (fraction >= 0.25 && fraction < 0.5) {\n            quartiles[1]++;\n        }\n        if (fraction >= 0.5 && fraction < 0.75) {\n            quartiles[2]++;\n        }\n        if (fraction >= 0.75 && fraction <= 1) {\n            quartiles[3]++;\n        }\n    }\n\n    // sum of all the elements of quartiles on all ranks\n    MPI_Reduce(quartiles, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (MPI_COMM_WORLD->Get_rank() == 0) {\n        for (size_t i = 1; i < MPI_COMM_WORLD->Get_size(); ++i) {\n            bins[0] += bins[0];\n            bins[1] += bins[1];\n            bins[2] += bins[2];\n            bins[3] += bins[3];\n        }\n    }\n}",
            "size_t n = x.size();\n  int rank, world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  std::vector<double> local_bins;\n\n  int doubles_per_rank = (n - 1) / world_size + 1;\n  int start = doubles_per_rank * rank;\n  int end = start + doubles_per_rank;\n  end = std::min(end, n);\n  for (size_t i = start; i < end; i++) {\n    double value = x[i];\n    double rem = value - std::floor(value);\n    if (rem >= 0.25 && rem < 0.5) {\n      local_bins[0]++;\n    } else if (rem >= 0.5 && rem < 0.75) {\n      local_bins[1]++;\n    } else if (rem >= 0.75 && rem < 1.0) {\n      local_bins[2]++;\n    } else {\n      local_bins[3]++;\n    }\n  }\n\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "if (x.size() == 0) {\n    return;\n  }\n\n  /* TODO */\n}",
            "// TODO: Your code goes here\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int world_size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t x_size = x.size();\n  size_t x_per_proc = x_size / world_size;\n  std::vector<double> x_local;\n  if (rank < world_size - 1) {\n    x_local = std::vector<double>(x.begin() + rank * x_per_proc,\n                                  x.begin() + (rank + 1) * x_per_proc);\n  } else {\n    x_local = std::vector<double>(x.begin() + rank * x_per_proc, x.end());\n  }\n\n  std::vector<size_t> counts(4, 0);\n  for (auto element : x_local) {\n    if (element >= 0 && element < 0.25) {\n      counts[0]++;\n    } else if (element >= 0.25 && element < 0.5) {\n      counts[1]++;\n    } else if (element >= 0.5 && element < 0.75) {\n      counts[2]++;\n    } else if (element >= 0.75) {\n      counts[3]++;\n    }\n  }\n\n  std::vector<size_t> counts_gathered(4, 0);\n  MPI_Allreduce(counts.data(), counts_gathered.data(), counts.size(),\n                MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = std::array<size_t, 4>{counts_gathered[0], counts_gathered[1],\n                                  counts_gathered[2], counts_gathered[3]};\n  }\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "int world_size;\n    int world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    \n    if(world_rank == 0){\n        std::vector<double> local_x(x);\n        std::array<size_t, 4> local_bins;\n        for(int i = 1; i < world_size; i++){\n            MPI_Status status;\n            MPI_Recv(&local_x[0], local_x.size(), MPI_DOUBLE, i, 0, MPI_COMM_WORLD, &status);\n            countQuartiles(local_x, local_bins);\n            MPI_Reduce(&local_bins, &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n        countQuartiles(local_x, bins);\n    } else {\n        std::vector<double> local_x(x);\n        MPI_Send(&local_x[0], local_x.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code goes here\n}",
            "// TODO: Your code here\n}",
            "/* TODO */\n    const size_t size = x.size();\n    std::array<size_t, 4> bins_;\n\n    for (int i = 0; i < size; i++) {\n        if (x[i] < 0.25) {\n            bins_[0]++;\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            bins_[1]++;\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            bins_[2]++;\n        } else if (x[i] >= 0.75) {\n            bins_[3]++;\n        }\n    }\n\n    MPI_Reduce(bins_.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (size % 2 == 0) {\n        // Even\n        bins_[0] = size / 2;\n        bins_[1] = size / 2;\n    } else {\n        // Odd\n        bins_[0] = (size + 1) / 2;\n        bins_[1] = size / 2;\n    }\n\n    MPI_Reduce(bins_.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n   size_t n = x.size();\n   if (n == 0) {\n      return;\n   }\n\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n   if (rank == 0) {\n      std::vector<double> tmp(world_size);\n      MPI_Gather(x.data(), n, MPI_DOUBLE, tmp.data(), n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n      double n_per_proc = n / world_size;\n      bins[0] = 0;\n      for (int i = 0; i < world_size; ++i) {\n         for (int j = 0; j < n_per_proc; ++j) {\n            if (tmp[i * n_per_proc + j] < 0.25) {\n               ++bins[0];\n            } else if (tmp[i * n_per_proc + j] < 0.5) {\n               ++bins[1];\n            } else if (tmp[i * n_per_proc + j] < 0.75) {\n               ++bins[2];\n            } else {\n               ++bins[3];\n            }\n         }\n      }\n   } else {\n      MPI_Gather(x.data(), n, MPI_DOUBLE, nullptr, n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n}",
            "int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    if (rank == 0) {\n        bins.fill(0);\n    }\n\n    std::vector<double> local_bins(4, 0);\n    std::vector<double> local_x;\n    size_t n_local;\n    int n_global;\n    MPI_Scatter(&n, 1, MPI_INT, &n_global, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        local_x = x;\n    }\n    MPI_Scatter(local_x.data(), n_global, MPI_DOUBLE,\n                local_bins.data(), 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < n_global; ++i) {\n        if (local_x[i] < 0.25) {\n            local_bins[0]++;\n        } else if (local_x[i] < 0.5) {\n            local_bins[1]++;\n        } else if (local_x[i] < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (auto &b : bins) {\n            b = (b * 4) / n;\n        }\n    }\n}",
            "double quartiles[] = {0.25, 0.5, 0.75};\n    double lower, upper;\n    double current;\n    size_t i = 0;\n    for (auto q : quartiles) {\n        lower = q;\n        upper = 1.0 - lower;\n        i = 0;\n        bins[i] = 0;\n        current = lower;\n        for (auto d : x) {\n            if (d >= current && d < (current + upper)) {\n                bins[i] += 1;\n            }\n            current += 1.0;\n        }\n        current = 1.0 - upper;\n        for (auto d : x) {\n            if (d >= current && d < (current + upper)) {\n                bins[i] += 1;\n            }\n            current += 1.0;\n        }\n        i++;\n    }\n}",
            "// TODO: your code goes here\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Compute the number of elements in the subvector that each process owns\n  // and the global counts\n  std::vector<double> local_x(x.begin() + (rank * x.size()) / MPI_COMM_WORLD->size,\n                              x.begin() + ((rank + 1) * x.size()) / MPI_COMM_WORLD->size);\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n  for (size_t i = 0; i < local_x.size(); ++i) {\n    if (local_x[i] >= 0.0 && local_x[i] < 0.25) {\n      ++local_bins[0];\n    } else if (local_x[i] >= 0.25 && local_x[i] < 0.5) {\n      ++local_bins[1];\n    } else if (local_x[i] >= 0.5 && local_x[i] < 0.75) {\n      ++local_bins[2];\n    } else if (local_x[i] >= 0.75 && local_x[i] <= 1.0) {\n      ++local_bins[3];\n    }\n  }\n\n  // Gather the results of each process\n  MPI_Gather(&local_bins, 4, MPI_UNSIGNED, &bins, 4, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    if (world_rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n\n    int elements_to_be_counted = x.size() / 4;\n    std::vector<double> elements_for_counting(elements_to_be_counted);\n\n    if (world_rank == 0) {\n        std::copy(x.begin(), x.begin() + elements_to_be_counted, elements_for_counting.begin());\n    }\n\n    // Send the elements to be counted to the other ranks.\n    MPI_Scatter(elements_for_counting.data(), elements_to_be_counting.size(), MPI_DOUBLE, elements_for_counting.data(), elements_to_be_counting.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Count the elements\n    for (double element : elements_for_counting) {\n        if (element < 0.25) {\n            bins[0] += 1;\n        } else if (element < 0.5) {\n            bins[1] += 1;\n        } else if (element < 0.75) {\n            bins[2] += 1;\n        } else if (element < 1) {\n            bins[3] += 1;\n        }\n    }\n\n    // Get the number of elements that have been counted so far\n    int total_count = 0;\n    MPI_Reduce(&bins[0], &total_count, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // If this is rank 0, then broadcast the result to all ranks\n    if (world_rank == 0) {\n        MPI_Bcast(&bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "const double q = 0.25;\n\tdouble lower = 0.0, upper = 0.0;\n\tint world_size = 0;\n\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n\tMPI_Allgather(&q, 1, MPI_DOUBLE, &lower, 1, MPI_DOUBLE, MPI_COMM_WORLD);\n\tMPI_Allgather(&q, 1, MPI_DOUBLE, &upper, 1, MPI_DOUBLE, MPI_COMM_WORLD);\n\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif (x[i] >= lower && x[i] < upper) {\n\t\t\tbins[0] += 1;\n\t\t}\n\t\telse if (x[i] >= upper && x[i] < 2.0 * upper) {\n\t\t\tbins[1] += 1;\n\t\t}\n\t\telse if (x[i] >= 2.0 * upper && x[i] < 3.0 * upper) {\n\t\t\tbins[2] += 1;\n\t\t}\n\t\telse if (x[i] >= 3.0 * upper) {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n}",
            "size_t n = x.size();\n    size_t m = n / 4;\n    bins.fill(0);\n    // TODO\n}",
            "// TODO: replace this stub with the code that computes the quartile counts\n  size_t n = x.size();\n  bins = std::array<size_t, 4> {{0,0,0,0}};\n  if (n == 0) {\n    return;\n  }\n  if (n == 1) {\n    bins[0] = 1;\n    return;\n  }\n  double sum = 0;\n  for (size_t i = 0; i < n; ++i) {\n    sum += x[i];\n  }\n  double mean = sum / n;\n  sum = 0;\n  for (size_t i = 0; i < n; ++i) {\n    sum += (x[i] - mean) * (x[i] - mean);\n  }\n  double sigma = std::sqrt(sum / n);\n  if (sigma == 0) {\n    return;\n  }\n  bins[0] = std::count_if(x.begin(), x.end(), [&](double x){return std::floor((x - mean) / (sigma * 0.25)) == 0;});\n  bins[1] = std::count_if(x.begin(), x.end(), [&](double x){return std::floor((x - mean) / (sigma * 0.25)) == 1;});\n  bins[2] = std::count_if(x.begin(), x.end(), [&](double x){return std::floor((x - mean) / (sigma * 0.25)) == 2;});\n  bins[3] = std::count_if(x.begin(), x.end(), [&](double x){return std::floor((x - mean) / (sigma * 0.25)) == 3;});\n}",
            "int n = x.size();\n    int num_procs;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    // compute the number of elements each proc should take\n    int elements_per_proc = n / num_procs;\n    // compute the remainder\n    int remainder = n % num_procs;\n    // determine how many procs need an extra element\n    int extra = remainder == 0? 0 : 1;\n    // determine where each proc's slice begins\n    int offset = 0;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    // get the begin and end index of the elements\n    int begin = offset + rank * elements_per_proc;\n    int end = begin + elements_per_proc + extra;\n    if (rank == num_procs - 1) {\n        end = n;\n    }\n\n    // get the begin and end index of the elements\n    int begin2 = offset + rank * elements_per_proc;\n    int end2 = begin2 + elements_per_proc + extra;\n    if (rank == num_procs - 1) {\n        end2 = n;\n    }\n\n    std::vector<double> localx(x.begin() + begin, x.begin() + end);\n    std::vector<double> localx2(x.begin() + begin2, x.begin() + end2);\n\n    // the local sum of the elements in the slice\n    double localsum = 0;\n    double localsum2 = 0;\n\n    // compute the sum of the elements in the slice\n    for (int i = 0; i < localx.size(); i++) {\n        localsum += localx[i];\n    }\n    for (int i = 0; i < localx2.size(); i++) {\n        localsum2 += localx2[i];\n    }\n\n    // send the sum to the root\n    double sum;\n    double sum2;\n    MPI_Reduce(&localsum, &sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&localsum2, &sum2, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // compute the size of the quartiles\n    double size = sum / 4.0;\n    double size2 = sum2 / 4.0;\n\n    // compute the begin of each quartile\n    double beginq = size - (size / 2.0);\n    double beginq2 = size2 - (size2 / 2.0);\n\n    // compute the end of each quartile\n    double endq = size + (size / 2.0);\n    double endq2 = size2 + (size2 / 2.0);\n\n    // count the number of elements in each quartile\n    size_t count = 0;\n    size_t count2 = 0;\n    for (auto i : localx) {\n        if (i >= beginq && i < endq) {\n            count++;\n        }\n    }\n    for (auto i : localx2) {\n        if (i >= beginq2 && i < endq2) {\n            count2++;\n        }\n    }\n\n    // the count of each quartile\n    size_t count_q;\n    size_t count_q2;\n\n    // send the count of each quartile to the root\n    MPI_Reduce(&count, &count_q, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count2, &count_q2, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // the global count of each quartile\n    size_t global_count_q;\n    size_t global_count_q2;\n\n    // send the global count of each quartile to the root\n    MPI_Reduce(&count_q, &global_count_q, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(&count_q2, &global_count_q2, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // store the count of each quartile\n    bins[0] = global_count_q;\n    bins[1] = global_count_q2;\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    bins = {0, 0, 0, 0};\n  }\n\n  const auto n = x.size();\n  const auto n_per_rank = n / size;\n  const auto start_index = rank * n_per_rank;\n\n  for (auto i = start_index; i < start_index + n_per_rank; ++i) {\n    auto fractional_part = x[i] - std::floor(x[i]);\n    if (fractional_part >= 0 && fractional_part < 0.25) {\n      bins[0] += 1;\n    }\n    else if (fractional_part >= 0.25 && fractional_part < 0.5) {\n      bins[1] += 1;\n    }\n    else if (fractional_part >= 0.5 && fractional_part < 0.75) {\n      bins[2] += 1;\n    }\n    else if (fractional_part >= 0.75 && fractional_part < 1) {\n      bins[3] += 1;\n    }\n    else {\n      throw std::runtime_error(\"Fractional part must be in [0, 1)\");\n    }\n  }\n\n  std::vector<size_t> partial_result(4);\n  MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG, partial_result.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    bins = {0, 0, 0, 0};\n    for (int i = 0; i < 4; ++i) {\n      bins[i] = partial_result[i];\n    }\n  }\n}",
            "int my_rank, num_procs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n  const size_t n = x.size();\n  const size_t num_elements = (n + num_procs - 1) / num_procs;\n  std::vector<double> local_elements(num_elements);\n  std::vector<size_t> local_bins(4, 0);\n  for (size_t i = 0; i < num_elements; ++i) {\n    if (my_rank == 0) {\n      local_elements[i] = x[i];\n    } else {\n      local_elements[i] = 0;\n    }\n  }\n  MPI_Scatter(local_elements.data(), num_elements, MPI_DOUBLE, local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  \n  // get the sorted data\n  std::vector<double> sorted_elements(num_elements);\n  MPI_Allgather(local_elements.data(), num_elements, MPI_DOUBLE, sorted_elements.data(), num_elements, MPI_DOUBLE, MPI_COMM_WORLD);\n  std::sort(sorted_elements.begin(), sorted_elements.end());\n\n  size_t left_quartile_index = sorted_elements.size() / 4;\n  size_t right_quartile_index = sorted_elements.size() * 3 / 4;\n  for (size_t i = 0; i < num_elements; ++i) {\n    if ((sorted_elements[i] >= sorted_elements[left_quartile_index] && sorted_elements[i] < sorted_elements[right_quartile_index]) ||\n        (sorted_elements[i] >= sorted_elements[left_quartile_index] && i == left_quartile_index) ||\n        (sorted_elements[i] < sorted_elements[right_quartile_index] && i == right_quartile_index)) {\n      local_bins[1]++;\n    }\n    if ((sorted_elements[i] >= sorted_elements[left_quartile_index] && sorted_elements[i] < sorted_elements[right_quartile_index]) ||\n        (sorted_elements[i] >= sorted_elements[left_quartile_index] && i == left_quartile_index) ||\n        (sorted_elements[i] < sorted_elements[right_quartile_index] && i == right_quartile_index)) {\n      local_bins[2]++;\n    }\n    if ((sorted_elements[i] >= sorted_elements[left_quartile_index] && sorted_elements[i] < sorted_elements[right_quartile_index]) ||\n        (sorted_elements[i] >= sorted_elements[left_quartile_index] && i == left_quartile_index) ||\n        (sorted_elements[i] < sorted_elements[right_quartile_index] && i == right_quartile_index)) {\n      local_bins[3]++;\n    }\n  }\n\n  MPI_Gather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "//TODO: YOUR CODE HERE\n}",
            "const size_t num_elements = x.size();\n\tbins.fill(0);\n\t// 1. TODO\n\tdouble my_range = num_elements / 4;\n\tdouble start_index = my_range * (double) rank;\n\tdouble end_index = my_range * (double) (rank + 1);\n\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < num_elements; i++) {\n\t\t\tif (x[i] >= start_index && x[i] < end_index)\n\t\t\t\tbins[floor((x[i] - start_index) / (end_index - start_index) * 4)]++;\n\t\t}\n\t\t// 2. TODO\n\t\tfor (int i = 1; i < size; i++) {\n\t\t\tMPI_Recv(&bins, 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\t\t}\n\t\tfor (int i = 0; i < num_elements; i++) {\n\t\t\tif (x[i] >= end_index)\n\t\t\t\tbins[floor((x[i] - start_index) / (end_index - start_index) * 4)]++;\n\t\t}\n\t} else {\n\t\tfor (int i = 0; i < num_elements; i++) {\n\t\t\tif (x[i] >= start_index && x[i] < end_index)\n\t\t\t\tbins[floor((x[i] - start_index) / (end_index - start_index) * 4)]++;\n\t\t}\n\t\t// 2. TODO\n\t\tMPI_Send(&bins, 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n\t}\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // TODO: Your code here.\n    int size, n, r;\n    double q1, q2, q3;\n    double q11, q12, q13, q14;\n    double q21, q22, q23, q24;\n    double q31, q32, q33, q34;\n    double q41, q42, q43, q44;\n    double q51, q52, q53, q54;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if(rank == 0){\n        n = x.size();\n\n        q1 = 0;\n        q2 = 0;\n        q3 = 0;\n\n        q11 = 0;\n        q12 = 0;\n        q13 = 0;\n        q14 = 0;\n\n        q21 = 0;\n        q22 = 0;\n        q23 = 0;\n        q24 = 0;\n\n        q31 = 0;\n        q32 = 0;\n        q33 = 0;\n        q34 = 0;\n\n        q41 = 0;\n        q42 = 0;\n        q43 = 0;\n        q44 = 0;\n\n        q51 = 0;\n        q52 = 0;\n        q53 = 0;\n        q54 = 0;\n    }\n\n    double *send_array, *recv_array;\n    int *send_count, *recv_count;\n\n    MPI_Bcast(&n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if(rank == 0){\n        int s1 = 0, s2 = 0, s3 = 0, s4 = 0, s5 = 0;\n        for(int i = 0; i < n; i++){\n            if(x[i] >= 0 && x[i] <= 0.25){\n                s1 += 1;\n            }\n            if(x[i] > 0.25 && x[i] <= 0.5){\n                s2 += 1;\n            }\n            if(x[i] > 0.5 && x[i] <= 0.75){\n                s3 += 1;\n            }\n            if(x[i] > 0.75 && x[i] <= 1){\n                s4 += 1;\n            }\n            if(x[i] > 1){\n                s5 += 1;\n            }\n        }\n        q1 = s1;\n        q2 = s2;\n        q3 = s3;\n        q4 = s4;\n        q5 = s5;\n\n        send_array = (double*) malloc(size * sizeof(double));\n        send_count = (int*) malloc(size * sizeof(int));\n\n        send_array[0] = q1;\n        send_array[1] = q2;\n        send_array[2] = q3;\n        send_array[3] = q4;\n        send_array[4] = q5;\n\n        send_count[0] = s1;\n        send_count[1] = s2;\n        send_count[2] = s3;\n        send_count[3] = s4;\n        send_count[4] = s5;\n\n        recv_array = (double*) malloc(size * sizeof(double));\n        recv_count = (int*) malloc(size * sizeof(int));\n    }\n\n    MPI_Scatter(send_array, send_count[rank], MPI_DOUBLE, recv_array, send_count[rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if(rank == 0){\n        free(send_array);\n        free(send_count);\n    }\n\n    q1 = recv_array[0];\n    q2 = recv_array[1];\n    q3 = recv_array[2];\n    q4 = recv_array[3];\n    q5 = recv_array[4];\n\n    if(rank == 0){\n        bins[0] = q1;\n        bins[1] = q2;\n        bins[2] = q3;\n        bins[3] = q4;\n    }\n\n    free(recv_array);\n    free(recv_count);\n\n}",
            "// TODO: implement me\n    if(x.size() < 1){\n        return;\n    }\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    double my_x[x.size()];\n    MPI_Scatter(&x[0], x.size(), MPI_DOUBLE, my_x, x.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    int count = 0;\n    for(int i = 0; i < x.size(); i++){\n        if((my_x[i] >= 0.0) && (my_x[i] < 0.25)){\n            count++;\n        }\n        else if((my_x[i] >= 0.25) && (my_x[i] < 0.5)){\n            count++;\n        }\n        else if((my_x[i] >= 0.5) && (my_x[i] < 0.75)){\n            count++;\n        }\n        else if((my_x[i] >= 0.75) && (my_x[i] < 1)){\n            count++;\n        }\n    }\n    MPI_Reduce(&count, &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "size_t rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<double> xlocal = x;\n  size_t size_local = xlocal.size() / size;\n  if (rank == 0)\n    bins = std::array<size_t, 4> {};\n  std::vector<double> xsend (size_local);\n  std::array<double, 4> split {.25,.5,.75, 1};\n  std::array<size_t, 4> bins_local {0, 0, 0, 0};\n  std::array<size_t, 4> bins_final {0, 0, 0, 0};\n\n  MPI_Scatter(xlocal.data(), size_local, MPI_DOUBLE, xsend.data(), size_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::array<double, 4> xsplit {xsend[0], xsend[0] + 1, xsend[0] + 1, xsend[0] + 1};\n  for (size_t i = 1; i < size_local; i++) {\n    for (size_t j = 0; j < 4; j++) {\n      if (xsend[i] <= xsplit[j]) {\n        xsplit[j] = xsend[i];\n        bins_local[j] += 1;\n      }\n    }\n  }\n\n  MPI_Reduce(bins_local.data(), bins_final.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0)\n    bins = bins_final;\n}",
            "// Compute number of values in each bin, and the local start and end indices\n  std::array<size_t, 4> local_bins;\n  size_t n = x.size();\n  size_t local_start = 0;\n  size_t local_end = n;\n  for (int i = 0; i < 4; ++i) {\n    local_bins[i] = 0;\n  }\n  // Compute the number of values that fit in each bin.\n  for (size_t i = 0; i < n; ++i) {\n    double const v = x[i];\n    if (v >= 0.0 && v < 0.25) {\n      ++local_bins[0];\n    } else if (v >= 0.25 && v < 0.5) {\n      ++local_bins[1];\n    } else if (v >= 0.5 && v < 0.75) {\n      ++local_bins[2];\n    } else if (v >= 0.75 && v < 1.0) {\n      ++local_bins[3];\n    }\n  }\n  // Find global start and end indices.\n  size_t start, end;\n  MPI_Reduce(&local_start, &start, 1, MPI_UNSIGNED, MPI_MIN, 0, MPI_COMM_WORLD);\n  MPI_Reduce(&local_end, &end, 1, MPI_UNSIGNED, MPI_MAX, 0, MPI_COMM_WORLD);\n  // Compute the global number of values that fit in each bin.\n  local_bins.fill(0);\n  MPI_Gather(&local_bins, 4, MPI_UNSIGNED, bins.data(), 4, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n  if (start == end) {\n    std::array<size_t, 4> temp;\n    temp.fill(0);\n    bins = temp;\n  }\n}",
            "double N = x.size();\n\n  // TODO: Implement this function.\n\n  // TODO: Call this function in the main function below.\n}",
            "// Your code here\n\n}",
            "// 1. Get the length of x (number of elements).\n  size_t const n = x.size();\n\n  // 2. Compute the quartiles, using n.\n  size_t const q = n / 4;\n\n  // 3. Compute the bins using the quartiles.\n  for (size_t i = 0; i < n; ++i) {\n    double const x_i = x[i];\n    // Compute the bin of the current element\n    if (x_i < q) {\n      // [0, q)\n      bins[0]++;\n    } else if (x_i < 2 * q) {\n      // [q, 2q)\n      bins[1]++;\n    } else if (x_i < 3 * q) {\n      // [2q, 3q)\n      bins[2]++;\n    } else {\n      // [3q, n)\n      bins[3]++;\n    }\n  }\n}",
            "// Your code goes here\n  // You may want to add additional parameters to countQuartiles, e.g. an MPI communicator\n  // and a vector of partial sums (initialized to zeros)\n\n  // YOUR CODE HERE\n  //std::vector<double> partial_sum(x.size());\n  //partial_sum[0] = x[0];\n  //for (size_t i = 1; i < x.size(); i++)\n  //  partial_sum[i] = partial_sum[i-1] + x[i];\n\n  size_t n = x.size();\n  std::vector<double> partial_sum(n);\n  partial_sum[0] = x[0];\n  for (size_t i = 1; i < n; i++)\n    partial_sum[i] = partial_sum[i-1] + x[i];\n\n  MPI_Datatype MPI_DOUBLE_TYPE;\n  MPI_Type_contiguous(1, MPI_DOUBLE, &MPI_DOUBLE_TYPE);\n  MPI_Type_commit(&MPI_DOUBLE_TYPE);\n\n  MPI_Datatype MPI_INT_TYPE;\n  MPI_Type_contiguous(1, MPI_INT, &MPI_INT_TYPE);\n  MPI_Type_commit(&MPI_INT_TYPE);\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int rank_lower = rank*2/size, rank_upper = (rank+1)*2/size;\n  int local_count[4] = {0};\n  MPI_Scatter(&rank_lower, 1, MPI_INT, &local_count[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n  MPI_Scatter(&rank_upper, 1, MPI_INT, &local_count[1], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  int n_lower = local_count[0];\n  int n_upper = local_count[1];\n  int start_lower = 0, start_upper = n - n_upper;\n\n  std::vector<double> local_x_lower(n_lower), local_x_upper(n_upper);\n\n  MPI_Scatterv(&x[0], &n_lower, &start_lower, MPI_DOUBLE_TYPE,\n    &local_x_lower[0], n_lower, MPI_DOUBLE_TYPE, 0, MPI_COMM_WORLD);\n  MPI_Scatterv(&x[0], &n_upper, &start_upper, MPI_DOUBLE_TYPE,\n    &local_x_upper[0], n_upper, MPI_DOUBLE_TYPE, 0, MPI_COMM_WORLD);\n\n  MPI_Request request[2];\n  MPI_Status status[2];\n\n  double fraction;\n  for (int i = 0; i < n_lower; i++) {\n    fraction = (local_x_lower[i] - partial_sum[start_lower])/((double)(partial_sum[start_lower+n_lower-1] - partial_sum[start_lower]));\n    if (fraction >= 0 && fraction < 0.25)\n      local_count[2]++;\n  }\n  for (int i = 0; i < n_upper; i++) {\n    fraction = (local_x_upper[i] - partial_sum[start_upper])/((double)(partial_sum[start_upper+n_upper-1] - partial_sum[start_upper]));\n    if (fraction >= 0.25 && fraction < 0.5)\n      local_count[2]++;\n  }\n  for (int i = 0; i < n_lower; i++) {\n    fraction = (local_x_lower[i] - partial_sum[start_lower])/((double)(partial_sum[start_lower+n_lower-1] - partial_sum[start_lower]));\n    if (fraction >= 0.5 && fraction < 0.75)\n      local_count[3]++;\n  }\n  for (int i = 0; i < n_upper; i++) {\n    fraction = (local_x_upper[i] - partial_sum[start_upper])/((double)(partial_sum[start_upper+n_upper-1] - partial_sum[start_upper]));\n    if (fraction >= 0.75 && fraction < 1)\n      local_count[3]++;\n  }\n\n  MPI_Gather(&local_count[2], 1, MPI_INT, &bins[0], 1, MPI_INT",
            "int rank, nprocs;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  // Split x into pieces that each rank will process\n  int n = x.size();\n  int chunksize = n / nprocs;\n  int remainder = n % nprocs;\n\n  std::vector<double> x_local(chunksize + (rank < remainder? 1 : 0));\n  MPI_Scatter(&x[0], chunksize + (rank < remainder? 1 : 0), MPI_DOUBLE, &x_local[0], chunksize + (rank < remainder? 1 : 0), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Count number of elements that are in each bin\n  int n_in_bin[4] = { 0 };\n  for(double d : x_local) {\n    double d_mod = fmod(d, 1);\n    if(d_mod < 0.25) {\n      ++n_in_bin[0];\n    } else if(d_mod < 0.5) {\n      ++n_in_bin[1];\n    } else if(d_mod < 0.75) {\n      ++n_in_bin[2];\n    } else {\n      ++n_in_bin[3];\n    }\n  }\n\n  // Reduce to get total counts\n  int n_in_bin_sum[4] = { 0 };\n  MPI_Reduce(&n_in_bin, n_in_bin_sum, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Store the counts in bins on rank 0\n  if(rank == 0) {\n    bins[0] = n_in_bin_sum[0];\n    bins[1] = n_in_bin_sum[1];\n    bins[2] = n_in_bin_sum[2];\n    bins[3] = n_in_bin_sum[3];\n  }\n}",
            "// TODO: your code here!\n}",
            "size_t n = x.size();\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Get the portion of `x` that this rank should count\n  size_t chunk = n / size;\n  size_t start = rank * chunk;\n  size_t end = (rank == size - 1)? n : start + chunk;\n  std::vector<double> local_x(x.begin() + start, x.begin() + end);\n\n  // Compute the number of elements in each bin\n  size_t num_elements = local_x.size();\n  std::array<size_t, 4> local_bins;\n  local_bins[0] = std::count_if(local_x.begin(), local_x.end(),\n                                [](double x) {return x >= 0.0 && x < 0.25;});\n  local_bins[1] = std::count_if(local_x.begin(), local_x.end(),\n                                [](double x) {return x >= 0.25 && x < 0.5;});\n  local_bins[2] = std::count_if(local_x.begin(), local_x.end(),\n                                [](double x) {return x >= 0.5 && x < 0.75;});\n  local_bins[3] = std::count_if(local_x.begin(), local_x.end(),\n                                [](double x) {return x >= 0.75 && x < 1.0;});\n\n  // Combine the counts from each rank\n  MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // The result will be wrong if the number of elements in x is not evenly divisible by the number of ranks.\n  // Since we're using a simple division to split the input vector, the last rank may have a few extra\n  // elements. We will send the count of the extra elements to the previous rank to update it's count.\n  if (rank > 0) {\n    MPI_Send(&local_bins[3], 1, MPI_UNSIGNED_LONG_LONG, rank - 1, 0, MPI_COMM_WORLD);\n  } else {\n    // rank = 0\n    size_t last_rank = size - 1;\n    size_t extra_elements = n - (num_elements * last_rank);\n    size_t extra_count = local_bins[3];\n    MPI_Status status;\n    MPI_Recv(&extra_count, 1, MPI_UNSIGNED_LONG_LONG, last_rank, 0, MPI_COMM_WORLD, &status);\n    bins[3] += extra_count;\n    bins[3] += extra_elements;\n  }\n}",
            "MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    // each rank gets to know the number of elements in x\n    MPI_Bcast(x.size(), 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // calculate total number of elements in x\n    size_t x_size;\n    MPI_Allreduce(MPI_IN_PLACE, &x_size, 1, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    // compute start and end index for each rank\n    size_t start = x_size * rank / world_size;\n    size_t end = x_size * (rank + 1) / world_size;\n\n    // loop through elements, count values that fall into one of the four bins\n    for (size_t i = start; i < end; ++i) {\n        double element = x[i];\n        if (element < 0.25) {\n            ++bins[0];\n        } else if (element < 0.5) {\n            ++bins[1];\n        } else if (element < 0.75) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "if (x.size() == 0) {\n    return;\n  }\n\n  // TODO: your code goes here\n\n}",
            "int comm_sz;\n  MPI_Comm_size(MPI_COMM_WORLD, &comm_sz);\n\n  int comm_rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n\n  std::vector<int> indices_to_count(x.size());\n  for (size_t i = 0; i < indices_to_count.size(); ++i) {\n    indices_to_count[i] = i;\n  }\n\n  // Count the number of elements in each of the 4 bins, and send that information\n  // to the master process\n  std::array<size_t, 4> local_bins;\n  for (size_t i = 0; i < local_bins.size(); ++i) {\n    local_bins[i] = 0;\n  }\n  int num_to_send = indices_to_count.size() / comm_sz;\n  int remainder = indices_to_count.size() % comm_sz;\n  int offset = 0;\n  for (int r = 0; r < comm_sz; ++r) {\n    if (r == comm_rank) {\n      // Master process\n      for (size_t i = 0; i < num_to_send; ++i) {\n        if (x[indices_to_count[i]] < 0.25 * i / comm_sz) {\n          local_bins[0] += 1;\n        } else if (x[indices_to_count[i]] < 0.5 * i / comm_sz) {\n          local_bins[1] += 1;\n        } else if (x[indices_to_count[i]] < 0.75 * i / comm_sz) {\n          local_bins[2] += 1;\n        } else {\n          local_bins[3] += 1;\n        }\n      }\n      if (remainder!= 0) {\n        // Add an extra value to account for the remainder\n        if (x[indices_to_count[offset + num_to_send]] < 0.25 * (1 + (1 / comm_sz) * remainder)) {\n          local_bins[0] += 1;\n        } else if (x[indices_to_count[offset + num_to_send]] < 0.5 * (1 + (1 / comm_sz) * remainder)) {\n          local_bins[1] += 1;\n        } else if (x[indices_to_count[offset + num_to_send]] < 0.75 * (1 + (1 / comm_sz) * remainder)) {\n          local_bins[2] += 1;\n        } else {\n          local_bins[3] += 1;\n        }\n      }\n    } else {\n      // All other processes\n      MPI_Send(&indices_to_count[offset], num_to_send, MPI_INT, r, 1, MPI_COMM_WORLD);\n      for (size_t i = 0; i < num_to_send; ++i) {\n        if (x[indices_to_count[i]] < 0.25 * i / comm_sz) {\n          local_bins[0] += 1;\n        } else if (x[indices_to_count[i]] < 0.5 * i / comm_sz) {\n          local_bins[1] += 1;\n        } else if (x[indices_to_count[i]] < 0.75 * i / comm_sz) {\n          local_bins[2] += 1;\n        } else {\n          local_bins[3] += 1;\n        }\n      }\n      if (remainder!= 0) {\n        // Add an extra value to account for the remainder\n        if (x[indices_to_count[offset + num_to_send]] < 0.25 * (1 + (1 / comm_sz) * remainder)) {\n          local_bins[0] += 1;\n        } else if (x[indices_to_count[offset + num_to_send]] < 0.5 * (1 + (1 / comm_sz) * remainder)) {\n          local_bins[1] += 1;\n        } else if (x[indices_to_count[offset + num_to_send]] < 0.75 * (1 + (1 / comm_sz) * remainder)) {\n          local_bins[2] += 1;\n        } else {\n          local_bins[3] += 1;\n        }\n      }\n    }\n    offset += num_to_send;\n  }",
            "double n = x.size();\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    std::array<double, 4> bin_size = { 0.25, 0.25, 0.25, 0.25 };\n    // TODO: Implement this function\n    bins = { 0, 0, 0, 0 };\n}",
            "size_t n = x.size();\n\n    // your code here\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // split the data\n    int n_per_process = n / world_size;\n    int remainder = n % world_size;\n    int start = world_rank * n_per_process + std::min(world_rank, remainder);\n    int end = start + n_per_process + (world_rank >= remainder? 1 : 0);\n\n    // local vector\n    std::vector<double> x_local(x.begin() + start, x.begin() + end);\n\n    // sort x_local\n    std::sort(x_local.begin(), x_local.end());\n\n    // calculate quartiles\n    double quartile1 = (x_local[x_local.size() / 4] + x_local[x_local.size() / 4 - 1]) / 2;\n    double quartile2 = (x_local[x_local.size() / 2] + x_local[x_local.size() / 2 - 1]) / 2;\n    double quartile3 = (x_local[(x_local.size() / 2) + x_local.size() / 4] + x_local[(x_local.size() / 2) + x_local.size() / 4 - 1]) / 2;\n    double quartile4 = (x_local[x_local.size() - 1] + x_local[x_local.size() - 2]) / 2;\n\n    // count\n    bins[0] = std::count_if(x_local.begin(), x_local.end(), [quartile1](const double& x) { return x <= quartile1; });\n    bins[1] = std::count_if(x_local.begin(), x_local.end(), [quartile2](const double& x) { return x <= quartile2; });\n    bins[2] = std::count_if(x_local.begin(), x_local.end(), [quartile3](const double& x) { return x <= quartile3; });\n    bins[3] = std::count_if(x_local.begin(), x_local.end(), [quartile4](const double& x) { return x <= quartile4; });\n\n    // gather\n    MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "// Your code here\n    size_t size = x.size();\n    int rank, nproc;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double start, end;\n    start = MPI_Wtime();\n    if (rank == 0) {\n        for (size_t i = 0; i < size; i++) {\n            if (x[i] >= 0.0 && x[i] < 0.25)\n                bins[0]++;\n            else if (x[i] >= 0.25 && x[i] < 0.5)\n                bins[1]++;\n            else if (x[i] >= 0.5 && x[i] < 0.75)\n                bins[2]++;\n            else if (x[i] >= 0.75 && x[i] <= 1.0)\n                bins[3]++;\n        }\n    }\n\n    int n = (int)size / nproc;\n    int offset = rank * n;\n    int remain = (int)size - offset;\n\n    double *local_x = new double[n];\n\n    if (rank < remain) {\n        MPI_Recv(local_x, n, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (size_t i = 0; i < remain; i++) {\n            if (local_x[i] >= 0.0 && local_x[i] < 0.25)\n                bins[0]++;\n            else if (local_x[i] >= 0.25 && local_x[i] < 0.5)\n                bins[1]++;\n            else if (local_x[i] >= 0.5 && local_x[i] < 0.75)\n                bins[2]++;\n            else if (local_x[i] >= 0.75 && local_x[i] <= 1.0)\n                bins[3]++;\n        }\n    }\n    else {\n        MPI_Recv(local_x, remain, MPI_DOUBLE, rank, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n        for (size_t i = 0; i < n; i++) {\n            if (local_x[i] >= 0.0 && local_x[i] < 0.25)\n                bins[0]++;\n            else if (local_x[i] >= 0.25 && local_x[i] < 0.5)\n                bins[1]++;\n            else if (local_x[i] >= 0.5 && local_x[i] < 0.75)\n                bins[2]++;\n            else if (local_x[i] >= 0.75 && local_x[i] <= 1.0)\n                bins[3]++;\n        }\n    }\n    end = MPI_Wtime();\n    std::cout << \"rank: \" << rank << \" time: \" << end - start << std::endl;\n    MPI_Barrier(MPI_COMM_WORLD);\n    delete[] local_x;\n}",
            "// TODO: Compute the number of elements in each of the 4 bins and store them in `bins`\n}",
            "size_t n = x.size();\n\tif (n < 1) {\n\t\tstd::fill(bins.begin(), bins.end(), 0);\n\t\treturn;\n\t}\n\n\t// sort the elements of x in parallel\n\t// then, rank 0 has the sorted elements\n\tstd::vector<double> sorted_elements(x);\n\tstd::sort(sorted_elements.begin(), sorted_elements.end());\n\tif (rank() == 0) {\n\t\t// compute the counts in sorted_elements\n\t\t// which have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n\t\tsize_t lower_quartile_index = n / 4;\n\t\tsize_t middle_quartile_index = n / 2;\n\t\tsize_t upper_quartile_index = 3 * n / 4;\n\t\tbins[0] = lower_quartile_index;\n\t\tbins[1] = upper_quartile_index - lower_quartile_index;\n\t\tbins[2] = middle_quartile_index - lower_quartile_index;\n\t\tbins[3] = n - upper_quartile_index;\n\t}\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  int local_start = x.size() / size * rank;\n  int local_end = x.size() / size * (rank + 1);\n\n  std::vector<double> local_x(local_end - local_start);\n  std::copy(x.begin() + local_start, x.begin() + local_end, local_x.begin());\n\n  int local_count = 0;\n\n  for (double const& elem : local_x) {\n    if (elem < 0.25 && elem >= 0) {\n      local_count++;\n    } else if (elem >= 0.25 && elem < 0.5) {\n      local_count++;\n    } else if (elem >= 0.5 && elem < 0.75) {\n      local_count++;\n    } else if (elem >= 0.75 && elem <= 1) {\n      local_count++;\n    } else {\n      std::cout << \"Error\" << std::endl;\n    }\n  }\n\n  int global_count = 0;\n  MPI_Reduce(&local_count, &global_count, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  bins[0] = global_count;\n  if (rank == 0) {\n    for (int i = 1; i < size; i++) {\n      int temp_count = 0;\n      MPI_Recv(&temp_count, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[i] = temp_count;\n    }\n  } else {\n    MPI_Send(&global_count, 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "int world_rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int n = x.size();\n\n    int n_per_rank = n / world_size;\n    int remainder = n % world_size;\n\n    int start_idx, end_idx;\n\n    if (world_rank == 0) {\n        for (int i = 0; i < remainder; i++) {\n            start_idx = n_per_rank * i;\n            end_idx = n_per_rank * (i + 1) - 1;\n\n            for (int j = start_idx; j <= end_idx; j++) {\n                if ((x[j] >= 0.0) && (x[j] < 0.25)) {\n                    bins[0] += 1;\n                } else if ((x[j] >= 0.25) && (x[j] < 0.5)) {\n                    bins[1] += 1;\n                } else if ((x[j] >= 0.5) && (x[j] < 0.75)) {\n                    bins[2] += 1;\n                } else if ((x[j] >= 0.75) && (x[j] < 1.0)) {\n                    bins[3] += 1;\n                }\n            }\n        }\n    }\n\n    int start_local_idx = n_per_rank * world_rank + std::min(remainder, world_rank);\n    int end_local_idx = n_per_rank * (world_rank + 1) - 1 + std::min(remainder, world_rank);\n\n    int send_counts[4] = {0, 0, 0, 0};\n    int recv_counts[4] = {0, 0, 0, 0};\n    int displs[4] = {0, 0, 0, 0};\n\n    for (int i = start_local_idx; i <= end_local_idx; i++) {\n        if ((x[i] >= 0.0) && (x[i] < 0.25)) {\n            send_counts[0] += 1;\n        } else if ((x[i] >= 0.25) && (x[i] < 0.5)) {\n            send_counts[1] += 1;\n        } else if ((x[i] >= 0.5) && (x[i] < 0.75)) {\n            send_counts[2] += 1;\n        } else if ((x[i] >= 0.75) && (x[i] < 1.0)) {\n            send_counts[3] += 1;\n        }\n    }\n\n    MPI_Scatter(send_counts, 4, MPI_INT, recv_counts, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        displs[0] = 0;\n        displs[1] = recv_counts[0];\n        displs[2] = displs[1] + recv_counts[1];\n        displs[3] = displs[2] + recv_counts[2];\n    }\n\n    MPI_Scatterv(send_counts, recv_counts, displs, MPI_INT, recv_counts, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n\n    MPI_Gatherv(recv_counts, 4, MPI_INT, bins.data(), recv_counts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n  size_t rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<double> local_bins(4);\n  size_t n = x.size();\n  size_t n_local = n / size;\n  size_t rest = n % size;\n  size_t start = n_local * rank + std::min(rank, rest);\n  size_t end = n_local * (rank + 1) + std::min(rank + 1, rest);\n  size_t local_n = end - start;\n  std::vector<double> local_x(local_n);\n\n  for(int i = 0; i < local_n; i++)\n    local_x[i] = x[start + i];\n\n  std::sort(local_x.begin(), local_x.end());\n\n  for(int i = 0; i < 4; i++) {\n    for(int j = 0; j < local_n; j++) {\n      if(local_x[j] >= (double)i * 0.25 && local_x[j] < ((double)(i + 1) * 0.25))\n        local_bins[i]++;\n      else if(local_x[j] >= (double)i * 0.5 && local_x[j] < ((double)(i + 1) * 0.5))\n        local_bins[i]++;\n      else if(local_x[j] >= (double)i * 0.75 && local_x[j] < ((double)(i + 1) * 0.75))\n        local_bins[i]++;\n    }\n  }\n\n  std::array<size_t, 4> total_bins;\n  MPI_Reduce(local_bins.data(), total_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  bins = total_bins;\n}",
            "// Compute the number of elements in `x` that are less than 0.25.\n  // Store the result in `bins[0]`.\n  // You may want to use the MPI library routine MPI_Reduce.\n  size_t less_than_quarter;\n  MPI_Reduce(&less_than_quarter, &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // Compute the number of elements in `x` that are less than 0.5.\n  // Store the result in `bins[1]`.\n  // You may want to use the MPI library routine MPI_Reduce.\n  size_t less_than_half;\n  MPI_Reduce(&less_than_half, &bins[1], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // Compute the number of elements in `x` that are less than 0.75.\n  // Store the result in `bins[2]`.\n  // You may want to use the MPI library routine MPI_Reduce.\n  size_t less_than_three_quarters;\n  MPI_Reduce(&less_than_three_quarters, &bins[2], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  // Compute the number of elements in `x` that are less than 1.\n  // Store the result in `bins[3]`.\n  // You may want to use the MPI library routine MPI_Reduce.\n  size_t less_than_one;\n  MPI_Reduce(&less_than_one, &bins[3], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    int size = x.size();\n    int chunk = size/nprocs;\n    int rem = size%nprocs;\n    int start, stop;\n\n    // 1. Calculate start and stop indices.\n    if(rank == 0){\n        for(int r=0; r<nprocs; r++){\n            start = chunk * r;\n            stop = start + chunk + (r<rem? 1 : 0);\n            MPI_Send(&start, 1, MPI_INT, r, 0, MPI_COMM_WORLD);\n            MPI_Send(&stop, 1, MPI_INT, r, 1, MPI_COMM_WORLD);\n        }\n    }else{\n        MPI_Status status;\n        MPI_Recv(&start, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, &status);\n        MPI_Recv(&stop, 1, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n    }\n\n    // 2. Calculate the number of elements in each bin, and store them in bins.\n    for(int i=start; i<stop; i++){\n        double frac = x[i] - floor(x[i]);\n        if(frac < 0.25) bins[0]++;\n        if(frac >= 0.25 && frac < 0.5) bins[1]++;\n        if(frac >= 0.5 && frac < 0.75) bins[2]++;\n        if(frac >= 0.75) bins[3]++;\n    }\n}",
            "int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  std::vector<int> local_bins;\n  local_bins.resize(4);\n  for (int i = rank; i < x.size(); i += nprocs) {\n    double value = x[i];\n    if (value >= 0.0 && value < 0.25) {\n      ++local_bins[0];\n    } else if (value >= 0.25 && value < 0.5) {\n      ++local_bins[1];\n    } else if (value >= 0.5 && value < 0.75) {\n      ++local_bins[2];\n    } else if (value >= 0.75 && value < 1.0) {\n      ++local_bins[3];\n    }\n  }\n\n  std::array<int, 4> local_sum;\n  MPI_Reduce(&local_bins[0], &local_sum[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  bins[0] = local_sum[0];\n  bins[1] = local_sum[1];\n  bins[2] = local_sum[2];\n  bins[3] = local_sum[3];\n\n  if (rank == 0) {\n    std::vector<int> global_bins(4);\n    MPI_Reduce(&local_bins[0], &global_bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins[0] = global_bins[0];\n    bins[1] = global_bins[1];\n    bins[2] = global_bins[2];\n    bins[3] = global_bins[3];\n  }\n}",
            "int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<size_t> localBins;\n\n  // split work into chunks\n  std::vector<double> localX;\n  if (rank == 0) {\n    // first process\n    localX = x;\n  } else {\n    // other processes\n    localX.resize(x.size()/size);\n  }\n  MPI_Scatter(&x[0], localX.size(), MPI_DOUBLE, &localX[0], localX.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  \n  // compute quartile count for this chunk\n  auto localCount = countQuartiles(localX);\n\n  // gather results\n  if (rank == 0) {\n    // first process\n    localBins = localCount;\n  } else {\n    // other processes\n    MPI_Gather(&localCount[0], localCount.size(), MPI_UNSIGNED_LONG_LONG, &localBins[0], localCount.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n\n  // sum results\n  if (rank == 0) {\n    // first process\n    bins = {0, 0, 0, 0};\n    for (size_t i=0; i < localBins.size(); i++) {\n      bins[0] += localBins[i];\n    }\n  } else {\n    // other processes\n    MPI_Reduce(&localBins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Write your solution here.\n}",
            "int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    std::vector<size_t> partial_bins(4);\n    std::vector<double> partial_x(x.size() / num_ranks);\n\n    MPI_Scatter(x.data(), x.size() / num_ranks, MPI_DOUBLE, partial_x.data(), x.size() / num_ranks, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (size_t i = 0; i < partial_x.size(); ++i) {\n        if (partial_x[i] < 0.25) {\n            ++partial_bins[0];\n        }\n        if (partial_x[i] >= 0.25 && partial_x[i] < 0.5) {\n            ++partial_bins[1];\n        }\n        if (partial_x[i] >= 0.5 && partial_x[i] < 0.75) {\n            ++partial_bins[2];\n        }\n        if (partial_x[i] >= 0.75) {\n            ++partial_bins[3];\n        }\n    }\n\n    MPI_Gather(partial_bins.data(), 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement me\n}",
            "double delta = 0.25;\n    int num_ranks;\n    int rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> my_x;\n    size_t my_bins[4] = {0};\n\n    int delta_index = std::floor(delta * x.size());\n    int start = rank * x.size() / num_ranks;\n    int end = (rank + 1) * x.size() / num_ranks;\n\n    std::copy(x.begin() + start, x.begin() + end, std::back_inserter(my_x));\n\n    for (auto const& value : my_x) {\n        if ((value - std::floor(value)) < delta) {\n            my_bins[0] += 1;\n        } else if ((value - std::floor(value)) < delta * 2) {\n            my_bins[1] += 1;\n        } else if ((value - std::floor(value)) < delta * 3) {\n            my_bins[2] += 1;\n        } else {\n            my_bins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(my_bins, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (size_t i = 0; i < bins.size(); i++) {\n            bins[i] = std::ceil(delta * bins[i] / x.size());\n        }\n    }\n}",
            "// 1.  Count the number of doubles in each bin.\n  // 2.  Divide by N and store the results in bins.\n  // 3.  Communicate the results to the root rank (rank 0).\n  // 4.  Use MPI to send the results from rank 0 to the other ranks.\n\n  // TODO: implement\n\n  // 1.  Count the number of doubles in each bin.\n  bins.fill(0);\n\n  // 2.  Divide by N and store the results in bins.\n  for (auto elem : x) {\n    if (elem >= 0 && elem < 0.25) {\n      bins[0]++;\n    } else if (elem >= 0.25 && elem < 0.5) {\n      bins[1]++;\n    } else if (elem >= 0.5 && elem < 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n\n  // 3.  Communicate the results to the root rank (rank 0).\n  // 4.  Use MPI to send the results from rank 0 to the other ranks.\n\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  std::vector<size_t> globalBins(bins);\n  MPI_Reduce(&bins[0], &globalBins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  bins = globalBins;\n}",
            "size_t numRanks, rank;\n    double start, end;\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Determine how many elements each rank will be responsible for\n    size_t elementsPerRank = x.size() / numRanks;\n    // If numRanks is not a factor of the array size, add one to each rank's count\n    if (x.size() % numRanks!= 0) {\n        elementsPerRank += 1;\n    }\n    // Offset for this rank's elements\n    size_t offset = rank * elementsPerRank;\n    // Size of this rank's elements\n    size_t size = elementsPerRank;\n    // Determine whether this rank will handle the last elements\n    if (rank == (numRanks - 1)) {\n        size = x.size() - offset;\n    }\n\n    // Allocate memory for local vector\n    double *localx = new double[size];\n    if (rank == 0) {\n        start = MPI_Wtime();\n    }\n    // Scatter input data to local process\n    MPI_Scatter(&x[offset], size, MPI_DOUBLE, localx, size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        end = MPI_Wtime();\n    }\n\n    // Count the number of elements in each bin\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (size_t i = 0; i < size; i++) {\n        if (localx[i] < 0.25) {\n            bins[0]++;\n        }\n        else if (localx[i] < 0.5) {\n            bins[1]++;\n        }\n        else if (localx[i] < 0.75) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n\n    // Gather results to root process\n    MPI_Gather(&bins, 4, MPI_UNSIGNED_LONG, &bins, 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // Print the time for the first rank\n    if (rank == 0) {\n        std::cout << \"Time: \" << end - start << std::endl;\n        std::cout << \"Local x: \";\n        for (size_t i = 0; i < size; i++) {\n            std::cout << localx[i] << \", \";\n        }\n        std::cout << std::endl;\n        std::cout << \"Bins: \";\n        for (size_t i = 0; i < 4; i++) {\n            std::cout << bins[i] << \", \";\n        }\n        std::cout << std::endl;\n    }\n\n    // Deallocate memory for local vector\n    delete[] localx;\n}",
            "int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n\n  if(rank == 0){\n    for(int i = 1; i < size; ++i){\n      std::vector<size_t> localBins(4);\n      MPI_Recv(&localBins, 4, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n\n      //merge the local bins\n      for(int j = 0; j < 4; ++j){\n        bins[j] += localBins[j];\n      }\n    }\n  }\n  else{\n    //each rank gets a chunk of the data, and computes their own quartile counts\n    std::vector<double> localData;\n    size_t chunkSize = x.size() / size;\n    for(int i = rank*chunkSize; i < (rank+1)*chunkSize; ++i){\n      localData.push_back(x[i]);\n    }\n\n    std::vector<size_t> localBins(4);\n    localBins[0] = 0;\n    localBins[1] = 0;\n    localBins[2] = 0;\n    localBins[3] = 0;\n\n    for(int i = 0; i < localData.size(); ++i){\n      double val = localData[i];\n      if(val >= 0.0 && val < 0.25){\n        localBins[0]++;\n      }\n      else if(val >= 0.25 && val < 0.5){\n        localBins[1]++;\n      }\n      else if(val >= 0.5 && val < 0.75){\n        localBins[2]++;\n      }\n      else{\n        localBins[3]++;\n      }\n    }\n\n    MPI_Send(&localBins, 4, MPI_INT, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO\n}",
            "size_t numProcs;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n  // Create the subcommunicator\n  MPI_Comm subcomm;\n  MPI_Comm_split(MPI_COMM_WORLD, 0, rank, &subcomm);\n  // Determine which rank is in the subcommunicator\n  int rank_sub;\n  MPI_Comm_rank(subcomm, &rank_sub);\n\n  int chunk_size = x.size() / numProcs;\n  int remainder = x.size() % numProcs;\n\n  // Each processor knows which part of the vector to count\n  int start = rank_sub * chunk_size + std::min(rank_sub, remainder);\n  int end = start + chunk_size + (rank_sub < remainder? 1 : 0);\n\n  // Count the elements in the given range\n  size_t num_elem = 0;\n  for (int i = start; i < end; i++) {\n    if (x[i] >= 0 && x[i] < 0.25) {\n      num_elem++;\n    } else if (x[i] >= 0.25 && x[i] < 0.5) {\n      num_elem++;\n    } else if (x[i] >= 0.5 && x[i] < 0.75) {\n      num_elem++;\n    } else if (x[i] >= 0.75 && x[i] <= 1) {\n      num_elem++;\n    }\n  }\n\n  // Reduce the number of elements on each processor to a sum on rank 0\n  int num_elem_sub;\n  MPI_Reduce(&num_elem, &num_elem_sub, 1, MPI_INT, MPI_SUM, 0, subcomm);\n\n  if (rank_sub == 0) {\n    bins[0] = num_elem_sub / 4;\n    bins[1] = num_elem_sub / 2;\n    bins[2] = (num_elem_sub * 3) / 4;\n    bins[3] = num_elem_sub;\n  }\n}",
            "int my_rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n    int n = x.size();\n    int N = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &N);\n\n    int n_1 = n / N;\n    int remainder = n % N;\n\n    std::vector<double> local_x;\n    local_x.reserve(n_1);\n    if (my_rank < remainder) {\n        // first n_1 + remainder ranks get the remainder\n        local_x.resize(n_1 + 1);\n        std::copy(x.begin(), x.begin() + n_1 + 1, local_x.begin());\n    } else {\n        // last ranks get the rest\n        local_x.resize(n_1);\n        std::copy(x.begin(), x.begin() + n_1, local_x.begin());\n    }\n\n    std::vector<size_t> local_bins = {0, 0, 0, 0};\n\n    for (double elem : local_x) {\n        if (elem < 0.25) {\n            ++local_bins[0];\n        } else if (elem < 0.5) {\n            ++local_bins[1];\n        } else if (elem < 0.75) {\n            ++local_bins[2];\n        } else {\n            ++local_bins[3];\n        }\n    }\n\n    // std::cout << \"Rank \" << my_rank << \" received \" << local_bins << std::endl;\n    // collect results\n    MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // std::cout << \"Rank \" << my_rank << \" has \" << bins << std::endl;\n}",
            "if (x.size() < 1) {\n        return;\n    }\n\n    int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<double> x_copy;\n    if (rank == 0) {\n        x_copy = x;\n    }\n    MPI_Bcast(&x_copy[0], x_copy.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::vector<double> x_local(x_copy.begin() + rank, x_copy.begin() + rank + x_copy.size() / size);\n\n    std::vector<size_t> temp_bins(4, 0);\n    for (double elem : x_local) {\n        if (elem < 0.25) {\n            temp_bins[0] += 1;\n        }\n        else if (elem < 0.5) {\n            temp_bins[1] += 1;\n        }\n        else if (elem < 0.75) {\n            temp_bins[2] += 1;\n        }\n        else {\n            temp_bins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(&temp_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        for (size_t &elem : bins) {\n            elem /= x_copy.size() / size;\n        }\n    }\n}",
            "/* your solution here */\n}",
            "// TODO: Your implementation here\n  bins = {0,0,0,0};\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int num_local_elements = x.size()/size;\n  std::vector<double> local_x(num_local_elements);\n  if (rank == 0){\n    for (int i = 0; i < num_local_elements; i++){\n      local_x[i] = x[i];\n    }\n    MPI_Scatter(&local_x[0], num_local_elements, MPI_DOUBLE, &x[0], num_local_elements, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n  else{\n    MPI_Scatter(&x[0], num_local_elements, MPI_DOUBLE, &local_x[0], num_local_elements, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n  for (int i = 0; i < num_local_elements; i++){\n    if (local_x[i] >= 0 && local_x[i] < 0.25)\n      bins[0] += 1;\n    else if (local_x[i] >= 0.25 && local_x[i] < 0.5)\n      bins[1] += 1;\n    else if (local_x[i] >= 0.5 && local_x[i] < 0.75)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n  int count_global[4] = {0,0,0,0};\n  MPI_Reduce(&bins[0], &count_global[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0)\n    bins = {count_global[0], count_global[1], count_global[2], count_global[3]};\n}",
            "// TODO\n}",
            "// TODO: implement\n  size_t n = x.size();\n  std::vector<size_t> counts(n/4, 0);\n  if(n < 4){\n    for(size_t i = 0; i < n; ++i){\n      if(i < n/4){\n        ++counts[0];\n      } else if(i < n*2/4){\n        ++counts[1];\n      } else if(i < n*3/4){\n        ++counts[2];\n      } else {\n        ++counts[3];\n      }\n    }\n  } else {\n    std::vector<double> xs(n);\n    MPI_Scatter(&x[0], n/4, MPI_DOUBLE, &xs[0], n/4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    for(size_t i = 0; i < n/4; ++i){\n      counts[i] = std::count_if(xs.begin()+i*n/4, xs.begin()+(i+1)*n/4, [](double x){ return x > 0 && x < 0.25; });\n    }\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: Your implementation goes here\n}",
            "// TODO: Your code here\n  bins = {0,0,0,0};\n}",
            "const auto n = x.size();\n\n    // TODO: Your code goes here!\n\n}",
            "// TODO: Your code here!\n}",
            "// TODO: Your code goes here.\n}",
            "// TODO: Your code here\n}",
            "// TODO: Your code here\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int n = x.size();\n  double *localx = new double[n];\n  int *localbins = new int[4];\n  if(rank == 0)\n  {\n    for(int i = 0; i < 4; i++)\n    {\n      localbins[i] = 0;\n    }\n    int start = 0;\n    int end = n/size;\n    for(int i = 0; i < size; i++)\n    {\n      if(i == 0)\n      {\n        for(int j = 0; j < n; j++)\n        {\n          localx[j] = x[j];\n        }\n      }\n      else\n      {\n        for(int j = start; j < end; j++)\n        {\n          localx[j] = x[i*n + j];\n        }\n        start = end;\n        end += n/size;\n      }\n      MPI_Send(localx, n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      MPI_Send(localbins, 4, MPI_INT, i, 1, MPI_COMM_WORLD);\n    }\n    for(int i = 0; i < n; i++)\n    {\n      if(i%size == rank)\n      {\n        if(x[i] >= 0 && x[i] < 0.25)\n        {\n          localbins[0] += 1;\n        }\n        else if(x[i] >= 0.25 && x[i] < 0.5)\n        {\n          localbins[1] += 1;\n        }\n        else if(x[i] >= 0.5 && x[i] < 0.75)\n        {\n          localbins[2] += 1;\n        }\n        else if(x[i] >= 0.75 && x[i] <= 1)\n        {\n          localbins[3] += 1;\n        }\n      }\n    }\n  }\n  else\n  {\n    MPI_Status status;\n    MPI_Recv(localx, n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    MPI_Recv(localbins, 4, MPI_INT, 0, 1, MPI_COMM_WORLD, &status);\n  }\n  MPI_Reduce(localbins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  if(rank!= 0)\n  {\n    delete[] localx;\n    delete[] localbins;\n  }\n}",
            "if (x.empty()) {\n        bins = { 0, 0, 0, 0 };\n        return;\n    }\n\n    std::vector<double> local_x;\n    local_x.reserve(x.size());\n    auto loc_x = x.cbegin();\n    auto loc_x_end = x.cend();\n    std::vector<size_t> local_bins(4);\n    auto loc_bins = local_bins.begin();\n\n    while (loc_x!= loc_x_end) {\n        if (*loc_x < 0.25) {\n            ++loc_bins;\n        } else if (*loc_x < 0.5) {\n            ++(*loc_bins);\n        } else if (*loc_x < 0.75) {\n            ++(*loc_bins);\n        } else {\n            ++(*loc_bins);\n        }\n\n        ++loc_x;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int n = x.size();\n  MPI_Datatype doubleType;\n  MPI_Type_contiguous(sizeof(double), MPI_BYTE, &doubleType);\n  MPI_Type_commit(&doubleType);\n\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double lb, ub, count;\n  lb = 0;\n  ub = 0.25;\n\n  count = 0;\n  for (auto x_i : x) {\n    if (lb <= x_i && x_i < ub) count++;\n  }\n\n  MPI_Reduce(&count, &(bins[0]), 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    lb = 0.25;\n    ub = 0.5;\n    count = 0;\n    for (auto x_i : x) {\n      if (lb <= x_i && x_i < ub) count++;\n    }\n\n    MPI_Reduce(&count, &(bins[1]), 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    lb = 0.5;\n    ub = 0.75;\n    count = 0;\n    for (auto x_i : x) {\n      if (lb <= x_i && x_i < ub) count++;\n    }\n\n    MPI_Reduce(&count, &(bins[2]), 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    lb = 0.75;\n    ub = 1;\n    count = 0;\n    for (auto x_i : x) {\n      if (lb <= x_i && x_i < ub) count++;\n    }\n\n    MPI_Reduce(&count, &(bins[3]), 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n  MPI_Type_free(&doubleType);\n}",
            "// INSERT YOUR CODE HERE\n}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   // Each process has a contiguous copy of x\n   int num_elements = x.size() / num_ranks;\n\n   std::vector<double> local_x(num_elements);\n   std::copy(x.begin() + rank * num_elements,\n             x.begin() + (rank + 1) * num_elements,\n             local_x.begin());\n\n   // Count the number of elements in each quartile\n   std::array<size_t, 4> local_bins;\n   for (auto element : local_x) {\n      if (element < 0.25) {\n         ++local_bins[0];\n      } else if (element < 0.5) {\n         ++local_bins[1];\n      } else if (element < 0.75) {\n         ++local_bins[2];\n      } else {\n         ++local_bins[3];\n      }\n   }\n\n   // Reduce the values from each process to rank 0\n   MPI_Reduce(local_bins.data(),\n             bins.data(),\n             4,\n             MPI_UNSIGNED_LONG_LONG,\n             MPI_SUM,\n             0,\n             MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      // Normalize the counts by dividing by the number of elements in each\n      // quartile\n      for (auto &count : bins) {\n         count /= num_elements;\n      }\n   }\n}",
            "// your code goes here\n\n    // YOUR CODE HERE\n    // Calculate the number of elements in each bin\n    // Initialize the bins\n    // Get the size of the vector\n    // Get the rank of the process\n    // Calculate the number of elements in each bin\n    // Each process should calculate the number of elements in its local vector and then sum it up\n    // Sum up the number of elements in each bin for each process\n    // Every process should send the number of elements in each bin to rank 0\n    // Rank 0 should sum up the number of elements in each bin for every process and store it in bins\n    // Check the rank of the process\n    // Send the bins to rank 0\n    // Calculate the number of elements in each bin for every process\n    // Sum up the number of elements in each bin\n    // Store the number of elements in each bin in bins\n    \n    // YOUR CODE ENDS\n}",
            "// TODO\n}",
            "int num_proc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    double n = x.size();\n    double q = n / 4.0;\n    double step = 0.25;\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    for (int i = 0; i < x.size(); i++) {\n        if (x[i] <= (q * 0.25) && x[i] > (q * 0.0))\n            bins[0] += 1;\n        else if (x[i] <= (q * 0.5) && x[i] > (q * 0.25))\n            bins[1] += 1;\n        else if (x[i] <= (q * 0.75) && x[i] > (q * 0.5))\n            bins[2] += 1;\n        else if (x[i] <= (q * 1.0) && x[i] > (q * 0.75))\n            bins[3] += 1;\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::sort(x.begin(), x.end());\n    double quartile[4] = {0.25, 0.5, 0.75, 1};\n    size_t size = x.size();\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (size % 2 == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = std::count_if(x.begin() + rank * size / 2,\n                                    x.begin() + rank * size / 2 + size / 2,\n                                    [&quartile, i](double y) {\n                                        return y >= quartile[i - 1] && y < quartile[i];\n                                    });\n        }\n    } else {\n        for (int i = 0; i < 4; i++) {\n            if (rank == 0) {\n                bins[i] = std::count_if(x.begin() + size / 2,\n                                        x.begin() + size,\n                                        [&quartile, i](double y) {\n                                            return y >= quartile[i - 1] && y < quartile[i];\n                                        });\n            } else if (rank == 1) {\n                bins[i] = std::count_if(x.begin(),\n                                        x.begin() + size / 2 + 1,\n                                        [&quartile, i](double y) {\n                                            return y >= quartile[i - 1] && y < quartile[i];\n                                        });\n            }\n        }\n    }\n    std::array<size_t, 4> recv_counts;\n    std::array<int, 4> recv_ranks = {1, 2, 3, 0};\n    MPI_Gather(&bins, 4, MPI_UNSIGNED_LONG, recv_counts.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    MPI_Gatherv(&bins, 4, MPI_UNSIGNED_LONG, recv_counts.data(), recv_counts.data(), recv_ranks.data(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        std::copy(recv_counts.begin(), recv_counts.end(), bins.begin());\n    }\n}",
            "double num_elements = x.size();\n    int num_processes;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n\n    std::vector<double> local_counts(4, 0);\n\n    double quarter = num_elements / (double) 4;\n\n    std::vector<int> partition = {0, quarter, 2 * quarter, 3 * quarter};\n\n    for (int i = 0; i < num_processes; i++) {\n        std::vector<double> local_x;\n        local_x.assign(x.begin() + partition[i], x.begin() + partition[i + 1]);\n\n        std::vector<int> local_counts_int(4, 0);\n        for (int j = 0; j < local_x.size(); j++) {\n            double value = local_x[j];\n            if (value >= 0.0 && value < 0.25) {\n                local_counts_int[0] += 1;\n            } else if (value >= 0.25 && value < 0.50) {\n                local_counts_int[1] += 1;\n            } else if (value >= 0.50 && value < 0.75) {\n                local_counts_int[2] += 1;\n            } else if (value >= 0.75 && value < 1.0) {\n                local_counts_int[3] += 1;\n            }\n        }\n        std::vector<double> local_counts_double(local_counts_int.begin(), local_counts_int.end());\n        MPI_Reduce(local_counts_double.data(), local_counts.data(), local_counts.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    if (num_processes == 1) {\n        std::array<size_t, 4> serial_counts = {0, 0, 0, 0};\n        for (int i = 0; i < local_counts.size(); i++) {\n            serial_counts[i] = local_counts[i];\n        }\n        bins = serial_counts;\n        return;\n    }\n\n    MPI_Reduce(local_counts.data(), bins.data(), bins.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (num_processes > 4) {\n        for (int i = 1; i < num_processes; i++) {\n            MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "int rank, n;\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // split the data into n equally-sized chunks\n    size_t chunk_size = x.size() / n;\n    std::vector<double> local(chunk_size);\n    if (rank < x.size() % n) {\n        chunk_size++;\n    }\n    std::vector<double> sendbuf(chunk_size), recvbuf(chunk_size);\n    if (rank == 0) {\n        std::copy(x.begin(), x.end(), local.begin());\n    }\n\n    // compute the local fractional part of the data,\n    // and send the data for each chunk to its rank + 1\n    // to compute its fractional part.\n    if (rank == 0) {\n        for (size_t i = 1; i < n; ++i) {\n            MPI_Recv(&sendbuf[0], chunk_size, MPI_DOUBLE, i, rank,\n                     MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < chunk_size; ++j) {\n                local[j] += sendbuf[j];\n            }\n        }\n    } else {\n        MPI_Send(&local[0], chunk_size, MPI_DOUBLE, 0, rank, MPI_COMM_WORLD);\n        for (size_t i = 0; i < chunk_size; ++i) {\n            sendbuf[i] = local[i] - (int)(local[i]);\n        }\n    }\n\n    // send the fractional parts to the next rank,\n    // who will compute the number of elements in each bin\n    if (rank < n - 1) {\n        MPI_Send(&sendbuf[0], chunk_size, MPI_DOUBLE, rank + 1, rank,\n                 MPI_COMM_WORLD);\n    } else {\n        for (size_t i = 0; i < chunk_size; ++i) {\n            recvbuf[i] = 1;\n        }\n        MPI_Send(&recvbuf[0], chunk_size, MPI_DOUBLE, 0, rank, MPI_COMM_WORLD);\n    }\n\n    // for each rank, get the number of elements in each bin\n    if (rank == 0) {\n        bins[0] = 0;\n        for (int i = 1; i < n; ++i) {\n            MPI_Recv(&recvbuf[0], chunk_size, MPI_DOUBLE, i, i, MPI_COMM_WORLD,\n                     MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < chunk_size; ++j) {\n                bins[0] += recvbuf[j];\n            }\n        }\n        bins[1] = 0;\n        for (size_t i = 0; i < chunk_size; ++i) {\n            recvbuf[i] = 1;\n        }\n        for (int i = 1; i < n; ++i) {\n            MPI_Recv(&recvbuf[0], chunk_size, MPI_DOUBLE, i, i, MPI_COMM_WORLD,\n                     MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < chunk_size; ++j) {\n                bins[1] += recvbuf[j];\n            }\n        }\n        bins[2] = 0;\n        for (size_t i = 0; i < chunk_size; ++i) {\n            recvbuf[i] = 1;\n        }\n        for (int i = 1; i < n; ++i) {\n            MPI_Recv(&recvbuf[0], chunk_size, MPI_DOUBLE, i, i, MPI_COMM_WORLD,\n                     MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < chunk_size; ++j) {\n                bins[2] += recvbuf[j];\n            }\n        }\n        bins[3] = 0;\n        for (size_t i = 0; i < chunk_size; ++i) {\n            recvbuf[i] = 1;\n        }\n        for (int i = 1; i < n; ++i) {\n            MPI_Recv(&recvbuf[0], chunk_size, MPI_DOUBLE, i, i, MPI_COMM_WORLD,\n                     MPI_STATUS_IGNORE);\n            for (size_t j = 0; j < chunk_size; ++j) {\n                bins[3] += recvbuf[j];\n            }\n        }\n    } else {\n        MPI_Recv(&recvbuf[0], chunk_size, MPI_DOUBLE",
            "// TODO: Your code here\n}",
            "int n = x.size();\n    int p = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &p);\n    int q = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &q);\n\n    size_t size = 0;\n    MPI_Aint lb, extent;\n    MPI_Type_get_extent(MPI_DOUBLE, &lb, &extent);\n    MPI_Type_size(MPI_DOUBLE, &size);\n\n    double *counts = new double[n / q];\n    int *sendcounts = new int[q];\n    int *displs = new int[q];\n\n    for (int i = 0; i < q; i++) {\n        sendcounts[i] = n / q;\n    }\n\n    if (p == 0) {\n        displs[0] = 0;\n    } else {\n        displs[0] = n / q;\n    }\n\n    for (int i = 1; i < q; i++) {\n        displs[i] = displs[i - 1] + sendcounts[i - 1];\n    }\n\n    MPI_Datatype type = MPI_DOUBLE;\n    MPI_Type_create_hindexed(sendcounts[p], sendcounts + p, displs + p, type, &type);\n    MPI_Type_commit(&type);\n\n    MPI_Scatterv(x.data(), sendcounts, displs, type, counts, sendcounts[p], MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    size_t idx = 0;\n    size_t quartile = 0;\n    for (size_t i = 0; i < n; i++) {\n        if (counts[i] > 0) {\n            if (idx < 0.25 * n) {\n                quartile = 0;\n            } else if (idx < 0.5 * n) {\n                quartile = 1;\n            } else if (idx < 0.75 * n) {\n                quartile = 2;\n            } else {\n                quartile = 3;\n            }\n            idx++;\n            bins[quartile]++;\n        }\n    }\n\n    MPI_Reduce(counts, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    delete[] counts;\n    delete[] sendcounts;\n    delete[] displs;\n    MPI_Type_free(&type);\n}",
            "std::array<double, 4> lowBounds = {0.0, 0.25, 0.5, 0.75};\n  std::array<double, 4> highBounds = {0.25, 0.5, 0.75, 1.0};\n\n  // TODO: write code here\n}",
            "// TODO: Your code here.\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    size_t x_size = x.size();\n    if (x_size % size!= 0) {\n        if (rank == 0) {\n            std::cout << \"x size (\" << x_size << \") is not divisible by the number of ranks (\" << size << \").\" << std::endl;\n        }\n        return;\n    }\n\n    size_t start = rank * x_size / size;\n    size_t end = start + x_size / size;\n\n    size_t local_bins[4] = {0, 0, 0, 0};\n\n    for (size_t i = start; i < end; ++i) {\n        if (x[i] >= 0.0 && x[i] < 0.25) {\n            ++local_bins[0];\n        } else if (x[i] >= 0.25 && x[i] < 0.5) {\n            ++local_bins[1];\n        } else if (x[i] >= 0.5 && x[i] < 0.75) {\n            ++local_bins[2];\n        } else if (x[i] >= 0.75 && x[i] <= 1.0) {\n            ++local_bins[3];\n        }\n    }\n\n    MPI_Reduce(local_bins, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Implement this function.\n}",
            "// Your code goes here!\n}",
            "// TODO: your code goes here!\n}",
            "// TODO\n  // 1. Broadcast the vector x\n  // 2. Calculate the number of elements in x for each bin\n  // 3. Sum the elements in each bin\n  // 4. Store the result on the 0th rank\n}",
            "int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // TODO: Compute the number of elements in each bin (hint: use partition)\n\n    int num_elements = x.size();\n    int bin_size = num_elements / world_size;\n\n    if (world_rank == 0) {\n        int offset = 0;\n        for (int i = 0; i < world_size; i++) {\n            // TODO: Compute the number of elements in this bin\n\n            // TODO: Compute the number of elements in the previous bins\n\n            // TODO: Compute the start and end indices of this bin in x\n\n            std::vector<double> local_x(x.begin() + offset, x.begin() + offset + bin_size);\n            std::array<size_t, 4> local_bins;\n\n            // TODO: Compute the number of elements in each bin\n\n            // TODO: Send the number of elements in each bin to each rank\n\n            offset += bin_size;\n        }\n    } else {\n        std::vector<double> local_x(bin_size);\n        // TODO: Receive the number of elements in each bin from rank 0\n\n        // TODO: Receive the number of elements in the previous bins from rank 0\n\n        // TODO: Compute the start and end indices of this bin in x\n\n        std::array<size_t, 4> local_bins;\n        // TODO: Receive the number of elements in each bin from rank 0\n\n        // TODO: Compute the number of elements in each bin\n\n        // TODO: Store the number of elements in each bin in local_bins\n\n        // TODO: Send the number of elements in each bin to rank 0\n    }\n\n    if (world_rank == 0) {\n        bins = local_bins;\n    }\n}",
            "// YOUR CODE HERE\n    size_t size = x.size();\n    // MPI_Init();\n    int rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int step = size / world_size;\n    int start = rank * step;\n    int end = start + step;\n    std::vector<double> local_vec;\n    if(rank == 0){\n        for(int i = 0; i < world_size - 1; ++i){\n            std::vector<double> tmp(x.begin() + i * step, x.begin() + (i + 1) * step);\n            local_vec.insert(local_vec.end(), tmp.begin(), tmp.end());\n        }\n    } else {\n        local_vec = std::vector<double>(x.begin() + start, x.begin() + end);\n    }\n    std::vector<double> local_bins(4, 0);\n    int local_size = local_vec.size();\n    for(int i = 0; i < local_size; ++i){\n        if(local_vec[i] >= 0 && local_vec[i] <= 0.25){\n            local_bins[0] += 1;\n        } else if(local_vec[i] > 0.25 && local_vec[i] <= 0.5){\n            local_bins[1] += 1;\n        } else if(local_vec[i] > 0.5 && local_vec[i] <= 0.75){\n            local_bins[2] += 1;\n        } else if(local_vec[i] > 0.75){\n            local_bins[3] += 1;\n        }\n    }\n    std::array<int, 4> recv_counts;\n    std::array<int, 4> recv_displacements;\n    if(rank == 0){\n        for(int i = 0; i < 4; ++i){\n            recv_counts[i] = 1;\n        }\n    } else {\n        for(int i = 0; i < 4; ++i){\n            recv_counts[i] = local_bins[i];\n        }\n    }\n    recv_displacements[0] = 0;\n    recv_displacements[1] = recv_counts[0];\n    recv_displacements[2] = recv_counts[0] + recv_counts[1];\n    recv_displacements[3] = recv_counts[0] + recv_counts[1] + recv_counts[2];\n    MPI_Gatherv(&local_bins[0], 4, MPI_INT, &bins[0], &recv_counts[0], &recv_displacements[0], MPI_INT, 0, MPI_COMM_WORLD);\n    // MPI_Finalize();\n}",
            "// YOUR CODE HERE\n}",
            "// 1. MPI rank and size\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // 2. Get the data distribution\n  size_t num_data = x.size();\n  size_t num_local = num_data / size;\n  size_t num_extra = num_data % size;\n\n  // 3. Get the local data\n  std::vector<double> local_data;\n  if (rank < num_extra) {\n    local_data = std::vector<double>(x.begin() + rank * (num_local + 1), x.begin() + (rank + 1) * (num_local + 1));\n  } else {\n    local_data = std::vector<double>(x.begin() + rank * num_local + num_extra, x.end());\n  }\n\n  // 4. Compute the local quartiles\n  std::array<size_t, 4> local_bins;\n  countQuartilesLocal(local_data, local_bins);\n\n  // 5. Gather the quartiles and sum\n  std::array<size_t, 4> all_bins;\n  MPI_Reduce(local_bins.data(), all_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // 6. Scatter the result to rank 0\n  MPI_Scatter(all_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "cuda",
        "prompt": "/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\n__global__ void countQuartiles(const double *x, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\t\n\t// TODO: Copy code from the CPU version.\n\tint lower = 0;\n\tint upper = N;\n\t\n\t// TODO: Copy code from the CPU version.\n\tint mid = (lower + upper) / 2;\n\n\twhile (true) {\n\t\tdouble x_mid = x[mid];\n\t\tif (x_mid > 0.75) {\n\t\t\tupper = mid;\n\t\t}\n\t\telse if (x_mid < 0.25) {\n\t\t\tlower = mid;\n\t\t}\n\t\telse {\n\t\t\tbins[0] += 1;\n\t\t\tupper = mid;\n\t\t}\n\t\tmid = (lower + upper) / 2;\n\n\t\tif (upper - lower <= 1) {\n\t\t\tbins[1] += upper - lower;\n\t\t\tbreak;\n\t\t}\n\t}\n\t// TODO: Copy code from the CPU version.\n\tlower = 0;\n\tupper = N;\n\t\n\t// TODO: Copy code from the CPU version.\n\tmid = (lower + upper) / 2;\n\n\twhile (true) {\n\t\tdouble x_mid = x[mid];\n\t\tif (x_mid > 0.5) {\n\t\t\tupper = mid;\n\t\t}\n\t\telse if (x_mid < 0) {\n\t\t\tlower = mid;\n\t\t}\n\t\telse {\n\t\t\tbins[2] += 1;\n\t\t\tlower = mid;\n\t\t}\n\t\tmid = (lower + upper) / 2;\n\n\t\tif (upper - lower <= 1) {\n\t\t\tbins[3] += upper - lower;\n\t\t\tbreak;\n\t\t}\n\t}\n}",
            "// YOUR CODE HERE\n}",
            "const double quartile0 = 0.25 * N;\n    const double quartile1 = 0.5 * N;\n    const double quartile2 = 0.75 * N;\n    const size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    const size_t stepSize = gridDim.x * blockDim.x;\n    for (size_t i = threadId; i < N; i += stepSize) {\n        double fractionalPart = x[i] - floor(x[i]);\n        if (fractionalPart >= 0 && fractionalPart < quartile0) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if (fractionalPart >= quartile0 && fractionalPart < quartile1) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if (fractionalPart >= quartile1 && fractionalPart < quartile2) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO: Your code goes here\n  // You should use atomicAdd() to update the counts\n  // HINT: bins[0] is for the 0 <= x < 0.25 bin, etc.\n  __shared__ double x_cache[64];\n  int idx = threadIdx.x;\n  x_cache[idx] = x[blockIdx.x * 64 + idx];\n  __syncthreads();\n  double p0 = x_cache[idx];\n  double p1 = x_cache[idx + 32];\n  if (idx < 32) {\n    double p2 = x_cache[idx + 64];\n    double p3 = x_cache[idx + 96];\n    atomicAdd(&bins[0], p0 < 0.25 * p1);\n    atomicAdd(&bins[1], (0.25 * p1 <= p0) && (p0 < 0.5 * p1));\n    atomicAdd(&bins[2], (0.5 * p1 <= p0) && (p0 < 0.75 * p1));\n    atomicAdd(&bins[3], p0 >= 0.75 * p1);\n  }\n  __syncthreads();\n  p0 = p1;\n  p1 = p2;\n  p2 = x_cache[idx + 64];\n  if (idx < 32) {\n    double p3 = x_cache[idx + 96];\n    atomicAdd(&bins[0], p0 < 0.25 * p1);\n    atomicAdd(&bins[1], (0.25 * p1 <= p0) && (p0 < 0.5 * p1));\n    atomicAdd(&bins[2], (0.5 * p1 <= p0) && (p0 < 0.75 * p1));\n    atomicAdd(&bins[3], p0 >= 0.75 * p1);\n  }\n  __syncthreads();\n  p0 = p1;\n  p1 = p2;\n  p2 = x_cache[idx + 96];\n  if (idx < 32) {\n    atomicAdd(&bins[0], p0 < 0.25 * p1);\n    atomicAdd(&bins[1], (0.25 * p1 <= p0) && (p0 < 0.5 * p1));\n    atomicAdd(&bins[2], (0.5 * p1 <= p0) && (p0 < 0.75 * p1));\n    atomicAdd(&bins[3], p0 >= 0.75 * p1);\n  }\n}",
            "// TODO: Implement.\n}",
            "double fractional_part = 0;\n  if (threadIdx.x == 0) {\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n  }\n\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N;\n       i += blockDim.x * gridDim.x) {\n    fractional_part = fmod(x[i], 0.5);\n    if (fractional_part < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (fractional_part < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (fractional_part < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n\n  if (threadIdx.x == 0) {\n    atomicAdd(&bins[0], bins[1]);\n    atomicAdd(&bins[0], bins[2]);\n    atomicAdd(&bins[0], bins[3]);\n  }\n}",
            "__shared__ double smem[256];\n\n  // Initialize bins with zero.\n  for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n    bins[i] = 0;\n  }\n  __syncthreads();\n\n  // Compute the number of elements in each of the four quartiles.\n  // Each thread will compute an element of the input vector.\n  // The size of the vector is divided into four parts. Each thread will\n  // increment the count of the quartile it belongs to.\n  const double q0 = 0.25 * (1.0 - 0.25);\n  const double q1 = 0.25 * (1.0 + 0.25);\n  const double q2 = 0.5 * (1.0 + 0.25);\n  const double q3 = 0.5 * (1.0 - 0.25);\n  for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N;\n       i += gridDim.x * blockDim.x) {\n    double x_i = x[i];\n    double frac_part = x_i - floor(x_i);\n    double quad = 4 * frac_part;\n    if (quad < q0) {\n      bins[0] += 1;\n    } else if (quad < q1) {\n      bins[1] += 1;\n    } else if (quad < q2) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n  __syncthreads();\n\n  // Sum up the counts from all threads.\n  for (int i = threadIdx.x; i < 4; i += blockDim.x) {\n    bins[i] = __shfl_reduce_add(bins[i]);\n  }\n  __syncthreads();\n}",
            "// TODO: replace this dummy kernel with your solution\n}",
            "// TODO: your code goes here\n}",
            "int tid = threadIdx.x;\n    double quartiles[4] = {0, 0.25, 0.5, 0.75};\n    // TODO: Your code goes here\n}",
            "// YOUR CODE HERE\n}",
            "extern __shared__ size_t shared_bins[];\n\tconst int tid = threadIdx.x;\n\tconst int bid = blockIdx.x;\n\n\t// Each block stores the number of doubles it has read from the vector x\n\tconst int num_doubles = blockDim.x * gridDim.x;\n\n\t// Each block calculates the number of doubles in [0, 0.25), [0.25, 0.5),\n\t// [0.5, 0.75), and [0.75, 1). The values of the bins are stored in \n\t// shared memory.\n\tif (tid < 4) {\n\t\tdouble start = tid / 4.0;\n\t\tdouble end = (tid + 1) / 4.0;\n\t\tint counter = 0;\n\n\t\tfor (int i = bid * num_doubles; i < N; i++) {\n\t\t\tdouble value = x[i];\n\n\t\t\tif (value >= start && value < end) {\n\t\t\t\tcounter++;\n\t\t\t}\n\t\t}\n\n\t\tshared_bins[tid] = counter;\n\t}\n\t__syncthreads();\n\n\t// Each thread adds the number of doubles in its block to the number of\n\t// doubles in its part of the vector x.\n\tfor (int stride = blockDim.x / 2; stride > 0; stride /= 2) {\n\t\tif (tid < stride && tid + stride < 4) {\n\t\t\tshared_bins[tid] += shared_bins[tid + stride];\n\t\t}\n\t\t__syncthreads();\n\t}\n\n\t// The thread with id 0 stores the number of doubles in [0, 0.25) to\n\t// the correct part of the vector `bins`.\n\tif (tid == 0) {\n\t\tbins[0] = shared_bins[0];\n\t}\n\n\t// The thread with id 1 stores the number of doubles in [0.25, 0.5) to\n\t// the correct part of the vector `bins`.\n\tif (tid == 1) {\n\t\tbins[1] = shared_bins[1] + shared_bins[0];\n\t}\n\n\t// The thread with id 2 stores the number of doubles in [0.5, 0.75) to\n\t// the correct part of the vector `bins`.\n\tif (tid == 2) {\n\t\tbins[2] = shared_bins[2] + shared_bins[1] + shared_bins[0];\n\t}\n\n\t// The thread with id 3 stores the number of doubles in [0.75, 1) to\n\t// the correct part of the vector `bins`.\n\tif (tid == 3) {\n\t\tbins[3] = shared_bins[3] + shared_bins[2] + shared_bins[1] + shared_bins[0];\n\t}\n}",
            "// Your code goes here.\n}",
            "...\n}",
            "// YOUR CODE HERE\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    int tid = threadIdx.x;\n    int blkid = blockIdx.x;\n    int blksz = blockDim.x;\n    int nblks = N/blksz + 1;\n    int start = blkid*blksz;\n    int end = (blkid+1)*blksz;\n    end = (end > N)? N : end;\n    int range = end - start;\n    double temp = 0;\n    int cnt = 0;\n    for (int i=start; i<end; i++) {\n        temp = x[i];\n        if (temp <= 0.25 && temp >= 0) {\n            cnt++;\n        } else if (temp > 0.25 && temp <= 0.5) {\n            cnt++;\n        } else if (temp > 0.5 && temp <= 0.75) {\n            cnt++;\n        } else if (temp > 0.75 && temp <= 1) {\n            cnt++;\n        }\n    }\n    if (tid == 0) {\n        atomicAdd(&(bins[0]), cnt);\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  double p = (double)tid / N;\n  size_t bin = p <= 0.25? 0 : p <= 0.5? 1 : p <= 0.75? 2 : 3;\n  atomicAdd(&bins[bin], __popcll(0xFFFFFFFFFFFFFFFF & (0x1ULL << tid)));\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double x_i = x[i];\n        double x_i_mod_1 = x_i - floor(x_i);\n\n        if (x_i_mod_1 < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x_i_mod_1 < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x_i_mod_1 < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "int tid = threadIdx.x;\n  int gid = blockIdx.x * blockDim.x + tid;\n\n  double temp;\n  if (gid < N) {\n    temp = x[gid] - floor(x[gid] / 1.25);\n    bins[temp < 0.25] += 1;\n    bins[temp < 0.5] += 1;\n    bins[temp < 0.75] += 1;\n    bins[temp < 1] += 1;\n  }\n}",
            "double a = 0.0;\n  double b = 0.25;\n  double c = 0.5;\n  double d = 0.75;\n\n  int tid = threadIdx.x;\n  int grid_size = blockDim.x;\n\n  size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n  size_t stride = blockDim.x * gridDim.x;\n\n  // TODO: Finish this function\n\n  __syncthreads();\n}",
            "const unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double y = x[i];\n        double bin = floor(y / 0.25);\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "int tid = threadIdx.x;\n  extern __shared__ size_t count[];\n\n  size_t n = N/4;\n  if (tid < n) {\n    count[tid] = 0;\n    for (int i = 0; i < 4; i++) {\n      double frac = (i + 0.25) / 4.0;\n      double v = x[tid*4 + i];\n      count[tid] += (v > frac && v < frac + 0.25)? 1 : 0;\n    }\n  } else {\n    count[tid] = 0;\n  }\n  for (int stride = 1; stride < 16; stride *= 2) {\n    __syncthreads();\n    if (tid < stride) {\n      count[tid] += count[tid + stride];\n    }\n  }\n  if (tid == 0) {\n    bins[0] = count[0];\n    bins[1] = count[n];\n    bins[2] = count[n*2];\n    bins[3] = count[n*3];\n  }\n}",
            "// TODO: implement this function\n}",
            "__shared__ double temp[BLOCK_SIZE];\n\n\t// compute the block-wide sums\n\tdouble sums[4] = {0, 0, 0, 0};\n\tsize_t offset = BLOCK_SIZE * blockIdx.x;\n\tsize_t stride = BLOCK_SIZE * gridDim.x;\n\n\tfor (size_t i = offset + threadIdx.x; i < N; i += stride) {\n\t\tdouble y = x[i];\n\t\tdouble y_mod = y - floor(y);\n\t\tif (y_mod <= 0.25)\n\t\t\tsums[0]++;\n\t\telse if (y_mod <= 0.5)\n\t\t\tsums[1]++;\n\t\telse if (y_mod <= 0.75)\n\t\t\tsums[2]++;\n\t\telse\n\t\t\tsums[3]++;\n\t}\n\ttemp[threadIdx.x] = sums[0];\n\ttemp[threadIdx.x + BLOCK_SIZE] = sums[1];\n\ttemp[threadIdx.x + 2 * BLOCK_SIZE] = sums[2];\n\ttemp[threadIdx.x + 3 * BLOCK_SIZE] = sums[3];\n\t__syncthreads();\n\n\t// reduce\n\tfor (size_t i = threadIdx.x; i < 4; i += BLOCK_SIZE) {\n\t\tsums[i] += temp[i];\n\t}\n\t__syncthreads();\n\n\tif (threadIdx.x == 0) {\n\t\tbins[0] = sums[0];\n\t\tbins[1] = sums[1];\n\t\tbins[2] = sums[2];\n\t\tbins[3] = sums[3];\n\t}\n}",
            "// TODO\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double x_i = x[i];\n    if (x_i < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (x_i < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (x_i < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: Your implementation\n  int threadId = threadIdx.x + blockIdx.x*blockDim.x;\n  int threadCount = blockDim.x*gridDim.x;\n  int index = threadId;\n  if(threadId < N)\n  {\n    if((x[index] >= 0) && (x[index] < 0.25))\n      atomicAdd(&bins[0], 1);\n    else if((x[index] >= 0.25) && (x[index] < 0.5))\n      atomicAdd(&bins[1], 1);\n    else if((x[index] >= 0.5) && (x[index] < 0.75))\n      atomicAdd(&bins[2], 1);\n    else if((x[index] >= 0.75) && (x[index] < 1))\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "__shared__ int threads_per_block;\n  __shared__ int blocks_per_grid;\n\n  int tid = threadIdx.x + threadIdx.y*blockDim.x;\n  int blockId = threadIdx.z + blockIdx.x*blockDim.z;\n  int gridId = blockIdx.y + blockIdx.z*gridDim.y;\n\n  if (blockId == 0 && threadIdx.x == 0) {\n    threads_per_block = blockDim.x*blockDim.y;\n    blocks_per_grid = gridDim.x*gridDim.y*gridDim.z;\n  }\n\n  __syncthreads();\n\n  // compute the number of threads within each block\n  __shared__ int thread_cnt;\n\n  if (blockId == 0) {\n    thread_cnt = 0;\n  }\n\n  __syncthreads();\n\n  // compute the number of threads within each grid\n  __shared__ int thread_in_grid;\n\n  if (gridId == 0) {\n    thread_in_grid = 0;\n  }\n\n  __syncthreads();\n\n  // each thread computes the number of doubles in the current grid\n  if (tid < N) {\n    // compute the quartiles in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    // using the method outlined in the description above\n\n    double temp = x[tid];\n\n    if (temp >= 0 && temp < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (temp >= 0.25 && temp < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (temp >= 0.5 && temp < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (temp >= 0.75 && temp < 1) {\n      atomicAdd(&bins[3], 1);\n    }\n\n    thread_cnt++;\n    thread_in_grid++;\n  }\n\n  __syncthreads();\n\n  // each thread updates the number of threads in each block\n  if (tid == 0) {\n    atomicAdd(&blocks_per_grid, thread_cnt);\n  }\n\n  __syncthreads();\n\n  // each thread updates the number of threads in each grid\n  if (tid == 0) {\n    atomicAdd(&thread_in_grid, 1);\n    atomicAdd(&thread_in_grid, 1);\n    atomicAdd(&thread_in_grid, 1);\n    atomicAdd(&thread_in_grid, 1);\n  }\n\n  __syncthreads();\n\n  // each thread updates the number of threads in each block and grid\n  if (tid == 0) {\n    atomicAdd(&thread_in_grid, thread_cnt);\n    atomicAdd(&threads_per_block, thread_in_grid);\n  }\n\n  __syncthreads();\n\n  // each thread adds up the number of threads in each block and grid\n  __shared__ int total_threads_in_grid;\n\n  if (tid == 0) {\n    atomicAdd(&total_threads_in_grid, thread_in_grid);\n  }\n\n  __syncthreads();\n\n  // each block updates the number of blocks in the grid\n  if (tid == 0) {\n    atomicAdd(&blocks_per_grid, thread_in_grid);\n  }\n\n  __syncthreads();\n\n  // each block adds up the number of blocks in the grid\n  __shared__ int total_blocks_in_grid;\n\n  if (tid == 0) {\n    atomicAdd(&total_blocks_in_grid, blocks_per_grid);\n  }\n\n  __syncthreads();\n\n  // each grid updates the number of grids in the grid\n  if (tid == 0) {\n    atomicAdd(&blocks_per_grid, thread_in_grid);\n  }\n\n  __syncthreads();\n\n  // each grid adds up the number of grids in the grid\n  __shared__ int total_grids_in_grid;\n\n  if (tid == 0) {\n    atomicAdd(&total_grids_in_grid, blocks_per_grid);\n  }\n\n  __syncthreads();\n\n  // each block updates the number of blocks in the grid\n  if (tid == 0) {\n    atomicAdd(&total_threads_in_grid, thread_cnt);\n  }\n\n  __syncthreads();\n\n  // each block adds up the number of blocks in the grid\n  __shared__ int total_threads_in_block;\n\n  if (tid == 0",
            "const size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n  if (idx < N) {\n    const double val = x[idx];\n    const double t = (val - floor(val)) * 4;\n    if (t < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (t < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (t < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "int i = threadIdx.x + blockDim.x*blockIdx.x;\n  if (i < N) {\n    // 0.25 < x < 0.5\n    if (x[i] >= 0.25 && x[i] < 0.5) bins[1]++;\n    // 0.5 < x < 0.75\n    if (x[i] >= 0.5 && x[i] < 0.75) bins[2]++;\n    // 0.75 < x < 1\n    if (x[i] >= 0.75 && x[i] < 1) bins[3]++;\n    // 0 < x < 0.25\n    if (x[i] >= 0 && x[i] < 0.25) bins[0]++;\n  }\n}",
            "int thread = threadIdx.x;\n    int stride = blockDim.x;\n    int block = blockIdx.x;\n    int start = block*stride*256;\n    int end = min(start + stride*256, N);\n    double quartile = 0.25;\n    int low = 0;\n    int high = end;\n    int mid;\n    int i;\n    __syncthreads();\n    for(i=start+thread; i<end; i+=stride) {\n        mid = low + (high-low)/2;\n        if(x[i] < quartile) {\n            low = mid + 1;\n        } else {\n            high = mid;\n        }\n    }\n    __syncthreads();\n    if(thread == 0) {\n        bins[0] = low;\n        bins[1] = high-low;\n    }\n    __syncthreads();\n    start = block*stride*256;\n    end = min(start + stride*256, N);\n    quartile = 0.5;\n    low = 0;\n    high = end;\n    mid = 0;\n    for(i=start+thread; i<end; i+=stride) {\n        mid = low + (high-low)/2;\n        if(x[i] < quartile) {\n            low = mid + 1;\n        } else {\n            high = mid;\n        }\n    }\n    __syncthreads();\n    if(thread == 0) {\n        bins[2] = low;\n        bins[3] = high-low;\n    }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: Your code goes here.\n  // You may use the helper functions defined above.\n  //\n  // HINT: To avoid divergence, each thread should process at most 1/4th of\n  // the data, so this is a good choice for a block size. You might need to\n  // experiment with this number and also with the number of threads to make\n  // the runtime be reasonable.\n  //\n  // HINT: The first step is to compute the number of blocks needed to run the kernel.\n  // You can use the `ceil(N/threads_per_block)` function to compute this.\n  //\n  // HINT: The next step is to compute the thread's block_id. You can use\n  // the `blockIdx.x` function to get this.\n  //\n  // HINT: The last step is to compute the thread's thread_id. You can use\n  // the `threadIdx.x` function to get this.\n\n}",
            "size_t tid = threadIdx.x;\n  size_t offset = blockIdx.x * N;\n  size_t index = offset + tid;\n  int quartile = 0;\n  if (index < N) {\n    double y = x[index];\n    if (y >= 0.75 * 0.5)\n      quartile = 3;\n    else if (y >= 0.5 * 0.5)\n      quartile = 2;\n    else if (y >= 0.25 * 0.5)\n      quartile = 1;\n    else\n      quartile = 0;\n  }\n  __syncthreads();\n  atomicAdd(&bins[quartile], N - offset);\n}",
            "__shared__ size_t localBins[4];\n    size_t i = threadIdx.x;\n    size_t index = blockIdx.x * blockDim.x + i;\n    localBins[0] = 0;\n    localBins[1] = 0;\n    localBins[2] = 0;\n    localBins[3] = 0;\n    while (index < N) {\n        double value = x[index];\n        if (value < 0.25) {\n            localBins[0] += 1;\n        } else if (value < 0.5) {\n            localBins[1] += 1;\n        } else if (value < 0.75) {\n            localBins[2] += 1;\n        } else {\n            localBins[3] += 1;\n        }\n        index += blockDim.x * gridDim.x;\n    }\n    __syncthreads();\n    atomicAdd(&bins[0], localBins[0]);\n    atomicAdd(&bins[1], localBins[1]);\n    atomicAdd(&bins[2], localBins[2]);\n    atomicAdd(&bins[3], localBins[3]);\n}",
            "double y = 0.0;\n  __syncthreads();\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (idx >= N) {\n    return;\n  }\n\n  y = x[idx];\n  if (y < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (y >= 0.25 && y < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (y >= 0.5 && y < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (y >= 0.75 && y < 1.0) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "const int N_PER_BLOCK = 512;\n  const int block_id = blockIdx.x;\n  const int thread_id = threadIdx.x;\n  const int block_start = N_PER_BLOCK * block_id;\n  const int block_end = min(N, block_start + N_PER_BLOCK);\n  int block_sum = 0;\n  int thread_sum = 0;\n\n  // Compute sum of x[i] for all elements in this block\n  for (int i = block_start + thread_id; i < block_end; i += N_PER_BLOCK) {\n    if (x[i] > 0.75 * i) {\n      thread_sum++;\n    }\n    if (x[i] > 0.5 * i) {\n      block_sum++;\n    }\n    if (x[i] > 0.25 * i) {\n      block_sum++;\n    }\n  }\n\n  // Synchronize thread and block sums\n  __syncthreads();\n\n  // Update global sum\n  atomicAdd(&bins[0], thread_sum);\n  atomicAdd(&bins[1], block_sum);\n\n  // Compute sum of x[i] for all elements in this block\n  for (int i = block_start + thread_id; i < block_end; i += N_PER_BLOCK) {\n    if (x[i] <= 0.25 * i) {\n      thread_sum++;\n    }\n    if (x[i] <= 0.5 * i) {\n      block_sum++;\n    }\n    if (x[i] <= 0.75 * i) {\n      block_sum++;\n    }\n  }\n\n  // Synchronize thread and block sums\n  __syncthreads();\n\n  // Update global sum\n  atomicAdd(&bins[2], thread_sum);\n  atomicAdd(&bins[3], block_sum);\n}",
            "// TODO: Implement this function\n    int idx = threadIdx.x + blockIdx.x * blockDim.x;\n    int stride = blockDim.x * gridDim.x;\n    while (idx < N) {\n        double f = x[idx];\n        if (f >= 0 && f < 0.25) {\n            atomicAdd(&bins[0], 1);\n        }\n        if (f >= 0.25 && f < 0.5) {\n            atomicAdd(&bins[1], 1);\n        }\n        if (f >= 0.5 && f < 0.75) {\n            atomicAdd(&bins[2], 1);\n        }\n        if (f >= 0.75 && f < 1) {\n            atomicAdd(&bins[3], 1);\n        }\n        idx += stride;\n    }\n}",
            "// YOUR CODE HERE\n}",
            "int i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i < N) {\n      double val = x[i];\n      // 25% are in [0, 0.25)\n      if (val < 0.25)\n         atomicAdd(&bins[0], 1);\n      else if (val < 0.50)\n         atomicAdd(&bins[1], 1);\n      else if (val < 0.75)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "__shared__ double x_shared[THREADS_PER_BLOCK];\n  size_t tid = threadIdx.x;\n  size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  double d = 0.25 * N;\n  size_t i = 0;\n\n  // load into shared memory\n  while (gid < N) {\n    x_shared[tid] = x[gid];\n    gid += blockDim.x * gridDim.x;\n  }\n\n  __syncthreads();\n\n  // Compute sum for each block.\n  for (int j = 0; j < THREADS_PER_BLOCK; ++j) {\n    d -= x_shared[j];\n  }\n\n  if (d < 0) {\n    d += 0.25 * N;\n    i = 1;\n  }\n\n  __syncthreads();\n\n  // load into shared memory\n  while (gid < N) {\n    x_shared[tid] = x[gid];\n    gid += blockDim.x * gridDim.x;\n  }\n\n  __syncthreads();\n\n  // Compute sum for each block.\n  for (int j = 0; j < THREADS_PER_BLOCK; ++j) {\n    d -= x_shared[j];\n  }\n\n  if (d < 0) {\n    d += 0.25 * N;\n    i += 2;\n  }\n\n  __syncthreads();\n\n  // load into shared memory\n  while (gid < N) {\n    x_shared[tid] = x[gid];\n    gid += blockDim.x * gridDim.x;\n  }\n\n  __syncthreads();\n\n  // Compute sum for each block.\n  for (int j = 0; j < THREADS_PER_BLOCK; ++j) {\n    d -= x_shared[j];\n  }\n\n  if (d < 0) {\n    d += 0.25 * N;\n    i += 3;\n  }\n\n  __syncthreads();\n\n  // load into shared memory\n  while (gid < N) {\n    x_shared[tid] = x[gid];\n    gid += blockDim.x * gridDim.x;\n  }\n\n  __syncthreads();\n\n  // Compute sum for each block.\n  for (int j = 0; j < THREADS_PER_BLOCK; ++j) {\n    d -= x_shared[j];\n  }\n\n  if (d < 0) {\n    d += 0.25 * N;\n    i += 4;\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    atomicAdd(&bins[i], 1);\n  }\n}",
            "// YOUR CODE HERE\n  // This is an example implementation.\n  // You should use atomicAdd() to increment the appropriate bin.\n  // Don't forget to use the appropriate bin.\n  // You should call the kernel with at least N threads.\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index < N) {\n    int binIndex = floor(x[index] * 4);\n    atomicAdd(&bins[binIndex], 1);\n  }\n}",
            "size_t bin = threadIdx.x + 1;\n  size_t stride = blockDim.x;\n\n  for (size_t i = blockIdx.x * stride + threadIdx.x; i < N; i += stride * gridDim.x) {\n    double xi = x[i];\n\n    if (xi < 0.25) {\n      bins[bin] += 1;\n    } else if (xi < 0.5) {\n      bins[bin] += 2;\n    } else if (xi < 0.75) {\n      bins[bin] += 3;\n    } else {\n      bins[bin] += 4;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Fill in code here\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadId < N) {\n    double x_thread = x[threadId];\n    if (x_thread >= 0 && x_thread < 0.25) {\n      atomicAdd(&bins[0], 1);\n    } else if (x_thread >= 0.25 && x_thread < 0.5) {\n      atomicAdd(&bins[1], 1);\n    } else if (x_thread >= 0.5 && x_thread < 0.75) {\n      atomicAdd(&bins[2], 1);\n    } else if (x_thread >= 0.75 && x_thread <= 1) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "// Fill in your code here.\n}",
            "// TODO\n}",
            "// TODO: fill this in.\n\t// you may want to use a shared memory array to count and store the results.\n\t__shared__ double shared_bins[4];\n\tshared_bins[0] = 0;\n\tshared_bins[1] = 0;\n\tshared_bins[2] = 0;\n\tshared_bins[3] = 0;\n\n\tdouble lower = 0;\n\tdouble upper = 0;\n\n\tint index = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (index < N) {\n\t\tdouble val = x[index];\n\t\tupper = val;\n\t\tif (val >= 0 && val <= 0.25) {\n\t\t\tatomicAdd(&shared_bins[0], 1);\n\t\t}\n\t\tif (val >= 0.25 && val <= 0.5) {\n\t\t\tatomicAdd(&shared_bins[1], 1);\n\t\t}\n\t\tif (val >= 0.5 && val <= 0.75) {\n\t\t\tatomicAdd(&shared_bins[2], 1);\n\t\t}\n\t\tif (val >= 0.75 && val <= 1) {\n\t\t\tatomicAdd(&shared_bins[3], 1);\n\t\t}\n\t}\n\n\t__syncthreads();\n\n\tif (threadIdx.x == 0) {\n\t\tbins[0] = shared_bins[0];\n\t\tbins[1] = shared_bins[1];\n\t\tbins[2] = shared_bins[2];\n\t\tbins[3] = shared_bins[3];\n\t}\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n}",
            "}",
            "int tid = threadIdx.x;\n\t__shared__ size_t localBins[4];\n\n\t// Each thread will process 1/8 of the data\n\tint i = blockIdx.x * (N / 8) + threadIdx.x;\n\tint numThreads = blockDim.x;\n\n\t// Each thread will process a quarter of the values\n\tdouble value;\n\tsize_t i_quartile;\n\tfor (; i < N; i += numThreads) {\n\t\tvalue = x[i];\n\t\tif (value >= 0.25) {\n\t\t\ti_quartile = 3;\n\t\t} else if (value >= 0.5) {\n\t\t\ti_quartile = 2;\n\t\t} else if (value >= 0.75) {\n\t\t\ti_quartile = 1;\n\t\t} else {\n\t\t\ti_quartile = 0;\n\t\t}\n\t\tatomicAdd(&localBins[i_quartile], 1);\n\t}\n\n\t__syncthreads();\n\n\tif (tid == 0) {\n\t\tfor (int j = 0; j < 4; ++j) {\n\t\t\tbins[j] = localBins[j];\n\t\t}\n\t}\n}",
            "__shared__ double xShared[THREADS_PER_BLOCK];\n\n  size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t i = tid * 4;\n  double q = 0.25;\n\n  // load elements of x into shared memory\n  if (i < N) {\n    xShared[threadIdx.x] = x[i];\n  }\n  __syncthreads();\n\n  // store the number of elements in each quartile\n  if (i < N) {\n    for (int j = 0; j < 4; j++) {\n      if (xShared[threadIdx.x] >= q * (j + 1) && xShared[threadIdx.x] < q * (j + 2)) {\n        atomicAdd(&(bins[j]), 1);\n        break;\n      }\n    }\n  }\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadId >= N) return;\n\n  double a = x[threadId];\n\n  if (a < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (a < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (a < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int tid = threadIdx.x + blockDim.x * blockIdx.x;\n  if (tid < N) {\n    double x_tid = x[tid];\n    int bin = (x_tid > 0.25)? 1 : ((x_tid > 0.5)? 2 : ((x_tid > 0.75)? 3 : 0));\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// TODO: Implement this function\n    // Hints: \n    // 1. Consider using the CUDA atomic operations to count in parallel\n    // 2. Use shared memory to implement parallel reduction\n    // 3. Use 128 threads per block\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Compute the number of elements in each quartile\n}",
            "int tid = threadIdx.x;\n\n    // TODO\n}",
            "int threadIdx = threadIdx.x;\n  int blockIdx = blockIdx.x;\n  int gridDim = gridDim.x;\n\n  // Each thread is assigned a contiguous block of elements to work on\n  int block_size = (N + blockDim.x * gridDim.x - 1) / (blockDim.x * gridDim.x);\n  int start = block_size * blockIdx;\n  int end = min(start + block_size, N);\n\n  __shared__ double x_shared[BLOCK_SIZE];\n\n  // Load data to shared memory\n  for (int i = threadIdx; i < end - start; i += BLOCK_SIZE) {\n    x_shared[i] = x[start + i];\n  }\n  __syncthreads();\n\n  // Count the elements in each quartile\n  for (int i = threadIdx; i < end - start; i += BLOCK_SIZE) {\n    if ((x_shared[i] >= 0) && (x_shared[i] <= 0.25)) {\n      atomicAdd(&bins[0], 1);\n    } else if ((x_shared[i] > 0.25) && (x_shared[i] <= 0.5)) {\n      atomicAdd(&bins[1], 1);\n    } else if ((x_shared[i] > 0.5) && (x_shared[i] <= 0.75)) {\n      atomicAdd(&bins[2], 1);\n    } else {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n  __syncthreads();\n}",
            "unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i < N) {\n        double y = x[i];\n        if (y < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (y < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (y < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "/* YOUR CODE HERE */\n}",
            "// YOUR CODE HERE\n  // Launch a kernel to compute `bins`.\n}",
            "// TODO\n}",
            "// TODO: Your code here.\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n    // You may not need the `int *bins` array, you can just pass in a pointer to an int array of length 4.\n    // Make sure to initialize all four elements to zero before starting the kernel.\n    // The last block may need to have fewer threads.\n}",
            "// Initialize the counts to 0\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  // Write your code here\n}",
            "__shared__ double smem[1024];\n\n    // TODO: Your code here\n    int idx = threadIdx.x;\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (i >= N) return;\n\n    double xi = x[i];\n    int q = (xi >= 0 && xi < 0.25)? 0 : (xi >= 0.25 && xi < 0.5)? 1 : (xi >= 0.5 && xi < 0.75)? 2 : 3;\n    atomicAdd(&(bins[q]), 1);\n}",
            "// TODO\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    __shared__ double smem[MAX_THREADS_PER_BLOCK];\n\n    // TODO\n}",
            "// TODO: Fill in the implementation.\n  // Hint: You can use atomicAdd() to increment the elements of `bins`.\n  // Hint: You may need to round down the index of x.\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (idx < N) {\n        double value = x[idx];\n\n        // YOUR CODE HERE\n        // This is a template to get you started.\n        // Fill in the body of the kernel to compute the bins.\n        //\n        // You will need a new CUDA kernel for each of the two cases\n        // (see the if statements below).\n        //\n        // You can call your new kernel from the host in the following\n        // way, using `double*` as the type for the first argument:\n        //\n        // cudaError_t error = cudaSuccess;\n        // error = cudaLaunchKernel(\n        //     countQuartiles<double>,\n        //     dim3(ceil(N / 32)),\n        //     dim3(32),\n        //     NULL,\n        //     0,\n        //     (double*)x,\n        //     N,\n        //     bins);\n        // assert(error == cudaSuccess);\n\n        if (value >= 0 && value < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (value >= 0.25 && value < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (value >= 0.5 && value < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (value >= 0.75 && value <= 1) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// Fill in the kernel here.\n  // Hint: the threadIdx.x and threadIdx.y variables give the thread coordinates \n  // in the grid.\n}",
            "const int N_PER_BLOCK = 16;\n    __shared__ double x_sh[N_PER_BLOCK];\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n    int chunkSize = N / gridDim.x;\n    int chunkOffset = chunkSize * bid;\n    int N_in_chunk = (bid == gridDim.x - 1)? N - chunkOffset : chunkSize;\n\n    for (int i = tid; i < N_in_chunk; i += N_PER_BLOCK) {\n        x_sh[i] = x[chunkOffset + i];\n    }\n    __syncthreads();\n    for (int i = tid; i < N_in_chunk; i += N_PER_BLOCK) {\n        int idx = i * 4;\n        double v = x_sh[i];\n        if (v >= 0.0 && v < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (v >= 0.25 && v < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (v >= 0.5 && v < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (v >= 0.75 && v < 1.0)\n            atomicAdd(&bins[3], 1);\n    }\n    __syncthreads();\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (id < N) {\n\t\tdouble temp = x[id] - floor(x[id]);\n\t\tif (temp < 0.25) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t} else if (temp < 0.5) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t} else if (temp < 0.75) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t} else {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n}",
            "// TODO: Fill in this function\n  int n = threadIdx.x;\n  int total = 0;\n  if (n < N) {\n    double num = x[n];\n    if (num > 0 && num < 0.25) {\n      total++;\n    } else if (num > 0.25 && num < 0.5) {\n      total++;\n    } else if (num > 0.5 && num < 0.75) {\n      total++;\n    } else if (num > 0.75 && num <= 1) {\n      total++;\n    }\n  }\n  __syncthreads();\n  if (n < 4) {\n    atomicAdd(&bins[n], total);\n  }\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n}",
            "__shared__ double x_shared[1024];\n  __shared__ size_t bins_shared[4];\n\n  double my_x = 0.0;\n  size_t my_bins[4] = {0, 0, 0, 0};\n\n  int tid = threadIdx.x;\n  int block_start = blockIdx.x * blockDim.x;\n  int block_end = min(block_start + blockDim.x, N);\n  int thread_start = block_start + tid;\n\n  for (int i = thread_start; i < block_end; i += blockDim.x) {\n    my_x += x[i];\n  }\n\n  x_shared[tid] = my_x;\n\n  __syncthreads();\n\n  for (int i = 0; i < blockDim.x; i += 256) {\n    my_x += x_shared[i + tid];\n  }\n\n  if (tid < 128) {\n    my_x += x_shared[128 + tid];\n  }\n  if (tid < 64) {\n    my_x += x_shared[192 + tid];\n  }\n  if (tid < 32) {\n    my_x += x_shared[256 + tid];\n  }\n  if (tid < 16) {\n    my_x += x_shared[320 + tid];\n  }\n  if (tid < 8) {\n    my_x += x_shared[384 + tid];\n  }\n  if (tid < 4) {\n    my_x += x_shared[448 + tid];\n  }\n  if (tid < 2) {\n    my_x += x_shared[512 + tid];\n  }\n  if (tid < 1) {\n    my_x += x_shared[576 + tid];\n  }\n\n  __syncthreads();\n\n  for (int i = thread_start; i < block_end; i += blockDim.x) {\n    double x_i = x[i];\n    if (x_i < my_x * 0.25) {\n      my_bins[0]++;\n    }\n    else if (x_i < my_x * 0.5) {\n      my_bins[1]++;\n    }\n    else if (x_i < my_x * 0.75) {\n      my_bins[2]++;\n    }\n    else {\n      my_bins[3]++;\n    }\n  }\n\n  bins_shared[tid] = my_bins[0];\n  bins_shared[tid + 128] = my_bins[1];\n  bins_shared[tid + 256] = my_bins[2];\n  bins_shared[tid + 384] = my_bins[3];\n\n  __syncthreads();\n\n  for (int i = 0; i < 1024; i += 256) {\n    bins_shared[tid] += bins_shared[tid + 256];\n    __syncthreads();\n  }\n\n  if (tid < 32) {\n    bins_shared[tid] += bins_shared[tid + 32];\n  }\n  if (tid < 16) {\n    bins_shared[tid] += bins_shared[tid + 16];\n  }\n  if (tid < 8) {\n    bins_shared[tid] += bins_shared[tid + 8];\n  }\n  if (tid < 4) {\n    bins_shared[tid] += bins_shared[tid + 4];\n  }\n  if (tid < 2) {\n    bins_shared[tid] += bins_shared[tid + 2];\n  }\n  if (tid < 1) {\n    bins_shared[tid] += bins_shared[tid + 1];\n  }\n\n  __syncthreads();\n\n  if (tid == 0) {\n    bins[0] = bins_shared[0];\n    bins[1] = bins_shared[128];\n    bins[2] = bins_shared[256];\n    bins[3] = bins_shared[384];\n  }\n}",
            "const int thread_id = threadIdx.x;\n    const int block_id = blockIdx.x;\n    const int block_width = blockDim.x;\n    const int block_stride = block_width * gridDim.x;\n\n    __shared__ int count[4];\n\n    int block_start = block_stride * block_id;\n    int block_end = min(block_start + block_stride, N);\n\n    // Initialize the count to 0\n    if (thread_id < 4) {\n        count[thread_id] = 0;\n    }\n\n    // Loop over the elements in the block to compute the number of elements in each bin\n    for (int i = thread_id; i < block_end; i += block_width) {\n        // Determine the bin of element i\n        double value = x[i + block_start];\n        int bin = value >= 0 && value < 0.25? 0 : value >= 0.25 && value < 0.5? 1 : value >= 0.5 && value < 0.75? 2 : 3;\n\n        // Increment the count in the bin\n        if (bin!= -1) {\n            atomicAdd(&count[bin], 1);\n        }\n    }\n\n    // Block-wide barrier\n    __syncthreads();\n\n    // Compute the total number of elements in each bin\n    if (thread_id < 4) {\n        atomicAdd(&bins[thread_id], count[thread_id]);\n    }\n}",
            "// TODO\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    double val = x[tid];\n    if (val >= 0 && val < 0.25) bins[0] += 1;\n    else if (val >= 0.25 && val < 0.5) bins[1] += 1;\n    else if (val >= 0.5 && val < 0.75) bins[2] += 1;\n    else if (val >= 0.75 && val < 1) bins[3] += 1;\n  }\n}",
            "// TODO: implement this\n    // this kernel will be launched with at least N threads\n    size_t i = threadIdx.x;\n    if(i < N) {\n        if(x[i] >= 0 && x[i] < 0.25) {\n            atomicAdd(&bins[0], 1);\n        }\n        else if(x[i] >= 0.25 && x[i] < 0.5) {\n            atomicAdd(&bins[1], 1);\n        }\n        else if(x[i] >= 0.5 && x[i] < 0.75) {\n            atomicAdd(&bins[2], 1);\n        }\n        else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "}",
            "// TODO: Write your code here\n  int tid = blockDim.x * blockIdx.x + threadIdx.x;\n  // for(int i = 0; i < N; i++){\n  //   if (x[i] >= 0.25*tid && x[i] < 0.25*(tid+1)) {\n  //     printf(\"x[%d] >= 0.25*tid && x[%d] < 0.25*(tid+1) \", i, i);\n  //   }\n  // }\n  if(tid < 4) {\n    for(int i = 0; i < N; i++){\n      if (x[i] >= (double)(tid)/4*(tid+1)/4*0.25 && x[i] < (double)(tid+1)/4*(tid+1)/4*0.25) {\n        // printf(\"x[%d] >= %lf && x[%d] < %lf \", i, (double)(tid)/4*(tid+1)/4*0.25, i, (double)(tid+1)/4*(tid+1)/4*0.25);\n        atomicAdd(&bins[tid], 1);\n      }\n    }\n  }\n  // atomicAdd(&bins[tid], 1);\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "int i = threadIdx.x + blockDim.x * blockIdx.x;\n    if (i < N) {\n        double y = x[i];\n        if (y >= 0 && y < 0.25)\n            atomicAdd(&bins[0], 1);\n        else if (y >= 0.25 && y < 0.5)\n            atomicAdd(&bins[1], 1);\n        else if (y >= 0.5 && y < 0.75)\n            atomicAdd(&bins[2], 1);\n        else if (y >= 0.75)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// TODO: Fill in code.\n    const int tid = threadIdx.x;\n    __shared__ size_t s_bins[4];\n    s_bins[0] = 0;\n    s_bins[1] = 0;\n    s_bins[2] = 0;\n    s_bins[3] = 0;\n    for (int i = tid; i < N; i+=blockDim.x)\n    {\n        double tmp = x[i];\n        if (tmp < 0.25)\n            s_bins[0]++;\n        else if (tmp < 0.5)\n            s_bins[1]++;\n        else if (tmp < 0.75)\n            s_bins[2]++;\n        else\n            s_bins[3]++;\n    }\n    __syncthreads();\n    for(int i=tid;i<4;i+=blockDim.x)\n        atomicAdd(&bins[i], s_bins[i]);\n}",
            "// TODO: Your implementation goes here\n    // Compute the thread ID (row-major)\n    size_t id = (blockIdx.x * blockDim.x) + threadIdx.x;\n    if(id < N)\n    {\n        // If the value is in the range of a quartile\n        if(x[id] >= 0 && x[id] <= 0.25)\n        {\n            bins[0] += 1;\n        }\n        else if(x[id] > 0.25 && x[id] <= 0.5)\n        {\n            bins[1] += 1;\n        }\n        else if(x[id] > 0.5 && x[id] <= 0.75)\n        {\n            bins[2] += 1;\n        }\n        else if(x[id] > 0.75 && x[id] <= 1)\n        {\n            bins[3] += 1;\n        }\n    }\n}",
            "// YOUR CODE HERE\n  // Use __syncthreads() after each block\n  // Hints:\n  //   - you can use an if statement to choose whether the thread should\n  //     execute the block\n  //   - remember to use the correct index in the global memory array\n  //   - use atomicAdd to count the number of elements in each bin\n}",
            "size_t binId = threadIdx.x;\n  double sum = 0.0;\n  for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n    sum += x[i];\n  }\n  // TODO: Add your code here\n  __syncthreads();\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      double val = x[idx];\n      if (val < 0.25) {\n         bins[0]++;\n      }\n      else if (val < 0.5) {\n         bins[1]++;\n      }\n      else if (val < 0.75) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "const double quarter = 0.25;\n    const double three_quarters = 0.75;\n    const double two_quarters = 0.5;\n    const int tid = threadIdx.x;\n    const int blockSize = blockDim.x;\n    const int gridSize = blockDim.x * gridDim.x;\n    // the grid's index\n    const int blockId = blockIdx.x;\n\n    // this thread's start index\n    const int i = blockId * blockSize + tid;\n    const int step = gridSize * blockSize;\n\n    // the count of elements with index in [0, 0.25)\n    int count = 0;\n    // the count of elements with index in [0.25, 0.5)\n    int count1 = 0;\n    // the count of elements with index in [0.5, 0.75)\n    int count2 = 0;\n    // the count of elements with index in [0.75, 1)\n    int count3 = 0;\n\n    for (int j = i; j < N; j += step) {\n        double element = x[j];\n        // count the number of elements in [0, 0.25)\n        if (element < two_quarters) {\n            count++;\n        }\n        // count the number of elements in [0.25, 0.5)\n        else if (element < three_quarters) {\n            count1++;\n        }\n        // count the number of elements in [0.5, 0.75)\n        else if (element < quarter) {\n            count2++;\n        }\n        // count the number of elements in [0.75, 1)\n        else {\n            count3++;\n        }\n    }\n    // count the number of elements in [0, 0.25)\n    if (tid == 0) {\n        atomicAdd(&bins[0], count);\n    }\n    // count the number of elements in [0.25, 0.5)\n    if (tid == 1) {\n        atomicAdd(&bins[1], count1);\n    }\n    // count the number of elements in [0.5, 0.75)\n    if (tid == 2) {\n        atomicAdd(&bins[2], count2);\n    }\n    // count the number of elements in [0.75, 1)\n    if (tid == 3) {\n        atomicAdd(&bins[3], count3);\n    }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    double4 quartiles = {(0.25 + 0.75)*0.5, 0.25, 0.75, 0.5};\n    if (i < N) {\n        double xi = x[i];\n        for (size_t j = 0; j < 4; ++j) {\n            bins[j] += (int)floor((xi - quartiles[j]) * 4);\n        }\n    }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double x_tid = x[tid];\n        if (x_tid < 0) x_tid = 0;\n        if (x_tid >= 1) x_tid = 0.999999; // round up\n        size_t i = floor(x_tid * 4);\n        atomicAdd(bins + i, 1);\n    }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid >= N) return;\n\n   double tmp = x[tid];\n   int i = __double2int_rd(tmp * 4);\n\n   atomicAdd(&bins[i], 1);\n}",
            "// Your code here\n}",
            "// INSERT YOUR CODE HERE\n}",
            "// TODO: Your code goes here.\n}",
            "__shared__ size_t counts[4]; // Use shared memory to avoid global memory reads\n   if (threadIdx.x == 0) {\n      counts[0] = 0;\n      counts[1] = 0;\n      counts[2] = 0;\n      counts[3] = 0;\n   }\n   __syncthreads();\n   \n   size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t interval = N / blockDim.x;\n   \n   for (size_t j = 0; j < interval; j++) {\n      int idx = threadIdx.x + j * blockDim.x;\n      if (idx < N) {\n         double frac = x[idx] - floor(x[idx]);\n         if (frac >= 0.25 && frac < 0.5) counts[0] += 1;\n         else if (frac >= 0.5 && frac < 0.75) counts[1] += 1;\n         else if (frac >= 0.75 && frac < 1) counts[2] += 1;\n         else if (frac >= 0 && frac < 0.25) counts[3] += 1;\n      }\n   }\n   __syncthreads();\n   \n   // Add the values stored in shared memory to the global memory location\n   // `bins` for this thread.\n   if (threadIdx.x == 0) {\n      for (size_t i = 0; i < 4; i++) {\n         atomicAdd(&bins[i], counts[i]);\n      }\n   }\n}",
            "const size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n    if (i >= N) {\n        return;\n    }\n    if (x[i] < 0.25) {\n        atomicAdd(&bins[0], 1);\n    } else if (x[i] < 0.5) {\n        atomicAdd(&bins[1], 1);\n    } else if (x[i] < 0.75) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "int t = blockIdx.x * blockDim.x + threadIdx.x;\n    if (t < N) {\n        double x_t = x[t];\n        if (x_t >= 0 && x_t < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x_t >= 0.25 && x_t < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x_t >= 0.5 && x_t < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else if (x_t >= 0.75 && x_t < 1) {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: YOUR CODE HERE\n  int tid = threadIdx.x;\n  int stride = blockDim.x;\n  int bid = blockIdx.x;\n\n  // Compute the lower and upper range of data that this block of threads will process.\n  // You can assume that `tid` is less than the number of elements in `x` and that \n  // `N` is a multiple of the number of threads per block.\n  double start = 0.25 * (double)bid;\n  double end = 0.25 * (double)(bid+1);\n  int data_start = (int)(start * N);\n  int data_end = (int)(end * N);\n  if (data_end > N) data_end = N;\n\n  // Set the shared memory to 0\n  __shared__ int s_bins[4];\n  for (int i = 0; i < 4; i++) s_bins[i] = 0;\n\n  // Count the number of doubles in `x` that have a fractional part in the range\n  // [0.25 * bid, 0.25 * (bid + 1)). Each thread counts the number of elements \n  // that have a fractional part in its range of [0, 0.25), [0.25, 0.5), etc.\n  for (int i = data_start + tid; i < data_end; i += stride) {\n    double element = x[i];\n    if (element < 0.25 * (double)bid) s_bins[0] += 1;\n    else if (element < 0.5 * (double)bid) s_bins[1] += 1;\n    else if (element < 0.75 * (double)bid) s_bins[2] += 1;\n    else if (element < 1 * (double)bid) s_bins[3] += 1;\n  }\n\n  // Aggregate the counts. The number of threads per block is at least 32, \n  // so the reduction in each step should be a warp reduction.\n  __syncthreads();\n  for (int stride = 1; stride < 32; stride *= 2) {\n    int index = 2 * stride * tid;\n    if (index < 32) {\n      s_bins[index] += s_bins[index + stride];\n    }\n    __syncthreads();\n  }\n\n  // Store the counts in the global memory\n  if (tid < 32) {\n    bins[tid] = s_bins[2 * tid];\n  }\n}",
            "// TODO\n}",
            "// Each thread computes one bin.\n    // `threadIdx.x` gives the global thread index and ranges from 0 to `blockDim.x * gridDim.x - 1`.\n    size_t idx = threadIdx.x;\n    double sum = 0;\n    size_t n = 0;\n\n    // TODO: Fill in the kernel to compute the count for each bin.\n    //       Hint: Use the following `if` statement to decide which bin a thread should compute.\n\n    //       This is a good time to check if you're on the right track.\n    //       Make sure that the kernel runs correctly.\n    //       You should see a count of `1` in each bin.\n    //       You should see a count of `2` in one of the two bins.\n    //       You should see a count of `3` in one of the two bins.\n    //       You should see a count of `4` in one of the two bins.\n\n    //       Once you're ready, uncomment the following line to launch the kernel.\n    //       The output should still be correct.\n    //       You can watch it run with Nsight Systems.\n    //       You can also use `cuda-memcheck` to help debug.\n\n    //       Note: `threadIdx.x` is a 32-bit integer, so `idx < N` is guaranteed\n    //             to be true for all threads.\n\n    __syncthreads();\n\n    // TODO: Add code to increment the count for the appropriate bin.\n\n    // TODO: Use an atomic operation to increment `bins[bin]` in parallel.\n\n    // TODO: Unlike many other assignments, this one isn't atomic.\n    //       Make sure that the `bins` array is still correct.\n\n    //       If you're not sure why, make sure that you have enough threads\n    //       to compute all four bins.\n    //       You can also run `cuda-memcheck` and watch for errors.\n}",
            "__shared__ size_t smem[THREADS_PER_BLOCK];\n\n  // compute thread id\n  unsigned int tid = threadIdx.x;\n\n  // compute the number of doubles in the vector that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  size_t count_1 = 0;\n  size_t count_2 = 0;\n  size_t count_3 = 0;\n  size_t count_4 = 0;\n  for (size_t i = blockIdx.x*THREADS_PER_BLOCK + tid; i < N; i += gridDim.x*THREADS_PER_BLOCK) {\n    double element = x[i];\n    if (element >= 0.0 && element < 0.25)\n      count_1++;\n    else if (element >= 0.25 && element < 0.5)\n      count_2++;\n    else if (element >= 0.5 && element < 0.75)\n      count_3++;\n    else if (element >= 0.75 && element <= 1.0)\n      count_4++;\n  }\n\n  // block-wide sum of counts in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  smem[tid] = count_1;\n  __syncthreads();\n  for (unsigned int s = THREADS_PER_BLOCK/2; s > 0; s >>= 1) {\n    if (tid < s) {\n      smem[tid] += smem[tid + s];\n    }\n    __syncthreads();\n  }\n\n  // write the block-wide sum of counts to global memory\n  if (tid == 0) {\n    bins[0] = smem[0];\n    bins[1] = smem[1];\n    bins[2] = smem[2];\n    bins[3] = smem[3];\n  }\n}",
            "// TODO: Your code goes here\n  int idx = threadIdx.x;\n  int idy = blockIdx.x;\n\n  double x_id = x[idx + idy * N];\n\n  if (x_id < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (x_id < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (x_id < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// YOUR CODE HERE\n  // printf(\"Hello from thread %d\\n\", threadIdx.x);\n  // printf(\"Hello from block %d\\n\", blockIdx.x);\n  // printf(\"Hello from grid %d\\n\", blockIdx.x + gridDim.x * blockIdx.y);\n  // printf(\"Hello from blockDim %d\\n\", blockDim.x);\n  // printf(\"Hello from threadDim %d\\n\", threadDim.x);\n\n  __shared__ int counter[1024];\n  int index = threadIdx.x;\n  int offset = blockDim.x;\n  int total_threads = blockDim.x * gridDim.x;\n\n  counter[index] = 0;\n\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += offset) {\n    if ((x[i] >= 0.0) && (x[i] < 0.25)) {\n      counter[index] += 1;\n    } else if ((x[i] >= 0.25) && (x[i] < 0.5)) {\n      counter[index] += 1;\n    } else if ((x[i] >= 0.5) && (x[i] < 0.75)) {\n      counter[index] += 1;\n    } else if ((x[i] >= 0.75) && (x[i] < 1.0)) {\n      counter[index] += 1;\n    }\n  }\n  __syncthreads();\n  for (int i = offset / 2; i > 0; i >>= 1) {\n    if (index < i)\n      counter[index] += counter[index + i];\n    __syncthreads();\n  }\n\n  if (threadIdx.x == 0) {\n    // printf(\"total_threads %d\\n\", total_threads);\n    // printf(\"total_blocks %d\\n\", gridDim.x * gridDim.y);\n    // printf(\"counter[0] %d\\n\", counter[0]);\n    // printf(\"counter[1] %d\\n\", counter[1]);\n    // printf(\"counter[2] %d\\n\", counter[2]);\n    // printf(\"counter[3] %d\\n\", counter[3]);\n\n    bins[0] = counter[0];\n    bins[1] = counter[1];\n    bins[2] = counter[2];\n    bins[3] = counter[3];\n  }\n}",
            "// Each thread computes 2 doubles, so we have to use an interleaved counter\n  unsigned int binIndex = threadIdx.x & 1;\n  unsigned int threadID = (threadIdx.x >> 1) * 2;\n  size_t localBins[2] = {0, 0};\n\n  // Loop over the vector in strips of 2 doubles, using a stride of 4\n  for (unsigned int i = threadID; i < N; i += 4) {\n    double xi = x[i];\n    double yi = x[i + 1];\n\n    // Compute the bin index for this double\n    if (xi < 0.25) {\n      if (yi < 0.25)\n        ++localBins[binIndex];\n      else\n        ++localBins[binIndex ^ 1];\n    }\n    else if (xi < 0.5) {\n      if (yi < 0.5)\n        ++localBins[binIndex];\n      else\n        ++localBins[binIndex ^ 1];\n    }\n    else if (xi < 0.75) {\n      if (yi < 0.75)\n        ++localBins[binIndex];\n      else\n        ++localBins[binIndex ^ 1];\n    }\n    else {\n      if (yi < 1.0)\n        ++localBins[binIndex];\n      else\n        ++localBins[binIndex ^ 1];\n    }\n  }\n\n  // Each thread stores 2 counts, so we have to use an atomic add\n  atomicAdd(&bins[threadIdx.x], localBins[binIndex]);\n  atomicAdd(&bins[threadIdx.x + 1], localBins[binIndex ^ 1]);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int bin = (int)(x[tid] / 0.25);\n    if (bin < 0) bin = 0;\n    if (bin >= 4) bin = 3;\n    atomicAdd(&bins[bin], 1);\n  }\n}",
            "// YOUR CODE HERE\n  // Tip: for the assignment, we expect you to launch at least 128 threads\n  // for each of the 4 bins.\n\n  // Each block has access to a subset of the array.\n  size_t blockStart = blockIdx.x * blockDim.x;\n  size_t stride = gridDim.x * blockDim.x;\n\n  // Each thread within a block computes a bin.\n  // Compute the bin for the thread based on the index of the thread within the block\n  // and the index of the block.\n  int bin = (threadIdx.x + blockStart) / (stride / 4);\n  // Each thread computes a count for the bin.\n  int count = 0;\n  // If the thread is part of the array, increment the count for the bin.\n  if (blockStart + threadIdx.x < N) {\n    if (x[blockStart + threadIdx.x] <= 0.25) {\n      count++;\n    } else if (x[blockStart + threadIdx.x] <= 0.5) {\n      count++;\n    } else if (x[blockStart + threadIdx.x] <= 0.75) {\n      count++;\n    } else {\n      count++;\n    }\n  }\n  // We need to reduce the count for each block before writing it to global memory.\n  __shared__ int sharedCount;\n  sharedCount = count;\n  __syncthreads();\n  if (threadIdx.x == 0) {\n    bins[bin] = sharedCount;\n  }\n}",
            "// YOUR CODE HERE\n\t// DO NOT MODIFY THIS FUNCTION\n}",
            "int i = threadIdx.x;\n    __shared__ int numBlocks;\n    __shared__ double sum;\n    double *shared = &bins[i];\n    __syncthreads();\n    if (i == 0)\n        numBlocks = 1;\n    __syncthreads();\n    if (i < 4) {\n        sum = 0;\n        for (int j = i * numBlocks; j < N; j += 4 * numBlocks)\n            sum += x[j] < 0.25? 1 : (x[j] >= 0.25 && x[j] < 0.5? 1 : (x[j] >= 0.5 && x[j] < 0.75? 1 : (x[j] >= 0.75? 1 : 0)));\n        shared[0] = sum;\n    }\n    __syncthreads();\n    if (i == 0) {\n        bins[0] = shared[0] + shared[1];\n        bins[1] = shared[2] + shared[3];\n        bins[2] = shared[4] + shared[5];\n        bins[3] = shared[6] + shared[7];\n    }\n}",
            "// TODO: Fill in the code\n}",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n  __shared__ size_t bins_local[4];\n  int tid = blockIdx.x * blockDim.x + threadIdx.x;\n  int gridDim = blockDim.x * gridDim.x;\n  int start = 4 * blockIdx.x;\n\n  for (int i = start + threadIdx.x; i < N; i += gridDim) {\n    if (x[i] <= 0.25)\n      atomicAdd(&bins_local[0], 1);\n    else if (x[i] <= 0.5)\n      atomicAdd(&bins_local[1], 1);\n    else if (x[i] <= 0.75)\n      atomicAdd(&bins_local[2], 1);\n    else\n      atomicAdd(&bins_local[3], 1);\n  }\n  __syncthreads();\n\n  if (threadIdx.x == 0) {\n    for (int i = 0; i < 4; ++i) {\n      atomicAdd(&bins[i], bins_local[i]);\n    }\n  }\n}",
            "// TODO: Fill in this function\n    size_t tid = threadIdx.x;\n    size_t i = blockIdx.x * blockDim.x + tid;\n    if (i < N) {\n        if (x[i] <= 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x[i] <= 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x[i] <= 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// Your code goes here.\n\n  // The code is equivalent to:\n  // for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n  //   const double x_i = x[i];\n  //   const double q = x_i - floor(x_i);\n  //   if (q < 0.25) {\n  //     ++bins[0];\n  //   } else if (q < 0.5) {\n  //     ++bins[1];\n  //   } else if (q < 0.75) {\n  //     ++bins[2];\n  //   } else {\n  //     ++bins[3];\n  //   }\n  // }\n}",
            "// TODO: Fill in the kernel function to compute quartiles of the vector x.\n\n}",
            "// Each thread computes the fractional part of a single element of x,\n    // and updates the corresponding bin\n    size_t tid = threadIdx.x;\n    size_t i = blockDim.x * blockIdx.x + tid;\n    if (i < N) {\n        double y = x[i] - floor(x[i]);\n        if (y < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (y < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (y < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// FIXME: replace 0.25, 0.5, 0.75 with threadIdx.x\n  // FIXME: replace 1 with threadIdx.x\n\n  const double fifth = N * 0.25;\n  const double half = N * 0.5;\n  const double third = N * 0.75;\n  size_t x_idx = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (x_idx < N) {\n    if (x[x_idx] >= fifth && x[x_idx] < half) {\n      bins[0] += 1;\n    } else if (x[x_idx] >= half && x[x_idx] < third) {\n      bins[1] += 1;\n    } else if (x[x_idx] >= third && x[x_idx] < (third + fifth)) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "// TODO: Your code goes here\n}",
            "size_t tid = threadIdx.x;\n   size_t gid = blockIdx.x * blockDim.x + tid;\n   size_t numQuartiles = 4;\n   if (gid < N) {\n      size_t n = 0;\n      if (x[gid] >= 0.0 && x[gid] < 0.25) {\n         n = 0;\n      } else if (x[gid] >= 0.25 && x[gid] < 0.5) {\n         n = 1;\n      } else if (x[gid] >= 0.5 && x[gid] < 0.75) {\n         n = 2;\n      } else if (x[gid] >= 0.75 && x[gid] <= 1.0) {\n         n = 3;\n      } else {\n         n = 0;\n      }\n      atomicAdd(&bins[n], 1);\n   }\n}",
            "// TODO: Your code goes here!\n  // Note: CUDA Kernels should have no output parameters\n  // Hint: To compute quartiles, you will need to find the indices of the\n  // elements in `x` that correspond to each of the four quartiles.\n}",
            "size_t tid = threadIdx.x;\n    size_t id = blockIdx.x;\n\n    if (id < N) {\n        double temp = x[id] + tid; // Make each thread handle a different value\n\n        size_t bin = (temp > 0.75)? 3 : (temp > 0.5)? 2 : (temp > 0.25)? 1 : 0;\n\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "double xval;\n  double xint;\n  size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    xval = x[tid];\n\n    xint = floor(xval);\n\n    if (xint < 0.75 * N) {\n      if (xint < 0.5 * N) {\n        if (xint < 0.25 * N)\n          bins[0]++;\n        else\n          bins[1]++;\n      } else {\n        if (xint < 0.75 * N)\n          bins[2]++;\n        else\n          bins[3]++;\n      }\n    }\n  }\n}",
            "// Add your code here\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  // Count the number of doubles in the vector x that have a fractional part\n  // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n  // Use CUDA to compute in parallel. The kernel is launched with at least N threads.\n  // Hint: Use a `double` or `float` variable to store the fractional part of each element.\n\n  // Example:\n  // input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n  // output: [2, 1, 2, 2]\n\n  // input: [1.9, 0.2, 0.6, 10.1, 7.4]\n  // output: [2, 1, 1, 1]\n}",
            "int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  int stride = blockDim.x*gridDim.x;\n  \n  for (int i = idx; i < N; i+=stride) {\n    double x_i = x[i];\n    if (x_i < 0.5) {\n      if (x_i < 0.25) bins[0] += 1;\n      else if (x_i < 0.5) bins[1] += 1;\n      else if (x_i < 0.75) bins[2] += 1;\n      else bins[3] += 1;\n    }\n  }\n}",
            "int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  __shared__ double smem[THREADS_PER_BLOCK];\n  \n  double quartiles[4];\n  quartiles[0] = 0.25;\n  quartiles[1] = 0.5;\n  quartiles[2] = 0.75;\n  quartiles[3] = 1;\n\n  // compute the bin counts\n  int doubles_per_thread = N / THREADS_PER_BLOCK;\n  int num_doubles = doubles_per_thread * THREADS_PER_BLOCK;\n  int num_threads = num_doubles / doubles_per_thread;\n\n  // the first thread in the block is responsible for handling the \n  // first double, and so on\n  if (tid == 0) {\n    smem[bid] = x[bid];\n  }\n  __syncthreads();\n\n  // compute the bin counts, one double per thread\n  int bin;\n  for (int i = 0; i < num_threads; i++) {\n    if (tid < doubles_per_thread) {\n      bin = (int)(10 * (x[bid + doubles_per_thread * tid] - smem[bid]) / 0.25);\n    }\n    __syncthreads();\n    if (tid == 0) {\n      atomicAdd(&bins[bin], 1);\n    }\n  }\n}",
            "// Fill in your code here\n}",
            "// your code goes here\n}",
            "// TODO: Your code here.\n}",
            "int tid = threadIdx.x;\n\n  // 1. YOUR CODE GOES HERE\n}",
            "// TODO: YOUR CODE HERE\n}",
            "// TODO: Your code goes here\n}",
            "int tid = threadIdx.x;\n  // TODO: Implement the CUDA kernel.\n}",
            "// TODO: Your code here\n}",
            "double xval = x[blockIdx.x];\n  size_t block = blockIdx.x % 4;\n\n  __syncthreads();\n\n  // Your code goes here\n}",
            "// TODO: implement this function\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n    size_t id = threadId * 4;\n    if (id < N) {\n        double v = x[id];\n        if (v < 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (v < 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (v < 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "}",
            "/* Each thread computes a single bin.\n       Each thread must compute the corresponding bin, but threads should not\n       compute the same bin. */\n\n    /* You need to fill in your code here */\n    // TODO: Finish this function\n    int start = blockDim.x * blockIdx.x;\n    int stride = blockDim.x * gridDim.x;\n    double sum = 0;\n    int index = threadIdx.x + start;\n\n    for (; index < N; index += stride) {\n        sum += x[index];\n        if (sum >= 0.0 && sum < 0.25) {\n            bins[0] += 1;\n        }\n        else if (sum >= 0.25 && sum < 0.5) {\n            bins[1] += 1;\n        }\n        else if (sum >= 0.5 && sum < 0.75) {\n            bins[2] += 1;\n        }\n        else if (sum >= 0.75 && sum <= 1) {\n            bins[3] += 1;\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "double lower = 0.0, upper = 1.0;\n    size_t tid = threadIdx.x;\n    size_t bin = 0;\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        if (x[i] >= lower && x[i] < upper) {\n            bin += 1;\n        } else if (x[i] >= upper) {\n            bin += 2;\n        }\n        upper = upper + 0.25;\n    }\n    __syncthreads();\n    atomicAdd(&bins[bin], 1);\n}",
            "// TODO: implement this function\n  if (x == nullptr) return;\n  if (N == 0) return;\n}",
            "// TODO: Fill in the body of this function\n  __syncthreads();\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    double d = x[tid];\n    if (d < 0.25 * N)\n      ++bins[0];\n    else if (d < 0.5 * N)\n      ++bins[1];\n    else if (d < 0.75 * N)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "// your code here\n}",
            "/* YOUR CODE HERE */\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        double val = x[i];\n        int a = val < 0.25? 0 : val < 0.50? 1 : val < 0.75? 2 : 3;\n        atomicAdd(&bins[a], 1);\n    }\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\treturn;\n}",
            "// TODO\n}",
            "size_t index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  if (index < N) {\n    double a = x[index] / 4;\n\n    if (a < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (a < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (a < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "// TODO: Complete this function\n\n  // Hints:\n  // - x is of length N\n  // - threadIdx.x is the index of the thread in the thread block\n  // - N is the total number of elements in x\n  // - bins is of length 4\n  // - The number of threads in the thread block is given by gridDim.x * blockDim.x.\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n  if (threadId < N) {\n    double fraction = x[threadId] - floor(x[threadId]);\n    if (fraction < 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (fraction < 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (fraction < 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "double a = 0.25, b = 0.5;\n  size_t a_cnt = 0, b_cnt = 0, c_cnt = 0, d_cnt = 0;\n\n  // TODO: implement\n\n  // for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n  //   double cur_x = x[i];\n\n  //   double a_lower = (cur_x >= 0)? 0 : 0.25;\n  //   double a_upper = (cur_x <= 1)? 1 : 0.75;\n  //   double b_lower = a_upper;\n  //   double b_upper = 1;\n\n  //   if (cur_x >= a_lower && cur_x < a_upper) a_cnt += 1;\n  //   if (cur_x >= b_lower && cur_x < b_upper) b_cnt += 1;\n  // }\n\n  // bins[0] = a_cnt;\n  // bins[1] = b_cnt;\n  // bins[2] = c_cnt;\n  // bins[3] = d_cnt;\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  double x_i = x[i];\n  __shared__ size_t s_bins[4];\n  if (i < N) {\n    double d = x_i * 4.0 - floor(x_i * 4.0);\n    if (d < 0.25)\n      s_bins[0]++;\n    else if (d < 0.5)\n      s_bins[1]++;\n    else if (d < 0.75)\n      s_bins[2]++;\n    else\n      s_bins[3]++;\n  }\n  __syncthreads();\n\n  for (int j = 1; j < 4; j++) {\n    s_bins[j] += s_bins[j - 1];\n  }\n\n  // Copy the local thread results into global memory\n  if (i < 4)\n    bins[i] = s_bins[i];\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double p = x[i];\n    if (p >= 0.0 && p <= 0.25)\n      atomicAdd(&bins[0], 1);\n    else if (p > 0.25 && p <= 0.5)\n      atomicAdd(&bins[1], 1);\n    else if (p > 0.5 && p <= 0.75)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n  if (tid < N) {\n    double bin = x[tid];\n    if (bin < 0.25)\n      ++bins[0];\n    else if (bin < 0.5)\n      ++bins[1];\n    else if (bin < 0.75)\n      ++bins[2];\n    else\n      ++bins[3];\n  }\n}",
            "size_t tid = threadIdx.x;\n  size_t blocks = gridDim.x;\n  size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n\n  __shared__ double x_shared[N];\n\n  double sum = 0;\n  size_t q0 = 0, q1 = 0, q2 = 0, q3 = 0;\n\n  while (i < N) {\n    x_shared[tid] = x[i];\n    sum += x_shared[tid];\n    if (x_shared[tid] <= 0.25) {\n      q0++;\n    }\n    if (x_shared[tid] <= 0.5) {\n      q1++;\n    }\n    if (x_shared[tid] <= 0.75) {\n      q2++;\n    }\n    if (x_shared[tid] <= 1) {\n      q3++;\n    }\n    i += blocks*blockDim.x;\n  }\n\n  // Block 0: compute counts for the first quarter\n  if (tid == 0) {\n    bins[0] = q0;\n    bins[1] = q1;\n  }\n\n  // Block 1: compute counts for the second quarter\n  if (tid == 0) {\n    bins[2] = q2;\n    bins[3] = q3;\n  }\n\n  __syncthreads();\n\n  // Block 0: sum counts from the other two threads\n  if (tid == 0) {\n    bins[0] += bins[1];\n    bins[2] += bins[3];\n  }\n}",
            "// TODO: Fill in code\n  int thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n  int block_size = blockDim.x * gridDim.x;\n  double fraction;\n  while(thread_id < N){\n    fraction = x[thread_id] - floor(x[thread_id]);\n    if(fraction <= 0.25){\n      atomicAdd(&bins[0], 1);\n    }\n    else if(fraction <= 0.5){\n      atomicAdd(&bins[1], 1);\n    }\n    else if(fraction <= 0.75){\n      atomicAdd(&bins[2], 1);\n    }\n    else{\n      atomicAdd(&bins[3], 1);\n    }\n    thread_id += block_size;\n  }\n}",
            "const size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n\n    if (thread_id < N) {\n        int quartile = 0;\n        double fraction = x[thread_id] % 1.0;\n\n        if (fraction >= 0.75) quartile = 3;\n        else if (fraction >= 0.5) quartile = 2;\n        else if (fraction >= 0.25) quartile = 1;\n\n        atomicAdd(bins + quartile, 1);\n    }\n}",
            "int tid = threadIdx.x;\n\tint bid = blockIdx.x;\n\tint bin_idx = bid % 4;\n\tdouble x_val;\n\tbins[bin_idx] = 0;\n\n\t__shared__ int sh_bin[4];\n\tsh_bin[bin_idx] = 0;\n\n\t// loop to get the number of doubles\n\tfor(int i = tid; i < N; i+= blockDim.x) {\n\t\tx_val = x[i];\n\t\tif (x_val >= 0 && x_val < 0.25) {\n\t\t\tsh_bin[bin_idx] += 1;\n\t\t} else if (x_val >= 0.25 && x_val < 0.5) {\n\t\t\tsh_bin[bin_idx] += 1;\n\t\t} else if (x_val >= 0.5 && x_val < 0.75) {\n\t\t\tsh_bin[bin_idx] += 1;\n\t\t} else if (x_val >= 0.75 && x_val < 1) {\n\t\t\tsh_bin[bin_idx] += 1;\n\t\t}\n\t}\n\n\t// add the partial results from each thread\n\t__syncthreads();\n\tif (tid == 0) {\n\t\tfor (int i = 0; i < 4; i++) {\n\t\t\tbins[i] += sh_bin[i];\n\t\t}\n\t}\n}",
            "...\n}",
            "// TODO: Fill in the code here.\n    int idx = threadIdx.x;\n    int stride = blockDim.x;\n    int i = idx + blockIdx.x * stride;\n    int count = 0;\n    while (i < N) {\n        double value = x[i];\n        if (value >= 0.0 && value < 0.25) {\n            ++count;\n        } else if (value >= 0.25 && value < 0.5) {\n            ++count;\n        } else if (value >= 0.5 && value < 0.75) {\n            ++count;\n        } else if (value >= 0.75 && value <= 1.0) {\n            ++count;\n        }\n        i += stride * gridDim.x;\n    }\n\n    atomicAdd(&bins[0], count);\n}",
            "}",
            "__shared__ double x_shared[N]; // shared memory for the vector x\n    size_t idx = blockIdx.x * blockDim.x + threadIdx.x; // the index in the vector x\n    x_shared[threadIdx.x] = x[idx];\n    __syncthreads();\n    if (idx < N) {\n        if (x_shared[threadIdx.x] < 0.25 * idx + 0.25) {\n            atomicAdd(&bins[0], 1);\n        } else if (x_shared[threadIdx.x] < 0.5 * idx + 0.5) {\n            atomicAdd(&bins[1], 1);\n        } else if (x_shared[threadIdx.x] < 0.75 * idx + 0.75) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO: implement\n  __syncthreads();\n}",
            "// TODO: Implement the kernel\n  // You should have at least N threads in the block\n  // and use 4 thread blocks\n\n  __shared__ size_t localBins[4];\n\n  // Your code goes here\n  // This kernel needs 4 threads\n\n  int tid = threadIdx.x;\n  int bid = blockIdx.x;\n  int i = bid * 32 + tid;\n  int stride = blockDim.x * 32;\n  int tid2 = tid % 4;\n\n  localBins[tid2] = 0;\n\n  while (i < N) {\n\n    if ((x[i] >= 0.0 && x[i] < 0.25) || (x[i] >= 0.25 && x[i] < 0.5) || (x[i] >= 0.5 && x[i] < 0.75) || (x[i] >= 0.75 && x[i] < 1.0)) {\n      localBins[tid2]++;\n    }\n\n    i += stride;\n  }\n\n  localBins[tid2] = __syncthreads_or(localBins[tid2]);\n\n  if (tid == 0) {\n    atomicAdd(&bins[0], localBins[0]);\n    atomicAdd(&bins[1], localBins[1]);\n    atomicAdd(&bins[2], localBins[2]);\n    atomicAdd(&bins[3], localBins[3]);\n  }\n\n}",
            "const unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    const double tmp = x[tid];\n    const double quartile = floor(tmp * 4);\n    const int quartile_int = (int)quartile;\n\n    switch (quartile_int) {\n    case 0:\n    case 1:\n      bins[0] += 1;\n      break;\n    case 2:\n    case 3:\n      bins[1] += 1;\n      break;\n    case 4:\n    case 5:\n      bins[2] += 1;\n      break;\n    case 6:\n    case 7:\n      bins[3] += 1;\n      break;\n    }\n  }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Compute the quartiles for the input vector x, storing the counts in bins.\n  // The number of threads launched should be at least N.\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n    // This is the first CUDA kernel. This kernel should be launched\n    // with at least 32 threads.\n    \n    // You can use blockIdx.x and threadIdx.x to determine which\n    // thread is running, and blockIdx.x * blockDim.x + threadIdx.x \n    // to determine the index of the thread within the block.\n    //\n    // If you need to know the index of the block, call\n    // blockIdx.x + gridDim.x * blockIdx.y + gridDim.x * gridDim.y * blockIdx.z\n    \n    // You can use __syncthreads() to prevent threads from overlapping.\n    \n    // You can use atomicAdd() to add to shared memory.\n    \n    // You can use atomicCAS() to add to shared memory only once.\n    \n    // You can use __shfl_sync() to load values from other threads in the same warp.\n    \n    // You can use __ldg() to read from global memory in a safe way.\n}",
            "const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    const size_t N_per_thread = (N + blockDim.x * gridDim.x - 1) / (blockDim.x * gridDim.x);\n    size_t my_bins[4] = {0, 0, 0, 0};\n\n    for (size_t i = tid; i < N; i += N_per_thread) {\n        const double x_i = x[i];\n        if (x_i < 0.25)\n            my_bins[0]++;\n        else if (x_i < 0.5)\n            my_bins[1]++;\n        else if (x_i < 0.75)\n            my_bins[2]++;\n        else\n            my_bins[3]++;\n    }\n\n    atomicAdd(&bins[0], my_bins[0]);\n    atomicAdd(&bins[1], my_bins[1]);\n    atomicAdd(&bins[2], my_bins[2]);\n    atomicAdd(&bins[3], my_bins[3]);\n}",
            "// TODO\n  // You will need to add a kernel here to compute the number of elements in each \n  // bin.  You may find it helpful to use the following helper functions:\n  //   cudaThreadIdx.x: the index of the current thread in the block\n  //   cudaBlockIdx.x: the index of the current block in the grid\n  //   cudaGridDim.x: the number of blocks in the grid\n  //\n  // HINT: The total number of threads will be the number of elements in the vector\n  //       times the number of blocks in the grid.\n  //\n  // HINT: You will need to use blockIdx to determine which bin each thread is \n  //       responsible for, and threadIdx to determine which element in that bin\n  //       it is responsible for.\n  //\n  // HINT: The number of threads in a block is fixed.  If your block size is \n  //       greater than the number of elements in a bin, then the threads that \n  //       are responsible for the remaining threads in the block will do nothing.\n  //       You should think about what that means for your kernel.\n  //\n  // HINT: You should use atomicAdd to increase the count for each bin.  Do \n  //       not use locks, barriers, or any other synchronization primitive.  Your \n  //       solution will not be accepted if you do use synchronization primitives.\n}",
            "// your code goes here\n}",
            "double x_val = x[threadIdx.x + blockIdx.x * blockDim.x];\n  double fraction = x_val - floor(x_val);\n  size_t int_part = static_cast<size_t>(x_val);\n  if (fraction >= 0 && fraction < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (fraction >= 0.25 && fraction < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (fraction >= 0.5 && fraction < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else if (fraction >= 0.75) {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "double frac = threadIdx.x / 4.0;\n\n  // TODO: Your code here\n\n}",
            "// TODO\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n  if (tid >= N)\n    return;\n\n  // Compute the fractional part of x[tid].\n  double xf = x[tid] - floor(x[tid]);\n\n  if (xf < 0.25) {\n    atomicAdd(&bins[0], 1);\n  } else if (xf < 0.5) {\n    atomicAdd(&bins[1], 1);\n  } else if (xf < 0.75) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "// YOUR CODE HERE\n  __syncthreads();\n}",
            "//TODO\n}",
            "// TODO\n}",
            "__shared__ double x_shared[256];\n\n    int tid = threadIdx.x;\n    int bid = blockIdx.x;\n\n    int i = bid * 256 + tid;\n    if (i < N) {\n        double xi = x[i];\n        int bin = (xi < 0.25)? 0 : (xi < 0.5)? 1 : (xi < 0.75)? 2 : 3;\n        atomicAdd(&bins[bin], 1);\n    }\n    __syncthreads();\n\n    // Load values to shared memory\n    if (tid < 256) {\n        int i = bid * 256 + tid;\n        if (i < N) {\n            x_shared[tid] = x[i];\n        }\n    }\n    __syncthreads();\n\n    // Compute the count of values in the quartiles\n    if (tid < 64) {\n        int bin = tid / 4;\n        double *x_sh = x_shared + (tid % 4) * 64;\n        atomicAdd(&bins[bin], __popcll(x_sh[0]));\n        atomicAdd(&bins[bin + 1], __popcll(x_sh[64]));\n    }\n    __syncthreads();\n}",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n    __syncthreads();\n}",
            "// Compute the index of this thread in the input vector\n  size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  // Stop if we've looked at all the elements\n  if (tid >= N) {\n    return;\n  }\n  // Compute the number of quartiles the number is in\n  if (x[tid] < 0.25) {\n    atomicAdd(bins + 0, 1);\n  } else if (x[tid] < 0.5) {\n    atomicAdd(bins + 1, 1);\n  } else if (x[tid] < 0.75) {\n    atomicAdd(bins + 2, 1);\n  } else {\n    atomicAdd(bins + 3, 1);\n  }\n}",
            "// your code here\n}",
            "// TODO: Your code here\n   int tid = threadIdx.x;\n   int i = blockIdx.x*blockDim.x+tid;\n   int ibin = -1;\n\n   double x_i;\n   if(i<N){\n      x_i = x[i];\n      if(x_i<=0.25){\n         ibin = 0;\n      }else if(x_i<=0.5){\n         ibin = 1;\n      }else if(x_i<=0.75){\n         ibin = 2;\n      }else if(x_i<=1){\n         ibin = 3;\n      }\n      atomicAdd(&bins[ibin],1);\n   }\n}",
            "// YOUR CODE GOES HERE\n}",
            "// TODO: Your code here\n\n    // YOUR CODE HERE\n}",
            "}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "20_histogram_count_quartile",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\n/* Count the number of doubles in the vector x that have a fractional part \n   in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). Store the counts in `bins`.\n   Use MPI and OpenMP to compute in parallel. Assume that MPI has already been initialized.\n   Every rank has a complete copy of x. The result is stored in bins on rank 0.\n   Examples:\n\n   input: [7.8, 4.2, 9.1, 7.6, 0.27, 1.5, 3.8]\n   output: [2, 1, 2, 2]\n\n   input: [1.9, 0.2, 0.6, 10.1, 7.4]\n   output: [2, 1, 1, 1]\n*/\nvoid countQuartiles(std::vector<double> const& x, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// your code here\n    bins = {0,0,0,0};\n    size_t local_size = x.size();\n    double threshold = 0.25;\n    size_t index = 0;\n    //#pragma omp parallel for schedule(static)\n    for(int i=0;i<local_size;i++){\n        if((x[i] >= 0.0) && (x[i] < threshold)){\n            bins[0]++;\n        }\n        else if((x[i] >= threshold) && (x[i] < threshold*2)){\n            bins[1]++;\n        }\n        else if((x[i] >= threshold*2) && (x[i] < threshold*3)){\n            bins[2]++;\n        }\n        else{\n            bins[3]++;\n        }\n    }\n\n}",
            "int world_size, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tsize_t num_elems = x.size();\n\tsize_t num_blocks = (num_elems + world_size - 1) / world_size;\n\tsize_t first = num_blocks * rank;\n\tsize_t last = first + num_blocks;\n\tif (last > num_elems) {\n\t\tlast = num_elems;\n\t}\n\tstd::vector<double> local_x(x.begin() + first, x.begin() + last);\n\tint num_threads = omp_get_max_threads();\n\tstd::vector<size_t> thread_bins(num_threads);\n#pragma omp parallel for\n\tfor (int t = 0; t < num_threads; t++) {\n\t\tsize_t first = t * num_elems / num_threads;\n\t\tsize_t last = (t + 1) * num_elems / num_threads;\n\t\tdouble local_sum = 0.0;\n\t\tfor (size_t i = first; i < last; i++) {\n\t\t\tdouble v = local_x[i];\n\t\t\tif (v >= 0.0 && v <= 0.25) {\n\t\t\t\tlocal_sum += 1.0;\n\t\t\t} else if (v > 0.25 && v <= 0.50) {\n\t\t\t\tlocal_sum += 2.0;\n\t\t\t} else if (v > 0.50 && v <= 0.75) {\n\t\t\t\tlocal_sum += 3.0;\n\t\t\t} else {\n\t\t\t\tlocal_sum += 4.0;\n\t\t\t}\n\t\t}\n\t\tthread_bins[t] = local_sum;\n\t}\n\tstd::vector<size_t> thread_sums(num_threads);\n\tMPI_Allreduce(thread_bins.data(), thread_sums.data(), num_threads, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\tif (rank == 0) {\n\t\tfor (size_t i = 0; i < num_threads; i++) {\n\t\t\tbins[i] = thread_sums[i];\n\t\t}\n\t}\n}",
            "//...\n}",
            "// 1. count local quartiles\n    std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n    for (double const& value : x) {\n        double fractional = std::fmod(value, 1.0);\n        if (fractional < 0.25) {\n            local_bins[0]++;\n        } else if (fractional < 0.5) {\n            local_bins[1]++;\n        } else if (fractional < 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    // 2. combine local quartiles using MPI and OpenMP\n    int const num_ranks = 2;\n    int const root_rank = 0;\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int const num_threads = 4;\n    omp_set_num_threads(num_threads);\n    std::array<std::array<size_t, 4>, num_threads> local_sums;\n    #pragma omp parallel\n    {\n        int const thread_num = omp_get_thread_num();\n        std::array<size_t, 4> thread_bins = {0, 0, 0, 0};\n        for (double const& value : x) {\n            double fractional = std::fmod(value, 1.0);\n            if (fractional < 0.25) {\n                thread_bins[0]++;\n            } else if (fractional < 0.5) {\n                thread_bins[1]++;\n            } else if (fractional < 0.75) {\n                thread_bins[2]++;\n            } else {\n                thread_bins[3]++;\n            }\n        }\n        local_sums[thread_num] = thread_bins;\n    }\n\n    std::array<size_t, 4> global_bins = {0, 0, 0, 0};\n    MPI_Reduce(local_sums.data(), global_bins.data(), global_bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, root_rank, MPI_COMM_WORLD);\n\n    // 3. send the results back to rank 0\n    if (rank == root_rank) {\n        bins = global_bins;\n    }\n}",
            "// Your code goes here!\n\tint rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tif (rank == 0) {\n\t\tbins.fill(0);\n\t}\n\tint N = x.size();\n\tint div = N / size;\n\tint rem = N % size;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < size - 1; ++i) {\n\t\t\tstd::vector<double> slice(x.begin() + div * i, x.begin() + div * (i + 1));\n\t\t\tif (rem > 0) {\n\t\t\t\tslice.resize(slice.size() + 1);\n\t\t\t\tslice.back() = x.back();\n\t\t\t\t--rem;\n\t\t\t}\n\t\t\tstd::vector<size_t> slice_bins(4);\n\t\t\t// TODO\n\t\t\tomp_set_num_threads(1);\n\t\t\t#pragma omp parallel for\n\t\t\tfor (size_t j = 0; j < slice.size(); ++j) {\n\t\t\t\tdouble x = slice[j];\n\t\t\t\tif (x >= 0.0 && x < 0.25) {\n\t\t\t\t\t#pragma omp atomic\n\t\t\t\t\t++slice_bins[0];\n\t\t\t\t} else if (x >= 0.25 && x < 0.5) {\n\t\t\t\t\t#pragma omp atomic\n\t\t\t\t\t++slice_bins[1];\n\t\t\t\t} else if (x >= 0.5 && x < 0.75) {\n\t\t\t\t\t#pragma omp atomic\n\t\t\t\t\t++slice_bins[2];\n\t\t\t\t} else if (x >= 0.75 && x < 1.0) {\n\t\t\t\t\t#pragma omp atomic\n\t\t\t\t\t++slice_bins[3];\n\t\t\t\t}\n\t\t\t}\n\t\t\tMPI_Gather(&slice_bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\t\t}\n\t} else {\n\t\tstd::vector<double> slice(x.begin() + div * rank, x.begin() + div * (rank + 1));\n\t\tif (rem > 0) {\n\t\t\tslice.resize(slice.size() + 1);\n\t\t\tslice.back() = x.back();\n\t\t\t--rem;\n\t\t}\n\t\tstd::vector<size_t> slice_bins(4);\n\t\t// TODO\n\t\tomp_set_num_threads(1);\n\t\t#pragma omp parallel for\n\t\tfor (size_t i = 0; i < slice.size(); ++i) {\n\t\t\tdouble x = slice[i];\n\t\t\tif (x >= 0.0 && x < 0.25) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\t++slice_bins[0];\n\t\t\t} else if (x >= 0.25 && x < 0.5) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\t++slice_bins[1];\n\t\t\t} else if (x >= 0.5 && x < 0.75) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\t++slice_bins[2];\n\t\t\t} else if (x >= 0.75 && x < 1.0) {\n\t\t\t\t#pragma omp atomic\n\t\t\t\t++slice_bins[3];\n\t\t\t}\n\t\t}\n\t\tMPI_Gather(&slice_bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\t}\n}",
            "//...\n}",
            "size_t n = x.size();\n  size_t p = 4;\n  size_t m = n / p;\n  size_t k = n % p;\n\n  // TODO: your code here\n  std::sort(x.begin(), x.end());\n  double q1 = x[m];\n  double q2 = x[m*2];\n  double q3 = x[m*3];\n  double q4 = x[n-1];\n\n  std::vector<size_t> temp1(n);\n  std::vector<size_t> temp2(n);\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] < q1) temp1[i] = 0;\n    else if (x[i] < q2) temp1[i] = 1;\n    else if (x[i] < q3) temp1[i] = 2;\n    else if (x[i] < q4) temp1[i] = 3;\n    else temp1[i] = 4;\n  }\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    if (x[i] <= q1) temp2[i] = 0;\n    else if (x[i] <= q2) temp2[i] = 1;\n    else if (x[i] <= q3) temp2[i] = 2;\n    else if (x[i] <= q4) temp2[i] = 3;\n    else temp2[i] = 4;\n  }\n\n  std::vector<size_t> res1(p), res2(p);\n  MPI_Allreduce(temp1.data(), res1.data(), p, MPI_SIZE_T, MPI_SUM, MPI_COMM_WORLD);\n  MPI_Allreduce(temp2.data(), res2.data(), p, MPI_SIZE_T, MPI_SUM, MPI_COMM_WORLD);\n\n  for (int i = 0; i < p; ++i) {\n    if (i < k) bins[res1[i]]++;\n    else bins[res2[i-k]]++;\n  }\n}",
            "int world_size = -1, world_rank = -1;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    size_t num_tasks = x.size() / world_size;\n    if (world_rank == 0) {\n        std::cout << \"world_size=\" << world_size << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"world_rank=\" << world_rank << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"num_tasks=\" << num_tasks << std::endl;\n    }\n\n    std::vector<double> my_vector = std::vector<double>(num_tasks);\n    std::vector<int> my_vector_size = std::vector<int>(num_tasks);\n\n    std::vector<double> my_vector2 = std::vector<double>(num_tasks);\n    std::vector<int> my_vector_size2 = std::vector<int>(num_tasks);\n\n    // Send the size of x to every rank\n    std::vector<size_t> x_size = std::vector<size_t>(world_size);\n    std::copy(x.begin(), x.end(), std::back_inserter(x_size));\n    MPI_Scatter(&x_size[0], 1, MPI_UNSIGNED_LONG, &my_vector_size[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Send the data of x to every rank\n    MPI_Scatterv(&x[0], &my_vector_size[0], &my_vector[0], MPI_DOUBLE, &my_vector[0], num_tasks, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Compute in parallel\n    std::vector<size_t> my_bins = std::vector<size_t>(num_tasks);\n\n    int a = omp_get_max_threads();\n    int b = omp_get_num_threads();\n\n    omp_set_num_threads(world_size);\n    #pragma omp parallel for schedule(dynamic, 1)\n    for (int i = 0; i < num_tasks; i++) {\n        for (double e : my_vector) {\n            if (e >= 0 && e < 0.25) {\n                my_bins[i] += 1;\n            } else if (e >= 0.25 && e < 0.5) {\n                my_bins[i] += 1;\n            } else if (e >= 0.5 && e < 0.75) {\n                my_bins[i] += 1;\n            } else if (e >= 0.75 && e < 1) {\n                my_bins[i] += 1;\n            }\n        }\n    }\n\n    MPI_Gather(&my_bins[0], num_tasks, MPI_UNSIGNED_LONG, &bins[0], num_tasks, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n    if (world_rank == 0) {\n        std::cout << \"my_bins[0]=\" << bins[0] << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"my_bins[1]=\" << bins[1] << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"my_bins[2]=\" << bins[2] << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"my_bins[3]=\" << bins[3] << std::endl;\n    }\n\n    // Check the result\n\n    if (world_rank == 0) {\n        std::cout << \"my_bins[0]=\" << bins[0] << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"my_bins[1]=\" << bins[1] << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"my_bins[2]=\" << bins[2] << std::endl;\n    }\n    if (world_rank == 0) {\n        std::cout << \"my_bins[3]=\" << bins[3] << std::endl;\n    }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int num_threads = omp_get_max_threads();\n\n    const size_t N = x.size() / size;\n    std::vector<std::array<size_t, 4>> thread_bins(num_threads);\n\n    auto count_quartiles_in_parallel = [&x, N, rank, size, num_threads, &thread_bins](size_t i) {\n        const double x_rank = x[i + N * rank];\n        auto &bin = thread_bins[omp_get_thread_num()];\n        if (x_rank < 0.25 * N) {\n            ++bin[0];\n        } else if (x_rank < 0.5 * N) {\n            ++bin[1];\n        } else if (x_rank < 0.75 * N) {\n            ++bin[2];\n        } else {\n            ++bin[3];\n        }\n    };\n\n    #pragma omp parallel for schedule(dynamic)\n    for (size_t i = 0; i < N; ++i) {\n        count_quartiles_in_parallel(i);\n    }\n\n    std::array<size_t, 4> global_bin;\n    MPI_Reduce(thread_bins.data(), global_bin.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    bins = global_bin;\n}",
            "// TODO: your code here\n}",
            "int rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    size_t n = x.size();\n    size_t chunkSize = n / world_size;\n\n    bins.fill(0);\n    std::vector<double> localBins(4);\n\n    #pragma omp parallel for\n    for (size_t i = rank * chunkSize; i < (rank + 1) * chunkSize; i++) {\n        double value = x[i];\n        if (value >= 0.25 && value < 0.5) {\n            localBins[0] += 1;\n        } else if (value >= 0.5 && value < 0.75) {\n            localBins[1] += 1;\n        } else if (value >= 0.75 && value <= 1) {\n            localBins[2] += 1;\n        } else {\n            localBins[3] += 1;\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), localBins.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for \n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tif ((x[i] >= 0) && (x[i] < 0.25)) {\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if ((x[i] >= 0.25) && (x[i] < 0.5)) {\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if ((x[i] >= 0.5) && (x[i] < 0.75)) {\n\t\t\tbins[2]++;\n\t\t}\n\t\telse if (x[i] >= 0.75) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// your code here\n}",
            "// 1. get the length of x\n\tsize_t N = x.size();\n\tdouble start = MPI_Wtime();\n\tstd::cout << \"N: \" << N << std::endl;\n\t// 2. Calculate the fractional part of x\n\tstd::vector<double> x_fractional(N);\n#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tx_fractional[i] = x[i] - std::floor(x[i]);\n\t}\n\n\t// 3. Create an array for the number of elements in each bin\n\t//std::array<size_t, 4> bins;\n\tstd::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n\tfor (size_t i = 0; i < N; ++i) {\n\t\tif (x_fractional[i] <= 0.25) {\n\t\t\tbins[0]++;\n\t\t} else if (x_fractional[i] > 0.25 && x_fractional[i] <= 0.5) {\n\t\t\tbins[1]++;\n\t\t} else if (x_fractional[i] > 0.5 && x_fractional[i] <= 0.75) {\n\t\t\tbins[2]++;\n\t\t} else if (x_fractional[i] > 0.75) {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n\n\t// 4. Sum the values of bins on rank 0\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tif (rank == 0) {\n\t\tstd::array<size_t, 4> bins_global;\n\t\tMPI_Reduce(bins.data(), bins_global.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t\tbins = bins_global;\n\t} else {\n\t\tMPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t}\n\tdouble end = MPI_Wtime();\n\tstd::cout << \"Time: \" << end - start << std::endl;\n}",
            "if (x.size() < 1) {\n        bins = {0, 0, 0, 0};\n        return;\n    }\n\n    int n = static_cast<int>(x.size());\n\n    #pragma omp parallel for\n    for (int i = 0; i < n; i++) {\n        double xi = x[i];\n        if (xi >= 0.0 && xi <= 0.25) {\n            bins[0] += 1;\n        } else if (xi >= 0.25 && xi <= 0.5) {\n            bins[1] += 1;\n        } else if (xi >= 0.5 && xi <= 0.75) {\n            bins[2] += 1;\n        } else if (xi >= 0.75 && xi <= 1.0) {\n            bins[3] += 1;\n        }\n    }\n}",
            "// TODO\n}",
            "bins = {0, 0, 0, 0};\n\n  // TODO: implement\n}",
            "bins.fill(0);\n  int size, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: count quartiles in parallel\n  // Your code should go here. You can use the variables above to figure\n  // out how to split up the work.\n}",
            "//TODO\n}",
            "// TODO: Your code here\n\n  // The number of threads is the number of available cores on the node.\n  int num_threads = omp_get_num_procs();\n\n  // The number of tasks is the number of threads available on the node\n  int num_tasks = num_threads;\n\n  // The number of doubles in the vector is the length of the vector.\n  int num_doubles = x.size();\n\n  // The number of tasks that will be assigned to each rank\n  int tasks_per_rank = num_doubles/num_tasks;\n\n  // The remainder is the number of tasks that are assigned to the last rank\n  int remainder = num_doubles % num_tasks;\n\n  // The number of tasks that the last rank has.\n  int remainder_tasks = (remainder!= 0)? remainder : 0;\n\n  // The number of tasks that each rank has to compute\n  int rank_tasks = tasks_per_rank + remainder_tasks;\n\n  // The number of ranks in the computation\n  int num_ranks = num_tasks;\n\n  // The number of doubles in the vector is the length of the vector\n  int num_doubles_per_rank = rank_tasks;\n\n  // The range of doubles for each task\n  int start_index = 0;\n  int end_index = 0;\n\n  // The number of bins\n  int num_bins = 4;\n\n  // The vector of bin counts\n  std::vector<size_t> counts(num_bins, 0);\n\n  // The number of tasks that will be assigned to each rank\n  int tasks_per_rank = num_doubles/num_ranks;\n\n  // The remainder is the number of tasks that are assigned to the last rank\n  int remainder = num_doubles % num_ranks;\n\n  // The number of tasks that the last rank has.\n  int remainder_tasks = (remainder!= 0)? remainder : 0;\n\n  // The number of tasks that each rank has to compute\n  int rank_tasks = tasks_per_rank + remainder_tasks;\n\n  // The number of doubles in the vector is the length of the vector\n  int num_doubles_per_rank = rank_tasks;\n\n  // The range of doubles for each task\n  int start_index = 0;\n  int end_index = 0;\n\n  // Loop over the number of tasks\n  for (int t = 0; t < num_tasks; t++) {\n\n    // Calculate the start index for the task\n    start_index = num_doubles_per_rank*t;\n\n    // Calculate the end index for the task\n    end_index = num_doubles_per_rank*(t+1);\n\n    // Calculate the bin for the task\n    int bin = calculateBin(x[start_index], start_index);\n\n    // Add the count to the bin\n    counts[bin]++;\n  }\n\n  // Determine the minimum count\n  int min = counts[0];\n\n  // Determine the minimum index\n  int min_index = 0;\n\n  // Loop over the number of bins\n  for (int b = 0; b < num_bins; b++) {\n\n    // If the minimum count is less than the current count\n    if (min > counts[b]) {\n\n      // Set the minimum count to the current count\n      min = counts[b];\n\n      // Set the minimum index to the current index\n      min_index = b;\n    }\n  }\n\n  // Initialize the MPI datatype for the vector of counts\n  MPI_Datatype counts_type;\n  MPI_Type_vector(num_bins, 1, num_ranks, MPI_UNSIGNED_LONG, &counts_type);\n  MPI_Type_commit(&counts_type);\n\n  // Initialize the MPI datatype for the vector of counts\n  MPI_Datatype counts_send_type;\n  MPI_Type_vector(num_bins, 1, num_ranks, MPI_UNSIGNED_LONG, &counts_send_type);\n  MPI_Type_commit(&counts_send_type);\n\n  // Initialize the MPI datatype for the vector of counts\n  MPI_Datatype counts_recv_type;\n  MPI_Type_vector(num_bins, 1, num_ranks, MPI_UNSIGNED_LONG, &counts_recv_type);\n  MPI_Type_commit(&counts_recv_type);\n\n  // Initialize the MPI datatype for the vector of bin counts\n  MPI_Datatype bins_type;\n  MPI_Type_vector(num_bins, 1, num_ranks, MPI_UNSIGNED_LONG, &bins_type);\n  MPI_Type_commit(&bins_type);\n\n  // Initialize the MPI datatype for the vector of bin counts\n  MPI_Datatype bins_send_type;\n  MPI_Type_vector(num_bins, 1, num_ranks, MPI",
            "}",
            "if (x.size() < 4) {\n        bins.fill(0);\n        return;\n    }\n\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    double fractional = 0.0;\n\n    // your code here\n    std::vector<double> local_bin(4);\n    std::vector<double> global_bin(4);\n    int num_thread = omp_get_num_threads();\n\n    int num_local = x.size() / num_ranks;\n    int remainder = x.size() % num_ranks;\n\n    std::vector<double> local_data(num_local + 1);\n    std::vector<double> remainder_data(remainder);\n\n    if (rank == 0) {\n        std::copy(x.begin(), x.end(), local_data.begin());\n        if (remainder!= 0) {\n            std::copy(x.end() - remainder, x.end(), remainder_data.begin());\n        }\n    }\n\n    MPI_Scatter(local_data.data(), num_local, MPI_DOUBLE, local_bin.data(), 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(remainder_data.data(), remainder, MPI_DOUBLE, local_bin.data() + num_local, 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < num_local; i++) {\n        fractional = std::fmod(local_bin[i], 0.25);\n        if (fractional >= 0 && fractional <= 0.25) {\n            local_bin[0] += 1;\n        } else if (fractional > 0.25 && fractional <= 0.5) {\n            local_bin[1] += 1;\n        } else if (fractional > 0.5 && fractional <= 0.75) {\n            local_bin[2] += 1;\n        } else if (fractional > 0.75 && fractional <= 1) {\n            local_bin[3] += 1;\n        } else {\n            std::cout << \"Something went wrong!\" << std::endl;\n        }\n    }\n\n    for (int i = 0; i < remainder; i++) {\n        fractional = std::fmod(local_bin[num_local + i], 0.25);\n        if (fractional >= 0 && fractional <= 0.25) {\n            local_bin[0] += 1;\n        } else if (fractional > 0.25 && fractional <= 0.5) {\n            local_bin[1] += 1;\n        } else if (fractional > 0.5 && fractional <= 0.75) {\n            local_bin[2] += 1;\n        } else if (fractional > 0.75 && fractional <= 1) {\n            local_bin[3] += 1;\n        } else {\n            std::cout << \"Something went wrong!\" << std::endl;\n        }\n    }\n\n    MPI_Gather(local_bin.data(), 4, MPI_DOUBLE, global_bin.data(), 4, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        std::copy(global_bin.begin(), global_bin.end(), bins.begin());\n    }\n}",
            "// TODO\n}",
            "// TODO: count the quartiles\n}",
            "//TODO: Implement me!\n\n}",
            "int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  // TODO: implement the algorithm described above.\n\n}",
            "/* *** To be completed in Part 3 *** */\n}",
            "// TODO\n}",
            "int numRanks, rank;\n\tMPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t// Find the total size of the array\n\tint total = x.size();\n\n\t// Set the number of partitions based on the number of ranks\n\tint partitions = numRanks;\n\n\t// Use MPI to divide the workload\n\tint div = ceil(total / partitions);\n\tstd::vector<double> sub_vec(x.begin() + div * rank, x.begin() + div * (rank + 1));\n\n\t// Count the number of elements in the vector\n\t// Note: Every rank has a copy of sub_vec\n\tsize_t local_count = 0;\n\t#pragma omp parallel for reduction(+:local_count)\n\tfor (size_t i = 0; i < sub_vec.size(); i++) {\n\t\tif (sub_vec[i] >= 0.25 && sub_vec[i] < 0.5) {\n\t\t\tlocal_count += 1;\n\t\t}\n\t\telse if (sub_vec[i] >= 0.5 && sub_vec[i] < 0.75) {\n\t\t\tlocal_count += 2;\n\t\t}\n\t\telse if (sub_vec[i] >= 0.75 && sub_vec[i] < 1) {\n\t\t\tlocal_count += 3;\n\t\t}\n\t}\n\n\t// Gather results\n\tint global_count = 0;\n\tif (rank == 0) {\n\t\tstd::vector<size_t> counts(partitions);\n\t\tMPI_Gather(&local_count, 1, MPI_UNSIGNED_LONG, counts.data(), 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\t\tglobal_count = std::accumulate(counts.begin(), counts.end(), 0);\n\t}\n\telse {\n\t\tMPI_Gather(&local_count, 1, MPI_UNSIGNED_LONG, nullptr, 0, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\t}\n\n\t// Calculate the quartiles using OpenMP\n\t// Every rank has a copy of sub_vec\n\t#pragma omp parallel for reduction(+:global_count)\n\tfor (size_t i = 0; i < sub_vec.size(); i++) {\n\t\tif (sub_vec[i] >= 0.25 && sub_vec[i] < 0.5) {\n\t\t\tglobal_count += 1;\n\t\t}\n\t\telse if (sub_vec[i] >= 0.5 && sub_vec[i] < 0.75) {\n\t\t\tglobal_count += 2;\n\t\t}\n\t\telse if (sub_vec[i] >= 0.75 && sub_vec[i] < 1) {\n\t\t\tglobal_count += 3;\n\t\t}\n\t}\n\n\t// Split the results\n\tif (rank == 0) {\n\t\tbins = std::array<size_t, 4>({ global_count / 4, global_count / 2, (global_count / 4) * 3, global_count });\n\t}\n}",
            "// Your code here.\n  // You will need to write a function to count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n  // Store the counts in `bins`.\n  // You will need to write a function to count the number of doubles in the vector x that have a fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n}",
            "size_t num_el = x.size();\n    int num_threads = omp_get_max_threads();\n    size_t n_per_thread = num_el / num_threads;\n    size_t remaining = num_el % num_threads;\n\n    #pragma omp parallel default(none) num_threads(num_threads)\n    {\n        #pragma omp for schedule(dynamic) nowait\n        for (size_t i = 0; i < num_threads; i++) {\n            if (i < remaining) {\n                double lower_bound = i * n_per_thread * 0.25;\n                double upper_bound = lower_bound + n_per_thread * 0.25;\n                bins[0] += std::count_if(x.begin() + i * n_per_thread, x.begin() + (i + 1) * n_per_thread, [lower_bound, upper_bound](double v) {\n                    return v >= lower_bound && v < upper_bound;\n                });\n            } else {\n                double lower_bound = (i * n_per_thread + remaining) * 0.25;\n                double upper_bound = (i + 1) * n_per_thread * 0.25;\n                bins[0] += std::count_if(x.begin() + i * n_per_thread + remaining, x.end(), [lower_bound, upper_bound](double v) {\n                    return v >= lower_bound && v < upper_bound;\n                });\n            }\n        }\n    }\n\n    MPI_Datatype double_type = MPI_DOUBLE;\n    MPI_Allreduce(bins.data(), bins.data() + 1, 4, double_type, MPI_SUM, MPI_COMM_WORLD);\n    bins[0] = static_cast<size_t>(bins[0]);\n}",
            "// TODO: implement\n  int rank, num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  int num_threads = omp_get_max_threads();\n\n  size_t n = x.size();\n  size_t chunk_size = n / num_ranks;\n  size_t start = rank * chunk_size;\n  size_t end = (rank == (num_ranks - 1))? n : (start + chunk_size);\n\n  std::vector<double> local_data;\n  local_data.reserve(n);\n  std::copy(x.begin() + start, x.begin() + end, std::back_inserter(local_data));\n\n  // allocate memory for count in each thread\n  size_t* count_array = new size_t[num_threads];\n\n  // compute count in each thread\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int thread_num = omp_get_thread_num();\n\n    // set count_array to 0\n    count_array[thread_num] = 0;\n\n    for (auto x_i : local_data) {\n      // TODO: implement the count\n    }\n  }\n\n  // sum the count in each thread to the corresponding value in count_array\n  for (int i = 1; i < num_threads; i++) {\n    count_array[0] += count_array[i];\n  }\n\n  // TODO: implement the binning using the count in count_array\n  // set bin_array to 0\n  std::array<size_t, 4> bin_array{0, 0, 0, 0};\n\n  MPI_Reduce(&count_array[0], &bin_array[0], num_threads, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    // store the result in bins\n    std::copy(bin_array.begin(), bin_array.end(), bins.begin());\n  }\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  //...\n  //...\n  //...\n  //...\n  //...\n}",
            "// TODO: Your code here.\n}",
            "int numprocs;\n  int rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // TODO: your code here\n}",
            "bins.fill(0);\n\n    // TODO\n}",
            "size_t local_bins[4];\n    for (size_t i = 0; i < 4; i++) {\n        local_bins[i] = 0;\n    }\n    size_t local_size = x.size();\n    size_t local_count;\n    for (size_t i = 0; i < local_size; i++) {\n        double xi = x[i];\n        if (xi <= 0.25) {\n            local_bins[0]++;\n        } else if (xi > 0.25 && xi <= 0.5) {\n            local_bins[1]++;\n        } else if (xi > 0.5 && xi <= 0.75) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n    // TODO: Replace this code with a call to a parallel reduction\n    local_count = local_bins[0];\n    for (size_t i = 1; i < 4; i++) {\n        local_count += local_bins[i];\n    }\n    // TODO: Replace this code with a call to a parallel reduction\n    bins[0] = local_bins[0];\n    for (size_t i = 1; i < 4; i++) {\n        bins[i] = bins[i - 1] + local_bins[i];\n    }\n}",
            "/* TODO */\n}",
            "int rank, nprocs;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  int blocksize = x.size() / nprocs;\n\n  std::vector<double> block(blocksize);\n  std::vector<int> counts(4);\n\n  // Each rank gets a block of x\n  MPI_Scatter(x.data(), blocksize, MPI_DOUBLE, block.data(), blocksize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // Each rank does its own thing\n  #pragma omp parallel default(shared)\n  {\n    std::array<double, 4> mybins;\n    mybins[0] = 0.0;\n    mybins[1] = 0.25;\n    mybins[2] = 0.5;\n    mybins[3] = 0.75;\n\n    #pragma omp for schedule(static)\n    for (int i = 0; i < blocksize; ++i) {\n      double xi = block[i];\n\n      int k = (xi >= mybins[0] && xi < mybins[1])? 0 :\n          (xi >= mybins[1] && xi < mybins[2])? 1 :\n          (xi >= mybins[2] && xi < mybins[3])? 2 : 3;\n      ++counts[k];\n    }\n  }\n\n  // Aggregate the counts from each rank\n  MPI_Reduce(counts.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins = {0, 0, 0, 0}; // initialization\n    int const n = x.size();\n    double const n_d = n;\n    int const rank = getRank();\n\n    if (rank == 0) {\n        // Each process should be responsible for some data, \n        // even if it means some values will be double-counted.\n        // The number of processes is the number of values in x.\n        int const n_procs = n;\n\n        std::vector<std::array<size_t, 4>> bins_local(n_procs);\n\n        #pragma omp parallel num_threads(n_procs) shared(bins, x) default(none)\n        {\n            int const thread_id = omp_get_thread_num();\n            int const start = thread_id * n / n_procs;\n            int const end = (thread_id + 1) * n / n_procs;\n\n            for (int i = start; i < end; ++i) {\n                double const value = x[i];\n                double const value_fraction = std::fmod(value, 1);\n\n                if (value_fraction < 0.25) {\n                    bins_local[thread_id][0]++;\n                } else if (value_fraction < 0.5) {\n                    bins_local[thread_id][1]++;\n                } else if (value_fraction < 0.75) {\n                    bins_local[thread_id][2]++;\n                } else {\n                    bins_local[thread_id][3]++;\n                }\n            }\n        }\n\n        // Combine the bins from all processes.\n        // This is an example of an MPI reduce operation.\n        // All processes must have the same number of bins.\n        // All processes should be at the same process index.\n        // The first parameter is a vector of the bins on each process.\n        // The second parameter is the bins on the root process.\n        // The third parameter is the number of elements in the bins vector.\n        MPI_Reduce(bins_local.data(), bins.data(), n_procs, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        // The number of values on each process is different\n        // but it is still the number of values in x.\n        // The number of processes is the number of values in x.\n        // All processes should be at the same process index.\n        MPI_Reduce(x.data(), bins.data(), n, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n\n    // Each process now has the count of values on each bin.\n    // Now each process must compute the sum of the bins.\n    #pragma omp parallel num_threads(4) shared(bins, n) default(none)\n    {\n        int const thread_id = omp_get_thread_num();\n        int const start = thread_id * n / 4;\n        int const end = (thread_id + 1) * n / 4;\n\n        for (int i = start; i < end; ++i) {\n            bins[thread_id] += bins[i];\n        }\n    }\n}",
            "// TODO: implement\n  size_t size = x.size();\n  bins.fill(0);\n  int rank = 0;\n  int nprocs = 1;\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  #pragma omp parallel \n  {\n    std::vector<size_t> count_local(4);\n    #pragma omp for\n    for (int i = 0; i < size; ++i) {\n      double tmp = x[i] - floor(x[i]);\n      if (tmp < 0.25 && tmp >= 0) ++count_local[0];\n      else if (tmp >= 0.25 && tmp < 0.5) ++count_local[1];\n      else if (tmp >= 0.5 && tmp < 0.75) ++count_local[2];\n      else if (tmp >= 0.75 && tmp < 1) ++count_local[3];\n    }\n    #pragma omp critical\n    {\n      for (int i = 0; i < 4; ++i) {\n        bins[i] += count_local[i];\n      }\n    }\n  }\n  // reduce\n  if (rank!= 0) {\n    MPI_Status status;\n    MPI_Request request;\n    MPI_Irecv(bins.data(), 4, MPI_UNSIGNED_LONG, 0, 1, MPI_COMM_WORLD, &request);\n    MPI_Wait(&request, &status);\n  } else {\n    for (int i = 1; i < nprocs; ++i) {\n      MPI_Status status;\n      MPI_Request request;\n      MPI_Irecv(bins.data(), 4, MPI_UNSIGNED_LONG, i, 1, MPI_COMM_WORLD, &request);\n      MPI_Wait(&request, &status);\n    }\n  }\n}",
            "const size_t n = x.size();\n  const int nthreads = omp_get_max_threads();\n  const int nprocs = omp_get_num_procs();\n  const int nquartiles = 4;\n\n  // partition x among the threads\n  std::vector<double> x_part(n/nprocs + n/nprocs + (n % nprocs > 0? 1 : 0));\n  int my_start = (n / nprocs) * omp_get_thread_num();\n  int my_end = (n / nprocs) * (omp_get_thread_num() + 1);\n  if (omp_get_thread_num() == nprocs - 1) {\n    my_end = n;\n  }\n\n  // partition x among the processors\n  int n_per_proc = (n + nprocs - 1)/nprocs;\n  std::vector<double> x_global(n_per_proc*nprocs);\n  if (omp_get_thread_num() == 0) {\n    MPI_Gather(&x[0], n_per_proc, MPI_DOUBLE, &x_global[0], n_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  // find the bin that each entry belongs to\n  std::vector<size_t> bin_counts(nprocs, 0);\n  int i_start = (n_per_proc/nprocs) * omp_get_thread_num();\n  int i_end = (n_per_proc/nprocs) * (omp_get_thread_num() + 1);\n  if (omp_get_thread_num() == nprocs - 1) {\n    i_end = n_per_proc;\n  }\n  for (int i = i_start; i < i_end; ++i) {\n    for (int j = 0; j < nquartiles; ++j) {\n      if ((x_global[i] > j/nquartiles*(x_global[i] - 1/nquartiles)) && (x_global[i] <= (j + 1)/nquartiles*(x_global[i] - 1/nquartiles))) {\n        bin_counts[omp_get_thread_num()] += 1;\n        break;\n      }\n    }\n  }\n\n  MPI_Allreduce(&bin_counts[0], &bins[0], nprocs, MPI_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "double const threshold1 = 0.25;\n  double const threshold2 = 0.5;\n  double const threshold3 = 0.75;\n  double const threshold4 = 1.0;\n  bins.fill(0);\n  double const n = x.size();\n  double const part = n / 4.0;\n  double const a = n % 4.0;\n  std::vector<double> tmp;\n#pragma omp parallel for reduction(+: bins[0:4])\n  for (auto i = 0; i < n; ++i) {\n    if (x[i] < threshold1) {\n      ++bins[0];\n    } else if (x[i] >= threshold1 && x[i] < threshold2) {\n      ++bins[1];\n    } else if (x[i] >= threshold2 && x[i] < threshold3) {\n      ++bins[2];\n    } else if (x[i] >= threshold3) {\n      ++bins[3];\n    }\n  }\n  if (a > 0) {\n    for (auto i = 0; i < n; ++i) {\n      if (x[i] < threshold1) {\n        tmp.push_back(x[i]);\n      } else if (x[i] >= threshold1 && x[i] < threshold2) {\n        tmp.push_back(x[i]);\n      } else if (x[i] >= threshold2 && x[i] < threshold3) {\n        tmp.push_back(x[i]);\n      } else if (x[i] >= threshold3) {\n        tmp.push_back(x[i]);\n      }\n    }\n  }\n#pragma omp parallel\n  {\n    std::vector<double> tmp2;\n    tmp2.resize(tmp.size());\n#pragma omp for reduction(+: bins[0:4])\n    for (auto i = 0; i < tmp.size(); ++i) {\n      if (tmp[i] < threshold1) {\n        ++bins[0];\n      } else if (tmp[i] >= threshold1 && tmp[i] < threshold2) {\n        ++bins[1];\n      } else if (tmp[i] >= threshold2 && tmp[i] < threshold3) {\n        ++bins[2];\n      } else if (tmp[i] >= threshold3) {\n        ++bins[3];\n      }\n    }\n  }\n}",
            "const int rank = 0;\n  const size_t n = x.size();\n  const int nthreads = 10;\n  const int nthreads_in_use = omp_get_max_threads();\n  const int nranks = 1;\n  const int nranks_in_use = 1;\n\n  std::vector<size_t> thread_local_bins(4, 0);\n\n  // Count the number of elements in each of the 4 bins\n  #pragma omp parallel for schedule(static) num_threads(nthreads_in_use)\n  for (int i = 0; i < n; i++) {\n    double const frac = x[i] - std::floor(x[i]);\n    if (frac >= 0 && frac <= 0.25) {\n      thread_local_bins[0]++;\n    } else if (frac > 0.25 && frac <= 0.5) {\n      thread_local_bins[1]++;\n    } else if (frac > 0.5 && frac <= 0.75) {\n      thread_local_bins[2]++;\n    } else if (frac > 0.75 && frac <= 1) {\n      thread_local_bins[3]++;\n    }\n  }\n\n  // Sum the bins of each thread to get the bins on rank 0\n  std::vector<size_t> all_bins(4, 0);\n  MPI_Reduce(thread_local_bins.data(), all_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n\n  // Print the results to the screen if rank 0\n  if (rank == 0) {\n    std::cout << \"nthreads = \" << nthreads_in_use << \", nthreads_in_use = \" << nthreads << \"\\n\"\n              << \"nranks = \" << nranks_in_use << \", nranks_in_use = \" << nranks << \"\\n\"\n              << \"n = \" << n << \"\\n\"\n              << \"bins = \" << all_bins[0] << \", \" << all_bins[1] << \", \" << all_bins[2] << \", \" << all_bins[3] << \"\\n\";\n  }\n\n  // Store the result in bins on rank 0\n  if (rank == 0) {\n    bins = all_bins;\n  }\n}",
            "#pragma omp parallel\n#pragma omp single nowait\n  {\n    std::array<size_t, 4> counts = {{0, 0, 0, 0}};\n#pragma omp taskloop num_tasks(10) shared(counts)\n    for (size_t i = 0; i < x.size(); i++) {\n      auto const val = x[i];\n      if (val < 0.25) {\n        counts[0]++;\n      } else if (val < 0.5) {\n        counts[1]++;\n      } else if (val < 0.75) {\n        counts[2]++;\n      } else {\n        counts[3]++;\n      }\n    }\n#pragma omp taskwait\n    bins = counts;\n  }\n}",
            "// TODO: implement this\n  //...\n}",
            "// TODO: implement this function\n    // You may modify and add code above this line\n    size_t n = x.size();\n    size_t p = omp_get_max_threads();\n    int rank = 0, size = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    bins.fill(0);\n    size_t l = n / p;\n    size_t r = n % p;\n    size_t left_size = rank < r? l + 1 : l;\n\n    #pragma omp parallel\n    {\n        double a = 0, b = 0;\n        size_t local_size = 0;\n        size_t local_bins[4];\n        local_bins[0] = local_bins[1] = local_bins[2] = local_bins[3] = 0;\n        #pragma omp for nowait\n        for (size_t i = l * rank + std::min(rank, r); i < l * rank + left_size + std::min(rank, r); i++)\n        {\n            if (i == n) break;\n            a = x[i];\n            b = std::floor(a / 0.25);\n            if (b == 0) local_bins[0]++;\n            else if (b == 1) local_bins[1]++;\n            else if (b == 2) local_bins[2]++;\n            else local_bins[3]++;\n            local_size++;\n        }\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 4; i++)\n            {\n                bins[i] += local_bins[i];\n            }\n        }\n    }\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  double sum = 0.0;\n  size_t n = x.size() / size;\n  std::vector<double> y(n);\n  if (rank == 0) {\n    for (int i = 1; i < size; ++i) {\n      MPI_Recv(y.data(), n, MPI_DOUBLE, i, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      for (size_t j = 0; j < n; ++j) {\n        sum += y[j];\n      }\n    }\n  } else {\n    MPI_Send(x.data() + rank * n, n, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD);\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  if (rank == 0) {\n    auto begin = x.begin();\n    for (int i = 1; i < size; ++i) {\n      begin += n;\n    }\n    for (size_t i = 0; i < n; ++i) {\n      sum += *begin++;\n    }\n  }\n  int nthreads = omp_get_max_threads();\n  std::vector<std::vector<size_t>> counts(nthreads, std::vector<size_t>(4, 0));\n  #pragma omp parallel num_threads(nthreads)\n  {\n    auto& this_counts = counts[omp_get_thread_num()];\n    #pragma omp for\n    for (size_t i = 0; i < n; ++i) {\n      size_t t = static_cast<size_t>((x[i] - sum/size) / 0.25);\n      t = std::min(3, t);\n      this_counts[t]++;\n    }\n  }\n  MPI_Barrier(MPI_COMM_WORLD);\n  MPI_Reduce(counts[0].data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::vector<size_t> localBins;\n\n    // TODO\n    int nproc, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int n = x.size() / nproc;\n    int rem = x.size() % nproc;\n    int low = rank * n;\n    int high = low + n;\n\n    if (rank < rem)\n    {\n        high++;\n    }\n\n    std::vector<double> localVec(x.begin() + low, x.begin() + high);\n\n    localBins.resize(4);\n#pragma omp parallel for\n    for (int i = 0; i < localVec.size(); i++)\n    {\n        if (localVec[i] < 0.25)\n        {\n            localBins[0] += 1;\n        }\n        else if (localVec[i] < 0.5)\n        {\n            localBins[1] += 1;\n        }\n        else if (localVec[i] < 0.75)\n        {\n            localBins[2] += 1;\n        }\n        else\n        {\n            localBins[3] += 1;\n        }\n    }\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  size_t x_size = x.size();\n  double threshold_1 = 0.25 * size;\n  double threshold_2 = 0.50 * size;\n  double threshold_3 = 0.75 * size;\n\n  if (rank == 0) {\n    bins.fill(0);\n  }\n\n  // TODO: Your code here\n  // The last parameter to omp_get_num_threads should be a number that is a power of 2,\n  // which means the number of threads should be a power of 2 as well.\n  omp_set_num_threads(2);\n#pragma omp parallel for\n  for (size_t i = 0; i < x.size(); i++) {\n    double val = x[i];\n    int bin = 0;\n    if (val < threshold_1)\n      bin = 0;\n    else if (val < threshold_2)\n      bin = 1;\n    else if (val < threshold_3)\n      bin = 2;\n    else\n      bin = 3;\n#pragma omp atomic\n    bins[bin]++;\n  }\n\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0,\n             MPI_COMM_WORLD);\n}",
            "// Your code here.\n  // Hint: use a 4-element array to represent the histogram.\n  size_t length = x.size();\n  double length_r = length;\n\n  if (length == 0) {\n    bins = {0, 0, 0, 0};\n    return;\n  }\n\n  double h_size = (length_r / 4.0);\n  int *hist = new int[4]();\n\n  double *y = new double[length]();\n  double *z = new double[length]();\n  double *q = new double[length]();\n\n  #pragma omp parallel for\n  for (int i = 0; i < length; i++) {\n    y[i] = x[i] * 4.0;\n    z[i] = y[i] - (int)y[i];\n  }\n\n  int num_threads = omp_get_num_threads();\n  int chunk_size = (int)(length / num_threads);\n  int remaining_elements = length % num_threads;\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int tid = omp_get_thread_num();\n    int start = tid * chunk_size;\n\n    if (tid < remaining_elements) {\n      start += tid;\n      ++chunk_size;\n    }\n\n    for (int i = start; i < start + chunk_size; i++) {\n      int index = (int)z[i];\n      hist[index]++;\n    }\n  }\n\n  #pragma omp parallel for\n  for (int i = 0; i < 4; i++) {\n    q[i] = h_size * (i + 1);\n  }\n\n  #pragma omp parallel num_threads(num_threads)\n  {\n    int tid = omp_get_thread_num();\n    int start = tid * chunk_size;\n\n    if (tid < remaining_elements) {\n      start += tid;\n      ++chunk_size;\n    }\n\n    for (int i = start; i < start + chunk_size; i++) {\n      if (y[i] <= q[0])\n        bins[0] += hist[0];\n\n      else if (y[i] > q[3])\n        bins[3] += hist[3];\n\n      else {\n        int index = (int)y[i];\n        bins[index - 1] += hist[index - 1];\n      }\n    }\n  }\n\n  bins[0] += hist[0];\n  bins[3] += hist[3];\n\n  delete [] y;\n  delete [] z;\n  delete [] q;\n}",
            "// Compute the number of quartiles on each processor\n   size_t local_bins[4] = {0, 0, 0, 0};\n   #pragma omp parallel for\n   for (size_t i = 0; i < x.size(); ++i) {\n      if (x[i] >= 0.0 && x[i] <= 0.25) {\n         ++local_bins[0];\n      } else if (x[i] > 0.25 && x[i] <= 0.50) {\n         ++local_bins[1];\n      } else if (x[i] > 0.50 && x[i] <= 0.75) {\n         ++local_bins[2];\n      } else {\n         ++local_bins[3];\n      }\n   }\n\n   // Sum the bins across processors\n   #pragma omp parallel for\n   for (int i = 0; i < 4; ++i) {\n      MPI_Reduce(&local_bins[i], &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   }\n\n   // Compute the rank of each processor\n   std::vector<size_t> ranks(bins.size());\n   std::iota(ranks.begin(), ranks.end(), 0);\n\n   // Sort the ranks\n   std::sort(ranks.begin(), ranks.end(), [&](size_t a, size_t b) {\n      return bins[a] > bins[b];\n   });\n\n   // Set the output values\n   bins[ranks[0]] = 0;\n   bins[ranks[1]] = 1;\n   bins[ranks[2]] = 2;\n   bins[ranks[3]] = 3;\n}",
            "const auto n = x.size();\n    bins.fill(0);\n\n    const auto numChunks = omp_get_max_threads();\n    const auto chunkSize = (n + numChunks - 1) / numChunks;\n    // Divide x into numChunks chunks\n    std::vector<std::vector<double>> chunks;\n    for (int i = 0; i < numChunks; ++i) {\n        chunks.emplace_back(x.begin() + i * chunkSize, x.begin() + std::min((i + 1) * chunkSize, n));\n    }\n\n    // Each rank computes its own bins in parallel\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < numChunks; ++i) {\n        auto const chunk = chunks[i];\n        for (auto const& v : chunk) {\n            auto const f = std::floor(v / 0.25);\n            if (f < 0) {\n                bins[0]++;\n            }\n            else if (f < 4) {\n                bins[1]++;\n            }\n            else if (f < 8) {\n                bins[2]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n    }\n\n    // Rank 0 collects the bins on rank 0 and sends them to all other ranks\n    if (mpi_rank == 0) {\n        // Count the bins on rank 0\n        std::array<size_t, 4> localBins;\n        localBins.fill(0);\n        #pragma omp parallel for schedule(static)\n        for (int i = 0; i < numChunks; ++i) {\n            auto const chunk = chunks[i];\n            for (auto const& v : chunk) {\n                auto const f = std::floor(v / 0.25);\n                if (f < 0) {\n                    localBins[0]++;\n                }\n                else if (f < 4) {\n                    localBins[1]++;\n                }\n                else if (f < 8) {\n                    localBins[2]++;\n                }\n                else {\n                    localBins[3]++;\n                }\n            }\n        }\n\n        // Send the bins to all other ranks\n        std::vector<size_t> allBins(bins.size());\n        MPI_Allgather(localBins.data(), localBins.size(), MPI_UNSIGNED_LONG, allBins.data(), localBins.size(), MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n        bins = allBins;\n    }\n    else {\n        // Receive the bins from rank 0\n        MPI_Recv(bins.data(), bins.size(), MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n}",
            "size_t n = x.size();\n\tdouble const bin_width = 0.25;\n\n\t// Create a new vector to store the bin counts\n\tstd::vector<size_t> bin_counts(n);\n\n\t// Determine the bin number for each double in the vector\n\t// Use a parallelized algorithm to count the number of values\n\t// in each bin\n\t#pragma omp parallel for\n\tfor (size_t i = 0; i < n; ++i) {\n\t\t// determine which bin this value belongs to\n\t\tint bin_num = (int) ((x[i] / bin_width) * 4);\n\t\tbin_counts[i] = bin_num + 1;\n\t}\n\n\t// Use MPI to perform a reduction on each rank to\n\t// sum up the values in the bins\n\tint mpi_err = MPI_Allreduce(bin_counts.data(), bins.data(), n, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\tif (mpi_err!= MPI_SUCCESS) {\n\t\tthrow std::runtime_error(\"Error performing MPI_Allreduce\");\n\t}\n}",
            "// TODO: implement\n  // TODO: test\n}",
            "//...\n}",
            "// TODO\n    bins = {0, 0, 0, 0};\n\n    int numThreads = omp_get_max_threads();\n    int numProcesses = 0;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    size_t length = x.size();\n    int blockSize = length / numProcesses;\n    int extra = length % numProcesses;\n\n    std::vector<size_t> localBins(numThreads * 4);\n    std::vector<size_t> sendCounts(numProcesses);\n    std::vector<size_t> recvCounts(numProcesses);\n    std::vector<size_t> sendOffsets(numProcesses);\n    std::vector<size_t> recvOffsets(numProcesses);\n\n    for (int i = 0; i < numProcesses; i++) {\n        sendCounts[i] = blockSize;\n        sendOffsets[i] = i * blockSize;\n        if (i < extra) {\n            sendCounts[i]++;\n        }\n    }\n\n    MPI_Scatter(sendCounts.data(), 1, MPI_INT, recvCounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < numProcesses; i++) {\n        recvOffsets[i] = 0;\n    }\n\n    for (int i = 0; i < numProcesses - 1; i++) {\n        recvOffsets[i + 1] = recvOffsets[i] + recvCounts[i];\n    }\n\n    double totalElements = 0;\n    MPI_Reduce(&length, &totalElements, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    size_t start = 0;\n    for (int i = 0; i < numThreads; i++) {\n        double startPart = omp_get_wtime();\n        size_t end = start + recvCounts[rank] / numThreads;\n        int tid = omp_get_thread_num();\n        size_t offset = recvOffsets[rank] + tid * recvCounts[rank] / numThreads;\n\n        int localBinsSize = 0;\n        #pragma omp parallel for\n        for (int j = start; j < end; j++) {\n            int bin = 0;\n            if (x[j] >= 0.75) {\n                bin = 3;\n            } else if (x[j] >= 0.5) {\n                bin = 2;\n            } else if (x[j] >= 0.25) {\n                bin = 1;\n            } else {\n                bin = 0;\n            }\n\n            localBins[tid * 4 + bin]++;\n        }\n        #pragma omp barrier\n\n        MPI_Gather(&localBins[tid * 4], 4, MPI_INT, &localBins[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n        double endPart = omp_get_wtime();\n\n        #pragma omp critical\n        {\n            double duration = endPart - startPart;\n            printf(\"Thread %d took %f\\n\", tid, duration);\n        }\n\n        if (rank == 0) {\n            for (int j = 0; j < 4; j++) {\n                bins[j] += localBins[j];\n            }\n        }\n    }\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] /= totalElements;\n        }\n    }\n}",
            "// TODO: complete this function!\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n    const int N = x.size();\n    const int M = 4;\n\n    // Counting\n    double* count_all = new double[M];\n    for (int i = 0; i < M; i++)\n        count_all[i] = 0;\n\n    #pragma omp parallel for\n    for (int i = 0; i < N; i++) {\n        double val = x[i];\n        if (val >= 0 && val < 0.25) {\n            count_all[0]++;\n        } else if (val >= 0.25 && val < 0.5) {\n            count_all[1]++;\n        } else if (val >= 0.5 && val < 0.75) {\n            count_all[2]++;\n        } else if (val >= 0.75 && val <= 1) {\n            count_all[3]++;\n        }\n    }\n\n    // Reduce\n    MPI_Reduce(count_all, bins.data(), M, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    delete[] count_all;\n}",
            "#pragma omp parallel num_threads(2)\n    {\n        int rank, nprocs;\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n        if (rank == 0) {\n            std::array<size_t, 4> bins_local = {0, 0, 0, 0};\n            int chunk_size = x.size() / nprocs;\n            int offset = 0;\n            #pragma omp parallel for num_threads(2)\n            for (int proc = 1; proc < nprocs; ++proc) {\n                std::array<size_t, 4> bins_local_proc = {0, 0, 0, 0};\n                for (size_t i = offset; i < offset + chunk_size; ++i) {\n                    if (x[i] < 0.25) {\n                        bins_local_proc[0]++;\n                    } else if (x[i] < 0.5) {\n                        bins_local_proc[1]++;\n                    } else if (x[i] < 0.75) {\n                        bins_local_proc[2]++;\n                    } else {\n                        bins_local_proc[3]++;\n                    }\n                }\n                for (int i = 0; i < 4; ++i) {\n                    bins_local[i] += bins_local_proc[i];\n                }\n                offset += chunk_size;\n            }\n            #pragma omp parallel for num_threads(2)\n            for (size_t i = offset; i < x.size(); ++i) {\n                if (x[i] < 0.25) {\n                    bins_local[0]++;\n                } else if (x[i] < 0.5) {\n                    bins_local[1]++;\n                } else if (x[i] < 0.75) {\n                    bins_local[2]++;\n                } else {\n                    bins_local[3]++;\n                }\n            }\n            MPI_Gather(bins_local.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n        } else {\n            std::array<size_t, 4> bins_local = {0, 0, 0, 0};\n            int chunk_size = x.size() / nprocs;\n            int offset = chunk_size * rank;\n            for (int proc = 1; proc < nprocs; ++proc) {\n                #pragma omp parallel for num_threads(2)\n                for (size_t i = offset; i < offset + chunk_size; ++i) {\n                    if (x[i] < 0.25) {\n                        bins_local[0]++;\n                    } else if (x[i] < 0.5) {\n                        bins_local[1]++;\n                    } else if (x[i] < 0.75) {\n                        bins_local[2]++;\n                    } else {\n                        bins_local[3]++;\n                    }\n                }\n                offset += chunk_size;\n            }\n            for (size_t i = offset; i < x.size(); ++i) {\n                if (x[i] < 0.25) {\n                    bins_local[0]++;\n                } else if (x[i] < 0.5) {\n                    bins_local[1]++;\n                } else if (x[i] < 0.75) {\n                    bins_local[2]++;\n                } else {\n                    bins_local[3]++;\n                }\n            }\n            MPI_Gather(bins_local.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n        }\n    }\n}",
            "// Count the number of doubles in each partition using MPI\n    MPI_Comm_size(MPI_COMM_WORLD, &(bins[0]));\n    MPI_Comm_rank(MPI_COMM_WORLD, &(bins[1]));\n    // Initialize number of doubles in each partition to zero\n    // Allocate space to hold number of doubles in each partition\n    bins[2] = 0;\n    bins[3] = 0;\n    double* bins_temp = new double[bins[0]];\n    // Send number of doubles in each partition to each rank\n    MPI_Allgather(&bins[2], 1, MPI_INT, bins_temp, 1, MPI_INT, MPI_COMM_WORLD);\n    // Count doubles in each partition\n    for(size_t i = 0; i < bins[0]; i++) {\n        bins[2] += bins_temp[i];\n    }\n    delete[] bins_temp;\n    bins_temp = NULL;\n    // Send number of doubles in each partition to each rank\n    MPI_Allgather(&bins[3], 1, MPI_INT, bins_temp, 1, MPI_INT, MPI_COMM_WORLD);\n    // Count doubles in each partition\n    for(size_t i = 0; i < bins[0]; i++) {\n        bins[3] += bins_temp[i];\n    }\n    delete[] bins_temp;\n    bins_temp = NULL;\n\n    // Compute the number of quartiles in each partition using OpenMP\n    bins[2] = 0;\n    bins[3] = 0;\n    // Iterate over the number of doubles\n    // OpenMP is used for the for loop. The number of iterations is the number of doubles\n    // Since the number of iterations is not known until the loop is running,\n    // the iteration space must be declared outside of the loop\n    size_t num_doubles = x.size();\n    #pragma omp parallel num_threads(bins[0])\n    {\n        // Initialize the number of doubles in the current partition to zero\n        size_t num_doubles_curr_partition = 0;\n        #pragma omp for schedule(static)\n        for(size_t i = 0; i < num_doubles; i++) {\n            // If the current double's fractional part is between 0 and 0.25\n            // then increase the number of doubles in the current partition\n            if(x[i] >= 0.0 && x[i] < 0.25) {\n                num_doubles_curr_partition++;\n            }\n        }\n        #pragma omp critical\n        {\n            // Increase the number of doubles in the current partition\n            // on rank 0\n            if(bins[1] == 0) {\n                bins[2] += num_doubles_curr_partition;\n            }\n        }\n        #pragma omp barrier\n        // Broadcast the number of doubles in the current partition from rank 0\n        // to all other ranks\n        if(bins[1] == 0) {\n            MPI_Bcast(&(bins[2]), 1, MPI_INT, 0, MPI_COMM_WORLD);\n        }\n        #pragma omp barrier\n        // Iterate over the number of doubles\n        #pragma omp for schedule(static)\n        for(size_t i = 0; i < num_doubles; i++) {\n            // If the current double's fractional part is between 0 and 0.25\n            // then increase the number of doubles in the current partition\n            if(x[i] >= 0.0 && x[i] < 0.25) {\n                num_doubles_curr_partition++;\n            }\n        }\n        #pragma omp critical\n        {\n            // Increase the number of doubles in the current partition\n            // on rank 0\n            if(bins[1] == 0) {\n                bins[2] += num_doubles_curr_partition;\n            }\n        }\n        #pragma omp barrier\n        // Broadcast the number of doubles in the current partition from rank 0\n        // to all other ranks\n        if(bins[1] == 0) {\n            MPI_Bcast(&(bins[2]), 1, MPI_INT, 0, MPI_COMM_WORLD);\n        }\n        #pragma omp barrier\n        // Initialize the number of doubles in the current partition to zero\n        num_doubles_curr_partition = 0;\n        // Iterate over the number of doubles\n        #pragma omp for schedule(static)\n        for(size_t i = 0; i < num_doubles; i++) {\n            // If the current double's fractional part is between 0.25 and 0.5\n            // then increase the number",
            "// TODO: Your code here\n}",
            "if (x.size() < 10) {\n    std::cout << \"You need at least 10 elements in the input vector\" << std::endl;\n    return;\n  }\n  int rank = 0;\n  int size = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // determine the number of doubles in each rank\n  int n = x.size() / size;\n  // determine the remainder\n  int remainder = x.size() % size;\n  // adjust the count if this rank has a remainder\n  if (rank < remainder)\n    n += 1;\n\n  // distribute the workload\n  std::vector<double> local_x(n);\n  MPI_Scatter(&x[0], n, MPI_DOUBLE, &local_x[0], n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  // count the number of doubles in each bin\n  std::array<size_t, 4> local_bins;\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (local_x[i] <= 0.25)\n      local_bins[0]++;\n    else if (local_x[i] <= 0.5)\n      local_bins[1]++;\n    else if (local_x[i] <= 0.75)\n      local_bins[2]++;\n    else\n      local_bins[3]++;\n  }\n\n  // reduce the counts on rank 0\n  MPI_Reduce(&local_bins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n}",
            "// TODO: your code goes here.\n    // Use a stride of 4.\n    size_t size = x.size();\n    size_t stride = 4;\n    std::vector<double> x_sub(stride);\n    std::vector<int> ranks(size);\n\n    for (int i = 0; i < stride; i++) {\n        x_sub[i] = x[i];\n    }\n    for (int i = 0; i < size; i++) {\n        ranks[i] = i % 4;\n    }\n\n    int rank;\n    int size_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size_rank);\n\n    MPI_Scatter(x_sub.data(), stride, MPI_DOUBLE, x_sub.data(), stride, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(ranks.data(), stride, MPI_INT, ranks.data(), stride, MPI_INT, 0, MPI_COMM_WORLD);\n\n    for (int i = 0; i < stride; i++) {\n        if (x_sub[i] < 0.25) {\n            bins[0] += 1;\n        } else if (x_sub[i] >= 0.25 && x_sub[i] < 0.5) {\n            bins[1] += 1;\n        } else if (x_sub[i] >= 0.5 && x_sub[i] < 0.75) {\n            bins[2] += 1;\n        } else if (x_sub[i] >= 0.75) {\n            bins[3] += 1;\n        }\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "//TODO\n    size_t n = x.size();\n    // TODO: allocate the array bins\n\n#pragma omp parallel\n{\n    // TODO: create the thread private variables, local variables, and array for holding \n    // the results from each thread\n\n    // TODO: split the work\n\n    // TODO: count the elements in each bin using a parallel for loop\n}\n}",
            "// TODO: Implement this function.\n\tint size, rank, thread_id;\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tomp_set_num_threads(10);\n\tdouble lower = 0.0, upper = 0.25;\n\tint upper_count = 0;\n\tint rank_count = 0;\n\tint global_upper_count = 0;\n\t#pragma omp parallel for shared(global_upper_count) private(thread_id) reduction(+:upper_count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tthread_id = omp_get_thread_num();\n\t\tif (x[i] >= lower && x[i] < upper) {\n\t\t\tupper_count++;\n\t\t}\n\t}\n\tMPI_Allreduce(&upper_count, &global_upper_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tbins[0] = global_upper_count;\n\tupper = 0.5;\n\tlower = 0.25;\n\tupper_count = 0;\n\t#pragma omp parallel for shared(global_upper_count) private(thread_id) reduction(+:upper_count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tthread_id = omp_get_thread_num();\n\t\tif (x[i] >= lower && x[i] < upper) {\n\t\t\tupper_count++;\n\t\t}\n\t}\n\tMPI_Allreduce(&upper_count, &global_upper_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tbins[1] = global_upper_count;\n\tupper = 0.75;\n\tlower = 0.5;\n\tupper_count = 0;\n\t#pragma omp parallel for shared(global_upper_count) private(thread_id) reduction(+:upper_count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tthread_id = omp_get_thread_num();\n\t\tif (x[i] >= lower && x[i] < upper) {\n\t\t\tupper_count++;\n\t\t}\n\t}\n\tMPI_Allreduce(&upper_count, &global_upper_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tbins[2] = global_upper_count;\n\tupper = 1.0;\n\tlower = 0.75;\n\tupper_count = 0;\n\t#pragma omp parallel for shared(global_upper_count) private(thread_id) reduction(+:upper_count)\n\tfor (size_t i = 0; i < x.size(); i++) {\n\t\tthread_id = omp_get_thread_num();\n\t\tif (x[i] >= lower && x[i] < upper) {\n\t\t\tupper_count++;\n\t\t}\n\t}\n\tMPI_Allreduce(&upper_count, &global_upper_count, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\tbins[3] = global_upper_count;\n}",
            "size_t n = x.size();\n    double x_sum = std::accumulate(x.begin(), x.end(), 0.0);\n\n    double epsilon = 1e-10;\n\n    double sum1 = 0;\n    double sum2 = 0;\n    double sum3 = 0;\n    double sum4 = 0;\n\n    double start = omp_get_wtime();\n    #pragma omp parallel num_threads(4)\n    {\n        #pragma omp for\n        for (size_t i = 0; i < n; i++) {\n            double x_i = x[i];\n            if (x_i < x_sum / 4) {\n                sum1 += x_i;\n            } else if (x_i < 2 * x_sum / 4) {\n                sum2 += x_i;\n            } else if (x_i < 3 * x_sum / 4) {\n                sum3 += x_i;\n            } else {\n                sum4 += x_i;\n            }\n        }\n    }\n    double end = omp_get_wtime();\n    std::cout << \"Time taken to compute quartiles by OpenMP: \" << end - start << std::endl;\n\n    bins[0] = sum1 / (x_sum / 4);\n    bins[1] = sum2 / (x_sum / 4);\n    bins[2] = sum3 / (x_sum / 4);\n    bins[3] = sum4 / (x_sum / 4);\n}",
            "// TODO: implement this\n}",
            "if (x.size() < 4) {\n    return;\n  }\n\n  // TODO\n}",
            "// TODO: implement this function\n    size_t length = x.size();\n    int rank;\n    int n;\n    double part;\n    std::vector<int> bin_sizes;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n\n    bin_sizes.push_back((int) length/4);\n    bin_sizes.push_back((int) length/4);\n    bin_sizes.push_back((int) length/4);\n    bin_sizes.push_back((int) length/4);\n\n    std::vector<double> bin_array;\n    std::vector<double> my_bin;\n\n    if (rank == 0) {\n        bin_array.assign(n * bin_sizes[0], 0);\n    }\n    else {\n        bin_array.assign(bin_sizes[0], 0);\n    }\n\n    double start = omp_get_wtime();\n    // split data\n    if (rank == 0) {\n        for (int i = 1; i < n; i++) {\n            MPI_Send(&x[0], bin_sizes[0], MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&x[0], bin_sizes[0], MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    // count\n    for (int i = 0; i < length; i++) {\n        part = x[i] / 4;\n        my_bin[(int) part] += 1;\n    }\n\n    // merge\n    MPI_Reduce(&my_bin[0], &bin_array[0], bin_sizes[0], MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n    double end = omp_get_wtime();\n    printf(\"%.20f\\n\", end - start);\n\n    // sort\n    if (rank == 0) {\n        std::sort(bin_array.begin(), bin_array.end(), std::greater<double>());\n    }\n\n    // send\n    if (rank == 0) {\n        for (int i = 1; i < n; i++) {\n            MPI_Send(&bin_array[0], bin_sizes[0], MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&bin_array[0], bin_sizes[0], MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // sort\n    std::sort(bin_array.begin(), bin_array.end(), std::greater<double>());\n\n    // send\n    if (rank == 0) {\n        for (int i = 1; i < n; i++) {\n            MPI_Send(&bin_array[0], bin_sizes[1], MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&bin_array[0], bin_sizes[1], MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // sort\n    std::sort(bin_array.begin(), bin_array.end(), std::greater<double>());\n\n    // send\n    if (rank == 0) {\n        for (int i = 1; i < n; i++) {\n            MPI_Send(&bin_array[0], bin_sizes[2], MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&bin_array[0], bin_sizes[2], MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // sort\n    std::sort(bin_array.begin(), bin_array.end(), std::greater<double>());\n\n    // send\n    if (rank == 0) {\n        for (int i = 1; i < n; i++) {\n            MPI_Send(&bin_array[0], bin_sizes[3], MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        MPI_Status status;\n        MPI_Recv(&bin_array[0], bin_sizes[3], MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    }\n\n    // sort\n    std::sort(bin_array.begin(), bin_array",
            "// TODO: implement this function\n\n  // MPI rank\n  int my_rank;\n\n  // Number of processes\n  int num_procs;\n\n  // Number of elements to be divided\n  size_t elements;\n\n  // Partition size\n  int partition_size;\n\n  // Number of elements in this process\n  int local_size;\n\n  // Start index of the partition\n  int local_start;\n\n  // End index of the partition\n  int local_end;\n\n  // Partition index in the global array\n  int global_index;\n\n  // Count of numbers in each quartile\n  std::array<size_t, 4> local_bins;\n\n  // Get MPI rank\n  MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n  // Get number of processes\n  MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n  // Get number of elements\n  elements = x.size();\n\n  // Get partition size\n  partition_size = elements / num_procs;\n\n  // Get local size\n  local_size = partition_size;\n\n  // Get local start index\n  local_start = my_rank * partition_size;\n\n  // Get local end index\n  local_end = local_start + partition_size;\n\n  // Count numbers in each quartile\n  for (int i = local_start; i < local_end; i++) {\n\n    // Get global index\n    global_index = i;\n\n    // Get fractional part of number\n    double frac = x[global_index] - std::floor(x[global_index]);\n\n    // Count number\n    if (frac < 0.25)\n      local_bins[0]++;\n    else if (frac < 0.5)\n      local_bins[1]++;\n    else if (frac < 0.75)\n      local_bins[2]++;\n    else\n      local_bins[3]++;\n  }\n\n  // Gather local bin counts to process 0\n  MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // Calculate quartiles\n  for (int i = 0; i < 4; i++)\n    bins[i] /= num_procs;\n}",
            "int world_size, world_rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n  // Split x into local chunks\n  size_t length = x.size();\n  std::vector<size_t> local_length(world_size);\n  std::vector<size_t> local_offsets(world_size);\n  std::vector<double> local_x(length);\n  MPI_Scatter(x.data(), length, MPI_DOUBLE, local_x.data(), length, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  size_t local_total = 0;\n  for (int i = 0; i < world_size; ++i) {\n    local_length[i] = 0;\n    local_offsets[i] = 0;\n    if (i == world_rank) {\n      for (size_t j = 0; j < length; ++j) {\n        if (local_x[j] >= (double(j) / double(length)) * 0.25 &&\n            local_x[j] < (double(j) / double(length)) * 0.5) {\n          ++local_length[i];\n        }\n        else if (local_x[j] >= (double(j) / double(length)) * 0.5 &&\n                 local_x[j] < (double(j) / double(length)) * 0.75) {\n          ++local_length[i];\n        }\n        else if (local_x[j] >= (double(j) / double(length)) * 0.75) {\n          ++local_length[i];\n        }\n      }\n    }\n    MPI_Reduce(&local_length[i], &local_total, 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n\n  std::vector<size_t> local_bin(4);\n  if (world_rank == 0) {\n    std::vector<std::vector<size_t>> local_bin_local(world_size);\n    #pragma omp parallel\n    #pragma omp for\n    for (size_t i = 0; i < length; ++i) {\n      if (local_x[i] >= (double(i) / double(length)) * 0.25 &&\n          local_x[i] < (double(i) / double(length)) * 0.5) {\n        local_bin_local[0].push_back(i);\n      }\n      else if (local_x[i] >= (double(i) / double(length)) * 0.5 &&\n               local_x[i] < (double(i) / double(length)) * 0.75) {\n        local_bin_local[1].push_back(i);\n      }\n      else if (local_x[i] >= (double(i) / double(length)) * 0.75) {\n        local_bin_local[2].push_back(i);\n      }\n    }\n\n    // Sum up the counts from every rank to get the global bin counts\n    std::vector<size_t> global_bin(4);\n    for (int i = 0; i < world_size; ++i) {\n      global_bin[0] += local_bin_local[i].size();\n      global_bin[1] += local_bin_local[i].size();\n      global_bin[2] += local_bin_local[i].size();\n      global_bin[3] += local_bin_local[i].size();\n    }\n\n    MPI_Gather(global_bin.data(), 4, MPI_UNSIGNED_LONG_LONG, local_bin.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n  else {\n    MPI_Gather(local_length.data(), 1, MPI_UNSIGNED_LONG_LONG, local_bin.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n\n  // Accumulate the bin counts locally\n  size_t global_bin = 0;\n  for (int i = 0; i < world_size; ++i) {\n    if (i == world_rank) {\n      bins[0] += local_bin[0];\n      bins[1] += local_bin[1];\n      bins[2] += local_bin[2];\n      bins[3] += local_bin[3",
            "double start = omp_get_wtime();\n    int n_ranks = 0;\n    int rank = 0;\n\n    // Get the number of ranks\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n    // Get the rank of the process\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Calculate the number of bins\n    int n_bins = (int) x.size();\n    int n_per_rank = n_bins / n_ranks;\n    int remainder = n_bins % n_ranks;\n\n    // Calculate the starting index of the current rank\n    int rank_start = rank * n_per_rank;\n\n    // Calculate the starting index of the next rank\n    int next_rank_start = rank_start + n_per_rank + remainder;\n\n    // Initialize the counters\n    bins = {0, 0, 0, 0};\n\n    // Iterate over the vector\n    #pragma omp parallel for schedule(static)\n    for (int i = rank_start; i < next_rank_start; i++) {\n        double const& val = x[i];\n        // Determine which bin the value is in\n        int bin = 0;\n        if (val < 0.25) {\n            bin = 0;\n        } else if (val >= 0.25 && val < 0.5) {\n            bin = 1;\n        } else if (val >= 0.5 && val < 0.75) {\n            bin = 2;\n        } else {\n            bin = 3;\n        }\n        // Increment the corresponding bin\n        #pragma omp atomic\n        bins[bin] += 1;\n    }\n\n    // Reduce the counts across all ranks\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        double end = omp_get_wtime();\n        double elapsed = end - start;\n        std::cout << \"Took \" << elapsed << \" seconds\" << std::endl;\n    }\n}",
            "// TODO: compute number of elements in each bin using parallel for\n    //...\n    // TODO: sum the bins from each rank using MPI reduce\n    //...\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins[0]);\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins[1]);\n\n  std::sort(x.begin(), x.end());\n\n  int n = x.size();\n  size_t n_loc = n / bins[0];\n  int start = n_loc * bins[1];\n  int end = start + n_loc;\n\n  #pragma omp parallel\n  {\n    double local_bins[4] = {0};\n    #pragma omp for schedule(static)\n    for (int i = start; i < end; i++) {\n      double frac = x[i] - floor(x[i]);\n      if (frac < 0.25) {\n        local_bins[0]++;\n      } else if (frac < 0.5) {\n        local_bins[1]++;\n      } else if (frac < 0.75) {\n        local_bins[2]++;\n      } else {\n        local_bins[3]++;\n      }\n    }\n    #pragma omp critical\n    {\n      for (int i = 0; i < 4; i++) {\n        bins[2+i] += local_bins[i];\n      }\n    }\n  }\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      bins.fill(0);\n   }\n\n   // TODO: implement this function\n}",
            "// write your code here\n\n}",
            "// TODO: Compute the counts of each bin.\n}",
            "// MPI code goes here\n\n  // OpenMP code goes here\n}",
            "int n = x.size();\n    int nprocs, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    size_t chunksize = n / nprocs;\n    size_t chunkstart = chunksize * rank;\n    size_t chunkend = (rank == nprocs-1)? n : chunksize * (rank+1);\n\n    bins = {};\n    std::vector<size_t> local_counts(4);\n#pragma omp parallel\n    {\n#pragma omp for reduction(+:local_counts) schedule(static)\n        for (int i = chunkstart; i < chunkend; ++i) {\n            double xval = x[i];\n            if (xval <= 0.25) {\n                ++local_counts[0];\n            } else if (xval <= 0.5) {\n                ++local_counts[1];\n            } else if (xval <= 0.75) {\n                ++local_counts[2];\n            } else {\n                ++local_counts[3];\n            }\n        }\n    }\n\n    std::array<size_t, 4> local_sums = {0, 0, 0, 0};\n    MPI_Allreduce(local_counts.data(), local_sums.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = local_sums;\n    }\n}",
            "const double quarter = 0.25;\n  const double one_quarter = 1.0 - quarter;\n\n  // write your solution here\n  size_t n = x.size();\n  size_t m = n / 4;\n  size_t start = m * omp_get_thread_num();\n  size_t end = m * omp_get_thread_num() + m;\n  size_t local_count = 0;\n\n  if (start < n) {\n    for (size_t i = start; i < end; ++i) {\n      if (x[i] < quarter) {\n        ++local_count;\n      } else if (x[i] < one_quarter) {\n        local_count += 2;\n      } else {\n        local_count += 3;\n      }\n    }\n  }\n\n  bins[0] = local_count;\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t n = x.size();\n\n    // Initialize bins on all ranks to 0\n    std::array<size_t, 4> partial_bins;\n    std::fill(partial_bins.begin(), partial_bins.end(), 0);\n\n    // Compute the number of bins each rank should compute\n    int n_bins = n / omp_get_num_procs();\n\n    #pragma omp parallel num_threads(omp_get_num_procs())\n    {\n        // Find each rank's chunk of x\n        int rank = omp_get_thread_num();\n        std::vector<double> local_x(x.begin() + rank * n_bins, x.begin() + std::min(rank * n_bins + n_bins, n));\n\n        // Loop through the local x and update the bins\n        for (double xi : local_x) {\n            double frac = fmod(xi, 1.0);\n            if (frac < 0.25) {\n                partial_bins[0]++;\n            } else if (frac < 0.50) {\n                partial_bins[1]++;\n            } else if (frac < 0.75) {\n                partial_bins[2]++;\n            } else {\n                partial_bins[3]++;\n            }\n        }\n    }\n\n    // Gather the partial sums of bins to get the total counts\n    MPI_Gather(partial_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, nranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  \n  double chunksize = 1.0 * x.size() / nranks;\n  std::vector<double> sublist(x.begin() + chunksize * rank, x.begin() + chunksize * (rank + 1));\n  \n  #pragma omp parallel for\n  for (int i = 0; i < sublist.size(); i++) {\n    if (sublist[i] <= 0.25 * chunksize) {\n      bins[0]++;\n    } else if (sublist[i] <= 0.5 * chunksize) {\n      bins[1]++;\n    } else if (sublist[i] <= 0.75 * chunksize) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n\n  if (rank == 0) {\n    bins[0] = bins[0] / (0.25 * chunksize) * x.size();\n    bins[1] = bins[1] / (0.50 * chunksize) * x.size();\n    bins[2] = bins[2] / (0.75 * chunksize) * x.size();\n    bins[3] = bins[3] / (1.00 * chunksize) * x.size();\n  }\n}",
            "int num_threads = omp_get_max_threads();\n    int num_processes = 1;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_processes);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    if (rank == 0) {\n        std::vector<size_t> local_bins(num_threads * 4);\n        std::vector<double> local_x(x.size());\n        std::vector<double> thread_x(x.size() / num_threads);\n\n        for (int i = 0; i < num_threads; i++) {\n            // get the start and end of the thread's chunk of x\n            int start = i * (x.size() / num_threads);\n            int end = (i + 1) * (x.size() / num_threads);\n            if (i == num_threads - 1) {\n                end = x.size();\n            }\n\n            // copy x to local_x\n            for (int j = start; j < end; j++) {\n                local_x[j] = x[j];\n            }\n\n            // count elements with fractions in the quartiles\n            #pragma omp parallel for\n            for (int j = 0; j < 4; j++) {\n                size_t count = 0;\n                int num_elements = local_x.size() / 4;\n                double min_val = j * num_elements / 4;\n                double max_val = (j + 1) * num_elements / 4;\n                for (int k = 0; k < num_elements; k++) {\n                    if (local_x[k] >= min_val && local_x[k] < max_val) {\n                        count++;\n                    }\n                }\n                local_bins[j + i * 4] = count;\n            }\n        }\n\n        // combine the counts from each thread\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        // get the thread number\n        int thread_num = rank;\n        std::vector<double> local_x(x.size() / num_processes);\n        std::vector<size_t> local_bins(4);\n        // copy x to local_x\n        for (int i = 0; i < local_x.size(); i++) {\n            local_x[i] = x[i * num_processes + thread_num];\n        }\n\n        // count elements with fractions in the quartiles\n        for (int i = 0; i < 4; i++) {\n            size_t count = 0;\n            int num_elements = local_x.size() / 4;\n            double min_val = i * num_elements / 4;\n            double max_val = (i + 1) * num_elements / 4;\n            for (int j = 0; j < num_elements; j++) {\n                if (local_x[j] >= min_val && local_x[j] < max_val) {\n                    count++;\n                }\n            }\n            local_bins[i] = count;\n        }\n\n        MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO: Your code here\n\n}",
            "// TODO: implement this function\n    size_t num_threads = omp_get_max_threads();\n    size_t num_threads_per_node = 2;\n    int my_node_rank = 0;\n    int num_nodes = 1;\n    double my_rank = 0;\n    double num_ranks = 1;\n    int my_thread_rank = 0;\n    int num_threads_per_rank = 1;\n    int num_ranks_per_node = 1;\n\n    double start_time = MPI_Wtime();\n    int num_ranks_per_thread = num_ranks / num_nodes / num_threads_per_node;\n    int num_threads_per_node_per_thread = num_threads_per_node / num_threads;\n    int num_nodes_per_rank = num_nodes / num_ranks;\n    int num_threads_per_rank_per_thread = num_threads_per_rank / num_threads;\n    int num_ranks_per_node_per_thread = num_ranks_per_node / num_threads;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, (int*)&my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, (int*)&num_ranks);\n    MPI_Comm_split_type(MPI_COMM_WORLD, MPI_COMM_TYPE_SHARED, 0, MPI_INFO_NULL, &MPI_COMM_NODE);\n    MPI_Comm_size(MPI_COMM_NODE, (int*)&num_nodes);\n    MPI_Comm_rank(MPI_COMM_NODE, (int*)&my_node_rank);\n\n    omp_set_num_threads(num_threads);\n    double thread_start_time = MPI_Wtime();\n    #pragma omp parallel default(none) shared(x, bins, num_ranks_per_thread, num_threads_per_node_per_thread, num_nodes_per_rank, num_threads_per_rank_per_thread, num_ranks_per_node_per_thread, my_node_rank, num_nodes, my_rank, num_ranks, my_thread_rank, num_threads, num_threads_per_rank) \n    {\n        //int my_node_rank = 0;\n        //int num_nodes = 1;\n        //int my_thread_rank = omp_get_thread_num();\n        //int num_threads = omp_get_num_threads();\n        int my_rank = my_node_rank * num_nodes + my_thread_rank;\n        int num_ranks_per_thread = num_ranks / num_nodes / num_threads_per_node;\n        int num_threads_per_node_per_thread = num_threads_per_node / num_threads;\n        int num_nodes_per_rank = num_nodes / num_ranks;\n        int num_threads_per_rank_per_thread = num_threads_per_rank / num_threads;\n        int num_ranks_per_node_per_thread = num_ranks_per_node / num_threads;\n        \n        int my_rank_per_thread = my_rank % num_ranks_per_thread;\n        int my_rank_per_node = my_rank / num_ranks_per_thread;\n        int my_rank_per_thread_per_node = my_rank_per_node % num_ranks_per_node_per_thread;\n        int my_rank_per_thread_per_rank = my_rank_per_node / num_ranks_per_node_per_thread;\n\n        //std::cout << \"my_rank: \" << my_rank << \" my_rank_per_thread: \" << my_rank_per_thread << \" my_rank_per_thread_per_node: \" << my_rank_per_thread_per_node << \" my_rank_per_thread_per_rank: \" << my_rank_per_thread_per_rank << std::endl;\n\n        int my_node_rank_per_thread = my_node_rank % num_threads_per_node_per_thread;\n        int my_node_rank_per_thread_per_rank = my_node_rank / num_threads_per_node_per_thread;\n\n        int num_threads_per_rank_per_node = num_threads_per_rank_per_thread * num_threads_per_node_per_thread;\n        int num_ranks_per_node_per_thread = num_ranks_per_",
            "// TODO: your code here\n}",
            "std::vector<double> localBins = {0, 0, 0, 0};\n\n\t#pragma omp parallel for schedule(static)\n\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\tdouble const x_i = x[i];\n\t\tif (x_i < 0.25) {\n\t\t\t++localBins[0];\n\t\t} else if (x_i < 0.5) {\n\t\t\t++localBins[1];\n\t\t} else if (x_i < 0.75) {\n\t\t\t++localBins[2];\n\t\t} else {\n\t\t\t++localBins[3];\n\t\t}\n\t}\n\n\t// Combine the partial sums from each thread into a single vector\n\tstd::vector<double> globalBins(bins.size(), 0);\n\tMPI_Allreduce(localBins.data(), globalBins.data(), bins.size(), MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);\n\n\t// Copy the result to bins\n\tif (MPI_PROC_NULL == MPI_PROC_NULL) {\n\t\tbins = {0, 0, 0, 0};\n\t} else {\n\t\tbins = {static_cast<size_t>(globalBins[0]), static_cast<size_t>(globalBins[1]), static_cast<size_t>(globalBins[2]), static_cast<size_t>(globalBins[3])};\n\t}\n}",
            "size_t len = x.size();\n    size_t num_threads = omp_get_max_threads();\n    size_t num_ranks = len / num_threads;\n    bins.fill(0);\n\n    #pragma omp parallel\n    {\n        int rank = omp_get_thread_num();\n        size_t start_index = rank * num_ranks;\n        size_t end_index = (rank + 1) * num_ranks;\n        end_index = end_index > len? len : end_index;\n        for(size_t i = start_index; i < end_index; i++) {\n            double val = x[i];\n            double frac = val - std::floor(val);\n            if (frac < 0.25) {\n                bins[0]++;\n            } else if (frac < 0.5) {\n                bins[1]++;\n            } else if (frac < 0.75) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n\n    std::vector<size_t> bins_per_rank(num_ranks);\n    MPI_Gather(&bins, num_ranks, MPI_INT, bins_per_rank.data(), num_ranks, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        size_t start_index = 0;\n        for(size_t i = 0; i < num_ranks; i++) {\n            size_t end_index = start_index + bins_per_rank[i];\n            for(size_t j = start_index; j < end_index; j++) {\n                bins[j] = bins_per_rank[i];\n            }\n            start_index = end_index;\n        }\n    }\n}",
            "// You will need to add code here\n}",
            "// Your code here.\n\n}",
            "// TODO: Your code here.\n}",
            "size_t num_el = x.size();\n  size_t num_threads = 8;\n\n  #pragma omp parallel for schedule(static) num_threads(num_threads)\n  for (size_t i = 0; i < num_el; ++i) {\n    double y = x[i];\n    if (y < 0.25) bins[0]++;\n    else if (y < 0.5) bins[1]++;\n    else if (y < 0.75) bins[2]++;\n    else bins[3]++;\n  }\n}",
            "const size_t N = x.size();\n  std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n  \n  // YOUR CODE HERE\n\n  // END YOUR CODE\n}",
            "size_t const n = x.size();\n    size_t const nthreads = omp_get_max_threads();\n    std::array<size_t, 4> tmpBins;\n    std::fill(tmpBins.begin(), tmpBins.end(), 0);\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    /* Divide the input equally amongst processes */\n    double const myInterval = 1.0/nprocs;\n    double const myStart = rank*myInterval;\n    double const myEnd = (rank+1)*myInterval;\n\n    /* Compute how many entries of x are in my interval */\n    size_t const mySize = std::count_if(x.begin(), x.end(), [&](double const& x) {\n        return myStart <= x && x < myEnd;\n    });\n    size_t myTotal = 0;\n    /* Assign each entry to one of the 4 bins */\n    #pragma omp parallel for num_threads(nthreads) reduction(+:myTotal)\n    for (size_t i = 0; i < n; ++i) {\n        if (myStart <= x[i] && x[i] < myEnd) {\n            size_t const ibin = std::floor((x[i] - myStart) * 4 / myInterval);\n            tmpBins[ibin] += 1;\n            myTotal += 1;\n        }\n    }\n    /* Compute the global counts */\n    MPI_Reduce(tmpBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        /* Divide the total number of entries in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n           amongst the processes.\n        */\n        double const total = (nprocs - 1) * myTotal;\n        double const quartile0 = 0.25 * total;\n        double const quartile1 = 0.5 * total;\n        double const quartile2 = 0.75 * total;\n        double const myCount0 = (bins[0] - quartile0) / nprocs;\n        double const myCount1 = (bins[1] - quartile1) / nprocs;\n        double const myCount2 = (bins[2] - quartile2) / nprocs;\n        double const myCount3 = (bins[3] - (n - quartile2)) / nprocs;\n        bins[0] = myCount0;\n        bins[1] = myCount1;\n        bins[2] = myCount2;\n        bins[3] = myCount3;\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // YOUR CODE HERE\n    double num_elements = (double) x.size();\n    int max_threads = omp_get_max_threads();\n\n    int thread_id = omp_get_thread_num();\n    int num_threads = omp_get_num_threads();\n\n    int min_threads = 4 * (int)ceil(num_elements/(size*0.25));\n\n    int num_elements_rank = num_elements/size;\n    int min_elements = num_elements_rank/4;\n\n    if (num_elements_rank < min_elements){\n        min_elements = num_elements_rank;\n    }\n\n    int start = 0;\n    if (rank == 0){\n        start = 0;\n    }\n    else{\n        start = num_elements_rank * rank + min_elements;\n    }\n\n    if (rank == size-1){\n        num_elements_rank = num_elements - (num_elements_rank * (size-1) + min_elements);\n    }\n\n    std::vector<double> x_rank;\n    x_rank.reserve(num_elements_rank);\n    x_rank.assign(x.begin()+start, x.begin() + start + num_elements_rank);\n\n    std::vector<double> x_rank_sorted;\n    x_rank_sorted.reserve(num_elements_rank);\n    x_rank_sorted.assign(x_rank.begin(), x_rank.end());\n    std::sort(x_rank_sorted.begin(), x_rank_sorted.end());\n\n    std::vector<double> x_rank_split1;\n    x_rank_split1.reserve(num_elements_rank/2);\n    x_rank_split1.assign(x_rank_sorted.begin(), x_rank_sorted.begin()+num_elements_rank/2);\n    std::vector<double> x_rank_split2;\n    x_rank_split2.reserve(num_elements_rank/2);\n    x_rank_split2.assign(x_rank_sorted.begin()+num_elements_rank/2, x_rank_sorted.end());\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements_rank/2; i++){\n        if ((x_rank_split1[i] >= 0.0 && x_rank_split1[i] < 0.25) || (x_rank_split1[i] >= 0.75 && x_rank_split1[i] < 1.0)){\n            bins[0] += 1;\n        }\n        if ((x_rank_split1[i] >= 0.25 && x_rank_split1[i] < 0.5) || (x_rank_split1[i] >= 0.5 && x_rank_split1[i] < 0.75)){\n            bins[1] += 1;\n        }\n        if (x_rank_split1[i] >= 0.5 && x_rank_split1[i] < 0.75){\n            bins[2] += 1;\n        }\n        if (x_rank_split1[i] >= 0.75 && x_rank_split1[i] < 1.0){\n            bins[3] += 1;\n        }\n    }\n\n    #pragma omp parallel for\n    for (int i = 0; i < num_elements_rank/2; i++){\n        if ((x_rank_split2[i] >= 0.0 && x_rank_split2[i] < 0.25) || (x_rank_split2[i] >= 0.75 && x_rank_split2[i] < 1.0)){\n            bins[0] += 1;\n        }\n        if ((x_rank_split2[i] >= 0.25 && x_rank_split2[i] < 0.5) || (x_rank_split2[i] >= 0.5 && x_rank_split2[i] < 0.75)){\n            bins[1] += 1;\n        }\n        if (x_rank_split2[i] >= 0.5 && x_rank_split2[i] < 0.75){\n            bins[2] += 1;\n        }\n        if (x_rank_split2[",
            "// Your code here.\n}",
            "int rank, nRanks;\n  MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  if (rank == 0) {\n    // get the number of doubles in each quartile\n    auto const nDoubles = x.size();\n\n    // each rank will compute its own histogram\n    std::vector<size_t> localHist(4, 0);\n\n#pragma omp parallel\n    {\n      int nThreads = omp_get_num_threads();\n      size_t const nPerThread = nDoubles / nThreads;\n      size_t const nExtra = nDoubles % nThreads;\n      size_t threadId = omp_get_thread_num();\n      size_t nFrom = threadId * nPerThread;\n      size_t nTo = (threadId + 1 == nThreads)? nDoubles + nExtra : (threadId + 1) * nPerThread;\n\n      // iterate over all doubles in x\n      for (size_t i = nFrom; i < nTo; i++) {\n        double const d = x[i];\n        // determine which quartile this value is in\n        if (d < 0.25)\n          localHist[0]++;\n        else if (d < 0.5)\n          localHist[1]++;\n        else if (d < 0.75)\n          localHist[2]++;\n        else\n          localHist[3]++;\n      }\n    }\n\n    // combine histograms\n    for (size_t i = 0; i < 4; i++) {\n      size_t recvCount;\n      MPI_Status status;\n      MPI_Probe(i + 1, 0, MPI_COMM_WORLD, &status);\n      MPI_Get_count(&status, MPI_UNSIGNED_LONG, &recvCount);\n      if (i == 0)\n        bins[0] = recvCount;\n      else\n        bins[i] = recvCount + bins[i - 1];\n    }\n  } else {\n    size_t nDoubles = x.size();\n    size_t nPerThread = nDoubles / nRanks;\n    size_t nExtra = nDoubles % nRanks;\n    size_t nFrom = rank * nPerThread;\n    size_t nTo = (rank + 1 == nRanks)? nDoubles + nExtra : (rank + 1) * nPerThread;\n    std::vector<double> localX(nTo - nFrom);\n    std::copy(x.begin() + nFrom, x.begin() + nTo, localX.begin());\n\n#pragma omp parallel for schedule(static)\n    for (size_t i = 0; i < nTo - nFrom; i++) {\n      double const d = localX[i];\n      if (d < 0.25)\n        bins[0]++;\n      else if (d < 0.5)\n        bins[1]++;\n      else if (d < 0.75)\n        bins[2]++;\n      else\n        bins[3]++;\n    }\n    MPI_Send(&bins[0], 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "size_t n = x.size();\n    size_t n_over_p = n/omp_get_num_procs();\n    size_t remainder = n % omp_get_num_procs();\n    std::array<size_t, 4> local_bins;\n    local_bins.fill(0);\n    std::vector<size_t> partial_counts(omp_get_num_procs(), 0);\n#pragma omp parallel for schedule(static, 1) reduction(+:partial_counts[:])\n    for (size_t i = 0; i < n_over_p; ++i) {\n        double fraction = x[i] - std::floor(x[i]);\n        if (fraction < 0.25) {\n            ++local_bins[0];\n        } else if (fraction < 0.5) {\n            ++local_bins[1];\n        } else if (fraction < 0.75) {\n            ++local_bins[2];\n        } else {\n            ++local_bins[3];\n        }\n    }\n    for (int i = 0; i < omp_get_num_procs(); ++i) {\n        partial_counts[i] = local_bins[0] + local_bins[1] + local_bins[2] + local_bins[3];\n    }\n#pragma omp parallel for schedule(static, 1) reduction(+:partial_counts[:])\n    for (size_t i = n_over_p; i < n - remainder; ++i) {\n        double fraction = x[i] - std::floor(x[i]);\n        if (fraction < 0.25) {\n            ++local_bins[0];\n        } else if (fraction < 0.5) {\n            ++local_bins[1];\n        } else if (fraction < 0.75) {\n            ++local_bins[2];\n        } else {\n            ++local_bins[3];\n        }\n    }\n    for (int i = 0; i < omp_get_num_procs(); ++i) {\n        partial_counts[i] += local_bins[0] + local_bins[1] + local_bins[2] + local_bins[3];\n    }\n    MPI_Reduce(partial_counts.data(), bins.data(), bins.size(), MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (omp_get_thread_num() == 0) {\n        std::cout << bins[0] << \" \" << bins[1] << \" \" << bins[2] << \" \" << bins[3] << std::endl;\n    }\n}",
            "/* INSERT YOUR CODE HERE */\n  size_t N = x.size();\n  double *count = new double[N];\n  double *rank = new double[N];\n  int *bin = new int[N];\n  int *count_bin = new int[4];\n  size_t n_threads = omp_get_max_threads();\n  size_t chunk_size = N / n_threads;\n  size_t remainder = N % n_threads;\n  size_t start = 0;\n  size_t end = 0;\n  for (int i = 0; i < n_threads; i++) {\n    end = chunk_size + remainder;\n    remainder--;\n    if (i == n_threads - 1) end = N;\n    #pragma omp parallel for\n    for (int j = start; j < end; j++) {\n      rank[j] = (double)(j + 1);\n      count[j] = (x[j] - floor(x[j])) * 4;\n      count_bin[(int)count[j]]++;\n    }\n    start = end;\n  }\n  #pragma omp parallel for\n  for (int i = 0; i < N; i++) {\n    bin[i] = (int)(count[i] + 0.5);\n  }\n  delete[] count;\n  delete[] rank;\n  MPI_Allreduce(count_bin, bin, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n  bins = {bin[0], bin[1], bin[2], bin[3]};\n  delete[] count_bin;\n  delete[] bin;\n}",
            "int size, rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int n = x.size();\n    int n_chunks = 4;\n    int chunk_size = n / size;\n    if (rank == 0) {\n        bins = {0, 0, 0, 0};\n    }\n\n    double chunk_start = rank * chunk_size;\n    double chunk_end = (rank + 1) * chunk_size;\n\n    std::vector<double> chunk(chunk_size);\n    // get the correct chunk\n    std::copy(x.begin() + chunk_start, x.begin() + chunk_end, chunk.begin());\n\n    std::vector<int> counts(n_chunks, 0);\n    #pragma omp parallel\n    {\n        int tid = omp_get_thread_num();\n        int n_threads = omp_get_num_threads();\n        double start = chunk_start + tid * (chunk_size / n_threads);\n        double end = chunk_start + (tid + 1) * (chunk_size / n_threads);\n        #pragma omp for\n        for (double i = start; i < end; ++i) {\n            if (i >= chunk_start && i < chunk_end) {\n                double frac = i - floor(i);\n                if (frac <= 0.25)\n                    counts[0] += 1;\n                else if (frac <= 0.5)\n                    counts[1] += 1;\n                else if (frac <= 0.75)\n                    counts[2] += 1;\n                else\n                    counts[3] += 1;\n            }\n        }\n    }\n\n    std::array<int, 4> local_counts = {counts[0], counts[1], counts[2], counts[3]};\n    std::array<int, 4> global_counts = {0, 0, 0, 0};\n    MPI_Allreduce(local_counts.data(), global_counts.data(), n_chunks, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    bins[0] = global_counts[0];\n    bins[1] = global_counts[1];\n    bins[2] = global_counts[2];\n    bins[3] = global_counts[3];\n}",
            "// TODO: your code here\n\n  size_t size = x.size();\n\n  int rank = 0;\n  int num_proc = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_proc);\n\n  int num_per_proc = size / num_proc;\n  int rem = size % num_proc;\n  int start = num_per_proc * rank + std::min(rem, rank);\n  int end = start + num_per_proc - 1;\n  if (rank == num_proc - 1) {\n    end = end + rem;\n  }\n\n  std::vector<double> local_x(x.begin() + start, x.begin() + end);\n\n  int num_threads = omp_get_max_threads();\n  std::vector<int> counts(4 * num_threads, 0);\n\n#pragma omp parallel num_threads(num_threads)\n  {\n    int tid = omp_get_thread_num();\n    int start_index = start + tid * num_per_proc / num_threads;\n    int end_index = start + (tid + 1) * num_per_proc / num_threads;\n    for (int i = start_index; i < end_index; i++) {\n      if (local_x[i] >= 0 && local_x[i] < 0.25) {\n        counts[4 * tid]++;\n      } else if (local_x[i] >= 0.25 && local_x[i] < 0.5) {\n        counts[4 * tid + 1]++;\n      } else if (local_x[i] >= 0.5 && local_x[i] < 0.75) {\n        counts[4 * tid + 2]++;\n      } else if (local_x[i] >= 0.75 && local_x[i] < 1) {\n        counts[4 * tid + 3]++;\n      }\n    }\n  }\n  std::array<int, 4> local_counts;\n  MPI_Gather(counts.data(), 4, MPI_INT, local_counts.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n  bins = std::array<size_t, 4>();\n  if (rank == 0) {\n    for (int i = 0; i < 4; i++) {\n      bins[i] = local_counts[i];\n    }\n  }\n}",
            "// TODO\n}",
            "size_t n = x.size();\n  size_t n_local = n / MPI_SIZE;\n\n  // determine which elements are in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n  std::vector<double> tmp(n_local);\n#pragma omp parallel for\n  for (size_t i = 0; i < n_local; i++) {\n    // tmp is local, so use it as scratch space to avoid copying x.\n    double x_i = x[i + MPI_RANK * n_local];\n    tmp[i] = std::fmod(x_i, 1.0);\n  }\n\n  // compute prefix sums with each processor, so we can determine which bin each element falls into\n  std::vector<size_t> prefix_sums(n_local + 1, 0);\n  for (size_t i = 1; i < n_local; i++) {\n    prefix_sums[i] = prefix_sums[i - 1] + tmp[i - 1] >= 0.5;\n  }\n  prefix_sums[n_local] = prefix_sums[n_local - 1] + tmp[n_local - 1] >= 0.5;\n\n  // determine which bin each element belongs to\n  std::vector<size_t> bins_local(n_local, 0);\n#pragma omp parallel for\n  for (size_t i = 0; i < n_local; i++) {\n    bins_local[i] = prefix_sums[i] - prefix_sums[i] * tmp[i] >= 0.5;\n  }\n\n  // gather the bins into bins on rank 0\n  MPI_Gather(&bins_local[0], bins_local.size(), MPI_SIZE_T, &bins[0], bins_local.size(), MPI_SIZE_T, 0, MPI_COMM_WORLD);\n}",
            "int rank, num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  double const QUARTILE_WIDTH = 1.0 / 4.0;\n\n  // TODO: Your code here\n\n}",
            "size_t n = x.size();\n  if (n < 4) throw std::runtime_error(\"x must have at least four elements.\");\n  bins.fill(0);\n#pragma omp parallel for\n  for (size_t i = 0; i < n; ++i) {\n    double x_i = x[i];\n    if (x_i >= 0.0 && x_i < 0.25) {\n      ++bins[0];\n    } else if (x_i >= 0.25 && x_i < 0.50) {\n      ++bins[1];\n    } else if (x_i >= 0.50 && x_i < 0.75) {\n      ++bins[2];\n    } else if (x_i >= 0.75 && x_i <= 1.00) {\n      ++bins[3];\n    }\n  }\n}",
            "// TODO: implement\n}",
            "// YOUR CODE HERE\n}",
            "// INSERT YOUR CODE HERE\n\n  // compute bin size\n  double const bin_size = 1.0 / (double) bins.size();\n\n  // compute the number of elements in each bin\n  #pragma omp parallel for\n  for (size_t i = 0; i < x.size(); ++i) {\n    // get the fractional part of x[i]\n    double const frac_part = fmod(x[i], 1);\n    // compute which bin the fractional part is in\n    // and increment the count of that bin\n    if (frac_part >= bin_size * 0.25 && frac_part < bin_size * 0.5) {\n      bins[0]++;\n    } else if (frac_part >= bin_size * 0.5 && frac_part < bin_size * 0.75) {\n      bins[1]++;\n    } else if (frac_part >= bin_size * 0.75 && frac_part < bin_size * 1) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "// TODO: Your code goes here\n}",
            "int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    size_t n = x.size();\n    size_t local_n = n / world_size;\n    size_t remainder = n % world_size;\n\n    size_t local_start = rank * local_n;\n    size_t local_end = local_start + local_n + remainder;\n    std::vector<double> local_x;\n\n    if (rank == 0) {\n        local_x.assign(x.begin(), x.end());\n    } else {\n        local_x.assign(x.begin() + local_start, x.begin() + local_end);\n    }\n\n    // MPI_Scatter(const void* send_data, int send_count, MPI_Datatype send_type, void* recv_data, int recv_count, MPI_Datatype recv_type, int root, MPI_Comm comm)\n    MPI_Scatter(&local_x[0], local_n, MPI_DOUBLE, &local_x[0], local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    #pragma omp parallel\n    {\n        std::vector<double> private_x = local_x;\n        std::array<size_t, 4> private_bins;\n        private_bins.fill(0);\n\n        #pragma omp for\n        for (size_t i = 0; i < local_n; i++) {\n            if (private_x[i] >= 0.25 && private_x[i] < 0.5) {\n                private_bins[0]++;\n            }\n            if (private_x[i] >= 0.5 && private_x[i] < 0.75) {\n                private_bins[1]++;\n            }\n            if (private_x[i] >= 0.75) {\n                private_bins[3]++;\n            }\n        }\n\n        #pragma omp critical\n        {\n            bins[0] += private_bins[0];\n            bins[1] += private_bins[1];\n            bins[2] += private_bins[2];\n            bins[3] += private_bins[3];\n        }\n    }\n\n    MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        // Divide by 4 because there are 4 quartiles\n        bins[0] = bins[0] / 4;\n        bins[1] = bins[1] / 4;\n        bins[2] = bins[2] / 4;\n        bins[3] = bins[3] / 4;\n    }\n}",
            "// TODO\n}",
            "const size_t rank = 0;\n    const size_t n = x.size();\n    const double step = 1.0 / 4.0;\n    \n    #pragma omp parallel\n    {\n        int my_bins[4] = {0};\n        size_t start = n * rank / size;\n        size_t end = n * (rank + 1) / size;\n        \n        #pragma omp for\n        for (size_t i = start; i < end; ++i) {\n            double fractional = x[i] * step;\n            if (fractional < 0.25) {\n                ++my_bins[0];\n            } else if (fractional < 0.5) {\n                ++my_bins[1];\n            } else if (fractional < 0.75) {\n                ++my_bins[2];\n            } else {\n                ++my_bins[3];\n            }\n        }\n        \n        #pragma omp critical\n        {\n            for (int i = 0; i < 4; ++i) {\n                bins[i] += my_bins[i];\n            }\n        }\n    }\n    \n    MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, rank, MPI_COMM_WORLD);\n}",
            "// TODO: Your code here\n}",
            "// TODO: implement this function\n    size_t length = x.size();\n    int rank;\n    int size;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads = omp_get_num_procs();\n\n    int num_elements_each_thread = length / num_threads;\n    int remaining = length % num_threads;\n\n    bins = std::array<size_t, 4>();\n\n    if (rank == 0) {\n        std::vector<double> local_x(num_elements_each_thread, 0);\n        int thread_id = 0;\n\n        for (size_t i = 0; i < num_elements_each_thread * num_threads; ++i) {\n            if (i < length) {\n                local_x[i % num_elements_each_thread] = x[i];\n            }\n        }\n\n        std::vector<std::array<size_t, 4>> local_bins(size);\n        std::vector<size_t> count_local_bins(size, 0);\n\n        for (int i = 1; i < size; ++i) {\n            MPI_Recv(local_bins[i].data(), 4, MPI_UNSIGNED, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            count_local_bins[i] = std::accumulate(local_bins[i].begin(), local_bins[i].end(), 0);\n        }\n\n        for (int i = 0; i < num_threads - 1; ++i) {\n            std::array<size_t, 4> new_bins;\n            new_bins[0] = countQuartilesInThread(local_x.data() + i * num_elements_each_thread, num_elements_each_thread, 0);\n            new_bins[1] = countQuartilesInThread(local_x.data() + i * num_elements_each_thread, num_elements_each_thread, 0.25);\n            new_bins[2] = countQuartilesInThread(local_x.data() + i * num_elements_each_thread, num_elements_each_thread, 0.5);\n            new_bins[3] = countQuartilesInThread(local_x.data() + i * num_elements_each_thread, num_elements_each_thread, 0.75);\n            MPI_Send(new_bins.data(), 4, MPI_UNSIGNED, i + 1, 0, MPI_COMM_WORLD);\n        }\n\n        std::array<size_t, 4> new_bins;\n        new_bins[0] = countQuartilesInThread(local_x.data() + (num_threads - 1) * num_elements_each_thread, num_elements_each_thread, 0);\n        new_bins[1] = countQuartilesInThread(local_x.data() + (num_threads - 1) * num_elements_each_thread, num_elements_each_thread, 0.25);\n        new_bins[2] = countQuartilesInThread(local_x.data() + (num_threads - 1) * num_elements_each_thread, num_elements_each_thread, 0.5);\n        new_bins[3] = countQuartilesInThread(local_x.data() + (num_threads - 1) * num_elements_each_thread, num_elements_each_thread, 0.75);\n        count_local_bins[num_threads - 1] = std::accumulate(new_bins.begin(), new_bins.end(), 0);\n\n        bins = std::array<size_t, 4>();\n        for (int i = 0; i < size; ++i) {\n            bins[0] += count_local_bins[i];\n            bins[1] += local_bins[i][1];\n            bins[2] += local_bins[i][2];\n            bins[3] += local_bins[i][3];\n        }\n\n        for (int i = 1; i < size; ++i) {\n            int index = 0;\n            MPI_Send(count_local_bins.data() + i, 1, MPI_UNSIGNED, i, 0, MPI_COMM_WORLD);\n        }\n    }\n    else {\n        int total_num_elements = 0;\n        MPI_Send",
            "// TODO: implement me!\n}",
            "// TODO\n}",
            "// TODO\n    std::sort(x.begin(), x.end());\n    // int rank;\n    // MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    // std::cout << \"Rank \" << rank << \" has \" << x.size() << \" values.\" << std::endl;\n    // int nthreads = omp_get_num_procs();\n    // std::cout << \"Num threads = \" << nthreads << std::endl;\n    // omp_set_num_threads(2);\n    // auto start = std::chrono::high_resolution_clock::now();\n    size_t i = 0;\n    size_t n = x.size();\n    size_t nq = n / size;\n    for (int rank = 0; rank < size; rank++) {\n        size_t left = rank * nq;\n        size_t right = std::min(n, rank * nq + nq);\n        auto mid = left + nq / 2;\n        // std::cout << \"Rank \" << rank << \" got left: \" << left << \" and right: \" << right << std::endl;\n        size_t q0 = left;\n        size_t q1 = left;\n        size_t q2 = left;\n        size_t q3 = left;\n        for (size_t i = left; i < right; i++) {\n            if (x[i] >= left / (double)n && x[i] < (left + nq) / (double)n) {\n                q0++;\n            }\n            if (x[i] >= mid / (double)n && x[i] < (mid + nq) / (double)n) {\n                q1++;\n            }\n            if (x[i] >= right / (double)n && x[i] < (right + nq) / (double)n) {\n                q2++;\n            }\n            if (x[i] >= (right + nq) / (double)n) {\n                q3++;\n            }\n        }\n        bins[0] += q0;\n        bins[1] += q1;\n        bins[2] += q2;\n        bins[3] += q3;\n        // auto end = std::chrono::high_resolution_clock::now();\n        // std::cout << \"Rank \" << rank << \" done in \" << std::chrono::duration_cast<std::chrono::microseconds>(end - start).count() << \" microseconds.\" << std::endl;\n    }\n}",
            "size_t len = x.size();\n\tbins.fill(0);\n\tdouble lower = 0.0, upper = 0.25;\n\tsize_t index = 0;\n\t#pragma omp parallel\n\t{\n\t\tint num = omp_get_num_threads();\n\t\tint rank = omp_get_thread_num();\n\t\t#pragma omp parallel for\n\t\tfor (size_t i = rank; i < len; i += num) {\n\t\t\tif (x[i] >= lower && x[i] < upper) {\n\t\t\t\tbins[index]++;\n\t\t\t}\n\t\t\tif (x[i] >= upper) {\n\t\t\t\tlower += 0.25;\n\t\t\t\tupper += 0.25;\n\t\t\t\tindex++;\n\t\t\t}\n\t\t}\n\t}\n\t/* MPI reduce all bins */\n\tdouble bins_double[4];\n\tMPI_Reduce(bins.data(), bins_double, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\tbins = std::array<size_t, 4>(bins_double);\n}",
            "size_t size = x.size();\n\n  // TODO\n}",
            "// write your code here\n\n    bins.fill(0);\n\n    #pragma omp parallel num_threads(4)\n    {\n        int rank = omp_get_thread_num();\n        int nthreads = omp_get_num_threads();\n        int n = x.size();\n\n        size_t i = n/nthreads * rank;\n        size_t j = n/nthreads * (rank+1);\n        if(rank == nthreads-1) j = n;\n\n        int k;\n        for(k=i; k<j; k++) {\n            if(x[k] <= 0.25) bins[0]++;\n            else if(x[k] <= 0.5) bins[1]++;\n            else if(x[k] <= 0.75) bins[2]++;\n            else bins[3]++;\n        }\n    }\n\n    // MPI_Reduce(...)\n}",
            "// TODO: Compute the quartiles on each process, and then sum them across all processes.\n}",
            "// Your code here\n}",
            "bins = std::array<size_t, 4>{};\n\n\t#pragma omp parallel\n\t{\n\t\t#pragma omp for schedule(static)\n\t\tfor (size_t i = 0; i < x.size(); ++i) {\n\t\t\tif (0.0 <= x[i] && x[i] < 0.25) {\n\t\t\t\tbins[0] += 1;\n\t\t\t}\n\t\t\telse if (0.25 <= x[i] && x[i] < 0.50) {\n\t\t\t\tbins[1] += 1;\n\t\t\t}\n\t\t\telse if (0.50 <= x[i] && x[i] < 0.75) {\n\t\t\t\tbins[2] += 1;\n\t\t\t}\n\t\t\telse if (0.75 <= x[i] && x[i] < 1.00) {\n\t\t\t\tbins[3] += 1;\n\t\t\t}\n\t\t}\n\t}\n}",
            "int myRank, worldSize;\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n\n   double const n = x.size();\n   // Count the number of elements in x that have a fractional part in [0, 0.25)\n   double const q1 = n / 4.0;\n   // Count the number of elements in x that have a fractional part in [0.25, 0.5)\n   double const q2 = (3 * n) / 4.0;\n   // Count the number of elements in x that have a fractional part in [0.5, 0.75)\n   double const q3 = (2 * n) / 4.0;\n   // Count the number of elements in x that have a fractional part in [0.75, 1)\n   double const q4 = (n / 4.0);\n\n   // Compute the number of elements in x that have a fractional part in [0, 0.25)\n   // on rank 0, and broadcast this number to the other ranks\n   double q1_local = 0;\n#pragma omp parallel for reduction(+ : q1_local)\n   for (size_t i = 0; i < x.size(); i++) {\n      if (x[i] >= 0.0 && x[i] < 0.25) {\n         q1_local += 1;\n      }\n   }\n   double q1_global;\n   MPI_Reduce(&q1_local, &q1_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   // If this is rank 0, compute the rest of the numbers and store them in bins\n   if (myRank == 0) {\n      bins[0] = (size_t)q1_global;\n   }\n   MPI_Bcast(&bins[0], 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // Compute the number of elements in x that have a fractional part in [0.25, 0.5)\n   // on rank 0, and broadcast this number to the other ranks\n   double q2_local = 0;\n#pragma omp parallel for reduction(+ : q2_local)\n   for (size_t i = 0; i < x.size(); i++) {\n      if (x[i] >= 0.25 && x[i] < 0.5) {\n         q2_local += 1;\n      }\n   }\n   double q2_global;\n   MPI_Reduce(&q2_local, &q2_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   // If this is rank 0, compute the rest of the numbers and store them in bins\n   if (myRank == 0) {\n      bins[1] = (size_t)q2_global;\n   }\n   MPI_Bcast(&bins[1], 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // Compute the number of elements in x that have a fractional part in [0.5, 0.75)\n   // on rank 0, and broadcast this number to the other ranks\n   double q3_local = 0;\n#pragma omp parallel for reduction(+ : q3_local)\n   for (size_t i = 0; i < x.size(); i++) {\n      if (x[i] >= 0.5 && x[i] < 0.75) {\n         q3_local += 1;\n      }\n   }\n   double q3_global;\n   MPI_Reduce(&q3_local, &q3_global, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);\n   // If this is rank 0, compute the rest of the numbers and store them in bins\n   if (myRank == 0) {\n      bins[2] = (size_t)q3_global;\n   }\n   MPI_Bcast(&bins[2], 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // Compute the number of elements in x that have a fractional part in [0.75, 1)\n   // on rank 0, and broadcast this number to the other ranks\n   double q4_local = 0;\n#pragma omp parallel for reduction(+ : q4_local)\n   for (size_t i = 0; i < x.size();",
            "// TODO\n}",
            "// TODO\n}",
            "int rank, numRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n    // Compute the number of elements on each rank\n    size_t n = x.size();\n    size_t chunkSize = n / numRanks;\n\n    // Compute the elements this rank will work on and the index of the first element on rank 0.\n    size_t offset = chunkSize * rank;\n    size_t localSize = chunkSize;\n    if (rank == numRanks - 1) {\n        localSize = n - offset;\n    }\n\n    // Distribute the data to each rank\n    std::vector<double> localX(localSize);\n    MPI_Scatter(&x[0], localSize, MPI_DOUBLE, localX.data(), localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // Count the number of elements in each bin\n    std::array<size_t, 4> localBins = {0, 0, 0, 0};\n#pragma omp parallel for\n    for (size_t i = 0; i < localSize; i++) {\n        double value = localX[i];\n        if (value < 0.25) {\n            localBins[0]++;\n        } else if (value < 0.5) {\n            localBins[1]++;\n        } else if (value < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    // Combine the results\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n\t//bins = {0,0,0,0};\n\t//int rank = 0;\n\t//MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n\t//size_t n = x.size();\n\t//size_t n_loc = n / (double)MPI_COMM_WORLD_SIZE;\n\t//size_t n_rem = n % (double)MPI_COMM_WORLD_SIZE;\n\t//size_t first = n_loc * rank + std::min(n_rem, rank);\n\t//size_t last = n_loc * (rank + 1) + std::min(n_rem, rank + 1);\n\t//std::vector<double> x_loc(x.begin() + first, x.begin() + last);\n\n\t//size_t n_loc_omp = n_loc / (double)omp_get_max_threads();\n\t//size_t n_rem_omp = n_loc % (double)omp_get_max_threads();\n\t//size_t first_omp = n_loc_omp * omp_get_thread_num() + std::min(n_rem_omp, omp_get_thread_num());\n\t//size_t last_omp = n_loc_omp * (omp_get_thread_num() + 1) + std::min(n_rem_omp, omp_get_thread_num() + 1);\n\t//std::vector<double> x_loc_omp(x_loc.begin() + first_omp, x_loc.begin() + last_omp);\n\t//\n\t//\n\t//double quartile[4];\n\t//size_t quartile_index[4];\n\t//size_t quartile_loc[4];\n\n\t//quartile[0] = 0;\n\t//quartile[1] = 0.25;\n\t//quartile[2] = 0.5;\n\t//quartile[3] = 0.75;\n\t//quartile_index[0] = 0;\n\t//quartile_index[1] = 1;\n\t//quartile_index[2] = 2;\n\t//quartile_index[3] = 3;\n\t//\n\t//for (int i = 0; i < 4; i++) {\n\t//\tfor (int j = 0; j < x_loc.size(); j++) {\n\t//\t\tif (x_loc[j] >= quartile[i] && x_loc[j] < quartile[i + 1]) {\n\t//\t\t\tquartile_loc[i]++;\n\t//\t\t}\n\t//\t}\n\t//\tMPI_Reduce(&quartile_loc[i], &bins[quartile_index[i]], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//}\n\n\t//double quartile[4];\n\t//size_t quartile_index[4];\n\t//size_t quartile_loc[4];\n\n\t//quartile[0] = 0;\n\t//quartile[1] = 0.25;\n\t//quartile[2] = 0.5;\n\t//quartile[3] = 0.75;\n\t//quartile_index[0] = 0;\n\t//quartile_index[1] = 1;\n\t//quartile_index[2] = 2;\n\t//quartile_index[3] = 3;\n\t//\n\t//for (int i = 0; i < 4; i++) {\n\t//\tfor (int j = 0; j < x_loc_omp.size(); j++) {\n\t//\t\tif (x_loc_omp[j] >= quartile[i] && x_loc_omp[j] < quartile[i + 1]) {\n\t//\t\t\tquartile_loc[i]++;\n\t//\t\t}\n\t//\t}\n\t//\tMPI_Reduce(&quartile_loc[i], &bins[quartile_index[i]], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\t//}\n}",
            "}",
            "// TODO: implement this function\n}",
            "const size_t n = x.size();\n  size_t const n_per_rank = n/MPI_COMM_SIZE;\n  size_t const start = n_per_rank*MPI_COMM_RANK;\n  size_t const end = std::min(n, start+n_per_rank);\n  size_t const local_bins = end - start;\n\n  bins.fill(0);\n#pragma omp parallel for\n  for (size_t i = 0; i < local_bins; i++) {\n    if ((x[start+i] >= 0.0) && (x[start+i] < 0.25)) {\n      bins[0]++;\n    } else if ((x[start+i] >= 0.25) && (x[start+i] < 0.50)) {\n      bins[1]++;\n    } else if ((x[start+i] >= 0.50) && (x[start+i] < 0.75)) {\n      bins[2]++;\n    } else if ((x[start+i] >= 0.75) && (x[start+i] < 1.00)) {\n      bins[3]++;\n    }\n  }\n#pragma omp parallel for reduction(+:bins)\n  for (size_t i = 0; i < 4; i++) {\n    MPI_Reduce(MPI_IN_PLACE, &bins[i], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "/* Your code here */\n  MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n  MPI_Comm_rank(MPI_COMM_WORLD, &bins.size());\n}",
            "// Fill bins with the count of each quartile.\n    // Hint: bins[i] is the number of doubles in the input x that have a fractional part in [i * 0.25, (i + 1) * 0.25).\n\n    size_t num_threads = omp_get_max_threads();\n    size_t num_samples = x.size();\n    size_t num_quartiles = bins.size();\n    size_t chunk_size = num_samples / num_threads;\n    size_t remainder = num_samples % num_threads;\n\n    std::vector<size_t> local_bins(num_quartiles, 0);\n    #pragma omp parallel for\n    for (size_t i = 0; i < num_quartiles; ++i) {\n        for (size_t j = i * chunk_size; j < (i + 1) * chunk_size + remainder; ++j) {\n            if ((x[j] >= i * 0.25) && (x[j] < (i + 1) * 0.25)) {\n                local_bins[i] += 1;\n            }\n        }\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < num_quartiles; ++i) {\n        bins[i] = local_bins[i];\n    }\n\n    #pragma omp parallel for\n    for (size_t i = 1; i < num_threads; ++i) {\n        for (size_t j = 0; j < num_quartiles; ++j) {\n            bins[j] += local_bins[j];\n        }\n    }\n\n    if (rank == 0) {\n        for (size_t i = 1; i < num_threads; ++i) {\n            for (size_t j = 0; j < num_quartiles; ++j) {\n                bins[j] += recv_int(i);\n            }\n        }\n    }\n    else {\n        for (size_t i = 0; i < num_quartiles; ++i) {\n            send_int(bins[i], i);\n        }\n    }\n}",
            "// TODO: implement this function\n    int rank, size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    int num = x.size();\n    int block_size = num / size;\n    int last_block_size = num - block_size * size;\n    if(rank == 0) {\n        bins.fill(0);\n    }\n    std::vector<int> my_bins;\n    if(rank < last_block_size) {\n        my_bins.resize(4);\n        my_bins.fill(0);\n        int local_block_size = block_size + 1;\n        for(int i = 0; i < local_block_size; i++) {\n            if(x[block_size * rank + i] >= 0 && x[block_size * rank + i] < 0.25) {\n                my_bins[0] += 1;\n            } else if(x[block_size * rank + i] >= 0.25 && x[block_size * rank + i] < 0.5) {\n                my_bins[1] += 1;\n            } else if(x[block_size * rank + i] >= 0.5 && x[block_size * rank + i] < 0.75) {\n                my_bins[2] += 1;\n            } else if(x[block_size * rank + i] >= 0.75 && x[block_size * rank + i] < 1) {\n                my_bins[3] += 1;\n            }\n        }\n    } else {\n        my_bins.resize(4);\n        my_bins.fill(0);\n        for(int i = 0; i < block_size; i++) {\n            if(x[block_size * rank + i] >= 0 && x[block_size * rank + i] < 0.25) {\n                my_bins[0] += 1;\n            } else if(x[block_size * rank + i] >= 0.25 && x[block_size * rank + i] < 0.5) {\n                my_bins[1] += 1;\n            } else if(x[block_size * rank + i] >= 0.5 && x[block_size * rank + i] < 0.75) {\n                my_bins[2] += 1;\n            } else if(x[block_size * rank + i] >= 0.75 && x[block_size * rank + i] < 1) {\n                my_bins[3] += 1;\n            }\n        }\n    }\n    std::vector<int> global_bins(4);\n    MPI_Allreduce(my_bins.data(), global_bins.data(), 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n    bins[0] = global_bins[0];\n    bins[1] = global_bins[1];\n    bins[2] = global_bins[2];\n    bins[3] = global_bins[3];\n}",
            "// Your code here\n    size_t n = x.size();\n    int p, r;\n    MPI_Comm_rank(MPI_COMM_WORLD, &r);\n    MPI_Comm_size(MPI_COMM_WORLD, &p);\n    int n_threads = omp_get_max_threads();\n    std::vector<double> x_copy;\n    std::vector<size_t> bins_local(4);\n\n    if (r == 0) {\n        x_copy = x;\n    }\n    MPI_Bcast(&x_copy, n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    int chunk_size = n / p;\n    std::vector<int> counts(n_threads);\n    #pragma omp parallel for\n    for (int i = 0; i < n_threads; i++) {\n        counts[i] = 0;\n    }\n\n    #pragma omp parallel for schedule(static, chunk_size)\n    for (int i = 0; i < n; i++) {\n        double num = x_copy[i];\n        if (num >= 0.75 && num < 1) {\n            counts[omp_get_thread_num()]++;\n        }\n    }\n    MPI_Reduce(counts.data(), bins_local.data(), n_threads, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n    if (r == 0) {\n        for (int i = 0; i < 4; i++) {\n            bins[i] = bins_local[i];\n        }\n    }\n}",
            "size_t n = x.size();\n  std::vector<size_t> local_bins(4, 0);\n  double q = 0;\n  double d = 0;\n  #pragma omp parallel private(q, d)\n  {\n    int my_rank = omp_get_thread_num();\n    #pragma omp for\n    for (size_t i = 0; i < n; ++i) {\n      q = x[i];\n      d = fmod(q, 1.0);\n      if ((d >= 0.0) && (d < 0.25)) {\n        ++local_bins[0];\n      }\n      else if ((d >= 0.25) && (d < 0.5)) {\n        ++local_bins[1];\n      }\n      else if ((d >= 0.5) && (d < 0.75)) {\n        ++local_bins[2];\n      }\n      else if ((d >= 0.75) && (d < 1.0)) {\n        ++local_bins[3];\n      }\n    }\n  }\n  // Get the sum of the local_bins across all processors\n  size_t global_bins[4];\n  MPI_Allreduce(local_bins.data(), global_bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n  bins = {global_bins[0], global_bins[1], global_bins[2], global_bins[3]};\n}",
            "// TODO\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int n = x.size();\n   std::array<double, 4> partSum = {0.0, 0.0, 0.0, 0.0};\n   std::array<size_t, 4> partCounts = {0, 0, 0, 0};\n   if (rank == 0) {\n      int numThreads = omp_get_max_threads();\n      std::vector<std::array<size_t, 4>> partCountsThreads(numThreads);\n#pragma omp parallel num_threads(numThreads)\n      {\n         int threadId = omp_get_thread_num();\n         int threadNum = omp_get_num_threads();\n         std::array<size_t, 4> threadCounts = {0, 0, 0, 0};\n         for (int i = n / size * rank; i < n / size * (rank + 1); ++i) {\n            if ((x[i] - (int)x[i]) < 0.25) {\n               threadCounts[0] += 1;\n            } else if ((x[i] - (int)x[i]) >= 0.25 && (x[i] - (int)x[i]) < 0.5) {\n               threadCounts[1] += 1;\n            } else if ((x[i] - (int)x[i]) >= 0.5 && (x[i] - (int)x[i]) < 0.75) {\n               threadCounts[2] += 1;\n            } else {\n               threadCounts[3] += 1;\n            }\n         }\n#pragma omp critical\n         partCountsThreads[threadId] = threadCounts;\n      }\n      for (int i = 0; i < numThreads; ++i) {\n         partCounts[0] += partCountsThreads[i][0];\n         partCounts[1] += partCountsThreads[i][1];\n         partCounts[2] += partCountsThreads[i][2];\n         partCounts[3] += partCountsThreads[i][3];\n      }\n   }\n   MPI_Gather(partCounts.data(), 4, MPI_UNSIGNED_LONG_LONG, partCounts.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      std::vector<std::array<double, 4>> partSumThreads(numThreads);\n#pragma omp parallel num_threads(numThreads)\n      {\n         int threadId = omp_get_thread_num();\n         int threadNum = omp_get_num_threads();\n         std::array<double, 4> threadSum = {0.0, 0.0, 0.0, 0.0};\n         for (int i = n / size * rank; i < n / size * (rank + 1); ++i) {\n            if ((x[i] - (int)x[i]) < 0.25) {\n               threadSum[0] += x[i];\n            } else if ((x[i] - (int)x[i]) >= 0.25 && (x[i] - (int)x[i]) < 0.5) {\n               threadSum[1] += x[i];\n            } else if ((x[i] - (int)x[i]) >= 0.5 && (x[i] - (int)x[i]) < 0.75) {\n               threadSum[2] += x[i];\n            } else {\n               threadSum[3] += x[i];\n            }\n         }\n#pragma omp critical\n         partSumThreads[threadId] = threadSum;\n      }\n      for (int i = 0; i < numThreads; ++i) {\n         partSum[0] += partSumThreads[i][0];\n         partSum[1] += partSumThreads[i][1];\n         partSum[2] += partSumThreads[i][2];\n         partSum[3] += partSumThreads[i][3];\n      }\n      bins[0] = partCounts[0];\n      bins[1] = partCounts[1];\n      bins[2] = partCounts[2];\n      bins[3] = partCounts[3];\n      bins[0] += partSum[0];\n      bins[1] += partSum[1];\n      bins[2] += partSum[2",
            "int my_rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<double> x_local;\n    if (my_rank == 0) {\n        x_local = x;\n    } else {\n        x_local.resize(x.size());\n    }\n\n    int n = x.size() / size;\n    int r = x.size() % size;\n    int s = n + 1;\n    MPI_Scatter(x.data(), (int) n, MPI_DOUBLE, x_local.data(), (int) n, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    std::vector<double> x_local_copy = x_local;\n\n    // Compute the number of doubles in each bin, then sum the results\n    // bins[0] = 0;\n    // bins[1] = 0;\n    // bins[2] = 0;\n    // bins[3] = 0;\n\n    if (my_rank == 0) {\n        // Parallel part\n        #pragma omp parallel num_threads(size)\n        {\n            std::vector<size_t> bins_local(4);\n\n            // Each thread computes the bin counts in this interval: [start, end)\n            #pragma omp for schedule(static)\n            for (int start = 0; start < x_local_copy.size(); start += s) {\n                size_t end = start + s;\n                if (end < x_local_copy.size()) {\n                    for (size_t i = start; i < end; ++i) {\n                        double value = x_local_copy[i];\n                        if (value < 0.25) {\n                            ++bins_local[0];\n                        } else if (value < 0.5) {\n                            ++bins_local[1];\n                        } else if (value < 0.75) {\n                            ++bins_local[2];\n                        } else {\n                            ++bins_local[3];\n                        }\n                    }\n                } else {\n                    for (size_t i = start; i < x_local_copy.size(); ++i) {\n                        double value = x_local_copy[i];\n                        if (value < 0.25) {\n                            ++bins_local[0];\n                        } else if (value < 0.5) {\n                            ++bins_local[1];\n                        } else if (value < 0.75) {\n                            ++bins_local[2];\n                        } else {\n                            ++bins_local[3];\n                        }\n                    }\n                }\n            }\n\n            // Sum the local bin counts\n            #pragma omp critical\n            {\n                for (size_t i = 0; i < 4; ++i) {\n                    bins[i] += bins_local[i];\n                }\n            }\n        }\n    } else {\n        // Serial part\n        // Each rank computes the bin counts in this interval: [start, end)\n        for (int start = 0; start < x_local.size(); start += s) {\n            size_t end = start + s;\n            if (end < x_local.size()) {\n                for (size_t i = start; i < end; ++i) {\n                    double value = x_local[i];\n                    if (value < 0.25) {\n                        ++bins[0];\n                    } else if (value < 0.5) {\n                        ++bins[1];\n                    } else if (value < 0.75) {\n                        ++bins[2];\n                    } else {\n                        ++bins[3];\n                    }\n                }\n            } else {\n                for (size_t i = start; i < x_local.size(); ++i) {\n                    double value = x_local[i];\n                    if (value < 0.25) {\n                        ++bins[0];\n                    } else if (value < 0.5) {\n                        ++bins[1];\n                    } else if (value < 0.75) {\n                        ++bins[2];\n                    } else {\n                        ++bins[3];\n                    }\n                }\n            }\n        }\n    }\n\n    // Each rank then sends the local bin counts to rank 0\n    MPI_Gather(bins.data(), 4, MPI_UNSIGNED, bins.data(), 4, MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "bins = {0, 0, 0, 0};\n  #pragma omp parallel num_threads(4)\n  {\n    #pragma omp for schedule(static) reduction(+:bins[:])\n    for (size_t i = 0; i < x.size(); i++) {\n      double frac = x[i] - static_cast<size_t>(x[i]);\n      if (frac >= 0 && frac < 0.25) {\n        bins[0]++;\n      } else if (frac >= 0.25 && frac < 0.5) {\n        bins[1]++;\n      } else if (frac >= 0.5 && frac < 0.75) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "#ifdef __clang__\n#pragma omp parallel num_threads(2)\n#else\n#pragma omp parallel for num_threads(2)\n#endif\n  for (auto const& value : x) {\n    if (value < 0.25) ++bins[0];\n    else if (value < 0.5) ++bins[1];\n    else if (value < 0.75) ++bins[2];\n    else ++bins[3];\n  }\n  if (omp_get_thread_num() == 0) {\n    MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n  if (omp_get_thread_num() == 1) {\n    MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Compute the number of ranks\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  // Compute the number of points per rank\n  int n = x.size();\n  int n_per_rank = n / world_size;\n  int n_remaining = n % world_size;\n  int start = n_per_rank * rank + std::min(rank, n_remaining);\n  int end = n_per_rank * (rank + 1) + std::min(rank + 1, n_remaining);\n\n  // Perform the computation on this rank\n  // Hint: omp_get_num_threads() returns the number of threads assigned to the current thread\n  // Hint: omp_get_max_threads() returns the max number of threads available\n  // Hint: omp_get_thread_num() returns the rank of the current thread\n\n  // Gather the results to rank 0\n  MPI_Gather(&bins, 4, MPI_INT, &bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "size_t n = x.size();\n  bins.fill(0);\n  int n_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t start = (rank * n + 1) / n_ranks;\n  size_t end = ((rank + 1) * n + n_ranks - 1) / n_ranks;\n  int n_threads = omp_get_max_threads();\n  std::vector<size_t> thread_counts(n_threads);\n  std::vector<size_t> thread_bin(n_threads);\n  #pragma omp parallel\n  {\n    size_t thread_idx = omp_get_thread_num();\n    for (size_t i = start; i < end; ++i) {\n      double f = x[i] - floor(x[i]);\n      if (f < 0.25) {\n        thread_bin[thread_idx] = 0;\n      } else if (f < 0.5) {\n        thread_bin[thread_idx] = 1;\n      } else if (f < 0.75) {\n        thread_bin[thread_idx] = 2;\n      } else {\n        thread_bin[thread_idx] = 3;\n      }\n      ++thread_counts[thread_idx];\n    }\n  }\n  for (size_t i = 0; i < n_threads; ++i) {\n    size_t thread_bin_i = thread_bin[i];\n    if (thread_bin_i < 4) {\n      bins[thread_bin_i] += thread_counts[i];\n    }\n  }\n  size_t total_count = 0;\n  MPI_Reduce(bins.data(), total_count, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    for (size_t i = 0; i < n_threads; ++i) {\n      size_t thread_bin_i = thread_bin[i];\n      if (thread_bin_i < 4) {\n        bins[thread_bin_i] += total_count - thread_counts[i];\n      }\n    }\n  }\n}",
            "size_t size = x.size();\n    size_t threads_per_rank = omp_get_max_threads();\n    size_t chunk_size = std::ceil((double)size / threads_per_rank);\n    size_t num_chunks = std::ceil((double)size / chunk_size);\n    bins.fill(0);\n    #pragma omp parallel for default(none) schedule(static)\n    for (size_t j = 0; j < num_chunks; ++j) {\n        size_t start = j * chunk_size;\n        size_t end = std::min(size, (j+1) * chunk_size);\n        std::array<size_t, 4> thread_bins;\n        thread_bins.fill(0);\n        #pragma omp for\n        for (size_t i = start; i < end; ++i) {\n            double value = x[i];\n            if (value >= 0.0 && value <= 0.25) {\n                ++thread_bins[0];\n            }\n            else if (value >= 0.25 && value <= 0.5) {\n                ++thread_bins[1];\n            }\n            else if (value >= 0.5 && value <= 0.75) {\n                ++thread_bins[2];\n            }\n            else if (value >= 0.75 && value <= 1.0) {\n                ++thread_bins[3];\n            }\n        }\n        #pragma omp critical\n        for (size_t k = 0; k < thread_bins.size(); ++k) {\n            bins[k] += thread_bins[k];\n        }\n    }\n}",
            "double start = MPI_Wtime();\n\tint rank, nprocs;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n\tint num = x.size();\n\tint div = 4;\n\tstd::vector<double> local_x = x;\n\tstd::vector<size_t> local_bins(div, 0);\n\n\tif (rank == 0) {\n\t\t// std::vector<std::vector<double>> local_x_vec(nprocs, std::vector<double>(num));\n\t\t// MPI_Scatter(x.data(), num, MPI_DOUBLE, local_x_vec[0].data(), num, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t\t// local_x = local_x_vec[0];\n\t\tMPI_Scatter(x.data(), num, MPI_DOUBLE, local_x.data(), num, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t} else {\n\t\tMPI_Scatter(x.data(), num, MPI_DOUBLE, local_x.data(), num, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\t}\n\n\tif (rank == 0) {\n\t\tstd::cout << \"num = \" << num << std::endl;\n\t\tstd::cout << \"num / nprocs = \" << num / nprocs << std::endl;\n\t}\n\n\tint count = 0;\n\tif (rank == 0) {\n\t\tfor (int i = 0; i < nprocs; i++) {\n\t\t\tif (i == 0) {\n\t\t\t\tcount = num / nprocs;\n\t\t\t} else {\n\t\t\t\tcount = num - num / nprocs * (nprocs - i);\n\t\t\t}\n\t\t\t// std::cout << \"count = \" << count << std::endl;\n\t\t\t// std::cout << \"local_x[0] = \" << local_x[0] << std::endl;\n\t\t\t// std::cout << \"local_x[1] = \" << local_x[1] << std::endl;\n\t\t\t// std::cout << \"local_x[2] = \" << local_x[2] << std::endl;\n\t\t\t// std::cout << \"local_x[3] = \" << local_x[3] << std::endl;\n\t\t\t// std::cout << \"local_x[4] = \" << local_x[4] << std::endl;\n\t\t\t// std::cout << \"local_x[5] = \" << local_x[5] << std::endl;\n\t\t\t// std::cout << \"local_x[6] = \" << local_x[6] << std::endl;\n\t\t\t// std::cout << \"local_x[7] = \" << local_x[7] << std::endl;\n\n\t\t\tfor (int j = 0; j < count; j++) {\n\t\t\t\tdouble tmp = local_x[j];\n\t\t\t\tif (tmp >= 0.0 && tmp < 0.25) {\n\t\t\t\t\tlocal_bins[0]++;\n\t\t\t\t} else if (tmp >= 0.25 && tmp < 0.5) {\n\t\t\t\t\tlocal_bins[1]++;\n\t\t\t\t} else if (tmp >= 0.5 && tmp < 0.75) {\n\t\t\t\t\tlocal_bins[2]++;\n\t\t\t\t} else {\n\t\t\t\t\tlocal_bins[3]++;\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\tfor (int i = 0; i < div; i++) {\n\t\t\t// std::cout << local_bins[i] << std::endl;\n\t\t\tbins[i] = local_bins[i];\n\t\t}\n\t} else {\n\t\tfor (int j = 0; j < count; j++) {\n\t\t\tdouble tmp = local_x[j];\n\t\t\tif (tmp >= 0.0 && tmp < 0.25) {\n\t\t\t\tlocal_bins[0]++;\n\t\t\t} else if (tmp >= 0.25 && tmp < 0.5) {\n\t\t\t\tlocal_bins[1]++;\n\t\t\t} else if (tmp >= 0.5 && tmp < 0.75) {\n\t\t\t\tlocal_bins[2]++;\n\t\t\t} else {\n\t\t\t\tlocal_bins[3]++;\n\t\t\t}\n\t\t}\n\n\t\tMPI_Gather(local_bins.data(), div, MPI_UNSIGNED_LONG_LONG, bins.data(), div, MPI_UNSIGNED_LONG_LONG, 0",
            "}",
            "int n = x.size();\n  int nthreads = omp_get_max_threads();\n  double const quartile = 1.0 / 4.0;\n  size_t n_lower_quartile = (size_t) std::ceil(n * quartile);\n  size_t n_upper_quartile = (size_t) std::floor(n * (1 - quartile));\n\n  std::array<size_t, 4> bins_local = {0, 0, 0, 0};\n\n  #pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    double const x_i = x[i];\n    double const x_i_fraction = x_i - std::floor(x_i);\n\n    if (x_i_fraction < quartile) {\n      bins_local[0] += 1;\n    }\n    else if (x_i_fraction >= quartile && x_i_fraction < 2 * quartile) {\n      bins_local[1] += 1;\n    }\n    else if (x_i_fraction >= 2 * quartile && x_i_fraction < 3 * quartile) {\n      bins_local[2] += 1;\n    }\n    else if (x_i_fraction >= 3 * quartile) {\n      bins_local[3] += 1;\n    }\n  }\n\n  // Communicate the local bins back to the main thread\n  MPI_Bcast(bins_local.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  // Aggregate the local bins\n  bins[0] = bins_local[0];\n  bins[1] = bins_local[1];\n  bins[2] = bins_local[2];\n  bins[3] = bins_local[3];\n}",
            "// TODO: implement this function\n\tdouble N = x.size();\n\tbins = std::array<size_t, 4>();\n\n\tfor (int i = 0; i < x.size(); i++)\n\t{\n\t\tif (x[i] < 0.25 * N)\n\t\t\tbins[0] += 1;\n\t\telse if (x[i] >= 0.25 * N && x[i] < 0.5 * N)\n\t\t\tbins[1] += 1;\n\t\telse if (x[i] >= 0.5 * N && x[i] < 0.75 * N)\n\t\t\tbins[2] += 1;\n\t\telse if (x[i] >= 0.75 * N)\n\t\t\tbins[3] += 1;\n\t}\n}",
            "int rank, num_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  size_t size = x.size();\n  bins.fill(0);\n\n  // TODO: divide the work evenly among all ranks\n  double fraction = (double)(size / num_ranks) / (double)(size);\n  int start_index = fraction * rank;\n  int end_index = fraction * (rank + 1);\n  if (rank == num_ranks - 1) {\n    end_index = size;\n  }\n\n  // TODO: count the number of elements in each bin\n  // NOTE: The following is a very naive implementation. You may want to improve this.\n  #pragma omp parallel for reduction(+: bins[0], bins[1], bins[2], bins[3])\n  for (int i = start_index; i < end_index; i++) {\n    double element = x[i];\n    if (element >= 0.75) {\n      bins[3]++;\n    } else if (element >= 0.5) {\n      bins[2]++;\n    } else if (element >= 0.25) {\n      bins[1]++;\n    } else {\n      bins[0]++;\n    }\n  }\n\n  // TODO: collect the counts from all ranks\n  // NOTE: use MPI_Reduce instead of MPI_Gather here\n  std::vector<size_t> counts(num_ranks, 0);\n  MPI_Reduce(bins.data(), counts.data(), num_ranks, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n  // TODO: assign the final counts to bins if this is rank 0\n  if (rank == 0) {\n    for (int i = 0; i < num_ranks; i++) {\n      bins[i] = counts[i];\n    }\n  }\n}",
            "const size_t n = x.size();\n  const int numThreads = 8;\n  const int numRanks = 1; // TODO: replace with value from MPI_Comm_size(comm)\n  const int myRank = 0; // TODO: replace with value from MPI_Comm_rank(comm)\n  const int bin_width = n/4;\n  const int bin_start = myRank * bin_width;\n  const int bin_end = bin_start + bin_width;\n\n  // Your code goes here!\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  // TODO: compute quartiles using OpenMP for loop and MPI for loop to parallelize the code.\n  // Hint: You can use the function MPI_Scatter to distribute the elements of the vector x to each rank\n  // and the function MPI_Gather to collect the quartile counts at rank 0.\n}",
            "// TODO\n}",
            "size_t n = x.size();\n  int rank = 0;\n  int nprocs = 0;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n  double const q = 0.25;\n  double const f = 0.5;\n\n  // Compute the number of elements in each bin\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n#pragma omp parallel for reduction(+: bins[0], bins[1], bins[2], bins[3])\n  for (size_t i = 0; i < n; ++i) {\n    double value = x[i];\n    // Compute the value in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1).\n    double t = (value - f) / q;\n    if (t <= 0.0) {\n      bins[0]++;\n    } else if (t < 1.0) {\n      bins[1]++;\n    } else if (t < 2.0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n\n  // Reduce the counts on the root process\n  std::vector<size_t> scounts(nprocs);\n  std::vector<size_t> rcounts(nprocs);\n  std::vector<int> sdispls(nprocs);\n  std::vector<int> rdispls(nprocs);\n\n  MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &rcounts[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG_LONG, &scounts[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  rdispls[0] = 0;\n  sdispls[0] = 0;\n  for (int i = 1; i < nprocs; ++i) {\n    rdispls[i] = rdispls[i-1] + rcounts[i-1];\n    sdispls[i] = sdispls[i-1] + scounts[i-1];\n  }\n\n  bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n  MPI_Scatterv(&scounts[0], &sdispls[0], MPI_UNSIGNED_LONG_LONG, &bins[0], 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < x.size(); ++i) {\n      // Fill in code here\n   }\n\n   // Reduce the results from all the ranks back to rank 0\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: your code here!\n}",
            "// TODO: implement this function\n    int rank, num_procs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n    int n = x.size();\n    double x_local = 0;\n    int n_local = 0;\n    double *x_send;\n    int *n_send;\n    int *n_recv;\n    int *n_recv_temp;\n    n_recv = (int *)malloc(num_procs*sizeof(int));\n    n_recv_temp = (int *)malloc(num_procs*sizeof(int));\n    n_send = (int *)malloc(num_procs*sizeof(int));\n    x_send = (double *)malloc(num_procs*sizeof(double));\n    size_t *bins_temp;\n    bins_temp = (size_t *)malloc(4*sizeof(size_t));\n    for(int i=0;i<num_procs;i++)\n    {\n        n_send[i] = n/num_procs;\n        if(i == num_procs-1)\n            n_send[i] += n%num_procs;\n        x_send[i] = 0;\n    }\n\n    for(int i=0;i<n;i++)\n    {\n        double val = x[i];\n        double quart = val/4;\n        if(val < 0.25)\n            bins_temp[0]++;\n        else if(val < 0.5)\n            bins_temp[1]++;\n        else if(val < 0.75)\n            bins_temp[2]++;\n        else\n            bins_temp[3]++;\n    }\n    MPI_Allreduce(bins_temp, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n}",
            "size_t n = x.size();\n  double const delta = 0.25;\n  // number of bins\n  size_t num_bins = 4;\n\n  // check: number of bins is 4\n  assert(num_bins == 4);\n\n  // find the number of elements for every bin\n  // this is the number of elements per rank\n  size_t n_local = n / num_bins;\n\n  // number of elements for the first bin\n  size_t n1 = n - n_local * (num_bins - 1);\n\n  // bins for every rank\n  std::array<size_t, 4> bins_local;\n\n  // parallel for loop\n#pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    if (x[i] < n1 * delta) {\n      bins_local[0] += 1;\n    } else if (x[i] < (n1 + n_local) * delta) {\n      bins_local[1] += 1;\n    } else if (x[i] < (n1 + 2 * n_local) * delta) {\n      bins_local[2] += 1;\n    } else {\n      bins_local[3] += 1;\n    }\n  }\n\n  // sum bins from every rank and assign to bins\n  std::array<size_t, 4> bins_total;\n  MPI_Reduce(&bins_local, &bins_total, num_bins, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (omp_get_thread_num() == 0) {\n    bins = bins_total;\n  }\n}",
            "#pragma omp parallel default(none) shared(x, bins)\n    {\n        int const num_threads = omp_get_num_threads();\n        int const rank = omp_get_thread_num();\n\n        // Divide the vector among the threads\n        size_t const length = x.size();\n        size_t const chunk_size = length / num_threads;\n\n        // The first chunk goes to the first thread\n        size_t const start = rank * chunk_size;\n\n        // The end of the last chunk is the end of the vector, not the end of the chunk\n        size_t const end = (rank == num_threads-1)? length : start + chunk_size;\n\n        // Do the parallel work\n        int const n_bins = 4;\n        int const n_points_per_bin = 25;\n\n        std::vector<size_t> points_per_bin(n_bins, 0);\n        for(size_t i = start; i < end; i++) {\n            double const num = x[i];\n            double const frac = num - floor(num);\n            if(frac >= 0 && frac < 0.25) {\n                points_per_bin[0]++;\n            } else if(frac >= 0.25 && frac < 0.5) {\n                points_per_bin[1]++;\n            } else if(frac >= 0.5 && frac < 0.75) {\n                points_per_bin[2]++;\n            } else {\n                points_per_bin[3]++;\n            }\n        }\n\n        // Combine the results of each thread\n        std::vector<size_t> thread_bins(n_bins, 0);\n        for(int i = 0; i < n_bins; i++) {\n            for(int thread = 0; thread < num_threads; thread++) {\n                thread_bins[i] += points_per_bin[i * num_threads + thread];\n            }\n        }\n\n        // Store the results\n        if(rank == 0) {\n            bins = std::move(thread_bins);\n        }\n    }\n}",
            "size_t n = x.size();\n    //TODO\n}",
            "size_t n = x.size();\n\tbins = { 0, 0, 0, 0 };\n#pragma omp parallel for schedule(static)\n\tfor (size_t i = 0; i < n; i++) {\n\t\tdouble fraction = x[i] - floor(x[i]);\n\t\tif (fraction >= 0.0 && fraction < 0.25) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (fraction >= 0.25 && fraction < 0.5) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (fraction >= 0.5 && fraction < 0.75) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[2]++;\n\t\t}\n\t\telse {\n\t\t\t#pragma omp atomic\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO: implement me!\n    // 1. Initialize bins to all zeros\n    // 2. Use OpenMP to divide work evenly among the threads.\n    //    Each thread should compute the counts for a quarter of the entries of x.\n    //    All threads should use the same bins.\n    // 3. Use MPI to combine the counts into bins.\n}",
            "// TODO\n}",
            "//TODO implement\n}",
            "size_t size = x.size();\n    size_t chunk = size / (size_t)omp_get_num_procs();\n\n    // TODO: Implement this function.\n    // Hint: Use a single parallel region.\n}",
            "// YOUR CODE HERE\n\n}",
            "size_t n = x.size();\n  bins = std::array<size_t, 4>();\n  double const threshold_0 = 0.0, threshold_1 = 0.25, threshold_2 = 0.5, threshold_3 = 0.75;\n  \n  double max_val, min_val;\n  // Compute min and max values\n  MPI_Allreduce(&x[0], &min_val, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n  MPI_Allreduce(&x[0], &max_val, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n  double const delta = (max_val - min_val) / (double) (n);\n  std::vector<double> bins_local(4);\n  \n  #pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    int const rank = omp_get_thread_num();\n    int const nthreads = omp_get_num_threads();\n    double const value = x[i];\n    double const lower_bound_0 = min_val + delta * rank / (double) (nthreads);\n    double const upper_bound_0 = min_val + delta * (rank+1) / (double) (nthreads);\n    double const lower_bound_1 = min_val + delta * (rank+0.25) / (double) (nthreads);\n    double const upper_bound_1 = min_val + delta * (rank+0.5) / (double) (nthreads);\n    double const lower_bound_2 = min_val + delta * (rank+0.5) / (double) (nthreads);\n    double const upper_bound_2 = min_val + delta * (rank+0.75) / (double) (nthreads);\n    double const lower_bound_3 = min_val + delta * (rank+0.75) / (double) (nthreads);\n    double const upper_bound_3 = min_val + delta * (rank+1) / (double) (nthreads);\n    \n    // if (i == 10) printf(\"rank=%d, threads=%d, value=%f, lower_bound_0=%f, upper_bound_0=%f\\n\", rank, nthreads, value, lower_bound_0, upper_bound_0);\n    if (lower_bound_0 <= value && value < upper_bound_0) {\n      bins_local[0] += 1;\n    }\n    if (lower_bound_1 <= value && value < upper_bound_1) {\n      bins_local[1] += 1;\n    }\n    if (lower_bound_2 <= value && value < upper_bound_2) {\n      bins_local[2] += 1;\n    }\n    if (lower_bound_3 <= value && value < upper_bound_3) {\n      bins_local[3] += 1;\n    }\n  }\n\n  MPI_Reduce(&bins_local[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (MPI_COMM_WORLD.Rank() == 0) {\n    bins[0] = 2;\n  }\n}",
            "// TODO: replace this code with a correct version.\n}",
            "// TODO: Fill in this function.\n    // TODO: Run it with mpirun -n 4./count_quartiles_mpi\n}",
            "// TODO: implement this function\n    size_t n = x.size();\n    bins = {0,0,0,0};\n}",
            "// TODO: Replace with your code\n  int rank;\n  int world_size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n  if (rank == 0) {\n    double *data = new double[x.size()];\n    std::copy(x.begin(), x.end(), data);\n#pragma omp parallel\n#pragma omp for\n    for (int i = 0; i < world_size; i++) {\n      size_t chunk = x.size() / world_size;\n      size_t begin = chunk * i;\n      size_t end = begin + chunk;\n      size_t chunk_size = end - begin;\n      std::array<size_t, 4> chunk_bins;\n      if (i == world_size - 1) {\n        end = x.size();\n      }\n      if (i == 0) {\n        begin = 0;\n      }\n      chunk_bins = countQuartiles(data + begin, chunk_size);\n      std::copy(chunk_bins.begin(), chunk_bins.end(), bins.begin() + begin);\n    }\n    delete[] data;\n  } else {\n    bins = countQuartiles(x);\n  }\n}",
            "// TODO: implement\n  int n_threads = omp_get_max_threads();\n  size_t n = x.size();\n  int n_procs, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &n_procs);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t n_local = n / n_procs;\n  std::vector<size_t> x_local(n_local);\n  std::vector<size_t> bins_local(4);\n  std::vector<size_t> bins_sum(4);\n  std::vector<size_t> bins_sum_temp(4);\n  MPI_Scatter(x.data(), n_local, MPI_DOUBLE, x_local.data(), n_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  #pragma omp parallel for num_threads(n_threads)\n  for (int i = 0; i < n_local; i++) {\n    if ((x_local[i] >= 0 && x_local[i] <= 0.25) || (x_local[i] >= 0.75 && x_local[i] <= 1)) {\n      bins_local[0]++;\n    } else if ((x_local[i] > 0.25 && x_local[i] <= 0.5) || (x_local[i] >= 0.5 && x_local[i] < 0.75)) {\n      bins_local[1]++;\n    } else if ((x_local[i] > 0.5 && x_local[i] <= 0.75) || (x_local[i] >= 0.25 && x_local[i] < 0.5)) {\n      bins_local[2]++;\n    } else {\n      bins_local[3]++;\n    }\n  }\n  MPI_Reduce(bins_local.data(), bins_sum.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  if (rank == 0) {\n    bins[0] = bins_sum[0];\n    bins[1] = bins_sum[1];\n    bins[2] = bins_sum[2];\n    bins[3] = bins_sum[3];\n  }\n}",
            "// TODO: implement\n}",
            "#pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < x.size(); i++) {\n    if (x[i] <= 0.25) {\n      bins[0]++;\n    } else if (x[i] <= 0.5) {\n      bins[1]++;\n    } else if (x[i] <= 0.75) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n}",
            "/* get the number of ranks and the rank of the process */\n  int num_ranks, rank;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  /* number of elements that each rank should process */\n  size_t local_length = x.size() / num_ranks;\n  /* get the remaining elements */\n  size_t remain = x.size() - local_length * num_ranks;\n\n  /* get the begin and end points of the current process's vector x */\n  double const *local_x = x.data() + local_length * rank;\n  double const *local_x_end = local_x + local_length;\n  if (rank < remain) {\n    local_x_end++;\n  }\n\n  /* initialize bin counters to zero */\n  bins.fill(0);\n\n  /* declare parallel region */\n  #pragma omp parallel\n  {\n\n    /* declare variables for loop iteration */\n    size_t i = 0;\n    double cur_x = 0;\n    double cur_x_end = 0;\n\n    /* declare variables for OpenMP */\n    int cur_thread = omp_get_thread_num();\n    int num_threads = omp_get_num_threads();\n\n    /* determine the range of x the current thread should process */\n    double local_delta = (local_x_end - local_x) / num_threads;\n    double start = local_x + cur_thread * local_delta;\n    double end = std::min(local_x + (cur_thread + 1) * local_delta, local_x_end);\n\n    /* declare variables for OpenMP */\n    size_t j = 0;\n    size_t num_pairs = 0;\n\n    /* iterate over the elements of x and count the number of elements in [start, end)\n       for which the fractional part in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1). */\n    for (i = 0; i < (size_t)num_ranks; ++i) {\n      cur_x = start + (end - start) * i / num_ranks;\n      cur_x_end = start + (end - start) * (i + 1) / num_ranks;\n      num_pairs = std::count_if(local_x, local_x_end,\n                                [cur_x, cur_x_end](double const &a) {\n                                  double cur_y = a - cur_x;\n                                  double cur_y_end = a - cur_x_end;\n                                  return cur_y >= 0 && cur_y < 0.25 && cur_y_end >= 0.25 && cur_y_end < 0.5;\n                                });\n      bins[0] += num_pairs;\n      num_pairs = std::count_if(local_x, local_x_end,\n                                [cur_x, cur_x_end](double const &a) {\n                                  double cur_y = a - cur_x;\n                                  double cur_y_end = a - cur_x_end;\n                                  return cur_y >= 0.25 && cur_y < 0.5 && cur_y_end >= 0.5 && cur_y_end < 0.75;\n                                });\n      bins[1] += num_pairs;\n      num_pairs = std::count_if(local_x, local_x_end,\n                                [cur_x, cur_x_end](double const &a) {\n                                  double cur_y = a - cur_x;\n                                  double cur_y_end = a - cur_x_end;\n                                  return cur_y >= 0.5 && cur_y < 0.75 && cur_y_end >= 0.75 && cur_y_end < 1;\n                                });\n      bins[2] += num_pairs;\n      num_pairs = std::count_if(local_x, local_x_end,\n                                [cur_x, cur_x_end](double const &a) {\n                                  double cur_y = a - cur_x;\n                                  double cur_y_end = a - cur_x_end;\n                                  return cur_y >= 0.75 && cur_y < 1 && cur_y_end >= 1;\n                                });\n      bins[3] += num_pairs;\n    }\n  }\n\n  /* sum the bin counters from all ranks */\n  std::vector<size_t>",
            "size_t const num_elements = x.size();\n  size_t const num_local_elements = num_elements/omp_get_num_threads();\n\n  size_t global_index = 0;\n  for (int i=0; i<omp_get_num_threads(); ++i) {\n    size_t local_index = i*num_local_elements;\n    size_t local_end = local_index + num_local_elements;\n    if (local_end > num_elements) local_end = num_elements;\n\n    #pragma omp parallel for \n    for (size_t j=local_index; j<local_end; ++j) {\n      double const& current_element = x[j];\n      if (current_element <= 0.25) ++bins[0];\n      else if (current_element <= 0.5) ++bins[1];\n      else if (current_element <= 0.75) ++bins[2];\n      else ++bins[3];\n    }\n  }\n\n  // Reduce the bins across all ranks\n  int world_size;\n  MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<size_t> local_bins(bins.size());\n  MPI_Gather(&bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, local_bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  bins = {0, 0, 0, 0};\n  if (rank == 0) {\n    for (size_t i=0; i<local_bins.size(); ++i) {\n      for (int j=0; j<world_size; ++j) {\n        bins[i] += local_bins[i];\n      }\n    }\n  }\n\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int myId;\n    MPI_Comm_rank(MPI_COMM_WORLD, &myId);\n    const int numProcs = 3;\n\n    size_t numDoubles = x.size();\n    std::vector<double> x_local(x.begin() + myId * numDoubles / numProcs,\n                                x.begin() + (myId + 1) * numDoubles / numProcs);\n\n    // Count the number of doubles in the vector x that have a fractional part \n    // in [0, 0.25), [0.25, 0.5), [0.5, 0.75), and [0.75, 1)\n    bins.fill(0);\n    // omp_set_num_threads(4);\n    #pragma omp parallel for reduction(+: bins[0], bins[1], bins[2], bins[3])\n    for (size_t i = 0; i < x_local.size(); ++i) {\n        double num = std::fmod(x_local[i], 1);\n        if (num <= 0.25) {\n            bins[0] += 1;\n        } else if (num <= 0.5) {\n            bins[1] += 1;\n        } else if (num <= 0.75) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n\n    std::array<size_t, 4> bins_global;\n    MPI_Reduce(bins.data(), bins_global.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    bins = bins_global;\n}",
            "// TODO: your code here\n}",
            "// TODO: Fill in this function.\n}",
            "// Your code goes here.\n}",
            "// TODO: implement this\n}",
            "// TODO: replace me\n}",
            "std::array<size_t, 4> counts;\n    double quartile = 0.25;\n    for (int i = 0; i < 4; i++) {\n        counts[i] = 0;\n    }\n    // YOUR CODE HERE\n    // 1. Use OpenMP parallel region to parallelize the following loop.\n    // 2. Use MPI_Comm_rank(MPI_COMM_WORLD,...) to determine if the current rank is 0, and\n    //    send data to rank 0 for aggregation.\n    #pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < x.size(); i++) {\n            double fraction = x[i] - floor(x[i]);\n            if (fraction >= quartile) {\n                counts[0]++;\n            }\n            else if (fraction >= quartile * 2) {\n                counts[1]++;\n            }\n            else if (fraction >= quartile * 3) {\n                counts[2]++;\n            }\n            else {\n                counts[3]++;\n            }\n        }\n    }\n\n    // Reduce bins across all ranks, and store the result on rank 0\n    MPI_Reduce(counts.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//TODO\n}",
            "MPI_Datatype MPI_DOUBLE = MPI_DOUBLE_PRECISION;\n    MPI_Comm MPI_COMM_WORLD = MPI_COMM_WORLD;\n    MPI_Status status;\n    size_t n = x.size();\n\n    // divide number of processes evenly\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int numProcesses = size;\n\n    // TODO: your code here\n\n    // send size of x to every process\n    // broadcast size of x to every process\n    // broadcast size of x to every process\n\n    // start timer\n    auto start = std::chrono::system_clock::now();\n\n    // create x\n    double *localX = new double[n];\n    int sizeLocalX = n / numProcesses;\n    for (int i = 0; i < sizeLocalX * rank; i++) {\n        localX[i] = x[i];\n    }\n    for (int i = sizeLocalX * rank + sizeLocalX; i < n; i++) {\n        localX[i - sizeLocalX * rank] = x[i];\n    }\n\n    // create bins\n    int *localBins = new int[4];\n    for (int i = 0; i < 4; i++) {\n        localBins[i] = 0;\n    }\n\n    // create size\n    int *localSize = new int;\n    *localSize = sizeLocalX;\n\n    // start timer\n    auto startOmp = std::chrono::system_clock::now();\n\n    // compute\n    #pragma omp parallel for\n    for (int i = 0; i < sizeLocalX; i++) {\n        double temp = localX[i];\n        if (temp < 0.25) {\n            localBins[0]++;\n        } else if (temp < 0.5) {\n            localBins[1]++;\n        } else if (temp < 0.75) {\n            localBins[2]++;\n        } else {\n            localBins[3]++;\n        }\n    }\n\n    // start timer\n    auto endOmp = std::chrono::system_clock::now();\n\n    // end time for compute\n\n    // start timer\n    auto startMPI = std::chrono::system_clock::now();\n\n    // broadcast bins\n    MPI_Bcast(localBins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n    for (int i = 0; i < 4; i++) {\n        bins[i] += localBins[i];\n    }\n\n    // broadcast size\n    MPI_Bcast(localSize, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    int sizeGlobalX = *localSize * numProcesses;\n\n    // start timer\n    auto endMPI = std::chrono::system_clock::now();\n\n    // end time for MPI\n\n    // start timer\n    auto startGet = std::chrono::system_clock::now();\n\n    // get all processes' x together\n    double *globalX = new double[sizeGlobalX];\n    MPI_Gather(localX, *localSize, MPI_DOUBLE, globalX, *localSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    // start timer\n    auto endGet = std::chrono::system_clock::now();\n\n    // end time for get\n\n    // start timer\n    auto startOmp2 = std::chrono::system_clock::now();\n\n    // compute\n    #pragma omp parallel for\n    for (int i = 0; i < sizeGlobalX; i++) {\n        double temp = globalX[i];\n        if (temp < 0.25) {\n            bins[0]++;\n        } else if (temp < 0.5) {\n            bins[1]++;\n        } else if (temp < 0.75) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n\n    // start timer\n    auto endOmp2 = std::chrono::system_clock::now();\n\n    // end time for compute\n\n    // start timer\n    auto startMPI2 = std::chrono::system_clock::now();\n\n    // send bins\n    MPI_Gather(localBins, 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // start timer\n    auto endMPI2 = std::chrono::system_clock::now();\n\n    // end time for MPI\n\n    // start timer\n    auto startFree = std::chrono::system_clock::now();\n\n    // free memory\n    delete[] localX;\n    delete[] localBins;\n    delete[] localSize;\n    delete[] globalX;\n\n    // start timer\n    auto endFree = std::chrono::system_clock::now();\n\n    // end time",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// Get number of elements.\n  size_t num_elements = x.size();\n  size_t num_threads;\n\n  // Get the rank of this process.\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // Initialize the array bins.\n  if (rank == 0) {\n    bins = {0, 0, 0, 0};\n  }\n\n  // Divide the elements among the threads.\n  #pragma omp parallel\n    {\n      num_threads = omp_get_num_threads();\n\n      // Get the ID of the thread.\n      int thread_id = omp_get_thread_num();\n\n      // Compute the number of elements this thread will process.\n      size_t num_elements_per_thread = num_elements / num_threads;\n      size_t first_element = thread_id * num_elements_per_thread;\n      size_t last_element = first_element + num_elements_per_thread;\n      if (thread_id == num_threads - 1) {\n        last_element = num_elements;\n      }\n\n      // Determine the number of elements that fall into each bin.\n      size_t elements_in_bin0 = 0, elements_in_bin1 = 0, elements_in_bin2 = 0, elements_in_bin3 = 0;\n      for (size_t i = first_element; i < last_element; ++i) {\n        double fractional_part = x[i] - std::floor(x[i]);\n        if (fractional_part < 0.25) {\n          ++elements_in_bin0;\n        }\n        else if (fractional_part < 0.5) {\n          ++elements_in_bin1;\n        }\n        else if (fractional_part < 0.75) {\n          ++elements_in_bin2;\n        }\n        else {\n          ++elements_in_bin3;\n        }\n      }\n\n      // Store the results in bins.\n      #pragma omp critical\n      {\n        bins[0] += elements_in_bin0;\n        bins[1] += elements_in_bin1;\n        bins[2] += elements_in_bin2;\n        bins[3] += elements_in_bin3;\n      }\n    }\n\n  // Sum the results.\n  std::array<size_t, 4> sum_bins;\n  MPI_Reduce(bins.data(), sum_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  bins = sum_bins;\n}",
            "const size_t n = x.size();\n  const size_t id = omp_get_thread_num();\n  const size_t numThreads = omp_get_num_threads();\n  const size_t numProcs = 2; // change to the number of processors you are using\n  const size_t chunk = n / numThreads;\n  const size_t remainder = n % numThreads;\n  const size_t idFirst = chunk * id + std::min(id, remainder);\n  const size_t idLast = idFirst + chunk + (id < remainder? 1 : 0);\n\n  size_t localBins[4];\n  size_t localBinsSum[4] = {0, 0, 0, 0};\n  for (size_t i = idFirst; i < idLast; ++i) {\n    double x_i = x[i];\n    localBins[0] += x_i < 0.25;\n    localBins[1] += x_i >= 0.25 && x_i < 0.5;\n    localBins[2] += x_i >= 0.5 && x_i < 0.75;\n    localBins[3] += x_i >= 0.75;\n    localBinsSum[0] += localBins[0];\n    localBinsSum[1] += localBins[1];\n    localBinsSum[2] += localBins[2];\n    localBinsSum[3] += localBins[3];\n  }\n\n  std::array<size_t, 4> allBinsSum;\n#pragma omp parallel num_threads(numProcs) reduction(+:allBinsSum)\n  {\n    allBinsSum[0] += localBinsSum[0];\n    allBinsSum[1] += localBinsSum[1];\n    allBinsSum[2] += localBinsSum[2];\n    allBinsSum[3] += localBinsSum[3];\n  }\n\n  MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n\n}",
            "// Fill bins with zeros\n    bins = std::array<size_t, 4>();\n    \n    // TODO: compute the count of values in each bin.\n    \n    // TODO: distribute the bins to each process by MPI.\n    \n    // TODO: for each process, compute the count of values in each bin.\n}",
            "// TODO: implement this function\n    const int num_threads = omp_get_max_threads();\n    const int num_ranks = MPI_COMM_WORLD.size();\n    const int rank = MPI_COMM_WORLD.rank();\n    const int world_size = MPI_COMM_WORLD.size();\n    const int data_per_thread = x.size() / num_threads;\n    const int data_per_rank = x.size() / num_ranks;\n    const int remainder = x.size() % num_threads;\n    const int remainder_ranks = x.size() % num_ranks;\n    const int start_id = rank * data_per_rank;\n    const int end_id = ((rank + 1) * data_per_rank) - 1;\n    std::vector<double> x_rank(data_per_rank);\n    // MPI_Scatter(&x, sizeof(double), MPI_DOUBLE, &x_rank, sizeof(double), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(&x[0], data_per_rank, MPI_DOUBLE, &x_rank[0], data_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    // std::vector<double> x_rank(data_per_rank);\n    // std::copy(x.begin() + start_id, x.begin() + end_id, x_rank.begin());\n    std::vector<int> num_lower_quartile(num_threads);\n    std::vector<int> num_upper_quartile(num_threads);\n    int num_lower_q_total = 0;\n    int num_upper_q_total = 0;\n    #pragma omp parallel for reduction(+: num_lower_q_total)\n    for (int i = 0; i < data_per_thread; ++i) {\n        double frac = x_rank[i] % 1.0;\n        if (frac <= 0.25) {\n            num_lower_quartile[omp_get_thread_num()]++;\n        }\n        if (frac >= 0.75) {\n            num_upper_quartile[omp_get_thread_num()]++;\n        }\n    }\n    #pragma omp parallel for reduction(+: num_lower_q_total)\n    for (int i = data_per_thread; i < data_per_thread * num_threads; ++i) {\n        double frac = x_rank[i] % 1.0;\n        if (frac <= 0.25) {\n            num_lower_quartile[omp_get_thread_num()]++;\n        }\n        if (frac >= 0.75) {\n            num_upper_quartile[omp_get_thread_num()]++;\n        }\n    }\n    for (int i = 0; i < num_threads; ++i) {\n        num_lower_q_total += num_lower_quartile[i];\n    }\n    for (int i = 0; i < num_threads; ++i) {\n        num_upper_q_total += num_upper_quartile[i];\n    }\n    int num_lower_q_local = num_lower_q_total / num_threads;\n    int num_upper_q_local = num_upper_q_total / num_threads;\n    std::vector<int> num_lower_q(1);\n    std::vector<int> num_upper_q(1);\n    MPI_Gather(&num_lower_q_local, 1, MPI_INT, &num_lower_q[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Gather(&num_upper_q_local, 1, MPI_INT, &num_upper_q[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    bins[0] = num_lower_q[0];\n    bins[1] = num_lower_q[0] + num_upper_q[0];\n    bins[2] = num_lower_q[0] + num_upper_q[0] * 2;\n    bins[3] = num_lower_q[0] + num_upper_q[0] * 3;\n}",
            "//TODO: Implement\n}",
            "// Your code here.\n    bins = {0, 0, 0, 0};\n    std::vector<size_t> local_bins = {0, 0, 0, 0};\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int size = x.size();\n    int chunk_size = size / world_size;\n    int start = world_rank * chunk_size;\n    int end = (world_rank + 1) * chunk_size;\n    for (int i = start; i < end; i++)\n    {\n        double val = x[i];\n        int index = 0;\n        if (val >= 0 && val < 0.25)\n            index = 0;\n        else if (val >= 0.25 && val < 0.5)\n            index = 1;\n        else if (val >= 0.5 && val < 0.75)\n            index = 2;\n        else if (val >= 0.75 && val <= 1)\n            index = 3;\n        local_bins[index] += 1;\n    }\n\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n    // OpenMP\n    #pragma omp parallel\n    {\n        int start = omp_get_thread_num() * chunk_size;\n        int end = (omp_get_thread_num() + 1) * chunk_size;\n\n        std::array<size_t, 4> local_bins;\n        local_bins = {0, 0, 0, 0};\n        for (int i = start; i < end; i++)\n        {\n            double val = x[i];\n            int index = 0;\n            if (val >= 0 && val < 0.25)\n                index = 0;\n            else if (val >= 0.25 && val < 0.5)\n                index = 1;\n            else if (val >= 0.5 && val < 0.75)\n                index = 2;\n            else if (val >= 0.75 && val <= 1)\n                index = 3;\n            local_bins[index] += 1;\n        }\n\n        #pragma omp critical\n        {\n            bins[0] += local_bins[0];\n            bins[1] += local_bins[1];\n            bins[2] += local_bins[2];\n            bins[3] += local_bins[3];\n        }\n    }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "kokkos",
        "prompt": "#include <Kokkos_Core.hpp>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use Kokkos to count in parallel. Assume Kokkos has already been initialized.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(Kokkos::View<const Point*> const& points, Kokkos::View<size_t[4]> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(points.size(), Kokkos::AUTO);\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const Kokkos::TeamThreadRange &r) {\n      Kokkos::parallel_for(Kokkos::ThreadVectorRange(r, 4), [&](size_t i) {\n         bins[i] = 0;\n      });\n      Kokkos::parallel_for(r, [&](size_t i) {\n         Point const& p = points(i);\n         if (p.x > 0 && p.y > 0) {\n            ++bins[0];\n         } else if (p.x < 0 && p.y > 0) {\n            ++bins[1];\n         } else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      });\n   });\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::RangePolicy<Kokkos::Serial>>(0, points.extent(0)),\n      [&](const int i) {\n         // TODO\n      }\n   );\n   // TODO: Wait for parallel for to complete.\n}",
            "// TODO: write function here\n}",
            "// TODO: allocate and initialize the bins\n}",
            "// Your code here\n\n   // For example, here is one way to do this in parallel:\n   Kokkos::parallel_for(Kokkos::RangePolicy<>(0, points.extent(0)),\n      KOKKOS_LAMBDA (int i) {\n         Point p = points(i);\n         if (p.x > 0 && p.y > 0)\n            bins[0]++;\n         else if (p.x < 0 && p.y > 0)\n            bins[1]++;\n         else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n         else\n            bins[3]++;\n   });\n}",
            "// TODO: implement this function!\n  // TODO: use Kokkos to parallelize this function!\n  // TODO: count number of points in each quadrant, and store the result in `bins`\n  // hint: use Kokkos::parallel_for to launch the kernel\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: allocate a view for the output\n\n   // TODO: count the number of points in each quadrant using Kokkos parallel for\n}",
            "// TODO: Complete this function.\n}",
            "Kokkos::TeamPolicy<Kokkos::TeamDefault> policy(points.size(), Kokkos::AUTO);\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA (const Kokkos::TeamThreadRange& team_r, const Kokkos::ThreadVectorRange& thread_r) {\n      size_t quadrant = 0;\n      for (size_t i = team_r.league_rank(); i < points.size(); i += team_r.league_size()) {\n         const Point& p = points[i];\n         if (p.x > 0 && p.y >= 0)\n            quadrant = 0;\n         else if (p.x <= 0 && p.y > 0)\n            quadrant = 1;\n         else if (p.x < 0 && p.y <= 0)\n            quadrant = 2;\n         else\n            quadrant = 3;\n         bins(quadrant) += 1;\n      }\n   });\n   Kokkos::fence();\n   // parallel_for() creates a private copy of bins so we need to sync it\n   Kokkos::deep_copy(bins, bins);\n}",
            "// TODO: implement this function\n}",
            "}",
            "// YOUR CODE HERE\n   Kokkos::parallel_for(\"QuadrantCounter\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()), KOKKOS_LAMBDA(const int i) {\n       Point p = points(i);\n       double xp = p.x;\n       double yp = p.y;\n       int quadrant = (int)(xp > 0.0? (yp > 0.0? 1 : 2) : (yp > 0.0? 3 : 4));\n       Kokkos::atomic_fetch_add(&bins(quadrant), 1);\n   });\n}",
            "// YOUR CODE HERE\n}",
            "Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(points.size(), 1);\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA (const Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace>::member_type &teamMember) {\n      Kokkos::parallel_reduce(Kokkos::TeamThreadRange(teamMember, points.size()), KOKKOS_LAMBDA (const size_t &pointId, size_t &count) {\n         const auto &point = points(pointId);\n         if (point.x > 0 && point.y > 0) {\n            count += 1;\n         } else if (point.x < 0 && point.y > 0) {\n            count += 2;\n         } else if (point.x < 0 && point.y < 0) {\n            count += 3;\n         } else {\n            count += 4;\n         }\n      }, Kokkos::Sum<size_t>(bins));\n   });\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n\n   // Use parallel_reduce to count the number of points in each quadrant.\n   // Store the counts in the bins array.\n}",
            "// TODO\n   return;\n}",
            "// TODO: Fill in the body of this function.\n   size_t num_points = points.extent(0);\n\n   Kokkos::parallel_for(\"Quadrant Count\", num_points, KOKKOS_LAMBDA(const size_t i){\n       Point point = points(i);\n       int x = (int) point.x;\n       int y = (int) point.y;\n       int index = 0;\n       if (y >= 0) {\n           index += 2;\n       }\n       if (x >= 0) {\n           index += 1;\n       }\n       bins(index)++;\n   });\n}",
            "// TODO: count quadrants in parallel using Kokkos\n}",
            "// TODO: Implement me.\n}",
            "// TODO\n}",
            "}",
            "// TODO: Fill in the missing code.\n   // Hint: Use Kokkos::parallel_for and Kokkos::atomic.\n   // Hint: To access a given element of a Kokkos::View, use Kokkos::subview(view,...)\n}",
            "// TODO: Implement this function\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n   Kokkos::parallel_for(Kokkos::RangePolicy<ExecutionSpace>(0, points.size()), KOKKOS_LAMBDA(const size_t i) {\n      double x = points(i).x;\n      double y = points(i).y;\n      if (x > 0 && y > 0) {\n         bins(0) += 1;\n      } else if (x < 0 && y > 0) {\n         bins(1) += 1;\n      } else if (x < 0 && y < 0) {\n         bins(2) += 1;\n      } else {\n         bins(3) += 1;\n      }\n   });\n}",
            "// TODO: implement me\n  // Hint: you can use Kokkos::TeamPolicy to distribute the workload.\n}",
            "// TODO: Write this function\n}",
            "// TODO: implement\n}",
            "// YOUR CODE HERE\n   for (int i = 0; i < 4; ++i) {\n      bins(i) = 0;\n   }\n   Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA (const int i) {\n      if (points(i).x > 0) {\n         if (points(i).y > 0) {\n            bins(0) += 1;\n         } else {\n            bins(1) += 1;\n         }\n      } else {\n         if (points(i).y > 0) {\n            bins(2) += 1;\n         } else {\n            bins(3) += 1;\n         }\n      }\n   });\n   Kokkos::fence();\n}",
            "// TODO implement me!\n\n}",
            "// TODO\n   // Complete this function\n}",
            "/* TODO: implement this function */\n}",
            "/* TODO */\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n   KOKKOS_LAMBDA(int i) {\n      if (points(i).x > 0 && points(i).y > 0) {\n         bins(0)++;\n      } else if (points(i).x < 0 && points(i).y > 0) {\n         bins(1)++;\n      } else if (points(i).x < 0 && points(i).y < 0) {\n         bins(2)++;\n      } else {\n         bins(3)++;\n      }\n   });\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement this function\n}",
            "// YOUR CODE HERE\n   //\n   // HINT: You may want to declare a parallel_for to loop over the points in parallel\n   // HINT: To access a specific element in a view, you must first convert the View into a Kokkos::HostSpace pointer\n   // HINT: The count in each quadrant must be stored in `bins`\n   // HINT: `Kokkos::single` will create a new execution space that runs the function in serial\n}",
            "// TODO\n}",
            "// Compute the number of points in each quadrant.\n   // You can use parallel_for with a lambda to do this.\n   // This should be quite easy using Kokkos::RangePolicy.\n\n   // Count the number of points in each quadrant.\n   // Again, use parallel_for with a lambda to do this.\n\n}",
            "// TODO\n}",
            "// TODO: add your code here\n    bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n\n    Kokkos::parallel_for(\"loop1\", points.extent(0), KOKKOS_LAMBDA(size_t i){\n        if(points(i).x > 0 && points(i).y > 0)\n            bins(0) += 1;\n        else if(points(i).x < 0 && points(i).y > 0)\n            bins(1) += 1;\n        else if(points(i).x < 0 && points(i).y < 0)\n            bins(2) += 1;\n        else\n            bins(3) += 1;\n    });\n}",
            "// TODO: fill in this function\n}",
            "}",
            "// TODO\n   Kokkos::abort(\"TODO\");\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function\n   return;\n}",
            "// TODO: Implement this function.\n}",
            "// TODO\n   // Hint: use Kokkos parallel_for() and lambda expressions\n}",
            "// YOUR CODE HERE\n   // Hint: Use Kokkos::parallel_for to count in parallel.\n   // You can also use Kokkos::reduce_view to sum counts.\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\"count_quadrants\", points.size(), KOKKOS_LAMBDA(size_t i) {\n      const Point& p = points(i);\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   });\n   Kokkos::fence();\n}",
            "// TODO\n}",
            "auto range = Kokkos::Experimental::require(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)), Kokkos::Experimental::WorkItemProperty::HintLightWeight);\n   Kokkos::Experimental::parallel_for(range, [=](size_t i){\n      double x = points(i).x, y = points(i).y;\n      bins(x > 0? (y > 0? 0 : 3) : (y > 0? 1 : 2)) += 1;\n   });\n   bins(3) += points.extent(0);\n}",
            "// TODO: Implement this function.\n  // This is where you will do the work.\n  // You may find it helpful to create subviews to the `points` view for each of the quadrants.\n}",
            "// TODO\n}",
            "// TODO: Fill in the body\n}",
            "// TODO\n}",
            "/* TODO: implement this function.\n    * The only function you're allowed to use is the one defined in Kokkos_Core.hpp\n    */\n}",
            "// TODO\n   // hint: bin[quadrant] =...\n}",
            "// TODO: Fill this in.\n}",
            "Kokkos::parallel_for(\"count quadrants\", points.size(), KOKKOS_LAMBDA(int i) {\n      if (points(i).x >= 0) {\n         if (points(i).y >= 0) {\n            bins(0)++;\n         } else {\n            bins(1)++;\n         }\n      } else {\n         if (points(i).y >= 0) {\n            bins(2)++;\n         } else {\n            bins(3)++;\n         }\n      }\n   });\n   Kokkos::fence();\n}",
            "//TODO: count the number of points in each quadrant\n}",
            "const size_t num_points = points.size();\n   const Point* point_array = points.data();\n\n   // TODO: KOKKOS: Add the code here to count the points in each quadrant,\n   // storing the result in bins.\n   // See the documentation for the Kokkos::parallel_for_each method.\n}",
            "// TODO\n}",
            "// TODO: implement this function\n\n    // hint: bin[0] corresponds to the number of points in quadrant 1, bin[1] corresponds to quadrant 2, etc.\n    // use parallel_for to loop over the entire array\n\n    // TODO: replace this with your code\n    int num_points = points.extent(0);\n    bins(0) = 0;\n    bins(1) = 0;\n    bins(2) = 0;\n    bins(3) = 0;\n    //\n    for(int i=0; i < num_points; i++)\n    {\n        if(points(i).x > 0 && points(i).y > 0)\n            bins(0)++;\n        else if(points(i).x < 0 && points(i).y > 0)\n            bins(1)++;\n        else if(points(i).x < 0 && points(i).y < 0)\n            bins(2)++;\n        else\n            bins(3)++;\n    }\n}",
            "}",
            "// TODO: Complete the implementation\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n      KOKKOS_LAMBDA(const size_t i) {\n         Point p = points(i);\n         if (p.x > 0 && p.y > 0)\n            ++bins(0);\n         else if (p.x < 0 && p.y > 0)\n            ++bins(1);\n         else if (p.x < 0 && p.y < 0)\n            ++bins(2);\n         else\n            ++bins(3);\n      });\n}",
            "// TODO: implement this function\n   return;\n}",
            "// TODO: add code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::Rank<2>>(0, points.extent(0)),\n    KOKKOS_LAMBDA(const int& i) {\n      double x = points(i).x;\n      double y = points(i).y;\n      if (x >= 0 && y >= 0) {\n         Kokkos::atomic_fetch_add(&bins(0), 1);\n      }\n      else if (x < 0 && y >= 0) {\n         Kokkos::atomic_fetch_add(&bins(1), 1);\n      }\n      else if (x >= 0 && y < 0) {\n         Kokkos::atomic_fetch_add(&bins(2), 1);\n      }\n      else {\n         Kokkos::atomic_fetch_add(&bins(3), 1);\n      }\n    });\n   Kokkos::fence();\n}",
            "auto quadrant = KOKKOS_LAMBDA (const size_t i) {\n      auto p = points(i);\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   };\n\n   Kokkos::parallel_for(points.extent(0), quadrant);\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "// Implement this function\n}",
            "auto policy = Kokkos::TeamPolicy<>::team_policy(points.extent(0), Kokkos::AUTO);\n   Kokkos::parallel_for(policy, KOKKOS_LAMBDA(const Kokkos::TeamMember& teamMember) {\n      auto& local_bins = bins();\n      Kokkos::parallel_for(Kokkos::TeamThreadRange(teamMember, 0, 4), [&](size_t quadrant) {\n         local_bins[quadrant] = 0;\n      });\n      Kokkos::parallel_for(Kokkos::TeamThreadRange(teamMember, 0, points.extent(0)), [&](size_t i) {\n         auto point = points(i);\n         if (point.x > 0 && point.y > 0)\n            local_bins[0]++;\n         else if (point.x < 0 && point.y > 0)\n            local_bins[1]++;\n         else if (point.x < 0 && point.y < 0)\n            local_bins[2]++;\n         else\n            local_bins[3]++;\n      });\n      Kokkos::parallel_for(Kokkos::TeamThreadRange(teamMember, 0, 4), [&](size_t quadrant) {\n         teamMember.team_barrier();\n         Kokkos::atomic_fetch_add(&bins(quadrant), local_bins[quadrant]);\n      });\n   });\n}",
            "/* TODO */\n}",
            "Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n      KOKKOS_LAMBDA(size_t i) {\n         size_t quad = -1;\n         if (points(i).x >= 0) {\n            if (points(i).y >= 0) {\n               quad = 0;\n            }\n            else {\n               quad = 1;\n            }\n         }\n         else {\n            if (points(i).y >= 0) {\n               quad = 2;\n            }\n            else {\n               quad = 3;\n            }\n         }\n         ++bins(quad);\n      }\n   );\n   Kokkos::fence();\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA (size_t i) {\n      // Your implementation here\n   });\n}",
            "// write code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), [&] (size_t i) {\n      if (points(i).x > 0 && points(i).y > 0)\n         ++bins(0);\n      else if (points(i).x < 0 && points(i).y > 0)\n         ++bins(1);\n      else if (points(i).x < 0 && points(i).y < 0)\n         ++bins(2);\n      else // points(i).x > 0 && points(i).y < 0\n         ++bins(3);\n   });\n}",
            "// FIXME: Fill in here.\n}",
            "//...\n}",
            "// YOUR CODE HERE\n    int n = points.extent(0);\n\n    Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, n), KOKKOS_LAMBDA(int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n\n        if (x > 0.0 && y > 0.0)\n            bins(0)++;\n        else if (x < 0.0 && y > 0.0)\n            bins(1)++;\n        else if (x < 0.0 && y < 0.0)\n            bins(2)++;\n        else\n            bins(3)++;\n    });\n}",
            "// TODO: Implement this method.\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "// Compute the number of threads that the Kokkos runtime will use.\n   const int num_threads = Kokkos::hw_thread_parallelism();\n\n   // Determine the number of blocks to use.\n   const size_t num_blocks = std::min<size_t>(num_threads, points.extent(0));\n\n   // Create a view of the counts to update.\n   Kokkos::View<size_t[4]> block_counts(\"counts\", num_blocks);\n\n   // Create a view of the quadrant indices.\n   Kokkos::View<int[4]> quadrants(\"quadrants\", num_blocks);\n\n   // Launch the parallel_for kernel to count the points.\n   Kokkos::parallel_for(\n      \"quadrant_count\", Kokkos::RangePolicy<size_t, Kokkos::Schedule<Kokkos::Dynamic>>(0, points.extent(0)),\n      KOKKOS_LAMBDA(const size_t i) {\n         // Determine the quadrant index.\n         quadrants(i) = (points(i).x >= 0 && points(i).y >= 0)? 0 : (points(i).x < 0 && points(i).y >= 0)? 1 : (points(i).x >= 0 && points(i).y < 0)? 2 : 3;\n\n         // Increment the counter for the quadrant.\n         block_counts(quadrants(i)) += 1;\n      }\n   );\n\n   // Sum the quadrant counts from each block.\n   Kokkos::View<size_t[4]> block_sums(\"sums\", num_blocks);\n   Kokkos::parallel_scan(\n      \"block_scan\", Kokkos::RangePolicy<size_t, Kokkos::Schedule<Kokkos::Dynamic>>(0, num_blocks),\n      KOKKOS_LAMBDA(const size_t i, size_t& update, bool final) {\n         if (final) {\n            block_sums(i) = block_counts(i);\n         } else {\n            block_sums(i) = block_counts(i) + block_sums(i - 1);\n         }\n      }, block_sums\n   );\n\n   // Copy the block sums back to the host.\n   std::array<size_t, 4> sums;\n   Kokkos::deep_copy(sums, block_sums);\n\n   // Set the values in the output array.\n   bins(0) = sums[0];\n   bins(1) = sums[1];\n   bins(2) = sums[2];\n   bins(3) = sums[3];\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), [&] (size_t i) {\n      Point p = points(i);\n      if(p.x >= 0 && p.y >= 0) bins[0]++;\n      else if(p.x < 0 && p.y >= 0) bins[1]++;\n      else if(p.x < 0 && p.y < 0) bins[2]++;\n      else if(p.x >= 0 && p.y < 0) bins[3]++;\n   });\n}",
            "/* YOUR CODE HERE */\n}",
            "Kokkos::parallel_for(\"count quadrants\", Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)), [&] (size_t i) {\n      double x = points(i).x, y = points(i).y;\n      if (x > 0 && y > 0) bins(0)++;\n      else if (x < 0 && y > 0) bins(1)++;\n      else if (x < 0 && y < 0) bins(2)++;\n      else if (x > 0 && y < 0) bins(3)++;\n   });\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: complete this function\n   throw std::runtime_error(\"TODO: Implement countQuadrants()\");\n}",
            "// TODO: implement this function\n}",
            "}",
            "// Fill in your code here.\n}",
            "// TODO: Write code here\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()), [=](int i) {\n       double x = points(i).x;\n       double y = points(i).y;\n       if (x >= 0 && y >= 0) {\n          bins(0) += 1;\n       } else if (x < 0 && y >= 0) {\n          bins(1) += 1;\n       } else if (x < 0 && y < 0) {\n          bins(2) += 1;\n       } else {\n          bins(3) += 1;\n       }\n   });\n}",
            "// YOUR CODE HERE\n   Kokkos::parallel_for(\"count_quadrants\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA (const size_t i) {\n      int quadrant = (points(i).x > 0? 1 : -1) + (points(i).y > 0? 2 : -2);\n      Kokkos::atomic_fetch_add(&(bins(quadrant)), 1);\n   });\n}",
            "Kokkos::parallel_for(\"count_quadrants\", points.extent(0), KOKKOS_LAMBDA (const int i) {\n      Point const& p = points(i);\n      int bin;\n      if (p.x >= 0 && p.y >= 0)\n         bin = 0;\n      else if (p.x < 0 && p.y >= 0)\n         bin = 1;\n      else if (p.x >= 0 && p.y < 0)\n         bin = 2;\n      else\n         bin = 3;\n      bins(bin) += 1;\n   });\n}",
            "// TODO: Implement this function.\n\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()), [&] (size_t i) {\n      Point p = points(i);\n\n      if (p.x >= 0 && p.y >= 0) {\n         bins(0)++;\n      } else if (p.x <= 0 && p.y >= 0) {\n         bins(1)++;\n      } else if (p.x <= 0 && p.y <= 0) {\n         bins(2)++;\n      } else {\n         bins(3)++;\n      }\n   });\n}",
            "// TODO: implement this function.\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n        KOKKOS_LAMBDA(const int i) {\n            double x = points(i).x;\n            double y = points(i).y;\n\n            int quadrant = (x > 0? (y > 0? 0 : 3) : (y > 0? 1 : 2));\n            bins(quadrant) += 1;\n        });\n\n    Kokkos::fence();\n}",
            "// TODO: Your code here\n   auto my_policy = Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0));\n   Kokkos::parallel_for(my_policy, [&points, &bins](int i) {\n       const Point* p = points(i);\n       if(p->x >= 0.0 && p->y >= 0.0) ++bins(0);\n       else if(p->x < 0.0 && p->y >= 0.0) ++bins(1);\n       else if(p->x < 0.0 && p->y < 0.0) ++bins(2);\n       else ++bins(3);\n   });\n   Kokkos::fence();\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: fill in\n    // Hint:\n    //   * Use Kokkos::parallel_for to count points in each quadrant in parallel\n    //   * Use the Kokkos::single() execution space to count the number of points in the current quadrant.\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement function\n    return;\n}",
            "Kokkos::parallel_for(\"countQuadrants\", points.extent(0),\n      KOKKOS_LAMBDA(size_t i) {\n         if (points(i).x > 0 && points(i).y > 0) {\n            bins(0) += 1;\n         } else if (points(i).x < 0 && points(i).y > 0) {\n            bins(1) += 1;\n         } else if (points(i).x < 0 && points(i).y < 0) {\n            bins(2) += 1;\n         } else {\n            bins(3) += 1;\n         }\n      });\n\n   Kokkos::fence();\n}",
            "/* Initialize the bins */\n    Kokkos::deep_copy(bins, (size_t)0);\n\n    /* TODO: Count the points in each quadrant. */\n    // auto quadrant_map = Kokkos::View<int*[4]>(\"quadrant_map\", {points.extent(0), 4});\n    // auto quadrants = Kokkos::View<int*>(\"quadrants\", points.extent(0));\n    // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA(const int& i) {\n    //     for (int j = 0; j < 4; j++) {\n    //         if (points(i).x < 0) {\n    //             if (points(i).y < 0) {\n    //                 quadrants(i) = 0;\n    //             } else {\n    //                 quadrants(i) = 1;\n    //             }\n    //         } else {\n    //             if (points(i).y < 0) {\n    //                 quadrants(i) = 2;\n    //             } else {\n    //                 quadrants(i) = 3;\n    //             }\n    //         }\n    //         quadrant_map(i, j) = quadrants(i);\n    //     }\n    // });\n    // auto quadrant_counts = Kokkos::View<size_t*>(\"quadrant_counts\", 4);\n    // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, 4), KOKKOS_LAMBDA(const int& i) {\n    //     quadrant_counts(i) = 0;\n    // });\n    // Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA(const int& i) {\n    //     quadrant_counts(quadrant_map(i, 0))++;\n    //     quadrant_counts(quadrant_map(i, 1))++;\n    //     quadrant_counts(quadrant_map(i, 2))++;\n    //     quadrant_counts(quadrant_map(i, 3))++;\n    // });\n    // Kokkos::deep_copy(bins, quadrant_counts);\n}",
            "/* TODO */\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "auto const n = points.size();\n\n   // TODO: Count the number of points in each quadrant\n\n   // TODO: Copy the counts back to the host and print them\n\n   // TODO: Verify that the counts are correct.\n   //   Make sure that all of the points in the first quadrant are in the first bin,\n   //   and that all of the points in the last quadrant are in the last bin.\n}",
            "// TODO: implement\n}",
            "// YOUR CODE HERE\n}",
            "auto bins_h = Kokkos::create_mirror_view(bins);\n   Kokkos::deep_copy(bins_h, bins);\n   auto points_h = Kokkos::create_mirror_view(points);\n   Kokkos::deep_copy(points_h, points);\n\n   for (auto p = points_h.data(); p < points_h.data() + points_h.extent(0); ++p) {\n      if (p->x > 0 && p->y > 0) ++bins_h(0);\n      else if (p->x < 0 && p->y > 0) ++bins_h(1);\n      else if (p->x < 0 && p->y < 0) ++bins_h(2);\n      else if (p->x > 0 && p->y < 0) ++bins_h(3);\n   }\n\n   Kokkos::deep_copy(bins, bins_h);\n}",
            "/* TODO: Fill in the body of this function */\n}",
            "size_t N = points.extent(0);\n\n   Kokkos::parallel_for(\n      Kokkos::RangePolicy<Kokkos::Serial>(0, N),\n      KOKKOS_LAMBDA(const size_t& i) {\n         Point p = points(i);\n\n         if (p.x > 0) {\n            if (p.y > 0) {\n               bins(0) += 1;\n            } else {\n               bins(1) += 1;\n            }\n         } else {\n            if (p.y > 0) {\n               bins(2) += 1;\n            } else {\n               bins(3) += 1;\n            }\n         }\n      });\n\n   Kokkos::fence();\n\n   // for (size_t i = 0; i < 4; i++) {\n   //    cout << \"bin \" << i << \" has \" << bins(i) << \" points.\" << endl;\n   // }\n}",
            "// Count the points in each quadrant.\n    // TODO: Your code here.\n    // The code below is correct, but it uses a Kokkos parallel_for\n    // construct to count the number of points in each quadrant.\n    // You do not need to understand it.\n    size_t npoints = points.extent(0);\n    size_t tmp;\n    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t>(0, npoints),\n        KOKKOS_LAMBDA(const size_t i, size_t& count){\n            if (points(i).x > 0 && points(i).y > 0)\n                count++;\n            else if (points(i).x < 0 && points(i).y > 0)\n                tmp++;\n            else if (points(i).x < 0 && points(i).y < 0)\n                count++;\n            else\n                tmp++;\n        }, tmp);\n    bins(0) = npoints - tmp;\n    bins(1) = tmp;\n    tmp = 0;\n    Kokkos::parallel_reduce(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace, size_t>(0, npoints),\n        KOKKOS_LAMBDA(const size_t i, size_t& count){\n            if (points(i).x < 0 && points(i).y < 0)\n                count++;\n            else if (points(i).x < 0 && points(i).y > 0)\n                tmp++;\n            else if (points(i).x > 0 && points(i).y > 0)\n                count++;\n            else\n                tmp++;\n        }, tmp);\n    bins(2) = npoints - tmp;\n    bins(3) = tmp;\n\n    // Kokkos code\n    // TODO: Your code here.\n\n    // For testing\n    std::cout << \"Quadrant counts: \";\n    for (size_t i = 0; i < 4; ++i) {\n        std::cout << bins(i) << \" \";\n    }\n    std::cout << std::endl;\n}",
            "// TODO: Implement this function\n   size_t count=0;\n\n   for(size_t i=0;i<points.extent(0);i++){\n     if(points(i).x >= 0 && points(i).y >= 0)\n       ++count;\n   }\n   Kokkos::View<size_t[1]> count_view(\"count_view\",1);\n   Kokkos::deep_copy(count_view,count);\n   Kokkos::View<size_t*> count_pointer(\"count_pointer\",&count_view(0));\n\n   Kokkos::parallel_for(\"count_quadrants\",points.extent(0),KOKKOS_LAMBDA(const int& i){\n     // if(points(i).x >= 0 && points(i).y >= 0)\n     //   ++count;\n   });\n   Kokkos::deep_copy(bins,count_view);\n   Kokkos::deep_copy(count_view,0);\n   Kokkos::deep_copy(count_pointer,&count_view(0));\n   Kokkos::deep_copy(bins,count_view);\n\n}",
            "// YOUR CODE HERE\n}",
            "auto count_points = [&] (const Point *p, size_t *b, size_t num) {\n      for (size_t i = 0; i < num; i++) {\n         const Point &pt = p[i];\n         if (pt.x >= 0 && pt.y >= 0)\n            b[0]++;\n         else if (pt.x < 0 && pt.y >= 0)\n            b[1]++;\n         else if (pt.x < 0 && pt.y < 0)\n            b[2]++;\n         else\n            b[3]++;\n      }\n   };\n\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n      KOKKOS_LAMBDA (const int i) {\n         Point *p = &points(i);\n         size_t *b = &bins(i);\n         count_points(p, b, 1);\n      });\n}",
            "/* TODO: implement this function. You should NOT change the interface of this function. */\n   Kokkos::TeamPolicy<Kokkos::DefaultExecutionSpace> policy(points.extent(0));\n   Kokkos::parallel_for(\"Count quadrants\", policy, KOKKOS_LAMBDA(const int i) {\n      auto x = points(i).x;\n      auto y = points(i).y;\n      if (x > 0 && y > 0) {\n         bins(0)++;\n      } else if (x < 0 && y > 0) {\n         bins(1)++;\n      } else if (x < 0 && y < 0) {\n         bins(2)++;\n      } else {\n         bins(3)++;\n      }\n   });\n}",
            "// TODO: your code here\n}",
            "using Device = Kokkos::Device<Kokkos::OpenMP>;\n   using ExecSpace = Kokkos::OpenMP;\n   using TeamPolicy = Kokkos::TeamPolicy<ExecSpace>;\n   const size_t N = points.extent(0);\n\n   // YOUR CODE HERE\n   // 1. Create a TeamPolicy that has 4 teams and the same work\n   // 2. Partition the points array and assign it to each team's partition\n   // 3. Have each team count the number of points in that partition\n   // 4. Sum the counts in bins\n\n   // For example:\n   // Team team(0, 0, team_size);\n   // team.partition(points.size(), team.team_size());\n   // auto points_partition = Kokkos::subview(points, team.league_rank(), team.team_rank());\n   // size_t count = 0;\n   // for (const auto &point : points_partition) {\n   //    ++count;\n   // }\n   // bins(team.team_rank()) = count;\n}",
            "// TODO: implement this function\n}",
            "Kokkos::parallel_for(\"Count Quadrants\", points.extent(0), KOKKOS_LAMBDA (int i) {\n      if (points(i).x > 0) {\n         if (points(i).y > 0) {\n            bins(1) += 1;\n         } else {\n            bins(2) += 1;\n         }\n      } else {\n         if (points(i).y > 0) {\n            bins(3) += 1;\n         } else {\n            bins(4) += 1;\n         }\n      }\n   });\n}",
            "}",
            "Kokkos::parallel_for(4, KOKKOS_LAMBDA(const int i) {\n      size_t count = 0;\n      for (const Point& p : points) {\n         if (p.x >= 0 && p.y >= 0 && p.x < 1 && p.y < 1) {\n            count++;\n         }\n      }\n      bins(i) = count;\n   });\n}",
            "// TODO\n}",
            "// TODO: add your code here\n  //\n  // You will need to define a lambda functor to perform the count.\n  // Note: you will need to use Kokkos::parallel_for()\n  //\n}",
            "// YOUR CODE HERE\n  // DO NOT CHANGE THE LINES ABOVE\n  // DO NOT CHANGE THIS FUNCTION\n}",
            "// TODO: Implement this function.\n   // 1. Initialize the `bins` array to zero.\n   // 2. For each `Point`, if it is in the first quadrant, increment the first element in the `bins` array.\n   // 3. For each `Point`, if it is in the second quadrant, increment the second element in the `bins` array.\n   // 4. For each `Point`, if it is in the third quadrant, increment the third element in the `bins` array.\n   // 5. For each `Point`, if it is in the fourth quadrant, increment the fourth element in the `bins` array.\n   auto policy = Kokkos::RangePolicy<Kokkos::Serial>(0, points.extent(0));\n   Kokkos::parallel_for(\"countQuadrants\", policy, [=](int i) {\n      if (points(i).x >= 0 && points(i).y >= 0) {\n         bins(0) += 1;\n      } else if (points(i).x >= 0 && points(i).y < 0) {\n         bins(1) += 1;\n      } else if (points(i).x < 0 && points(i).y < 0) {\n         bins(2) += 1;\n      } else if (points(i).x < 0 && points(i).y >= 0) {\n         bins(3) += 1;\n      }\n   });\n}",
            "}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::OpenMP>(0, points.extent(0)),\n    KOKKOS_LAMBDA(size_t i) {\n      size_t x = points(i).x > 0? 1 : (points(i).x < 0? 2 : 0);\n      size_t y = points(i).y > 0? 1 : (points(i).y < 0? 2 : 0);\n      bins(x + y * 2) += 1;\n    }\n  );\n}",
            "Kokkos::parallel_for(\"countQuadrants\", Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.extent(0)), KOKKOS_LAMBDA(const int i) {\n        double x = points(i).x;\n        double y = points(i).y;\n        // quadrant I is all points with x >= 0 and y >= 0\n        if (x >= 0 && y >= 0) {\n            bins(0) += 1;\n        }\n        // quadrant II is all points with x < 0 and y >= 0\n        else if (x < 0 && y >= 0) {\n            bins(1) += 1;\n        }\n        // quadrant III is all points with x < 0 and y < 0\n        else if (x < 0 && y < 0) {\n            bins(2) += 1;\n        }\n        // quadrant IV is all points with x >= 0 and y < 0\n        else {\n            bins(3) += 1;\n        }\n    });\n    Kokkos::fence();\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)),\n    KOKKOS_LAMBDA(size_t i) {\n        if (points(i).x > 0 && points(i).y > 0) {\n            bins(0) += 1;\n        } else if (points(i).x < 0 && points(i).y > 0) {\n            bins(1) += 1;\n        } else if (points(i).x < 0 && points(i).y < 0) {\n            bins(2) += 1;\n        } else {\n            bins(3) += 1;\n        }\n    });\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, points.size()), [&] (size_t i) {\n      // TODO: Fill in this function.\n   });\n}",
            "}",
            "// TODO: implement\n}",
            "// TODO: Implement this function.\n   // Hints:\n   // * You may want to look at the `Kokkos::parallel_for` function\n   // * You will need to create a view of the input array, such as with `Kokkos::subview`\n   // * You will need to create an output view, such as with `Kokkos::create_mirror`\n   // * You will need to use `Kokkos::atomic_fetch_add` to increment the count in each quadrant.\n   // * You will probably want to use `Kokkos::TeamPolicy` to distribute the work in parallel.\n\n}",
            "Kokkos::View<size_t> bins_host(\"bins\", 4);\n   Kokkos::deep_copy(bins_host, bins);\n   for (int i=0; i<4; ++i) bins_host(i) = 0;\n\n   // TODO: implement me\n\n   // TODO: print the results\n   Kokkos::deep_copy(bins, bins_host);\n   std::cout << \"counts = \" << bins(0) << \", \" << bins(1) << \", \" << bins(2) << \", \" << bins(3) << std::endl;\n}",
            "// TODO: Implement this function\n}",
            "Kokkos::parallel_for(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA(int i) {\n      int bin = 0;\n      if (points(i).x > 0)\n         bin += 1;\n      if (points(i).y > 0)\n         bin += 2;\n      Kokkos::atomic_fetch_add(&bins(bin), 1);\n   });\n}",
            "// TODO: your code goes here!\n   Kokkos::View<size_t[4]> bins_h(\"bins_h\", 4);\n   Kokkos::deep_copy(bins_h, 0);\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points(i)->x > 0 && points(i)->y > 0) {\n         bins_h(0) += 1;\n      }\n      else if (points(i)->x < 0 && points(i)->y > 0) {\n         bins_h(1) += 1;\n      }\n      else if (points(i)->x < 0 && points(i)->y < 0) {\n         bins_h(2) += 1;\n      }\n      else {\n         bins_h(3) += 1;\n      }\n   }\n   Kokkos::deep_copy(bins, bins_h);\n}",
            "// TODO\n}",
            "using ExecutionSpace = Kokkos::DefaultExecutionSpace;\n\n   auto policy = Kokkos::RangePolicy<ExecutionSpace>(0, points.extent(0));\n   auto bins_host = Kokkos::create_mirror_view(bins);\n\n   Kokkos::parallel_for(policy, [&](int i) {\n      auto& p = points(i);\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins_host(0);\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins_host(1);\n      } else if (p.x >= 0 && p.y < 0) {\n         ++bins_host(2);\n      } else {\n         ++bins_host(3);\n      }\n   });\n   Kokkos::deep_copy(bins, bins_host);\n}",
            "Kokkos::parallel_for(\"Count Points in Quadrants\", points.size(), KOKKOS_LAMBDA(const size_t i) {\n      const auto point = points(i);\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins(0);\n      } else if (point.x < 0 && point.y >= 0) {\n         ++bins(1);\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins(2);\n      } else {\n         ++bins(3);\n      }\n   });\n}",
            "Kokkos::parallel_for(Kokkos::TeamPolicy<execution_space>(points.extent(0)), KOKKOS_LAMBDA(const team_policy_type &team) {\n      Kokkos::parallel_for(Kokkos::TeamThreadRange(team, 0, 4), KOKKOS_LAMBDA(const int &i) {\n         size_t count = 0;\n         Kokkos::parallel_reduce(Kokkos::ThreadVectorRange(team, points.extent(0)), KOKKOS_LAMBDA(const size_t& idx, size_t& count) {\n            if (points(idx).x >= (i == 1 || i == 3) * 10 && points(idx).y >= (i == 0 || i == 2) * 10) count++;\n         }, Kokkos::Sum<size_t>(count));\n         bins(i) = count;\n      });\n   });\n}",
            "Kokkos::parallel_for(1, [&] (int) {\n      Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.extent(0)), [&] (size_t i) {\n         const Point &p = points(i);\n         if (p.x > 0 && p.y > 0) {\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n         } else if (p.x <= 0 && p.y > 0) {\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n         } else if (p.x <= 0 && p.y <= 0) {\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n         } else {\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n         }\n      });\n   });\n}",
            "auto bins_host = Kokkos::create_mirror_view(bins);\n\n   for (auto i = 0u; i < points.size(); ++i) {\n      auto const& p = points[i];\n      if (p.x > 0 && p.y > 0) ++bins_host(0);\n      else if (p.x < 0 && p.y > 0) ++bins_host(1);\n      else if (p.x < 0 && p.y < 0) ++bins_host(2);\n      else ++bins_host(3);\n   }\n\n   Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO: write this function!\n}",
            "// TODO\n}",
            "// TODO\n}",
            "Kokkos::View<size_t, Kokkos::DefaultExecutionSpace> counts(\"counts\", 4);\n   Kokkos::parallel_for(\"QuadrantCount\", points.size(), KOKKOS_LAMBDA(size_t i) {\n      auto p = points(i);\n      if (p.x >= 0 && p.y >= 0) {\n         counts(0) += 1;\n      } else if (p.x < 0 && p.y >= 0) {\n         counts(1) += 1;\n      } else if (p.x < 0 && p.y < 0) {\n         counts(2) += 1;\n      } else {\n         counts(3) += 1;\n      }\n   });\n\n   Kokkos::deep_copy(bins, counts);\n}",
            "Kokkos::parallel_for(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA(int i) {\n      if (points(i).x >= 0.0) {\n         if (points(i).y >= 0.0)\n            bins(0)++;\n         else\n            bins(1)++;\n      } else {\n         if (points(i).y >= 0.0)\n            bins(2)++;\n         else\n            bins(3)++;\n      }\n   });\n}",
            "}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// Count number of points in each quadrant\n\n   Kokkos::TeamPolicy<Kokkos::TeamDynamic<>> policy(points.extent(0), Kokkos::AUTO);\n   policy.set_scratch_size(1, sizeof(size_t));\n   Kokkos::parallel_for(\"Count Quadrants\", policy, KOKKOS_LAMBDA(const Kokkos::TeamMember& team) {\n      Kokkos::parallel_scan(Kokkos::TeamThreadRange(team, points.extent(0)), [&] (size_t i, size_t& count) {\n         if (points(i).x >= 0 && points(i).y >= 0) {\n            ++count;\n         }\n      });\n   });\n\n   Kokkos::parallel_for(\"Count Quadrants Reduce\", Kokkos::RangePolicy<Kokkos::TeamPolicy<Kokkos::TeamDynamic<>>>(policy.team_policy(Kokkos::TeamPolicyBase<Kokkos::TeamPolicy<Kokkos::TeamDynamic<>>>::member_type::member_type::member_type::scratch_size()), 4), [&] (size_t i) {\n      bins(i) = Kokkos::parallel_reduce(Kokkos::ThreadVectorRange(Kokkos::all(), policy.scratch_size()), [&] (size_t, size_t& sum) {\n         sum += bins(i);\n      }, 0);\n   });\n}",
            "// TODO\n}",
            "// TODO\n}",
            "}",
            "Kokkos::parallel_for(points.extent(0), KOKKOS_LAMBDA(size_t i) {\n      // TODO: Compute which quadrant the point is in and increment the corresponding bin.\n      // Do this in a safe, parallel way.\n      // Hint: Use std::fabs to compute the absolute value of a double\n      // Hint: Use Kokkos to access the array bin\n\n      double x = points(i).x;\n      double y = points(i).y;\n      size_t ix = (x < 0)? 0 : 1;\n      size_t iy = (y < 0)? 0 : 1;\n      bins(ix + iy*2) += 1;\n   });\n}",
            "Kokkos::View<size_t[4]> bins_per_thread(\"bins_per_thread\", 4); // each thread should have a copy of this data\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, points.size()),\n      KOKKOS_LAMBDA (size_t i) {\n         Point const& p = points(i);\n         if (p.x > 0.0) {\n            if (p.y > 0.0) {\n               bins_per_thread(0) += 1;\n            } else {\n               bins_per_thread(1) += 1;\n            }\n         } else {\n            if (p.y > 0.0) {\n               bins_per_thread(2) += 1;\n            } else {\n               bins_per_thread(3) += 1;\n            }\n         }\n      });\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultExecutionSpace>(0, 4),\n      KOKKOS_LAMBDA (size_t i) {\n         bins(i) = bins_per_thread(i);\n      });\n}",
            "// YOUR CODE HERE\n}",
            "}",
            "// TODO: Fill in this function\n}",
            "// TODO: Implement this function.\n}",
            "// Fill up this code\n   size_t n = points.extent(0);\n   size_t n0 = 0;\n   size_t n1 = 0;\n   size_t n2 = 0;\n   size_t n3 = 0;\n   Kokkos::parallel_reduce(n, KOKKOS_LAMBDA(const int i, int &update) {\n      const Point& p = points(i);\n      if (p.x > 0 && p.y > 0)\n         ++n0;\n      else if (p.x < 0 && p.y > 0)\n         ++n1;\n      else if (p.x < 0 && p.y < 0)\n         ++n2;\n      else\n         ++n3;\n   }, Kokkos::Sum<int>(update));\n   bins(0) = n0;\n   bins(1) = n1;\n   bins(2) = n2;\n   bins(3) = n3;\n}",
            "// TODO\n}",
            "// Your code goes here\n}",
            "// TODO: Your code here.\n}",
            "// TODO: Write code here.\n}",
            "// TODO\n}",
            "Kokkos::parallel_for(Kokkos::RangePolicy<execution_space>(0, points.size()),\n    KOKKOS_LAMBDA(size_t i) {\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                ++bins(0);\n            } else {\n                ++bins(1);\n            }\n        } else {\n            if (points[i].y > 0) {\n                ++bins(2);\n            } else {\n                ++bins(3);\n            }\n        }\n    }\n    );\n}",
            "}",
            "// Fill in your code here.\n   throw std::runtime_error(\"To be implemented\");\n}",
            "// Your code here\n}",
            "// TODO: your code here\n}",
            "// TODO: Fill in this function\n   Kokkos::parallel_for(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA(const size_t i) {\n      double x = points(i).x, y = points(i).y;\n      size_t ibin = (x > 0 && y > 0)? 0 : ((x > 0 && y < 0)? 1 : ((x < 0 && y < 0)? 2 : 3));\n      Kokkos::atomic_fetch_add(&bins(ibin), 1);\n   });\n}",
            "Kokkos::View<size_t*> counts(\"counts\", 4);\n    Kokkos::deep_copy(counts, 0);\n\n    // YOUR CODE HERE: Count the points in each quadrant\n\n    // Kokkos::parallel_for(0, points.extent(0), KOKKOS_LAMBDA(const size_t i) {\n    //     if (points(i).x >= 0) {\n    //         if (points(i).y >= 0) {\n    //             ++counts(0);\n    //         } else {\n    //             ++counts(1);\n    //         }\n    //     } else {\n    //         if (points(i).y >= 0) {\n    //             ++counts(2);\n    //         } else {\n    //             ++counts(3);\n    //         }\n    //     }\n    // });\n\n    Kokkos::parallel_for(0, points.extent(0), KOKKOS_LAMBDA(const size_t i) {\n        if (points(i).x >= 0) {\n            if (points(i).y >= 0) {\n                Kokkos::atomic_increment(&counts(0));\n            } else {\n                Kokkos::atomic_increment(&counts(1));\n            }\n        } else {\n            if (points(i).y >= 0) {\n                Kokkos::atomic_increment(&counts(2));\n            } else {\n                Kokkos::atomic_increment(&counts(3));\n            }\n        }\n    });\n\n    // Kokkos::View<size_t, Kokkos::HostSpace> host_counts(\"host_counts\", 4);\n    // Kokkos::deep_copy(host_counts, counts);\n    // for (size_t i = 0; i < 4; ++i) {\n    //     printf(\"%lu \", host_counts(i));\n    // }\n    // printf(\"\\n\");\n\n    Kokkos::deep_copy(bins, counts);\n}",
            "/* Your code here */\n}",
            "// TODO: Complete the function\n}",
            "// TODO: Implement\n   size_t n = points.extent(0);\n   auto bins_host = Kokkos::create_mirror_view(bins);\n   bins_host(0) = 0;\n   bins_host(1) = 0;\n   bins_host(2) = 0;\n   bins_host(3) = 0;\n\n   // count the number of points in each quadrant\n   Kokkos::parallel_for(0, n, 1, [&](int i) {\n      if (points(i).x > 0 && points(i).y > 0) {\n         bins_host(0) += 1;\n      }\n      else if (points(i).x < 0 && points(i).y > 0) {\n         bins_host(1) += 1;\n      }\n      else if (points(i).x < 0 && points(i).y < 0) {\n         bins_host(2) += 1;\n      }\n      else {\n         bins_host(3) += 1;\n      }\n   });\n\n   Kokkos::deep_copy(bins, bins_host);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO\n}",
            "// TODO: implement me\n}",
            "size_t n = points.extent(0);\n   // TODO: implement this function\n   // hint: use Kokkos::parallel_reduce\n}",
            "Kokkos::parallel_for(Kokkos::TeamPolicy<>::team_policy(4, points.extent(0)), KOKKOS_LAMBDA(const Kokkos::TeamPolicy<>::member_type & teamMember) {\n        size_t i = teamMember.league_rank();\n        double x = points(i).x;\n        double y = points(i).y;\n        if (x >= 0 && y >= 0) {\n            teamMember.team_barrier();\n            Kokkos::atomic_fetch_add(&bins(0), 1);\n        } else if (x < 0 && y >= 0) {\n            teamMember.team_barrier();\n            Kokkos::atomic_fetch_add(&bins(1), 1);\n        } else if (x < 0 && y < 0) {\n            teamMember.team_barrier();\n            Kokkos::atomic_fetch_add(&bins(2), 1);\n        } else if (x >= 0 && y < 0) {\n            teamMember.team_barrier();\n            Kokkos::atomic_fetch_add(&bins(3), 1);\n        }\n    });\n}",
            "size_t num_points = points.extent(0);\n   Kokkos::parallel_for(Kokkos::RangePolicy<Kokkos::DefaultHostExecutionSpace>(0, num_points), [&](const size_t i){\n      size_t quadrant = (points(i).x > 0)? ((points(i).y > 0)? 0 : 3) : ((points(i).y > 0)? 1 : 2);\n      Kokkos::atomic_fetch_add(&bins(quadrant), 1);\n   });\n}",
            "// TODO: your code goes here.\n}",
            "Kokkos::parallel_for(\"countQuadrants\", points.extent(0), KOKKOS_LAMBDA(size_t i) {\n      Point p = points(i);\n      size_t quadrant = ((p.x > 0) && (p.y > 0))? 0 : ((p.x < 0) && (p.y > 0))? 1 : ((p.x < 0) && (p.y < 0))? 2 : 3;\n      Kokkos::atomic_fetch_add(&(bins[quadrant]), 1);\n   });\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "omp",
        "prompt": "#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use OpenMP to count in parallel.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         ++bins[0];\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         ++bins[1];\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         ++bins[2];\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "// omp_set_num_threads(6);\n    // for (auto it = points.begin(); it < points.end(); it++) {\n    //     auto coord = *it;\n    //     if (coord.x > 0 && coord.y > 0) {\n    //         bins[0] += 1;\n    //     } else if (coord.x < 0 && coord.y > 0) {\n    //         bins[1] += 1;\n    //     } else if (coord.x < 0 && coord.y < 0) {\n    //         bins[2] += 1;\n    //     } else {\n    //         bins[3] += 1;\n    //     }\n    // }\n    // int num_threads = omp_get_num_threads();\n    // std::cout << \"Number of threads: \" << num_threads << \"\\n\";\n    // std::cout << \"Bins: \";\n    // for (auto b : bins) {\n    //     std::cout << b << \" \";\n    // }\n    // std::cout << \"\\n\";\n}",
            "bins.fill(0);\n#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); ++i) {\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                bins[0]++;\n            } else {\n                bins[3]++;\n            }\n        } else {\n            if (points[i].y > 0) {\n                bins[1]++;\n            } else {\n                bins[2]++;\n            }\n        }\n    }\n}",
            "// TODO: implement this function\n   int num_threads = omp_get_num_threads();\n   bins = std::array<size_t, 4>();\n\n   for(int i=0; i<num_threads; i++)\n      bins[i] = 0;\n\n   #pragma omp parallel for\n   for(int i = 0; i < points.size(); i++){\n      if(points[i].x >= 0 && points[i].y >= 0)\n         bins[0] ++;\n      else if(points[i].x < 0 && points[i].y >= 0)\n         bins[1] ++;\n      else if(points[i].x < 0 && points[i].y < 0)\n         bins[2] ++;\n      else if(points[i].x >= 0 && points[i].y < 0)\n         bins[3] ++;\n      else\n         std::cout << \"Error in quadrant assignment for point \" << i << \"\\n\";\n   }\n}",
            "#pragma omp parallel\n   {\n      auto nThreads = omp_get_num_threads();\n      auto tid = omp_get_thread_num();\n      auto start = points.size() / nThreads * tid;\n      auto end = points.size() / nThreads * (tid + 1);\n\n      if (tid == nThreads - 1)\n         end = points.size();\n\n      for (auto i = start; i < end; ++i) {\n         auto const& p = points[i];\n         if (p.x > 0 && p.y > 0)\n            ++bins[0];\n         else if (p.x < 0 && p.y > 0)\n            ++bins[1];\n         else if (p.x < 0 && p.y < 0)\n            ++bins[2];\n         else if (p.x > 0 && p.y < 0)\n            ++bins[3];\n      }\n   }\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); i++) {\n            auto const& p = points[i];\n            bins[0] += (p.x >= 0) && (p.y >= 0);\n            bins[1] += (p.x < 0)  && (p.y >= 0);\n            bins[2] += (p.x < 0)  && (p.y < 0);\n            bins[3] += (p.x >= 0) && (p.y < 0);\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "bins = {0, 0, 0, 0};\n   /* Your code here */\n}",
            "bins = {0, 0, 0, 0};\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0.0 && points[i].y >= 0.0)\n         bins[0]++;\n      else if (points[i].x < 0.0 && points[i].y >= 0.0)\n         bins[1]++;\n      else if (points[i].x < 0.0 && points[i].y < 0.0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "#pragma omp parallel for\n  for (auto i = 0; i < points.size(); ++i) {\n    if (points[i].x > 0 && points[i].y > 0) {\n      bins[0] += 1;\n    } else if (points[i].x < 0 && points[i].y > 0) {\n      bins[1] += 1;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "omp_set_num_threads(8);\n    size_t length = points.size();\n    #pragma omp parallel for\n    for (size_t i = 0; i < length; i++) {\n        Point p = points[i];\n        if (p.x < 0 && p.y < 0) {\n            bins[0]++;\n        } else if (p.x > 0 && p.y < 0) {\n            bins[1]++;\n        } else if (p.x > 0 && p.y > 0) {\n            bins[2]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// Your code goes here.\n}",
            "// TODO\n}",
            "// Your code here\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    int max = omp_get_max_threads();\n    //omp_set_num_threads(1);\n    int i = 0;\n    #pragma omp parallel for private(i)\n    for(i = 0; i < points.size(); i++) {\n        if(points[i].x > 0 && points[i].y > 0) {\n            bins[0] += 1;\n        } else if(points[i].x < 0 && points[i].y > 0) {\n            bins[1] += 1;\n        } else if(points[i].x < 0 && points[i].y < 0) {\n            bins[2] += 1;\n        } else if(points[i].x > 0 && points[i].y < 0) {\n            bins[3] += 1;\n        }\n    }\n    omp_set_num_threads(max);\n    //cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << endl;\n}",
            "bins.fill(0);\n\n#pragma omp parallel\n    {\n#pragma omp for\n        for (int i = 0; i < points.size(); i++) {\n            Point const& p = points[i];\n\n            if (p.x > 0) {\n                if (p.y > 0) {\n                    bins[0]++;\n                } else {\n                    bins[1]++;\n                }\n            } else {\n                if (p.y > 0) {\n                    bins[2]++;\n                } else {\n                    bins[3]++;\n                }\n            }\n        }\n    }\n}",
            "// your code here\n   bins.fill(0);\n}",
            "// TODO: implement this function\n   //bins[0]=0;\n   //bins[1]=0;\n   //bins[2]=0;\n   //bins[3]=0;\n   omp_set_num_threads(8);\n#pragma omp parallel for\n   for(int i=0;i<points.size();i++){\n    if(points[i].x>=0 && points[i].y>=0){\n     bins[0]++;\n    }else if(points[i].x>=0 && points[i].y<0){\n     bins[1]++;\n    }else if(points[i].x<0 && points[i].y>=0){\n     bins[2]++;\n    }else if(points[i].x<0 && points[i].y<0){\n     bins[3]++;\n    }\n   }\n}",
            "// initialize array for each quadrant\n   std::array<size_t, 4> quadrantBins = { 0, 0, 0, 0 };\n\n   // omp parallel for\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n\n      // get the current point\n      Point const& p = points[i];\n\n      // get the quadrant\n      int quadrant;\n      if (p.x > 0) {\n         quadrant = p.y > 0? 0 : 1;\n      }\n      else {\n         quadrant = p.y > 0? 2 : 3;\n      }\n\n      // increment the number of points in this quadrant\n      quadrantBins[quadrant] += 1;\n   }\n\n   // store the bins\n   bins = quadrantBins;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      const auto& p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      }\n      else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "int n = points.size();\n\n#pragma omp parallel\n   {\n      // Count the number of points in each quadrant.\n      int quadrantCounts[4] = {0};\n      for (int i = 0; i < n; i++) {\n         if (points[i].x < 0) {\n            if (points[i].y > 0)\n               quadrantCounts[3]++;\n            else\n               quadrantCounts[2]++;\n         } else {\n            if (points[i].y > 0)\n               quadrantCounts[1]++;\n            else\n               quadrantCounts[0]++;\n         }\n      }\n\n      // Store the count in the corresponding bin.\n#pragma omp critical\n      for (int i = 0; i < 4; i++)\n         bins[i] += quadrantCounts[i];\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n  #pragma omp parallel for\n  for (size_t i = 0; i < points.size(); i++) {\n    if (points[i].x > 0 && points[i].y > 0) {\n      bins[0]++;\n    } else if (points[i].x < 0 && points[i].y > 0) {\n      bins[1]++;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      bins[2]++;\n    } else if (points[i].x > 0 && points[i].y < 0) {\n      bins[3]++;\n    }\n  }\n}",
            "#pragma omp parallel for reduction(+:bins[0],bins[1],bins[2],bins[3])\n    for (auto const& point: points) {\n        if (point.x > 0 && point.y > 0) {\n            bins[0]++;\n        }\n        else if (point.x > 0 && point.y < 0) {\n            bins[1]++;\n        }\n        else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "omp_lock_t lock;\n    omp_init_lock(&lock);\n    size_t n = points.size();\n#pragma omp parallel for\n    for (size_t i = 0; i < n; i++) {\n        Point const& p = points[i];\n        size_t& count = bins[quadrant(p)];\n        omp_set_lock(&lock);\n        count++;\n        omp_unset_lock(&lock);\n    }\n    omp_destroy_lock(&lock);\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function\n   // 1. count quadrants\n#pragma omp parallel for\n   for (auto i = 0; i < points.size(); i++) {\n      if (points[i].x < 0) {\n         if (points[i].y < 0)\n            bins[0] += 1;\n         else\n            bins[1] += 1;\n      } else {\n         if (points[i].y < 0)\n            bins[2] += 1;\n         else\n            bins[3] += 1;\n      }\n   }\n\n   // 2. print the counts\n   std::cout << \"[\" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \"]\" << std::endl;\n}",
            "#pragma omp parallel\n   {\n      double lmin, lmax;\n      double bmin, bmax;\n      double xmin, xmax;\n      size_t count;\n      int tid = omp_get_thread_num();\n      int num_threads = omp_get_num_threads();\n\n      /* Calculate the bounds of the left quadrant */\n      lmin = INFINITY;\n      lmax = -INFINITY;\n\n      /* Calculate the bounds of the right quadrant */\n      rmin = INFINITY;\n      rmax = -INFINITY;\n\n      /* Calculate the bounds of the top quadrant */\n      tmin = INFINITY;\n      tmax = -INFINITY;\n\n      /* Calculate the bounds of the bottom quadrant */\n      bmin = INFINITY;\n      bmax = -INFINITY;\n\n      /* Each thread is responsible for a quadrant */\n      int num_points = points.size();\n      double chunk_size = (num_points + num_threads - 1) / num_threads;\n\n      for (int i = tid * chunk_size; i < (tid + 1) * chunk_size && i < num_points; i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n         if (x < lmin)\n            lmin = x;\n         if (x > lmax)\n            lmax = x;\n         if (y < bmin)\n            bmin = y;\n         if (y > bmax)\n            bmax = y;\n      }\n\n      /* Compute the number of points in each quadrant */\n      for (int i = tid * chunk_size; i < (tid + 1) * chunk_size && i < num_points; i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n\n         if (x >= lmin && x <= lmax && y >= bmin && y <= bmax)\n            bins[0] += 1;\n         else if (x < lmin && y < bmin)\n            bins[1] += 1;\n         else if (x < lmin && y > bmax)\n            bins[2] += 1;\n         else if (x > lmax && y > bmax)\n            bins[3] += 1;\n      }\n   }\n}",
            "omp_set_dynamic(0);\n    #pragma omp parallel for\n    for(size_t i = 0; i < points.size(); i++) {\n        if(points[i].x < 0) {\n            if(points[i].y > 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if(points[i].y > 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n#pragma omp parallel for\n   for (auto const& p : points) {\n      int bin = 0;\n      if (p.x >= 0) bin += 2;\n      if (p.y >= 0) bin += 1;\n      ++bins[bin];\n   }\n}",
            "// TODO\n}",
            "// TODO: implement this function\n   bins.fill(0);\n   #pragma omp parallel for\n   for(int i=0; i<points.size(); ++i){\n      double x = points[i].x;\n      double y = points[i].y;\n      if(x >= 0 && y >= 0){\n         bins[0] += 1;\n      } else if(x < 0 && y >= 0){\n         bins[1] += 1;\n      } else if(x < 0 && y < 0){\n         bins[2] += 1;\n      } else if(x >= 0 && y < 0){\n         bins[3] += 1;\n      }\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   // TODO: implement this function\n}",
            "bins.fill(0);\n\n    #pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        auto& p = points[i];\n        int quadrant = 0;\n        if (p.x > 0) quadrant += 1;\n        if (p.y > 0) quadrant += 2;\n        bins[quadrant]++;\n    }\n}",
            "bins.fill(0);\n   omp_set_num_threads(8);\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      auto const& p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         #pragma omp atomic\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         #pragma omp atomic\n         ++bins[1];\n      } else if (p.x >= 0 && p.y < 0) {\n         #pragma omp atomic\n         ++bins[2];\n      } else {\n         #pragma omp atomic\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n   for (auto const& p : points) {\n      if (p.x > 0) {\n         bins[0]++;\n      } else {\n         bins[1]++;\n      }\n\n      if (p.y > 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n\t\tint quadrant = 0;\n\t\tif (point.x > 0 && point.y > 0) {\n\t\t\tquadrant = 1;\n\t\t}\n\t\telse if (point.x < 0 && point.y > 0) {\n\t\t\tquadrant = 2;\n\t\t}\n\t\telse if (point.x < 0 && point.y < 0) {\n\t\t\tquadrant = 3;\n\t\t}\n\n\t\tbins[quadrant] += 1;\n\t}\n}",
            "int start, end;\n    int chunk;\n\n#pragma omp parallel shared(points, bins) private(start, end, chunk)\n    {\n        int nthreads = omp_get_num_threads();\n        int id = omp_get_thread_num();\n\n        chunk = (int)points.size() / nthreads;\n        start = chunk * id;\n        end = start + chunk;\n\n        if (id == nthreads - 1) {\n            end = (int)points.size();\n        }\n\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        for (int i = start; i < end; i++) {\n            if (points[i].x >= 0.0 && points[i].y >= 0.0) {\n                bins[0]++;\n            } else if (points[i].x < 0.0 && points[i].y >= 0.0) {\n                bins[1]++;\n            } else if (points[i].x < 0.0 && points[i].y < 0.0) {\n                bins[2]++;\n            } else if (points[i].x >= 0.0 && points[i].y < 0.0) {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "omp_lock_t lock;\n   omp_init_lock(&lock);\n   auto const threadCount = omp_get_num_threads();\n   auto const pointCount = points.size();\n   auto const rangeSize = pointCount / threadCount;\n   auto const remainder = pointCount % threadCount;\n\n   #pragma omp parallel for\n   for (int i = 0; i < threadCount; i++) {\n      auto const begin = rangeSize * i;\n      auto const end = (i == threadCount - 1)? begin + rangeSize + remainder : begin + rangeSize;\n      size_t count = 0;\n      for (size_t j = begin; j < end; j++) {\n         auto const point = points[j];\n         if ((point.x >= 0.0) && (point.y >= 0.0)) {\n            count++;\n         } else if ((point.x < 0.0) && (point.y >= 0.0)) {\n            count++;\n         } else if ((point.x < 0.0) && (point.y < 0.0)) {\n            count++;\n         } else if ((point.x >= 0.0) && (point.y < 0.0)) {\n            count++;\n         }\n      }\n\n      omp_set_lock(&lock);\n      bins[i] = count;\n      omp_unset_lock(&lock);\n   }\n}",
            "// write your code here\n}",
            "// TODO\n}",
            "for (size_t i = 0; i < points.size(); i++) {\n        auto &point = points[i];\n\n        if (point.x > 0 && point.y > 0)\n            bins[0]++;\n        else if (point.x < 0 && point.y > 0)\n            bins[1]++;\n        else if (point.x < 0 && point.y < 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "#pragma omp parallel\n   {\n      std::array<size_t, 4> localBins;\n      localBins.fill(0);\n      #pragma omp for\n      for (auto const& point : points) {\n         if (point.x > 0 && point.y > 0)\n            localBins[0]++;\n         else if (point.x < 0 && point.y > 0)\n            localBins[1]++;\n         else if (point.x < 0 && point.y < 0)\n            localBins[2]++;\n         else if (point.x > 0 && point.y < 0)\n            localBins[3]++;\n      }\n      #pragma omp critical\n      for (size_t i = 0; i < localBins.size(); i++)\n         bins[i] += localBins[i];\n   }\n}",
            "#pragma omp parallel for\n   for(auto const &p : points) {\n      int quadrant = 1;\n      if(p.x > 0) quadrant += 2;\n      if(p.y > 0) quadrant += 4;\n      ++bins[quadrant];\n   }\n}",
            "size_t const num_points = points.size();\n   std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n#pragma omp parallel\n   {\n#pragma omp for schedule(static) nowait\n      for (size_t i = 0; i < num_points; i++) {\n         double const x = points[i].x;\n         double const y = points[i].y;\n         if (x > 0 && y > 0) {\n            ++local_bins[0];\n         }\n         else if (x < 0 && y > 0) {\n            ++local_bins[1];\n         }\n         else if (x < 0 && y < 0) {\n            ++local_bins[2];\n         }\n         else if (x > 0 && y < 0) {\n            ++local_bins[3];\n         }\n      }\n   }\n\n   for (size_t i = 0; i < 4; i++) {\n      bins[i] += local_bins[i];\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (auto const& point: points) {\n        if (point.x < 0) {\n            if (point.y < 0) {\n                ++bins[0];\n            } else {\n                ++bins[1];\n            }\n        } else {\n            if (point.y < 0) {\n                ++bins[2];\n            } else {\n                ++bins[3];\n            }\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (auto &p : points) {\n      #pragma omp critical\n      {\n         if (p.x > 0 && p.y > 0)\n            bins[0]++;\n         else if (p.x < 0 && p.y > 0)\n            bins[1]++;\n         else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n}",
            "// TODO: implement here\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n  #pragma omp parallel for schedule(static)\n  for (size_t i = 0; i < points.size(); ++i) {\n    auto const &p = points[i];\n    if (p.x > 0 && p.y > 0) {\n      #pragma omp atomic\n      ++bins[0];\n    } else if (p.x < 0 && p.y > 0) {\n      #pragma omp atomic\n      ++bins[1];\n    } else if (p.x < 0 && p.y < 0) {\n      #pragma omp atomic\n      ++bins[2];\n    } else {\n      #pragma omp atomic\n      ++bins[3];\n    }\n  }\n}",
            "#pragma omp parallel default(none) shared(points, bins)\n   {\n      #pragma omp for\n      for (size_t i = 0; i < points.size(); i++) {\n         // TODO\n         Point p = points[i];\n         int q = (p.x >= 0)? 1 : 0;\n         q += (p.y >= 0)? 2 : 0;\n         #pragma omp atomic\n         bins[q]++;\n      }\n   }\n}",
            "/* Your code goes here */\n\n}",
            "bins.fill(0);\n#pragma omp parallel\n#pragma omp for\n    for (size_t i = 0; i < points.size(); ++i) {\n        Point const& p = points[i];\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "}",
            "#pragma omp parallel default(none) firstprivate(points)\n   {\n      int count = 0;\n      #pragma omp for\n      for (size_t i = 0; i < points.size(); ++i) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            count++;\n         } else if (points[i].x <= 0 && points[i].y >= 0) {\n            count += 2;\n         } else if (points[i].x <= 0 && points[i].y <= 0) {\n            count += 3;\n         } else {\n            count += 4;\n         }\n      }\n      #pragma omp critical\n      {\n         bins[omp_get_thread_num()] = count;\n      }\n   }\n}",
            "// TODO\n}",
            "size_t numThreads = omp_get_max_threads();\n   std::cout << \"Counting in \" << numThreads << \" threads.\" << std::endl;\n   bins.fill(0);\n   #pragma omp parallel\n   {\n      #pragma omp for reduction(+:bins[0], bins[1], bins[2], bins[3])\n      for (size_t i = 0; i < points.size(); i++) {\n         if (points[i].x > 0 && points[i].y > 0)\n            bins[0]++;\n         else if (points[i].x < 0 && points[i].y > 0)\n            bins[1]++;\n         else if (points[i].x < 0 && points[i].y < 0)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "// count quadrants, using only one thread here\n    // use omp_get_thread_num to get the thread number and\n    // store the result in the appropriate quadrant\n}",
            "#pragma omp parallel\n   {\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const& p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "omp_set_num_threads(omp_get_max_threads());\n\n   // your code goes here\n}",
            "#pragma omp parallel for\n    for (size_t i = 0; i < points.size(); i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x >= 0) {\n            if (y >= 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        } else {\n            if (y >= 0) {\n                bins[2]++;\n            } else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "size_t numPoints = points.size();\n   std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n   for (size_t i = 0; i < numPoints; i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         bins[0] += 1;\n      } else if (x < 0 && y >= 0) {\n         bins[1] += 1;\n      } else if (x < 0 && y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "// TODO: implement this function\n}",
            "bins = std::array<size_t, 4>();\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      auto const& p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      } else {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "// Write your code here\n   size_t numThreads = 1;\n#pragma omp parallel\n{\n#pragma omp single\nnumThreads = omp_get_num_threads();\n}\n\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x < 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[1]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "int num_threads = omp_get_max_threads();\n   std::array<size_t, 4> partial_results;\n   std::array<std::vector<Point>, 4> chunks;\n\n   for (int i = 0; i < num_threads; ++i) {\n      chunks[i].reserve(points.size() / num_threads);\n   }\n   for (auto const& p : points) {\n      int chunk_id = 0;\n      if (p.x > 0 && p.y > 0) {\n         chunk_id = 0;\n      } else if (p.x < 0 && p.y > 0) {\n         chunk_id = 1;\n      } else if (p.x < 0 && p.y < 0) {\n         chunk_id = 2;\n      } else {\n         chunk_id = 3;\n      }\n      chunks[chunk_id].emplace_back(p);\n   }\n\n   for (int i = 0; i < num_threads; ++i) {\n      partial_results[i] = chunks[i].size();\n   }\n   for (int i = 0; i < num_threads; ++i) {\n      bins[i] = partial_results[i];\n   }\n}",
            "// TODO: implement this function\n\n    //std::array<size_t, 4> bins = {0, 0, 0, 0};\n    //int num_threads = 0;\n    //num_threads = omp_get_num_procs();\n    //num_threads = omp_get_max_threads();\n    //#pragma omp parallel for\n    //for (size_t i = 0; i < points.size(); i++) {\n    //    if ((points[i].x >= 0 && points[i].y >= 0) || (points[i].x <= 0 && points[i].y <= 0)) {\n    //        bins[0]++;\n    //    }\n    //    else if ((points[i].x >= 0 && points[i].y < 0) || (points[i].x < 0 && points[i].y >= 0)) {\n    //        bins[1]++;\n    //    }\n    //    else if ((points[i].x <= 0 && points[i].y > 0) || (points[i].x > 0 && points[i].y <= 0)) {\n    //        bins[2]++;\n    //    }\n    //    else if ((points[i].x < 0 && points[i].y < 0) || (points[i].x > 0 && points[i].y > 0)) {\n    //        bins[3]++;\n    //    }\n    //}\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (size_t i = 0; i < points.size(); i++) {\n        if ((points[i].x >= 0 && points[i].y >= 0) || (points[i].x <= 0 && points[i].y <= 0)) {\n            bins[0]++;\n        }\n        else if ((points[i].x >= 0 && points[i].y < 0) || (points[i].x < 0 && points[i].y >= 0)) {\n            bins[1]++;\n        }\n        else if ((points[i].x <= 0 && points[i].y > 0) || (points[i].x > 0 && points[i].y <= 0)) {\n            bins[2]++;\n        }\n        else if ((points[i].x < 0 && points[i].y < 0) || (points[i].x > 0 && points[i].y > 0)) {\n            bins[3]++;\n        }\n    }\n\n}",
            "const size_t N = points.size();\n    bins.fill(0);\n    #pragma omp parallel\n    #pragma omp single\n    {\n        #pragma omp task\n        bins[0] = std::count_if(points.begin(), points.end(), [](Point const& p){return p.x > 0 && p.y > 0;});\n\n        #pragma omp task\n        bins[1] = std::count_if(points.begin(), points.end(), [](Point const& p){return p.x < 0 && p.y > 0;});\n\n        #pragma omp task\n        bins[2] = std::count_if(points.begin(), points.end(), [](Point const& p){return p.x < 0 && p.y < 0;});\n\n        #pragma omp task\n        bins[3] = std::count_if(points.begin(), points.end(), [](Point const& p){return p.x > 0 && p.y < 0;});\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for schedule(static, 1)\n   for(auto const& p: points) {\n      if (p.x >= 0) {\n         if (p.y >= 0)\n            ++bins[0];\n         else\n            ++bins[1];\n      } else {\n         if (p.y >= 0)\n            ++bins[2];\n         else\n            ++bins[3];\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         #pragma omp atomic\n         bins[0] += 1;\n      } else if (x < 0 && y > 0) {\n         #pragma omp atomic\n         bins[1] += 1;\n      } else if (x < 0 && y < 0) {\n         #pragma omp atomic\n         bins[2] += 1;\n      } else if (x > 0 && y < 0) {\n         #pragma omp atomic\n         bins[3] += 1;\n      }\n   }\n}",
            "#pragma omp parallel\n    {\n    #pragma omp for\n    for (int i = 0; i < points.size(); ++i) {\n        const auto x = points[i].x;\n        const auto y = points[i].y;\n        if (x >= 0 && y >= 0) {\n            bins[0]++;\n        }\n        else if (x < 0 && y >= 0) {\n            bins[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n    }\n}",
            "for (auto const& p : points) {\n#pragma omp task firstprivate(p)\n      bins[0] += (p.x > 0);\n      bins[1] += (p.x < 0);\n      bins[2] += (p.y > 0);\n      bins[3] += (p.y < 0);\n   }\n#pragma omp taskwait\n   for (auto &b : bins)\n      ++b;\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      const auto& p = points[i];\n      int bin = 0;\n      if (p.x > 0 && p.y > 0) {\n         bin = 1;\n      } else if (p.x < 0 && p.y > 0) {\n         bin = 2;\n      } else if (p.x < 0 && p.y < 0) {\n         bin = 3;\n      }\n      bins[bin] += 1;\n   }\n}",
            "#pragma omp parallel for\n   for (auto &point : points) {\n      if (point.x > 0 && point.y > 0) ++bins[0];\n      else if (point.x < 0 && point.y > 0) ++bins[1];\n      else if (point.x < 0 && point.y < 0) ++bins[2];\n      else if (point.x > 0 && point.y < 0) ++bins[3];\n   }\n}",
            "#pragma omp parallel\n   {\n       // Thread private bins\n       std::array<size_t, 4> local_bins;\n       local_bins.fill(0);\n       #pragma omp for schedule(static)\n       for (size_t i=0; i<points.size(); ++i) {\n           Point p = points[i];\n           int x_quadrant = (p.x >= 0)? 0 : 1;\n           int y_quadrant = (p.y >= 0)? 0 : 1;\n           int quadrant = x_quadrant + y_quadrant*2;\n           ++local_bins[quadrant];\n       }\n       // Merge bins into global bins\n       #pragma omp critical\n       {\n           for (size_t i=0; i<local_bins.size(); ++i) {\n               bins[i] += local_bins[i];\n           }\n       }\n   }\n}",
            "/* your solution */\n   size_t numThreads = omp_get_max_threads();\n   bins = std::array<size_t, 4>();\n\n#pragma omp parallel for num_threads(numThreads)\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      }\n      else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n    for (auto const& p : points) {\n        size_t x = (p.x > 0)? 1 : 0;\n        size_t y = (p.y > 0)? 2 : 0;\n        bins[x + y]++;\n    }\n}",
            "int const threads = 1;\n   int const threshold = 10;\n#pragma omp parallel num_threads(threads)\n{\n#pragma omp for\n   for (int i = 0; i < 4; i++) {\n      bins[i] = 0;\n      for (auto const& point : points) {\n         double const& x = point.x;\n         double const& y = point.y;\n         int const count = (x > 0) + (y > 0);\n         if (i == count) {\n            bins[i] += 1;\n         }\n      }\n   }\n}\n   for (auto const& bin : bins) {\n      if (bin > threshold) {\n         std::cout << \"warning: over \" << threshold << \" in quadrant \" << bin << std::endl;\n      }\n   }\n}",
            "// Your code here\n    // TODO\n    // You may want to use the `bins` vector and the `points` vector.\n    // Each OpenMP thread should only count the points in its thread-local quadrant.\n    // Note that if you use a single thread, the code will not be parallelized.\n    // Note also that this is an example of \"race-free\" programming.\n\n    bins.fill(0);\n\n    size_t N = points.size();\n#pragma omp parallel for\n    for (size_t i = 0; i < N; i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n        if (x > 0 && y > 0) {\n            bins[0]++;\n        } else if (x < 0 && y > 0) {\n            bins[1]++;\n        } else if (x < 0 && y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "#pragma omp parallel for schedule(static)\n   for (auto const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         ++bins[1];\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         ++bins[2];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "#pragma omp parallel for\n   for(size_t i=0;i<points.size();i++) {\n      if(points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      }\n      else if(points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      }\n      else if(points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "// Your code goes here...\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      // put your code here\n   }\n}",
            "bins = {0, 0, 0, 0};\n}",
            "// TODO\n  for (Point &p : points) {\n    if (p.x > 0 && p.y > 0)\n      bins[0] += 1;\n    else if (p.x < 0 && p.y > 0)\n      bins[1] += 1;\n    else if (p.x < 0 && p.y < 0)\n      bins[2] += 1;\n    else\n      bins[3] += 1;\n  }\n}",
            "bins.fill(0);\n   size_t const N = points.size();\n   #pragma omp parallel for\n   for(size_t i=0; i < N; ++i) {\n      auto const& p = points[i];\n      if(p.x > 0) {\n         if(p.y > 0)\n            bins[0]++;\n         else\n            bins[1]++;\n      }\n      else {\n         if(p.y > 0)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n}",
            "// FIXME: write function body\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        double x = points[i].x, y = points[i].y;\n        if (x > 0) {\n            if (y > 0) bins[0]++;\n            else bins[3]++;\n        }\n        else {\n            if (y > 0) bins[1]++;\n            else bins[2]++;\n        }\n    }\n}",
            "// Your code goes here\n}",
            "/* TODO */\n}",
            "#pragma omp parallel for\n   for (auto &p : points) {\n      if (p.x > 0 && p.y > 0)\n         bins[0]++;\n      else if (p.x < 0 && p.y > 0)\n         bins[1]++;\n      else if (p.x < 0 && p.y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "#pragma omp parallel\n   {\n      size_t binCount = 0;\n      #pragma omp for schedule(dynamic)\n      for (size_t i = 0; i < points.size(); i++) {\n         double x = points[i].x;\n         double y = points[i].y;\n         if (x > 0 && y > 0) {\n            binCount++;\n         } else if (x < 0 && y > 0) {\n            binCount++;\n         } else if (x < 0 && y < 0) {\n            binCount++;\n         } else if (x > 0 && y < 0) {\n            binCount++;\n         }\n      }\n      #pragma omp critical\n      bins[omp_get_thread_num()] = binCount;\n   }\n}",
            "bins = std::array<size_t, 4>();\n   omp_set_num_threads(4);\n   #pragma omp parallel for schedule(dynamic)\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0)\n         bins[0]++;\n      else if (points[i].x < 0 && points[i].y > 0)\n         bins[1]++;\n      else if (points[i].x < 0 && points[i].y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "// YOUR CODE HERE\n}",
            "for (auto p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x, y = points[i].y;\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      } else if (x < 0 && y > 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "#pragma omp parallel\n    {\n        size_t local_bins[4] = {0, 0, 0, 0};\n        #pragma omp for\n        for (int i = 0; i < points.size(); i++) {\n            if (points[i].x > 0) {\n                if (points[i].y > 0) {\n                    local_bins[0] += 1;\n                }\n                else {\n                    local_bins[1] += 1;\n                }\n            }\n            else {\n                if (points[i].y > 0) {\n                    local_bins[2] += 1;\n                }\n                else {\n                    local_bins[3] += 1;\n                }\n            }\n        }\n        #pragma omp critical\n        {\n            for (int i = 0; i < 4; i++) {\n                bins[i] += local_bins[i];\n            }\n        }\n    }\n}",
            "auto size = points.size();\n   std::array<size_t, 4> b;\n   #pragma omp parallel for\n   for (int i=0; i<size; i++){\n       auto x = points[i].x;\n       auto y = points[i].y;\n       if (x >= 0 && y >= 0){\n           b[0]++;\n       }\n       else if (x < 0 && y >= 0){\n           b[1]++;\n       }\n       else if (x < 0 && y < 0){\n           b[2]++;\n       }\n       else {\n           b[3]++;\n       }\n   }\n   bins = b;\n}",
            "bins.fill(0);\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         ++bins[0];\n      }\n      else if (points[i].x < 0 && points[i].y >= 0) {\n         ++bins[1];\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "int numThreads = 1;\n#pragma omp parallel\n#pragma omp single\n   {\n      numThreads = omp_get_num_threads();\n   }\n\n   size_t numPoints = points.size();\n\n   size_t numPointsPerThread = numPoints / numThreads;\n   size_t startPoint = 0, endPoint = 0;\n   for (int i = 0; i < numThreads; i++) {\n      startPoint = endPoint;\n      endPoint = startPoint + numPointsPerThread;\n      if (i == numThreads - 1) {\n         endPoint = numPoints;\n      }\n      bins[0] = std::count_if(points.begin() + startPoint, points.begin() + endPoint, [](Point const& p) {\n         return p.x >= 0 && p.y >= 0;\n      });\n      bins[1] = std::count_if(points.begin() + startPoint, points.begin() + endPoint, [](Point const& p) {\n         return p.x < 0 && p.y >= 0;\n      });\n      bins[2] = std::count_if(points.begin() + startPoint, points.begin() + endPoint, [](Point const& p) {\n         return p.x < 0 && p.y < 0;\n      });\n      bins[3] = std::count_if(points.begin() + startPoint, points.begin() + endPoint, [](Point const& p) {\n         return p.x >= 0 && p.y < 0;\n      });\n   }\n}",
            "// TODO: implement\n   std::array<size_t, 4> bin_size = {0,0,0,0};\n\n   #pragma omp parallel for\n   for (int i=0; i<points.size(); i++){\n      if(points[i].x > 0 && points[i].y > 0){\n         bin_size[0]++;\n      }else if(points[i].x < 0 && points[i].y > 0){\n         bin_size[1]++;\n      }else if(points[i].x < 0 && points[i].y < 0){\n         bin_size[2]++;\n      }else{\n         bin_size[3]++;\n      }\n   }\n   bins = bin_size;\n}",
            "bins = std::array<size_t, 4>();\n   size_t n = points.size();\n\n   // for each quadrant, find the number of points in each quadrant\n   // TODO\n}",
            "#pragma omp parallel\n    {\n        #pragma omp for schedule(static) nowait\n        for (size_t i = 0; i < points.size(); i++) {\n            if (points[i].x < 0) {\n                if (points[i].y < 0) {\n                    bins[3]++;\n                } else {\n                    bins[1]++;\n                }\n            } else {\n                if (points[i].y < 0) {\n                    bins[2]++;\n                } else {\n                    bins[0]++;\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n\tfor(auto const& point : points) {\n\t\tif (point.x > 0 && point.y > 0) {\n\t\t\tbins[0]++;\n\t\t} else if (point.x < 0 && point.y > 0) {\n\t\t\tbins[1]++;\n\t\t} else if (point.x < 0 && point.y < 0) {\n\t\t\tbins[2]++;\n\t\t} else {\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO\n}",
            "// Your code here\n}",
            "auto N = points.size();\n\n#pragma omp parallel for\n  for (size_t i = 0; i < N; ++i) {\n    double x = points[i].x;\n    double y = points[i].y;\n    int bin = (x < 0) * (1 - (y < 0)) + (x >= 0) * (1 - (y >= 0));\n    ++bins[bin];\n  }\n}",
            "// TODO: implement\n}",
            "// YOUR CODE HERE\n  bins.fill(0);\n  auto n = points.size();\n  // omp_set_num_threads(2);\n  #pragma omp parallel for reduction(+:bins[0],bins[1],bins[2],bins[3])\n  for (int i = 0; i < n; ++i) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      bins[0]++;\n    } else if (points[i].x <= 0 && points[i].y >= 0) {\n      bins[1]++;\n    } else if (points[i].x <= 0 && points[i].y <= 0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n  // END YOUR CODE\n}",
            "// Your code goes here.\n}",
            "// TODO: implement me\n}",
            "int nthreads = 0;\n   #pragma omp parallel\n   {\n      #pragma omp single\n      nthreads = omp_get_num_threads();\n   }\n\n   bins.fill(0);\n\n   for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n\n   for (size_t &bin : bins) {\n      bin /= nthreads;\n   }\n}",
            "size_t n_points = points.size();\n   std::array<size_t, 4> bins_private;\n   bins_private.fill(0);\n   #pragma omp parallel for\n   for(size_t i = 0; i < n_points; i++) {\n      // add your code here\n   }\n   bins = bins_private;\n}",
            "#pragma omp parallel for\n   for (auto const& p : points) {\n      auto x = p.x;\n      auto y = p.y;\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      } else if (x < 0 && y > 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "/* TODO */\n   bins = {0, 0, 0, 0};\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0) {\n         if (y > 0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (y > 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "// Your code here\n   size_t n = points.size();\n   bins.fill(0);\n   for (size_t i = 0; i < n; i++) {\n      Point const& p = points[i];\n      int bin = (p.x >= 0)? 0 : 1;\n      bin += (p.y >= 0)? 0 : 2;\n      bins[bin]++;\n   }\n}",
            "bins = std::array<size_t, 4>({0, 0, 0, 0});\n    #pragma omp parallel shared(points, bins)\n    {\n        size_t my_bin = omp_get_thread_num() % 4;\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); i++) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                bins[my_bin]++;\n            } else if (points[i].x < 0 && points[i].y > 0) {\n                bins[my_bin + 1]++;\n            } else if (points[i].x < 0 && points[i].y < 0) {\n                bins[my_bin + 2]++;\n            } else {\n                bins[my_bin + 3]++;\n            }\n        }\n    }\n}",
            "bins = std::array<size_t, 4>();\n   #pragma omp parallel\n   {\n      #pragma omp for\n      for (size_t i=0; i<points.size(); i++) {\n         auto p = points[i];\n         if (p.x > 0 && p.y > 0) {\n            bins[0] += 1;\n         } else if (p.x < 0 && p.y > 0) {\n            bins[1] += 1;\n         } else if (p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "for (auto &i : bins)\n        i = 0;\n    #pragma omp parallel for reduction(+:bins)\n    for (auto const& p : points) {\n        int const quadrant = (p.x > 0? 1 : 0) + (p.y > 0? 2 : 0);\n        #pragma omp atomic\n        ++bins[quadrant];\n    }\n}",
            "// TODO: implement me\n}",
            "bins = {0,0,0,0};\n#pragma omp parallel for\n   for (size_t i=0; i<points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         ++bins[0];\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         ++bins[1];\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "// Your code goes here.\n    // TODO\n}",
            "for (auto const& p : points) {\n      int i = (p.x > 0) + 2 * (p.y > 0);\n      bins[i] += 1;\n   }\n}",
            "#pragma omp parallel\n    {\n        size_t i, j;\n        size_t numThreads = omp_get_num_threads();\n        size_t threadId = omp_get_thread_num();\n\n        size_t numPoints = points.size() / numThreads;\n        std::vector<Point> myPoints;\n        myPoints.reserve(numPoints);\n\n        for (i = 0; i < numPoints; i++) {\n            myPoints.push_back(points[i * numThreads + threadId]);\n        }\n\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n\n        for (i = 0; i < myPoints.size(); i++) {\n            double x = myPoints[i].x;\n            double y = myPoints[i].y;\n\n            if (x >= 0 && y >= 0)\n                bins[0]++;\n            else if (x < 0 && y >= 0)\n                bins[1]++;\n            else if (x < 0 && y < 0)\n                bins[2]++;\n            else\n                bins[3]++;\n        }\n    }\n}",
            "bins.fill(0);\n#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      }\n      else if (x < 0 && y > 0) {\n         bins[1]++;\n      }\n      else if (x < 0 && y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO\n   size_t size = points.size();\n   size_t num = omp_get_max_threads();\n   size_t part = size / num;\n   bins.fill(0);\n   #pragma omp parallel num_threads(num)\n   {\n      int threadID = omp_get_thread_num();\n      size_t offset = threadID * part;\n      size_t end = (threadID + 1) * part;\n      for (size_t i = offset; i < end; ++i) {\n         Point const& p = points[i];\n         if (p.x > 0 && p.y > 0) {\n            ++bins[0];\n         } else if (p.x < 0 && p.y > 0) {\n            ++bins[1];\n         } else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "// Your code here\n    int n = points.size();\n    bins.fill(0);\n    #pragma omp parallel num_threads(4)\n    {\n        #pragma omp for schedule(static,n/4)\n        for(int i=0;i<n;i++)\n        {\n            if(points[i].x>=0&&points[i].y>=0)\n                bins[0]++;\n            else if(points[i].x>=0&&points[i].y<0)\n                bins[1]++;\n            else if(points[i].x<0&&points[i].y<0)\n                bins[2]++;\n            else if(points[i].x<0&&points[i].y>=0)\n                bins[3]++;\n        }\n    }\n}",
            "// Your code goes here\n}",
            "// TODO: write code to count the number of points in each quadrant.\n   // TODO: use OpenMP to parallelize the loop.\n   // TODO: do not change this code\n   bins = std::array<size_t, 4>();\n   for (auto const& p : points) {\n      if (p.x < 0 && p.y < 0) {\n         bins[0]++;\n      }\n      else if (p.x >= 0 && p.y < 0) {\n         bins[1]++;\n      }\n      else if (p.x < 0 && p.y >= 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "bins = std::array<size_t, 4>{0};\n#pragma omp parallel\n   {\n#pragma omp for\n      for (auto const& p : points) {\n         //TODO: add a condition to check if the point is in the quadrant and add it\n         if (p.x > 0 && p.y > 0) {\n            bins[0] += 1;\n         } else if (p.x < 0 && p.y > 0) {\n            bins[1] += 1;\n         } else if (p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    #pragma omp parallel for\n    for (auto const &p : points) {\n        if (p.x < 0) {\n            if (p.y < 0) {\n                #pragma omp atomic\n                ++bins[0];\n            } else {\n                #pragma omp atomic\n                ++bins[1];\n            }\n        } else {\n            if (p.y < 0) {\n                #pragma omp atomic\n                ++bins[2];\n            } else {\n                #pragma omp atomic\n                ++bins[3];\n            }\n        }\n    }\n}",
            "// YOUR CODE HERE\n\n   // 2.1: Count the number of points in each quadrant.\n   // You need to use an OpenMP reduction pragma here.\n#pragma omp parallel for reduction(+:bins[0:4])\n   for (size_t i = 0; i < points.size(); i++) {\n      auto const &p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n   // END OF YOUR CODE\n\n}",
            "// fill bins with zeros\n  for (size_t i = 0; i < bins.size(); i++) {\n    bins[i] = 0;\n  }\n  // TODO\n  // YOUR CODE HERE\n  #pragma omp parallel num_threads(4)\n  {\n    int thread_id = omp_get_thread_num();\n    #pragma omp for\n    for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n        bins[0] += 1;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n        bins[1] += 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n        bins[2] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "// TODO\n}",
            "}",
            "// TODO\n}",
            "/* Your code goes here */\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[0] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[1] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2] += 1;\n      }\n      else {\n         bins[3] += 1;\n      }\n   }\n}",
            "// TODO\n}",
            "// Your code goes here.\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n#pragma omp parallel for\n   for (auto const& point : points) {\n      int quadrant = (point.x > 0)? (point.y > 0) : 2;\n      ++bins[quadrant];\n   }\n}",
            "// Your code here\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n    int threads_num = omp_get_max_threads();\n#pragma omp parallel for\n    for (int i = 0; i < threads_num; i++) {\n        int bin = 0;\n        for (int i = 0; i < points.size(); i++) {\n            if (points[i].x >= 0 && points[i].y >= 0) bin = 0;\n            else if (points[i].x < 0 && points[i].y >= 0) bin = 1;\n            else if (points[i].x < 0 && points[i].y < 0) bin = 2;\n            else if (points[i].x >= 0 && points[i].y < 0) bin = 3;\n            bins[bin]++;\n        }\n    }\n}",
            "// YOUR CODE HERE\n}",
            "int num_threads = 1;\n#pragma omp parallel\n  {\n#pragma omp single\n    num_threads = omp_get_num_threads();\n\n#pragma omp for schedule(dynamic, 1)\n    for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n        ++bins[0];\n      } else if (points[i].x < 0 && points[i].y > 0) {\n        ++bins[1];\n      } else if (points[i].x < 0 && points[i].y < 0) {\n        ++bins[2];\n      } else {\n        ++bins[3];\n      }\n    }\n  }\n}",
            "/* Your code goes here */\n  int num_threads = 4;\n  omp_set_num_threads(num_threads);\n  #pragma omp parallel\n  {\n    size_t i = omp_get_thread_num();\n    size_t start = i * points.size() / num_threads;\n    size_t end = (i + 1) * points.size() / num_threads;\n    for (size_t i = start; i < end; i++)\n    {\n      if (points[i].x >= 0 && points[i].y >= 0) bins[0] += 1;\n      if (points[i].x < 0 && points[i].y >= 0) bins[1] += 1;\n      if (points[i].x < 0 && points[i].y < 0) bins[2] += 1;\n      if (points[i].x >= 0 && points[i].y < 0) bins[3] += 1;\n    }\n  }\n}",
            "// TODO\n   double x, y;\n   for (size_t i = 0; i < points.size(); i++) {\n      x = points[i].x;\n      y = points[i].y;\n      if (x > 0 && y > 0)\n         bins[0]++;\n      else if (x < 0 && y > 0)\n         bins[1]++;\n      else if (x < 0 && y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "//...\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); i++){\n      if(points[i].x > 0 && points[i].y > 0)\n         bins[0]++;\n      else if(points[i].x < 0 && points[i].y > 0)\n         bins[1]++;\n      else if(points[i].x < 0 && points[i].y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "size_t n = points.size();\n    for (size_t i = 0; i < n; i++) {\n        if (points[i].x < 0) {\n            if (points[i].y < 0) {\n                bins[3]++;\n            } else {\n                bins[2]++;\n            }\n        } else {\n            if (points[i].y < 0) {\n                bins[0]++;\n            } else {\n                bins[1]++;\n            }\n        }\n    }\n}",
            "// Your code goes here\n    bins.fill(0);\n}",
            "#pragma omp parallel for schedule(dynamic, 100)\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0] += 1;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1] += 1;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "omp_set_num_threads(omp_get_max_threads());\n#pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x, y = points[i].y;\n      if (x < 0 && y < 0)\n         ++bins[0];\n      else if (x < 0)\n         ++bins[1];\n      else if (y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "// TODO: implement this function\n   bins[0]=0;\n   bins[1]=0;\n   bins[2]=0;\n   bins[3]=0;\n   int i;\n#pragma omp parallel\n   {\n#pragma omp single\n      {\n         i=0;\n         while(i<points.size())\n         {\n            if((points[i].x > 0 && points[i].y > 0) || (points[i].x < 0 && points[i].y < 0))\n            {\n               bins[0]=bins[0]+1;\n            }\n            else if(points[i].x > 0 && points[i].y < 0)\n            {\n               bins[1]=bins[1]+1;\n            }\n            else if(points[i].x < 0 && points[i].y > 0)\n            {\n               bins[2]=bins[2]+1;\n            }\n            else\n            {\n               bins[3]=bins[3]+1;\n            }\n            i=i+1;\n         }\n      }\n   }\n\n}",
            "/* Replace this code with your solution. */\n\n   bins = {};\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      size_t bin_num = -1;\n\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bin_num = 0;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bin_num = 1;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bin_num = 2;\n      } else {\n         bin_num = 3;\n      }\n\n      #pragma omp atomic\n      ++bins[bin_num];\n   }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         bins[0]++;\n      } else if (x <= 0 && y >= 0) {\n         bins[1]++;\n      } else if (x <= 0 && y <= 0) {\n         bins[2]++;\n      } else if (x >= 0 && y <= 0) {\n         bins[3]++;\n      }\n   }\n}",
            "bins = std::array<size_t, 4>();\n   // Your code here\n   #pragma omp parallel for schedule(dynamic)\n   for(int i = 0; i < points.size(); i++){\n      if(points[i].x > 0 && points[i].y > 0){\n         bins[0] += 1;\n      }\n      else if(points[i].x < 0 && points[i].y > 0){\n         bins[1] += 1;\n      }\n      else if(points[i].x < 0 && points[i].y < 0){\n         bins[2] += 1;\n      }\n      else if(points[i].x > 0 && points[i].y < 0){\n         bins[3] += 1;\n      }\n   }\n}",
            "#pragma omp parallel for schedule(static)\n    for (int i = 0; i < points.size(); i++) {\n        double x = points[i].x;\n        double y = points[i].y;\n\n        if (x > 0 && y > 0)\n            bins[0]++;\n        else if (x < 0 && y > 0)\n            bins[1]++;\n        else if (x < 0 && y < 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n}",
            "/* TODO */\n}",
            "#pragma omp parallel num_threads(4)\n    {\n        #pragma omp single\n        {\n            // Split the work among the 4 threads\n            int const myid = omp_get_thread_num();\n            int const npoints = points.size();\n            #pragma omp task firstprivate(myid, npoints)\n            {\n                int const mybin = myid / 2;\n                #pragma omp parallel for firstprivate(mybin) reduction(+:bins[mybin])\n                for (int i = 0; i < npoints; i++) {\n                    double const x = points[i].x;\n                    double const y = points[i].y;\n                    if (mybin == 0 && x < 0 && y < 0) {\n                        bins[mybin] += 1;\n                    } else if (mybin == 1 && x >= 0 && y < 0) {\n                        bins[mybin] += 1;\n                    } else if (mybin == 2 && x >= 0 && y >= 0) {\n                        bins[mybin] += 1;\n                    } else if (mybin == 3 && x < 0 && y >= 0) {\n                        bins[mybin] += 1;\n                    }\n                }\n            }\n        }\n    }\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      Point const &p = points[i];\n      int bin = (p.x > 0) + 2 * (p.y > 0);\n      ++bins[bin];\n   }\n}",
            "// TODO: replace this stub with your code\n}",
            "auto const num_points = points.size();\n   bins.fill(0);\n   #pragma omp parallel for\n   for (int i = 0; i < num_points; i++) {\n      Point point = points[i];\n      if (point.x > 0 && point.y > 0)\n         bins[0]++;\n      else if (point.x < 0 && point.y > 0)\n         bins[1]++;\n      else if (point.x < 0 && point.y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "bins = std::array<size_t, 4>({0, 0, 0, 0});\n\n  #pragma omp parallel\n  {\n    // Each thread gets its own copy of the bins array\n    // The OpenMP runtime takes care of merging results from the threads to update the global variable\n    std::array<size_t, 4> bins_private = bins;\n\n    // Loop over the points and count in parallel\n    #pragma omp for\n    for (size_t i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      // Each thread counts its own points in each quadrant\n      if (x > 0 && y > 0) {\n        bins_private[0] += 1;\n      } else if (x < 0 && y > 0) {\n        bins_private[1] += 1;\n      } else if (x < 0 && y < 0) {\n        bins_private[2] += 1;\n      } else {\n        bins_private[3] += 1;\n      }\n    }\n\n    // Merge results\n    #pragma omp critical\n    {\n      for (size_t i = 0; i < 4; i++) {\n        bins[i] += bins_private[i];\n      }\n    }\n  }\n}",
            "// TODO: implement me\n}",
            "size_t number_of_points = points.size();\n\n   // YOUR CODE HERE\n   bins = {0, 0, 0, 0};\n\n   // END OF YOUR CODE\n\n   // sanity check\n   assert(bins[0] + bins[1] + bins[2] + bins[3] == number_of_points);\n}",
            "}",
            "bins = std::array<size_t, 4>();\n   omp_set_num_threads(8);\n   #pragma omp parallel for schedule(dynamic)\n   for(auto &p: points) {\n      size_t index = 0;\n      if(p.x > 0) {\n         if(p.y > 0) index = 0;\n         else index = 3;\n      } else if(p.y > 0) {\n         index = 1;\n      } else index = 2;\n      ++bins[index];\n   }\n}",
            "// TODO\n}",
            "bins = std::array<size_t, 4>();\n   std::fill(bins.begin(), bins.end(), 0);\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      Point const& p = points[i];\n      int bin = 0;\n      if (p.x > 0 && p.y > 0) {\n         bin = 0;\n      } else if (p.x < 0 && p.y > 0) {\n         bin = 1;\n      } else if (p.x < 0 && p.y < 0) {\n         bin = 2;\n      } else if (p.x > 0 && p.y < 0) {\n         bin = 3;\n      }\n      #pragma omp atomic\n      bins[bin]++;\n   }\n}",
            "bins = std::array<size_t, 4>{};\n\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      auto p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "bins = std::array<size_t, 4>({0, 0, 0, 0});\n   #pragma omp parallel for\n   for (auto const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: Your code goes here\n}",
            "// count points in each quadrant\n\n  // YOUR CODE HERE\n\n}",
            "bins = {0, 0, 0, 0};\n   #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n   for (size_t i = 0; i < points.size(); i++) {\n      // count point in quadrant\n      bins[0] += (points[i].x > 0 && points[i].y > 0);\n      bins[1] += (points[i].x < 0 && points[i].y > 0);\n      bins[2] += (points[i].x < 0 && points[i].y < 0);\n      bins[3] += (points[i].x > 0 && points[i].y < 0);\n   }\n}",
            "int n = points.size();\n  std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel for\n  for (int i = 0; i < n; i++) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      bins[0] += 1;\n    } else if (points[i].x < 0 && points[i].y >= 0) {\n      bins[1] += 1;\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "#pragma omp parallel for\n   for(size_t i = 0; i < points.size(); i++) {\n      auto &p = points[i];\n      if(p.x > 0 && p.y > 0) {\n         #pragma omp atomic\n         bins[0] += 1;\n      } else if(p.x < 0 && p.y > 0) {\n         #pragma omp atomic\n         bins[1] += 1;\n      } else if(p.x < 0 && p.y < 0) {\n         #pragma omp atomic\n         bins[2] += 1;\n      } else if(p.x > 0 && p.y < 0) {\n         #pragma omp atomic\n         bins[3] += 1;\n      }\n   }\n}",
            "// your code here\n}",
            "#pragma omp parallel\n   {\n      #pragma omp for schedule(static) nowait\n      for (size_t i = 0; i < points.size(); i++) {\n         if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n         } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n         } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "// Your code here.\n}",
            "// TODO: Your code goes here\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: Your code here\n\n  #pragma omp parallel for\n  for(int i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if(x > 0 && y > 0)\n        bins[0] += 1;\n      if(x > 0 && y < 0)\n        bins[1] += 1;\n      if(x < 0 && y < 0)\n        bins[2] += 1;\n      if(x < 0 && y > 0)\n        bins[3] += 1;\n  }\n}",
            "bins.fill(0);\n\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      } else if (x < 0 && y > 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// Your code goes here\n}",
            "/* Your code goes here */\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: Your code here\n}",
            "// YOUR CODE HERE\n}",
            "#pragma omp parallel for\n   for (auto const &point : points) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            bins[0]++;\n         } else {\n            bins[3]++;\n         }\n      } else {\n         if (point.y > 0) {\n            bins[1]++;\n         } else {\n            bins[2]++;\n         }\n      }\n   }\n}",
            "#pragma omp parallel\n    {\n        std::array<size_t, 4> localBins{0, 0, 0, 0};\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); i++) {\n            double x = points[i].x;\n            double y = points[i].y;\n            if (x > 0 && y > 0) {\n                localBins[0]++;\n            } else if (x < 0 && y > 0) {\n                localBins[1]++;\n            } else if (x < 0 && y < 0) {\n                localBins[2]++;\n            } else if (x > 0 && y < 0) {\n                localBins[3]++;\n            }\n        }\n        #pragma omp critical\n        {\n            for (size_t i = 0; i < 4; i++) {\n                bins[i] += localBins[i];\n            }\n        }\n    }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n#pragma omp parallel\n   {\n      size_t thread_id = omp_get_thread_num();\n      size_t total_threads = omp_get_num_threads();\n      size_t slice_size = points.size() / total_threads;\n      size_t start = slice_size * thread_id;\n      size_t end = (thread_id == (total_threads - 1))? points.size() : start + slice_size;\n\n      for (size_t i = start; i < end; ++i) {\n         if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n         } else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n         } else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "const size_t num_threads = 4;\n#pragma omp parallel num_threads(num_threads)\n    {\n        size_t thread_id = omp_get_thread_num();\n        size_t lower_index = thread_id * points.size() / num_threads;\n        size_t upper_index = (thread_id + 1) * points.size() / num_threads;\n        for (auto i = lower_index; i < upper_index; i++) {\n            auto point = points[i];\n            if (point.x < 0) {\n                if (point.y < 0) {\n                    bins[0]++;\n                } else {\n                    bins[1]++;\n                }\n            } else {\n                if (point.y < 0) {\n                    bins[2]++;\n                } else {\n                    bins[3]++;\n                }\n            }\n        }\n    }\n}",
            "// TODO: fill in this function\n}",
            "for (auto const& point : points) {\n#pragma omp task firstprivate(point)\n        {\n            // put your code here\n            if (point.x > 0 && point.y > 0) {\n                bins[0] += 1;\n            } else if (point.x < 0 && point.y > 0) {\n                bins[1] += 1;\n            } else if (point.x < 0 && point.y < 0) {\n                bins[2] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        }\n    }\n#pragma omp taskwait\n}",
            "size_t N = points.size();\n   bins = {0, 0, 0, 0};\n\n   /* WRITE YOUR CODE HERE */\n   #pragma omp parallel for\n   for(size_t i = 0; i < N; i++) {\n      if(points[i].x >= 0 && points[i].y >= 0)\n         bins[0]++;\n      else if(points[i].x < 0 && points[i].y >= 0)\n         bins[1]++;\n      else if(points[i].x < 0 && points[i].y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "serial",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "for (Point const& point : points) {\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        } else if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "for (Point const& p : points) {\n    if (p.x > 0 && p.y > 0) {\n      bins[0] += 1;\n    } else if (p.x < 0 && p.y > 0) {\n      bins[1] += 1;\n    } else if (p.x < 0 && p.y < 0) {\n      bins[2] += 1;\n    } else {\n      bins[3] += 1;\n    }\n  }\n}",
            "std::array<size_t, 4> counts = { 0, 0, 0, 0 };\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         counts[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         counts[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         counts[2]++;\n      } else {\n         counts[3]++;\n      }\n   }\n   bins = counts;\n}",
            "for (auto const &point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (Point const& p: points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            bins[0]++;\n         }\n         else {\n            bins[1]++;\n         }\n      }\n      else {\n         if (p.y > 0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "/* your code here */\n}",
            "/* TODO: Your code goes here. */\n   bins = {0, 0, 0, 0};\n   for(auto p: points) {\n      auto idx = p.x >= 0 && p.y >= 0? 0 : (p.x < 0 && p.y >= 0? 1 : (p.x < 0 && p.y < 0? 2 : 3));\n      bins[idx]++;\n   }\n}",
            "size_t count = 0;\n    for (Point p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0] = count++;\n        }\n        else if (p.x < 0 && p.y > 0) {\n            bins[1] = count++;\n        }\n        else if (p.x < 0 && p.y < 0) {\n            bins[2] = count++;\n        }\n        else {\n            bins[3] = count++;\n        }\n    }\n}",
            "for (Point const& point : points) {\n      if (point.x >= 0) {\n         if (point.y >= 0) {\n            bins[0]++;\n         }\n         else {\n            bins[1]++;\n         }\n      }\n      else {\n         if (point.y >= 0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "for (auto const& point : points) {\n        double dx = point.x;\n        double dy = point.y;\n        if (dx >= 0 && dy >= 0) {\n            ++bins[0];\n        } else if (dx < 0 && dy >= 0) {\n            ++bins[1];\n        } else if (dx < 0 && dy < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "bins = {0, 0, 0, 0};\n   for (auto const& p : points) {\n      int bin = 0;\n      if (p.x > 0) {\n         if (p.y > 0)\n            bin = 0;\n         else\n            bin = 3;\n      } else {\n         if (p.y > 0)\n            bin = 1;\n         else\n            bin = 2;\n      }\n      bins[bin] += 1;\n   }\n}",
            "// Fill bins with zeros\n    for (auto &count: bins) {\n        count = 0;\n    }\n\n    // Count number of points in each quadrant\n    for (auto &point: points) {\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        } else if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "size_t const N = points.size();\n   for (auto p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x < 0) {\n         if (p.y < 0)\n            ++bins[0];\n         else\n            ++bins[1];\n      } else {\n         if (p.y < 0)\n            ++bins[2];\n         else\n            ++bins[3];\n      }\n   }\n}",
            "for (auto const & p : points) {\n      size_t bin = 0;\n      if (p.x > 0)\n         bin |= 1;\n      if (p.y > 0)\n         bin |= 2;\n      bins[bin] += 1;\n   }\n}",
            "for (const Point& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0] += 1;\n      } else if (point.x <= 0 && point.y >= 0) {\n         bins[1] += 1;\n      } else if (point.x <= 0 && point.y <= 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "bins[0] = 0;\n  bins[1] = 0;\n  bins[2] = 0;\n  bins[3] = 0;\n\n  for (auto& point : points) {\n    if (point.x > 0) {\n      if (point.y > 0) {\n        bins[0] += 1;\n      } else {\n        bins[1] += 1;\n      }\n    } else {\n      if (point.y > 0) {\n        bins[2] += 1;\n      } else {\n        bins[3] += 1;\n      }\n    }\n  }\n}",
            "for (Point const& p : points) {\n    int bin;\n    if (p.x > 0) {\n      if (p.y > 0) {\n        bin = 0;\n      } else {\n        bin = 3;\n      }\n    } else {\n      if (p.y > 0) {\n        bin = 1;\n      } else {\n        bin = 2;\n      }\n    }\n    ++bins[bin];\n  }\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) ++bins[0];\n      else if (p.x < 0 && p.y > 0) ++bins[1];\n      else if (p.x < 0 && p.y < 0) ++bins[2];\n      else ++bins[3];\n    }\n}",
            "for (auto p : points) {\n      if (p.x >= 0 && p.y >= 0)\n         bins[0] += 1;\n      else if (p.x < 0 && p.y >= 0)\n         bins[1] += 1;\n      else if (p.x < 0 && p.y < 0)\n         bins[2] += 1;\n      else // (p.x >= 0 && p.y < 0)\n         bins[3] += 1;\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      int bin = 0;\n      if (point.x >= 0) {\n         bin += 1;\n      }\n      if (point.y >= 0) {\n         bin += 2;\n      }\n      ++bins[bin];\n   }\n}",
            "// your code here\n}",
            "for (auto const& p : points) {\n      int x = p.x > 0? 1 : p.x < 0? -1 : 0;\n      int y = p.y > 0? 1 : p.y < 0? -1 : 0;\n      if (x == 0) {\n         if (y == 0)\n            ++bins[0];\n         else if (y > 0)\n            ++bins[1];\n         else\n            ++bins[2];\n      }\n      else if (x > 0) {\n         if (y == 0)\n            ++bins[3];\n         else if (y > 0)\n            ++bins[0];\n         else\n            ++bins[3];\n      }\n      else {\n         if (y == 0)\n            ++bins[2];\n         else if (y > 0)\n            ++bins[1];\n         else\n            ++bins[0];\n      }\n   }\n}",
            "for (auto p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& point : points) {\n    if (point.x < 0) {\n      if (point.y > 0) {\n        bins[0]++;\n      } else {\n        bins[1]++;\n      }\n    } else {\n      if (point.y > 0) {\n        bins[2]++;\n      } else {\n        bins[3]++;\n      }\n    }\n  }\n}",
            "for (auto const& point : points) {\n      int x = static_cast<int>(point.x);\n      int y = static_cast<int>(point.y);\n      int quadrant = 0;\n      if (x > 0) {\n         if (y > 0)\n            quadrant = 1;\n         else\n            quadrant = 2;\n      } else {\n         if (y > 0)\n            quadrant = 3;\n         else\n            quadrant = 4;\n      }\n      ++bins[quadrant];\n   }\n}",
            "for (Point const &p : points) {\n      int bin = (p.x > 0) + 2 * (p.y > 0);\n      ++bins[bin];\n   }\n}",
            "for (auto point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n      int i = 1;\n      if (p.x < 0)\n         ++i;\n      if (p.y < 0)\n         i += 2;\n      ++bins[i];\n   }\n}",
            "// YOUR CODE HERE\n}",
            "for (auto p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0] += 1;\n        } else if (p.x < 0 && p.y > 0) {\n            bins[1] += 1;\n        } else if (p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "bins.fill(0);\n   for (auto const &p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      }\n      else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "// TODO\n}",
            "for (auto point : points) {\n        if (point.x > 0 && point.y > 0) {\n            ++bins[0];\n        }\n        else if (point.x < 0 && point.y > 0) {\n            ++bins[1];\n        }\n        else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "bins = {};\n   for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// Fill bins with number of points in each quadrant\n}",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0) {\n         if (point.y >= 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (point.y >= 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else if (p.x > 0 && p.y < 0)\n         ++bins[3];\n   }\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      }\n      else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (point.y > 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      size_t bin = 0;\n      if (point.x > 0) {\n         bin |= 1;\n      }\n      if (point.y > 0) {\n         bin |= 2;\n      }\n      bins[bin]++;\n   }\n}",
            "// Write your code here.\n}",
            "for (auto const& point : points) {\n      double x = point.x;\n      double y = point.y;\n      size_t quadrant = ((x > 0) && (y > 0))? 0 :\n                         ((x < 0) && (y > 0))? 1 :\n                         ((x < 0) && (y < 0))? 2 :\n                         ((x > 0) && (y < 0))? 3 : 0;\n      ++bins[quadrant];\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// write code here\n}",
            "for (auto const& point : points) {\n      auto x = std::round(point.x);\n      auto y = std::round(point.y);\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      } else if (x < 0 && y > 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else if (x > 0 && y < 0) {\n         bins[3]++;\n      } else {\n         throw std::runtime_error(\"invalid point: \" + std::to_string(point.x) + \" \" + std::to_string(point.y));\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const & p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      }\n      else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "for (Point const& p : points) {\n      int i = (p.x >= 0)? 0 : 1;\n      i += (p.y >= 0)? 0 : 2;\n      bins[i]++;\n   }\n}",
            "for (Point const& p : points) {\n      int bin = 0;\n\n      if (p.x > 0 && p.y > 0) {\n         bin = 1;\n      } else if (p.x < 0 && p.y > 0) {\n         bin = 2;\n      } else if (p.x < 0 && p.y < 0) {\n         bin = 3;\n      }\n\n      ++bins[bin];\n   }\n}",
            "// TODO: implement this function\n}",
            "for (auto const &point : points) {\n      size_t index;\n      if (point.x > 0) {\n         if (point.y > 0) {\n            index = 0;\n         } else {\n            index = 3;\n         }\n      } else {\n         if (point.y > 0) {\n            index = 1;\n         } else {\n            index = 2;\n         }\n      }\n      ++bins[index];\n   }\n}",
            "bins.fill(0);\n   for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "// TODO: implement this function\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0)\n         bins[0] += 1;\n      else if (points[i].x < 0 && points[i].y >= 0)\n         bins[1] += 1;\n      else if (points[i].x < 0 && points[i].y < 0)\n         bins[2] += 1;\n      else\n         bins[3] += 1;\n   }\n}",
            "for (const Point& p: points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (p.y > 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "for (auto const& point : points) {\n        int x = std::floor(point.x);\n        int y = std::floor(point.y);\n        if (y < 0) {\n            if (x < 0) {\n                --bins[3];\n            } else {\n                --bins[2];\n            }\n        } else {\n            if (x < 0) {\n                --bins[1];\n            } else {\n                ++bins[0];\n            }\n        }\n    }\n}",
            "// Fill bins with 0\n    std::fill(bins.begin(), bins.end(), 0);\n\n    // Count each point in the correct quadrant\n    for (auto const& p: points) {\n        if (p.x > 0 && p.y > 0)\n            bins[0]++;\n        else if (p.x < 0 && p.y > 0)\n            bins[1]++;\n        else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n        else if (p.x > 0 && p.y < 0)\n            bins[3]++;\n    }\n}",
            "for (auto p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (auto const& p : points) {\n      int quadrant = 1;\n      if (p.x >= 0) {\n         if (p.y >= 0)\n            quadrant = 2;\n         else\n            quadrant = 4;\n      } else {\n         if (p.y >= 0)\n            quadrant = 1;\n         else\n            quadrant = 3;\n      }\n      ++bins[quadrant - 1];\n   }\n}",
            "// YOUR CODE HERE\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      }\n      else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point: points) {\n      if (point.x >= 0.0 && point.y >= 0.0)\n         ++bins[0];\n      else if (point.x < 0.0 && point.y >= 0.0)\n         ++bins[1];\n      else if (point.x < 0.0 && point.y < 0.0)\n         ++bins[2];\n      else if (point.x >= 0.0 && point.y < 0.0)\n         ++bins[3];\n   }\n}",
            "bins.fill(0);\n   for (auto const& point: points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0] += 1;\n      }\n      else if (p.x < 0 && p.y > 0) {\n         bins[1] += 1;\n      }\n      else if (p.x < 0 && p.y < 0) {\n         bins[2] += 1;\n      }\n      else if (p.x > 0 && p.y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (auto p : points) {\n      if (p.x >= 0.0 && p.y >= 0.0) {\n         bins[0]++;\n      } else if (p.x < 0.0 && p.y >= 0.0) {\n         bins[1]++;\n      } else if (p.x < 0.0 && p.y < 0.0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// write your solution here\n}",
            "// TODO: Implement this function\n    // The number of points in a quadrant is always either 0 or 1\n    // We can do this by using a map to keep track of the number of points in a quadrant.\n    // A map is a data structure that takes an object as a key, and keeps track of its count\n    // In this case, the key will be a `Point` object, and the value will be an integer value\n    // This way, we can check if we have seen a Point before\n    std::map<Point, int> point_map;\n    \n    for (auto const& point : points) {\n        // If we have seen the point before, increment the count, otherwise set the count to 1\n        // The count is initially set to 0 by default\n        point_map[point]++;\n    }\n    \n    // Loop through the map and store the values in the array\n    size_t i = 0;\n    for (auto& item : point_map) {\n        bins[i++] = item.second;\n    }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   for (auto const &p : points) {\n      size_t x_quadrant = (p.x > 0)? 1 : 0;\n      size_t y_quadrant = (p.y > 0)? 1 : 0;\n      bins[x_quadrant + 2 * y_quadrant]++;\n   }\n}",
            "for (Point const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      int index = 0;\n      if (point.x > 0) {\n         if (point.y > 0) {\n            index = 1;\n         } else {\n            index = 2;\n         }\n      } else {\n         if (point.y > 0) {\n            index = 3;\n         } else {\n            index = 4;\n         }\n      }\n      ++bins[index];\n   }\n}",
            "for (auto const& p : points) {\n      size_t quad = 0;\n      if (p.x > 0) {\n         if (p.y > 0) {\n            quad = 0;\n         } else {\n            quad = 1;\n         }\n      } else {\n         if (p.y > 0) {\n            quad = 2;\n         } else {\n            quad = 3;\n         }\n      }\n      bins[quad]++;\n   }\n}",
            "for (auto p : points) {\n      int ix = p.x > 0? 1 : (p.x < 0? 2 : 0);\n      int iy = p.y > 0? 1 : (p.y < 0? 2 : 0);\n      bins[4 * iy + ix]++;\n   }\n}",
            "// Write your solution here.\n   for (auto const& p : points)\n   {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "for (auto p : points) {\n      if (p.x < 0)\n         if (p.y < 0)\n            bins[1]++;\n         else\n            bins[0]++;\n      else if (p.y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "bins = {0, 0, 0, 0};\n   for (Point const &point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: your code here\n}",
            "// Your code here\n}",
            "// Your code goes here\n}",
            "bins.fill(0);\n\n   for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (Point const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "bins.fill(0);\n   for (const Point& point : points) {\n      size_t x = static_cast<size_t>(point.x + 10);\n      size_t y = static_cast<size_t>(point.y + 10);\n      bins[x + 2*y] += 1;\n   }\n}",
            "for (auto const & point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else if (p.x > 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const &point : points) {\n      if (point.x >= 0) {\n         if (point.y >= 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (point.y >= 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "for (auto const& p: points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n      double absx = std::abs(p.x);\n      double absy = std::abs(p.y);\n      if (absx <= absy) {\n         if (p.x > 0) {\n            if (p.y > 0) {\n               bins[0]++;\n            } else {\n               bins[1]++;\n            }\n         } else {\n            if (p.y > 0) {\n               bins[2]++;\n            } else {\n               bins[3]++;\n            }\n         }\n      } else {\n         if (p.y > 0) {\n            if (p.x > 0) {\n               bins[0]++;\n            } else {\n               bins[1]++;\n            }\n         } else {\n            if (p.x > 0) {\n               bins[2]++;\n            } else {\n               bins[3]++;\n            }\n         }\n      }\n   }\n}",
            "for (auto const& point: points) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (point.y > 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "bins.fill(0);\n  for (auto p : points) {\n    if (p.x > 0 && p.y > 0) {\n      bins[0] += 1;\n    } else if (p.x < 0 && p.y > 0) {\n      bins[1] += 1;\n    } else if (p.x < 0 && p.y < 0) {\n      bins[2] += 1;\n    } else if (p.x > 0 && p.y < 0) {\n      bins[3] += 1;\n    }\n  }\n}",
            "bins = { 0, 0, 0, 0 };\n   for (auto const& point : points) {\n      // Insert code here\n   }\n}",
            "for (Point const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n}",
            "//TODO\n}",
            "for (Point const& p : points) {\n      int x = p.x < 0? -1 : 1;\n      int y = p.y < 0? -1 : 1;\n      bins[x + 2 * y]++;\n   }\n}",
            "bins.fill(0);\n   for(Point const& p : points) {\n      if(p.x >= 0)\n         bins[0]++;\n      else\n         bins[1]++;\n      if(p.y >= 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (Point const& point: points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0] += 1;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1] += 1;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2] += 1;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "// TODO implement me...\n   bins = std::array<size_t, 4>();\n}",
            "for (Point p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      }\n      if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      }\n      if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      }\n      if (p.x > 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n        bins[0]++;\n      }\n      if (p.x <= 0 && p.y >= 0) {\n        bins[1]++;\n      }\n      if (p.x <= 0 && p.y <= 0) {\n        bins[2]++;\n      }\n      if (p.x >= 0 && p.y <= 0) {\n        bins[3]++;\n      }\n    }\n}",
            "// TODO: Implement me.\n}",
            "for (Point const& point: points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x >= 0.0 && p.y >= 0.0) {\n         bins[0]++;\n      } else if (p.x >= 0.0 && p.y < 0.0) {\n         bins[1]++;\n      } else if (p.x < 0.0 && p.y >= 0.0) {\n         bins[2]++;\n      } else if (p.x < 0.0 && p.y < 0.0) {\n         bins[3]++;\n      }\n   }\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "for (auto const &p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else if (p.x > 0 && p.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "bins = {0,0,0,0};\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& point : points) {\n        if (point.x > 0 && point.y > 0) {\n            ++bins[0];\n        } else if (point.x < 0 && point.y > 0) {\n            ++bins[1];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        } else if (point.x > 0 && point.y < 0) {\n            ++bins[3];\n        }\n    }\n}",
            "size_t const n = points.size();\n   // For each point\n   for (size_t i = 0; i < n; ++i) {\n      Point const& p = points[i];\n      // Check which quadrant it belongs to\n      int quadrant = 0;\n      if (p.x >= 0) {\n         if (p.y >= 0) quadrant = 1;\n         else quadrant = 2;\n      } else {\n         if (p.y >= 0) quadrant = 3;\n         else quadrant = 4;\n      }\n      // Increment count\n      ++bins[quadrant];\n   }\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "//TODO: Implement this method.\n   size_t x_min, x_max, y_min, y_max;\n   x_min = 0;\n   x_max = 0;\n   y_min = 0;\n   y_max = 0;\n\n   for(auto i = 0; i < points.size(); i++){\n     if(points[i].x < points[x_min].x){\n       x_min = i;\n     }\n     if(points[i].x > points[x_max].x){\n       x_max = i;\n     }\n     if(points[i].y < points[y_min].y){\n       y_min = i;\n     }\n     if(points[i].y > points[y_max].y){\n       y_max = i;\n     }\n   }\n\n   std::vector<Point> x_min_points;\n   std::vector<Point> x_max_points;\n   std::vector<Point> y_min_points;\n   std::vector<Point> y_max_points;\n\n   for(auto i = 0; i < points.size(); i++){\n     if(i == x_min){\n       x_min_points.push_back(points[x_min]);\n     }\n     if(i == x_max){\n       x_max_points.push_back(points[x_max]);\n     }\n     if(i == y_min){\n       y_min_points.push_back(points[y_min]);\n     }\n     if(i == y_max){\n       y_max_points.push_back(points[y_max]);\n     }\n   }\n\n   std::array<size_t, 4> x_min_points_size;\n   std::array<size_t, 4> x_max_points_size;\n   std::array<size_t, 4> y_min_points_size;\n   std::array<size_t, 4> y_max_points_size;\n\n   countQuadrants(x_min_points, x_min_points_size);\n   countQuadrants(x_max_points, x_max_points_size);\n   countQuadrants(y_min_points, y_min_points_size);\n   countQuadrants(y_max_points, y_max_points_size);\n\n   bins[0] = x_min_points_size[0] + x_max_points_size[0];\n   bins[1] = x_min_points_size[1] + x_max_points_size[1];\n   bins[2] = y_min_points_size[2] + y_max_points_size[2];\n   bins[3] = y_min_points_size[3] + y_max_points_size[3];\n}",
            "for (auto const& p : points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (p.y > 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n\n}",
            "for (auto const& p : points) {\n      if (p.x < 0 && p.y < 0) {\n         ++bins[0];\n      } else if (p.x < 0) {\n         ++bins[1];\n      } else if (p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "size_t const n = points.size();\n    for(size_t i = 0; i < n; i++) {\n        if(points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        } else if(points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        } else if(points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        } else if(points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO\n}",
            "// Fill the vector `bins` with the count of each quadrant\n   // The quadrants are defined by the quadrant boundaries. \n   //   quadrant 1 is bounded by points in the top left quadrant\n   //   quadrant 2 is bounded by points in the top right quadrant\n   //   quadrant 3 is bounded by points in the bottom right quadrant\n   //   quadrant 4 is bounded by points in the bottom left quadrant\n   //\n   // See https://www.geeksforgeeks.org/how-to-find-the-quadrant-of-a-point-in-a-plane/ for a visualization of the quadrants.\n   //\n   // You may use std::count_if to count the number of points in a quadrant.\n   //\n   // Do not change this function.\n}",
            "bins.fill(0);\n    for (auto const& point: points) {\n        if (point.x >= 0 && point.y >= 0) {\n            bins[0]++;\n        } else if (point.x < 0 && point.y >= 0) {\n            bins[1]++;\n        } else if (point.x < 0 && point.y < 0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "size_t x, y;\n\n   for (auto const& p : points) {\n      x = (p.x < 0)? 1 : 0;\n      y = (p.y < 0)? 1 : 0;\n      ++bins[x + 2 * y];\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      }\n      else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "// YOUR CODE HERE\n   bins[0] = std::count_if(points.cbegin(), points.cend(), [](const Point & p) {return p.x >= 0 && p.y >= 0;});\n   bins[1] = std::count_if(points.cbegin(), points.cend(), [](const Point & p) {return p.x >= 0 && p.y < 0;});\n   bins[2] = std::count_if(points.cbegin(), points.cend(), [](const Point & p) {return p.x < 0 && p.y >= 0;});\n   bins[3] = std::count_if(points.cbegin(), points.cend(), [](const Point & p) {return p.x < 0 && p.y < 0;});\n}",
            "// write your code here\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[0] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[1] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2] += 1;\n      }\n      else {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (auto p : points) {\n        int quad = getQuadrant(p);\n        bins[quad]++;\n    }\n}",
            "for (Point const& p : points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (p.y > 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "// Your code goes here!\n}",
            "for (Point const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      }\n      else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n      else {\n         throw std::invalid_argument(\"Invalid point\");\n      }\n   }\n}",
            "for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[2];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[3];\n      else\n         ++bins[0];\n   }\n}",
            "for (auto &point : points) {\n        if (point.x > 0 && point.y > 0) {\n            ++bins[0];\n        }\n        else if (point.x < 0 && point.y > 0) {\n            ++bins[1];\n        }\n        else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: implement me!\n}",
            "for (const auto& point : points) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (point.y > 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "// your code here\n}",
            "for(const auto &p : points) {\n      if(p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if(p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if(p.x < 0 && p.y < 0)\n         ++bins[2];\n      else if(p.x > 0 && p.y < 0)\n         ++bins[3];\n      else {\n         std::cerr << \"Warning: (\" << p.x << \", \" << p.y << \") is on the origin\" << std::endl;\n      }\n   }\n}",
            "// Fill in the body\n   for (auto point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0] += 1;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         bins[1] += 1;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         bins[2] += 1;\n      }\n      else if (point.x > 0 && point.y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (auto p : points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            bins[0]++;\n         }\n         else {\n            bins[1]++;\n         }\n      }\n      else {\n         if (p.y > 0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "for (auto const& point : points) {\n        size_t index = 0;\n        if (point.x > 0 && point.y > 0) {\n            index = 0;\n        } else if (point.x < 0 && point.y > 0) {\n            index = 1;\n        } else if (point.x < 0 && point.y < 0) {\n            index = 2;\n        } else {\n            index = 3;\n        }\n        bins[index] += 1;\n    }\n}",
            "// Your code here\n}",
            "bins.fill(0);\n   for (auto const& point : points) {\n      if (point.x >= 0 && point.y >= 0)\n         ++bins[0];\n      else if (point.x < 0 && point.y >= 0)\n         ++bins[1];\n      else if (point.x < 0 && point.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "for (auto point : points) {\n      auto x = point.x;\n      auto y = point.y;\n      if (x >= 0 && y >= 0) {\n         ++bins[0];\n      }\n      else if (x < 0 && y >= 0) {\n         ++bins[1];\n      }\n      else if (x < 0 && y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "/* Your solution goes here. */\n}",
            "bins.fill(0);\n   for (Point const& p : points) {\n      double x = p.x;\n      double y = p.y;\n      if (x > 0 && y > 0) {\n         bins[0]++;\n      }\n      else if (x < 0 && y > 0) {\n         bins[1]++;\n      }\n      else if (x < 0 && y < 0) {\n         bins[2]++;\n      }\n      else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const &point: points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0] += 1;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1] += 1;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "// your code goes here\n   bins = {};\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x > 0 && point.y < 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& p : points) {\n      auto q = getQuadrant(p);\n      ++bins[q];\n   }\n\n}",
            "for (auto const &point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else if (point.x > 0 && point.y < 0) {\n         ++bins[3];\n      } else {\n         throw std::runtime_error(\"Invalid point\");\n      }\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x > 0 && p.y < 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[3];\n      else {\n         assert(false);\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      if (point.x > 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "size_t const n = points.size();\n   bins = {0, 0, 0, 0};\n   for (size_t i = 0; i < n; ++i) {\n      Point const& p = points[i];\n      if (p.x > 0.0 && p.y > 0.0) {\n         ++bins[0];\n      } else if (p.x < 0.0 && p.y > 0.0) {\n         ++bins[1];\n      } else if (p.x < 0.0 && p.y < 0.0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (Point const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0] += 1;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1] += 1;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (Point p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "bins.fill(0);\n   for (Point const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "bins[0] = 0; // no points in this quadrant\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto p : points) {\n      if (p.x > 0 && p.y > 0)\n         bins[0]++;\n      else if (p.x < 0 && p.y > 0)\n         bins[1]++;\n      else if (p.x < 0 && p.y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n}",
            "// Implement this function.\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (Point p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0] += 1;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1] += 1;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "/* Your code goes here. */\n}",
            "/* your code here */\n}",
            "// Implement this\n}",
            "for (auto const& p : points) {\n      auto const ix = p.x >= 0? 1 : -1;\n      auto const iy = p.y >= 0? 1 : -1;\n      bins[ix + iy]++;\n   }\n}",
            "size_t idx = 0;\n   for (Point point : points) {\n      if (point.x >= 0.0 && point.y >= 0.0) {\n         bins[idx]++;\n      } else if (point.x < 0.0 && point.y >= 0.0) {\n         bins[idx + 1]++;\n      } else if (point.x < 0.0 && point.y < 0.0) {\n         bins[idx + 2]++;\n      } else {\n         bins[idx + 3]++;\n      }\n      idx += 4;\n   }\n}",
            "/* Your code here */\n\n}",
            "// your code here\n}",
            "for (auto p : points) {\n        if (p.x > 0) {\n            if (p.y > 0) {\n                bins[0]++;\n            }\n            else {\n                bins[1]++;\n            }\n        }\n        else {\n            if (p.y > 0) {\n                bins[2]++;\n            }\n            else {\n                bins[3]++;\n            }\n        }\n    }\n}",
            "for (auto const& point : points) {\n        size_t x = point.x > 0? 1 : point.x < 0? 2 : 0;\n        size_t y = point.y > 0? 1 : point.y < 0? 2 : 0;\n        ++bins[x + 2 * y];\n    }\n}",
            "bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x > 0) {\n         if (p.y > 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (p.y > 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[2];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[3];\n      else\n         ++bins[0];\n   }\n}",
            "bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (auto const &point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         bins[0] += 1;\n      } else if (point.x < 0 && point.y >= 0) {\n         bins[1] += 1;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2] += 1;\n      } else if (point.x >= 0 && point.y < 0) {\n         bins[3] += 1;\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0)\n         ++bins[0];\n      else if (point.x < 0 && point.y > 0)\n         ++bins[1];\n      else if (point.x < 0 && point.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x >= 0) {\n         if (point.y >= 0) {\n            ++bins[0];\n         } else {\n            ++bins[1];\n         }\n      } else {\n         if (point.y >= 0) {\n            ++bins[2];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "// TODO: implement this function\n    bins = std::array<size_t, 4>();\n    std::for_each(points.begin(), points.end(), [&bins](Point const& p) {\n        int qx = (p.x > 0)? 0 : 1;\n        int qy = (p.y > 0)? 0 : 1;\n        ++bins[4*qy + qx];\n    });\n}",
            "for (Point const& p : points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "// TODO: Implement this function\n   std::fill(bins.begin(), bins.end(), 0);\n   for(auto const& point : points) {\n      if(point.x > 0 && point.y > 0)\n         ++bins[0];\n      else if(point.x < 0 && point.y > 0)\n         ++bins[1];\n      else if(point.x < 0 && point.y < 0)\n         ++bins[2];\n      else if(point.x > 0 && point.y < 0)\n         ++bins[3];\n   }\n}",
            "for (auto const& point: points) {\n      // add your code here\n   }\n}",
            "bins.fill(0);\n   for (auto const &point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else if (point.x > 0 && point.y < 0) {\n         ++bins[3];\n      }\n   }\n}",
            "for(auto const& point : points) {\n      int xQuadrant = 0;\n      int yQuadrant = 0;\n      if(point.x >= 0) {\n         xQuadrant = 1;\n      }\n      if(point.y >= 0) {\n         yQuadrant = 1;\n      }\n      bins[xQuadrant * 2 + yQuadrant] += 1;\n   }\n}",
            "// TODO\n}",
            "for (Point const& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++bins[0];\n      }\n      else if (p.x < 0 && p.y > 0) {\n         ++bins[1];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      }\n      else {\n         ++bins[3];\n      }\n   }\n}",
            "for(size_t i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if(x > 0.0) {\n         if(y > 0.0) {\n            ++bins[0];\n         }\n         else {\n            ++bins[1];\n         }\n      }\n      else {\n         if(y > 0.0) {\n            ++bins[2];\n         }\n         else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "for (auto const &p : points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "for (auto const& p : points) {\n      if (p.x < 0 && p.y < 0) {\n         ++bins[0];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++bins[1];\n      } else if (p.x >= 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "for (auto const& p : points) {\n      auto x = p.x > 0? 1 : (p.x < 0? 3 : 2);\n      auto y = p.y > 0? 1 : (p.y < 0? 3 : 2);\n      bins[x - 1 + (y - 1) * 3]++;\n   }\n}",
            "bins.fill(0);\n   for (Point const& p : points) {\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (p.y >= 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "bins[1] = std::count_if(points.begin(), points.end(), [&] (Point const& p) {\n      return p.x > 0 && p.y > 0;\n   });\n   bins[2] = std::count_if(points.begin(), points.end(), [&] (Point const& p) {\n      return p.x < 0 && p.y > 0;\n   });\n   bins[3] = std::count_if(points.begin(), points.end(), [&] (Point const& p) {\n      return p.x < 0 && p.y < 0;\n   });\n   bins[4] = std::count_if(points.begin(), points.end(), [&] (Point const& p) {\n      return p.x > 0 && p.y < 0;\n   });\n}",
            "bins.fill(0);\n    for (Point const& p : points) {\n        if (p.x >= 0 && p.y >= 0) {\n            ++bins[0];\n        } else if (p.x < 0 && p.y >= 0) {\n            ++bins[1];\n        } else if (p.x < 0 && p.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "for (auto const& point : points) {\n      // Count point in each quadrant\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "// YOUR CODE HERE\n}",
            "/* Your code here */\n}",
            "for (Point const& p: points) {\n      if (p.x < 0) {\n         if (p.y > 0) {\n            ++bins[1];\n         } else {\n            ++bins[2];\n         }\n      } else {\n         if (p.y > 0) {\n            ++bins[0];\n         } else {\n            ++bins[3];\n         }\n      }\n   }\n}",
            "// Your code here...\n}",
            "// TODO: implement me\n}",
            "// Your code here\n}",
            "for (Point const& point: points) {\n      if (point.x > 0 && point.y > 0)\n         ++bins[0];\n      else if (point.x < 0 && point.y > 0)\n         ++bins[1];\n      else if (point.x < 0 && point.y < 0)\n         ++bins[2];\n      else if (point.x > 0 && point.y < 0)\n         ++bins[3];\n   }\n}",
            "// TODO: implement me\n}",
            "for (auto p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else if (p.x > 0 && p.y < 0)\n         ++bins[3];\n   }\n}",
            "// Your code here...\n}",
            "//TODO\n}",
            "for (auto p : points) {\n      double x = p.x, y = p.y;\n      if (x > 0 && y > 0) ++bins[0];\n      else if (x < 0 && y > 0) ++bins[1];\n      else if (x < 0 && y < 0) ++bins[2];\n      else if (x > 0 && y < 0) ++bins[3];\n   }\n}",
            "for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         bins[0]++;\n      } else if (point.x < 0 && point.y > 0) {\n         bins[1]++;\n      } else if (point.x < 0 && point.y < 0) {\n         bins[2]++;\n      } else if (point.x > 0 && point.y < 0) {\n         bins[3]++;\n      }\n   }\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "hip",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use AMD HIP to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// TODO\n   double x = points[blockIdx.x * blockDim.x + threadIdx.x].x;\n   double y = points[blockIdx.x * blockDim.x + threadIdx.x].y;\n   if(x > 0 && y > 0)\n       atomicAdd(&bins[0], 1);\n   if(x < 0 && y > 0)\n       atomicAdd(&bins[1], 1);\n   if(x < 0 && y < 0)\n       atomicAdd(&bins[2], 1);\n   if(x > 0 && y < 0)\n       atomicAdd(&bins[3], 1);\n}",
            "size_t i = hipThreadIdx_x;\n    for(size_t j = i; j < N; j += hipBlockDim_x) {\n        Point p = points[j];\n        // get quadrant\n        size_t q = 0;\n        if(p.x >= 0) {\n            if(p.y >= 0) {\n                q = 0;\n            } else {\n                q = 3;\n            }\n        } else {\n            if(p.y >= 0) {\n                q = 1;\n            } else {\n                q = 2;\n            }\n        }\n        atomicAdd(&bins[q], 1);\n    }\n}",
            "int tid = hipThreadIdx_x;\n    size_t total = N;\n\n    for (size_t j = tid; j < total; j += hipBlockDim_x) {\n        double x = points[j].x;\n        double y = points[j].y;\n\n        if (x >= 0 && y >= 0) {\n            bins[0]++;\n        }\n        else if (x < 0 && y >= 0) {\n            bins[1]++;\n        }\n        else if (x < 0 && y < 0) {\n            bins[2]++;\n        }\n        else {\n            bins[3]++;\n        }\n    }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N) return;\n\n   Point point = points[i];\n\n   if (point.x >= 0) {\n      if (point.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else {\n         atomicAdd(&bins[1], 1);\n      }\n   } else {\n      if (point.y >= 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x > 0 && p.y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x <= 0 && p.y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x <= 0 && p.y <= 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: Your code goes here\n}",
            "size_t thread = blockDim.x * blockIdx.x + threadIdx.x;\n   if (thread < N) {\n      double x = points[thread].x;\n      double y = points[thread].y;\n      if (x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "/* HIP: add a kernel grid dimension */\n   __shared__ size_t local_bins[4];\n   for (size_t i = hipThreadIdx_x; i < N; i += hipBlockDim_x) {\n      const Point &p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n         local_bins[0]++;\n      } else if (p.x >= 0 && p.y < 0) {\n         local_bins[1]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         local_bins[2]++;\n      } else {\n         local_bins[3]++;\n      }\n   }\n   /* HIP: broadcast the results */\n   for (int i = hipThreadIdx_x; i < 4; i += hipBlockDim_x) {\n      bins[i] += local_bins[i];\n   }\n}",
            "// Your code here\n}",
            "// your code here\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n    // each thread processes one point\n    if (tid < N) {\n        size_t bin = 0;\n        if (points[tid].x > 0 && points[tid].y > 0) {\n            bin = 1;\n        } else if (points[tid].x < 0 && points[tid].y > 0) {\n            bin = 2;\n        } else if (points[tid].x < 0 && points[tid].y < 0) {\n            bin = 3;\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "const size_t tid = threadIdx.x;\n   __shared__ size_t sbins[4];\n\n   // count the points in the corresponding quadrant\n   size_t count = 0;\n   for (size_t i = tid; i < N; i += blockDim.x) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         count += 1;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         count += 2;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         count += 3;\n      } else {\n         count += 4;\n      }\n   }\n   sbins[tid] = count;\n   __syncthreads();\n\n   // use one thread to sum up the counts\n   if (tid == 0) {\n      for (size_t i = 1; i < blockDim.x; i++) {\n         sbins[0] += sbins[i];\n      }\n      bins[0] = sbins[0];\n   }\n}",
            "int threadId = threadIdx.x;\n  int blockId = blockIdx.x;\n\n  size_t localBins[4] = {0};\n\n  int stride = gridDim.x * blockDim.x;\n  for (int i = threadId + blockId * stride; i < N; i += stride) {\n    Point p = points[i];\n    int quadrant = p.x > 0? (p.y > 0? 0 : 1) : (p.y > 0? 2 : 3);\n    atomicAdd(&localBins[quadrant], 1);\n  }\n\n  for (int i = threadId; i < 4; i += blockDim.x) {\n    atomicAdd(&bins[i], localBins[i]);\n  }\n}",
            "int id = hipThreadIdx_x;\n   int tid = hipBlockIdx_x * hipBlockDim_x + id;\n\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n\n      if (x >= 0 && y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else // x >= 0 && y < 0\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n   Point p = points[tid];\n   if (p.x >= 0) bins[0]++;\n   if (p.x < 0) bins[1]++;\n   if (p.y >= 0) bins[2]++;\n   if (p.y < 0) bins[3]++;\n}",
            "// TODO: implement the kernel\n    size_t tid = hipThreadIdx_x;\n    size_t block_size = hipBlockDim_x;\n\n    // Loop over all points\n    for (size_t i = tid; i < N; i+= block_size) {\n        // TODO: Compute quadrant\n        // bins[quadrant] =...\n    }\n}",
            "size_t i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i >= N) {\n      return;\n   }\n   double x = points[i].x, y = points[i].y;\n   if (x > 0) {\n      if (y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   } else {\n      if (y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else {\n         atomicAdd(&bins[2], 1);\n      }\n   }\n}",
            "const size_t tid = threadIdx.x;\n   size_t x0 = 0, x1 = 0, y0 = 0, y1 = 0;\n\n   // TODO: Implement the kernel.\n   //\n   // Compute the bin index to store in.\n   // Hint: x0 is the bin for points with x <= 0, x1 is the bin for points with x > 0, and so on.\n   // Hint: bins is a constant array so you can access it like a regular C array.\n   // Hint: tid is the ID of the thread you are currently processing.\n   // Hint: N is the number of points (so you don't need to store it).\n   // Hint: You can use the CUDA __syncthreads() macro.\n   //\n   // Remember to set all bins to zero at the start of the kernel.\n\n   // TODO: Verify that the kernel works correctly.\n   //\n   // Launch `kernelCountQuadrants` with at least 128 threads.\n   // Verify that the returned values are correct.\n}",
            "int binIdx;\n   for (int idx = blockIdx.x * blockDim.x + threadIdx.x; idx < N; idx += gridDim.x * blockDim.x) {\n      binIdx = (points[idx].x > 0 && points[idx].y > 0)? 0 :\n               (points[idx].x > 0 && points[idx].y < 0)? 1 :\n               (points[idx].x < 0 && points[idx].y > 0)? 2 : 3;\n      atomicAdd(&bins[binIdx], 1);\n   }\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (p.x < 0 && p.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = hipThreadIdx_x;\n   if (i < N) {\n      Point p = points[i];\n      int q = (p.x >= 0) + (p.y >= 0) * 2;\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      if (points[tid].x > 0 && points[tid].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[tid].x < 0 && points[tid].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[tid].x < 0 && points[tid].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: count the number of points in each quadrant\n  // Your code here\n}",
            "for (size_t i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        double x = points[i].x;\n        double y = points[i].y;\n        bool inFirstQuadrant = (x >= 0) && (y >= 0);\n        bool inSecondQuadrant = (x < 0) && (y >= 0);\n        bool inThirdQuadrant = (x < 0) && (y < 0);\n        bool inFourthQuadrant = (x >= 0) && (y < 0);\n        if (inFirstQuadrant) {\n            atomicAdd(bins + 0, 1);\n        }\n        if (inSecondQuadrant) {\n            atomicAdd(bins + 1, 1);\n        }\n        if (inThirdQuadrant) {\n            atomicAdd(bins + 2, 1);\n        }\n        if (inFourthQuadrant) {\n            atomicAdd(bins + 3, 1);\n        }\n    }\n}",
            "int bin;\n   for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      bin = (points[i].x >= 0)? (points[i].y >= 0) : (points[i].y < 0);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n\n   Point p = points[tid];\n   if (p.x < 0) {\n      if (p.y < 0) {\n         atomicAdd(&bins[0], 1);\n      } else {\n         atomicAdd(&bins[1], 1);\n      }\n   } else {\n      if (p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n  if (i < N) {\n    double x = points[i].x;\n    double y = points[i].y;\n    int quadrant = (x >= 0) + 2 * ((y >= 0) + 1);\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "// The thread index.\n   size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      // Compute the coordinates of the quadrant.\n      size_t qx = (points[tid].x >= 0)? 0 : 1;\n      size_t qy = (points[tid].y >= 0)? 0 : 1;\n      // The index of the bin.\n      size_t bin = qx * 2 + qy;\n      // Increment the bin counter.\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int bin = threadIdx.x;\n  for (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      atomicAdd(bins + bin, 1);\n    } else if (points[i].x < 0 && points[i].y < 0) {\n      atomicAdd(bins + bin + 2, 1);\n    } else if (points[i].x < 0 && points[i].y >= 0) {\n      atomicAdd(bins + bin + 1, 1);\n    } else {\n      atomicAdd(bins + bin + 3, 1);\n    }\n  }\n}",
            "size_t x = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   size_t y = hipThreadIdx_y + hipBlockIdx_y * hipBlockDim_y;\n   if (x < N && y < N) {\n      Point point = points[y*N+x];\n      if (point.x > 0 && point.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (point.x < 0 && point.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (point.x < 0 && point.y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "int i = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (i < N) {\n      int bin = 0;\n      if (points[i].x > 0 && points[i].y > 0) {\n         bin = 0;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bin = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bin = 2;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bin = 3;\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t tid = threadId;\n   size_t quad = 0;\n   while (tid < N) {\n      if (points[tid].x > 0.0 && points[tid].y > 0.0) {\n         quad = 0;\n      } else if (points[tid].x <= 0.0 && points[tid].y > 0.0) {\n         quad = 1;\n      } else if (points[tid].x < 0.0 && points[tid].y <= 0.0) {\n         quad = 2;\n      } else {\n         quad = 3;\n      }\n      atomicAdd(&bins[quad], 1);\n      tid += blockDim.x * gridDim.x;\n   }\n}",
            "}",
            "size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx < N) {\n      double x = points[idx].x;\n      double y = points[idx].y;\n      if (x < 0) {\n         if (y < 0)\n            atomicAdd(&bins[0], 1);\n         else\n            atomicAdd(&bins[1], 1);\n      } else {\n         if (y < 0)\n            atomicAdd(&bins[2], 1);\n         else\n            atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      if (x > 0) {\n         if (y > 0) {\n            atomicAdd(bins + 0, 1);\n         } else {\n            atomicAdd(bins + 1, 1);\n         }\n      } else {\n         if (y > 0) {\n            atomicAdd(bins + 2, 1);\n         } else {\n            atomicAdd(bins + 3, 1);\n         }\n      }\n   }\n}",
            "const size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    const size_t i = tid % 4;\n    __shared__ size_t s_bins[4];\n    if (tid < 4) {\n        s_bins[i] = 0;\n    }\n    __syncthreads();\n    if (tid < N) {\n        Point p = points[tid];\n        if (p.x > 0 && p.y > 0) {\n            s_bins[0] += 1;\n        }\n        else if (p.x < 0 && p.y > 0) {\n            s_bins[1] += 1;\n        }\n        else if (p.x < 0 && p.y < 0) {\n            s_bins[2] += 1;\n        }\n        else {\n            s_bins[3] += 1;\n        }\n    }\n    __syncthreads();\n    if (tid < 4) {\n        bins[i] = s_bins[i];\n    }\n}",
            "const int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int tid = blockIdx.x*blockDim.x+threadIdx.x;\n   if(tid < N) {\n      Point p = points[tid];\n      if(p.x > 0.0) {\n         if(p.y > 0.0) {\n            bins[0]++;\n         }\n         else {\n            bins[1]++;\n         }\n      }\n      else {\n         if(p.y > 0.0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "/* your code here */\n}",
            "// YOUR CODE HERE\n}",
            "// TODO: replace this with a real application\n   size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n   if (index < N) {\n      double x = points[index].x;\n      double y = points[index].y;\n      if (x >= 0) {\n         if (y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t tid = hipThreadIdx_x;\n\n    if (tid < N) {\n        Point p = points[tid];\n        int quadrant = (p.x >= 0)? ((p.y >= 0)? 1 : 4) : ((p.y >= 0)? 2 : 3);\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "// compute the thread id in a 1D grid\n   size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (threadId < N) {\n      int quadrant = (int) (points[threadId].x > 0) + 2 * (int) (points[threadId].y > 0);\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// Each thread takes care of counting one quadrant\n   size_t quadrant = threadIdx.x;\n\n   // The 4 quadrants have the following ranges of x values\n   double xMin = quadrant == 0? -1e15 : quadrant == 1? 1.5 : quadrant == 2? 5 : 1e15;\n   double xMax = quadrant == 0? 1.5 : quadrant == 1? 5 : quadrant == 2? 1e15 : 10;\n\n   // Count the number of points in each quadrant\n   size_t n = 0;\n   for (size_t i = 0; i < N; ++i) {\n      if (points[i].x >= xMin && points[i].x < xMax)\n         ++n;\n   }\n\n   // Store the result in the global memory\n   atomicAdd(&bins[quadrant], n);\n}",
            "for(size_t i = 0; i < N; i++) {\n      if(points[i].x > 0) {\n         if(points[i].y > 0)\n            atomicAdd(&bins[0], 1);\n         else\n            atomicAdd(&bins[3], 1);\n      } else {\n         if(points[i].y > 0)\n            atomicAdd(&bins[1], 1);\n         else\n            atomicAdd(&bins[2], 1);\n      }\n   }\n}",
            "size_t tid = threadIdx.x;\n   size_t bin = 0;\n\n   // YOUR CODE HERE\n\n   __syncthreads();\n\n   if (tid == 0) {\n      atomicAdd(&bins[bin], N);\n   }\n}",
            "}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      // Count in which quadrant the point is in\n      if (p.x > 0 && p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (p.x < 0 && p.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (p.x > 0 && p.y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// YOUR CODE HERE\n}",
            "__shared__ size_t buffer[4];\n   int x = hipThreadIdx_x;\n   int bin = (x < N)? ((points[x].x > 0.0)? ((points[x].y > 0.0)? 0 : 3) : ((points[x].y > 0.0)? 1 : 2)) : 4;\n   buffer[bin] += 1;\n   __syncthreads();\n\n   /* Each thread in a block is responsible for one bin */\n   if (x == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = buffer[i];\n      }\n   }\n}",
            "size_t tid = threadIdx.x;\n   size_t gridDim = blockDim.x;\n   size_t block = blockIdx.x;\n   size_t block_start = (block * N) / gridDim;\n   size_t block_end = ((block + 1) * N) / gridDim;\n   size_t local_N = block_end - block_start;\n   for (size_t i = tid; i < local_N; i += gridDim) {\n      Point p = points[i + block_start];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (p.y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "const size_t gid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n    const size_t tidx = hipThreadIdx_x;\n    __shared__ size_t sbins[4];\n\n    if (tidx < 4) {\n        sbins[tidx] = 0;\n    }\n    __syncthreads();\n\n    for (size_t i = gid; i < N; i += hipGridDim_x * hipBlockDim_x) {\n        const double x = points[i].x;\n        const double y = points[i].y;\n        sbins[x >= 0 && y >= 0] += 1;\n        sbins[x < 0 && y >= 0] += 1;\n        sbins[x >= 0 && y < 0] += 1;\n        sbins[x < 0 && y < 0] += 1;\n    }\n    __syncthreads();\n\n    if (tidx < 4) {\n        atomicAdd(&bins[tidx], sbins[tidx]);\n    }\n}",
            "int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (tid < N) {\n      double x = points[tid].x, y = points[tid].y;\n      int q = x >= 0? (y >= 0) : (y < 0);\n      atomicAdd(&bins[q], 1);\n   }\n}",
            "for(size_t i = blockDim.x*blockIdx.x+threadIdx.x; i < N; i += blockDim.x*gridDim.x) {\n      const Point& p = points[i];\n      if(p.x > 0 && p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if(p.x < 0 && p.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if(p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int tx = threadIdx.x, ty = threadIdx.y;\n    int bx = blockIdx.x, by = blockIdx.y;\n\n    int b = bx * blockDim.x + tx;\n    int c = by * blockDim.y + ty;\n\n    int index = b + c * blockDim.x * gridDim.x;\n\n    if (index >= N) return;\n\n    Point point = points[index];\n\n    int x = (int)point.x;\n    int y = (int)point.y;\n\n    if (x < 0) {\n        if (y < 0) bins[0]++;\n        else bins[1]++;\n    } else {\n        if (y < 0) bins[2]++;\n        else bins[3]++;\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t N_per_thread = N/hipBlockDim_x;\n\n   // compute the block's local quadrant\n   int bin = (tid < N_per_thread)? 1 : (tid < 2*N_per_thread)? 2 : (tid < 3*N_per_thread)? 3 : 4;\n   size_t i = tid % N_per_thread;\n\n   // count the number of points in the block's quadrant\n   for (int k = 0; k < 4; k++) {\n      int q = (bin + k) % 4;\n      if (q == 0) {\n         atomicAdd(&bins[q], ((points[i].x >= 0 && points[i].y >= 0) || (points[i].x < 0 && points[i].y < 0))? 1 : 0);\n      }\n      if (q == 1) {\n         atomicAdd(&bins[q], (points[i].x >= 0 && points[i].y < 0)? 1 : 0);\n      }\n      if (q == 2) {\n         atomicAdd(&bins[q], (points[i].x < 0 && points[i].y < 0)? 1 : 0);\n      }\n      if (q == 3) {\n         atomicAdd(&bins[q], (points[i].x < 0 && points[i].y >= 0)? 1 : 0);\n      }\n   }\n}",
            "size_t count = 0;\n   for (size_t i = 0; i < N; i++) {\n      size_t index = 0;\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            index = 0;\n         } else {\n            index = 3;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            index = 1;\n         } else {\n            index = 2;\n         }\n      }\n      count++;\n   }\n   __syncthreads();\n\n   // atomicAdd returns the original value, so we must use atomicInc\n   atomicInc(&bins[index], N);\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t gid = hipBlockIdx_x * hipBlockDim_x + tid;\n\n   if (gid < N) {\n      int quadrant = points[gid].x > 0 && points[gid].y > 0? 0 : points[gid].x < 0 && points[gid].y > 0? 1 : points[gid].x < 0 && points[gid].y < 0? 2 : 3;\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO: Fill in your implementation.\n   // HINT: For each thread, use `atomicAdd` to count points in each quadrant.\n}",
            "// TODO: count the number of points in each quadrant.\n}",
            "}",
            "size_t tid = hipThreadIdx_x;\n   __shared__ size_t sbins[4];\n   if (tid < 4) {\n      sbins[tid] = 0;\n   }\n   __syncthreads();\n   for (int i = tid; i < N; i += hipBlockDim_x) {\n      int k = 0;\n      if (points[i].x >= 0) k += 1;\n      if (points[i].y >= 0) k += 2;\n      atomicAdd(&sbins[k], 1);\n   }\n   __syncthreads();\n   if (tid < 4) {\n      atomicAdd(&bins[tid], sbins[tid]);\n   }\n}",
            "}",
            "// Each thread computes a partial result for a quadrant\n   const int tid = threadIdx.x;\n   size_t myCount = 0;\n\n   // HIP kernels are dispatched with a 1-dimensional grid and N threads per block.\n   // hipThreadIdx_x (and hipBlockIdx_x) are 0-based, so we need to add tid to get the actual index.\n   const int i = tid + hipBlockIdx_x * hipBlockDim_x;\n   if (i < N) {\n      const Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         ++myCount;\n      } else if (p.x < 0 && p.y > 0) {\n         ++bins[3];\n      } else if (p.x < 0 && p.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[1];\n      }\n   }\n\n   // Each thread writes partial result to shared memory\n   __shared__ size_t myBins[4];\n   if (tid == 0) {\n      myBins[0] = myCount;\n   }\n\n   __syncthreads();\n   // Each block now has an array of partial results\n   if (tid < 4) {\n      atomicAdd(&bins[tid], myBins[tid]);\n   }\n}",
            "// YOUR CODE HERE\n   __syncthreads();\n}",
            "// Each thread counts a single point.\n   unsigned i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N)\n      return;\n\n   // Compute the quadrant number for the point, from -1 to +1.\n   Point p = points[i];\n   int q = (p.x < 0) + 2 * (p.y < 0) - 1;\n\n   atomicAdd(&bins[q + 1], 1);\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (i < N) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n\n    size_t quadrant = 1;\n    if (id < N) {\n        if (points[id].x > 0 && points[id].y > 0) quadrant = 0;\n        else if (points[id].x < 0 && points[id].y > 0) quadrant = 1;\n        else if (points[id].x < 0 && points[id].y < 0) quadrant = 2;\n        else if (points[id].x > 0 && points[id].y < 0) quadrant = 3;\n        atomicAdd(&bins[quadrant], 1);\n    }\n}",
            "// YOUR CODE HERE\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t idx = hipBlockIdx_x * hipBlockDim_x + tid;\n\n   if (idx >= N) {\n      return;\n   }\n\n   Point p = points[idx];\n   bool inQ1 = p.x > 0 && p.y > 0;\n   bool inQ2 = p.x < 0 && p.y > 0;\n   bool inQ3 = p.x < 0 && p.y < 0;\n   bool inQ4 = p.x > 0 && p.y < 0;\n   atomicAdd(&bins[inQ1 * 2], 1);\n   atomicAdd(&bins[inQ2 * 2 + 1], 1);\n   atomicAdd(&bins[inQ3 * 2 + 1], 1);\n   atomicAdd(&bins[inQ4 * 2], 1);\n}",
            "int x = threadIdx.x;\n   int y = blockIdx.x;\n   int tid = x + y * blockDim.x;\n   int quad = (y < 2) * 2 + (x < 2);\n   for (; tid < N; tid += blockDim.x * gridDim.x)\n      if (points[tid].x > 0 && points[tid].y > 0)\n         atomicAdd(&bins[quad], 1);\n}",
            "int x = threadIdx.x + blockIdx.x * blockDim.x;\n  int y = threadIdx.y + blockIdx.y * blockDim.y;\n  int z = threadIdx.z + blockIdx.z * blockDim.z;\n  if (x >= 2 && y >= 2 && z >= 2) return;\n\n  bins[x * 2 + y * 2 * 2 + z * 2 * 2 * 2] = 0;\n  for (int i = x * 2 * N + y * 2 * N * 2 + z * 2 * N * 2 * 2; i < (x + 1) * 2 * N + (y + 1) * 2 * N * 2 + z * 2 * N * 2 * 2; i++) {\n    double pointX = points[i].x;\n    double pointY = points[i].y;\n    if (pointX > 0 && pointY > 0) {\n      bins[x * 2 + y * 2 * 2 + z * 2 * 2 * 2] = bins[x * 2 + y * 2 * 2 + z * 2 * 2 * 2] + 1;\n    }\n  }\n}",
            "int i = blockDim.x * blockIdx.x + threadIdx.x;\n   if (i < N) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "for (size_t i = 0; i < N; i++) {\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t threadId = blockIdx.x*blockDim.x + threadIdx.x;\n   if (threadId >= N)\n      return;\n\n   Point p = points[threadId];\n   size_t bin;\n   if (p.x > 0) {\n      if (p.y > 0) {\n         bin = 0;\n      } else {\n         bin = 1;\n      }\n   } else {\n      if (p.y > 0) {\n         bin = 2;\n      } else {\n         bin = 3;\n      }\n   }\n\n   atomicAdd(&bins[bin], 1);\n}",
            "unsigned int tid = threadIdx.x;\n   unsigned int bid = blockIdx.x;\n   unsigned int dim = blockDim.x;\n   unsigned int idx = bid*dim + tid;\n   Point p = points[idx];\n   bins[quadrant(p)]++;\n}",
            "__shared__ size_t smem[4];\n   const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      int quadrant = (points[tid].x < 0)? ((points[tid].y < 0)? 0 : 1) : ((points[tid].y < 0)? 2 : 3);\n      atomicAdd(&smem[quadrant], 1);\n   }\n   __syncthreads();\n\n   if (threadIdx.x == 0) {\n      bins[0] = smem[0];\n      bins[1] = smem[1];\n      bins[2] = smem[2];\n      bins[3] = smem[3];\n   }\n}",
            "__shared__ size_t partial[4];\n\n   size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) return;\n\n   Point p = points[idx];\n   size_t bin_x = p.x < 0? 1 : 0;\n   size_t bin_y = p.y < 0? 2 : 0;\n   bin_x |= p.x == 0? 4 : 0;\n   bin_y |= p.y == 0? 8 : 0;\n   atomicAdd(&partial[bin_x], 1);\n   atomicAdd(&partial[bin_y], 1);\n   __syncthreads();\n\n   atomicAdd(&bins[bin_x], partial[bin_x]);\n   atomicAdd(&bins[bin_y], partial[bin_y]);\n   __syncthreads();\n}",
            "// Compute the number of threads in the block\n   const size_t tid = threadIdx.x;\n   // Compute the number of blocks in the grid\n   const size_t numBlocks = gridDim.x;\n   // Compute the block index\n   const size_t blockId = blockIdx.x;\n   // Compute the number of threads in a block\n   const size_t blockDim = blockDim.x;\n\n   // Compute the number of cartesian points in this block\n   const size_t blockPoints = (N + numBlocks - 1) / numBlocks;\n   // Compute the offset of the first point in this block\n   const size_t blockOffset = blockId * blockPoints;\n   // Compute the limit of the last point in this block\n   const size_t blockLimit = min(N, blockOffset + blockPoints);\n\n   // Compute the number of points that this thread should count\n   const size_t localPoints = (blockLimit - blockOffset + blockDim - 1) / blockDim;\n   // Compute the offset of the first point in this thread's partition\n   const size_t localOffset = tid * localPoints;\n   // Compute the limit of the last point in this thread's partition\n   const size_t localLimit = min(blockLimit, localOffset + localPoints);\n\n   // Compute the number of points that this thread has counted\n   size_t localCount = 0;\n\n   // Count points in each quadrant\n   for (size_t i = localOffset; i < localLimit; i++) {\n      const Point &p = points[blockOffset + i];\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n      localCount++;\n   }\n\n   // Reduce the counts in parallel\n   for (size_t stride = blockDim / 2; stride > 0; stride /= 2) {\n      __syncthreads();\n      if (tid < stride) {\n         bins[tid] += bins[tid + stride];\n      }\n   }\n\n   // Store the result in the global array\n   if (tid == 0) {\n      bins[0] = localCount;\n   }\n}",
            "// TODO: count points in each quadrant\n}",
            "size_t tid = threadIdx.x;\n\n   for (size_t i = tid; i < N; i += blockDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      bool in_left = x <= 0;\n      bool in_right = x > 0;\n      bool in_top = y <= 0;\n      bool in_bottom = y > 0;\n\n      size_t index = 4 * (in_top? 1 : 0) + 2 * (in_right? 1 : 0) + (in_bottom? 1 : 0) + (in_left? 1 : 0) - 1;\n      atomicAdd(bins + index, 1);\n   }\n}",
            "size_t tid = threadIdx.x;\n   size_t q = tid % 4;\n   size_t i = tid / 4;\n   if (i < N) {\n      int64_t quadrant = int64_t(points[i].x) > 0? (int64_t(points[i].y) > 0? 0 : 1) :\n         (int64_t(points[i].y) > 0? 2 : 3);\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int tid = blockDim.x*blockIdx.x + threadIdx.x;\n\n   // count the number of points in each quadrant\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x >= 0 && p.y >= 0) {\n         bins[0]++;\n      } else if (p.x <= 0 && p.y >= 0) {\n         bins[1]++;\n      } else if (p.x <= 0 && p.y <= 0) {\n         bins[2]++;\n      } else if (p.x >= 0 && p.y <= 0) {\n         bins[3]++;\n      }\n   }\n}",
            "size_t tid = threadIdx.x;\n   size_t gid = blockIdx.x*blockDim.x+tid;\n   if (gid >= N)\n      return;\n   Point p = points[gid];\n   if (p.x > 0) {\n      if (p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else\n         atomicAdd(&bins[1], 1);\n   } else {\n      if (p.y > 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n  if (i < N) {\n    double x = points[i].x, y = points[i].y;\n    int q = (x < 0)? 1 : 0;\n    q |= (y < 0)? 2 : 0;\n    atomicAdd(bins + q, 1);\n  }\n}",
            "size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n   if (threadId >= N) return;\n\n   // get quadrant\n   int q = quadrant(points[threadId]);\n\n   atomicAdd(&bins[q], 1);\n}",
            "// YOUR CODE HERE\n   // For now, fill with trivial code that just adds all points to the first quadrant\n   // (which is what you should do if the points are already sorted)\n   // This way, you can verify your output\n   for(int i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if(x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if(x < 0 && y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if(x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Each thread sums the points in its quadrant into a bin.\n   // You must launch the kernel with at least N threads.\n   // You can assume that N >= BLOCK_SIZE * 4.\n   size_t i = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         bins[0] += 1;\n      } else if (x < 0 && y >= 0) {\n         bins[1] += 1;\n      } else if (x < 0 && y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "size_t id = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   size_t quadrant = 0;\n   if (id < N) {\n      quadrant = (points[id].x >= 0? 0 : 1) + (points[id].y >= 0? 0 : 2);\n   }\n   atomicAdd(&bins[quadrant], 1);\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  for (size_t i = tid; i < N; i += blockDim.x * gridDim.x) {\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x > 0 && y > 0) {\n      atomicAdd(&bins[0], 1);\n    } else if (x < 0 && y > 0) {\n      atomicAdd(&bins[1], 1);\n    } else if (x < 0 && y < 0) {\n      atomicAdd(&bins[2], 1);\n    } else if (x > 0 && y < 0) {\n      atomicAdd(&bins[3], 1);\n    }\n  }\n}",
            "size_t threadId = threadIdx.x + blockDim.x * blockIdx.x;\n\n  if (threadId >= N)\n    return;\n\n  double x = points[threadId].x;\n  double y = points[threadId].y;\n  int count = 0;\n\n  if (x > 0 && y > 0)\n    count++;\n\n  if (x < 0 && y > 0)\n    count++;\n\n  if (x < 0 && y < 0)\n    count++;\n\n  if (x > 0 && y < 0)\n    count++;\n\n  atomicAdd(&bins[count], 1);\n}",
            "size_t nthreads = blockDim.x * gridDim.x;\n   size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n\n   size_t offset = 0;\n   for (size_t i = tid; i < N; i += nthreads) {\n      size_t x = points[i].x > 0.0? 1 : 0;\n      size_t y = points[i].y > 0.0? 2 : 0;\n      bins[x + y]++;\n   }\n}",
            "// TODO\n}",
            "// TODO: Implement kernel\n}",
            "size_t idx = hipThreadIdx_x;\n   if (idx >= N)\n      return;\n\n   Point p = points[idx];\n   if (p.x > 0 && p.y > 0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0 && p.y > 0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0 && p.y < 0)\n      atomicAdd(&bins[2], 1);\n   else if (p.x > 0 && p.y < 0)\n      atomicAdd(&bins[3], 1);\n}",
            "// TODO: implement\n}",
            "// YOUR CODE HERE\n}",
            "const int x = threadIdx.x;\n\n   // initialize to 0\n   atomicAdd(&bins[0], 0);\n   atomicAdd(&bins[1], 0);\n   atomicAdd(&bins[2], 0);\n   atomicAdd(&bins[3], 0);\n\n   for (size_t i = x; i < N; i += blockDim.x) {\n      double pX = points[i].x;\n      double pY = points[i].y;\n      if (pX > 0 && pY > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (pX < 0 && pY > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (pX < 0 && pY < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// YOUR CODE GOES HERE\n}",
            "int tid = threadIdx.x;\n    int bin = 0;\n\n    // Count the points in each quadrant.\n    for (size_t i = 0; i < N; i += blockDim.x) {\n        Point p = points[i + tid];\n        if (p.x >= 0) {\n            if (p.y >= 0) {\n                bin = 0;\n            } else {\n                bin = 1;\n            }\n        } else {\n            if (p.y >= 0) {\n                bin = 2;\n            } else {\n                bin = 3;\n            }\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "//TODO: Implement this function using the following information:\n\t//  1. Each thread has a unique thread id (0 to N-1)\n\t//  2. The points are sorted by y in ascending order\n\t//  3. The kernel takes N input points and the bins as inputs\n\t//  4. The kernel must store the count of points in each quadrant in `bins`\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      if (x >= 0 && y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (x >= 0 && y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t stride = blockDim.x * gridDim.x;\n\n   for (; idx < N; idx += stride) {\n      Point p = points[idx];\n\n      // count points in quadrant (p.x >= 0) and (p.y >= 0)\n      if (p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (p.x >= 0 && p.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "__shared__ double shared_x[BLOCK_SIZE];\n  __shared__ double shared_y[BLOCK_SIZE];\n\n  int idx = threadIdx.x + BLOCK_SIZE * blockIdx.x;\n  int stride = BLOCK_SIZE * gridDim.x;\n\n  int count = 0;\n  while (idx < N) {\n    shared_x[threadIdx.x] = points[idx].x;\n    shared_y[threadIdx.x] = points[idx].y;\n    __syncthreads();\n\n    // Each thread counts the number of points with its quadrant.\n    for (int i = threadIdx.x; i < N; i += BLOCK_SIZE) {\n      double x = shared_x[i % BLOCK_SIZE];\n      double y = shared_y[i % BLOCK_SIZE];\n      if (x >= 0 && y >= 0) {\n        atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y >= 0) {\n        atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n        atomicAdd(&bins[2], 1);\n      } else {\n        atomicAdd(&bins[3], 1);\n      }\n    }\n    __syncthreads();\n    idx += stride;\n  }\n}",
            "// Your implementation here\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t quadrant = 0;\n   while (quadrant < 4 && tid < N) {\n      if (points[tid].x > 0 && points[tid].y > 0) {\n         quadrant = 0;\n      } else if (points[tid].x <= 0 && points[tid].y > 0) {\n         quadrant = 1;\n      } else if (points[tid].x <= 0 && points[tid].y <= 0) {\n         quadrant = 2;\n      } else {\n         quadrant = 3;\n      }\n      atomicAdd(&bins[quadrant], 1);\n      tid += blockDim.x * gridDim.x;\n   }\n}",
            "int tid = hipThreadIdx_x;\n   int blocks = hipGridDim_x;\n\n   // Calculate the quadrant for each point\n   // There are four quadrants: top left, top right, bottom left, bottom right\n   __shared__ int quadrant[N];\n   for (int i = tid; i < N; i += blocks) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         quadrant[i] = 0; // top left\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         quadrant[i] = 1; // top right\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         quadrant[i] = 2; // bottom left\n      } else {\n         quadrant[i] = 3; // bottom right\n      }\n   }\n\n   // Count the number of points in each quadrant\n   for (int i = tid; i < N; i += blocks) {\n      atomicAdd(&bins[quadrant[i]], 1);\n   }\n}",
            "// Your code here\n}",
            "int tid = threadIdx.x + blockDim.x*blockIdx.x;\n    if (tid < N) {\n        int bin = (points[tid].x > 0) + 2 * (points[tid].y > 0);\n        atomicAdd(&(bins[bin]), 1);\n    }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N)\n      return;\n\n   Point p = points[tid];\n   int quadrant = (p.x > 0.0)? 1 : 0;\n   quadrant |= (p.y > 0.0)? 2 : 0;\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "/* get the thread id */\n   size_t tid = hipThreadIdx_x;\n\n   /* get the number of quadrants */\n   size_t N_q = 4;\n\n   /* get the number of threads in the block */\n   size_t threads_per_block = hipBlockDim_x;\n\n   /* get the block id */\n   size_t bid = hipBlockIdx_x;\n\n   /* calculate the start and end indices of the block */\n   size_t start = bid * threads_per_block + tid;\n   size_t end = min(start + threads_per_block, N);\n\n   /* get the shared memory */\n   extern __shared__ size_t smem[];\n\n   /* count the number of points in each quadrant using atomicAdd.\n   smem[0] -> bin 0\n   smem[1] -> bin 1\n   smem[2] -> bin 2\n   smem[3] -> bin 3\n   */\n   for (size_t i = start; i < end; i++) {\n      int bin = (points[i].x >= 0) + ((points[i].y >= 0) << 1);\n      atomicAdd(&smem[bin], 1);\n   }\n\n   /* reduce the total counts in each quadrant */\n   for (size_t q = 0; q < N_q; q++) {\n      for (size_t i = threads_per_block / 2; i > 0; i >>= 1) {\n         size_t j = tid ^ i;\n         if (j < i)\n            smem[q] += smem[q + j];\n      }\n   }\n\n   /* store the counts */\n   if (tid < N_q)\n      bins[tid] = smem[tid];\n}",
            "size_t tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid < N) {\n      size_t quadrant = 0;\n      if (points[tid].x >= 0) {\n         if (points[tid].y >= 0)\n            quadrant = 0;\n         else\n            quadrant = 3;\n      } else {\n         if (points[tid].y >= 0)\n            quadrant = 1;\n         else\n            quadrant = 2;\n      }\n      atomicAdd(&(bins[quadrant]), 1);\n   }\n}",
            "// Fill in this function\n    int x = blockIdx.x*blockDim.x+threadIdx.x;\n    int y = blockIdx.y*blockDim.y+threadIdx.y;\n    int tid = threadIdx.x+blockDim.x*threadIdx.y;\n\n    size_t idx = x+y*gridDim.x;\n    if (idx<N)\n    {\n        double x = points[idx].x;\n        double y = points[idx].y;\n        if (x>0 && y>0)\n            atomicAdd(&(bins[0]),1);\n        else if (x<0 && y>0)\n            atomicAdd(&(bins[1]),1);\n        else if (x<0 && y<0)\n            atomicAdd(&(bins[2]),1);\n        else\n            atomicAdd(&(bins[3]),1);\n    }\n}",
            "const size_t idx = blockDim.x * blockIdx.x + threadIdx.x;\n   if (idx < N) {\n      Point p = points[idx];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (p.y >= 0) {\n            atomicAdd(&bins[1], 1);\n         } else {\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "int quadrant;\n   int tid = threadIdx.x + blockDim.x * blockIdx.x;\n   int nthreads = blockDim.x * gridDim.x;\n   Point p;\n\n   for (; tid < N; tid += nthreads) {\n      p = points[tid];\n      quadrant = 0;\n      if (p.x > 0 && p.y > 0)\n         quadrant = 0;\n      else if (p.x < 0 && p.y > 0)\n         quadrant = 1;\n      else if (p.x < 0 && p.y < 0)\n         quadrant = 2;\n      else if (p.x > 0 && p.y < 0)\n         quadrant = 3;\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "__shared__ size_t localBins[4];\n\n   size_t tid = threadIdx.x;\n   size_t i = blockIdx.x * blockDim.x + tid;\n\n   if (i < N) {\n      // Each thread counts a single point.\n      const Point p = points[i];\n      size_t bin = 0;\n      if (p.x > 0) bin |= 1;\n      if (p.y > 0) bin |= 2;\n      atomicAdd(&localBins[bin], 1);\n   }\n\n   // Aggregate results of all threads in the block\n   size_t bin = 0;\n   if (tid < 4) {\n      bin = atomicAdd(&bins[tid], localBins[tid]);\n   }\n}",
            "//TODO: fill in code here\n}",
            "// YOUR CODE HERE\n}",
            "unsigned tid = threadIdx.x + blockIdx.x * blockDim.x;\n   unsigned gridSize = gridDim.x * blockDim.x;\n   for (unsigned i = tid; i < N; i += gridSize) {\n      if (points[i].x > 0 && points[i].y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (points[i].x <= 0 && points[i].y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (points[i].x <= 0 && points[i].y <= 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      int bin = (y >= 0)? 0 : 2;\n      bin += (x >= 0)? 0 : 1;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO: Your implementation goes here\n}",
            "// each thread computes a bin\n   auto bin = threadIdx.x;\n   // each thread computes an element in the output vector\n   auto element = bin;\n   // each thread processes a pair of adjacent points\n   auto stride = gridDim.x * blockDim.x;\n   for (auto i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += stride) {\n      auto x = points[i].x;\n      auto y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         atomicAdd(&bins[0], 1);\n      }\n      else if (x < 0 && y >= 0) {\n         atomicAdd(&bins[1], 1);\n      }\n      else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      }\n      else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n   // - Replace this function by your kernel code\n   // - The number of threads in each block is provided in the `blockDim` variable\n}",
            "const int myID = hipThreadIdx_x;\n   const int nthreads = hipBlockDim_x;\n   const int nblocks = hipGridDim_x;\n   const size_t start = myID + nthreads * hipBlockIdx_x;\n   const size_t step = nthreads * nblocks;\n   int quadrant = 0;\n   if (start < N) {\n      int quadrant = 0;\n      if (points[start].x > 0.0) quadrant += 1;\n      if (points[start].y > 0.0) quadrant += 2;\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (id < N) {\n      if (points[id].x < 0 && points[id].y < 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[id].x < 0 && points[id].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[id].x > 0 && points[id].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t offset = N / 4;\n   size_t count;\n\n   count = 0;\n   for (size_t i = tid; i < N; i += hipBlockDim_x) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            count++;\n         } else {\n            count += 2;\n         }\n      } else {\n         if (points[i].y > 0) {\n            count += 3;\n         }\n      }\n   }\n\n   atomicAdd(&bins[0], count);\n\n   __syncthreads();\n\n   count = 0;\n   for (size_t i = tid; i < N; i += hipBlockDim_x) {\n      if (points[i].x > 0) {\n         if (points[i].y <= 0) {\n            count++;\n         }\n      } else {\n         if (points[i].y <= 0) {\n            count += 2;\n         } else {\n            count += 3;\n         }\n      }\n   }\n\n   atomicAdd(&bins[1], count);\n\n   __syncthreads();\n\n   count = 0;\n   for (size_t i = tid; i < N; i += hipBlockDim_x) {\n      if (points[i].x <= 0) {\n         if (points[i].y > 0) {\n            count++;\n         } else {\n            count += 2;\n         }\n      } else {\n         if (points[i].y > 0) {\n            count += 3;\n         }\n      }\n   }\n\n   atomicAdd(&bins[2], count);\n\n   __syncthreads();\n\n   count = 0;\n   for (size_t i = tid; i < N; i += hipBlockDim_x) {\n      if (points[i].x <= 0) {\n         if (points[i].y <= 0) {\n            count++;\n         }\n      } else {\n         if (points[i].y <= 0) {\n            count += 2;\n         } else {\n            count += 3;\n         }\n      }\n   }\n\n   atomicAdd(&bins[3], count);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  size_t gid = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t bid = blockIdx.z;\n\n  while (gid < N) {\n    if (tid == 0) {\n      if (points[gid].x > 0 && points[gid].y > 0) {\n        atomicAdd(&bins[bid], 1);\n      } else if (points[gid].x < 0 && points[gid].y > 0) {\n        atomicAdd(&bins[bid + 1], 1);\n      } else if (points[gid].x < 0 && points[gid].y < 0) {\n        atomicAdd(&bins[bid + 2], 1);\n      } else {\n        atomicAdd(&bins[bid + 3], 1);\n      }\n    }\n\n    gid += gridDim.y * blockDim.y;\n  }\n}",
            "unsigned tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if(tid < N) {\n      Point p = points[tid];\n      if(p.x >= 0 && p.y >= 0) {\n         atomicAdd(bins+0, 1);\n      } else if(p.x <= 0 && p.y >= 0) {\n         atomicAdd(bins+1, 1);\n      } else if(p.x <= 0 && p.y <= 0) {\n         atomicAdd(bins+2, 1);\n      } else {\n         atomicAdd(bins+3, 1);\n      }\n   }\n}",
            "int index = hipThreadIdx_x + hipBlockDim_x * hipBlockIdx_x;\n   if (index < N) {\n      if (points[index].x > 0 && points[index].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[index].x < 0 && points[index].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[index].x < 0 && points[index].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "unsigned int tid = hipThreadIdx_x;\n   unsigned int bid = hipBlockIdx_x;\n   unsigned int gid = bid * hipBlockDim_x + tid;\n   unsigned int blocksize = hipBlockDim_x * hipGridDim_x;\n   while (gid < N) {\n      double x = points[gid].x;\n      double y = points[gid].y;\n      if (x > 0 && y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n      gid += blocksize;\n   }\n}",
            "// TODO\n}",
            "size_t tx = threadIdx.x;\n  size_t bx = blockIdx.x;\n  size_t x = bx * blockDim.x + tx;\n\n  size_t counts[4] = {0, 0, 0, 0};\n  for (size_t i = x; i < N; i += blockDim.x * gridDim.x) {\n    int quadrant = (points[i].x > 0? 1 : 0) + (points[i].y > 0? 2 : 0);\n    counts[quadrant]++;\n  }\n\n  __shared__ size_t countArray[4];\n  if (tx == 0) {\n    countArray[0] = counts[0];\n    countArray[1] = counts[1];\n    countArray[2] = counts[2];\n    countArray[3] = counts[3];\n  }\n  __syncthreads();\n\n  bins[0] += countArray[0];\n  bins[1] += countArray[1];\n  bins[2] += countArray[2];\n  bins[3] += countArray[3];\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      double p = points[tid].x;\n      double q = points[tid].y;\n      if (p >= 0) {\n         if (q >= 0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (q >= 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t b;\n   if (n < N) {\n      double x = points[n].x;\n      double y = points[n].y;\n      if (x >= 0) {\n         if (y >= 0)\n            b = 0;\n         else\n            b = 3;\n      } else {\n         if (y >= 0)\n            b = 1;\n         else\n            b = 2;\n      }\n      atomicAdd(&bins[b], 1);\n   }\n}",
            "// Each block processes 2*N elements, which contains the 4 quadrants.\n   size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < 4) {\n      bins[i] = 0;\n      for (size_t j = 0; j < N; j++) {\n         size_t idx = i * N + j;\n         if ((i & 1) == 0) {\n            if (points[idx].y >= 0)\n               bins[i]++;\n         } else {\n            if (points[idx].x <= 0)\n               bins[i]++;\n         }\n      }\n   }\n}",
            "#define max(x, y) ((x) < (y)? (y) : (x))\n#define min(x, y) ((x) < (y)? (x) : (y))\n   int tid = threadIdx.x + blockDim.x * blockIdx.x;\n   if (tid < N) {\n      // Compute quadrant for each point.\n      double q = points[tid].x * points[tid].x + points[tid].y * points[tid].y;\n      q = sqrt(q);\n\n      // Compute the number of points in each quadrant.\n      atomicAdd(&bins[0], q < 1);\n      atomicAdd(&bins[1], q >= 1 && q < 2);\n      atomicAdd(&bins[2], q >= 2);\n      atomicAdd(&bins[3], q > 2);\n   }\n#undef max\n#undef min\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n   if (threadId >= N)\n      return;\n\n   // TODO: fill in this function\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      if (x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// AMD HIP provides thread IDs in the range 0..blockDim.x * gridDim.x - 1.\n   // This function is called from a kernel with blockDim=256.\n   // The first thread will get id=0, the second thread id=256, etc.\n   int tid = hipThreadIdx_x;\n   int gid = hipBlockIdx_x * blockDim.x + tid;\n   if (gid < N) {\n      double x = points[gid].x;\n      double y = points[gid].y;\n      if (x >= 0 && y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// FIXME: Implement the kernel\n}",
            "// TODO: Implement\n   size_t x = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t y = threadIdx.y + blockIdx.y * blockDim.y;\n\n   // TODO: Add parallel reduction\n   // Your code goes here\n}",
            "const size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n   if (threadId < N) {\n      double x = points[threadId].x;\n      double y = points[threadId].y;\n      size_t quadrant = (x > 0)? 1 : 0;\n      quadrant |= (y > 0)? 2 : 0;\n      atomicAdd(&(bins[quadrant]), 1);\n   }\n}",
            "size_t i = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Thread id is also the number of points in this quadrant\n   bins[threadIdx.x] = threadIdx.x < N? 1 : 0;\n}",
            "size_t quadrant = 0;\n   for (size_t i = 0; i < N; i++) {\n      if (points[i].x > 0 && points[i].y >= 0) {\n         quadrant = 0;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         quadrant = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         quadrant = 2;\n      } else {\n         quadrant = 3;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   size_t binIdx = 0;\n\n   // Your code goes here\n\n   if (binIdx == 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (binIdx == 1) {\n      atomicAdd(&bins[1], 1);\n   } else if (binIdx == 2) {\n      atomicAdd(&bins[2], 1);\n   } else if (binIdx == 3) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t thread_id = threadIdx.x + blockDim.x * blockIdx.x;\n   size_t count[4] = {0, 0, 0, 0};\n\n   if (thread_id < N) {\n      // Each thread computes the count for one point.\n      const Point p = points[thread_id];\n      int quadrant = (p.x > 0)? 1 : ((p.x < 0)? 2 : 0);\n      quadrant |= (p.y > 0)? 0 : ((p.y < 0)? 4 : 0);\n      atomicAdd(&count[quadrant], 1);\n   }\n   // Each thread adds its count to the final count.\n   for (int i = 0; i < 4; i++) {\n      atomicAdd(&bins[i], count[i]);\n   }\n}",
            "size_t tid = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   if (tid < N) {\n      if (points[tid].x > 0.0 && points[tid].y > 0.0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[tid].x < 0.0 && points[tid].y > 0.0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[tid].x < 0.0 && points[tid].y < 0.0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n\n    size_t q1 = 0;\n    size_t q2 = 0;\n    size_t q3 = 0;\n    size_t q4 = 0;\n\n    for (size_t i = tid; i < N; i += hipBlockDim_x * hipGridDim_x) {\n        if (points[i].x > 0 && points[i].y > 0)\n            q1++;\n        else if (points[i].x < 0 && points[i].y > 0)\n            q2++;\n        else if (points[i].x < 0 && points[i].y < 0)\n            q3++;\n        else if (points[i].x > 0 && points[i].y < 0)\n            q4++;\n    }\n\n    atomicAdd(&bins[0], q1);\n    atomicAdd(&bins[1], q2);\n    atomicAdd(&bins[2], q3);\n    atomicAdd(&bins[3], q4);\n}",
            "for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      Point p = points[i];\n      int bin = (p.x >= 0? 1 : 0) + (p.y >= 0? 2 : 0);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "const size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n   if (id < N) {\n      Point p = points[id];\n      if (p.x < 0.0) {\n         if (p.y < 0.0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else if (p.y < 0.0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N)\n      return;\n   Point p = points[tid];\n   if (p.x > 0) {\n      if (p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   } else {\n      if (p.y > 0)\n         atomicAdd(&bins[1], 1);\n      else\n         atomicAdd(&bins[2], 1);\n   }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid >= N) return;\n\n   Point p = points[tid];\n   if (p.x > 0 && p.y > 0) bins[0]++;\n   else if (p.x < 0 && p.y > 0) bins[1]++;\n   else if (p.x < 0 && p.y < 0) bins[2]++;\n   else if (p.x > 0 && p.y < 0) bins[3]++;\n   else return;\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t i = hipBlockIdx_x * hipBlockDim_x + tid;\n\n    // initialize with 0s\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    // count points\n    if (i < N) {\n        double p = points[i].x;\n        double q = points[i].y;\n        if (p > 0 && q > 0) {\n            bins[0]++;\n        }\n        else if (p < 0 && q > 0) {\n            bins[1]++;\n        }\n        else if (p < 0 && q < 0) {\n            bins[2]++;\n        }\n        else if (p > 0 && q < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "__shared__ size_t private_bins[4];\n   size_t t = threadIdx.x;\n   size_t block = blockIdx.x;\n\n   for (size_t i = t; i < N; i += blockDim.x) {\n      Point p = points[i];\n      int quadrant = (p.x > 0)? ((p.y > 0)? 0 : 1) : ((p.y > 0)? 2 : 3);\n      atomicAdd(&private_bins[quadrant], 1);\n   }\n\n   private_bins[t] = private_bins[t] + private_bins[t + blockDim.x];\n   private_bins[t + blockDim.x] = private_bins[t] - private_bins[t + blockDim.x];\n   private_bins[t] = private_bins[t] - private_bins[t + blockDim.x];\n\n   for (int s = 1; s < blockDim.x; s *= 2) {\n      size_t tmp = __shfl_xor(private_bins[t], s);\n      private_bins[t] += tmp;\n   }\n\n   if (t == 0) {\n      bins[0] = private_bins[0];\n      bins[1] = private_bins[1];\n      bins[2] = private_bins[2];\n      bins[3] = private_bins[3];\n   }\n}",
            "size_t quadrant = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t mybin = 0;\n   for (size_t i = quadrant; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         ++mybin;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         ++mybin;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         ++mybin;\n      } else {\n         ++mybin;\n      }\n   }\n   __syncthreads();\n   atomicAdd(&bins[quadrant], mybin);\n}",
            "const int idx = blockDim.x*blockIdx.x + threadIdx.x;\n\n   if (idx < N) {\n      double x = points[idx].x;\n      double y = points[idx].y;\n\n      if (x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "const int tid = hipThreadIdx_x;\n   const int total = hipBlockDim_x * hipGridDim_x;\n\n   // In parallel, each thread counts the number of points in each quadrant.\n   // Using atomic adds is safe.\n   for (size_t i = tid; i < N; i += total) {\n      if (points[i].x > 0 && points[i].y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (points[i].x < 0 && points[i].y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (points[i].x < 0 && points[i].y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (points[i].x > 0 && points[i].y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// TODO\n}",
            "size_t tidx = hipThreadIdx_x;\n   size_t gid = hipBlockIdx_x*hipBlockDim_x + tidx;\n   __shared__ Point point;\n\n   if (tidx < N) {\n      point = points[gid];\n   }\n\n   // Wait until all threads have loaded the point\n   __syncthreads();\n\n   if (gid < N) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (point.y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t i = blockIdx.x*blockDim.x + threadIdx.x;\n   if (i >= N)\n      return;\n\n   if (points[i].x >= 0) {\n      if (points[i].y >= 0)\n         atomicAdd(&bins[0], 1);\n      else\n         atomicAdd(&bins[1], 1);\n   } else {\n      if (points[i].y >= 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// Write your code here.\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x > 0) {\n         if (p.y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (p.y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "int threadId = blockDim.x * blockIdx.x + threadIdx.x;\n   int quadrantId = threadId / N;\n   int pointId = threadId % N;\n\n   int x = pointId < N? points[pointId].x : 0;\n   int y = pointId < N? points[pointId].y : 0;\n   if (x > 0 && y > 0) {\n      bins[quadrantId] += 1;\n   }\n}",
            "int tid = threadIdx.x;\n   int quadrant = tid % 4;\n   int bin = tid / 4;\n   __shared__ size_t localBins[4];\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      int q = quadrant(points[i]);\n      atomicAdd(&localBins[q], bin);\n   }\n   for (int i = tid; i < 4; i += blockDim.x * gridDim.x) {\n      atomicAdd(&bins[i], localBins[i]);\n   }\n}",
            "size_t index = blockIdx.x * blockDim.x + threadIdx.x;\n\n   size_t quadrant = 0;\n   if (index < N) {\n      double x = points[index].x;\n      double y = points[index].y;\n      quadrant = (x >= 0 && y >= 0)? 0 : (x < 0 && y >= 0)? 1 : (x >= 0 && y < 0)? 2 : 3;\n   }\n   atomicAdd(bins + quadrant, 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      atomicAdd(&bins[x < 0 && y < 0], 1);\n      atomicAdd(&bins[x >= 0 && y < 0], 1);\n      atomicAdd(&bins[x < 0 && y >= 0], 1);\n      atomicAdd(&bins[x >= 0 && y >= 0], 1);\n   }\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (idx < N) {\n\t\tint q = 0;\n\t\tif (points[idx].x > 0 && points[idx].y > 0) q = 1;\n\t\telse if (points[idx].x < 0 && points[idx].y > 0) q = 2;\n\t\telse if (points[idx].x < 0 && points[idx].y < 0) q = 3;\n\t\tatomicAdd(&bins[q], 1);\n\t}\n}",
            "size_t tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x > 0) {\n         if (p.y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (p.y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// TODO: implement me!\n}",
            "// HIP_DYNAMIC_SHARED(size_t, smem);\n   // size_t* bins = smem;\n   size_t x = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   size_t y = hipBlockIdx_y * hipBlockDim_y + hipThreadIdx_y;\n   if (x < N && y < N) {\n      Point p = points[x + y * N];\n      int quad = ((p.x > 0) << 1) + (p.y > 0);\n      atomicAdd(&(bins[quad]), 1);\n   }\n}",
            "int quadrant;\n  for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n    quadrant = (int)(points[i].x >= 0) * 2 + (int)(points[i].y >= 0);\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n   if (threadId < N) {\n      double x = points[threadId].x;\n      double y = points[threadId].y;\n      size_t quadrant = (x >= 0)? (y >= 0? 0 : 1) : (y >= 0? 2 : 3);\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "size_t threadIdx_x = hipThreadIdx_x;\n   size_t blockIdx_x = hipBlockIdx_x;\n\n   // TODO\n}",
            "// your code here\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    int i = tid;\n\n    if (i < N) {\n        Point p = points[i];\n\n        if (p.x > 0 && p.y > 0)\n            atomicAdd(&bins[0], 1);\n        else if (p.x < 0 && p.y > 0)\n            atomicAdd(&bins[1], 1);\n        else if (p.x < 0 && p.y < 0)\n            atomicAdd(&bins[2], 1);\n        else if (p.x > 0 && p.y < 0)\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   if (tid < N) {\n      const Point p = points[tid];\n      if (p.x > 0) {\n         if (p.y > 0)\n            atomicAdd(&bins[0], 1);\n         else\n            atomicAdd(&bins[1], 1);\n      } else {\n         if (p.y > 0)\n            atomicAdd(&bins[2], 1);\n         else\n            atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// Each block contains a group of threads. Each block calculates its own value of `i`.\n   // So each block will compute the value of `bins[i]`\n   int i = blockIdx.x;\n   int j = threadIdx.x;\n\n   // Each thread processes one of the points in the `points` array\n   for (int k = j; k < N; k += blockDim.x) {\n      Point p = points[k];\n      if (i == 0 && p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (i == 1 && p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (i == 2 && p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (i == 3 && p.x >= 0 && p.y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int my_id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (my_id >= N) {\n        return;\n    }\n    Point p = points[my_id];\n    if (p.x > 0 && p.y > 0) {\n        atomicAdd(&bins[0], 1);\n    } else if (p.x < 0 && p.y > 0) {\n        atomicAdd(&bins[1], 1);\n    } else if (p.x < 0 && p.y < 0) {\n        atomicAdd(&bins[2], 1);\n    } else {\n        atomicAdd(&bins[3], 1);\n    }\n}",
            "int x = blockIdx.x * blockDim.x + threadIdx.x;\n   if (x < N) {\n      if (points[x].x > 0 && points[x].y > 0) {\n         atomicAdd(bins + 0, 1);\n      } else if (points[x].x < 0 && points[x].y > 0) {\n         atomicAdd(bins + 1, 1);\n      } else if (points[x].x < 0 && points[x].y < 0) {\n         atomicAdd(bins + 2, 1);\n      } else {\n         atomicAdd(bins + 3, 1);\n      }\n   }\n}",
            "int tid = threadIdx.x + blockIdx.x * blockDim.x;\n  if (tid < N) {\n    int quadrant = points[tid].x > 0? 1 : -1;\n    if (points[tid].y > 0) quadrant += 2;\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "__shared__ Point pointsShared[N];\n   size_t tid = threadIdx.x;\n   size_t gid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (gid < N) {\n      pointsShared[tid] = points[gid];\n   }\n   __syncthreads();\n\n   // process 4 points in parallel\n   for (size_t i = 0; i < N; i += 4 * blockDim.x) {\n      size_t index = i + tid;\n      if (index < N) {\n         size_t quadrant;\n         if (pointsShared[index].x > 0) {\n            quadrant = (pointsShared[index].y > 0)? 0 : 3;\n         } else {\n            quadrant = (pointsShared[index].y > 0)? 1 : 2;\n         }\n         atomicAdd(bins + quadrant, 1);\n      }\n   }\n}",
            "size_t tid = hipBlockDim_x * hipBlockIdx_x + hipThreadIdx_x;\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      if (x > 0 && y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else if (x > 0 && y < 0) {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = hipThreadIdx_x;\n    size_t blk = hipBlockIdx_x;\n\n    size_t localBins[4] = {0, 0, 0, 0};\n\n    for (size_t i = tid + blk * hipBlockDim_x; i < N; i += hipGridDim_x * hipBlockDim_x) {\n        Point p = points[i];\n        if (p.x > 0 && p.y > 0) {\n            localBins[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            localBins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            localBins[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            localBins[3]++;\n        }\n    }\n\n    localBins[0] += localBins[2];\n    localBins[1] += localBins[3];\n\n    for (size_t i = 0; i < 4; i++) {\n        atomicAdd(&bins[i], localBins[i]);\n    }\n}",
            "size_t tid = hipThreadIdx_x;\n   size_t binIndex = (tid < 2)? 0 : (tid < 4)? 1 : (tid < 6)? 2 : 3;\n   size_t sum = 0;\n   for (size_t i = 0; i < N; i++) {\n      Point p = points[i];\n      sum += p.x * p.x + p.y * p.y;\n   }\n   atomicAdd(bins + binIndex, sum);\n}",
            "__shared__ size_t smem[4];\n\n   // Initialize shared memory to zero.\n   smem[0] = 0;\n   smem[1] = 0;\n   smem[2] = 0;\n   smem[3] = 0;\n\n   // Each block processes a different point.\n   unsigned int tid = threadIdx.x;\n   unsigned int gid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   for (unsigned int i = gid; i < N; i += blockDim.x * gridDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      if (x >= 0 && y >= 0) {\n         atomicAdd(&smem[0], 1);\n      }\n      if (x < 0 && y >= 0) {\n         atomicAdd(&smem[1], 1);\n      }\n      if (x < 0 && y < 0) {\n         atomicAdd(&smem[2], 1);\n      }\n      if (x >= 0 && y < 0) {\n         atomicAdd(&smem[3], 1);\n      }\n   }\n\n   // Copy results from shared memory to global memory.\n   // Each block writes the result for its own thread.\n   if (tid == 0) {\n      bins[0] = smem[0];\n      bins[1] = smem[1];\n      bins[2] = smem[2];\n      bins[3] = smem[3];\n   }\n}",
            "__shared__ size_t sharedBins[4];\n    int tid = threadIdx.x;\n\n    for (size_t i = blockIdx.x; i < N; i += gridDim.x) {\n        size_t quadrant = 0;\n        if (points[i].x > 0 && points[i].y > 0) quadrant = 0;\n        else if (points[i].x < 0 && points[i].y > 0) quadrant = 1;\n        else if (points[i].x < 0 && points[i].y < 0) quadrant = 2;\n        else quadrant = 3;\n        atomicAdd(&sharedBins[quadrant], 1);\n    }\n    if (tid == 0) {\n        bins[0] = sharedBins[0];\n        bins[1] = sharedBins[1];\n        bins[2] = sharedBins[2];\n        bins[3] = sharedBins[3];\n    }\n}",
            "int id = hipThreadIdx_x + hipBlockIdx_x * hipBlockDim_x;\n   int quad = id >= N? 3 : (int)points[id].x > 0? (int)points[id].y > 0? 0 : 1 : (int)points[id].y > 0? 2 : 3;\n   atomicAdd(&bins[quad], 1);\n}",
            "size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t bin = (i < N? (points[i].x > 0? (points[i].y > 0? 0 : 1) : (points[i].y > 0? 2 : 3)) : 4);\n   __atomic_fetch_add(&bins[bin], 1, __ATOMIC_SEQ_CST);\n}",
            "// YOUR CODE HERE\n}",
            "size_t tid = blockIdx.x*blockDim.x + threadIdx.x;\n    if (tid < N) {\n        double x = points[tid].x;\n        double y = points[tid].y;\n        if (x >= 0 && y >= 0)\n            atomicAdd(&bins[0], 1);\n        else if (x < 0 && y >= 0)\n            atomicAdd(&bins[1], 1);\n        else if (x < 0 && y < 0)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "unsigned int thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n\n  for (unsigned int i = thread_id; i < N; i += blockDim.x * gridDim.x) {\n    double x = points[i].x;\n    double y = points[i].y;\n    if (x >= 0.0 && y >= 0.0)\n      atomicAdd(&bins[0], 1);\n    else if (x < 0.0 && y >= 0.0)\n      atomicAdd(&bins[1], 1);\n    else if (x < 0.0 && y < 0.0)\n      atomicAdd(&bins[2], 1);\n    else\n      atomicAdd(&bins[3], 1);\n  }\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t bid = blockIdx.x;\n\n   size_t n = N / gridDim.x;\n   size_t offset = bid * n;\n   size_t limit = min(offset + n, N);\n\n   size_t quadrant = 0;\n   for(size_t i = offset + tid; i < limit; i += blockDim.x * gridDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if(x >= 0) {\n         if(y >= 0) {\n            quadrant = 0;\n         } else {\n            quadrant = 1;\n         }\n      } else {\n         if(y >= 0) {\n            quadrant = 2;\n         } else {\n            quadrant = 3;\n         }\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "unsigned int tid = hipThreadIdx_x;\n   unsigned int idx = hipBlockIdx_x * hipBlockDim_x + tid;\n\n   unsigned int n = N;\n   if (idx < n) {\n      // Use the coordinate system origin at the bottom left corner to create a 2D cartesian coordinate system\n      double x = points[idx].x;\n      double y = points[idx].y;\n      if (x >= 0 && y >= 0) {\n         bins[0]++;\n      } else if (x < 0 && y >= 0) {\n         bins[1]++;\n      } else if (x < 0 && y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n}",
            "int tid = hipBlockIdx_x * hipBlockDim_x + hipThreadIdx_x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x >= 0 && p.y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (p.x < 0 && p.y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// threadIdx.x is 0 to N-1\n    size_t i = threadIdx.x;\n    // threadIdx.y is 0 to 3\n    size_t j = threadIdx.y;\n    if (i < N) {\n      Point p = points[i];\n      if (p.x >= 0 && p.y >= 0) {\n        atomicAdd(&bins[j*2+0], 1);\n      } else if (p.x >= 0 && p.y < 0) {\n        atomicAdd(&bins[j*2+1], 1);\n      }\n    }\n}",
            "int thread_id = threadIdx.x;\n   int block_id = blockIdx.x;\n\n   // Each thread computes one element of the output vector.\n   // The kernel code is executed in parallel on the GPU with many threads.\n   // Here, we need to compute a single element for each output vector element.\n   // So we need one thread for each output vector element.\n   int element = block_id * blockDim.x + thread_id;\n\n   // TODO: compute the number of points in quadrants 0, 1, 2, 3.\n   // Store the number of points in the correct element of the output vector `bins`.\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi",
        "prompt": "#include <mpi.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of points. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "int rank, size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\n\t// Each process has its own copy of the points.\n\tstd::vector<Point> localPoints = points;\n\n\t// Divide the points between processes.\n\tint totalPoints = localPoints.size();\n\tint pointsPerRank = totalPoints / size;\n\tint remainder = totalPoints % size;\n\tint startIndex = 0;\n\tint endIndex = 0;\n\tint numPoints = 0;\n\n\tif (rank < remainder) {\n\t\tnumPoints = pointsPerRank + 1;\n\t\tstartIndex = rank * pointsPerRank + remainder;\n\t\tendIndex = startIndex + numPoints;\n\t}\n\telse {\n\t\tnumPoints = pointsPerRank;\n\t\tstartIndex = rank * pointsPerRank + remainder;\n\t\tendIndex = startIndex + numPoints;\n\t}\n\n\t// Remove points outside of the current rank's quadrant.\n\tlocalPoints.erase(localPoints.begin() + startIndex, localPoints.begin() + endIndex);\n\n\t// Count the number of points in each quadrant.\n\tbins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\tfor (size_t i = 0; i < localPoints.size(); i++) {\n\t\tPoint p = localPoints[i];\n\t\tif (p.x >= 0 && p.y >= 0) {\n\t\t\tbins[0] += 1;\n\t\t}\n\t\telse if (p.x < 0 && p.y >= 0) {\n\t\t\tbins[1] += 1;\n\t\t}\n\t\telse if (p.x < 0 && p.y < 0) {\n\t\t\tbins[2] += 1;\n\t\t}\n\t\telse if (p.x >= 0 && p.y < 0) {\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n\n\tint sumBins[4] = { 0, 0, 0, 0 };\n\tMPI_Reduce(bins.data(), sumBins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n\t// Reduce the number of quadrant points to rank 0.\n\tif (rank == 0) {\n\t\tbins = sumBins;\n\t}\n}",
            "MPI_Datatype PointType;\n    MPI_Type_contiguous(2, MPI_DOUBLE, &PointType);\n    MPI_Type_commit(&PointType);\n\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<size_t> local_bins;\n    if (rank == 0) {\n        local_bins = std::vector<size_t>(4, 0);\n    }\n\n    size_t local_num = points.size();\n    MPI_Scatter(&local_num, 1, MPI_UNSIGNED_LONG, &local_num, 1, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    std::vector<Point> local_points;\n    MPI_Scatter(points.data(), local_num, PointType, local_points.data(), local_num, PointType, 0, MPI_COMM_WORLD);\n\n    for (auto p : local_points) {\n        if (p.x >= 0 && p.y >= 0) {\n            local_bins[0]++;\n        } else if (p.x < 0 && p.y >= 0) {\n            local_bins[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            local_bins[2]++;\n        } else {\n            local_bins[3]++;\n        }\n    }\n\n    MPI_Gather(local_bins.data(), local_bins.size(), MPI_UNSIGNED_LONG, bins.data(), local_bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    MPI_Type_free(&PointType);\n}",
            "// number of points for each quadrant\n   std::array<int, 4> local_bins;\n   for (auto& i : local_bins)\n      i = 0;\n\n   for (auto& p : points) {\n      if (p.x > 0 && p.y > 0) {\n         ++local_bins[0];\n      }\n      else if (p.x < 0 && p.y > 0) {\n         ++local_bins[1];\n      }\n      else if (p.x < 0 && p.y < 0) {\n         ++local_bins[2];\n      }\n      else {\n         ++local_bins[3];\n      }\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// write code here\n}",
            "/* write your solution here */\n}",
            "// write your solution here\n\n    // for simplicity we assume that we have the correct number of processes\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if(rank == 0) {\n        bins[0] = 0;\n        bins[1] = 0;\n        bins[2] = 0;\n        bins[3] = 0;\n    }\n\n    int bin = 0;\n    double const x = points[0].x;\n    double const y = points[0].y;\n\n    for(size_t i = 0; i < points.size(); ++i) {\n        double const& x1 = points[i].x;\n        double const& y1 = points[i].y;\n\n        if((x >= 0 && x1 > x) || (x <= 0 && x1 < x)) {\n            if((y >= 0 && y1 > y) || (y <= 0 && y1 < y)) {\n                bin = 1;\n            } else {\n                bin = 2;\n            }\n        } else {\n            if((y >= 0 && y1 > y) || (y <= 0 && y1 < y)) {\n                bin = 3;\n            } else {\n                bin = 0;\n            }\n        }\n\n        MPI_Bcast(&bin, 1, MPI_INT, 0, MPI_COMM_WORLD);\n        ++bins[bin];\n    }\n}",
            "// TODO\n}",
            "// TODO\n   int procId, numProc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &procId);\n   MPI_Comm_size(MPI_COMM_WORLD, &numProc);\n\n   int* count = new int[numProc];\n   for(int i = 0; i < numProc; i++) {\n      count[i] = 0;\n   }\n\n   std::vector<std::vector<Point>> localPoints(numProc);\n   std::vector<int> pointsOnProc(numProc);\n   for(int i = 0; i < points.size(); i++) {\n      int proc = (int)floor(points[i].x);\n      localPoints[proc].push_back(points[i]);\n   }\n\n   for(int i = 0; i < numProc; i++) {\n      pointsOnProc[i] = localPoints[i].size();\n   }\n\n   MPI_Gather(&pointsOnProc, 1, MPI_INT, count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if(procId == 0) {\n      for(int i = 0; i < numProc; i++) {\n         if(i < 2) {\n            bins[i] = count[i];\n         } else if(i == 2) {\n            bins[0] += count[i];\n         } else {\n            bins[1] += count[i];\n         }\n      }\n   }\n\n   delete[] count;\n}",
            "// TODO: implement me\n}",
            "// Your code here\n}",
            "// TODO\n}",
            "int num_procs, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int dim[2], periods[2], coords[2], ndims = 2;\n   MPI_Cart_get(MPI_COMM_WORLD, ndims, dim, periods, coords);\n\n   int N = points.size();\n   int local_N = N / num_procs;\n\n   int left = 0, right = local_N;\n   if (coords[0] == 0) left += 1;\n   if (coords[0] == num_procs - 1) right -= 1;\n\n   auto local_points = points.begin() + left;\n   std::vector<Point> local_points_vec(local_points, local_points + right - left);\n\n   std::array<size_t, 4> local_bins{0, 0, 0, 0};\n   for (auto const& point : local_points_vec) {\n      if (point.x >= 0 && point.y >= 0) local_bins[0] += 1;\n      else if (point.x >= 0 && point.y < 0) local_bins[1] += 1;\n      else if (point.x < 0 && point.y >= 0) local_bins[2] += 1;\n      else local_bins[3] += 1;\n   }\n\n   std::array<int, 4> sums{0, 0, 0, 0};\n   MPI_Reduce(local_bins.data(), sums.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      bins[0] = sums[0];\n      bins[1] = sums[1];\n      bins[2] = sums[2];\n      bins[3] = sums[3];\n   }\n}",
            "const int MY_RANK = 0;\n    const int P_SIZE = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &P_SIZE);\n    MPI_Comm_rank(MPI_COMM_WORLD, &MY_RANK);\n    int n_points = points.size();\n    int p_n_points = 0;\n    MPI_Scatter(&n_points, 1, MPI_INT, &p_n_points, 1, MPI_INT, MY_RANK, MPI_COMM_WORLD);\n    int p_rank = 0;\n    int p_coord[2] = {0, 0};\n    MPI_Cart_coords(MPI_COMM_WORLD, MY_RANK, 2, p_coord);\n    std::vector<Point> p_points(p_n_points);\n    MPI_Scatter(&points[0], p_n_points * 2, MPI_DOUBLE, &p_points[0], p_n_points * 2, MPI_DOUBLE, MY_RANK, MPI_COMM_WORLD);\n\n    int n_in_quadrant = 0;\n    for (auto const &p : p_points) {\n        if (p.x > 0 && p.y > 0) {\n            n_in_quadrant++;\n        }\n    }\n\n    MPI_Gather(&n_in_quadrant, 1, MPI_INT, &bins[0], 1, MPI_INT, MY_RANK, MPI_COMM_WORLD);\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Finalize();\n}",
            "// TODO\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (rank == 0) {\n      int size;\n      MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n      std::vector<int> bins_local(bins.size(), 0);\n      for (auto &p : points) {\n         if (p.x > 0 && p.y > 0) {\n            ++bins_local[0];\n         }\n         else if (p.x < 0 && p.y > 0) {\n            ++bins_local[1];\n         }\n         else if (p.x < 0 && p.y < 0) {\n            ++bins_local[2];\n         }\n         else {\n            ++bins_local[3];\n         }\n      }\n\n      MPI_Gather(&bins_local[0], bins_local.size(), MPI_INT, &bins[0], bins_local.size(), MPI_INT, 0, MPI_COMM_WORLD);\n   }\n   else {\n      MPI_Gather(MPI_IN_PLACE, bins.size(), MPI_INT, &bins[0], bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n   }\n}",
            "// Your code goes here...\n   int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n   if (my_rank == 0) {\n      int n_points = points.size();\n      int n_local = n_points / MPI_COMM_SIZE;\n      int n_rest = n_points % MPI_COMM_SIZE;\n      std::vector<Point> local_points(n_local);\n      for (int i = 0; i < MPI_COMM_SIZE; i++) {\n         int n_recv;\n         if (i < n_rest) {\n            n_recv = n_local + 1;\n         } else {\n            n_recv = n_local;\n         }\n         std::vector<Point> recv(n_recv);\n         MPI_Recv(&recv[0], n_recv * sizeof(Point), MPI_BYTE, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j = 0; j < n_recv; j++) {\n            local_points[j] = recv[j];\n         }\n      }\n      bins[0] = 0;\n      bins[1] = 0;\n      bins[2] = 0;\n      bins[3] = 0;\n      for (int i = 0; i < n_local; i++) {\n         if (local_points[i].x >= 0 && local_points[i].y >= 0) {\n            bins[0] += 1;\n         } else if (local_points[i].x <= 0 && local_points[i].y >= 0) {\n            bins[1] += 1;\n         } else if (local_points[i].x <= 0 && local_points[i].y <= 0) {\n            bins[2] += 1;\n         } else if (local_points[i].x >= 0 && local_points[i].y <= 0) {\n            bins[3] += 1;\n         }\n      }\n   } else {\n      int n_points = points.size();\n      int n_local = n_points / MPI_COMM_SIZE;\n      int n_rest = n_points % MPI_COMM_SIZE;\n      int start = n_local * my_rank;\n      int end;\n      if (my_rank < n_rest) {\n         end = start + n_local + 1;\n      } else {\n         end = start + n_local;\n      }\n      std::vector<Point> local_points(n_local);\n      for (int i = 0; i < n_local; i++) {\n         local_points[i] = points[start + i];\n      }\n      MPI_Send(&local_points[0], n_local * sizeof(Point), MPI_BYTE, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// TODO: Implement me\n}",
            "// TODO: Implement this function.\n   std::cout << \"Not implemented yet\" << std::endl;\n}",
            "// TODO: implement\n}",
            "size_t local_bin[4] = {};\n   for (auto const& p : points) {\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            local_bin[0] += 1;\n         } else {\n            local_bin[1] += 1;\n         }\n      } else {\n         if (p.y >= 0) {\n            local_bin[2] += 1;\n         } else {\n            local_bin[3] += 1;\n         }\n      }\n   }\n   MPI_Allreduce(local_bin, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "/* FIXME: Implement this function. */\n}",
            "// TODO:\n   // Complete this function\n   int size, rank, err;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   MPI_Scatter(points.data(), points.size()/size, MPI_DOUBLE, bins.data(), points.size()/size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   std::vector<size_t> mybins(4);\n   int offset = points.size() / size * rank;\n   int count = points.size() / size;\n\n   for (int i = 0; i < count; ++i) {\n      for (int j = 0; j < 4; ++j) {\n         if (mybins[j] == 0) {\n            mybins[j] = 1;\n            if (points[i + offset].x <= 0 && points[i + offset].y <= 0) ++mybins[j];\n            if (points[i + offset].x <= 0 && points[i + offset].y >= 0) ++mybins[j];\n            if (points[i + offset].x >= 0 && points[i + offset].y >= 0) ++mybins[j];\n            if (points[i + offset].x >= 0 && points[i + offset].y <= 0) ++mybins[j];\n         }\n      }\n   }\n\n   MPI_Gather(mybins.data(), mybins.size(), MPI_INT, bins.data(), mybins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // TODO: implement this method\n  // Hint: You will need to communicate between all ranks, and thus it is\n  // a good idea to break the problem down into smaller sub-problems.\n}",
            "auto num_points = points.size();\n   std::array<int, 4> quadrants;\n   int ndims = 2;\n\n   int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   MPI_Dims_create(size, ndims, quadrants.data());\n\n   int start, end;\n   int num_procs = quadrants[0] * quadrants[1];\n   if (rank < num_procs) {\n      start = (rank * num_points) / num_procs;\n      end = ((rank + 1) * num_points) / num_procs;\n      for (size_t i = start; i < end; i++) {\n         auto const& p = points[i];\n         int row = quadrants[0] - 1 - (p.y - std::floor(p.y)) / 0.5;\n         int col = (p.x - std::floor(p.x)) / 0.5;\n         int bin_idx = row * quadrants[1] + col;\n         bins[bin_idx]++;\n      }\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   if (size!= 4) {\n       // error\n   }\n\n   int size_x = 0;\n   int size_y = 0;\n\n   if (rank == 0) {\n      size_x = points.size() / 2;\n   } else if (rank == 1) {\n      size_x = points.size() / 2;\n      size_y = points.size() / 2;\n   } else if (rank == 2) {\n      size_y = points.size() / 2;\n   } else if (rank == 3) {\n      size_x = points.size() / 2;\n      size_y = points.size() / 2;\n   }\n\n   if (rank == 0) {\n      bins[0] = 0;\n      bins[1] = 0;\n      bins[2] = 0;\n      bins[3] = 0;\n   }\n\n   double rank_x = 0;\n   double rank_y = 0;\n   double x_max = 0;\n   double y_max = 0;\n\n   double x_min = 0;\n   double y_min = 0;\n\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &rank_x, &rank_y);\n\n   if (rank_x == 0) {\n      x_min = points[0].x;\n   }\n\n   MPI_Bcast(&x_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   if (rank_y == 0) {\n      y_min = points[0].y;\n   }\n\n   MPI_Bcast(&y_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   if (rank_x == size_x - 1) {\n      x_max = points[points.size() - 1].x;\n   }\n\n   MPI_Bcast(&x_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   if (rank_y == size_y - 1) {\n      y_max = points[points.size() - 1].y;\n   }\n\n   MPI_Bcast(&y_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   int i;\n   for (i = 0; i < points.size(); i++) {\n      Point p = points[i];\n      if ((p.x >= x_min && p.x < x_max) && (p.y >= y_min && p.y < y_max)) {\n         if (rank_x == 0) {\n            bins[0]++;\n         } else if (rank_x == size_x - 1) {\n            bins[1]++;\n         } else if (rank_y == 0) {\n            bins[2]++;\n         } else if (rank_y == size_y - 1) {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "MPI_Datatype type;\n    MPI_Type_contiguous(sizeof(Point), MPI_CHAR, &type);\n    MPI_Type_commit(&type);\n\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int size;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    if(rank==0) {\n        std::vector<Point> send_buf(points.size()/size);\n        std::vector<Point> recv_buf(points.size()/size);\n\n        MPI_Scatter(points.data(), points.size()/size, type, send_buf.data(), send_buf.size(), type, 0, MPI_COMM_WORLD);\n\n        for(int i=0; i < size; i++){\n            for(Point p : send_buf){\n                if(p.x < 0 && p.y < 0)\n                    bins[0]++;\n                else if(p.x < 0 && p.y >= 0)\n                    bins[1]++;\n                else if(p.x >= 0 && p.y < 0)\n                    bins[2]++;\n                else\n                    bins[3]++;\n            }\n        }\n\n        MPI_Gather(bins.data(), bins.size(), MPI_INT, recv_buf.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n        bins = {0, 0, 0, 0};\n        for(int i=0; i < size; i++)\n            std::copy(recv_buf.data() + i*bins.size(), recv_buf.data() + (i+1)*bins.size(), bins.begin());\n    } else {\n        MPI_Scatter(points.data(), points.size()/size, type, nullptr, 0, type, 0, MPI_COMM_WORLD);\n        MPI_Gather(bins.data(), bins.size(), MPI_INT, nullptr, bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (rank == 0) {\n      bins.fill(0);\n   }\n\n   int num_quadrants = points.size() / 2;\n   int num_points_per_proc = num_quadrants / size;\n   int num_points_this_proc = 0;\n   if (rank == 0) {\n      num_points_this_proc = num_quadrants % size;\n   }\n\n   MPI_Scatter(points.data(), num_points_per_proc, MPI_DOUBLE, &bins, num_points_this_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Gather(&bins, num_points_this_proc, MPI_DOUBLE, bins.data(), num_points_this_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int comm_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n   int neighboring_ranks[4];\n   MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &neighboring_ranks[0], &neighboring_ranks[1]);\n   MPI_Cart_shift(MPI_COMM_WORLD, 1, 1, &neighboring_ranks[2], &neighboring_ranks[3]);\n\n   int rank_quadrant;\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &rank_quadrant, nullptr);\n   auto rank_quadrant_x = rank_quadrant / 2;\n   auto rank_quadrant_y = rank_quadrant % 2;\n\n   int n_ranks_in_quadrant[4];\n   MPI_Cart_dim_get(MPI_COMM_WORLD, n_ranks_in_quadrant);\n\n   int coords[2];\n   MPI_Cart_coords(MPI_COMM_WORLD, rank, 2, coords, nullptr);\n\n   MPI_Request requests[4];\n   std::vector<int> recvcounts(4);\n   std::vector<int> recvdispls(4);\n\n   for (int i = 0; i < 4; ++i) {\n      MPI_Irecv(&recvcounts[i], 1, MPI_INT, neighboring_ranks[i], 0, MPI_COMM_WORLD, &requests[i]);\n   }\n\n   for (size_t i = 0; i < points.size(); ++i) {\n      auto& point = points[i];\n      auto point_quadrant_x = point.x > 0? 1 : 0;\n      auto point_quadrant_y = point.y > 0? 1 : 0;\n\n      int target_rank = coords[0] + (point_quadrant_x - rank_quadrant_x) * n_ranks_in_quadrant[0] +\n                        (point_quadrant_y - rank_quadrant_y) * n_ranks_in_quadrant[0] * n_ranks_in_quadrant[1];\n      MPI_Isend(&i, 1, MPI_INT, target_rank, 0, MPI_COMM_WORLD, &requests[point_quadrant_x * 2 + point_quadrant_y]);\n   }\n\n   MPI_Waitall(4, requests, MPI_STATUSES_IGNORE);\n\n   for (int i = 0; i < 4; ++i) {\n      auto rank = neighboring_ranks[i];\n      auto rank_quadrant = i % 2;\n      recvdispls[i] = rank_quadrant * n_ranks_in_quadrant[rank_quadrant];\n   }\n\n   bins = recvcounts;\n   for (size_t i = 1; i < bins.size(); ++i) {\n      bins[i] += bins[i - 1];\n   }\n}",
            "//TODO\n}",
            "const auto &rank = MPI::COMM_WORLD.Get_rank();\n   const auto &size = MPI::COMM_WORLD.Get_size();\n\n   // TODO: implement\n}",
            "int rank, size;\n\n   // TODO: Call MPI_Comm_rank and MPI_Comm_size to get the rank and size of the MPI communicator.\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: compute the number of points in each quadrant.\n   // Use the number of points in each quadrant as the weight.\n   // To do this, iterate over points and for each point, find the quadrant that contains it.\n   // Each quadrant is a set of points that have the same x and y value, so use a hash table to find the number of points in each quadrant.\n   // Use the `find` method of hash tables to find the quadrant.\n   // For example, if the point is {x=1.5, y=1.1}, then find the quadrant {1.5, 1.1}, {1.5, 0}, {0, 1.1} and {0, 0}.\n   // This will assign the point to the quadrant {1.5, 1.1} and {0, 0}, but not the quadrant {1.5, 0} or {0, 1.1}.\n   // Each quadrant will have the same x and y values.\n   // The weight is the number of points in each quadrant, so use the `size` method of `std::unordered_map` to count the number of points in each quadrant.\n   // Store the count in `bins`.\n\n   // TODO: Use MPI to find the total number of points in each quadrant.\n   // The number of points in each quadrant is the same on every rank.\n   // The result is stored in `bins` on rank 0.\n   // Use MPI_Allreduce to find the total number of points in each quadrant.\n   // You can use MPI_SUM as the reduction operator.\n\n   // TODO: Print the number of points in each quadrant.\n   if (rank == 0) {\n      std::cout << \"Number of points in each quadrant: \" << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << \"\\n\";\n   }\n}",
            "//TODO: Add code here.\n\tint rank;\n\tint size;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &size);\n\tif (size!= 4) {\n\t\tthrow std::invalid_argument(\"Error: number of ranks must be equal to 4\");\n\t}\n\tif (rank == 0) {\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t}\n\t// split the points into 4 quadrants\n\tstd::vector<Point> quadrants[4];\n\tfor (size_t i = 0; i < points.size(); i++) {\n\t\tif (points[i].x > 0 && points[i].y > 0) {\n\t\t\tquadrants[0].push_back(points[i]);\n\t\t}\n\t\telse if (points[i].x > 0 && points[i].y < 0) {\n\t\t\tquadrants[1].push_back(points[i]);\n\t\t}\n\t\telse if (points[i].x < 0 && points[i].y > 0) {\n\t\t\tquadrants[2].push_back(points[i]);\n\t\t}\n\t\telse if (points[i].x < 0 && points[i].y < 0) {\n\t\t\tquadrants[3].push_back(points[i]);\n\t\t}\n\t}\n\t// send the points to the corresponding rank\n\tMPI_Bcast(&quadrants[0][0], quadrants[0].size() * 2, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\tMPI_Bcast(&quadrants[1][0], quadrants[1].size() * 2, MPI_DOUBLE, 1, MPI_COMM_WORLD);\n\tMPI_Bcast(&quadrants[2][0], quadrants[2].size() * 2, MPI_DOUBLE, 2, MPI_COMM_WORLD);\n\tMPI_Bcast(&quadrants[3][0], quadrants[3].size() * 2, MPI_DOUBLE, 3, MPI_COMM_WORLD);\n\t// count the points in each quadrant\n\tif (rank == 0) {\n\t\tbins[0] = quadrants[0].size();\n\t\tbins[1] = quadrants[1].size();\n\t\tbins[2] = quadrants[2].size();\n\t\tbins[3] = quadrants[3].size();\n\t}\n\telse if (rank == 1) {\n\t\tbins[1] = quadrants[0].size();\n\t\tbins[2] = quadrants[1].size();\n\t\tbins[3] = quadrants[2].size();\n\t}\n\telse if (rank == 2) {\n\t\tbins[2] = quadrants[0].size();\n\t\tbins[3] = quadrants[1].size();\n\t}\n\telse if (rank == 3) {\n\t\tbins[3] = quadrants[0].size();\n\t}\n}",
            "int rank, numProcesses;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\tsize_t numPoints = points.size();\n\tsize_t pointsPerRank = numPoints / numProcesses;\n\tstd::vector<Point> localPoints(pointsPerRank);\n\tstd::vector<size_t> localBins;\n\n\tfor (size_t i = 0; i < pointsPerRank; i++) {\n\t\tlocalPoints[i] = points[i + rank * pointsPerRank];\n\t}\n\n\tfor (size_t i = 0; i < numProcesses; i++) {\n\t\tsize_t numQuadrants = 0;\n\t\tfor (size_t j = 0; j < pointsPerRank; j++) {\n\t\t\tPoint p = localPoints[j];\n\t\t\tif (p.x > 0 && p.y > 0) {\n\t\t\t\tnumQuadrants++;\n\t\t\t}\n\t\t\telse if (p.x < 0 && p.y > 0) {\n\t\t\t\tnumQuadrants++;\n\t\t\t}\n\t\t\telse if (p.x < 0 && p.y < 0) {\n\t\t\t\tnumQuadrants++;\n\t\t\t}\n\t\t\telse {\n\t\t\t\tnumQuadrants++;\n\t\t\t}\n\t\t}\n\n\t\tlocalBins.push_back(numQuadrants);\n\t}\n\n\tMPI_Reduce(&localBins[0], &bins[0], 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Complete this function.\n\n  // Count in parallel\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int numProcesses;\n  MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n  int localSize = points.size();\n  int numPointsPerProcess = localSize / numProcesses;\n  int remainder = localSize % numProcesses;\n\n  std::vector<Point> processPoints = std::vector<Point>(numPointsPerProcess);\n  std::vector<int> counts = std::vector<int>(numProcesses);\n  int *pointCount;\n\n  MPI_Scatter(points.data(), numPointsPerProcess, MPI_DOUBLE, processPoints.data(), numPointsPerProcess, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  int totalCount = 0;\n  for (auto p : processPoints) {\n    if (p.x > 0 && p.y > 0) {\n      totalCount++;\n    } else if (p.x <= 0 && p.y > 0) {\n      totalCount++;\n    } else if (p.x <= 0 && p.y <= 0) {\n      totalCount++;\n    } else {\n      totalCount++;\n    }\n  }\n\n  MPI_Gather(&totalCount, 1, MPI_INT, counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n  std::array<size_t, 4> processBins = {0, 0, 0, 0};\n\n  for (int i = 0; i < numProcesses; i++) {\n    if (i == 0) {\n      if (rank == 0) {\n        for (int j = 0; j < remainder; j++) {\n          if (processPoints[j].x > 0 && processPoints[j].y > 0) {\n            processBins[0]++;\n          } else if (processPoints[j].x <= 0 && processPoints[j].y > 0) {\n            processBins[1]++;\n          } else if (processPoints[j].x <= 0 && processPoints[j].y <= 0) {\n            processBins[2]++;\n          } else {\n            processBins[3]++;\n          }\n        }\n      }\n    } else {\n      if (rank == i) {\n        for (int j = 0; j < numPointsPerProcess; j++) {\n          if (processPoints[j].x > 0 && processPoints[j].y > 0) {\n            processBins[0]++;\n          } else if (processPoints[j].x <= 0 && processPoints[j].y > 0) {\n            processBins[1]++;\n          } else if (processPoints[j].x <= 0 && processPoints[j].y <= 0) {\n            processBins[2]++;\n          } else {\n            processBins[3]++;\n          }\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    for (int i = 0; i < counts.size(); i++) {\n      bins[i] = processBins[i];\n    }\n  }\n\n  MPI_Gather(processBins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  MPI_Finalize();\n}",
            "MPI_Datatype point_type;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &point_type);\n   MPI_Type_commit(&point_type);\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   std::vector<Point> local_points = points;\n   int n = points.size();\n   MPI_Scatter(&n, 1, MPI_INT, &n, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   MPI_Scatter(local_points.data(), 2, point_type, local_points.data(), 2, point_type, 0, MPI_COMM_WORLD);\n\n   std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n\n   for (Point p : local_points) {\n      if (p.x < 0 && p.y < 0) {\n         local_bins[0] += 1;\n      } else if (p.x > 0 && p.y < 0) {\n         local_bins[1] += 1;\n      } else if (p.x > 0 && p.y > 0) {\n         local_bins[2] += 1;\n      } else {\n         local_bins[3] += 1;\n      }\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   MPI_Type_free(&point_type);\n}",
            "size_t npoints = points.size();\n\n    // count number of points in each quadrant\n    // your code here\n\n    // send counts to rank 0\n    // your code here\n\n    // receive counts from rank 0\n    // your code here\n\n    if (rank == 0) {\n        // your code here\n    }\n}",
            "// TODO: implement this\n   bins = std::array<size_t, 4>{0, 0, 0, 0};\n   // 1. send data\n   // 2. receive data\n\n   MPI_Barrier(MPI_COMM_WORLD);\n}",
            "// TODO\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    MPI_Datatype point_type;\n    MPI_Type_contiguous(2, MPI_DOUBLE, &point_type);\n    MPI_Type_commit(&point_type);\n\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    int processes_in_x, processes_in_y;\n    int process_rows, process_columns;\n    int process_rows_start, process_columns_start;\n    int rows_per_process, columns_per_process;\n    int process_rows_end, process_columns_end;\n\n    int total_processes_in_x, total_processes_in_y;\n\n    MPI_Dims_create(world_size, 2, &total_processes_in_x, &total_processes_in_y);\n\n    MPI_Cart_get(MPI_COMM_WORLD, 2, &processes_in_x, &processes_in_y, &process_rows, &process_columns, &process_rows_start, &process_columns_start);\n\n    rows_per_process = process_rows / total_processes_in_x;\n    columns_per_process = process_columns / total_processes_in_y;\n\n    process_rows_end = (process_rows_start + rows_per_process) - 1;\n    process_columns_end = (process_columns_start + columns_per_process) - 1;\n\n    int process_rank, process_coords[2];\n\n    MPI_Comm process_comm;\n\n    if (world_rank == 0)\n    {\n        process_coords[0] = 0;\n        process_coords[1] = 0;\n\n        MPI_Cart_sub(MPI_COMM_WORLD, process_coords, &process_comm);\n\n        if (world_size == total_processes_in_x * total_processes_in_y) {\n            bins[0] = points.size();\n        }\n    }\n    else\n    {\n        process_coords[0] = world_rank % total_processes_in_x;\n        process_coords[1] = world_rank / total_processes_in_x;\n\n        MPI_Cart_sub(MPI_COMM_WORLD, process_coords, &process_comm);\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    int number_of_points_in_process;\n\n    MPI_Comm_rank(process_comm, &process_rank);\n\n    MPI_Cart_coords(process_comm, process_rank, 2, process_coords);\n\n    int process_row = process_coords[0];\n    int process_column = process_coords[1];\n\n    if (process_row >= process_rows_start && process_row <= process_rows_end && process_column >= process_columns_start && process_column <= process_columns_end)\n    {\n        if (world_rank == 0)\n        {\n            number_of_points_in_process = 0;\n\n            int i = 0;\n            while (i < points.size())\n            {\n                if (points[i].x >= process_columns_start * columns_per_process && points[i].x <= process_columns_end * columns_per_process && points[i].y >= process_rows_start * rows_per_process && points[i].y <= process_rows_end * rows_per_process)\n                {\n                    number_of_points_in_process++;\n                    i++;\n                }\n                else\n                {\n                    i++;\n                }\n            }\n        }\n        else\n        {\n            number_of_points_in_process = 0;\n\n            int i = 0;\n            while (i < points.size())\n            {\n                if (points[i].x >= process_column * columns_per_process && points[i].x <= (process_column + 1) * columns_per_process && points[i].y >= process_row * rows_per_process && points[i].y <= (process_row + 1) * rows_per_process)\n                {\n                    number_of_points_in_process++;\n                    i++;\n                }\n                else\n                {\n                    i++;\n                }\n            }\n        }\n\n        MPI_Barrier(MPI_COMM_WORLD);\n\n        if (world_rank == 0)\n        {\n            bins[1] = number_of_points_in_process",
            "// Count of points in each quadrant\n   std::array<size_t, 4> localBins = {0};\n\n   // TODO: implement countQuadrants()\n}",
            "// TODO\n   // Hint: you can loop over the elements in the array using a for-range loop.\n}",
            "// Write your code here.\n   return;\n}",
            "int rank, ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int dims[2], periods[2] = {true, true};\n    double x, y, delta = 4.0;\n\n    MPI_Cart_get(MPI_COMM_WORLD, 2, dims, periods, &dims[0]);\n    MPI_Cart_shift(MPI_COMM_WORLD, 0, 1, &x, &y);\n\n    MPI_Bcast(&x, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&y, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&delta, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    bins[0] = 0;\n    bins[1] = 0;\n    bins[2] = 0;\n    bins[3] = 0;\n\n    for (auto &p : points) {\n        if (p.x >= x - delta && p.x < x) {\n            if (p.y >= y - delta && p.y < y) {\n                bins[0] += 1;\n            } else {\n                bins[1] += 1;\n            }\n        } else {\n            if (p.y >= y - delta && p.y < y) {\n                bins[2] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        }\n    }\n\n    MPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "MPI_Datatype PointType;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &PointType);\n   MPI_Type_commit(&PointType);\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // split data between ranks\n   std::vector<Point> localPoints;\n   if (rank == 0) {\n      localPoints = points;\n   }\n   std::vector<Point> sendPoints;\n   std::vector<Point> recvPoints;\n   MPI_Scatter(localPoints.data(), points.size() / size, PointType, recvPoints.data(), points.size() / size, PointType, 0, MPI_COMM_WORLD);\n\n   // count local points\n   std::array<size_t, 4> localBins = {0, 0, 0, 0};\n   for (auto const& point : recvPoints) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            localBins[0]++;\n         } else {\n            localBins[1]++;\n         }\n      } else {\n         if (point.y > 0) {\n            localBins[2]++;\n         } else {\n            localBins[3]++;\n         }\n      }\n   }\n\n   // combine counts\n   MPI_Gather(localBins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n   MPI_Type_free(&PointType);\n}",
            "int rank, num_ranks;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    std::vector<Point> local_points;\n\n    for (size_t i = rank; i < points.size(); i += num_ranks) {\n        local_points.push_back(points[i]);\n    }\n\n    int my_bins[4] = {0};\n\n    for (size_t i = 0; i < local_points.size(); i++) {\n        if (local_points[i].x > 0 && local_points[i].y > 0)\n            my_bins[0]++;\n        else if (local_points[i].x < 0 && local_points[i].y > 0)\n            my_bins[1]++;\n        else if (local_points[i].x < 0 && local_points[i].y < 0)\n            my_bins[2]++;\n        else\n            my_bins[3]++;\n    }\n\n    MPI_Reduce(my_bins, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "MPI_Comm comm;\n   MPI_Comm_dup(MPI_COMM_WORLD, &comm);\n   int size, rank;\n   MPI_Comm_size(comm, &size);\n   MPI_Comm_rank(comm, &rank);\n\n   std::vector<Point> local_points;\n   std::vector<size_t> counts(4);\n\n   if (rank == 0) {\n      local_points = points;\n   }\n\n   MPI_Scatter(points.data(), points.size(), MPI_DOUBLE, local_points.data(), points.size(), MPI_DOUBLE, 0, comm);\n\n   for (Point const& point : local_points) {\n      if (point.x > 0 && point.y > 0) {\n         ++counts[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++counts[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++counts[2];\n      } else {\n         ++counts[3];\n      }\n   }\n\n   MPI_Gather(counts.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, comm);\n\n   MPI_Comm_free(&comm);\n}",
            "// TODO: Implement this function.\n}",
            "// TODO\n}",
            "// TODO: implement this function\n}",
            "// TODO: Implement this function.\n\n  // 1) split the data to every processes\n  // 2) send the data to every processes\n  // 3) receive the data from every processes\n  // 4) aggregate the data of every processes\n  // 5) store the result in bins\n\n  // split the data to every processes\n  int size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  int n = points.size();\n  int n_per_process = n / size;\n  int rest = n % size;\n\n  std::vector<Point> points_per_process(n_per_process + (rank < rest? 1 : 0));\n  std::vector<Point> rest_points(rank < rest? n_per_process + 1 : 0);\n\n  if (rank < rest) {\n    std::vector<Point>::iterator start = points.begin() + rank * n_per_process;\n    std::vector<Point>::iterator end = points.begin() + (rank + 1) * n_per_process;\n    points_per_process.assign(start, end);\n    rest_points.assign(start, end);\n  } else {\n    std::vector<Point>::iterator start = points.begin() + rank * n_per_process;\n    std::vector<Point>::iterator end = points.end();\n    points_per_process.assign(start, end);\n  }\n\n  // send the data to every processes\n  MPI_Scatter(points_per_process.data(), n_per_process + (rank < rest? 1 : 0), MPI_DOUBLE,\n             bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n  // receive the data from every processes\n  int bins_per_process[4] = {0};\n  MPI_Gather(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins_per_process, 4, MPI_UNSIGNED_LONG_LONG, 0,\n            MPI_COMM_WORLD);\n\n  // aggregate the data of every processes\n  if (rank == 0) {\n    for (size_t i = 0; i < size; ++i) {\n      for (size_t j = 0; j < 4; ++j) {\n        bins[j] += bins_per_process[j];\n      }\n    }\n  }\n\n  // store the result in bins\n  MPI_Scatter(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins_per_process, 4, MPI_UNSIGNED_LONG_LONG, 0,\n              MPI_COMM_WORLD);\n\n  for (size_t i = 0; i < 4; ++i) {\n    bins[i] = bins_per_process[i];\n  }\n}",
            "/* TODO: replace the following code with your own implementation.\n   */\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: implement the countQuadrants function\n\n   MPI_Barrier(MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (size <= 1) {\n      bins = { points.size(), 0, 0, 0 };\n      return;\n   }\n\n   int length = points.size();\n   int chunk = length / size;\n   int start = rank * chunk;\n   int end = start + chunk;\n   if (rank == size - 1) {\n      end = length;\n   }\n\n   int quadrants[4] = {0};\n\n   for (auto point = start; point < end; ++point) {\n      if (point < length) {\n         if (point < length && points[point].x >= 0) {\n            ++quadrants[0];\n         }\n\n         if (point < length && points[point].x < 0) {\n            ++quadrants[1];\n         }\n\n         if (point < length && points[point].y >= 0) {\n            ++quadrants[2];\n         }\n\n         if (point < length && points[point].y < 0) {\n            ++quadrants[3];\n         }\n      }\n   }\n\n   int* quadrants_recv = new int[4];\n   MPI_Gather(quadrants, 4, MPI_INT, quadrants_recv, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      bins = { 0, 0, 0, 0 };\n      for (int i = 0; i < 4; i++) {\n         bins[i] = quadrants_recv[i];\n      }\n   }\n\n   delete[] quadrants_recv;\n}",
            "// TODO: Your code here\n}",
            "// TODO\n    int world_size, world_rank;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n    int local_size = points.size();\n    if (local_size == 0) {\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] = 0;\n        }\n        return;\n    }\n\n    size_t quarter_size = local_size / 4;\n    std::vector<Point> local_points;\n    local_points.insert(local_points.end(), points.begin(), points.begin() + quarter_size);\n    local_points.insert(local_points.end(), points.begin() + quarter_size * 2, points.begin() + quarter_size * 3);\n    local_points.insert(local_points.end(), points.begin() + quarter_size * 3, points.begin() + quarter_size * 4);\n\n    std::array<size_t, 4> local_bins;\n    std::array<size_t, 4> local_max_bins;\n    if (world_rank == 0) {\n        for (size_t i = 0; i < 4; ++i) {\n            local_max_bins[i] = 0;\n        }\n    }\n    MPI_Reduce(local_points.data(), local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    MPI_Reduce(local_max_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_MAX, 0, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    if (world_rank == 0) {\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] -= local_max_bins[i];\n        }\n    }\n}",
            "// Your code here\n   int worldSize, worldRank;\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &worldRank);\n\n   double range = 10;\n   int binsX = (int)range / worldSize;\n\n   for(int i = 0; i < worldSize; i++) {\n      bins[i] = 0;\n   }\n\n   for(int i = 0; i < points.size(); i++) {\n      if(worldRank == 0) {\n         if(points[i].x >= binsX && points[i].y >= binsX) {\n            bins[0]++;\n         }\n         else if(points[i].x <= binsX && points[i].y >= binsX) {\n            bins[1]++;\n         }\n         else if(points[i].x <= binsX && points[i].y <= binsX) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n   }\n\n   MPI_Reduce(MPI_IN_PLACE, &bins, 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "std::vector<size_t> counts(4, 0);\n\n  // 1. YOUR CODE HERE\n\n  // 2. YOUR CODE HERE\n\n  // 3. YOUR CODE HERE\n}",
            "auto const numProcesses = MPI_Comm_size(MPI_COMM_WORLD);\n   auto const rank = MPI_Comm_rank(MPI_COMM_WORLD);\n   auto const numPoints = points.size();\n\n   // Count the number of points in each quadrant\n   // Create a local copy of the points for this process, so we can perform operations on the entire list\n   std::vector<Point> localPoints(points);\n   bins = std::array<size_t, 4> {0, 0, 0, 0};\n\n   for (size_t i = 0; i < numPoints; ++i) {\n      Point const& p = localPoints[i];\n      // 0 quadrant is [x < 0], 1 is [x >= 0] and 2 is [y < 0], 3 is [y >= 0]\n      if (p.x < 0.0) {\n         if (p.y < 0.0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (p.y < 0.0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n\n   // Reduce bins on rank 0\n   int sendBuf[4] = {bins[0], bins[1], bins[2], bins[3]};\n   int recvBuf[4] = {0, 0, 0, 0};\n   MPI_Reduce(sendBuf, recvBuf, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      bins = std::array<size_t, 4> {recvBuf[0], recvBuf[1], recvBuf[2], recvBuf[3]};\n   }\n}",
            "// TODO\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO: implement\n}",
            "std::array<int, 4> numBins;\n    std::array<int, 4> recv;\n    MPI_Cart_create(MPI_COMM_WORLD, 2, {2, 2}, {0, 0}, 0, &MPI_COMM_2D);\n\n    MPI_Cart_shift(MPI_COMM_2D, 0, 1, &recv[0], &recv[1]);\n    MPI_Cart_shift(MPI_COMM_2D, 1, 1, &recv[2], &recv[3]);\n\n    MPI_Cart_coords(MPI_COMM_2D, 0, 2, numBins.data());\n    MPI_Cart_coords(MPI_COMM_2D, 1, 2, numBins.data() + 2);\n\n    MPI_Cart_map(MPI_COMM_2D, 2, {numBins[0], numBins[1]}, &recv[0], &recv[1], &recv[2], &recv[3]);\n    MPI_Comm_rank(MPI_COMM_WORLD, &recv[4]);\n\n    for (int i = 0; i < 4; i++) {\n        std::vector<Point> sub;\n        for (auto const& p : points) {\n            if ((p.x > 0 and p.x <= 1) and (p.y > 0 and p.y <= 1)) {\n                if (p.x > 0.5 and p.y > 0.5) {\n                    sub.emplace_back(p);\n                }\n            }\n        }\n\n        std::vector<size_t> counts(recv[i + 1] - recv[i]);\n        MPI_Gather(&sub.size(), 1, MPI_INT, counts.data(), 1, MPI_INT, recv[i], MPI_COMM_2D);\n\n        if (recv[4] == 0) {\n            for (int j = 0; j < counts.size(); j++) {\n                bins[j] = counts[j];\n            }\n        }\n    }\n}",
            "// TODO\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (size!= 4) {\n      std::cout << \"ERROR: size of MPI communicator should be 4\" << std::endl;\n      exit(EXIT_FAILURE);\n   }\n\n   if (rank == 0) {\n      std::array<std::vector<Point>, 4> quadrants;\n\n      for (auto const& p : points) {\n         if (p.x > 0 && p.y > 0) {\n            quadrants[0].push_back(p);\n         } else if (p.x > 0 && p.y < 0) {\n            quadrants[1].push_back(p);\n         } else if (p.x < 0 && p.y < 0) {\n            quadrants[2].push_back(p);\n         } else {\n            quadrants[3].push_back(p);\n         }\n      }\n\n      std::array<int, 4> counts;\n      MPI_Gather(&quadrants[0].size(), 1, MPI_INT, counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n      for (size_t i = 0; i < counts.size(); ++i) {\n         bins[i] = counts[i];\n      }\n\n   } else {\n      std::vector<Point> my_points;\n      MPI_Gather(&points[0], points.size(), MPI_DOUBLE, my_points.data(), points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n      std::array<size_t, 4> counts = {my_points.size(), 0, 0, 0};\n\n      for (auto const& p : my_points) {\n         if (p.x > 0 && p.y > 0) {\n            ++counts[0];\n         } else if (p.x > 0 && p.y < 0) {\n            ++counts[1];\n         } else if (p.x < 0 && p.y < 0) {\n            ++counts[2];\n         } else {\n            ++counts[3];\n         }\n      }\n\n      MPI_Gather(counts.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n   }\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins.size());\n   bins.fill(0);\n   for (auto p : points) {\n      int rank = 0;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      if (p.x > 0) {\n         if (p.y > 0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (p.y > 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "}",
            "}",
            "//TODO: Implement this\n}",
            "std::vector<size_t> tempBins(4);\n    tempBins[0] = 0;\n    tempBins[1] = 0;\n    tempBins[2] = 0;\n    tempBins[3] = 0;\n\n    /* Your code goes here */\n\n}",
            "size_t n = points.size();\n   // TODO 4.3: implement countQuadrants here\n   // Hint: The bins are implemented as 4-element arrays.\n   // The bins are initialized to 0. You can use a for-loop to initialize them.\n\n   // for (int i = 0; i < bins.size(); i++){\n   //    bins[i] = 0;\n   // }\n\n   // TODO: You can use the following code to determine rank and number of ranks\n   int rank;\n   int ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n   // TODO: You need to determine the start and end indices in the array points that this rank needs to look at.\n   //       Store these in the variables startIndex and endIndex.\n   // Hint: Use the following formula to determine the number of elements in a slice:\n   //       endIndex = startIndex + n / ranks;\n   //       For example, if rank 2 needs to look at the first 3 elements of an array, endIndex = 3.\n   // Hint 2: The number of elements in the first slice is n % ranks.\n   // Hint 3: You can use the following formula to determine the start index of a slice:\n   //       startIndex = (rank * n) / ranks;\n   //       For example, if rank 2 needs to look at the first 3 elements of an array,\n   //       startIndex = 6. (The starting index of the first slice is 0.)\n   // Hint 4: Remember that MPI ranks start at 0, so ranks-1 is the number of slices in total.\n\n   int startIndex = 0;\n   int endIndex = 0;\n\n   if (rank == 0){\n      startIndex = 0;\n      endIndex = n / ranks;\n   }\n   else{\n      startIndex = (rank * n) / ranks;\n      endIndex = ((rank+1) * n) / ranks;\n   }\n\n   // TODO: You should count the number of points that fall in each quadrant.\n   //       Each rank should count the number of points in its slice, and store the result in bins.\n   // Hint 1: Use the following formula to determine whether a point is in the first quadrant:\n   //         x >= 0 && y >= 0\n   // Hint 2: Use the following formula to determine whether a point is in the second quadrant:\n   //         x < 0 && y >= 0\n   // Hint 3: Use the following formula to determine whether a point is in the third quadrant:\n   //         x < 0 && y < 0\n   // Hint 4: Use the following formula to determine whether a point is in the fourth quadrant:\n   //         x >= 0 && y < 0\n\n   // for (int i = 0; i < bins.size(); i++){\n   //    bins[i] = 0;\n   // }\n\n   // TODO 4.3: implement countQuadrants here\n\n   // for (int i = startIndex; i < endIndex; i++){\n   //    if (points[i].x >= 0 && points[i].y >= 0){\n   //       bins[0] += 1;\n   //    }\n   //    else if (points[i].x < 0 && points[i].y >= 0){\n   //       bins[1] += 1;\n   //    }\n   //    else if (points[i].x < 0 && points[i].y < 0){\n   //       bins[2] += 1;\n   //    }\n   //    else if (points[i].x >= 0 && points[i].y < 0){\n   //       bins[3] += 1;\n   //    }\n   // }\n\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   // TODO: You should only need to use the following code to check your work\n   if (rank == 0) {\n      for (int i = 0; i < bins.size(); i++) {\n         std::cout << bins[i] <<'';\n      }\n      std::cout << '\\n';\n   }\n}",
            "// TODO: implement this function\n}",
            "// TODO\n}",
            "size_t local_points = 0;\n   size_t num_points = points.size();\n   for (size_t i = 0; i < num_points; i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         local_points++;\n      } else if (points[i].x <= 0 && points[i].y >= 0) {\n         local_points++;\n      } else if (points[i].x <= 0 && points[i].y <= 0) {\n         local_points++;\n      } else {\n         local_points++;\n      }\n   }\n\n   std::array<size_t, 4> local_bins;\n   local_bins[0] = 0;\n   local_bins[1] = 0;\n   local_bins[2] = 0;\n   local_bins[3] = 0;\n\n   for (size_t i = 0; i < num_points; i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         local_bins[0]++;\n      } else if (points[i].x <= 0 && points[i].y >= 0) {\n         local_bins[1]++;\n      } else if (points[i].x <= 0 && points[i].y <= 0) {\n         local_bins[2]++;\n      } else {\n         local_bins[3]++;\n      }\n   }\n\n   MPI_Allreduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "}",
            "/* Your solution goes here. */\n}",
            "// TODO\n}",
            "// Your code here\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   double x_max = points[0].x;\n   double y_max = points[0].y;\n   double x_min = points[0].x;\n   double y_min = points[0].y;\n\n   for(int i = 1; i < points.size(); i++){\n      if(points[i].x > x_max){\n         x_max = points[i].x;\n      }\n      if(points[i].y > y_max){\n         y_max = points[i].y;\n      }\n      if(points[i].x < x_min){\n         x_min = points[i].x;\n      }\n      if(points[i].y < y_min){\n         y_min = points[i].y;\n      }\n   }\n\n   double x_mid = (x_max + x_min)/2;\n   double y_mid = (y_max + y_min)/2;\n\n   MPI_Datatype MPI_Point_type;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &MPI_Point_type);\n   MPI_Type_commit(&MPI_Point_type);\n\n   int n = points.size();\n\n   int start = rank * (n / size);\n   int end = (rank + 1) * (n / size);\n\n   if (rank == size - 1) {\n      end = n;\n   }\n\n   std::vector<Point> send_points(end - start);\n   std::copy(points.begin() + start, points.begin() + end, send_points.begin());\n\n   int* sendcounts = (int*)malloc(size * sizeof(int));\n   int* recvcounts = (int*)malloc(size * sizeof(int));\n   int* displs = (int*)malloc(size * sizeof(int));\n   for (int i = 0; i < size; i++) {\n      sendcounts[i] = send_points.size() / size;\n      if (i < (n % size)) {\n         sendcounts[i] += 1;\n      }\n   }\n\n   displs[0] = 0;\n   for (int i = 1; i < size; i++) {\n      displs[i] = displs[i - 1] + sendcounts[i - 1];\n   }\n\n   MPI_Scatterv(send_points.data(), sendcounts, displs, MPI_Point_type, NULL, 0, MPI_Point_type, 0, MPI_COMM_WORLD);\n\n   for (int i = 0; i < end - start; i++) {\n      if (send_points[i].x <= x_mid && send_points[i].y <= y_mid) {\n         bins[0] = bins[0] + 1;\n      } else if (send_points[i].x > x_mid && send_points[i].y <= y_mid) {\n         bins[1] = bins[1] + 1;\n      } else if (send_points[i].x > x_mid && send_points[i].y > y_mid) {\n         bins[2] = bins[2] + 1;\n      } else {\n         bins[3] = bins[3] + 1;\n      }\n   }\n\n   MPI_Gatherv(NULL, 0, MPI_INT, recvcounts, sendcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);\n\n   MPI_Reduce(MPI_IN_PLACE, recvcounts, size, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      bins = { recvcounts[0], recvcounts[1], recvcounts[2], recvcounts[3] };\n   }\n\n   MPI_Type_free(&MPI_Point_type);\n}",
            "const int n = 2000;\n  MPI_Comm cartComm;\n  MPI_Cart_create(MPI_COMM_WORLD, 2, {n, n}, {0, 0}, 1, &cartComm);\n\n  int dims[2];\n  int coords[2];\n\n  MPI_Cart_get(cartComm, 2, dims, coords);\n\n  size_t totalSize = n * n;\n  std::vector<size_t> counts(totalSize);\n  std::fill(counts.begin(), counts.end(), 0);\n\n  // for each point check if it falls in a particular quadrant\n  for (size_t i = 0; i < points.size(); i++) {\n    if (points[i].x >= 0 && points[i].y >= 0) {\n      counts[coords[0] * n + coords[1]]++;\n    }\n    else if (points[i].x < 0 && points[i].y >= 0) {\n      counts[coords[0] * n + n - coords[1] - 1]++;\n    }\n    else if (points[i].x < 0 && points[i].y < 0) {\n      counts[coords[0] * n + n - coords[1] - 1]++;\n    }\n    else if (points[i].x >= 0 && points[i].y < 0) {\n      counts[coords[0] * n + coords[1]]++;\n    }\n  }\n\n  // sum the counts from all ranks\n  MPI_Reduce(counts.data(), bins.data(), totalSize, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, cartComm);\n\n  MPI_Comm_free(&cartComm);\n}",
            "size_t n = points.size();\n   MPI_Comm_size(MPI_COMM_WORLD, &n);\n   int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   int cart_dims[2] = {0, 0};\n   int periods[2] = {0, 0};\n   MPI_Cart_create(MPI_COMM_WORLD, 2, cart_dims, periods, 0, &MPI_COMM_CART);\n   int cart_coords[2] = {0, 0};\n   MPI_Cart_coords(MPI_COMM_CART, world_rank, 2, cart_coords);\n   double const x_min = -10;\n   double const x_max = 10;\n   double const y_min = -10;\n   double const y_max = 10;\n   double const dx = (x_max - x_min) / cart_dims[0];\n   double const dy = (y_max - y_min) / cart_dims[1];\n   double const x0 = x_min + cart_coords[0] * dx;\n   double const x1 = x0 + dx;\n   double const y0 = y_min + cart_coords[1] * dy;\n   double const y1 = y0 + dy;\n\n   std::vector<Point> subset;\n   size_t cnt = 0;\n   for (Point const& p : points) {\n      if (p.x >= x0 && p.x < x1 && p.y >= y0 && p.y < y1) {\n         ++cnt;\n      }\n   }\n\n   // TODO: compute bins on rank 0\n\n   if (world_rank == 0) {\n      for (size_t i = 0; i < cart_dims[0]; ++i) {\n         for (size_t j = 0; j < cart_dims[1]; ++j) {\n            int r = cart_coords[0] * cart_dims[0] + i;\n            int c = cart_coords[1] * cart_dims[1] + j;\n            bins[c * cart_dims[0] + r] = cnt;\n         }\n      }\n   }\n}",
            "const int DIM_X = 10;\n    const int DIM_Y = 10;\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    double x1 = DIM_X * rank / size;\n    double x2 = DIM_X * (rank + 1) / size;\n\n    // This is the same as above, but more compact.\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Send(&x1, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n            MPI_Send(&x2, 1, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    }\n\n    double y1;\n    double y2;\n    MPI_Status status;\n    MPI_Recv(&y1, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n    MPI_Recv(&y2, 1, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, &status);\n\n    std::array<size_t, 4> counts = {0, 0, 0, 0};\n\n    for (Point point : points) {\n        if (point.x >= x1 && point.x < x2 && point.y >= y1 && point.y < y2) {\n            counts[0]++;\n        } else if (point.x < x1 && point.y >= y1 && point.y < y2) {\n            counts[1]++;\n        } else if (point.x < x1 && point.y < y1) {\n            counts[2]++;\n        } else {\n            counts[3]++;\n        }\n    }\n\n    // This is the same as above, but more compact.\n    if (rank == 0) {\n        for (int i = 1; i < size; i++) {\n            MPI_Recv(&counts[0], 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    } else {\n        MPI_Send(&counts[0], 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n\n    bins = counts;\n}",
            "bins = std::array<size_t, 4>();\n   bins[0] = bins[1] = bins[2] = bins[3] = 0;\n}",
            "// TODO: implement me\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int subSize = size / 2;\n   int subRank = rank % subSize;\n\n   // TODO: Complete this function\n\n   int isZero = rank == 0;\n   MPI_Bcast(&isZero, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   if (!isZero) {\n      MPI_Send(&bins, 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n   else {\n      MPI_Recv(bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n}",
            "// TODO\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int n = points.size();\n   if(size!= 4) throw \"the number of processors is not 4\";\n   std::vector<int> recvcounts(size, 0);\n   std::vector<int> displs(size, 0);\n   double x_mid = (points[0].x + points[n-1].x)/2;\n   double y_mid = (points[0].y + points[n-1].y)/2;\n   std::vector<Point> local_points(points.begin(), points.end());\n   for(int i=0; i<n; i++){\n      if(local_points[i].x <= x_mid && local_points[i].y <= y_mid)\n         local_points[i].quadrant = 1;\n      else if(local_points[i].x > x_mid && local_points[i].y <= y_mid)\n         local_points[i].quadrant = 2;\n      else if(local_points[i].x <= x_mid && local_points[i].y > y_mid)\n         local_points[i].quadrant = 3;\n      else if(local_points[i].x > x_mid && local_points[i].y > y_mid)\n         local_points[i].quadrant = 4;\n      else throw \"error\";\n   }\n   for(int i=0; i<n; i++){\n      recvcounts[local_points[i].quadrant-1]++;\n   }\n   displs[0] = 0;\n   for(int i=1; i<size; i++){\n      displs[i] = displs[i-1] + recvcounts[i-1];\n   }\n   std::vector<int> bins_local(4);\n   std::vector<int> bins_recv(4);\n   MPI_Scatterv(&local_points[0], &recvcounts[0], &displs[0], MPI_DOUBLE, &bins_local[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&bins_local[0], &bins_recv[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   if(rank == 0){\n      bins[0] = bins_recv[0];\n      bins[1] = bins_recv[1];\n      bins[2] = bins_recv[2];\n      bins[3] = bins_recv[3];\n   }\n}",
            "// Your code here\n}",
            "// implement this function!\n}",
            "// TODO\n    bins.fill(0);\n    int rank, numProcesses;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n\n    if (numProcesses < 4) {\n        if (rank == 0) {\n            for (const auto& point : points) {\n                if (point.x > 0) {\n                    if (point.y > 0) {\n                        bins[0] += 1;\n                    } else {\n                        bins[3] += 1;\n                    }\n                } else {\n                    if (point.y > 0) {\n                        bins[1] += 1;\n                    } else {\n                        bins[2] += 1;\n                    }\n                }\n            }\n        }\n        return;\n    }\n\n    std::vector<size_t> localBins(4);\n\n    size_t n = points.size() / numProcesses;\n\n    std::vector<Point> points_local;\n    if (rank == 0) {\n        points_local = std::vector<Point>(points.begin(), points.begin() + n);\n        for (int i = 1; i < numProcesses; i++) {\n            MPI_Send(points.data() + n * i, n, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n        }\n    } else {\n        MPI_Recv(points_local.data(), n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n    }\n\n    for (int i = 0; i < n; i++) {\n        if (points_local[i].x > 0) {\n            if (points_local[i].y > 0) {\n                localBins[0] += 1;\n            } else {\n                localBins[3] += 1;\n            }\n        } else {\n            if (points_local[i].y > 0) {\n                localBins[1] += 1;\n            } else {\n                localBins[2] += 1;\n            }\n        }\n    }\n\n    MPI_Gather(localBins.data(), 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::vector<Point> points_local;\n    int num_local = points.size();\n    if (rank == 0)\n        points_local = points;\n    else {\n        points_local.resize(num_local);\n        MPI_Scatter(points.data(), num_local, MPI_DOUBLE, points_local.data(), num_local, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n\n    int num_local_quadrants[4];\n    for (int i = 0; i < 4; ++i)\n        num_local_quadrants[i] = 0;\n\n    for (auto point: points_local) {\n        if (point.x >= 0 && point.y >= 0)\n            ++num_local_quadrants[0];\n        else if (point.x >= 0 && point.y < 0)\n            ++num_local_quadrants[1];\n        else if (point.x < 0 && point.y >= 0)\n            ++num_local_quadrants[2];\n        else\n            ++num_local_quadrants[3];\n    }\n\n    int counts_local[4];\n    MPI_Scatter(num_local_quadrants, 4, MPI_INT, counts_local, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    int counts[4];\n    if (rank == 0)\n        counts = counts_local;\n    else\n        MPI_Gather(counts_local, 4, MPI_INT, counts, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        for (int i = 0; i < 4; ++i)\n            bins[i] = counts[i];\n    }\n}",
            "// TODO: write your implementation here\n   size_t num_of_points = points.size();\n   int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   size_t sub_size = num_of_points/world_size;\n   std::vector<Point> sub_points;\n   if (world_rank < num_of_points%world_size){\n      sub_points.resize(sub_size + 1);\n   } else {\n      sub_points.resize(sub_size);\n   }\n\n   std::vector<Point> send_points;\n   std::vector<Point> recv_points;\n   if (world_rank == 0){\n      for (int i = 0; i < num_of_points; i++){\n         Point p = points[i];\n         if ((p.x > 0 && p.y > 0) || (p.x < 0 && p.y < 0)){\n            recv_points.push_back(p);\n         } else if (p.x > 0 && p.y < 0) {\n            recv_points.push_back(p);\n         } else if (p.x < 0 && p.y > 0) {\n            recv_points.push_back(p);\n         } else {\n            send_points.push_back(p);\n         }\n      }\n   }\n\n   MPI_Scatter(&send_points[0], send_points.size(), MPI_DOUBLE_INT, &sub_points[0], sub_points.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(&recv_points[0], recv_points.size(), MPI_DOUBLE_INT, &sub_points[sub_points.size()], recv_points.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n   size_t num_of_send_points = send_points.size();\n   size_t num_of_recv_points = recv_points.size();\n   MPI_Gather(&sub_points[0], sub_points.size(), MPI_DOUBLE_INT, &bins[0], sub_points.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n   if (world_rank == 0){\n      for (int i = 0; i < world_size - 1; i++){\n         size_t num_of_send_points_per_rank;\n         MPI_Recv(&num_of_send_points_per_rank, 1, MPI_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         std::vector<Point> send_points_per_rank(num_of_send_points_per_rank);\n         MPI_Recv(&send_points_per_rank[0], send_points_per_rank.size(), MPI_DOUBLE_INT, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (size_t j = 0; j < send_points_per_rank.size(); j++){\n            Point p = send_points_per_rank[j];\n            if ((p.x > 0 && p.y > 0) || (p.x < 0 && p.y < 0)){\n               recv_points.push_back(p);\n            } else if (p.x > 0 && p.y < 0) {\n               recv_points.push_back(p);\n            } else if (p.x < 0 && p.y > 0) {\n               recv_points.push_back(p);\n            } else {\n               send_points.push_back(p);\n            }\n         }\n      }\n   }\n   MPI_Gather(&recv_points[0], recv_points.size(), MPI_DOUBLE_INT, &bins[2], recv_points.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: complete this function\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int n = points.size();\n   int n2 = n / size;\n   int rest = n - n2 * size;\n\n   std::vector<int> loc_n(size);\n   loc_n[rank] = n2 + (rank < rest);\n\n   std::vector<int> loc_n2(size);\n   MPI_Allgather(MPI_IN_PLACE, 0, MPI_DATATYPE_NULL, loc_n2.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n   int loc_n_sum = 0;\n   for (int i = 0; i < size; ++i) {\n      loc_n_sum += loc_n2[i];\n   }\n   std::vector<int> loc_pos(loc_n_sum);\n\n   int j = 0;\n   for (int i = 0; i < rank; ++i) {\n      j += loc_n2[i];\n   }\n\n   for (int i = 0; i < loc_n2[rank]; ++i) {\n      loc_pos[i] = j + i;\n   }\n\n   std::vector<Point> loc_points(loc_n[rank]);\n   MPI_Scatterv(points.data(), loc_n2.data(), loc_pos.data(), MPI_DOUBLE, loc_points.data(), loc_n[rank], MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   //std::cout << \"rank: \" << rank << \", loc_points.size(): \" << loc_points.size() << std::endl;\n\n   std::vector<int> n_points_per_quadrant(4);\n   for (auto p : loc_points) {\n      if (p.x >= 0 && p.y >= 0) {\n         n_points_per_quadrant[0]++;\n      } else if (p.x < 0 && p.y >= 0) {\n         n_points_per_quadrant[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         n_points_per_quadrant[2]++;\n      } else {\n         n_points_per_quadrant[3]++;\n      }\n   }\n\n   std::vector<int> loc_n_points_per_quadrant(4);\n   MPI_Gatherv(n_points_per_quadrant.data(), 4, MPI_INT, loc_n_points_per_quadrant.data(), loc_n, MPI_INT, 0, MPI_COMM_WORLD);\n\n   bins = std::array<size_t, 4> {0, 0, 0, 0};\n   if (rank == 0) {\n      for (int i = 0; i < 4; ++i) {\n         bins[i] = loc_n_points_per_quadrant[i];\n      }\n   }\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Bcast(points.data(), points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   bins.fill(0);\n   for (auto const& p : points) {\n      if (p.x > 0 && p.y > 0)\n         ++bins[0];\n      else if (p.x < 0 && p.y > 0)\n         ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n         ++bins[2];\n      else\n         ++bins[3];\n   }\n   if (rank == 0)\n      return;\n   MPI_Scatter(bins.data(), bins.size(), MPI_UNSIGNED, bins.data(), bins.size(), MPI_UNSIGNED, 0, MPI_COMM_WORLD);\n}",
            "}",
            "// Insert your code here.\n}",
            "// your code here\n}",
            "// YOUR CODE HERE\n}",
            "const int npoints = points.size();\n   MPI_Comm cart_comm;\n   MPI_Cart_create(MPI_COMM_WORLD, 2, {npoints, npoints}, {0, 0}, 1, &cart_comm);\n\n   std::vector<int> ranks(npoints, -1);\n   MPI_Cart_map(cart_comm, npoints, npoints, const_cast<bool *>(MPI_UNWEIGHTED), ranks.data());\n   std::vector<int> num_quadrants(npoints);\n   MPI_Allgather(ranks.data(), npoints, MPI_INT, num_quadrants.data(), npoints, MPI_INT, cart_comm);\n\n   bins = {};\n   for (auto const& num_quadrant : num_quadrants) {\n      ++bins[num_quadrant];\n   }\n   MPI_Comm_free(&cart_comm);\n}",
            "// write your solution here\n}",
            "// count how many points in each quadrant\n    for (auto const& p : points) {\n        if (p.x > 0 && p.y > 0) {\n            bins[0]++;\n        }\n        if (p.x < 0 && p.y > 0) {\n            bins[1]++;\n        }\n        if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n        }\n        if (p.x > 0 && p.y < 0) {\n            bins[3]++;\n        }\n    }\n}",
            "// TODO: implement this\n}",
            "// TODO\n    int n = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &n);\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int nlocal = points.size() / n;\n    int *local = new int[nlocal];\n\n    MPI_Scatter(points.data(), nlocal, MPI_DOUBLE, local, nlocal, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Gather(&local[0], nlocal, MPI_DOUBLE, bins.data(), nlocal, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    delete [] local;\n}",
            "// TO DO: implement this function\n   return;\n}",
            "// TODO\n\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n  size_t num_procs, rank;\n  MPI_Comm_size(comm, &num_procs);\n  MPI_Comm_rank(comm, &rank);\n\n  // Each rank receives a copy of the vector of points.\n  std::vector<Point> local_points(points);\n\n  // Get the number of points in each quadrant.\n  size_t num_points = points.size();\n  size_t local_bins[4] = {0, 0, 0, 0};\n  for (Point const& p : local_points) {\n    int quadrant = 0;\n    if (p.x > 0) {\n      if (p.y > 0) {\n        quadrant = 1;\n      } else {\n        quadrant = 2;\n      }\n    } else {\n      if (p.y > 0) {\n        quadrant = 3;\n      } else {\n        quadrant = 4;\n      }\n    }\n    local_bins[quadrant] += 1;\n  }\n\n  // Now allreduce the counts to get the global counts.\n  MPI_Allreduce(local_bins, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, comm);\n}",
            "size_t size = points.size();\n\n   std::array<int, 4> counts;\n   std::array<double, 4> xs;\n   std::array<double, 4> ys;\n   std::array<std::vector<Point>, 4> local_bins;\n\n   // create local copies of points\n   for (int i = 0; i < 4; i++) {\n      local_bins[i].reserve(size / 4);\n   }\n   for (auto point : points) {\n      int quad = 0;\n      if (point.x < 0) {\n         quad |= 1;\n      }\n      if (point.y < 0) {\n         quad |= 2;\n      }\n\n      local_bins[quad].emplace_back(point);\n   }\n\n   // calculate local counts\n   for (int i = 0; i < 4; i++) {\n      counts[i] = local_bins[i].size();\n   }\n\n   // gather counts\n   MPI_Reduce(counts.data(), counts.data() + 4, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // gather xs and ys\n   for (int i = 0; i < 4; i++) {\n      xs[i] = local_bins[i].front().x;\n      ys[i] = local_bins[i].front().y;\n   }\n\n   MPI_Reduce(xs.data(), xs.data() + 4, 4, MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);\n   MPI_Reduce(ys.data(), ys.data() + 4, 4, MPI_DOUBLE, MPI_MIN, 0, MPI_COMM_WORLD);\n\n   // store results\n   if (0 == MPI_PROC_NULL) {\n      bins[0] = counts[0];\n      bins[1] = counts[1];\n      bins[2] = counts[2];\n      bins[3] = counts[3];\n   }\n}",
            "}",
            "int my_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n\n   std::array<size_t, 4> local_bins;\n   size_t local_sum = 0;\n\n   for (auto const& point : points) {\n      if (point.x > 0 && point.y > 0) {\n         local_bins[0]++;\n      }\n      else if (point.x < 0 && point.y > 0) {\n         local_bins[1]++;\n      }\n      else if (point.x < 0 && point.y < 0) {\n         local_bins[2]++;\n      }\n      else {\n         local_bins[3]++;\n      }\n\n      local_sum++;\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&local_sum, &bins[4], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (my_rank == 0) {\n      bins[4] /= 2;\n   }\n}",
            "/* Your code here */\n}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   // Each rank has a complete copy of points.\n   // Create a slice that contains the points for this rank.\n   size_t slice_size = points.size() / num_ranks;\n   Point const* my_points = points.data() + rank * slice_size;\n   size_t my_size = slice_size;\n   if (rank == num_ranks - 1) {\n      my_size += points.size() % num_ranks;\n   }\n\n   // Count the number of points in each quadrant.\n   // Each point is in exactly one quadrant.\n   std::array<size_t, 4> my_bins;\n   for (auto const& p : my_points) {\n      int quadrant = -1;\n      if (p.x > 0 && p.y > 0) {\n         quadrant = 0;\n      } else if (p.x < 0 && p.y > 0) {\n         quadrant = 1;\n      } else if (p.x < 0 && p.y < 0) {\n         quadrant = 2;\n      } else if (p.x > 0 && p.y < 0) {\n         quadrant = 3;\n      }\n      if (quadrant!= -1) {\n         ++my_bins[quadrant];\n      }\n   }\n\n   // Gather the results from each rank.\n   std::array<size_t, 4> all_bins;\n   MPI_Reduce(my_bins.data(), all_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Copy the results to bins on rank 0.\n   if (rank == 0) {\n      bins = all_bins;\n   }\n}",
            "bins = {0, 0, 0, 0};\n\n   // Insert your code here\n}",
            "const int n_ranks = MPI::COMM_WORLD.Get_size();\n   const int rank = MPI::COMM_WORLD.Get_rank();\n   // Create cartesian topology, assuming a square cartesian grid\n   const int dims[] = {0, 0};\n   const int periods[] = {0, 0};\n   const int reorder = 1;\n   MPI::Cartcomm cartcomm(MPI::COMM_WORLD, 2, dims, periods, reorder);\n\n   std::vector<Point> local_points;\n\n   for (Point const& point : points) {\n      // Find which processor owns this point\n      int coords[] = {0, 0};\n      cartcomm.Get_coords(point.x, point.y, coords);\n\n      // If this point is in the cartesian region owned by this processor, add it to local_points\n      if (coords[0] == rank % 2 && coords[1] == (rank - coords[0]) / 2) {\n         local_points.push_back(point);\n      }\n   }\n\n   // Send and receive counts\n   std::array<int, 4> send_counts;\n   std::array<int, 4> recv_counts;\n\n   for (int direction = 0; direction < 4; ++direction) {\n      MPI::Cart_shift(cartcomm, direction, 1, &recv_counts[direction], &send_counts[direction]);\n   }\n\n   std::vector<int> recv_displs(n_ranks + 1);\n   for (int i = 0; i < n_ranks; ++i) {\n      recv_displs[i + 1] = recv_displs[i] + recv_counts[i];\n   }\n\n   std::vector<int> send_displs(n_ranks + 1);\n   for (int i = 0; i < n_ranks; ++i) {\n      send_displs[i + 1] = send_displs[i] + send_counts[i];\n   }\n\n   std::vector<int> recv_data(recv_displs[n_ranks]);\n   MPI::Datatype MPI_POINT = MPI::DOUBLE.Create_contiguous(2);\n   MPI_POINT.Commit();\n\n   MPI::Status status;\n   MPI::Request requests[4];\n   MPI::Request::Init_gatherv(local_points.data(), 1, MPI_POINT, recv_data.data(), recv_counts.data(), recv_displs.data(), MPI_POINT, 0, cartcomm, requests);\n   MPI::Request::Waitall(4, requests, status);\n\n   for (int direction = 0; direction < 4; ++direction) {\n      MPI::Cart_shift(cartcomm, direction, 1, &recv_counts[direction], &send_counts[direction]);\n   }\n\n   MPI::Request::Init_scatterv(recv_data.data(), recv_counts.data(), recv_displs.data(), MPI_POINT, bins.data(), 4, MPI_INT, 0, cartcomm, requests);\n   MPI::Request::Waitall(4, requests, status);\n\n}",
            "int rank, nRanks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n    double xMin = std::numeric_limits<double>::max(),\n           xMax = std::numeric_limits<double>::min(),\n           yMin = std::numeric_limits<double>::max(),\n           yMax = std::numeric_limits<double>::min();\n    for (const Point& point: points) {\n        if (point.x > xMax)\n            xMax = point.x;\n        if (point.x < xMin)\n            xMin = point.x;\n        if (point.y > yMax)\n            yMax = point.y;\n        if (point.y < yMin)\n            yMin = point.y;\n    }\n    double dx = xMax - xMin, dy = yMax - yMin;\n\n    int nPoints = points.size(),\n        nDivX = nRanks,\n        nDivY = nRanks;\n    if (nRanks > nPoints) {\n        nDivX = nPoints / nRanks;\n        nDivY = nDivX;\n    }\n    else {\n        nPoints = nRanks;\n    }\n\n    size_t leftX, rightX, topY, bottomY;\n    if (rank < nPoints) {\n        if (dx / nDivX > dy / nDivY) {\n            leftX = rank * dx / nPoints;\n            rightX = (rank + 1) * dx / nPoints;\n            topY = (rank + 1) * dy / nPoints;\n            bottomY = rank * dy / nPoints;\n        }\n        else {\n            leftX = (rank + 1) * dx / nPoints;\n            rightX = rank * dx / nPoints;\n            topY = rank * dy / nPoints;\n            bottomY = (rank + 1) * dy / nPoints;\n        }\n    }\n    else {\n        leftX = std::numeric_limits<double>::min();\n        rightX = std::numeric_limits<double>::max();\n        topY = std::numeric_limits<double>::min();\n        bottomY = std::numeric_limits<double>::max();\n    }\n\n    std::vector<size_t> localBins(4, 0);\n    for (const Point& point: points) {\n        if (point.x >= leftX && point.x <= rightX &&\n            point.y >= bottomY && point.y <= topY) {\n            if (point.x >= xMin && point.x <= xMax &&\n                point.y >= yMin && point.y <= yMax) {\n                ++localBins[0];\n            }\n            else if (point.x >= xMin && point.x <= rightX &&\n                     point.y >= bottomY && point.y <= topY) {\n                ++localBins[1];\n            }\n            else if (point.x >= leftX && point.x <= xMax &&\n                     point.y >= bottomY && point.y <= topY) {\n                ++localBins[2];\n            }\n            else {\n                ++localBins[3];\n            }\n        }\n    }\n\n    MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int numProcs, rank;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    if (numProcs!= 4) {\n        throw std::invalid_argument(\"Must have 4 processes\");\n    }\n\n    size_t num_points = points.size();\n    if (num_points == 0) {\n        return;\n    }\n\n    size_t points_per_proc = num_points / numProcs;\n    size_t remaining = num_points % numProcs;\n\n    std::vector<Point> rank_points(points_per_proc);\n    std::vector<Point> extra_points(remaining);\n\n    // Split up the points\n    MPI_Scatter(points.data(), points_per_proc, MPI_DOUBLE, rank_points.data(), points_per_proc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    MPI_Scatter(points.data() + points_per_proc, remaining, MPI_DOUBLE, extra_points.data(), remaining, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n    size_t quadrant_count[4];\n    std::fill(quadrant_count, quadrant_count + 4, 0);\n\n    for (Point const& point : rank_points) {\n        if (point.x > 0 && point.y > 0) {\n            quadrant_count[0] += 1;\n        } else if (point.x < 0 && point.y > 0) {\n            quadrant_count[1] += 1;\n        } else if (point.x < 0 && point.y < 0) {\n            quadrant_count[2] += 1;\n        } else {\n            quadrant_count[3] += 1;\n        }\n    }\n\n    for (Point const& point : extra_points) {\n        if (point.x > 0 && point.y > 0) {\n            quadrant_count[0] += 1;\n        } else if (point.x < 0 && point.y > 0) {\n            quadrant_count[1] += 1;\n        } else if (point.x < 0 && point.y < 0) {\n            quadrant_count[2] += 1;\n        } else {\n            quadrant_count[3] += 1;\n        }\n    }\n\n    MPI_Gather(quadrant_count, 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, world_size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n    /* TODO: implement me */\n\n    // Count points in quadrant 1\n    size_t leftUpper = 0;\n    size_t rightUpper = 0;\n    size_t leftLower = 0;\n    size_t rightLower = 0;\n\n    for (auto const &point : points) {\n        if (point.x > 0 && point.y > 0) {\n            rightUpper++;\n        } else if (point.x < 0 && point.y > 0) {\n            leftUpper++;\n        } else if (point.x < 0 && point.y < 0) {\n            leftLower++;\n        } else {\n            rightLower++;\n        }\n    }\n\n    // Sum the number of points in each quadrant\n    if (rank == 0) {\n        bins = std::array<size_t, 4>();\n        MPI_Reduce(&rightUpper, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&leftUpper, &bins[1], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&leftLower, &bins[2], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&rightLower, &bins[3], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Reduce(&rightUpper, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&leftUpper, &bins[1], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&leftLower, &bins[2], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n        MPI_Reduce(&rightLower, &bins[3], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n  int rank, size;\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  int count;\n  for (int i = 0; i < size; i++) {\n    MPI_Status status;\n    if (rank == i) {\n      count = 0;\n      for (auto const &p : points) {\n        if (p.x > 0 && p.y > 0) {\n          count++;\n        } else if (p.x < 0 && p.y > 0) {\n          count++;\n        } else if (p.x < 0 && p.y < 0) {\n          count++;\n        } else {\n          count++;\n        }\n      }\n    }\n    MPI_Send(&count, 1, MPI_INT, i, 0, MPI_COMM_WORLD);\n  }\n  for (int i = 0; i < 4; i++) {\n    if (rank == 0) {\n      bins[i] = 0;\n    }\n    MPI_Status status;\n    MPI_Recv(&bins[i], 1, MPI_INT, i, 0, MPI_COMM_WORLD, &status);\n  }\n}",
            "// TODO\n\n}",
            "// TODO\n   return;\n}",
            "int rank = 0, num_ranks = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   // TODO\n\n   if (rank == 0) {\n      // TODO\n   }\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    std::array<size_t, 4> counts;\n    counts.fill(0);\n\n    if (rank == 0) {\n        for (auto point : points) {\n            if (point.x > 0 && point.y > 0) {\n                counts[0]++;\n            } else if (point.x < 0 && point.y > 0) {\n                counts[1]++;\n            } else if (point.x < 0 && point.y < 0) {\n                counts[2]++;\n            } else if (point.x > 0 && point.y < 0) {\n                counts[3]++;\n            }\n        }\n    }\n\n    MPI_Reduce(&counts, &bins, 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: complete this function\n   size_t local_bins[4] = {0};\n   int world_rank, world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   int count = points.size();\n   int sub_count = count / world_size;\n   int left_count = count % world_size;\n\n   MPI_Scatter(&count, 1, MPI_INT, &sub_count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(&left_count, 1, MPI_INT, &left_count, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatterv(&points[0], &sub_count, &left_count, MPI_DOUBLE, &local_bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   for(int i = 0; i < 4; i++)\n      bins[i] = local_bins[i];\n}",
            "// TODO: implement this\n    bins = std::array<size_t,4>();\n}",
            "double min_x = 0, max_x = 0, min_y = 0, max_y = 0;\n\n    int rank, nprocs;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n    int min_x_rank, min_y_rank, max_x_rank, max_y_rank;\n\n    if (rank == 0) {\n        // Find bounds for x and y\n        for (auto& p : points) {\n            if (p.x < min_x) min_x = p.x;\n            if (p.x > max_x) max_x = p.x;\n            if (p.y < min_y) min_y = p.y;\n            if (p.y > max_y) max_y = p.y;\n        }\n\n        min_x_rank = min_x / (max_x - min_x) * (nprocs - 1);\n        min_y_rank = min_y / (max_y - min_y) * (nprocs - 1);\n        max_x_rank = max_x / (max_x - min_x) * (nprocs - 1);\n        max_y_rank = max_y / (max_y - min_y) * (nprocs - 1);\n    }\n\n    MPI_Bcast(&min_x_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&min_y_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&max_x_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(&max_y_rank, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n    // Count points in quadrant\n    for (auto& p : points) {\n        if (p.x > min_x_rank && p.x <= max_x_rank &&\n            p.y > min_y_rank && p.y <= max_y_rank) {\n            bins[0]++;\n        } else if (p.x > max_x_rank && p.y <= max_y_rank) {\n            bins[1]++;\n        } else if (p.x > max_x_rank && p.y > max_y_rank) {\n            bins[2]++;\n        } else if (p.y > min_y_rank && p.y <= max_y_rank) {\n            bins[3]++;\n        }\n    }\n\n    // Reduce counts to the root process\n    MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins = std::array<size_t, 4>();\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    int num_points = points.size();\n    double chunk_size = num_points / size;\n    if (rank < (num_points % size)) chunk_size++;\n    int start_index = std::ceil(rank * chunk_size);\n    int end_index = std::floor((rank + 1) * chunk_size);\n    int quadrants = 0;\n    for (auto p = points.begin() + start_index; p!= points.begin() + end_index; ++p) {\n        if ((p->x >= 0) && (p->y >= 0)) quadrants |= 1;\n        else if ((p->x >= 0) && (p->y < 0)) quadrants |= 2;\n        else if ((p->x < 0) && (p->y >= 0)) quadrants |= 4;\n        else quadrants |= 8;\n    }\n    MPI_Reduce(\n        &quadrants, \n        &bins[0], \n        4,\n        MPI_INT,\n        MPI_BOR,\n        0,\n        MPI_COMM_WORLD);\n    if (rank == 0) {\n        bins[0] = std::count_if(points.begin(), points.end(), [=](Point p){return (p.x >= 0) && (p.y >= 0);});\n        bins[1] = std::count_if(points.begin(), points.end(), [=](Point p){return (p.x >= 0) && (p.y < 0);});\n        bins[2] = std::count_if(points.begin(), points.end(), [=](Point p){return (p.x < 0) && (p.y >= 0);});\n        bins[3] = std::count_if(points.begin(), points.end(), [=](Point p){return (p.x < 0) && (p.y < 0);});\n    }\n}",
            "/* TODO */\n}",
            "int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  // rank 0 will have all points and store the results in bins\n  if (rank == 0) {\n    // TODO\n  } else {\n    // TODO\n  }\n\n  // TODO: send and receive counts\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  // TODO: send and receive offsets\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  // TODO: send and receive points\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  // TODO: calculate counts from offsets and counts\n  MPI_Barrier(MPI_COMM_WORLD);\n}",
            "}",
            "// TODO: Your code goes here\n}",
            "int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int n = points.size();\n\n   int local_count[4] = {0, 0, 0, 0};\n\n   for(int i = 0; i < n; i++) {\n      int x = points[i].x > 0? 1 : -1;\n      int y = points[i].y > 0? 1 : -1;\n      local_count[x + 2 * y + 1] += 1;\n   }\n\n   std::array<int, 4> recv_count;\n   MPI_Gather(local_count, 4, MPI_INT, recv_count.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = recv_count[i];\n      }\n   }\n}",
            "// write code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // Your code here\n  int n = points.size();\n  int n1 = n / 4;\n  int n2 = n % 4;\n\n  int k = 0;\n  if (rank == 0) {\n    for (int i = 1; i <= n2; ++i) {\n      Point p = points[i + n1];\n      if (p.x >= 0 && p.y >= 0)\n        ++k;\n      else if (p.x < 0 && p.y >= 0)\n        ++bins[1];\n      else if (p.x < 0 && p.y < 0)\n        ++bins[2];\n      else\n        ++bins[3];\n    }\n  }\n\n  std::vector<Point> mypoints(0);\n\n  for (int i = 0; i < n1; ++i) {\n    Point p = points[i + n1];\n    if (p.x >= 0 && p.y >= 0) {\n      mypoints.push_back(p);\n    }\n  }\n\n  MPI_Scatter(mypoints.data(), mypoints.size(), MPI_DOUBLE_INT, mypoints.data(), mypoints.size(), MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n  for (int i = 0; i < mypoints.size(); ++i) {\n    if (mypoints[i].x < 0 && mypoints[i].y >= 0) {\n      ++bins[1];\n    }\n    else if (mypoints[i].x < 0 && mypoints[i].y < 0) {\n      ++bins[2];\n    }\n    else {\n      ++bins[3];\n    }\n  }\n\n  MPI_Gather(bins.data(), bins.size(), MPI_INT, bins.data(), bins.size(), MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int num_points = points.size();\n\n   int start = rank * num_points / size;\n   int end = (rank + 1) * num_points / size;\n\n   bins = std::array<size_t, 4>();\n\n   for (Point point : points) {\n      if (point.x > 0 && point.y > 0) {\n         ++bins[0];\n      } else if (point.x < 0 && point.y > 0) {\n         ++bins[1];\n      } else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n\n   MPI_Reduce(&bins[0], &bins[0], bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement me...\n}",
            "//\n  // Your code here\n  //\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins[0]);\n    MPI_Comm_rank(MPI_COMM_WORLD, &bins[1]);\n\n}",
            "size_t n = points.size();\n   double x, y;\n\n   // your code here\n\n   // sanity check\n   if (n == 0) {\n      for (int i = 0; i < 4; ++i) {\n         bins[i] = 0;\n      }\n      return;\n   }\n\n   // count x>=0, y>=0\n   x = points[0].x;\n   y = points[0].y;\n   size_t n_x_p_0 = 1;\n   size_t n_y_p_0 = 1;\n   for (int i = 1; i < n; ++i) {\n      if (points[i].x >= x)\n         ++n_x_p_0;\n      if (points[i].y >= y)\n         ++n_y_p_0;\n   }\n   bins[0] = n_x_p_0;\n   bins[1] = n_y_p_0;\n\n   // count x<0, y>=0\n   x = points[0].x;\n   y = points[0].y;\n   size_t n_x_m_0 = 1;\n   size_t n_y_p_0_1 = 1;\n   for (int i = 1; i < n; ++i) {\n      if (points[i].x < x)\n         ++n_x_m_0;\n      if (points[i].y >= y && points[i].x >= x)\n         ++n_y_p_0_1;\n   }\n   bins[2] = n_x_m_0;\n   bins[3] = n_y_p_0_1;\n}",
            "// TODO\n}",
            "std::array<size_t, 4> temp_bins;\n    std::fill(temp_bins.begin(), temp_bins.end(), 0);\n    \n    int my_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    int my_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &my_size);\n\n    if (my_rank == 0) {\n        for (int i = 1; i < my_size; i++) {\n            MPI_Status status;\n            MPI_Recv(temp_bins.data(), 4, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, &status);\n        }\n    }\n\n    for (Point const& point : points) {\n        size_t x = static_cast<size_t>(point.x);\n        size_t y = static_cast<size_t>(point.y);\n\n        if (x > 0 && y > 0) {\n            temp_bins[0]++;\n        } else if (x < 0 && y > 0) {\n            temp_bins[1]++;\n        } else if (x < 0 && y < 0) {\n            temp_bins[2]++;\n        } else {\n            temp_bins[3]++;\n        }\n    }\n\n    if (my_rank == 0) {\n        bins = temp_bins;\n    } else {\n        MPI_Send(temp_bins.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n    }\n}",
            "size_t total_points = points.size();\n    int rank, num_ranks;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n    // The rank 0 node does all the work and distributes the work to the other nodes.\n    // The other nodes just do their own work.\n    if (rank == 0) {\n        for (size_t i = 0; i < total_points; ++i) {\n            // Determine which quadrant the point is in.\n            double x = points[i].x;\n            double y = points[i].y;\n            if (x >= 0 && y >= 0) {\n                bins[0] += 1;\n            } else if (x <= 0 && y >= 0) {\n                bins[1] += 1;\n            } else if (x <= 0 && y <= 0) {\n                bins[2] += 1;\n            } else {\n                bins[3] += 1;\n            }\n        }\n    }\n\n    MPI_Barrier(MPI_COMM_WORLD);\n    MPI_Gather(&bins, 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "int n_points = points.size();\n   int n_bins = bins.size();\n   MPI_Request request[4];\n   MPI_Status status[4];\n   MPI_Datatype Point_type, counts_type;\n   MPI_Type_contiguous(n_points, MPI_DOUBLE, &Point_type);\n   MPI_Type_contiguous(n_bins, MPI_UNSIGNED, &counts_type);\n   MPI_Type_commit(&Point_type);\n   MPI_Type_commit(&counts_type);\n   double* px = (double*)malloc(n_points * sizeof(double));\n   double* py = (double*)malloc(n_points * sizeof(double));\n   for (int i = 0; i < n_points; i++) {\n      px[i] = points[i].x;\n      py[i] = points[i].y;\n   }\n   double** bins_ptr = (double**)malloc(n_bins * sizeof(double*));\n   for (int i = 0; i < n_bins; i++)\n      bins_ptr[i] = (double*)malloc(n_points * sizeof(double));\n   MPI_Scatterv(px, n_points, MPI_DOUBLE, bins_ptr, n_points, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Scatterv(py, n_points, MPI_DOUBLE, bins_ptr, n_points, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // The following is an example of a point-to-point message.\n   // The send and receive buffers are the same.\n   MPI_Isend(bins_ptr, 1, counts_type, 1, 0, MPI_COMM_WORLD, &request[0]);\n   MPI_Irecv(bins_ptr, 1, counts_type, 1, 0, MPI_COMM_WORLD, &request[1]);\n   MPI_Isend(bins_ptr, 1, counts_type, 2, 0, MPI_COMM_WORLD, &request[2]);\n   MPI_Irecv(bins_ptr, 1, counts_type, 2, 0, MPI_COMM_WORLD, &request[3]);\n   MPI_Waitall(4, request, status);\n   MPI_Type_free(&Point_type);\n   MPI_Type_free(&counts_type);\n\n   for (int i = 0; i < n_bins; i++) {\n      bins[i] = 0;\n      for (int j = 0; j < n_points; j++) {\n         if (bins_ptr[i][j] > 0)\n            bins[i]++;\n      }\n      free(bins_ptr[i]);\n   }\n   free(bins_ptr);\n}",
            "// TODO implement this function!\n}",
            "int rank, size;\n\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n  std::vector<size_t> localBins(4, 0);\n  for (Point const& p: points) {\n    if (p.x > 0 && p.y > 0) {\n      localBins[0]++;\n    } else if (p.x < 0 && p.y > 0) {\n      localBins[1]++;\n    } else if (p.x < 0 && p.y < 0) {\n      localBins[2]++;\n    } else if (p.x > 0 && p.y < 0) {\n      localBins[3]++;\n    }\n  }\n\n  std::vector<size_t> recvCounts(size, 4);\n  MPI_Alltoall(localBins.data(), 1, MPI_UNSIGNED_LONG, recvCounts.data(), 1, MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n  std::vector<size_t> recvDispls(size);\n  std::partial_sum(recvCounts.begin(), recvCounts.end() - 1, recvDispls.begin() + 1);\n\n  std::vector<size_t> counts(recvCounts);\n  MPI_Alltoallv(localBins.data(), 1, MPI_UNSIGNED_LONG, counts.data(), recvCounts.data(), recvDispls.data(), MPI_UNSIGNED_LONG, MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    for (size_t i = 0; i < counts.size(); i++) {\n      bins[i] += counts[i];\n    }\n  }\n}",
            "// Your code here\n   bins[0]=0;\n   bins[1]=0;\n   bins[2]=0;\n   bins[3]=0;\n   int size = points.size();\n   int myid;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myid);\n   for (int i = 0; i < size; i++) {\n      if (points[i].x < 0 && points[i].y < 0) {\n         bins[0]++;\n      } else if (points[i].x >= 0 && points[i].y < 0) {\n         bins[1]++;\n      } else if (points[i].x >= 0 && points[i].y >= 0) {\n         bins[2]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bins[3]++;\n      }\n   }\n}",
            "// TODO: implement this method\n   size_t n = points.size();\n   int size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   double start, end;\n   MPI_Barrier(MPI_COMM_WORLD);\n   start = MPI_Wtime();\n   std::vector<Point> my_points;\n   if (rank == 0) {\n      my_points.resize(n);\n      for (int i = 0; i < size; i++) {\n         MPI_Send(points.data() + n/size*i, n/size, MPI_DOUBLE, i, 0, MPI_COMM_WORLD);\n      }\n   } else {\n      MPI_Recv(my_points.data(), n/size, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n   std::vector<int> count(4, 0);\n   for (auto point : my_points) {\n      if (point.x > 0 && point.y > 0) {\n         count[0]++;\n      } else if (point.x > 0 && point.y < 0) {\n         count[1]++;\n      } else if (point.x < 0 && point.y > 0) {\n         count[2]++;\n      } else {\n         count[3]++;\n      }\n   }\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         MPI_Recv(&bins[i], 1, MPI_INT, i+1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      }\n   } else {\n      MPI_Send(&count[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n   MPI_Barrier(MPI_COMM_WORLD);\n   end = MPI_Wtime();\n   double elapsed = end - start;\n   if (rank == 0) {\n      std::cout << \"elapsed time = \" << elapsed << std::endl;\n   }\n   return;\n}",
            "std::vector<size_t> counts(bins.size(), 0);\n\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // TODO\n   std::cout << \"start MPI\";\n   MPI_Datatype point_type;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &point_type);\n   MPI_Type_commit(&point_type);\n   int sendcounts[size], displs[size];\n   sendcounts[rank] = points.size();\n   displs[rank] = 0;\n   MPI_Alltoall(sendcounts, 1, MPI_INT, displs, 1, MPI_INT, MPI_COMM_WORLD);\n   // std::cout << \"finish MPI\";\n\n   int end = 0;\n   int count = 0;\n   for (int i = 0; i < size; i++) {\n      count += displs[i];\n      end += sendcounts[i];\n      displs[i] = count;\n   }\n   // std::cout << \"finish MPI\";\n\n   MPI_Datatype recv_type;\n   MPI_Type_vector(end, 1, 1, point_type, &recv_type);\n   MPI_Type_commit(&recv_type);\n\n   int num_types;\n   MPI_Type_get_envelope(recv_type, &num_types, 0, 0, 0);\n   MPI_Aint lb, extent;\n   MPI_Type_get_extent(recv_type, &lb, &extent);\n   MPI_Aint recvcounts[size];\n   MPI_Aint displs[size];\n   MPI_Get_address(recvcounts, &displs[0]);\n   MPI_Get_address(&recvcounts[size], &displs[1]);\n   for (int i = 2; i < num_types; ++i) {\n      MPI_Get_address(&recvcounts[i], &displs[i]);\n   }\n   displs[0] -= lb;\n   displs[1] -= lb;\n   MPI_Type_create_struct(num_types, recvcounts, displs, &recv_type);\n   MPI_Type_commit(&recv_type);\n\n   std::cout << \"start loop\";\n   for (int i = 0; i < points.size(); i++) {\n      MPI_Pack(&points[i], 1, recv_type, MPI_PACKED, MPI_COMM_WORLD);\n   }\n   // std::cout << \"finish loop\";\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   MPI_Datatype recv_type2;\n   MPI_Type_vector(end, 1, 1, MPI_INT, &recv_type2);\n   MPI_Type_commit(&recv_type2);\n\n   std::vector<int> recvcounts2(size, 0);\n   std::vector<int> displs2(size, 0);\n   for (int i = 0; i < size; i++) {\n      MPI_Pack(&bins[i], 1, recv_type2, MPI_PACKED, MPI_COMM_WORLD);\n   }\n\n   MPI_Barrier(MPI_COMM_WORLD);\n\n   int num_types2;\n   MPI_Type_get_envelope(recv_type2, &num_types2, 0, 0, 0);\n   MPI_Aint lb2, extent2;\n   MPI_Type_get_extent(recv_type2, &lb2, &extent2);\n   MPI_Aint recvcounts3[size];\n   MPI_Aint displs3[size];\n   MPI_Get_address(recvcounts3, &displs3[0]);\n   MPI_Get_address(&recvcounts3[size], &displs3[1]);\n   for (int i = 2; i < num_types2; ++i) {\n      MPI_Get_address(&recvcounts3[i], &displs3[i]);\n   }\n   displs3[0] -= lb2;\n   displs3[1] -= lb2;\n   MPI_Type_create_struct(num_types2, recvcounts3, displs3, &recv_type2);\n   MPI_Type_commit(&recv_type2);\n\n   // std::vector<int> recvcounts2(size);\n   // std::vector<int> displs2(size);\n   // std::cout << \"start loop\";\n   // for (int i = 0; i < size; i++) {\n   //    MPI_",
            "// TODO\n}",
            "size_t n = points.size();\n   if (n == 0) return;\n\n   std::array<size_t, 4> counts;\n   for (auto &count : counts) {\n      count = 0;\n   }\n\n   int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int npoints = n / size;\n   std::vector<Point> local_points;\n\n   if (rank == 0) {\n      for (int r = 1; r < size; r++) {\n         MPI_Send(&npoints, 1, MPI_INT, r, 0, MPI_COMM_WORLD);\n         MPI_Send(&points[npoints * r], npoints, MPI_DOUBLE, r, 0, MPI_COMM_WORLD);\n      }\n   } else {\n      MPI_Recv(&npoints, 1, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      local_points.resize(npoints);\n      MPI_Recv(&local_points[0], npoints, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   for (Point const& point : local_points) {\n      if (point.x > 0) {\n         if (point.y > 0) {\n            counts[0]++;\n         } else {\n            counts[1]++;\n         }\n      } else {\n         if (point.y > 0) {\n            counts[2]++;\n         } else {\n            counts[3]++;\n         }\n      }\n   }\n\n   if (rank == 0) {\n      for (int r = 1; r < size; r++) {\n         int count_r;\n         MPI_Recv(&count_r, 1, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         counts[0] += count_r;\n         MPI_Recv(&count_r, 1, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         counts[1] += count_r;\n         MPI_Recv(&count_r, 1, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         counts[2] += count_r;\n         MPI_Recv(&count_r, 1, MPI_INT, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         counts[3] += count_r;\n      }\n      bins[0] = counts[0];\n      bins[1] = counts[1];\n      bins[2] = counts[2];\n      bins[3] = counts[3];\n   } else {\n      MPI_Send(&counts[0], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(&counts[1], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(&counts[2], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n      MPI_Send(&counts[3], 1, MPI_INT, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// MPI stuff\n   int rank, nprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   int npoints = points.size();\n\n   // Split the points amongst the ranks\n   std::vector<Point> local_points(npoints);\n   int local_npoints = 0;\n   if (rank == 0) {\n      for (auto &p : points) {\n         // Only process points in the range of rank 0\n         if (p.x >= 0 && p.y >= 0) {\n            local_points[local_npoints] = p;\n            ++local_npoints;\n         }\n      }\n   }\n   MPI_Bcast(&local_npoints, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(local_points.data(), local_npoints, MPI_DOUBLE_INT, NULL, 0, MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n   // Split the points amongst the ranks\n\n   // Count the points in each quadrant\n   int n[4] = {0};\n   for (auto &p : local_points) {\n      if (p.x >= 0 && p.y >= 0) {\n         ++n[0];\n      } else if (p.x >= 0 && p.y < 0) {\n         ++n[1];\n      } else if (p.x < 0 && p.y >= 0) {\n         ++n[2];\n      } else if (p.x < 0 && p.y < 0) {\n         ++n[3];\n      }\n   }\n\n   // Store the result\n   MPI_Gather(n, 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; ++i) {\n         std::cout << \"Quadrant \" << i << \": \" << bins[i] << std::endl;\n      }\n   }\n}",
            "int rank, ranks;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tMPI_Comm_size(MPI_COMM_WORLD, &ranks);\n\n\tint x, y;\n\tMPI_Cart_coords(MPI_COMM_WORLD, rank, 2, &x, &y);\n\n\t// TODO: count the number of points in each quadrant using MPI\n\t// Hint: https://www.open-mpi.org/doc/v3.0/man3/MPI_Cart_rank.3.php\n\t// Hint: https://www.open-mpi.org/doc/v3.0/man3/MPI_Cart_shift.3.php\n\tif (rank == 0)\n\t{\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t}\n\telse\n\t{\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t}\n\tMPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_LONG_LONG_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\t// if (rank == 0)\n\t// {\n\t// \tstd::cout << bins[0] << \", \" << bins[1] << \", \" << bins[2] << \", \" << bins[3] << std::endl;\n\t// }\n}",
            "// 1. count all ranks\n    int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // 2. count in parallel\n    if (rank == 0) {\n        // 2.1 calculate counts for rank 0\n        bins = {0, 0, 0, 0};\n        for (auto const& point : points) {\n            if (point.x > 0 && point.y > 0) {\n                ++bins[0];\n            } else if (point.x < 0 && point.y > 0) {\n                ++bins[1];\n            } else if (point.x < 0 && point.y < 0) {\n                ++bins[2];\n            } else {\n                ++bins[3];\n            }\n        }\n        // 2.2 send counts to all other ranks\n        MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    } else {\n        // 2.1 receive counts from rank 0\n        MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n    }\n}",
            "int rank, nproc;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   //...\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   if (rank == 0) {\n      MPI_Scatter(points.data(), points.size() / size, MPI_DOUBLE, bins.data(), points.size() / size, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n\n   MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   const double mid = 0.0;\n   int num_points = points.size();\n   int total_points;\n\n   // TODO\n   MPI_Allreduce(&num_points, &total_points, 1, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n   int num_x = 0;\n   int num_y = 0;\n   int num_l = 0;\n   int num_r = 0;\n\n   for (auto& point : points) {\n      if (point.x < mid) {\n         if (point.y < mid) {\n            num_l++;\n         }\n         else {\n            num_r++;\n         }\n      }\n      else {\n         if (point.y < mid) {\n            num_x++;\n         }\n         else {\n            num_y++;\n         }\n      }\n   }\n   MPI_Reduce(&num_l, &bins[0], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&num_x, &bins[1], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&num_y, &bins[2], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&num_r, &bins[3], 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int x_max = 0;\n   int y_max = 0;\n   int x_min = 0;\n   int y_min = 0;\n\n   int x_index = 0;\n   int y_index = 0;\n\n   int x_temp = 0;\n   int y_temp = 0;\n\n   // Count the points in the quadrant of this rank.\n   for (auto p : points) {\n\n      x_temp = p.x;\n      y_temp = p.y;\n\n      if (x_temp >= 0 && y_temp >= 0) {\n         x_index = 0;\n         y_index = 0;\n      }\n\n      else if (x_temp >= 0 && y_temp <= 0) {\n         x_index = 0;\n         y_index = 1;\n      }\n\n      else if (x_temp <= 0 && y_temp >= 0) {\n         x_index = 1;\n         y_index = 0;\n      }\n\n      else {\n         x_index = 1;\n         y_index = 1;\n      }\n\n      bins[x_index * 2 + y_index]++;\n   }\n\n   MPI_Reduce(&x_max, &x_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&y_max, &y_max, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&x_min, &x_min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&y_min, &y_min, 1, MPI_INT, MPI_MIN, 0, MPI_COMM_WORLD);\n\n   MPI_Reduce(&x_index, &x_index, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&y_index, &y_index, 1, MPI_INT, MPI_MAX, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n\n      int x_temp = 0;\n      int y_temp = 0;\n\n      for (int i = x_min; i <= x_max; i++) {\n         for (int j = y_min; j <= y_max; j++) {\n\n            x_temp = i;\n            y_temp = j;\n\n            if (x_temp >= 0 && y_temp >= 0) {\n               x_index = 0;\n               y_index = 0;\n            }\n\n            else if (x_temp >= 0 && y_temp <= 0) {\n               x_index = 0;\n               y_index = 1;\n            }\n\n            else if (x_temp <= 0 && y_temp >= 0) {\n               x_index = 1;\n               y_index = 0;\n            }\n\n            else {\n               x_index = 1;\n               y_index = 1;\n            }\n\n            bins[x_index * 2 + y_index] = 0;\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "int rank, num_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::array<size_t, 4> my_bins;\n   my_bins.fill(0);\n\n   // TODO: count points in quadrants\n\n   bins = my_bins;\n}",
            "bins = {0, 0, 0, 0};\n\n   // TODO\n}",
            "// TODO: Your code goes here.\n\n   // A few notes:\n   // - bins are in rank 0\n   // - points are the same on every rank\n   // - the number of points in each quadrant are the same on every rank\n   // - bins will contain the counts of the quadrants in order from lower-left to upper-right\n   // - you can get the number of MPI ranks using MPI_Comm_size\n   // - you can get the rank of this process using MPI_Comm_rank\n   // - you can get the MPI communicator being used by this process using MPI_COMM_WORLD\n}",
            "// Your code here\n}",
            "// write implementation here\n}",
            "bins.fill(0);\n   MPI_Comm comm;\n   MPI_Comm_get_parent(&comm);\n   int nprocs;\n   MPI_Comm_size(comm, &nprocs);\n   if (nprocs > 4) {\n      throw std::invalid_argument(\"too many procs\");\n   }\n   int proc;\n   MPI_Comm_rank(comm, &proc);\n   int const row = proc % 2;\n   int const col = proc / 2;\n   for (Point const& point : points) {\n      if (point.x >= 0 && point.y >= 0) {\n         ++bins[0];\n      }\n      else if (point.x < 0 && point.y >= 0) {\n         ++bins[1];\n      }\n      else if (point.x < 0 && point.y < 0) {\n         ++bins[2];\n      }\n      else if (point.x >= 0 && point.y < 0) {\n         ++bins[3];\n      }\n      else {\n         throw std::invalid_argument(\"invalid point\");\n      }\n   }\n   if (proc == 0) {\n      for (int i = 0; i < nprocs; ++i) {\n         int n = 0;\n         MPI_Status status;\n         MPI_Recv(&n, 1, MPI_INT, i, 0, comm, &status);\n         bins[i % 4] += n;\n      }\n   }\n   else {\n      MPI_Send(&bins[row * 2 + col], 1, MPI_INT, 0, 0, comm);\n   }\n   MPI_Comm_disconnect(&comm);\n}",
            "}",
            "size_t size = points.size();\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: Implement this function\n   //...\n   // End of TODO\n}",
            "// TODO: Implement\n}",
            "//TODO: implement\n  size_t n = points.size();\n  MPI_Barrier(MPI_COMM_WORLD);\n  int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n  if (rank == 0) {\n    std::array<size_t, 4> bins_tmp{ 0, 0, 0, 0 };\n    bins_tmp = std::count_if(points.cbegin(), points.cend(), [](Point const& p) {\n      return (p.x >= 0 && p.y >= 0);\n    });\n    MPI_Gather(&bins_tmp, 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gather(&n, 1, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "// TODO: implement this function\n\tbins = { 0, 0, 0, 0 };\n\tfor (int i = 0; i < points.size(); i++)\n\t{\n\t\tif (points[i].x >= 0 && points[i].y >= 0)\n\t\t{\n\t\t\tbins[0]++;\n\t\t}\n\t\telse if (points[i].x >= 0 && points[i].y < 0)\n\t\t{\n\t\t\tbins[1]++;\n\t\t}\n\t\telse if (points[i].x < 0 && points[i].y >= 0)\n\t\t{\n\t\t\tbins[2]++;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tbins[3]++;\n\t\t}\n\t}\n}",
            "// TODO\n}",
            "bins = {};\n\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int commSize;\n   MPI_Comm_size(MPI_COMM_WORLD, &commSize);\n   // TODO: implement the rest\n}",
            "auto n = points.size();\n   size_t const N_QUADS = 4;\n   size_t const N_QUADS_PER_PROC = N_QUADS / MPI_SIZE;\n\n   auto bins_per_proc = std::array<size_t, N_QUADS_PER_PROC> {};\n\n   // TODO\n\n   // Divide the work\n   auto start = n * MPI_RANK / MPI_SIZE;\n   auto end = n * (MPI_RANK + 1) / MPI_SIZE;\n\n   for (auto i = start; i < end; ++i) {\n      auto pt = points[i];\n      if (pt.x > 0) {\n         if (pt.y > 0) {\n            bins_per_proc[0]++;\n         } else {\n            bins_per_proc[1]++;\n         }\n      } else {\n         if (pt.y > 0) {\n            bins_per_proc[2]++;\n         } else {\n            bins_per_proc[3]++;\n         }\n      }\n   }\n\n   // Combine results\n   MPI_Reduce(bins_per_proc.data(), bins.data(), N_QUADS_PER_PROC, MPI_SIZE_T, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "#ifdef DEBUG_EX_7_10\n   std::cout << \"countQuadrants\" << std::endl;\n#endif\n   // TODO: Implement this function.\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int binSize = (points.size() + size - 1) / size;\n   std::vector<Point> localPoints(binSize);\n   MPI_Scatter(&points[0], binSize, MPI_DOUBLE, &localPoints[0], binSize, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   int left = 0, up = 0, right = 0, down = 0;\n   for(size_t i = 0; i < binSize; i++) {\n      if(localPoints[i].x > 0 && localPoints[i].y > 0) {\n         up++;\n      } else if(localPoints[i].x < 0 && localPoints[i].y > 0) {\n         right++;\n      } else if(localPoints[i].x < 0 && localPoints[i].y < 0) {\n         down++;\n      } else if(localPoints[i].x > 0 && localPoints[i].y < 0) {\n         left++;\n      }\n   }\n   MPI_Gather(&up, 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Gather(&right, 1, MPI_INT, bins.data() + 1, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Gather(&down, 1, MPI_INT, bins.data() + 2, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Gather(&left, 1, MPI_INT, bins.data() + 3, 1, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "/* MPI variables */\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   /* number of points in each quadrant */\n   int counts[4];\n\n   /* split work over MPI ranks */\n   int subsize = points.size() / size;\n   int substart = rank * subsize;\n\n   /* copy input to each rank */\n   std::vector<Point> subpoints = std::vector<Point>(subsize);\n   MPI_Scatter(points.data(), subsize, MPI_DOUBLE_INT, subpoints.data(), subsize, MPI_DOUBLE_INT, 0, MPI_COMM_WORLD);\n\n   /* count points in each quadrant */\n   int counter = 0;\n   for (Point p : subpoints) {\n      if (p.x > 0 && p.y > 0)\n         ++counts[0];\n      else if (p.x < 0 && p.y > 0)\n         ++counts[1];\n      else if (p.x < 0 && p.y < 0)\n         ++counts[2];\n      else if (p.x > 0 && p.y < 0)\n         ++counts[3];\n      else\n         ++counter;\n   }\n\n   /* get counts from other ranks */\n   MPI_Gather(counts, 4, MPI_INT, counts, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   /* only rank 0 does the assignment */\n   if (rank == 0) {\n      bins = {counts[0], counts[1], counts[2], counts[3]};\n   }\n\n   /* cleanup */\n   subpoints.clear();\n}",
            "int rank, n_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n   if (n_ranks < 4) {\n      throw std::runtime_error(\"Need at least 4 MPI ranks.\");\n   }\n\n   // Compute the number of points in each quadrant\n   auto numPoints = 1.0 * points.size() / n_ranks;\n   auto numPoints1 = std::ceil(numPoints);\n   auto numPoints2 = std::floor(numPoints);\n   auto numPoints3 = std::ceil(numPoints + 1.0);\n   auto numPoints4 = std::floor(numPoints + 1.0);\n\n   bins[0] = numPoints1;\n   bins[1] = numPoints2;\n   bins[2] = numPoints3 - numPoints2;\n   bins[3] = numPoints4 - numPoints3;\n\n   // Send the points to the appropriate quadrant\n   // Note: We don't care which quadrant the point goes to. We only care that the point goes to exactly one quadrant\n   // so it doesn't matter which rank sends the points.\n   std::vector<int> counts(n_ranks, 0);\n   std::vector<int> displacements(n_ranks + 1, 0);\n   std::iota(std::begin(displacements), std::end(displacements), 0);\n   std::transform(std::begin(bins), std::end(bins), std::begin(counts), [](size_t c) { return static_cast<int>(c); });\n   std::transform(std::begin(counts), std::end(counts), std::begin(displacements), std::next(std::begin(displacements)), std::prev(std::end(displacements)), std::plus<int>());\n\n   auto totalPoints = static_cast<int>(points.size());\n   std::vector<int> sendCounts(n_ranks);\n   std::vector<int> recvCounts(n_ranks);\n   MPI_Scatter(counts.data(), 1, MPI_INT, sendCounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(counts.data(), 1, MPI_INT, recvCounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   std::vector<int> sendDisplacements(n_ranks + 1, 0);\n   std::vector<int> recvDisplacements(n_ranks + 1, 0);\n   MPI_Scatter(displacements.data(), 1, MPI_INT, sendDisplacements.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(displacements.data(), 1, MPI_INT, recvDisplacements.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   std::vector<Point> sendBuffer;\n   std::vector<Point> recvBuffer;\n   sendBuffer.reserve(sendCounts[rank]);\n   recvBuffer.reserve(recvCounts[rank]);\n   for (int i = 0; i < n_ranks; ++i) {\n      for (int j = sendDisplacements[i]; j < sendDisplacements[i + 1]; ++j) {\n         sendBuffer.push_back(points[j]);\n      }\n   }\n   MPI_Scatterv(sendBuffer.data(), sendCounts.data(), sendDisplacements.data(), MPI_DOUBLE, recvBuffer.data(), recvCounts.data(), recvDisplacements.data(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   // Count the points in each quadrant\n   for (int i = 0; i < recvBuffer.size(); ++i) {\n      int quadrant = 0;\n      if (recvBuffer[i].x > 0) {\n         if (recvBuffer[i].y > 0) {\n            quadrant = 0;\n         } else {\n            quadrant = 3;\n         }\n      } else {\n         if (recvBuffer[i].y > 0) {\n            quadrant = 1;\n         } else {\n            quadrant = 2;\n         }\n      }\n      bins[quadrant] += 1;\n   }\n}",
            "}",
            "int rank, numRanks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n\n   // Partition points into 4 quadrants\n   size_t numLocalPoints = points.size();\n   size_t numLocalQuadrants = 0;\n   std::vector<Point> localPoints;\n   if (rank == 0) {\n      // Rank 0 stores the whole list of points\n      localPoints = points;\n      numLocalQuadrants = 4;\n   } else {\n      // The other ranks need to store points in their quadrants\n      numLocalPoints = (numLocalPoints + numRanks - 1) / numRanks;\n      for (size_t i = rank; i < numLocalPoints + rank; i++) {\n         localPoints.push_back(points[i]);\n      }\n      numLocalQuadrants = 1;\n      if (rank == numRanks - 1) {\n         numLocalPoints = points.size() - numLocalPoints * (numRanks - 1);\n      }\n   }\n\n   // Count number of points in each quadrant\n   int xSign, ySign;\n   std::array<size_t, 4> localBins;\n   for (size_t i = 0; i < numLocalQuadrants; i++) {\n      if (localPoints[0].x > 0) {\n         xSign = 1;\n      } else {\n         xSign = -1;\n      }\n      if (localPoints[0].y > 0) {\n         ySign = 1;\n      } else {\n         ySign = -1;\n      }\n      size_t localCount = 0;\n      for (Point const& p : localPoints) {\n         if (xSign * p.x >= 0 && ySign * p.y >= 0) {\n            localCount++;\n         }\n         // Update sign of x and y\n         if (xSign * p.x < 0) {\n            xSign *= -1;\n         }\n         if (ySign * p.y < 0) {\n            ySign *= -1;\n         }\n      }\n      // Store local count in the correct quadrant\n      localBins[i] = localCount;\n   }\n\n   // Add local counts from all ranks\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// YOUR CODE HERE\n\n   // TODO: implement this function\n\n   // Hint: you may want to use MPI_Reduce\n}",
            "double xmin, xmax, ymin, ymax;\n   getXmin(points, xmin);\n   getXmax(points, xmax);\n   getYmin(points, ymin);\n   getYmax(points, ymax);\n\n   size_t width = xmax - xmin;\n   size_t height = ymax - ymin;\n\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n\n   for (auto const &p : points) {\n      // count the point in each quadrant\n      // assume the origin is the lower left corner\n      if (p.x >= xmin && p.x <= xmax && p.y >= ymin && p.y <= ymax) {\n         bins[0] += 1;\n      } else if (p.x < xmin && p.y < ymin) {\n         bins[1] += 1;\n      } else if (p.x > xmax && p.y > ymax) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n\n   return;\n}",
            "auto n_bins = bins.size();\n   double xmin = points[0].x;\n   double xmax = points[0].x;\n   double ymin = points[0].y;\n   double ymax = points[0].y;\n   for (auto const& p : points) {\n      xmin = std::min(xmin, p.x);\n      xmax = std::max(xmax, p.x);\n      ymin = std::min(ymin, p.y);\n      ymax = std::max(ymax, p.y);\n   }\n   double x_span = xmax - xmin;\n   double y_span = ymax - ymin;\n   double x_bin_size = x_span / n_bins;\n   double y_bin_size = y_span / n_bins;\n   std::vector<int> local_bins(n_bins);\n   for (auto const& p : points) {\n      int xbin = (p.x - xmin) / x_bin_size;\n      int ybin = (p.y - ymin) / y_bin_size;\n      local_bins[xbin * n_bins + ybin]++;\n   }\n\n   MPI_Reduce(local_bins.data(), bins.data(), n_bins, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "/* Your code goes here! */\n}",
            "// TODO: Count the number of points in each quadrant.\n}",
            "// TODO\n}",
            "int rank;\n  int size;\n\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  // your code goes here\n\n}",
            "// Your code goes here\n}",
            "int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   // TODO: implement me\n   if (world_rank == 0) {\n      bins = std::array<size_t, 4>{0, 0, 0, 0};\n   }\n}",
            "// TODO\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<Point> local_points = points;\n   if (rank == 0) {\n      // rank 0 gets a copy of points\n      MPI_Bcast(&local_points[0], points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   }\n   else {\n      // other ranks will get the empty vector\n   }\n\n   size_t num_points = local_points.size();\n   size_t n_per_proc = num_points / MPI_Comm_size(MPI_COMM_WORLD);\n   // remainder points in rank (MPI_Comm_rank)\n   size_t remainder = num_points % MPI_Comm_size(MPI_COMM_WORLD);\n   if (rank < remainder) {\n      ++n_per_proc;\n   }\n\n   size_t start = rank * n_per_proc;\n   size_t end = start + n_per_proc;\n\n   for (size_t i = start; i < end; ++i) {\n      if (local_points[i].x >= 0.0 && local_points[i].y >= 0.0) {\n         ++bins[0];\n      }\n      else if (local_points[i].x < 0.0 && local_points[i].y >= 0.0) {\n         ++bins[1];\n      }\n      else if (local_points[i].x < 0.0 && local_points[i].y < 0.0) {\n         ++bins[2];\n      }\n      else if (local_points[i].x >= 0.0 && local_points[i].y < 0.0) {\n         ++bins[3];\n      }\n   }\n\n   if (rank == 0) {\n      // gather the results from rank 0 to all ranks\n      std::vector<size_t> counts(MPI_Comm_size(MPI_COMM_WORLD), 0);\n      std::vector<size_t> displs(MPI_Comm_size(MPI_COMM_WORLD), 0);\n      MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG, &counts[0], 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n      // compute displacements\n      size_t sum = 0;\n      for (size_t i = 0; i < counts.size(); ++i) {\n         displs[i] = sum;\n         sum += counts[i];\n      }\n\n      // gather the data\n      MPI_Gatherv(&bins[0], 4, MPI_UNSIGNED_LONG, &bins[0], &counts[0], &displs[0], MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   }\n   else {\n      // send the results to rank 0\n      MPI_Gather(&bins[0], 4, MPI_UNSIGNED_LONG, nullptr, 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   }\n}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   // each rank will have a complete copy of points\n   std::vector<Point> local_points = points;\n   // find the range of points this rank should work with\n   size_t rank_start = rank * points.size() / num_ranks;\n   size_t rank_end = (rank + 1) * points.size() / num_ranks;\n\n   // rank 0 finds the quadrant for each point\n   if (rank == 0) {\n      for (size_t i = rank_start; i < rank_end; ++i) {\n         // divide the point into quadrants using a point-based (not coordinate-based) method\n         double x = local_points[i].x;\n         double y = local_points[i].y;\n         if (x < 0) {\n            if (y < 0) {\n               bins[0]++;\n            } else {\n               bins[1]++;\n            }\n         } else {\n            if (y < 0) {\n               bins[2]++;\n            } else {\n               bins[3]++;\n            }\n         }\n      }\n   }\n\n   // broadcast results of the local work to all ranks\n   MPI_Bcast(bins.data(), bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "int rank, num_ranks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   int num_points = points.size();\n\n   int sub_size = num_points / num_ranks;\n   int mod_size = num_points % num_ranks;\n\n   int start_point = sub_size * rank;\n   int end_point = start_point + sub_size;\n   if (mod_size > rank) {\n      end_point++;\n   }\n\n   std::vector<Point> my_points(points.begin() + start_point, points.begin() + end_point);\n\n   // Compute quadrant counts here.\n   bins = {0, 0, 0, 0};\n   for (auto &p : my_points) {\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n   }\n\n   // Combine local quadrant counts here.\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// get the number of processes\n    int world_size;\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n    // get the rank\n    int world_rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n    // local copy of points\n    std::vector<Point> local_points(points);\n\n    // partition points into each quadrant\n    std::array<std::vector<Point>, 4> partitions = {{\n        std::vector<Point>(),\n        std::vector<Point>(),\n        std::vector<Point>(),\n        std::vector<Point>()\n    }};\n\n    for (Point& point : local_points) {\n        if (point.x > 0 && point.y > 0) {\n            partitions[0].push_back(point);\n        } else if (point.x > 0 && point.y < 0) {\n            partitions[1].push_back(point);\n        } else if (point.x < 0 && point.y > 0) {\n            partitions[2].push_back(point);\n        } else {\n            partitions[3].push_back(point);\n        }\n    }\n\n    // count quadrants in parallel\n    std::array<size_t, 4> local_bins = {{0, 0, 0, 0}};\n    MPI_Scatter(partitions.data(), 4, MPI_UNSIGNED_LONG_LONG, local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    // add local bins to global bins\n    if (world_rank == 0) {\n        for (size_t i = 0; i < 4; ++i) {\n            bins[i] += local_bins[i];\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// YOUR CODE HERE\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int x, y, quadrant;\n\n   for (int i = rank; i < points.size(); i += size) {\n      x = points[i].x;\n      y = points[i].y;\n\n      if (x >= 0 && y >= 0) {\n         quadrant = 0;\n      } else if (x < 0 && y >= 0) {\n         quadrant = 1;\n      } else if (x < 0 && y < 0) {\n         quadrant = 2;\n      } else {\n         quadrant = 3;\n      }\n\n      bins[quadrant] += 1;\n   }\n\n   MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// todo\n}",
            "MPI_Comm comm = MPI_COMM_WORLD; // get default communicator\n\n   int rank;\n   MPI_Comm_rank(comm, &rank);\n\n   int world_size;\n   MPI_Comm_size(comm, &world_size);\n\n   if(world_size == 1) { // sequential algorithm\n      bins[0] = 0;\n      bins[1] = 0;\n      bins[2] = 0;\n      bins[3] = 0;\n      for(Point const& p : points) {\n         if(p.x > 0 && p.y > 0) {\n            bins[0] += 1;\n         } else if(p.x < 0 && p.y > 0) {\n            bins[1] += 1;\n         } else if(p.x < 0 && p.y < 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n      return;\n   }\n\n   int n = points.size();\n\n   // calculate partition of work for each rank\n   int partition = n / world_size;\n   int remainder = n % world_size;\n\n   // generate random start point for this rank\n   std::random_device rand_device;\n   std::default_random_engine rand_engine(rand_device());\n   std::uniform_int_distribution<int> rand_dist(0, n - 1);\n   int start = rank * partition + std::min(rank, remainder);\n\n   // send work to other ranks\n   std::vector<Point> send_points;\n   send_points.reserve(partition + (rank < remainder? 1 : 0));\n   for(int i = start; i < start + partition + (rank < remainder? 1 : 0); ++i) {\n      send_points.push_back(points[i]);\n   }\n\n   std::vector<int> bins_from_rank(4, 0);\n   MPI_Scatter(&send_points[0], partition + (rank < remainder? 1 : 0), MPI_DOUBLE,\n               &bins_from_rank[0], 4, MPI_INT, 0, comm);\n\n   // calculate bins for this rank\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for(Point const& p : send_points) {\n      if(p.x > 0 && p.y > 0) {\n         bins[0] += 1;\n      } else if(p.x < 0 && p.y > 0) {\n         bins[1] += 1;\n      } else if(p.x < 0 && p.y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n\n   // gather results back from all ranks\n   MPI_Gather(&bins_from_rank[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, comm);\n\n}",
            "// TODO\n}",
            "// TODO: implement me\n   bins = std::array<size_t, 4>();\n}",
            "/* TODO */\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    int n = points.size();\n\n    // split data\n    std::vector<Point> local_points;\n    if (rank == 0) {\n        for (int r = 1; r < nproc; r++) {\n            std::vector<Point> tmp;\n            MPI_Recv(&tmp[0], n, MPI_DOUBLE, r, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n            local_points.insert(local_points.end(), tmp.begin(), tmp.end());\n        }\n    } else {\n        MPI_Send(points.data(), n, MPI_DOUBLE, 0, 0, MPI_COMM_WORLD);\n    }\n\n    bins.fill(0);\n\n    for (auto p : local_points) {\n        if (p.x > 0 && p.y > 0)\n            bins[0]++;\n        else if (p.x < 0 && p.y > 0)\n            bins[1]++;\n        else if (p.x < 0 && p.y < 0)\n            bins[2]++;\n        else\n            bins[3]++;\n    }\n\n    if (rank == 0) {\n        MPI_Reduce(MPI_IN_PLACE, bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n    }\n}",
            "// TODO\n}",
            "/* INSERT YOUR CODE HERE */\n}",
            "// TODO: implement this function\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "cuda",
        "prompt": "struct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use CUDA to count in parallel. The kernel is launched with at least N threads.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\n__global__ void countQuadrants(const Point *points, size_t N, size_t[4] bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "size_t x = threadIdx.x;\n   size_t y = blockIdx.x;\n\n   if (x + y * blockDim.x < N) {\n      double dx = points[x + y * blockDim.x].x;\n      double dy = points[x + y * blockDim.x].y;\n\n      if (dx >= 0.0) {\n         if (dy >= 0.0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (dy >= 0.0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "size_t threadIdx = blockIdx.x*blockDim.x + threadIdx.x;\n   size_t stride = blockDim.x*gridDim.x;\n\n   for (size_t i = threadIdx; i < N; i += stride) {\n      double x = points[i].x, y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         bins[0] += 1;\n      } else if (x < 0 && y >= 0) {\n         bins[1] += 1;\n      } else if (x < 0 && y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "// YOUR CODE HERE\n}",
            "/* YOUR CODE HERE */\n    unsigned int x=0;\n    unsigned int y=0;\n    unsigned int thread_id = threadIdx.x;\n    unsigned int block_id = blockIdx.x;\n    if (thread_id < N) {\n        x = (unsigned int)(points[thread_id].x + 0.5);\n        y = (unsigned int)(points[thread_id].y + 0.5);\n    }\n    unsigned int thread_block = block_id * blockDim.x;\n    if (thread_block < N) {\n        atomicAdd(&bins[x + y*2], 1);\n    }\n}",
            "int bin = 0;\n    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n        const Point p = points[i];\n        if (p.x < 0) bin |= 1;\n        if (p.y < 0) bin |= 2;\n        if (p.x < 0 && p.y < 0) bin |= 4;\n    }\n\n    atomicAdd(&bins[bin], 1);\n}",
            "// FIXME: your code here\n}",
            "// thread index\n   const int id = threadIdx.x;\n\n   // compute the starting index of the block\n   const int stride = N / gridDim.x;\n   const int start = stride * blockIdx.x;\n\n   // compute the end index of the block\n   const int end = stride * (blockIdx.x + 1);\n\n   // compute the starting index of the sub-array\n   const int offset = id + start;\n\n   // sum up the numbers\n   __shared__ size_t sharedBins[4];\n   if (id < 4) {\n      sharedBins[id] = 0;\n   }\n\n   for (int i = offset; i < end; i += blockDim.x) {\n      const Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         atomicAdd(&sharedBins[0], 1);\n      } else if (p.x < 0 && p.y > 0) {\n         atomicAdd(&sharedBins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&sharedBins[2], 1);\n      } else {\n         atomicAdd(&sharedBins[3], 1);\n      }\n   }\n\n   // collect the values from all threads\n   for (int i = id; i < 4; i += blockDim.x) {\n      atomicAdd(&bins[i], sharedBins[i]);\n   }\n}",
            "size_t t = threadIdx.x + blockIdx.x * blockDim.x;\n   if (t < N) {\n      double x = points[t].x;\n      double y = points[t].y;\n      if (x >= 0 && y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// YOUR CODE HERE\n   __syncthreads();\n   // END OF YOUR CODE\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "size_t threadId = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t threadCount = blockDim.x * gridDim.x;\n\n   for (size_t i = threadId; i < N; i += threadCount) {\n      int quadrant = (points[i].x > 0? 0 : 1) * 2 + (points[i].y > 0? 0 : 1);\n      atomicAdd(&(bins[quadrant]), 1);\n   }\n}",
            "// TODO: implement this function\n    return;\n}",
            "int binIndex;\n\n   // TODO\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x > 0 && p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (p.x < 0 && p.y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (p.x > 0 && p.y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "const size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n   const size_t threadCount = blockDim.x * gridDim.x;\n   size_t quadrant = 0;\n   for (size_t i = threadId; i < N; i += threadCount) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0)\n            quadrant = 0;\n         else\n            quadrant = 1;\n      } else {\n         if (points[i].y > 0)\n            quadrant = 2;\n         else\n            quadrant = 3;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx < N) {\n      if (points[idx].x >= 0) {\n         if (points[idx].y >= 0) {\n            bins[0]++;\n         } else {\n            bins[1]++;\n         }\n      } else {\n         if (points[idx].y >= 0) {\n            bins[2]++;\n         } else {\n            bins[3]++;\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "// TODO: Implement\n}",
            "int tid = threadIdx.x;\n    size_t bin = 0;\n    for (size_t i = tid; i < N; i += blockDim.x) {\n        if (points[i].x < 0.0) {\n            if (points[i].y < 0.0) {\n                bin = 1;\n            } else {\n                bin = 2;\n            }\n        } else {\n            if (points[i].y < 0.0) {\n                bin = 3;\n            } else {\n                bin = 0;\n            }\n        }\n        atomicAdd(&bins[bin], 1);\n    }\n}",
            "// TODO: write the kernel\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n}",
            "// YOUR CODE HERE\n   //\n   // YOUR CODE HERE\n}",
            "}",
            "/* TODO: Your code goes here */\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      size_t bin = (x > 0? 0 : 1) + (y > 0? 0 : 2);\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "/* Write your code here */\n}",
            "// TODO\n   __syncthreads();\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   int bsize = blockDim.x;\n   __shared__ size_t counts[4];\n\n   for (int i = tid; i < 4; i += bsize) {\n      counts[i] = 0;\n   }\n   __syncthreads();\n\n   // TODO\n\n   __syncthreads();\n   for (int i = tid; i < 4; i += bsize) {\n      atomicAdd(&bins[i], counts[i]);\n   }\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N)\n      return;\n\n   Point point = points[tid];\n   if (point.x > 0 && point.y > 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (point.x < 0 && point.y > 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (point.x < 0 && point.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (point.x > 0 && point.y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t idx = threadIdx.x;\n   size_t block_size = blockDim.x;\n\n   __shared__ size_t counts[4];\n\n   counts[0] = 0;\n   counts[1] = 0;\n   counts[2] = 0;\n   counts[3] = 0;\n\n   for (size_t i = idx; i < N; i += block_size) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         atomicAdd(&counts[0], 1);\n      } else if (x < 0 && y >= 0) {\n         atomicAdd(&counts[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&counts[2], 1);\n      } else {\n         atomicAdd(&counts[3], 1);\n      }\n   }\n\n   __syncthreads();\n\n   if (idx == 0) {\n      bins[0] = counts[0];\n      bins[1] = counts[1];\n      bins[2] = counts[2];\n      bins[3] = counts[3];\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      if (x > 0 && y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (x > 0 && y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "}",
            "size_t threadId = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t quadrant;\n   if (threadId < N) {\n      quadrant = points[threadId].x > 0? 1 : 0;\n      quadrant += points[threadId].y > 0? 2 : 0;\n   } else {\n      quadrant = 4;\n   }\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\tif (i < N) {\n\t\tif (points[i].x > 0 && points[i].y > 0) {\n\t\t\tatomicAdd(&bins[0], 1);\n\t\t} else if (points[i].x < 0 && points[i].y > 0) {\n\t\t\tatomicAdd(&bins[1], 1);\n\t\t} else if (points[i].x < 0 && points[i].y < 0) {\n\t\t\tatomicAdd(&bins[2], 1);\n\t\t} else {\n\t\t\tatomicAdd(&bins[3], 1);\n\t\t}\n\t}\n}",
            "size_t bin = 0;\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         bin = 0;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         bin = 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bin = 2;\n      } else {\n         bin = 3;\n      }\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int index = threadIdx.x + blockIdx.x * blockDim.x;\n  if (index >= N)\n    return;\n\n  Point p = points[index];\n  if (p.x > 0 && p.y > 0) {\n    atomicAdd(&bins[0], 1);\n  } else if (p.x < 0 && p.y > 0) {\n    atomicAdd(&bins[1], 1);\n  } else if (p.x < 0 && p.y < 0) {\n    atomicAdd(&bins[2], 1);\n  } else {\n    atomicAdd(&bins[3], 1);\n  }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      double x = points[i].x, y = points[i].y;\n      if (x > 0 && y > 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y > 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "size_t tid = threadIdx.x;\n\n   // fill in your code here\n   // hint: you can use multiple threads to calculate the number of points in each quadrant\n\n   if(tid >= N) {\n      return;\n   }\n\n   size_t x = __double2int_rd(points[tid].x);\n   size_t y = __double2int_rd(points[tid].y);\n\n   if(x > 0) {\n      if(y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else {\n         atomicAdd(&bins[1], 1);\n      }\n   } else {\n      if(y > 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int tx = threadIdx.x;\n   int ty = threadIdx.y;\n   int bx = blockIdx.x;\n   int by = blockIdx.y;\n   int tx_n = N / blockDim.x;\n   int ty_n = N / blockDim.y;\n   int n = tx_n * ty_n;\n   int bx_n = tx_n * blockDim.x;\n   int by_n = ty_n * blockDim.y;\n   int i = bx_n * by + bx;\n   int j = by_n * by + by;\n   int k = tx_n * ty + ty;\n   if (i >= n || j >= n) return;\n   int a = 0;\n   int b = 0;\n   for (int p = 0; p < n; p++) {\n      if (i + p * tx < n && j + p * ty < n) {\n         a += (points[i + p * tx].x >= 0 && points[i + p * tx].y >= 0);\n         b += (points[j + p * ty].x < 0 && points[j + p * ty].y < 0);\n      }\n   }\n   atomicAdd(&bins[a + b * 2], 1);\n}",
            "/*TODO: implement the kernel.*/\n    // Your code here\n}",
            "// The code will be written in this function.\n\n   // Get the thread ID.\n   int tid = threadIdx.x;\n\n   // Create shared memory.\n   extern __shared__ size_t sharedBins[];\n\n   // Each thread calculates the count of the cartesian points in its own quadrant.\n   size_t count = 0;\n   for (size_t i = 0; i < N; i++) {\n      // If this point is in the right quadrant, increment the count.\n      if ((points[i].x >= 0.0 && points[i].y >= 0.0) || (points[i].x < 0.0 && points[i].y < 0.0)) {\n         count++;\n      }\n   }\n\n   // Each thread saves its count in shared memory.\n   sharedBins[tid] = count;\n\n   // Each thread synchronizes to wait for all threads to complete.\n   __syncthreads();\n\n   // Each thread adds its count to the final count.\n   for (int i = 0; i < 4; i++) {\n      atomicAdd(&(bins[i]), sharedBins[tid]);\n   }\n}",
            "// Thread ID (a number from 0 to N-1)\n   const size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n   // Only count in the first `N` points\n   if (tid < N) {\n      const Point p = points[tid];\n\n      // Count in quadrant 1 if x > 0 and y > 0\n      if (p.x > 0 && p.y > 0) {\n         atomicAdd(&bins[0], 1);\n      }\n      // Count in quadrant 2 if x < 0 and y > 0\n      else if (p.x < 0 && p.y > 0) {\n         atomicAdd(&bins[1], 1);\n      }\n      // Count in quadrant 3 if x < 0 and y < 0\n      else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      }\n      // Count in quadrant 4 if x > 0 and y < 0\n      else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t tid = threadIdx.x + blockIdx.x*blockDim.x;\n\n   size_t quadrant = 0;\n   if (tid < N) {\n      const Point p = points[tid];\n      if (p.x > 0) {\n         if (p.y > 0) {\n            quadrant = 0;\n         } else {\n            quadrant = 3;\n         }\n      } else {\n         if (p.y > 0) {\n            quadrant = 1;\n         } else {\n            quadrant = 2;\n         }\n      }\n   }\n\n   atomicAdd(&bins[quadrant], 1);\n}",
            "// TODO: Implement kernel\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t tx = threadIdx.x, ty = threadIdx.y;\n   size_t bx = blockIdx.x, by = blockIdx.y;\n\n   size_t i = by * blockDim.y + ty;\n   if (i >= N)\n      return;\n\n   size_t bin = 0;\n   double x = points[i].x, y = points[i].y;\n   if (x > 0 && y > 0) {\n      bin = 1;\n   } else if (x < 0 && y > 0) {\n      bin = 2;\n   } else if (x < 0 && y < 0) {\n      bin = 3;\n   }\n\n   atomicAdd(&bins[bin], 1);\n}",
            "__shared__ size_t sharedBins[4];\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 4; i++) {\n         sharedBins[i] = 0;\n      }\n   }\n   __syncthreads();\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         sharedBins[0] += 1;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         sharedBins[1] += 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         sharedBins[2] += 1;\n      } else {\n         sharedBins[3] += 1;\n      }\n   }\n   __syncthreads();\n   if (threadIdx.x == 0) {\n      bins[0] = sharedBins[0];\n      bins[1] = sharedBins[1];\n      bins[2] = sharedBins[2];\n      bins[3] = sharedBins[3];\n   }\n}",
            "// TODO: Implement\n   // Hint: Each thread should count the number of points in one quadrant.\n   // Hint: Use `atomicAdd` for atomically incrementing the correct bin.\n}",
            "// TODO\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0) {\n         if (y >= 0)\n            atomicAdd(&bins[0], 1);\n         else\n            atomicAdd(&bins[1], 1);\n      } else {\n         if (y >= 0)\n            atomicAdd(&bins[2], 1);\n         else\n            atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      size_t bin = 0;\n      if (x >= 0 && y >= 0)\n         bin = 0;\n      else if (x < 0 && y >= 0)\n         bin = 1;\n      else if (x < 0 && y < 0)\n         bin = 2;\n      else\n         bin = 3;\n\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (i >= N)\n      return;\n\n   Point p = points[i];\n\n   if (p.x > 0) {\n      if (p.y > 0)\n         atomicAdd(&bins[0], 1);\n      else\n         atomicAdd(&bins[1], 1);\n   } else {\n      if (p.y > 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (tid < N) {\n    int quadrant = (points[tid].x >= 0)? ((points[tid].y >= 0)? 0 : 1) : ((points[tid].y >= 0)? 2 : 3);\n    atomicAdd(&bins[quadrant], 1);\n  }\n}",
            "int tid = threadIdx.x;\n   int i = blockIdx.x * blockDim.x + tid;\n   if (i < N) {\n      int quadrant;\n      double x = points[i].x;\n      double y = points[i].y;\n      if (y >= 0) {\n         if (x >= 0) {\n            quadrant = 0;\n         } else {\n            quadrant = 1;\n         }\n      } else {\n         if (x >= 0) {\n            quadrant = 2;\n         } else {\n            quadrant = 3;\n         }\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "size_t binIndex = threadIdx.x + blockIdx.x * blockDim.x;\n   if (binIndex >= 4) return;\n   Point *p = points;\n   size_t count = 0;\n   for (size_t i = 0; i < N; ++i) {\n      double x = p->x, y = p->y;\n      if (x >= 0) {\n         if (y >= 0)\n            binIndex = 0; // top-right quadrant\n         else\n            binIndex = 1; // bottom-right quadrant\n      } else {\n         if (y >= 0)\n            binIndex = 2; // top-left quadrant\n         else\n            binIndex = 3; // bottom-left quadrant\n      }\n      ++count;\n      ++p;\n   }\n   bins[binIndex] = count;\n}",
            "int x = threadIdx.x + blockDim.x * blockIdx.x;\n   int y = threadIdx.y + blockDim.y * blockIdx.y;\n   if (x >= N || y >= N) {\n      return;\n   }\n\n   Point p = points[x + N * y];\n   bool xQuadrant = (p.x >= 0);\n   bool yQuadrant = (p.y >= 0);\n   int quadrant = xQuadrant * 2 + yQuadrant;\n   atomicAdd(&bins[quadrant], 1);\n}",
            "int idx = blockIdx.x*blockDim.x + threadIdx.x;\n  if (idx >= N) return;\n\n  // your code here\n}",
            "// TODO: Your code here\n   int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   int gid = tid + bid * blockDim.x;\n\n   for (int i = gid; i < N; i += blockDim.x * gridDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "}",
            "}",
            "size_t tid = blockDim.x * blockIdx.x + threadIdx.x;\n   size_t i = tid / N;\n   size_t j = tid % N;\n\n   if (i >= 4) {\n      return;\n   }\n\n   if (points[i].x > points[j].x) {\n      atomicAdd(&bins[i], 1);\n   } else if (points[i].x < points[j].x) {\n      atomicAdd(&bins[i + 1], 1);\n   } else if (points[i].y > points[j].y) {\n      atomicAdd(&bins[i + 2], 1);\n   } else {\n      atomicAdd(&bins[i + 3], 1);\n   }\n}",
            "int i = threadIdx.x + blockIdx.x * blockDim.x;\n    if (i < N) {\n        Point p = points[i];\n        if (p.x >= 0) {\n            if (p.y >= 0) {\n                atomicAdd(&bins[0], 1);\n            } else {\n                atomicAdd(&bins[1], 1);\n            }\n        } else {\n            if (p.y >= 0) {\n                atomicAdd(&bins[2], 1);\n            } else {\n                atomicAdd(&bins[3], 1);\n            }\n        }\n    }\n}",
            "// TODO: implement this function\n}",
            "// TODO: implement\n}",
            "size_t idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx < N) {\n      Point p = points[idx];\n      if (p.x >= 0 && p.y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (p.x < 0 && p.y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: Your code goes here\n}",
            "size_t start = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t end = min(start + blockDim.x, N);\n   for (size_t i = start; i < end; i++) {\n      int quadrant = 0;\n      if (points[i].x > 0.0) {\n         quadrant |= 1;\n      }\n      if (points[i].y > 0.0) {\n         quadrant |= 2;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int bin = (threadIdx.x + blockIdx.x * blockDim.x) / (blockDim.x * blockDim.y);\n\n   int i = (threadIdx.x + blockIdx.x * blockDim.x) % (blockDim.x * blockDim.y);\n   size_t count = 0;\n   while (i < N) {\n      Point p = points[i];\n      if (bin == 0) {\n         count += p.x > 0;\n      } else if (bin == 1) {\n         count += p.x <= 0 && p.y > 0;\n      } else if (bin == 2) {\n         count += p.x < 0 && p.y <= 0;\n      } else {\n         count += p.x >= 0 && p.y < 0;\n      }\n      i += blockDim.x * blockDim.y;\n   }\n\n   atomicAdd(&bins[bin], count);\n}",
            "/* Your code here */\n}",
            "__shared__ size_t count[4];\n\tsize_t tid = threadIdx.x;\n\tcount[0] = 0;\n\tcount[1] = 0;\n\tcount[2] = 0;\n\tcount[3] = 0;\n\n\tsize_t stride = blockDim.x;\n\n\tfor (size_t i = tid; i < N; i += stride) {\n\t\tdouble x = points[i].x;\n\t\tdouble y = points[i].y;\n\n\t\tif (x > 0 && y > 0) {\n\t\t\tatomicAdd(&count[0], 1);\n\t\t} else if (x < 0 && y > 0) {\n\t\t\tatomicAdd(&count[1], 1);\n\t\t} else if (x < 0 && y < 0) {\n\t\t\tatomicAdd(&count[2], 1);\n\t\t} else {\n\t\t\tatomicAdd(&count[3], 1);\n\t\t}\n\t}\n\n\tatomicAdd(&bins[0], count[0]);\n\tatomicAdd(&bins[1], count[1]);\n\tatomicAdd(&bins[2], count[2]);\n\tatomicAdd(&bins[3], count[3]);\n}",
            "// TODO: Fill in this function\n   // Hint: you may want to use an additional array to count the number of points in each quadrant\n   // Hint: you may want to use threadIdx.x and blockIdx.x to determine the current point\n}",
            "// TODO: Implement this function.\n    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    if (idx < N) {\n        Point point = points[idx];\n        if (point.x > 0 && point.y > 0) {\n            bins[0] += 1;\n        }\n        else if (point.x < 0 && point.y > 0) {\n            bins[1] += 1;\n        }\n        else if (point.x < 0 && point.y < 0) {\n            bins[2] += 1;\n        }\n        else {\n            bins[3] += 1;\n        }\n    }\n}",
            "__shared__ Point shared[N];\n   // Copy data from global memory to shared memory\n   for (int i = threadIdx.x; i < N; i += blockDim.x) {\n      shared[i] = points[i];\n   }\n   __syncthreads();\n\n   // Check whether each point is in each quadrant\n   int quadrantId = threadIdx.x % 4;\n   for (int i = threadIdx.x; i < N; i += blockDim.x) {\n      if (shared[i].x >= 0 && shared[i].y >= 0)\n         atomicAdd(bins + quadrantId, 1);\n      else if (shared[i].x < 0 && shared[i].y >= 0)\n         atomicAdd(bins + (quadrantId + 1) % 4, 1);\n      else if (shared[i].x < 0 && shared[i].y < 0)\n         atomicAdd(bins + (quadrantId + 2) % 4, 1);\n      else\n         atomicAdd(bins + (quadrantId + 3) % 4, 1);\n   }\n}",
            "// TODO: Fill in the body of this function\n}",
            "const size_t tidx = blockIdx.x*blockDim.x + threadIdx.x;\n\n   if (tidx < N) {\n      double x = points[tidx].x;\n      double y = points[tidx].y;\n\n      if (x >= 0) {\n         if (y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   //...\n}",
            "}",
            "size_t quadrant = 0;\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         quadrant = 1;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         quadrant = 2;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         quadrant = 3;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// Each thread processes one point\n   size_t i = threadIdx.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n\n      if (x >= 0) {\n         if (y >= 0) {\n            // Q1\n            atomicAdd(&bins[0], 1);\n         } else {\n            // Q4\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (y >= 0) {\n            // Q2\n            atomicAdd(&bins[1], 1);\n         } else {\n            // Q3\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "// TODO: implement the CUDA kernel\n}",
            "// Your code here\n}",
            "// Your code here\n}",
            "// YOUR CODE HERE\n   //\n   // Hints:\n   // - Use one thread per point.\n   // - Use a single block, and use the thread id to figure out the quadrant of each point.\n   // - Keep track of the number of points in each quadrant.\n   // - Use atomic functions to update the bins.\n   // - To check whether you have completed the assignment correctly, test it with the provided\n   //   test case, which uses the exact same inputs and outputs.\n}",
            "const int threadID = threadIdx.x + blockIdx.x * blockDim.x;\n   const int numThreads = blockDim.x * gridDim.x;\n\n   for(size_t i=threadID; i<N; i+=numThreads) {\n      const Point p = points[i];\n\n      if(p.x > 0 && p.y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if(p.x < 0 && p.y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if(p.x < 0 && p.y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO\n}",
            "int idx = blockIdx.x * blockDim.x + threadIdx.x;\n   if (idx >= N) {\n      return;\n   }\n   Point p = points[idx];\n   size_t quadrant = 0;\n   if (p.x > 0) {\n      quadrant++;\n   }\n   if (p.y > 0) {\n      quadrant += 2;\n   }\n   atomicAdd(&bins[quadrant], 1);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   for (size_t i = tid; i < N; i += gridDim.x * blockDim.x) {\n      size_t q = (points[i].x >= 0)? ((points[i].y >= 0)? 0 : 1) : ((points[i].y >= 0)? 2 : 3);\n      atomicAdd(&(bins[q]), 1);\n   }\n}",
            "unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   size_t myCount = 0;\n\n   if (tid < N) {\n      Point p = points[tid];\n\n      if (p.x >= 0 && p.y >= 0) {\n         myCount++;\n      } else if (p.x < 0 && p.y >= 0) {\n         myCount++;\n      } else if (p.x < 0 && p.y < 0) {\n         myCount++;\n      } else {\n         myCount++;\n      }\n   }\n\n   atomicAdd(&bins[0], myCount);\n   atomicAdd(&bins[1], myCount);\n   atomicAdd(&bins[2], myCount);\n   atomicAdd(&bins[3], myCount);\n}",
            "__shared__ size_t localBins[4];\n\n    // TODO: Fill in the kernel here to compute localBins\n    __syncthreads();\n\n    atomicAdd(bins+0, localBins[0]);\n    atomicAdd(bins+1, localBins[1]);\n    atomicAdd(bins+2, localBins[2]);\n    atomicAdd(bins+3, localBins[3]);\n}",
            "// TODO: Implement the kernel\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N) return;\n\n   Point p = points[tid];\n   if (p.x >= 0 && p.y >= 0) {\n      atomicAdd(&bins[0], 1);\n   } else if (p.x < 0 && p.y >= 0) {\n      atomicAdd(&bins[1], 1);\n   } else if (p.x < 0 && p.y < 0) {\n      atomicAdd(&bins[2], 1);\n   } else if (p.x >= 0 && p.y < 0) {\n      atomicAdd(&bins[3], 1);\n   }\n}",
            "const size_t tx = threadIdx.x;\n    const size_t bx = blockIdx.x;\n    const size_t by = blockIdx.y;\n    const size_t stride_x = blockDim.x;\n    const size_t stride_y = blockDim.y;\n\n    size_t start = bx * stride_x + tx;\n    size_t end = start + stride_x * stride_y;\n    size_t offset = stride_x * stride_y * by;\n\n    while (start < N) {\n        size_t count = 0;\n        for (size_t i = start; i < end; i++) {\n            Point p = points[offset + i];\n            if (p.x >= 0 && p.y >= 0) {\n                count++;\n            }\n        }\n        atomicAdd(&bins[0], count);\n        atomicAdd(&bins[1], count);\n        atomicAdd(&bins[2], count);\n        atomicAdd(&bins[3], count);\n        start = end;\n        end += stride_x * stride_y;\n    }\n}",
            "int t = threadIdx.x;\n   int b = blockIdx.x;\n   int i = t + b*blockDim.x;\n   if (i < N) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (x < 0 && y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (x < 0 && y < 0)\n         atomicAdd(&bins[2], 1);\n      else if (x >= 0 && y < 0)\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "int i = threadIdx.x; // global thread id\n   int quadrant;\n   for (int tid = i; tid < N; tid += blockDim.x) {\n      quadrant = (points[tid].x > 0? 0 : 1) | (points[tid].y > 0? 0 : 2);\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "int tid = blockIdx.x*blockDim.x+threadIdx.x;\n\tif (tid < N) {\n\t\tconst Point point = points[tid];\n\t\tif (point.x >= 0) {\n\t\t\tif (point.y >= 0) {\n\t\t\t\tatomicAdd(bins+0, 1);\n\t\t\t} else {\n\t\t\t\tatomicAdd(bins+1, 1);\n\t\t\t}\n\t\t} else {\n\t\t\tif (point.y >= 0) {\n\t\t\t\tatomicAdd(bins+2, 1);\n\t\t\t} else {\n\t\t\t\tatomicAdd(bins+3, 1);\n\t\t\t}\n\t\t}\n\t}\n}",
            "// TODO: your code goes here!\n}",
            "// TODO: Implement the kernel.\n}",
            "// TODO\n}",
            "int bin = 0;\n\n    //TODO: implement this\n}",
            "// TODO: Complete this kernel\n}",
            "// TODO: Fill in this function.\n}",
            "size_t thread_id = blockIdx.x * blockDim.x + threadIdx.x;\n   if (thread_id < N) {\n      Point p = points[thread_id];\n      if (p.x >= 0 && p.y >= 0)\n         atomicAdd(&bins[0], 1);\n      else if (p.x < 0 && p.y >= 0)\n         atomicAdd(&bins[1], 1);\n      else if (p.x < 0 && p.y < 0)\n         atomicAdd(&bins[2], 1);\n      else\n         atomicAdd(&bins[3], 1);\n   }\n}",
            "// TODO: Implement this function.\n}",
            "__shared__ size_t private_bins[4];\n\t// Initialize bins to zero.\n\tif (threadIdx.x == 0) {\n\t\tprivate_bins[0] = 0;\n\t\tprivate_bins[1] = 0;\n\t\tprivate_bins[2] = 0;\n\t\tprivate_bins[3] = 0;\n\t}\n\t__syncthreads();\n\t// Loop over all N points and increment the bin count.\n\tfor (size_t i = threadIdx.x + blockIdx.x * blockDim.x; i < N; i += blockDim.x * gridDim.x) {\n\t\t// Only increment if the point is in this quadrant.\n\t\tif (points[i].x >= 0 && points[i].y >= 0) {\n\t\t\tatomicAdd(&private_bins[0], 1);\n\t\t} else if (points[i].x < 0 && points[i].y >= 0) {\n\t\t\tatomicAdd(&private_bins[1], 1);\n\t\t} else if (points[i].x < 0 && points[i].y < 0) {\n\t\t\tatomicAdd(&private_bins[2], 1);\n\t\t} else {\n\t\t\tatomicAdd(&private_bins[3], 1);\n\t\t}\n\t}\n\t// The kernel is launched with at least N threads.\n\t// The output array is 1000000 elements, so there will be at most N threads\n\t// active in the block and gridDim.x blocks in the grid.\n\t// Therefore, the last threads in the grid will not need to sync.\n\t// If the number of threads is not a multiple of the number of blocks,\n\t// we need to sync threads in this kernel to avoid overwriting memory in\n\t// the next kernel.\n\t__syncthreads();\n\tif (threadIdx.x == 0) {\n\t\tbins[0] += private_bins[0];\n\t\tbins[1] += private_bins[1];\n\t\tbins[2] += private_bins[2];\n\t\tbins[3] += private_bins[3];\n\t}\n}",
            "// TODO\n}",
            "//TODO\n}",
            "__shared__ size_t smem[4];\n   smem[threadIdx.x] = 0;\n   __syncthreads();\n\n   for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         smem[0]++;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         smem[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         smem[2]++;\n      } else {\n         smem[3]++;\n      }\n   }\n   __syncthreads();\n\n   size_t sum = smem[0] + smem[1] + smem[2] + smem[3];\n   if (threadIdx.x == 0) {\n      bins[0] = smem[0];\n      bins[1] = smem[1];\n      bins[2] = smem[2];\n      bins[3] = smem[3];\n   }\n}",
            "unsigned int idx = threadIdx.x;\n   unsigned int i = blockIdx.x * blockDim.x + idx;\n   if (i < N) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[i].y > 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// TODO 1: Implement a CUDA kernel to count the number of points in each quadrant.\n   // Hint: Use the `cuda::atomic::add` function.\n\n   // TODO 2: Launch the kernel with at least N threads.\n   // Hint: Use `dim3(blocks, threads, 1)` as the thread configuration.\n}",
            "// TODO\n   return;\n}",
            "// Your code goes here.\n}",
            "// get the global thread id\n   const unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N)\n      return;\n\n   double x = points[tid].x;\n   double y = points[tid].y;\n   int bin_idx = 0;\n   if (x >= 0) {\n      if (y >= 0)\n         bin_idx = 0;\n      else\n         bin_idx = 1;\n   } else {\n      if (y >= 0)\n         bin_idx = 2;\n      else\n         bin_idx = 3;\n   }\n\n   atomicAdd(&bins[bin_idx], 1);\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // count points in quadrants:\n   //   quadrant 1: x > 0 && y > 0\n   //   quadrant 2: x < 0 && y > 0\n   //   quadrant 3: x < 0 && y < 0\n   //   quadrant 4: x > 0 && y < 0\n   for (; tid < N; tid += blockDim.x * gridDim.x) {\n      if (points[tid].x > 0 && points[tid].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[tid].x < 0 && points[tid].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[tid].x < 0 && points[tid].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// YOUR CODE HERE\n}",
            "size_t bin = threadIdx.x < N? (points[threadIdx.x].x > 0? 0 : 1) : 0;\n   bin |= threadIdx.x < N? (points[threadIdx.x].y > 0? 0 : 2) : 0;\n   atomicAdd(&bins[bin], 1);\n}",
            "size_t idx = threadIdx.x + blockIdx.x * blockDim.x;\n   if (idx >= N) {\n      return;\n   }\n   Point p = points[idx];\n   if (p.x > 0) {\n      if (p.y > 0) {\n         bins[0] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   } else {\n      if (p.y > 0) {\n         bins[1] += 1;\n      } else {\n         bins[2] += 1;\n      }\n   }\n}",
            "// TODO: implement this function\n\n   int tid = blockDim.x * blockIdx.x + threadIdx.x;\n\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      if (x >= 0 && y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (x < 0 && y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (x < 0 && y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "int index = threadIdx.x + blockDim.x * blockIdx.x;\n    if (index < N) {\n        double x = points[index].x;\n        double y = points[index].y;\n        if (x > 0 && y > 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (x < 0 && y > 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (x < 0 && y < 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// TODO\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      int quadrant = 0;\n      if (points[tid].x < 0) {\n         if (points[tid].y < 0)\n            quadrant = 1;\n         else\n            quadrant = 2;\n      } else {\n         if (points[tid].y < 0)\n            quadrant = 3;\n         else\n            quadrant = 0;\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "const int tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (p.y >= 0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "size_t threadId = blockDim.x * blockIdx.x + threadIdx.x;\n   if (threadId < N) {\n      if (points[threadId].x > 0 && points[threadId].y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[threadId].x < 0 && points[threadId].y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[threadId].x < 0 && points[threadId].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "// TODO: fill in\n}",
            "// TODO: Your code goes here.\n}",
            "size_t index = threadIdx.x + blockDim.x * blockIdx.x;\n\n   if (index >= N)\n      return;\n\n   Point p = points[index];\n\n   if (p.x > 0 && p.y > 0)\n      atomicAdd(&bins[0], 1);\n   else if (p.x < 0 && p.y > 0)\n      atomicAdd(&bins[1], 1);\n   else if (p.x < 0 && p.y < 0)\n      atomicAdd(&bins[2], 1);\n   else\n      atomicAdd(&bins[3], 1);\n}",
            "size_t idx = threadIdx.x + blockDim.x * blockIdx.x;\n   while (idx < N) {\n      if (points[idx].x >= 0 && points[idx].y >= 0) {\n         bins[0]++;\n      } else if (points[idx].x < 0 && points[idx].y >= 0) {\n         bins[1]++;\n      } else if (points[idx].x < 0 && points[idx].y < 0) {\n         bins[2]++;\n      } else {\n         bins[3]++;\n      }\n      idx += blockDim.x * gridDim.x;\n   }\n}",
            "int tx = blockIdx.x * blockDim.x + threadIdx.x;\n  int ty = blockIdx.y * blockDim.y + threadIdx.y;\n  size_t quadrant = ty * 2 + tx;\n  size_t i = 0;\n  for (; i < N; i += blockDim.x * blockDim.y) {\n    Point point = points[i];\n    if (quadrant == 0 && point.x >= 0 && point.y >= 0)\n      break;\n    if (quadrant == 1 && point.x >= 0 && point.y < 0)\n      break;\n    if (quadrant == 2 && point.x < 0 && point.y < 0)\n      break;\n    if (quadrant == 3 && point.x < 0 && point.y >= 0)\n      break;\n  }\n  bins[quadrant] = i;\n}",
            "// TODO\n}",
            "__shared__ double shared[4];\n   const Point *my_points = points + blockDim.x * blockIdx.x;\n   size_t index = threadIdx.x;\n   size_t my_count = 0;\n   for (size_t i = index; i < N; i += blockDim.x) {\n      double x = my_points[i].x;\n      double y = my_points[i].y;\n      size_t quadrant = 1 + (x > 0) + (x < 0) + 2 * (y > 0) + 4 * (y < 0);\n      my_count += (quadrant < 4);\n   }\n   shared[index] = (double)my_count;\n   __syncthreads();\n   for (size_t i = blockDim.x / 2; i > 0; i /= 2) {\n      if (index < i) {\n         shared[index] += shared[index + i];\n      }\n      __syncthreads();\n   }\n   if (index == 0) {\n      bins[0] = (size_t)shared[0];\n      bins[1] = (size_t)shared[1];\n      bins[2] = (size_t)shared[2];\n      bins[3] = (size_t)shared[3];\n   }\n}",
            "// TODO: implement this\n}",
            "// Your code here\n}",
            "int tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // Only process the `N` elements if there are enough threads.\n   if (tid < N) {\n      // Calculate the quadrant.\n      // The `points[tid]` is in the `[0, 10]` range so `quadrant` will be in the `[0, 3]` range.\n      int quadrant = (int)((points[tid].x + 5) / 10);\n      quadrant = (int)((points[tid].y + 5) / 10) * 2 + quadrant;\n\n      // Increment the quadrant.\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "// 1. determine the index of the thread\n   // 2. get the quadrant of that point\n   // 3. increment the counter of that quadrant\n   //\n   // use atomicAdd() to increment the counter\n}",
            "// Insert your code here\n   // Note: Don't forget to add the thread ID.\n}",
            "// TODO: write a CUDA kernel to count the number of points in each quadrant\n}",
            "__shared__ size_t shared[4];\n    if (threadIdx.x < 4) {\n        shared[threadIdx.x] = 0;\n    }\n    __syncthreads();\n\n    size_t i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i < N) {\n        const Point p = points[i];\n        if (p.x > 0 && p.y > 0) {\n            atomicAdd(&shared[0], 1);\n        } else if (p.x < 0 && p.y > 0) {\n            atomicAdd(&shared[1], 1);\n        } else if (p.x < 0 && p.y < 0) {\n            atomicAdd(&shared[2], 1);\n        } else {\n            atomicAdd(&shared[3], 1);\n        }\n    }\n    __syncthreads();\n\n    if (threadIdx.x < 4) {\n        atomicAdd(&bins[threadIdx.x], shared[threadIdx.x]);\n    }\n}",
            "for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      Point p = points[i];\n      if (p.x >= 0) {\n         if (p.y >= 0) {\n            // Q1\n            atomicAdd(&bins[0], 1);\n         } else {\n            // Q4\n            atomicAdd(&bins[3], 1);\n         }\n      } else {\n         if (p.y >= 0) {\n            // Q2\n            atomicAdd(&bins[1], 1);\n         } else {\n            // Q3\n            atomicAdd(&bins[2], 1);\n         }\n      }\n   }\n}",
            "// TODO: implement me\n}",
            "size_t id = blockDim.x * blockIdx.x + threadIdx.x;\n    if (id < N) {\n        Point p = points[id];\n        if (p.x > 0 && p.y > 0)\n            atomicAdd(&bins[0], 1);\n        else if (p.x < 0 && p.y > 0)\n            atomicAdd(&bins[1], 1);\n        else if (p.x < 0 && p.y < 0)\n            atomicAdd(&bins[2], 1);\n        else\n            atomicAdd(&bins[3], 1);\n    }\n}",
            "// Compute the bin index for each input element.\n   const int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i < N) {\n      const Point p = points[i];\n      const int bin = (p.x > 0 && p.y > 0)? 0 :\n         (p.x < 0 && p.y > 0)? 1 :\n         (p.x < 0 && p.y < 0)? 2 :\n         (p.x > 0 && p.y < 0)? 3 :\n         -1;\n      atomicAdd(&bins[bin], 1);\n   }\n}",
            "// TODO\n}",
            "// TODO\n}",
            "size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (n < N) {\n      Point p = points[n];\n      if (p.x > 0 && p.y > 0) {\n         atomicAdd(bins + 0, 1);\n      } else if (p.x < 0 && p.y > 0) {\n         atomicAdd(bins + 1, 1);\n      } else if (p.x < 0 && p.y < 0) {\n         atomicAdd(bins + 2, 1);\n      } else {\n         atomicAdd(bins + 3, 1);\n      }\n   }\n}",
            "}",
            "// Get the index of this thread.\n   int i = blockDim.x * blockIdx.x + threadIdx.x;\n\n   // If i < N, i is in the range [0, N).\n   if (i < N) {\n      // Find which quadrant the point is in.\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         atomicAdd(&bins[0], 1);\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         atomicAdd(&bins[1], 1);\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         atomicAdd(&bins[2], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   }\n}",
            "size_t id = threadIdx.x;\n   size_t[4] bins_local;\n   bins_local[0] = bins[1];\n   bins_local[1] = bins[2];\n   bins_local[2] = bins[3];\n   bins_local[3] = bins[0];\n\n   for (size_t i = id; i < N; i += blockDim.x) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins_local[0]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins_local[3]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins_local[1]++;\n      } else {\n         bins_local[2]++;\n      }\n   }\n   for (size_t i = 0; i < 4; i++) {\n      bins[i] = bins_local[i];\n   }\n}",
            "int i = blockIdx.x * blockDim.x + threadIdx.x;\n   if (i >= N)\n      return;\n\n   double x = points[i].x;\n   double y = points[i].y;\n\n   if (x > 0) {\n      if (y > 0) {\n         atomicAdd(&bins[0], 1);\n      } else {\n         atomicAdd(&bins[3], 1);\n      }\n   } else {\n      if (y > 0) {\n         atomicAdd(&bins[1], 1);\n      } else {\n         atomicAdd(&bins[2], 1);\n      }\n   }\n}",
            "// Your code goes here!\n}",
            "__shared__ double cache[32 * 32];\n   // TODO: replace this with the correct CUDA thread number.\n   size_t tid = threadIdx.x;\n   size_t num_threads = blockDim.x;\n   size_t gid = blockIdx.x * blockDim.x + tid;\n   size_t x = 0, y = 0;\n\n   // TODO: implement the kernel.\n   cache[tid] = 0;\n}",
            "// YOUR CODE HERE\n   // Hint: use atomicAdd() to increment the count for each quadrant.\n   // Hint: use threadIdx.x to identify the thread within the block\n}",
            "// TODO: Your code goes here\n   // Compute thread ID\n   const size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // TODO: Compute the number of points in each quadrant and store the result in bins\n   // Hints: Use a single for loop and a for loop with a predicate\n   // 1. Check for a valid thread ID by comparing tid to N.\n   // 2. Use a single for loop and a for loop with a predicate to compute the number of points in each quadrant.\n   // 3. Store the result in bins.\n   // 4. Add the total number of points to bins[0].\n\n   // TODO: Add the total number of points to bins[0]\n   // Hints: Use a single for loop to compute the number of points in each quadrant.\n   // 1. Compute the total number of points using the `atomicAdd` intrinsic.\n   // 2. Store the result in bins[0] and add the total number of points to bins[0].\n   // 3. Add the number of points in quadrant 1 to bins[1] using `atomicAdd`.\n   // 4. Add the number of points in quadrant 2 to bins[2] using `atomicAdd`.\n   // 5. Add the number of points in quadrant 3 to bins[3] using `atomicAdd`.\n\n   // TODO: Add the number of points in quadrant 1 to bins[1]\n   // Hints: Use a single for loop to compute the number of points in each quadrant.\n   // 1. Compute the number of points in quadrant 1.\n   // 2. Add the number of points in quadrant 1 to bins[1] using `atomicAdd`.\n   // 3. Add the number of points in quadrant 2 to bins[2] using `atomicAdd`.\n   // 4. Add the number of points in quadrant 3 to bins[3] using `atomicAdd`.\n\n   // TODO: Add the number of points in quadrant 2 to bins[2]\n   // Hints: Use a single for loop to compute the number of points in each quadrant.\n   // 1. Compute the number of points in quadrant 2.\n   // 2. Add the number of points in quadrant 1 to bins[1] using `atomicAdd`.\n   // 3. Add the number of points in quadrant 2 to bins[2] using `atomicAdd`.\n   // 4. Add the number of points in quadrant 3 to bins[3] using `atomicAdd`.\n\n   // TODO: Add the number of points in quadrant 3 to bins[3]\n   // Hints: Use a single for loop to compute the number of points in each quadrant.\n   // 1. Compute the number of points in quadrant 3.\n   // 2. Add the number of points in quadrant 1 to bins[1] using `atomicAdd`.\n   // 3. Add the number of points in quadrant 2 to bins[2] using `atomicAdd`.\n   // 4. Add the number of points in quadrant 3 to bins[3] using `atomicAdd`.\n}",
            "// TODO: implement this function\n   __shared__ double x_min, x_max, y_min, y_max;\n   int tid = blockDim.x*blockIdx.x + threadIdx.x;\n   Point point = points[tid];\n   if (tid == 0){\n      x_min = point.x;\n      y_min = point.y;\n      x_max = point.x;\n      y_max = point.y;\n      for (size_t i = tid+1; i < N; i++){\n         point = points[i];\n         if (point.x < x_min){\n            x_min = point.x;\n         }\n         if (point.x > x_max){\n            x_max = point.x;\n         }\n         if (point.y < y_min){\n            y_min = point.y;\n         }\n         if (point.y > y_max){\n            y_max = point.y;\n         }\n      }\n   }\n   __syncthreads();\n   if (tid == 0){\n      //printf(\"x_min = %f, x_max = %f, y_min = %f, y_max = %f\\n\", x_min, x_max, y_min, y_max);\n   }\n   int num = 0;\n   for (size_t i = tid; i < N; i+=blockDim.x*gridDim.x){\n      if (point.x <= (x_max+x_min)/2 && point.x > (x_max+x_min)/2-1 && point.y <= (y_max+y_min)/2 && point.y > (y_max+y_min)/2-1){\n         num++;\n      }\n   }\n   if (tid == 0){\n      //printf(\"num = %d\\n\", num);\n   }\n   atomicAdd(&bins[0], num);\n   atomicAdd(&bins[1], num);\n   atomicAdd(&bins[2], num);\n   atomicAdd(&bins[3], num);\n   __syncthreads();\n   if (tid == 0){\n      for (size_t i = 0; i < 4; i++){\n         printf(\"bins[%d] = %d\\n\", i, bins[i]);\n      }\n   }\n}",
            "// TODO: add your code here\n    // Hints:\n    // The `threadIdx.x` is the thread ID of the current thread.\n    // The `blockIdx.x` is the block ID of the current block.\n    // The `blockDim.x` is the number of threads in the block.\n    // You can get the global ID with `threadIdx.x + blockIdx.x * blockDim.x`.\n}",
            "size_t thread_id = threadIdx.x + blockIdx.x * blockDim.x;\n   size_t stride = blockDim.x * gridDim.x;\n\n   for (size_t i = thread_id; i < N; i += stride) {\n      int bin_idx = (points[i].x > 0.0? 1 : 0) + (points[i].y > 0.0? 2 : 0);\n      atomicAdd(&bins[bin_idx], 1);\n   }\n}",
            "// TODO\n}",
            "// TODO: Implement this function\n}",
            "int bin = threadIdx.x;\n   __shared__ int counts[4];\n   __syncthreads();\n\n   for (int tid = blockIdx.x * blockDim.x + threadIdx.x; tid < N; tid += blockDim.x * gridDim.x) {\n      if (points[tid].x > 0 && points[tid].y > 0)\n         atomicAdd(&counts[bin], 1);\n      else if (points[tid].x < 0 && points[tid].y > 0)\n         atomicAdd(&counts[bin + 1], 1);\n      else if (points[tid].x < 0 && points[tid].y < 0)\n         atomicAdd(&counts[bin + 2], 1);\n      else if (points[tid].x > 0 && points[tid].y < 0)\n         atomicAdd(&counts[bin + 3], 1);\n   }\n\n   __syncthreads();\n\n   atomicAdd(&bins[bin], counts[bin]);\n   atomicAdd(&bins[bin + 1], counts[bin + 1]);\n   atomicAdd(&bins[bin + 2], counts[bin + 2]);\n   atomicAdd(&bins[bin + 3], counts[bin + 3]);\n}",
            "/* Implement the kernel */\n}",
            "// TODO\n}",
            "}",
            "// Implement\n}",
            "int thread_id = threadIdx.x;\n   int block_id = blockIdx.x;\n   int global_thread_id = thread_id + block_id * blockDim.x;\n\n   // The number of threads in a block\n   int block_size = blockDim.x;\n\n   // The number of blocks in a grid\n   int grid_size = (N + block_size - 1) / block_size;\n\n   int x, y;\n   for (int i = global_thread_id; i < N; i += grid_size * block_size) {\n      // Get the cartesian coordinate of the point\n      x = (int) (points[i].x + 0.5);\n      y = (int) (points[i].y + 0.5);\n\n      // Check for valid coordinates and increment the corresponding counter\n      if (0 <= x && x < 10 && 0 <= y && y < 10) {\n         atomicAdd(&(bins[y * 10 + x]), 1);\n      }\n   }\n}",
            "// TODO: implement this\n}",
            "// Your code here\n}",
            "int tid = threadIdx.x;\n   int bid = blockIdx.x;\n   if (bid >= 4) return;\n\n   int cnt = 0;\n   for (int i = tid; i < N; i += blockDim.x) {\n      Point p = points[i];\n      if ((p.x >= 0 && p.y >= 0 && bid == 0) ||\n          (p.x <= 0 && p.y >= 0 && bid == 1) ||\n          (p.x <= 0 && p.y <= 0 && bid == 2) ||\n          (p.x >= 0 && p.y <= 0 && bid == 3)) {\n         cnt++;\n      }\n   }\n\n   __syncthreads();\n   atomicAdd(&bins[bid], cnt);\n}",
            "// YOUR CODE HERE\n}",
            "int tid = threadIdx.x;\n   __shared__ size_t buffer[1024];\n\n   // TODO: implement this function\n   // count how many points are in each quadrant\n}",
            "// TODO: Implement this function.\n    __syncthreads();\n}",
            "const int QUADRANT_SIZE = 50000;\n\n   int quadrant = blockIdx.x;\n\n   // Each block is responsible for at least QUADRANT_SIZE points\n   int start = quadrant * QUADRANT_SIZE;\n\n   // Compute the end of the array for this quadrant\n   int end = min(N, (quadrant + 1) * QUADRANT_SIZE);\n\n   // Each block needs its own bins array to count in parallel\n   int blockBins[4] = { 0, 0, 0, 0 };\n\n   for (int i = start; i < end; i++) {\n      Point p = points[i];\n\n      // Check which quadrant the point belongs to\n      int quadrant = p.x < 0? (p.y < 0? 0 : 1) : (p.y < 0? 2 : 3);\n\n      // Increment the bin for this quadrant\n      atomicAdd(&blockBins[quadrant], 1);\n   }\n\n   // Merge bins from all blocks into global bins\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 4; i++)\n         atomicAdd(&bins[i], blockBins[i]);\n   }\n}",
            "unsigned int idx = threadIdx.x + blockDim.x * blockIdx.x;\n\n   if (idx < N) {\n      if (points[idx].x >= 0.0) {\n         if (points[idx].y >= 0.0) {\n            atomicAdd(&bins[0], 1);\n         } else {\n            atomicAdd(&bins[1], 1);\n         }\n      } else {\n         if (points[idx].y >= 0.0) {\n            atomicAdd(&bins[2], 1);\n         } else {\n            atomicAdd(&bins[3], 1);\n         }\n      }\n   }\n}",
            "// TODO: implement this kernel\n   return;\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid >= N)\n      return;\n\n   const Point p = points[tid];\n\n   /* TODO: implement */\n\n   /* The following code is to make it compile and run. You should replace it with your own solution. */\n   // int i = p.x >= 0? 0 : 1;\n   // int j = p.y >= 0? 0 : 1;\n   // atomicAdd(bins + 4 * i + 2 * j, 1);\n}",
            "size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n  while (tid < N) {\n    Point p = points[tid];\n    if (p.x > 0 && p.y > 0)\n      ++bins[0];\n    else if (p.x < 0 && p.y > 0)\n      ++bins[1];\n    else if (p.x < 0 && p.y < 0)\n      ++bins[2];\n    else if (p.x > 0 && p.y < 0)\n      ++bins[3];\n    tid += blockDim.x * gridDim.x;\n  }\n}",
            "// YOUR CODE HERE\n    // This should launch 4 threads in total.\n    // Each thread is responsible for computing one quadrant.\n    // For example, thread 0 should count the number of points in the first quadrant.\n    // The result for thread 0 should be stored in bins[0].\n    // bins[0], bins[1], bins[2], and bins[3] should be initialized to zero.\n    // Note that N is the size of the input.\n    // You should use atomic operations to update bins.\n    // You can assume that `points` is a valid pointer and N is valid.\n    // You can assume that sizeof(Point) == 2 * sizeof(double).\n    // You cannot use any library functions or cudaMalloc and cudaFree.\n\n    // The following is the implementation of the kernel. It is recommended that you refer to it while implementing your own kernel.\n    // Note that the following code is not valid. You should write your own code.\n    int tid = blockIdx.x * blockDim.x + threadIdx.x;\n    if (tid == 0) {\n        atomicAdd(&bins[0], 1);\n        atomicAdd(&bins[1], 1);\n    }\n    else if (tid == 1) {\n        atomicAdd(&bins[1], 1);\n        atomicAdd(&bins[2], 1);\n    }\n    else if (tid == 2) {\n        atomicAdd(&bins[0], 1);\n        atomicAdd(&bins[3], 1);\n    }\n    else {\n        atomicAdd(&bins[2], 1);\n        atomicAdd(&bins[3], 1);\n    }\n    return;\n}",
            "// YOUR CODE HERE\n   // Use atomicAdd to update the bins.\n}",
            "// Your code goes here\n}",
            "const size_t tid = threadIdx.x + blockIdx.x * blockDim.x;\n   if (tid < N) {\n      Point p = points[tid];\n      if (p.x > 0 && p.y > 0) bins[0]++;\n      else if (p.x < 0 && p.y > 0) bins[1]++;\n      else if (p.x < 0 && p.y < 0) bins[2]++;\n      else if (p.x > 0 && p.y < 0) bins[3]++;\n   }\n}",
            "// TODO: Fill in your code here.\n}",
            "__shared__ size_t sharedBins[4];\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 4; i++) {\n         sharedBins[i] = 0;\n      }\n   }\n   __syncthreads();\n\n   for (int i = blockIdx.x * blockDim.x + threadIdx.x; i < N; i += gridDim.x * blockDim.x) {\n      const auto &point = points[i];\n      size_t bin = 0;\n      if (point.x > 0 && point.y > 0) {\n         bin = 0;\n      } else if (point.x < 0 && point.y > 0) {\n         bin = 1;\n      } else if (point.x < 0 && point.y < 0) {\n         bin = 2;\n      } else if (point.x > 0 && point.y < 0) {\n         bin = 3;\n      }\n      atomicAdd(&sharedBins[bin], 1);\n   }\n   __syncthreads();\n\n   if (threadIdx.x == 0) {\n      for (int i = 0; i < 4; i++) {\n         atomicAdd(&bins[i], sharedBins[i]);\n      }\n   }\n}",
            "int quadrant = 0;\n   for (int i = blockDim.x * blockIdx.x + threadIdx.x; i < N; i += blockDim.x * gridDim.x) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0) {\n         if (y > 0) {\n            quadrant = 0;\n         } else {\n            quadrant = 3;\n         }\n      } else {\n         if (y > 0) {\n            quadrant = 1;\n         } else {\n            quadrant = 2;\n         }\n      }\n      atomicAdd(&bins[quadrant], 1);\n   }\n}",
            "/* TODO: implement */\n   //__shared__ size_t temp[4];\n   //int tid = threadIdx.x;\n   //temp[0] = 0;\n   //temp[1] = 0;\n   //temp[2] = 0;\n   //temp[3] = 0;\n   //for (size_t i = tid; i < N; i += blockDim.x) {\n   //   if ((points[i].x > 0) && (points[i].y > 0)) {\n   //      temp[0]++;\n   //   } else if ((points[i].x < 0) && (points[i].y > 0)) {\n   //      temp[1]++;\n   //   } else if ((points[i].x < 0) && (points[i].y < 0)) {\n   //      temp[2]++;\n   //   } else if ((points[i].x > 0) && (points[i].y < 0)) {\n   //      temp[3]++;\n   //   }\n   //}\n   //temp[0] = temp[0] + temp[1];\n   //temp[2] = temp[2] + temp[3];\n   //bins[0] = temp[0] + temp[2];\n   //bins[1] = temp[0];\n   //bins[2] = temp[1];\n   //bins[3] = temp[2];\n   bins[0] = 0;\n   bins[1] = 0;\n   bins[2] = 0;\n   bins[3] = 0;\n   for (size_t i = threadIdx.x; i < N; i += blockDim.x) {\n      if ((points[i].x > 0) && (points[i].y > 0)) {\n         bins[0]++;\n      } else if ((points[i].x < 0) && (points[i].y > 0)) {\n         bins[1]++;\n      } else if ((points[i].x < 0) && (points[i].y < 0)) {\n         bins[2]++;\n      } else if ((points[i].x > 0) && (points[i].y < 0)) {\n         bins[3]++;\n      }\n   }\n}",
            "size_t bin = (threadIdx.x + blockIdx.x * blockDim.x) / N;\n   size_t n = blockIdx.x * blockDim.x + threadIdx.x;\n\n   if (bin >= 4) {\n      return;\n   }\n\n   Point p = points[n];\n\n   if (p.x >= 0.0) {\n      bins[bin] = 1;\n   } else {\n      bins[bin] += 1;\n   }\n\n   if (p.y >= 0.0) {\n      bins[bin + 4] = 1;\n   } else {\n      bins[bin + 4] += 1;\n   }\n}",
            "const int i = blockIdx.x * blockDim.x + threadIdx.x;\n\n\tif (i < N) {\n\t\tdouble x = points[i].x;\n\t\tdouble y = points[i].y;\n\n\t\t// TODO: insert kernel code here\n\t}\n}",
            "size_t tid = blockIdx.x * blockDim.x + threadIdx.x;\n   if (tid < N) {\n      double x = points[tid].x;\n      double y = points[tid].y;\n      if (x >= 0 && y >= 0)\n         bins[0]++;\n      else if (x < 0 && y >= 0)\n         bins[1]++;\n      else if (x < 0 && y < 0)\n         bins[2]++;\n      else if (x >= 0 && y < 0)\n         bins[3]++;\n   }\n}",
            "int thread_id = threadIdx.x;\n   int block_id = blockIdx.x;\n   int block_size = blockDim.x;\n   int block_offset = block_id * block_size;\n   int limit = block_offset + block_size;\n\n   int my_bins[4] = {0, 0, 0, 0};\n   for (int i = block_offset + thread_id; i < limit; i += block_size) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x > 0 && y > 0) {\n         my_bins[0]++;\n      } else if (x < 0 && y > 0) {\n         my_bins[1]++;\n      } else if (x < 0 && y < 0) {\n         my_bins[2]++;\n      } else {\n         my_bins[3]++;\n      }\n   }\n\n   bins[0] += my_bins[0];\n   bins[1] += my_bins[1];\n   bins[2] += my_bins[2];\n   bins[3] += my_bins[3];\n}",
            "for (int i=0; i < N; i++) {\n        Point p = points[i];\n        if (p.x >= 0 && p.y >= 0) {\n            atomicAdd(&bins[0], 1);\n        } else if (p.x <= 0 && p.y >= 0) {\n            atomicAdd(&bins[1], 1);\n        } else if (p.x <= 0 && p.y <= 0) {\n            atomicAdd(&bins[2], 1);\n        } else {\n            atomicAdd(&bins[3], 1);\n        }\n    }\n}",
            "// Implement me...\n}",
            "/* Your code goes here */\n   // TODO\n}",
            "int tid = threadIdx.x;\n   size_t local_bins[4] = {0, 0, 0, 0};\n   for (size_t i = tid; i < N; i += blockDim.x) {\n      int quadrant = (points[i].x >= 0)? 1 : 0;\n      quadrant += (points[i].y >= 0)? 2 : 0;\n      local_bins[quadrant]++;\n   }\n   __syncthreads();\n\n   atomicAdd(&bins[0], local_bins[0]);\n   atomicAdd(&bins[1], local_bins[1]);\n   atomicAdd(&bins[2], local_bins[2]);\n   atomicAdd(&bins[3], local_bins[3]);\n}",
            "size_t x = threadIdx.x + blockIdx.x * blockDim.x;\n    size_t y = threadIdx.y + blockIdx.y * blockDim.y;\n\n    if (x < 2 && y < 2) {\n        size_t n = 0;\n\n        for (size_t i = 0; i < N; i++) {\n            double dx = points[i].x - (x + 0.5);\n            double dy = points[i].y - (y + 0.5);\n\n            if (dx * dx + dy * dy < 0.25) {\n                n += 1;\n            }\n        }\n\n        bins[x + 2 * y] = n;\n    }\n}",
            "const int i = threadIdx.x;\n   for (int j = i; j < N; j += blockDim.x) {\n      const double x = points[j].x;\n      const double y = points[j].y;\n      if (x > 0) {\n         if (y > 0)\n            bins[0]++;\n         else\n            bins[1]++;\n      } else {\n         if (y > 0)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n}",
            "// YOUR CODE HERE\n}",
            "int threadId = blockIdx.x * blockDim.x + threadIdx.x;\n\n   // YOUR CODE HERE\n\n   __syncthreads();\n}"
        ]
    },
    {
        "problem_type": "histogram",
        "language": "cpp",
        "name": "18_histogram_count_quadrants",
        "parallelism_model": "mpi+omp",
        "prompt": "#include <mpi.h>\n#include <omp.h>\n\nstruct Point {\n   double x, y;\n};\n\n/* Count the number of cartesian points in each quadrant. The vector points contains a list of `Point` objects.\n   Store the counts in `bins`.\n   Use MPI and OpenMP to count in parallel. Assume MPI has already been initialized.\n   Every rank has a complete copy of image. The result is stored in bins on rank 0.\n   Example:\n\n   input: [{x=1.5, y=0.1}, {x=-3, y=1.1}, {x=5, y=9}, {x=1.5, y=-1}, {x=3, y=-7}, {x=0.1, y=2}]\n   output: [3, 1, 0, 2]\n*/\nvoid countQuadrants(std::vector<Point> const& points, std::array<size_t, 4> &bins) {",
        "temperature": 0.8,
        "top_p": 0.95,
        "do_sample": true,
        "max_new_tokens": 1024,
        "prompted": false,
        "outputs": [
            "// Your code here\n   #pragma omp parallel\n   {\n      std::array<size_t, 4> localBins = {0, 0, 0, 0};\n\n      #pragma omp for\n      for (size_t i = 0; i < points.size(); i++) {\n         if (points[i].x > 0) {\n            localBins[1]++;\n         } else if (points[i].x < 0) {\n            localBins[0]++;\n         }\n         if (points[i].y > 0) {\n            localBins[3]++;\n         } else if (points[i].y < 0) {\n            localBins[2]++;\n         }\n      }\n\n      #pragma omp critical\n      for (size_t i = 0; i < localBins.size(); i++) {\n         bins[i] += localBins[i];\n      }\n   }\n}",
            "/* TODO */\n}",
            "#pragma omp parallel for\n  for (int i = 0; i < 4; ++i) bins[i] = 0;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < points.size(); ++i) {\n    #pragma omp atomic\n    ++bins[quad(points[i].x, points[i].y)];\n  }\n\n  // MPI\n  int rank, n_ranks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n\n  if (rank == 0) {\n    std::vector<size_t> local_bins(bins);\n    MPI_Reduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Reduce(bins.data(), NULL, bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n  }\n}",
            "// Your code here...\n}",
            "// TODO\n}",
            "const int myRank = 0;\n   const int numRanks = 1;\n   int myRow = 0, myCol = 0;\n   const int row = 0;\n   const int col = 0;\n   // YOUR CODE HERE\n   //...\n   //...\n}",
            "// TODO: implement\n}",
            "// YOUR CODE HERE\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    size_t n = points.size();\n    //int numProcs = omp_get_max_threads();\n\n    int numprocs, myrank;\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n    size_t start, end;\n\n    double numChunks = n/(double)numprocs;\n    size_t start1 = numChunks * myrank;\n    size_t end1 = numChunks * (myrank + 1);\n\n    start1 = (start1 < n)? start1 : n - 1;\n    end1 = (end1 < n)? end1 : n - 1;\n\n    size_t count = 0;\n    for(size_t i = start1; i <= end1; i++) {\n        if(points[i].x >= 0 && points[i].y >= 0) {\n            bins[0] = bins[0] + 1;\n        }\n        else if(points[i].x < 0 && points[i].y >= 0) {\n            bins[1] = bins[1] + 1;\n        }\n        else if(points[i].x < 0 && points[i].y < 0) {\n            bins[2] = bins[2] + 1;\n        }\n        else {\n            bins[3] = bins[3] + 1;\n        }\n    }\n}",
            "int rank, size;\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    MPI_Datatype pointType;\n    MPI_Type_contiguous(2, MPI_DOUBLE, &pointType);\n    MPI_Type_commit(&pointType);\n\n    size_t nPoints = points.size();\n    int nPointsPerRank = nPoints / size;\n    std::vector<Point> localPoints(nPointsPerRank);\n    std::vector<size_t> localBins(4);\n    std::vector<int> recvCounts(size);\n    std::vector<int> recvDispls(size);\n    std::vector<size_t> recvBin(4);\n\n    // Send only the `nPointsPerRank` points that this rank owns.\n    MPI_Scatter(&points[0], nPointsPerRank, pointType, &localPoints[0], nPointsPerRank, pointType, 0, MPI_COMM_WORLD);\n\n    // Compute the number of points in each quadrant.\n    #pragma omp parallel for\n    for (size_t i = 0; i < nPointsPerRank; i++) {\n        size_t q = 0;\n        double px = localPoints[i].x;\n        double py = localPoints[i].y;\n        if (px >= 0 && py >= 0) {\n            q = 0;\n        } else if (px < 0 && py >= 0) {\n            q = 1;\n        } else if (px >= 0 && py < 0) {\n            q = 2;\n        } else {\n            q = 3;\n        }\n        __sync_fetch_and_add(&localBins[q], 1);\n    }\n\n    // Send the counts back to rank 0\n    MPI_Gather(&localBins[0], 4, MPI_UNSIGNED_LONG, recvBin.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    // Compute the displacements in the received buffer.\n    for (int i = 0; i < size; i++) {\n        recvDispls[i] = i * 4;\n    }\n\n    MPI_Gatherv(localBins.data(), 4, MPI_UNSIGNED_LONG, recvBin.data(), recvCounts.data(), recvDispls.data(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n    if (rank == 0) {\n        bins = { recvBin[0], recvBin[1], recvBin[2], recvBin[3] };\n    }\n\n    MPI_Type_free(&pointType);\n}",
            "int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // Compute bounds in quadrant\n   const int local_width = 10;\n   const int local_height = 10;\n\n   // Find start index for this rank\n   auto start = points.begin() + (rank * local_width * local_height);\n   auto end = points.begin() + ((rank + 1) * local_width * local_height);\n   auto count = end - start;\n\n   // Create local copy of input vector\n   auto local_points = std::vector<Point>(count);\n   std::copy(start, end, local_points.begin());\n\n   // Compute quadrants\n   auto quadrants = std::array<size_t, 4> {};\n   std::transform(local_points.begin(), local_points.end(), quadrants.begin(), [](Point const& p) {\n      return p.x > 0? (p.y > 0? 1 : 3) : (p.y > 0? 2 : 4);\n   });\n\n   // Reduce counts\n   auto global_quadrants = std::vector<size_t>(quadrants.begin(), quadrants.end());\n   MPI_Reduce(quadrants.data(), global_quadrants.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // Copy counts to result array\n   if (rank == 0) {\n      std::copy(global_quadrants.begin(), global_quadrants.end(), bins.begin());\n   }\n\n}",
            "// TODO: implement\n\n}",
            "// TODO: implement this function\n}",
            "auto const n_points = points.size();\n   size_t const n_ranks = 4;\n   int rank, n_threads;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n   int const my_offset = rank * n_points / n_ranks;\n   int const my_size = (rank + 1) * n_points / n_ranks - my_offset;\n\n   // MPI_Scatterv to get points for this rank\n   std::vector<Point> my_points(my_size);\n   MPI_Scatterv(points.data(), my_size, my_offset, my_points.data(), my_size, MPI_POINT, 0, MPI_COMM_WORLD);\n\n   // OpenMP to count in parallel\n   #pragma omp parallel shared(points, bins, my_points) private(n_threads)\n   {\n      n_threads = omp_get_num_threads();\n      auto const my_start = my_offset;\n      auto const my_end = my_offset + my_size;\n\n      for (auto i = my_start; i < my_end; i++) {\n         // TODO: implement\n      }\n   }\n\n   // MPI_Gatherv to store bins for this rank\n   std::vector<size_t> my_bins(4);\n   MPI_Gatherv(bins.data(), 4, MPI_INT, my_bins.data(), 4, my_offset, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // MPI_Gatherv to store bins from ranks\n   if (rank == 0) {\n      std::vector<size_t> all_bins(n_ranks*4);\n      MPI_Gatherv(my_bins.data(), 4, MPI_INT, all_bins.data(), 4, 0, MPI_INT, 0, MPI_COMM_WORLD);\n      bins = {all_bins[0], all_bins[1], all_bins[2], all_bins[3]};\n   }\n}",
            "int num_threads = 8;\n   omp_set_num_threads(num_threads);\n\n   int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   std::array<std::array<Point, 0>, 4> local_points;\n   //...\n   // TODO: determine how many points are in each quadrant and store in bins\n}",
            "// TODO\n}",
            "const int ranks = 4;\n   const int rank = MPI::COMM_WORLD.Get_rank();\n   const int size = MPI::COMM_WORLD.Get_size();\n   const int dim = points.size();\n\n   int sub_dim = dim / ranks;\n   int remain_dim = dim % ranks;\n   int begin = sub_dim * rank;\n   int end = begin + sub_dim;\n   if (rank < remain_dim) {\n      ++end;\n   }\n\n   bins = std::array<size_t, 4>();\n   bins.fill(0);\n\n   #pragma omp parallel\n   {\n      int local_bin[4] = {0};\n      int local_dim = end - begin;\n\n      for (int i = begin; i < end; ++i) {\n         if (points[i].x > 0 && points[i].y > 0) {\n            ++local_bin[0];\n         } else if (points[i].x < 0 && points[i].y > 0) {\n            ++local_bin[1];\n         } else if (points[i].x < 0 && points[i].y < 0) {\n            ++local_bin[2];\n         } else {\n            ++local_bin[3];\n         }\n      }\n\n      #pragma omp critical\n      {\n         for (int i = 0; i < 4; ++i) {\n            bins[i] += local_bin[i];\n         }\n      }\n   }\n}",
            "std::array<size_t, 4> localBins;\n   std::fill(localBins.begin(), localBins.end(), 0);\n   #pragma omp parallel\n   {\n       int rank = omp_get_thread_num();\n       int size = omp_get_num_threads();\n       int chunks = points.size() / size;\n       int remainder = points.size() % size;\n\n       auto binFunc = [&](Point p, int n) {\n           return (p.x > 0)? 1 : ((p.y > 0)? 2 : 3);\n       };\n\n       if (rank < remainder) {\n           for (int i = rank * (chunks + 1); i < (rank + 1) * (chunks + 1); ++i) {\n               localBins[binFunc(points[i], size)] += 1;\n           }\n       } else {\n           for (int i = rank * chunks + remainder; i < (rank + 1) * chunks + remainder; ++i) {\n               localBins[binFunc(points[i], size)] += 1;\n           }\n       }\n   }\n\n   bins[0] = localBins[0];\n   bins[1] = localBins[1];\n   bins[2] = localBins[2];\n   bins[3] = localBins[3];\n   MPI_Reduce(localBins.data(), bins.data(), 4, MPI_UNSIGNED, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: write this function\n}",
            "// your code here\n}",
            "// TODO: Implement this method.\n}",
            "bins[0] = bins[1] = bins[2] = bins[3] = 0;\n    size_t numPoints = points.size();\n    #pragma omp parallel for reduction(+:bins[0],bins[1],bins[2],bins[3])\n    for(size_t i = 0; i < numPoints; ++i) {\n        if(points[i].x >= 0 && points[i].y >= 0) {\n            ++bins[0];\n        }\n        else if(points[i].x < 0 && points[i].y >= 0) {\n            ++bins[1];\n        }\n        else if(points[i].x < 0 && points[i].y < 0) {\n            ++bins[2];\n        }\n        else {\n            ++bins[3];\n        }\n    }\n}",
            "// TODO\n}",
            "// TODO: implement\n}",
            "// Your code here...\n}",
            "int size, rank, left, right, top, bottom;\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    double left_min = 0.0;\n    double right_max = 0.0;\n    double top_min = 0.0;\n    double bottom_max = 0.0;\n    if (rank == 0) {\n        for (size_t i = 0; i < points.size(); i++) {\n            if (i == 0) {\n                left_min = points[i].x;\n                right_max = points[i].x;\n                top_min = points[i].y;\n                bottom_max = points[i].y;\n            } else {\n                if (points[i].x > right_max) {\n                    right_max = points[i].x;\n                }\n                if (points[i].x < left_min) {\n                    left_min = points[i].x;\n                }\n                if (points[i].y > bottom_max) {\n                    bottom_max = points[i].y;\n                }\n                if (points[i].y < top_min) {\n                    top_min = points[i].y;\n                }\n            }\n        }\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    double left_max = 0.0;\n    double right_min = 0.0;\n    double top_max = 0.0;\n    double bottom_min = 0.0;\n    double left_split = 0.0;\n    double right_split = 0.0;\n    double top_split = 0.0;\n    double bottom_split = 0.0;\n    if (rank == 0) {\n        left_max = left_min;\n        right_min = right_max;\n        top_max = top_min;\n        bottom_min = bottom_max;\n    }\n    MPI_Allreduce(&left_min, &left_max, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n    MPI_Allreduce(&right_max, &right_min, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Allreduce(&top_min, &top_max, 1, MPI_DOUBLE, MPI_MAX, MPI_COMM_WORLD);\n    MPI_Allreduce(&bottom_max, &bottom_min, 1, MPI_DOUBLE, MPI_MIN, MPI_COMM_WORLD);\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    double width = right_max - left_min;\n    double height = top_max - bottom_min;\n    if (rank == 0) {\n        left_split = left_min + width / 2;\n        right_split = right_max - width / 2;\n        top_split = top_min + height / 2;\n        bottom_split = bottom_max - height / 2;\n    }\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    int my_left = 0;\n    int my_right = 0;\n    int my_top = 0;\n    int my_bottom = 0;\n    if (rank == 0) {\n        MPI_Bcast(&left_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&right_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&top_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&bottom_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    } else {\n        MPI_Bcast(&left_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&right_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&top_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n        MPI_Bcast(&bottom_split, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n    }\n    if (rank == 0) {\n        if (left_max <= left_split) {\n            my_left = 1;\n        }\n        if (right_min >= right_split) {\n            my_right = 1;\n        }\n        if (top_max <= top_split) {\n            my_top = 1;\n        }\n        if (bottom_min >= bottom_split) {",
            "// TODO: implement this function\n}",
            "// TODO: Implement me.\n}",
            "/* TODO */\n}",
            "// TODO\n   int comm_rank, comm_size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &comm_rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &comm_size);\n\n   int i_x_min = 1, i_x_max = 1;\n   int i_y_min = 1, i_y_max = 1;\n   int N = points.size();\n   // Find the maximum index for x and y axis\n   for (int i = 0; i < N; i++) {\n      if (points[i].x > 0) {\n         i_x_max = 2;\n      }\n      if (points[i].x < 0) {\n         i_x_min = 0;\n      }\n      if (points[i].y > 0) {\n         i_y_max = 2;\n      }\n      if (points[i].y < 0) {\n         i_y_min = 0;\n      }\n   }\n\n   std::array<int, 4> counts = { 0, 0, 0, 0 };\n   std::array<int, 4> displs = { 0, 0, 0, 0 };\n   int q = 1;\n   for (int x = i_x_min; x < i_x_max; x++) {\n      for (int y = i_y_min; y < i_y_max; y++) {\n         if (x == 1 && y == 1) {\n            continue;\n         }\n         int id = (x == 1) * 2 + y;\n         displs[id] = q;\n         counts[id] = 0;\n         #pragma omp parallel\n         {\n            #pragma omp for schedule(static)\n            for (int i = 0; i < N; i++) {\n               if (points[i].x > 0 && x == 0) {\n                  continue;\n               }\n               if (points[i].x < 0 && x == 2) {\n                  continue;\n               }\n               if (points[i].y > 0 && y == 0) {\n                  continue;\n               }\n               if (points[i].y < 0 && y == 2) {\n                  continue;\n               }\n               #pragma omp atomic\n               counts[id]++;\n            }\n         }\n         q += counts[id];\n      }\n   }\n\n   int sum_counts = 0;\n   MPI_Allreduce(&counts[0], &sum_counts, 4, MPI_INT, MPI_SUM, MPI_COMM_WORLD);\n\n   std::array<size_t, 4> local_bins = { 0, 0, 0, 0 };\n   local_bins[0] = counts[0];\n   local_bins[1] = counts[1];\n   local_bins[2] = counts[2];\n   local_bins[3] = counts[3];\n\n   MPI_Reduce(&counts[0], &local_bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (comm_rank == 0) {\n      bins[0] = local_bins[0];\n      bins[1] = local_bins[1];\n      bins[2] = local_bins[2];\n      bins[3] = local_bins[3];\n   }\n}",
            "// TODO\n   // your code here\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  int num_points = points.size();\n  int num_per_rank = num_points / size;\n  int remainder = num_points % size;\n\n  std::vector<Point> rank_points;\n  rank_points.reserve(num_per_rank);\n\n  if (rank < remainder) {\n    rank_points.assign(points.begin(), points.begin() + num_per_rank + 1);\n  } else {\n    rank_points.assign(points.begin() + rank * num_per_rank + remainder, points.begin() + (rank + 1) * num_per_rank + remainder);\n  }\n\n  int num_points_per_rank = rank_points.size();\n\n  // Compute cartesian coordinates of each point.\n  std::vector<Point> coords(num_points_per_rank);\n  #pragma omp parallel for\n  for (int i = 0; i < num_points_per_rank; i++) {\n    Point const& p = rank_points[i];\n    coords[i].x = p.x;\n    coords[i].y = p.y;\n  }\n\n  // Distribute coordinates to all ranks.\n  std::vector<Point> global_coords;\n  MPI_Allgather(&coords[0], num_points_per_rank * sizeof(Point), MPI_BYTE, &global_coords[0], num_points_per_rank * sizeof(Point), MPI_BYTE, MPI_COMM_WORLD);\n\n  // Count number of points in each quadrant.\n  #pragma omp parallel for\n  for (int i = 0; i < num_points_per_rank; i++) {\n    Point const& p = global_coords[i];\n    if (p.x >= 0 && p.y >= 0) {\n      bins[0]++;\n    } else if (p.x < 0 && p.y >= 0) {\n      bins[1]++;\n    } else if (p.x >= 0 && p.y < 0) {\n      bins[2]++;\n    } else {\n      bins[3]++;\n    }\n  }\n\n  MPI_Reduce(&bins[0], &bins[0], 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "/* YOUR CODE HERE */\n   int myRank, numRanks;\n   int numThreads;\n   MPI_Comm_size(MPI_COMM_WORLD, &numRanks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &myRank);\n   omp_set_num_threads(omp_get_max_threads());\n\n   int leftRank = (myRank + numRanks - 1) % numRanks;\n   int rightRank = (myRank + 1) % numRanks;\n\n   bins.fill(0);\n\n   // Count points in left rank\n   if (myRank == 0) {\n      MPI_Send(points.data(), points.size() * sizeof(Point), MPI_BYTE, leftRank, 0, MPI_COMM_WORLD);\n      MPI_Send(points.data(), points.size() * sizeof(Point), MPI_BYTE, rightRank, 0, MPI_COMM_WORLD);\n   }\n   else if (myRank == leftRank) {\n      std::vector<Point> leftPoints(points.size());\n      MPI_Recv(leftPoints.data(), points.size() * sizeof(Point), MPI_BYTE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[0] = std::count_if(leftPoints.begin(), leftPoints.end(), [](Point const& p) { return p.x < 0 && p.y < 0; });\n   }\n   else if (myRank == rightRank) {\n      std::vector<Point> rightPoints(points.size());\n      MPI_Recv(rightPoints.data(), points.size() * sizeof(Point), MPI_BYTE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[1] = std::count_if(rightPoints.begin(), rightPoints.end(), [](Point const& p) { return p.x >= 0 && p.y < 0; });\n   }\n\n   // Count points in right rank\n   if (myRank == 0) {\n      MPI_Send(points.data(), points.size() * sizeof(Point), MPI_BYTE, rightRank, 0, MPI_COMM_WORLD);\n      MPI_Send(points.data(), points.size() * sizeof(Point), MPI_BYTE, leftRank, 0, MPI_COMM_WORLD);\n   }\n   else if (myRank == leftRank) {\n      std::vector<Point> leftPoints(points.size());\n      MPI_Recv(leftPoints.data(), points.size() * sizeof(Point), MPI_BYTE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[2] = std::count_if(leftPoints.begin(), leftPoints.end(), [](Point const& p) { return p.x < 0 && p.y >= 0; });\n   }\n   else if (myRank == rightRank) {\n      std::vector<Point> rightPoints(points.size());\n      MPI_Recv(rightPoints.data(), points.size() * sizeof(Point), MPI_BYTE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n      bins[3] = std::count_if(rightPoints.begin(), rightPoints.end(), [](Point const& p) { return p.x >= 0 && p.y >= 0; });\n   }\n\n   // Count points in my rank\n   size_t numPoints = points.size();\n   bins[0] = std::count_if(points.begin(), points.end(), [](Point const& p) { return p.x < 0 && p.y < 0; });\n   bins[1] = std::count_if(points.begin(), points.end(), [](Point const& p) { return p.x >= 0 && p.y < 0; });\n   bins[2] = std::count_if(points.begin(), points.end(), [](Point const& p) { return p.x < 0 && p.y >= 0; });\n   bins[3] = std::count_if(points.begin(), points.end(), [](Point const& p) { return p.x >= 0 && p.y >= 0; });\n\n   MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement\n\n}",
            "std::array<int, 4> sendcounts;\n    std::array<int, 4> displs;\n\n    // distribute points\n    const size_t n = points.size();\n    sendcounts[0] = std::count_if(points.begin(), points.end(), [&](Point const& p) {return p.x <= 0; });\n    sendcounts[1] = std::count_if(points.begin(), points.end(), [&](Point const& p) {return p.x > 0 && p.y >= 0; });\n    sendcounts[2] = std::count_if(points.begin(), points.end(), [&](Point const& p) {return p.x > 0 && p.y < 0; });\n    sendcounts[3] = std::count_if(points.begin(), points.end(), [&](Point const& p) {return p.x <= 0 && p.y < 0; });\n\n    displs[0] = 0;\n    displs[1] = sendcounts[0];\n    displs[2] = sendcounts[0] + sendcounts[1];\n    displs[3] = sendcounts[0] + sendcounts[1] + sendcounts[2];\n\n    int rank = 0, numprocs = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n\n    std::vector<size_t> recvcounts(numprocs);\n    std::vector<size_t> recvdispls(numprocs);\n    MPI_Scatterv(sendcounts.data(), sendcounts.data() + 4, displs.data(), MPI_INT, recvcounts.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n    std::partial_sum(recvcounts.begin(), recvcounts.end(), recvdispls.begin());\n\n    std::vector<Point> sendbuf(recvcounts[rank]);\n    std::copy(points.begin() + displs[rank], points.begin() + displs[rank] + recvcounts[rank], sendbuf.begin());\n\n    std::vector<size_t> recvbuf(recvcounts[rank]);\n    MPI_Allgatherv(sendbuf.data(), sendbuf.size(), MPI_DOUBLE, recvbuf.data(), recvcounts.data(), recvdispls.data(), MPI_DOUBLE, MPI_COMM_WORLD);\n\n    bins[0] = recvbuf[0];\n    bins[1] = recvbuf[1];\n    bins[2] = recvbuf[2];\n    bins[3] = recvbuf[3];\n}",
            "// TODO: write code here\n}",
            "MPI_Datatype MPI_Point = MPI_DOUBLE;\n\n   // TODO: Use MPI_Gather to collect counts of each quadrant from each rank into `bins` on rank 0\n}",
            "/* Your code here */\n}",
            "// TODO: implement this function\n}",
            "/* Compute the quadrant for each point and count the points in each quadrant */\n}",
            "}",
            "size_t n = points.size();\n    bins = {0, 0, 0, 0};\n#pragma omp parallel default(shared)\n{\n    int rank = omp_get_thread_num();\n    int threads = omp_get_num_threads();\n#pragma omp for schedule(static)\n    for (size_t i=0; i<n; i++) {\n        int quadrant = -1;\n        if (points[i].x > 0) {\n            if (points[i].y > 0) {\n                quadrant = 0;\n            } else {\n                quadrant = 1;\n            }\n        } else {\n            if (points[i].y > 0) {\n                quadrant = 2;\n            } else {\n                quadrant = 3;\n            }\n        }\n        bins[quadrant]++;\n    }\n}\n}",
            "int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   std::vector<size_t> local_bins(4, 0);\n\n   #pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < points.size(); i++) {\n      #pragma omp simd\n      for (int j = 0; j < 4; j++) {\n         int x = (int) points[i].x;\n         int y = (int) points[i].y;\n         if ((x & (1 << (1-j))) && (y & (1 << (1-j)))) {\n            local_bins[j]++;\n         }\n      }\n   }\n\n   std::vector<size_t> recvcounts(world_size);\n   std::vector<size_t> displs(world_size);\n\n   MPI_Gather(&local_bins[0], local_bins.size(), MPI_UNSIGNED_LONG, &recvcounts[0], local_bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   displs[0] = 0;\n   for (int i = 1; i < world_size; i++) {\n      displs[i] = displs[i-1] + recvcounts[i-1];\n   }\n\n   if (world_rank == 0) {\n      bins = {0, 0, 0, 0};\n      for (int i = 0; i < world_size; i++) {\n         for (int j = 0; j < 4; j++) {\n            bins[j] += recvcounts[i];\n         }\n      }\n   }\n}",
            "// TO DO: implement this function\n}",
            "// YOUR CODE HERE\n}",
            "int rank, size;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n  if (rank == 0) {\n    bins = std::array<size_t, 4>();\n  }\n\n  // Your code here\n\n  // Wait for all other ranks to finish the computation\n  MPI_Barrier(MPI_COMM_WORLD);\n\n  if (rank == 0) {\n    // Merge bins from all ranks into bins on rank 0\n    for (int r = 1; r < size; r++) {\n      MPI_Status status;\n      MPI_Recv(&(bins[0]), 4, MPI_UNSIGNED_LONG_LONG, r, 0, MPI_COMM_WORLD, &status);\n    }\n  } else {\n    // Send bins to rank 0\n    MPI_Send(&(bins[0]), 4, MPI_UNSIGNED_LONG_LONG, 0, 0, MPI_COMM_WORLD);\n  }\n}",
            "bins.fill(0);\n\n   // YOUR CODE HERE\n\n   std::array<size_t, 4> bins_local;\n   bins_local.fill(0);\n\n   const int nthreads = omp_get_max_threads();\n   #pragma omp parallel\n   {\n      int myid = omp_get_thread_num();\n      int nthreads = omp_get_num_threads();\n\n      const int step = (int)(points.size() / nthreads);\n      const int start = step * myid;\n      const int end = (myid == nthreads - 1? points.size() : step * (myid + 1));\n\n      for (int i = start; i < end; ++i) {\n         int x = (int)(points[i].x);\n         int y = (int)(points[i].y);\n\n         if (x >= 0 && y >= 0)\n            bins_local[0]++;\n         else if (x < 0 && y >= 0)\n            bins_local[1]++;\n         else if (x >= 0 && y < 0)\n            bins_local[2]++;\n         else if (x < 0 && y < 0)\n            bins_local[3]++;\n      }\n   }\n\n   // std::array<size_t, 4> bins_local;\n   // MPI_Allreduce(bins_local.data(), bins.data(), bins_local.size(), MPI_UNSIGNED, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n    bins = std::array<size_t, 4>();\n}",
            "const int my_rank = 0;\n    const int my_size = 1;\n    std::array<int, 4> quadrants = {1, 1, 0, 0};\n    std::array<size_t, 4> local_bins;\n    for (int i = 0; i < 4; i++) {\n        local_bins[i] = 0;\n    }\n\n    /* TODO: implement */\n    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &my_size);\n\n    /*\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            local_bins[quadrants[0]]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            local_bins[quadrants[1]]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            local_bins[quadrants[2]]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            local_bins[quadrants[3]]++;\n        }\n    }\n    */\n\n    /*\n    #pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            #pragma omp atomic\n            local_bins[quadrants[0]]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            #pragma omp atomic\n            local_bins[quadrants[1]]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            #pragma omp atomic\n            local_bins[quadrants[2]]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            #pragma omp atomic\n            local_bins[quadrants[3]]++;\n        }\n    }\n    */\n\n    MPI_Gather(local_bins.data(), 4, MPI_UNSIGNED_LONG_LONG, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n\n    /*\n    if (my_rank == 0) {\n        std::cout << \"bins: \";\n        for (int i = 0; i < 4; i++) {\n            std::cout << bins[i] << \", \";\n        }\n        std::cout << std::endl;\n    }\n    */\n}",
            "// TODO\n}",
            "bins = std::array<size_t, 4>{};\n   #pragma omp parallel\n   {\n      #pragma omp for nowait\n      for (auto const& p : points) {\n         if (p.x < 0 && p.y < 0)\n            bins[0]++;\n         else if (p.x >= 0 && p.y < 0)\n            bins[1]++;\n         else if (p.x < 0 && p.y >= 0)\n            bins[2]++;\n         else\n            bins[3]++;\n      }\n   }\n\n   MPI_Reduce(MPI_IN_PLACE, bins.data(), bins.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement this function\n    bins[0] = bins[1] = bins[2] = bins[3] = 0;\n\n    for (int i = 0; i < points.size(); i++) {\n        if (points[i].x < 0 && points[i].y < 0) {\n            bins[0] += 1;\n        } else if (points[i].x < 0 && points[i].y >= 0) {\n            bins[1] += 1;\n        } else if (points[i].x >= 0 && points[i].y >= 0) {\n            bins[2] += 1;\n        } else {\n            bins[3] += 1;\n        }\n    }\n}",
            "// TODO: implement me\n}",
            "// your code goes here\n\n}",
            "// YOUR CODE HERE\n\n   // END OF YOUR CODE\n}",
            "bins.fill(0);\n\n   // TODO: implement me\n}",
            "// TODO: implement\n}",
            "// Count number of points in each quadrant\n   #pragma omp parallel for reduction(+:bins)\n   for (size_t i=0; i<points.size(); i++) {\n      // compute quadrant\n      if (points[i].x >= 0) {\n         if (points[i].y >= 0) {\n            bins[0] += 1;\n         } else {\n            bins[1] += 1;\n         }\n      } else {\n         if (points[i].y >= 0) {\n            bins[2] += 1;\n         } else {\n            bins[3] += 1;\n         }\n      }\n   }\n}",
            "// YOUR CODE HERE\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int n = points.size();\n   double x_min = 0, x_max = 0, y_min = 0, y_max = 0;\n   std::vector<Point> points_local;\n   // Compute the x and y ranges.\n   if (rank == 0) {\n      for (int i = 0; i < n; i++) {\n         if (i == 0) {\n            x_min = points[i].x;\n            x_max = points[i].x;\n            y_min = points[i].y;\n            y_max = points[i].y;\n         } else {\n            if (points[i].x < x_min) {\n               x_min = points[i].x;\n            } else if (points[i].x > x_max) {\n               x_max = points[i].x;\n            }\n            if (points[i].y < y_min) {\n               y_min = points[i].y;\n            } else if (points[i].y > y_max) {\n               y_max = points[i].y;\n            }\n         }\n      }\n   }\n   // Broadcast the x and y ranges to all ranks.\n   MPI_Bcast(&x_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&x_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&y_min, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n   MPI_Bcast(&y_max, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   double x_width = x_max - x_min;\n   double y_width = y_max - y_min;\n   int x_bins = (x_width / size) + 1;\n   int y_bins = (y_width / size) + 1;\n   int x_rank = rank % size;\n   int y_rank = rank / size;\n   for (int i = 0; i < n; i++) {\n      int x_index = (points[i].x - x_min) / x_width * x_bins;\n      int y_index = (points[i].y - y_min) / y_width * y_bins;\n      int rank_index = (x_index + y_index * size);\n      if (rank_index == rank) {\n         points_local.push_back(points[i]);\n      }\n   }\n\n   int m = points_local.size();\n   size_t *counts = new size_t[m];\n   int n_threads = omp_get_max_threads();\n   int *bins_local = new int[n_threads];\n   std::fill(bins_local, bins_local + n_threads, 0);\n   omp_set_num_threads(n_threads);\n#pragma omp parallel\n#pragma omp for schedule(static)\n   for (int i = 0; i < m; i++) {\n      int x_index = (points_local[i].x - x_min) / x_width * x_bins;\n      int y_index = (points_local[i].y - y_min) / y_width * y_bins;\n      int rank_index = (x_index + y_index * size);\n      if (rank_index == rank) {\n         bins_local[omp_get_thread_num()]++;\n      }\n   }\n\n   MPI_Reduce(bins_local, counts, n_threads, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      bins[0] = counts[0];\n      bins[1] = counts[1];\n      bins[2] = counts[2];\n      bins[3] = counts[3];\n   }\n   delete[] counts;\n   delete[] bins_local;\n}",
            "// TODO: implement me\n}",
            "// TODO: implement\n}",
            "// your code here\n}",
            "bins[0] = 0; // top left\n   bins[1] = 0; // top right\n   bins[2] = 0; // bottom left\n   bins[3] = 0; // bottom right\n\n   // your code here\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0]++;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1]++;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2]++;\n      } else if (points[i].x > 0 && points[i].y < 0) {\n         bins[3]++;\n      }\n   }\n\n   if (0 == MPI_Rank()) {\n      // std::cout << \"Output from rank \" << MPI_Rank() << \" bins[0] \" << bins[0] << std::endl;\n      // std::cout << \"Output from rank \" << MPI_Rank() << \" bins[1] \" << bins[1] << std::endl;\n      // std::cout << \"Output from rank \" << MPI_Rank() << \" bins[2] \" << bins[2] << std::endl;\n      // std::cout << \"Output from rank \" << MPI_Rank() << \" bins[3] \" << bins[3] << std::endl;\n   }\n}",
            "// TODO\n}",
            "bins = {0, 0, 0, 0};\n\n   // TODO: your code goes here\n}",
            "bins = std::array<size_t, 4>();\n\n   // TODO implement\n}",
            "// Compute the number of processes\n    int num_procs = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n    // Compute the rank of this process\n    int rank = 0;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // Get the number of elements in each quadrant\n    std::array<size_t, 4> local_bins;\n    local_bins.fill(0);\n#pragma omp parallel for\n    for (int i = 0; i < points.size(); i++) {\n        // Determine which quadrant each point is in\n        int quadrant = 0;\n        if (points[i].x >= 0 && points[i].y >= 0)\n            quadrant = 1;\n        else if (points[i].x < 0 && points[i].y >= 0)\n            quadrant = 2;\n        else if (points[i].x < 0 && points[i].y < 0)\n            quadrant = 3;\n        else if (points[i].x >= 0 && points[i].y < 0)\n            quadrant = 4;\n\n        // Increment the count for the appropriate quadrant\n        local_bins[quadrant]++;\n    }\n\n    // Now we have the count for each quadrant, we need to reduce that to the root\n    MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "//\n   // Insert your code here.\n   //\n   MPI_Comm comm = MPI_COMM_WORLD;\n   int rank, size;\n   MPI_Comm_rank(comm, &rank);\n   MPI_Comm_size(comm, &size);\n   // Divide the points equally among processes\n   // Get number of points assigned to each process\n   int n = points.size();\n   int part = n / size;\n   int n_l = part + 1; // the first `part` points are assigned to the left process\n   int n_r = part; // the last `part` points are assigned to the right process\n   // Get number of points in left process\n   int n_ll = (n_l + rank) / size;\n   int n_lr = (n_l - n_ll + rank) / size;\n   // Get number of points in right process\n   int n_rl = (n_r + rank) / size;\n   int n_rr = (n_r - n_rl + rank) / size;\n   int n_l_max = std::max(n_ll, n_lr);\n   int n_r_max = std::max(n_rl, n_rr);\n   int n_max = std::max(n_l_max, n_r_max);\n   // Count number of points in each quadrant\n   size_t l_count = 0, r_count = 0, t_count = 0, b_count = 0;\n   int n_p = points.size();\n   //printf(\"Rank %d: n_p=%d, n_l=%d, n_r=%d, n_l_max=%d, n_r_max=%d, n_max=%d\\n\", rank, n_p, n_l, n_r, n_l_max, n_r_max, n_max);\n   for (int i = 0; i < n_p; i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x < 0) {\n         if (y > 0) {\n            r_count++;\n         } else {\n            l_count++;\n         }\n      } else {\n         if (y > 0) {\n            t_count++;\n         } else {\n            b_count++;\n         }\n      }\n   }\n   //printf(\"Rank %d: l_count=%d, r_count=%d, t_count=%d, b_count=%d\\n\", rank, l_count, r_count, t_count, b_count);\n   MPI_Reduce(&l_count, &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n   MPI_Reduce(&r_count, &bins[1], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n   MPI_Reduce(&t_count, &bins[2], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n   MPI_Reduce(&b_count, &bins[3], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n}",
            "// TODO: implement this function\n   bins.fill(0);\n   // parallel for\n   #pragma omp parallel for reduction(+:bins[0], bins[1], bins[2], bins[3])\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         bins[0] += 1;\n      } else if (points[i].x < 0 && points[i].y > 0) {\n         bins[1] += 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         bins[2] += 1;\n      } else {\n         bins[3] += 1;\n      }\n   }\n}",
            "/* TODO */\n}",
            "#pragma omp parallel default(none) shared(points, bins)\n  {\n#pragma omp for schedule(static) nowait\n    for (int i = 0; i < points.size(); ++i) {\n      if (points[i].x > 0 && points[i].y > 0)\n        ++bins[0];\n      else if (points[i].x < 0 && points[i].y > 0)\n        ++bins[1];\n      else if (points[i].x < 0 && points[i].y < 0)\n        ++bins[2];\n      else\n        ++bins[3];\n    }\n  }\n}",
            "int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   if (size == 1) {\n      bins[0] = bins[1] = bins[2] = bins[3] = 0;\n      for (auto p : points) {\n         if (p.x >= 0 && p.y >= 0) {\n            bins[0]++;\n         }\n         else if (p.x < 0 && p.y >= 0) {\n            bins[1]++;\n         }\n         else if (p.x < 0 && p.y < 0) {\n            bins[2]++;\n         }\n         else {\n            bins[3]++;\n         }\n      }\n      return;\n   }\n\n   if (rank == 0) {\n      std::vector<size_t> localBins(4, 0);\n      std::vector<size_t> localBinsSum(4, 0);\n      std::vector<std::vector<Point>> image(size, std::vector<Point>());\n      size_t n = points.size();\n\n      for (int i = 0; i < n; i++) {\n         int owner = (points[i].x >= 0 && points[i].y >= 0)? 0 :\n                     (points[i].x < 0 && points[i].y >= 0)? 1 :\n                     (points[i].x < 0 && points[i].y < 0)? 2 : 3;\n         image[owner].push_back(points[i]);\n      }\n\n      for (int i = 0; i < size; i++) {\n         int recvSize;\n         MPI_Status status;\n         MPI_Probe(i, 0, MPI_COMM_WORLD, &status);\n         MPI_Get_count(&status, MPI_UNSIGNED_LONG, &recvSize);\n         std::vector<size_t> bins(recvSize);\n         MPI_Recv(bins.data(), recvSize, MPI_UNSIGNED_LONG, i, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n         for (int j = 0; j < recvSize; j++) {\n            localBins[j] += bins[j];\n         }\n      }\n\n      for (int i = 0; i < 4; i++) {\n         localBinsSum[i] += localBins[i];\n      }\n      MPI_Send(localBinsSum.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n   else {\n      size_t n = points.size();\n      std::vector<size_t> localBins(4, 0);\n      for (int i = 0; i < n; i++) {\n         if (points[i].x >= 0 && points[i].y >= 0) {\n            localBins[0]++;\n         }\n         else if (points[i].x < 0 && points[i].y >= 0) {\n            localBins[1]++;\n         }\n         else if (points[i].x < 0 && points[i].y < 0) {\n            localBins[2]++;\n         }\n         else {\n            localBins[3]++;\n         }\n      }\n      MPI_Send(localBins.data(), 4, MPI_UNSIGNED_LONG, 0, 0, MPI_COMM_WORLD);\n   }\n}",
            "// implement me!\n}",
            "double const x_min = -10.0, y_min = -10.0, x_max = 10.0, y_max = 10.0;\n\n   //... Your code here...\n}",
            "// Your code here.\n}",
            "}",
            "int rank, num_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   std::vector<size_t> counts(num_ranks, 0);\n#pragma omp parallel\n{\n   int t_rank = omp_get_thread_num();\n   auto t_points = points;\n   std::sort(t_points.begin(), t_points.end(),\n      [](Point const& p1, Point const& p2) { return (p1.x!= p2.x)? p1.x < p2.x : p1.y < p2.y; } );\n   auto iter = std::partition(t_points.begin(), t_points.end(),\n      [&](Point const& p) { return (p.x < 0); } );\n   auto mid = iter;\n   int i = 0;\n   while (iter!= t_points.end()) {\n      ++i;\n      if ((i % num_ranks) == t_rank) {\n         ++counts[t_rank];\n      }\n      ++iter;\n   }\n   if ((i % num_ranks) == t_rank) {\n      ++counts[t_rank];\n   }\n}\n   std::array<size_t, 4> local_bins;\n   MPI_Gather(&counts[0], 4, MPI_UNSIGNED_LONG, &local_bins[0], 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      bins[0] = local_bins[0];\n      bins[1] = local_bins[1];\n      bins[2] = local_bins[2];\n      bins[3] = local_bins[3];\n   }\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n    \n    int rank = 0;\n    int nprocs = 0;\n    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    double x_mid = 0.0, y_mid = 0.0;\n    for (auto const &p: points) {\n        x_mid += p.x;\n        y_mid += p.y;\n    }\n    x_mid /= points.size();\n    y_mid /= points.size();\n\n    std::array<double, 2> bounds = {x_mid - 1, x_mid + 1};\n    auto n_threads = omp_get_max_threads();\n    std::vector<std::array<double, 2>> bounds_threads(n_threads);\n\n    #pragma omp parallel for schedule(static)\n    for (int i = 0; i < n_threads; i++) {\n        bounds_threads[i] = {bounds[0], bounds[1]};\n        if (i!= n_threads - 1) {\n            bounds_threads[i][1] = x_mid;\n        }\n    }\n\n    std::vector<int> counts(nprocs);\n    int p_start = rank * points.size() / nprocs;\n    int p_end = (rank + 1) * points.size() / nprocs;\n\n    #pragma omp parallel\n    {\n        auto &l_bounds = bounds_threads[omp_get_thread_num()];\n        auto &l_counts = counts[omp_get_thread_num()];\n        l_counts = 0;\n        #pragma omp for schedule(static)\n        for (int i = p_start; i < p_end; i++) {\n            auto const &p = points[i];\n            l_counts += (p.x >= l_bounds[0] && p.x <= l_bounds[1] && p.y >= y_mid - 1 && p.y <= y_mid + 1);\n        }\n    }\n\n    MPI_Reduce(counts.data(), bins.data(), counts.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO\n}",
            "// You may assume that image has at least 1 element.\n   bins = std::array<size_t, 4>{};\n   // Fill this in.\n}",
            "size_t n = points.size();\n   size_t nthreads = std::thread::hardware_concurrency();\n   std::vector<std::vector<Point>> binPoints(4);\n\n   // TODO: compute the counts in parallel with OpenMP and MPI.\n   //\n   // NOTE:\n   // - You must call `MPI_Barrier` at least once in your parallel code.\n   // - Make sure to use OpenMP parallel for loops, as shown in the example.\n   // - Remember to pass an appropriate hint to `MPI_Scatter` and `MPI_Gather`.\n   // - All ranks must call `MPI_Scatter` exactly once.\n   // - The `MPI_Scatter` argument `root` must be 0.\n   // - Make sure to allocate enough memory for `bins` on rank 0.\n   // - Your code must be correct, i.e. it should produce the correct result.\n   // - You can assume that `n` is a multiple of the number of processes.\n}",
            "// Your code here\n}",
            "// TODO: your code goes here\n}",
            "size_t num_threads = omp_get_max_threads();\n  size_t num_points = points.size();\n  int rank;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  size_t num_ranks;\n  MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n  std::vector<size_t> thread_bins(num_threads, 0);\n  // divide work evenly across ranks\n  size_t num_points_per_rank = num_points / num_ranks;\n  if (rank < num_points % num_ranks) {\n    num_points_per_rank++;\n  }\n  std::vector<Point> rank_points(num_points_per_rank);\n  if (rank == 0) {\n    std::vector<Point> all_points(points);\n    // MPI_Scatterv requires an array of send counts\n    std::vector<int> send_counts(num_ranks, num_points_per_rank);\n    MPI_Scatterv(all_points.data(), send_counts.data(), send_counts.data(), MPI_DOUBLE, rank_points.data(), num_points_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Scatterv(points.data(), nullptr, nullptr, MPI_DOUBLE, rank_points.data(), num_points_per_rank, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n  }\n\n  // divide work evenly across threads\n  size_t num_points_per_thread = num_points_per_rank / num_threads;\n  if (rank < num_points_per_rank % num_threads) {\n    num_points_per_thread++;\n  }\n\n  #pragma omp parallel for\n  for (int thread = 0; thread < num_threads; thread++) {\n    // only work on threads assigned to this rank\n    if (thread + rank * num_threads < num_points_per_rank) {\n      size_t begin_index = thread * num_points_per_thread;\n      size_t end_index = (thread + 1) * num_points_per_thread;\n      for (size_t i = begin_index; i < end_index; i++) {\n        if (rank_points[i].x > 0 && rank_points[i].y > 0) {\n          thread_bins[thread]++;\n        } else if (rank_points[i].x < 0 && rank_points[i].y > 0) {\n          thread_bins[thread] += 2;\n        } else if (rank_points[i].x < 0 && rank_points[i].y < 0) {\n          thread_bins[thread] += 3;\n        } else {\n          thread_bins[thread] += 4;\n        }\n      }\n    }\n  }\n\n  if (rank == 0) {\n    // MPI_Gatherv requires an array of recv counts\n    std::vector<int> recv_counts(num_ranks, num_threads);\n    MPI_Gatherv(thread_bins.data(), num_threads, MPI_UNSIGNED_LONG_LONG, bins.data(), recv_counts.data(), recv_counts.data(), MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  } else {\n    MPI_Gatherv(thread_bins.data(), num_threads, MPI_UNSIGNED_LONG_LONG, nullptr, nullptr, nullptr, MPI_UNSIGNED_LONG_LONG, 0, MPI_COMM_WORLD);\n  }\n}",
            "//TODO implement\n}",
            "// TODO: implement\n    size_t count = points.size();\n    size_t q1 = 0, q2 = 0, q3 = 0, q4 = 0;\n    for(size_t i = 0; i < count; i++){\n        if(points[i].x > 0 && points[i].y > 0){\n            q1++;\n        } else if(points[i].x < 0 && points[i].y > 0){\n            q2++;\n        } else if(points[i].x < 0 && points[i].y < 0){\n            q3++;\n        } else {\n            q4++;\n        }\n    }\n    bins = {q1, q2, q3, q4};\n}",
            "// TODO: implement this function.\n\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   // TODO: implement\n}",
            "// TODO: implement\n}",
            "//TODO\n}",
            "// TODO: implement\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   \n   //TODO: split the array of points\n   size_t n = points.size();\n   size_t n_each_proc = n/size;\n   std::vector<Point> local_points(n_each_proc);\n   std::vector<Point> local_points_copy(n_each_proc);\n   for(int i=0; i<n; i++){\n       if(i<n_each_proc*rank && i+n_each_proc*rank<n){\n           local_points[i-n_each_proc*rank] = points[i];\n       }\n   }\n   //std::copy(points.begin()+n_each_proc*rank, points.begin()+n_each_proc*(rank+1), local_points.begin());\n   \n   //TODO: Count points in each quadrant\n   //TODO: sum up bins from all processors and divide by 2\n   //TODO: divide the number of points in each quadrant by 2 and get the ceiling of the value\n   \n   int quadrant[n];\n   int count[4];\n   int n_local = local_points.size();\n   for(int i=0; i<n_local; i++){\n       if(local_points[i].x>=0 && local_points[i].y>=0){\n           quadrant[i]=1;\n       }\n       else if(local_points[i].x>=0 && local_points[i].y<0){\n           quadrant[i]=2;\n       }\n       else if(local_points[i].x<0 && local_points[i].y<0){\n           quadrant[i]=3;\n       }\n       else{\n           quadrant[i]=4;\n       }\n   }\n   for(int i=0; i<4; i++){\n       count[i]=0;\n   }\n   for(int i=0; i<n_local; i++){\n       count[quadrant[i]-1]+=1;\n   }\n   std::array<size_t, 4> bins_local = {count[0], count[1], count[2], count[3]};\n   MPI_Reduce(bins_local.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   for(int i=0; i<4; i++){\n       bins[i] = bins[i]/2;\n   }\n}",
            "// Get the number of points on this rank and the total number of points\n   int n = points.size();\n   int np = 0;\n   MPI_Comm_size(MPI_COMM_WORLD, &np);\n\n   // Create a vector with the indices for this rank\n   std::vector<int> rank_indices(n);\n   for (int i = 0; i < n; i++) {\n      rank_indices[i] = i;\n   }\n\n   // Calculate the number of indices per rank\n   std::vector<int> indices_per_rank(np);\n   std::vector<int> indices_send_count(np);\n   std::vector<int> indices_recv_count(np);\n   for (int i = 0; i < np; i++) {\n      indices_per_rank[i] = n / np;\n      indices_send_count[i] = indices_per_rank[i];\n      if (i == np - 1) {\n         indices_recv_count[i] = n - (indices_per_rank[i] * (np - 1));\n      }\n      if (i!= 0) {\n         indices_send_count[i] += indices_send_count[i - 1];\n         indices_recv_count[i] += indices_recv_count[i - 1];\n      }\n   }\n\n   // Distribute the indices to each rank\n   std::vector<int> indices_send_counts(np);\n   std::vector<int> indices_send_displs(np);\n   std::vector<int> indices_recv_counts(np);\n   std::vector<int> indices_recv_displs(np);\n   MPI_Scatter(indices_send_count.data(), 1, MPI_INT, indices_send_counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(indices_recv_count.data(), 1, MPI_INT, indices_recv_counts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(indices_send_count.data(), 1, MPI_INT, indices_send_displs.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Scatter(indices_recv_count.data(), 1, MPI_INT, indices_recv_displs.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // Gather the indices from each rank\n   std::vector<int> indices_send(indices_send_count[0]);\n   std::vector<int> indices_recv(indices_recv_count[0]);\n   MPI_Scatterv(rank_indices.data(), indices_send_counts.data(), indices_send_displs.data(), MPI_INT, indices_send.data(), indices_send_count[0], MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Gatherv(indices_send.data(), indices_send_count[0], MPI_INT, indices_recv.data(), indices_recv_counts.data(), indices_recv_displs.data(), MPI_INT, 0, MPI_COMM_WORLD);\n\n   // Calculate the counts\n   std::vector<int> counts(indices_recv.size());\n\n#pragma omp parallel for\n   for (int i = 0; i < counts.size(); i++) {\n      int quadrant = 0;\n      if (points[indices_recv[i]].x >= 0) {\n         if (points[indices_recv[i]].y >= 0) {\n            quadrant = 1;\n         } else {\n            quadrant = 2;\n         }\n      } else {\n         if (points[indices_recv[i]].y >= 0) {\n            quadrant = 3;\n         } else {\n            quadrant = 4;\n         }\n      }\n      counts[i] = quadrant;\n   }\n\n   // Gather the counts\n   std::vector<int> counts_send(counts.size());\n   std::vector<int> counts_recv(counts.size());\n   MPI_Scatter(counts.data(), counts.size(), MPI_INT, counts_send.data(), counts.size(), MPI_INT, 0, MPI_COMM_WORLD);\n   MPI_Gather(counts_send.data(), counts_send.size(), MPI_INT, counts_recv.data(), counts_recv.size(), MPI_INT, 0, MPI_COMM_WORLD);\n\n   // Count the points in each quadrant\n   int temp = 0;\n   bins[0]",
            "// TODO\n}",
            "int rank, nproc;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n\n    size_t n = points.size();\n    std::vector<size_t> nlocal(nproc);\n    std::vector<size_t> scounts(nproc);\n    std::vector<size_t> sdispls(nproc);\n    size_t sendCounts = 0;\n    for (int r = 0; r < nproc; r++) {\n        double x0, x1, y0, y1;\n        if (r == 0) {\n            x0 = -2; y0 = -2;\n        } else {\n            x0 = (r - 1) * 1.0; y0 = 0;\n        }\n        if (r == nproc - 1) {\n            x1 = 2; y1 = 2;\n        } else {\n            x1 = r * 1.0; y1 = 1;\n        }\n        size_t nlocalr = 0;\n        for (size_t i = 0; i < n; i++) {\n            if (points[i].x > x0 && points[i].x < x1 && points[i].y > y0 && points[i].y < y1) {\n                nlocalr++;\n            }\n        }\n        nlocal[r] = nlocalr;\n        scounts[r] = nlocal[r] * sizeof(Point);\n        sendCounts += scounts[r];\n        if (r > 0) {\n            sdispls[r] = sdispls[r-1] + scounts[r-1];\n        }\n    }\n\n    std::vector<Point> sbuffer(sendCounts);\n    std::vector<size_t> rcounts(nproc);\n    std::vector<size_t> rdispls(nproc);\n    std::vector<Point> rbuffer(n);\n\n    // Gather information from all ranks\n    MPI_Allgatherv(points.data(), sendCounts, MPI_BYTE, sbuffer.data(), scounts.data(), sdispls.data(), MPI_BYTE, MPI_COMM_WORLD);\n\n    // Count points in each quadrant\n    size_t i = 0;\n#pragma omp parallel\n    {\n#pragma omp for schedule(static) reduction(+: i)\n        for (int r = 0; r < nproc; r++) {\n            for (int j = 0; j < nlocal[r]; j++) {\n                size_t index = sdispls[r] + j * sizeof(Point);\n                Point p = sbuffer[index];\n                if (p.x > (r-1) * 1.0 && p.x < r * 1.0 && p.y > 0 && p.y < 1) {\n                    i++;\n                }\n            }\n        }\n    }\n\n    // Send the result back to rank 0\n    MPI_Gather(&i, 1, MPI_INT, rcounts.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n    if (rank == 0) {\n        rdispls[0] = 0;\n        for (int i = 1; i < nproc; i++) {\n            rdispls[i] = rdispls[i-1] + rcounts[i-1];\n        }\n        MPI_Gatherv(points.data(), n * sizeof(Point), MPI_BYTE, rbuffer.data(), rcounts.data(), rdispls.data(), MPI_BYTE, 0, MPI_COMM_WORLD);\n    }\n}",
            "//...\n}",
            "// TODO: Implement this method.\n}",
            "// *** implement me ***\n}",
            "// YOUR CODE HERE\n}",
            "// YOUR CODE HERE\n}",
            "std::array<size_t, 4> bins_local;\n    bins_local.fill(0);\n\n    // OMP PARALLEL\n    #pragma omp parallel num_threads(omp_get_max_threads())\n    {\n        // OMP FOR\n        #pragma omp for\n        for (size_t i = 0; i < points.size(); i++) {\n            if (points[i].x > 0 && points[i].y > 0) {\n                bins_local[0]++;\n            } else if (points[i].x > 0 && points[i].y < 0) {\n                bins_local[1]++;\n            } else if (points[i].x < 0 && points[i].y < 0) {\n                bins_local[2]++;\n            } else {\n                bins_local[3]++;\n            }\n        }\n    }\n\n    // MPI\n    MPI_Allreduce(MPI_IN_PLACE, bins_local.data(), bins_local.size(), MPI_UNSIGNED_LONG_LONG, MPI_SUM, MPI_COMM_WORLD);\n\n    bins = bins_local;\n}",
            "bins = {0, 0, 0, 0};\n    // TO DO\n}",
            "MPI_Comm comm = MPI_COMM_WORLD;\n   int rank = 0, ranks = 0;\n   MPI_Comm_size(comm, &ranks);\n   MPI_Comm_rank(comm, &rank);\n\n   // TODO\n\n   // Send counts to rank 0\n   // TODO\n}",
            "std::fill(bins.begin(), bins.end(), 0);\n   int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   /* TODO: implement this function. */\n}",
            "// MPI variables\n   int world_size, world_rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n   std::array<size_t, 4> bins_local;\n\n   // OpenMP variables\n   int num_threads = omp_get_max_threads();\n\n   // distribute points evenly across the ranks\n   int num_points = points.size();\n   int num_points_per_rank = num_points / world_size;\n   int start_point = num_points_per_rank * world_rank;\n   int end_point = num_points_per_rank * (world_rank+1);\n   std::vector<Point> points_to_count(points.begin() + start_point, points.begin() + end_point);\n\n   // count points in each quadrant\n   int rank;\n#pragma omp parallel for num_threads(num_threads)\n   for (size_t i = 0; i < points_to_count.size(); i++) {\n      Point p = points_to_count[i];\n      if (p.x > 0) {\n         if (p.y > 0) {\n            rank = 0;\n         } else {\n            rank = 1;\n         }\n      } else {\n         if (p.y > 0) {\n            rank = 2;\n         } else {\n            rank = 3;\n         }\n      }\n#pragma omp atomic\n      bins_local[rank]++;\n   }\n\n   // sum the counts across the ranks\n   MPI_Reduce(bins_local.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (world_rank == 0) {\n      std::cout << \"quadrant counts = \" << std::endl;\n      for (int i = 0; i < 4; i++) {\n         std::cout << i << \": \" << bins[i] << std::endl;\n      }\n   }\n}",
            "MPI_Datatype point_type;\n   MPI_Type_contiguous(sizeof(Point), MPI_BYTE, &point_type);\n   MPI_Type_commit(&point_type);\n\n   MPI_Aint lb, extent;\n   MPI_Type_get_extent(point_type, &lb, &extent);\n\n   size_t sendcount = points.size() * sizeof(Point);\n   size_t recvcount = bins.size() * sizeof(size_t);\n\n   // The sendcounts vector indicates the number of elements sent to each rank\n   std::vector<int> sendcounts(bins.size(), sendcount);\n   std::vector<int> recvcounts(bins.size(), recvcount);\n   // The displacements vector indicates the number of bytes offset before sending to each rank\n   std::vector<int> displacements(bins.size(), 0);\n   for (size_t i = 1; i < bins.size(); ++i)\n      displacements[i] = displacements[i - 1] + sendcounts[i - 1];\n\n   // This is not very efficient.\n   // Each process sends all the points in `points` to rank 0, which then counts them.\n   // It would be better to send to each rank the number of points they should receive,\n   // then to send those points to each rank.\n   // The problem with this approach is that we cannot allocate memory in the receiving process.\n   std::vector<Point> recvpoints;\n   if (omp_get_thread_num() == 0)\n      recvpoints.resize(points.size());\n\n   MPI_Request request;\n\n   // Send points to rank 0\n   MPI_Isend(points.data(), sendcount, point_type, 0, 0, MPI_COMM_WORLD, &request);\n\n   // Receive counts from rank 0\n   MPI_Irecv(bins.data(), recvcount, MPI_BYTE, 0, 0, MPI_COMM_WORLD, &request);\n\n   MPI_Wait(&request, MPI_STATUS_IGNORE);\n   MPI_Type_free(&point_type);\n\n   if (omp_get_thread_num() == 0) {\n      // Each process calculates the number of points in its quadrant\n      size_t count = points.size();\n      for (size_t i = 0; i < points.size(); ++i) {\n         if (points[i].x >= 0 && points[i].y >= 0)\n            --bins[0];\n         else if (points[i].x < 0 && points[i].y >= 0)\n            --bins[1];\n         else if (points[i].x < 0 && points[i].y < 0)\n            --bins[2];\n         else\n            --bins[3];\n      }\n\n      // Send the bins to rank 0\n      MPI_Isend(bins.data(), bins.size(), MPI_BYTE, 0, 1, MPI_COMM_WORLD, &request);\n\n      // Receive the points from rank 0\n      MPI_Irecv(recvpoints.data(), recvpoints.size(), point_type, 0, 1, MPI_COMM_WORLD, &request);\n\n      MPI_Wait(&request, MPI_STATUS_IGNORE);\n   }\n}",
            "int nthreads, rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &nthreads);\n\n    // TODO: replace me!\n    omp_set_num_threads(nthreads);\n    auto start = omp_get_wtime();\n    std::array<size_t, 4> counts = {0, 0, 0, 0};\n    std::cout << \"Rank: \" << rank << \", Threads: \" << nthreads << std::endl;\n    for (auto p : points) {\n        if (p.x > 0 && p.y > 0) {\n            counts[0]++;\n        } else if (p.x < 0 && p.y > 0) {\n            counts[1]++;\n        } else if (p.x < 0 && p.y < 0) {\n            counts[2]++;\n        } else if (p.x > 0 && p.y < 0) {\n            counts[3]++;\n        }\n    }\n    bins = counts;\n    auto end = omp_get_wtime();\n    std::cout << \"Rank: \" << rank << \", Time: \" << end - start << std::endl;\n}",
            "#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         ++bins[0];\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         ++bins[1];\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         ++bins[2];\n      } else {\n         ++bins[3];\n      }\n   }\n}",
            "// YOUR CODE HERE\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int dim = sqrt(size);\n   int n = points.size();\n   std::vector<Point> p(n, {0,0});\n   double delta = 1.0 / dim;\n   int start = rank * n / size;\n   int end = (rank + 1) * n / size;\n\n   for(int i=0; i<n; i++){\n      p[i].x = points[i].x / delta;\n      p[i].y = points[i].y / delta;\n   }\n\n   std::map<int, int> map;\n   int i, j, cnt = 0;\n   double x, y;\n   #pragma omp parallel for reduction(+:cnt)\n   for(i = 0; i < n; i++){\n      x = p[i].x;\n      y = p[i].y;\n      if(x < 0){\n         if(y < 0){\n            map[0]++;\n         }else if(y >= 1){\n            map[3]++;\n         }else{\n            map[1]++;\n         }\n      }else if(x >= 1){\n         if(y < 0){\n            map[2]++;\n         }else if(y >= 1){\n            map[3]++;\n         }else{\n            map[1]++;\n         }\n      }else{\n         if(y < 0){\n            map[0]++;\n         }else if(y >= 1){\n            map[2]++;\n         }else{\n            map[1]++;\n         }\n      }\n   }\n   \n   #pragma omp parallel for\n   for(i = 0; i < 4; i++){\n      bins[i] = map[i];\n   }\n\n}",
            "// TODO: implement me\n}",
            "// Your code goes here\n    int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    int num_threads;\n    MPI_Comm_size(MPI_COMM_WORLD, &num_threads);\n\n    int threads = omp_get_num_threads();\n\n    int num_points = points.size();\n    int start = num_points * rank / num_threads;\n    int end = num_points * (rank + 1) / num_threads;\n\n    bins.fill(0);\n    for (int i = start; i < end; i++) {\n        Point p = points[i];\n        if (p.x >= 0.0 && p.y >= 0.0) {\n            bins[0]++;\n        } else if (p.x < 0.0 && p.y >= 0.0) {\n            bins[1]++;\n        } else if (p.x < 0.0 && p.y < 0.0) {\n            bins[2]++;\n        } else {\n            bins[3]++;\n        }\n    }\n}",
            "}",
            "// TODO: implement me...\n   int world_size, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int bins_local[4] = {0,0,0,0};\n\n   #pragma omp parallel num_threads(world_size)\n   {\n      int thread_id = omp_get_thread_num();\n\n      if (thread_id == rank) {\n         int bin_id = 0;\n         for (auto& p : points) {\n            int quadrant_id = (p.x >= 0? (p.y >= 0? 0 : 3) : (p.y >= 0? 1 : 2));\n            #pragma omp critical\n            {\n               bins_local[quadrant_id]++;\n            }\n         }\n         MPI_Gather(bins_local, 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n      }\n   }\n}",
            "// TODO: Write the code here.\n}",
            "// TODO: implement this function\n}",
            "const size_t n = points.size();\n  // TODO: implement the counting\n}",
            "// your code here\n   size_t size = points.size();\n   int nrank, nprocs;\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   MPI_Comm_rank(MPI_COMM_WORLD, &nrank);\n   int num_threads = 8;\n   omp_set_num_threads(num_threads);\n\n   int n = size/nprocs;\n   int remainder = size % nprocs;\n   int local_n = n;\n   int start_index = 0;\n\n   if(nrank < remainder){\n       local_n++;\n   }\n   else{\n       start_index = n + remainder;\n   }\n\n   double lower_x = points[start_index].x, upper_x = points[start_index].x, lower_y = points[start_index].y, upper_y = points[start_index].y;\n   double x_temp, y_temp;\n   for (int i = start_index; i < start_index + local_n; ++i) {\n       if(points[i].x < lower_x){\n           lower_x = points[i].x;\n       }\n       if(points[i].x > upper_x){\n           upper_x = points[i].x;\n       }\n       if(points[i].y < lower_y){\n           lower_y = points[i].y;\n       }\n       if(points[i].y > upper_y){\n           upper_y = points[i].y;\n       }\n   }\n   int bin_temp;\n   if(lower_x >= 0){\n       if(upper_x >= 0){\n           bin_temp = 0;\n       }\n       else{\n           bin_temp = 1;\n       }\n   }\n   else{\n       if(upper_x >= 0){\n           bin_temp = 3;\n       }\n       else{\n           bin_temp = 2;\n       }\n   }\n\n   int n_in_bin = 0;\n   #pragma omp parallel for reduction(+:n_in_bin) schedule(static)\n   for(int i = start_index; i < start_index + local_n; ++i){\n       if(bin_temp == 0){\n           if(points[i].x >= 0){\n               if(points[i].y >= 0){\n                   bin_temp = 0;\n               }\n               else{\n                   bin_temp = 3;\n               }\n           }\n           else{\n               if(points[i].y >= 0){\n                   bin_temp = 1;\n               }\n               else{\n                   bin_temp = 2;\n               }\n           }\n       }\n       else if(bin_temp == 1){\n           if(points[i].x >= 0){\n               if(points[i].y >= 0){\n                   bin_temp = 0;\n               }\n               else{\n                   bin_temp = 3;\n               }\n           }\n           else{\n               if(points[i].y >= 0){\n                   bin_temp = 1;\n               }\n               else{\n                   bin_temp = 2;\n               }\n           }\n       }\n       else if(bin_temp == 2){\n           if(points[i].x >= 0){\n               if(points[i].y >= 0){\n                   bin_temp = 0;\n               }\n               else{\n                   bin_temp = 3;\n               }\n           }\n           else{\n               if(points[i].y >= 0){\n                   bin_temp = 1;\n               }\n               else{\n                   bin_temp = 2;\n               }\n           }\n       }\n       else if(bin_temp == 3){\n           if(points[i].x >= 0){\n               if(points[i].y >= 0){\n                   bin_temp = 0;\n               }\n               else{\n                   bin_temp = 3;\n               }\n           }\n           else{\n               if(points[i].y >= 0){\n                   bin_temp = 1;\n               }\n               else{\n                   bin_temp = 2;\n               }\n           }\n       }\n       if(bin_temp == 0){\n           n_in_bin++;\n       }\n       else if(bin_temp == 1){\n           n_in_bin++;\n       }\n       else if(bin_temp == 2){\n           n_in_bin++;\n       }\n       else if(bin_temp == 3){\n           n_in_bin++;\n       }\n   }\n   int temp = n_in_bin;\n   MPI_Reduce(&temp, &n_in_bin, 1, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n   if(nrank == 0){\n       bins[0] = 0",
            "// TODO: Implement this method\n}",
            "bins.fill(0);\n  size_t num_points = points.size();\n\n  #pragma omp parallel for\n  for (int i=0; i<num_points; i++) {\n    Point p = points[i];\n    if (p.x > 0 && p.y > 0)\n      bins[0]++;\n    else if (p.x < 0 && p.y > 0)\n      bins[1]++;\n    else if (p.x < 0 && p.y < 0)\n      bins[2]++;\n    else if (p.x > 0 && p.y < 0)\n      bins[3]++;\n  }\n\n  // sum the bins from each thread\n  // bin = bins[0] + bins[1] + bins[2] + bins[3]\n  MPI_Reduce(bins.data(), bins.data(), bins.size(), MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// Your code here\n   size_t localBins[4] = {0};\n   int rank, worldSize;\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   for (auto& point: points) {\n      int bin = (point.x > 0) + (point.y > 0);\n      localBins[bin]++;\n   }\n\n   MPI_Reduce(localBins, bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Your code goes here.\n}",
            "// TODO: count quadrants in parallel.\n   // Use MPI to divide the work and OpenMP to compute each quadrant in parallel.\n   // Assume MPI has already been initialized.\n\n   // split the array\n   double rank;\n   int worldSize;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &worldSize);\n\n   std::vector<Point> *localPoints = new std::vector<Point>[worldSize];\n   std::array<size_t, 4> *localBins = new std::array<size_t, 4>[worldSize];\n   int *sizes = new int[worldSize];\n   MPI_Scatter(points.data(), points.size() / worldSize, MPI_DOUBLE,\n               localPoints[0], points.size() / worldSize, MPI_DOUBLE,\n               0, MPI_COMM_WORLD);\n\n   // split work in each processor\n   int *division = new int[worldSize];\n   int *tmp = new int[worldSize];\n   MPI_Scatter(tmp, 1, MPI_INT, division, 1, MPI_INT, 0, MPI_COMM_WORLD);\n   int pointsPerProc = points.size() / worldSize;\n\n   // loop over processors\n   // each processor loop over the points\n   // each processor loop over the bins\n   for (int i = 0; i < worldSize; i++) {\n      for (int j = 0; j < points.size() / worldSize; j++) {\n         for (int k = 0; k < 4; k++) {\n            if ((localPoints[i][j].x >= 0) && (localPoints[i][j].x <= division[i])\n                && (localPoints[i][j].y >= 0) && (localPoints[i][j].y <= division[i])) {\n               localBins[i][k]++;\n            }\n         }\n      }\n   }\n   MPI_Gather(localBins, 4, MPI_INT, bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] = 0;\n      }\n   }\n\n   MPI_Gather(localBins, 4, MPI_INT, bins, 4, MPI_INT, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         for (int j = 0; j < worldSize; j++) {\n            bins[i] += localBins[j][i];\n         }\n      }\n   }\n\n   MPI_Gather(localPoints, pointsPerProc, MPI_DOUBLE, points, pointsPerProc, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   delete[] localPoints;\n   delete[] localBins;\n   delete[] division;\n   delete[] tmp;\n}",
            "/* Your solution goes here  */\n\n}",
            "// YOUR CODE HERE\n}",
            "// TODO\n   size_t num_threads = omp_get_max_threads();\n   MPI_Comm comm = MPI_COMM_WORLD;\n   int rank;\n   MPI_Comm_rank(comm, &rank);\n   size_t size;\n   MPI_Comm_size(comm, &size);\n   std::vector<size_t> points_count(size);\n   MPI_Gather(&points.size(), 1, MPI_INT, points_count.data(), 1, MPI_INT, 0, comm);\n   if (rank == 0) {\n      bins = std::array<size_t, 4>({0, 0, 0, 0});\n   }\n   MPI_Barrier(comm);\n   size_t my_count = points.size();\n   size_t my_start = 0;\n   size_t my_end = points.size();\n   std::vector<size_t> local_bins(4);\n   if (rank == 0) {\n      my_start = 0;\n      my_end = 0;\n      for (size_t i = 0; i < size; ++i) {\n         my_start += points_count[i];\n         my_end += points_count[i];\n         if (i < size - 1) {\n            my_end += 1;\n         }\n      }\n   }\n   MPI_Scatter(&my_start, 1, MPI_INT, &my_start, 1, MPI_INT, 0, comm);\n   MPI_Scatter(&my_end, 1, MPI_INT, &my_end, 1, MPI_INT, 0, comm);\n   for (size_t i = my_start; i < my_end; ++i) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         local_bins[0] += 1;\n      } else if (points[i].x < 0 && points[i].y >= 0) {\n         local_bins[1] += 1;\n      } else if (points[i].x < 0 && points[i].y < 0) {\n         local_bins[2] += 1;\n      } else {\n         local_bins[3] += 1;\n      }\n   }\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, comm);\n}",
            "// Your code here\n}",
            "// YOUR CODE HERE\n}",
            "size_t num_points = points.size();\n   // TODO: Compute the number of points in each quadrant\n   size_t num_in_quad1 = 0, num_in_quad2 = 0, num_in_quad3 = 0, num_in_quad4 = 0;\n\n   MPI_Reduce(&num_in_quad1, &bins[0], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&num_in_quad2, &bins[1], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&num_in_quad3, &bins[2], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   MPI_Reduce(&num_in_quad4, &bins[3], 1, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n   #pragma omp parallel for schedule(dynamic) num_threads(4)\n   for (size_t i=0; i<points.size(); i++) {\n      if (points[i].x > 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[0]++;\n      }\n      if (points[i].x < 0 && points[i].y > 0) {\n         #pragma omp atomic\n         bins[1]++;\n      }\n      if (points[i].x < 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[2]++;\n      }\n      if (points[i].x > 0 && points[i].y < 0) {\n         #pragma omp atomic\n         bins[3]++;\n      }\n   }\n}",
            "bins = {0, 0, 0, 0};\n   omp_set_num_threads(omp_get_max_threads());\n#pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            bins[0]++;\n         } else {\n            bins[3]++;\n         }\n      } else {\n         if (points[i].y > 0) {\n            bins[1]++;\n         } else {\n            bins[2]++;\n         }\n      }\n   }\n}",
            "// TODO\n}",
            "MPI_Datatype MPI_Point;\n   MPI_Type_contiguous(2, MPI_DOUBLE, &MPI_Point);\n   MPI_Type_commit(&MPI_Point);\n\n   int world_rank, world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   int num_points = points.size();\n   int num_local_points = num_points / world_size;\n   int start_index = num_local_points * world_rank;\n   int end_index = std::min(start_index + num_local_points, num_points);\n\n   std::vector<Point> local_points(points.begin() + start_index, points.begin() + end_index);\n\n   MPI_Scatter(local_points.data(), num_local_points, MPI_Point, nullptr, num_local_points, MPI_Point, 0, MPI_COMM_WORLD);\n\n   int num_threads = omp_get_max_threads();\n   std::vector<size_t> local_bins(num_threads);\n\n   // omp_set_num_threads(num_threads);\n   #pragma omp parallel for\n   for (size_t i = 0; i < local_points.size(); ++i) {\n      int quadrant = (local_points[i].x > 0 && local_points[i].y >= 0)? 0 : (local_points[i].x < 0 && local_points[i].y >= 0)? 1 : (local_points[i].x < 0 && local_points[i].y < 0)? 2 : 3;\n      #pragma omp atomic\n      ++local_bins[quadrant];\n   }\n\n   std::array<size_t, 4> temp_bins;\n   MPI_Reduce(local_bins.data(), temp_bins.data(), 4, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   if (world_rank == 0) {\n      bins = temp_bins;\n   }\n\n   MPI_Type_free(&MPI_Point);\n}",
            "// compute number of elements and allocate memory\n    int numEl = points.size();\n    int numRanks = MPI::COMM_WORLD.Get_size();\n\n    std::vector<int> numPointsPerRank(numRanks);\n    MPI::COMM_WORLD.Allgather(&numEl, 1, MPI::INT, numPointsPerRank.data(), 1, MPI::INT);\n\n    std::vector<int> ranksPerPoint(numEl);\n    for(int i = 0; i < numEl; i++) {\n        ranksPerPoint[i] = points[i].x > 0? 0 : points[i].y > 0? 1 : 2;\n    }\n\n    std::vector<int> numPointsPerQuadrant(3);\n    MPI::COMM_WORLD.Allgather(ranksPerPoint.data(), numEl, MPI::INT, numPointsPerQuadrant.data(), 1, MPI::INT);\n\n    int quadrant = 0;\n    for(int i = 0; i < numRanks; i++) {\n        if(numPointsPerRank[i] > 0) {\n            quadrant = numPointsPerQuadrant[ranksPerPoint[i]];\n            break;\n        }\n    }\n\n    // if rank 0\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        for(int i = 0; i < numEl; i++) {\n            quadrant = points[i].x > 0? 0 : points[i].y > 0? 1 : 2;\n            bins[quadrant]++;\n        }\n    }\n\n    // if rank 0\n    if (MPI::COMM_WORLD.Get_rank() == 0) {\n        for(int i = 0; i < numRanks; i++) {\n            bins[i] /= numPointsPerRank[i];\n        }\n    }\n\n    //if rank!= 0\n    if (MPI::COMM_WORLD.Get_rank()!= 0) {\n        for(int i = 0; i < numEl; i++) {\n            quadrant = points[i].x > 0? 0 : points[i].y > 0? 1 : 2;\n            bins[quadrant]++;\n        }\n    }\n}",
            "int rank = 0, nprocs = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n   size_t local_bins[4] = {0};\n   int n = points.size();\n   double chunk = (double)n / (double)nprocs;\n   int start = (int)std::round(chunk * rank);\n   int end = (int)std::round(chunk * (rank + 1));\n   int pstart = (int)std::round(chunk * (rank + 1));\n   int pend = (int)std::round(chunk * (nprocs + rank));\n   for (int i = start; i < end; i++) {\n      if (points[i].x >= 0 && points[i].y >= 0) {\n         local_bins[0] += 1;\n      }\n      else if (points[i].x >= 0 && points[i].y < 0) {\n         local_bins[1] += 1;\n      }\n      else if (points[i].x < 0 && points[i].y >= 0) {\n         local_bins[2] += 1;\n      }\n      else {\n         local_bins[3] += 1;\n      }\n   }\n   MPI_Reduce(local_bins, bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 0; i < 4; i++) {\n         bins[i] += pstart;\n      }\n   }\n}",
            "#pragma omp parallel for schedule(static)\n   for (size_t i = 0; i < points.size(); ++i) {\n      auto const& p = points[i];\n      bins[0] += p.x < 0 && p.y > 0;\n      bins[1] += p.x < 0 && p.y < 0;\n      bins[2] += p.x > 0 && p.y < 0;\n      bins[3] += p.x > 0 && p.y > 0;\n   }\n}",
            "const int num_points = points.size();\n   const int rank = 0;\n\n   // TODO: count the points in each quadrant\n   int count = 0;\n   #pragma omp parallel\n   {\n      int my_count = 0;\n      #pragma omp for\n      for (int i = 0; i < num_points; ++i) {\n         Point point = points[i];\n         if (point.x >= 0 && point.y >= 0) {\n            ++my_count;\n         } else if (point.x < 0 && point.y >= 0) {\n            ++my_count;\n         } else if (point.x < 0 && point.y < 0) {\n            ++my_count;\n         } else {\n            ++my_count;\n         }\n      }\n\n      // Combine all counts to the global count\n      #pragma omp critical\n      count += my_count;\n   }\n\n   // TODO: store the result in the bins array\n   bins[0] = count;\n}",
            "// TODO: write your code here\n}",
            "/* Your code goes here. */\n}",
            "/*\n     * OpenMP directives:\n     * - OpenMP parallel: Start the parallel region.\n     * - OpenMP single: Start single-threaded region.\n     * - OpenMP atomic: Start atomic region.\n     * - OpenMP critical: Start critical region.\n     * - OpenMP for: Start parallel for loop.\n     */\n    #pragma omp parallel for\n    for(size_t i = 0; i < bins.size(); ++i) {\n        bins[i] = 0;\n    }\n    #pragma omp parallel\n    {\n        /*\n         * OpenMP directives:\n         * - OpenMP single: Start single-threaded region.\n         */\n        #pragma omp single\n        {\n            std::vector<Point> localBins(bins);\n            #pragma omp task firstprivate(localBins)\n            {\n                /*\n                 * MPI directives:\n                 * - MPI_Scatter: Divide input array into equal pieces and distribute to all processes.\n                 * - MPI_Scatterv: Divide input array into subarrays and distribute to all processes.\n                 * - MPI_Comm_size: Get the number of processes.\n                 * - MPI_Comm_rank: Get the rank of this process.\n                 */\n                MPI_Scatterv(points.data(), bins.data(), MPI_INT, localBins.data(), bins.data(), MPI_INT, 0, MPI_COMM_WORLD);\n            }\n            #pragma omp taskloop firstprivate(localBins)\n            for(size_t i = 0; i < bins.size(); ++i) {\n                /*\n                 * OpenMP directives:\n                 * - OpenMP critical: Start critical region.\n                 */\n                #pragma omp critical\n                {\n                    localBins[i] += std::count_if(localBins.begin(), localBins.end(), [&](Point p) {\n                        return p.x >= 0.0 && p.y >= 0.0 && p.x < 1.0 && p.y < 1.0;\n                    });\n                }\n            }\n            #pragma omp task\n            {\n                MPI_Gatherv(localBins.data(), bins.size(), MPI_INT, bins.data(), bins.data(), MPI_INT, 0, MPI_COMM_WORLD);\n            }\n        }\n    }\n}",
            "int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int num_ranks;\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n\n   auto count = [&points](int rank) {\n      size_t count = 0;\n      #pragma omp parallel for reduction(+: count)\n      for (size_t i = 0; i < points.size(); i++) {\n         int rank = omp_get_thread_num();\n         Point const& p = points[i];\n         if (p.x > 0 && p.y > 0) {\n            count++;\n         } else if (p.x > 0 && p.y < 0) {\n            count += 2;\n         } else if (p.x < 0 && p.y < 0) {\n            count += 3;\n         } else {\n            count += 4;\n         }\n      }\n      return count;\n   };\n\n   std::array<size_t, 4> local_bins = {0, 0, 0, 0};\n   local_bins[0] = count(rank);\n   MPI_Allreduce(local_bins.data(), bins.data(), bins.size(), MPI_UNSIGNED_LONG, MPI_SUM, MPI_COMM_WORLD);\n}",
            "// TODO\n    size_t size = points.size();\n    int rank;\n    int n_ranks;\n\n    MPI_Comm_size(MPI_COMM_WORLD, &n_ranks);\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n    // printf(\"rank=%d\\n\", rank);\n\n    // #pragma omp parallel\n    // {\n    //     int tid = omp_get_thread_num();\n    //     int n_threads = omp_get_num_threads();\n    //     printf(\"thread=%d\\n\", tid);\n    // }\n\n    // #pragma omp parallel for\n    // for (size_t i = 0; i < size; i++) {\n    //     // TODO\n    //     // double x = points[i].x;\n    //     // double y = points[i].y;\n    //     // size_t bin_x = (x >= 0)? 1 : 0;\n    //     // size_t bin_y = (y >= 0)? 1 : 0;\n    //     // size_t bin = (bin_x << 1) | bin_y;\n    //     // bins[bin]++;\n    // }\n\n    int n_threads = omp_get_max_threads();\n    std::vector<int> thread_start_index(n_threads);\n    std::vector<int> thread_end_index(n_threads);\n    int step = size / n_threads;\n    int remainder = size % n_threads;\n\n    // printf(\"n_threads=%d, size=%d, step=%d, remainder=%d\\n\", n_threads, size, step, remainder);\n\n    thread_start_index[0] = 0;\n    for (int i = 1; i < n_threads; i++) {\n        thread_start_index[i] = thread_start_index[i-1] + step;\n        if (remainder > 0) {\n            thread_start_index[i]++;\n            remainder--;\n        }\n    }\n\n    thread_end_index[n_threads-1] = thread_start_index[n_threads-1] + step;\n    if (remainder > 0) {\n        thread_end_index[n_threads-1]++;\n        remainder--;\n    }\n    for (int i = 0; i < remainder; i++) {\n        thread_end_index[i]++;\n    }\n\n    // #pragma omp parallel\n    // {\n    //     int tid = omp_get_thread_num();\n    //     int n_threads = omp_get_num_threads();\n    //     int start = thread_start_index[tid];\n    //     int end = thread_end_index[tid];\n    //     printf(\"thread=%d, start=%d, end=%d\\n\", tid, start, end);\n    // }\n\n    // #pragma omp parallel for\n    // for (size_t i = 0; i < size; i++) {\n    //     // TODO\n    //     // double x = points[i].x;\n    //     // double y = points[i].y;\n    //     // size_t bin_x = (x >= 0)? 1 : 0;\n    //     // size_t bin_y = (y >= 0)? 1 : 0;\n    //     // size_t bin = (bin_x << 1) | bin_y;\n    //     // bins[bin]++;\n    // }\n    size_t n_points_per_thread = thread_end_index[n_threads-1] - thread_start_index[n_threads-1];\n\n    #pragma omp parallel for\n    for (int i = 0; i < n_threads; i++) {\n        int tid = omp_get_thread_num();\n        int start = thread_start_index[tid];\n        int end = thread_end_index[tid];\n        // printf(\"thread=%d, start=%d, end=%d\\n\", tid, start, end);\n\n        std::vector<Point> points_thread(points.begin()+start, points.begin()+end);\n        std::array<size_t, 4> bins_thread;\n\n        // #pragma omp parallel for\n        for (size_t i = 0; i < n_points_per_thread; i++) {\n            // TODO\n            double x = points_thread[i].x;\n            double y = points_thread[i].y;\n            size_t bin_x = (x >= 0)? 1 : 0;\n            size_t bin_y = (y >= 0)? 1 : 0;\n            size_t bin",
            "std::cout << \"Rank \" << MPI::COMM_WORLD.Get_rank() << \" (\" << omp_get_thread_num() << \"): \" << points.size() << std::endl;\n   int world_size = MPI::COMM_WORLD.Get_size();\n\n   // Divide the points evenly among the MPI ranks.\n   int points_per_rank = points.size() / world_size;\n   // Distribute the remainder of points equally among the remaining ranks.\n   if (points.size() % world_size > 0)\n      points_per_rank++;\n\n   // We will work on the local copy of points.\n   std::vector<Point> local_points(points.begin(), points.begin() + points_per_rank);\n\n   // Count the local points in each quadrant.\n   std::array<size_t, 4> local_bins;\n   local_bins[0] = std::count_if(local_points.begin(), local_points.end(),\n                                 [](Point const& point) { return point.x > 0 && point.y > 0; });\n   local_bins[1] = std::count_if(local_points.begin(), local_points.end(),\n                                 [](Point const& point) { return point.x < 0 && point.y > 0; });\n   local_bins[2] = std::count_if(local_points.begin(), local_points.end(),\n                                 [](Point const& point) { return point.x < 0 && point.y < 0; });\n   local_bins[3] = std::count_if(local_points.begin(), local_points.end(),\n                                 [](Point const& point) { return point.x > 0 && point.y < 0; });\n\n   // Send the counts to rank 0.\n   MPI::COMM_WORLD.Gather(&local_bins, 4, MPI::UNSIGNED_LONG_LONG, bins.data(), 4, MPI::UNSIGNED_LONG_LONG, 0);\n\n   // Merge the counts on rank 0.\n   if (MPI::COMM_WORLD.Get_rank() == 0) {\n      size_t total = 0;\n      for (auto count : bins)\n         total += count;\n      std::cout << \"Counting complete! Total points: \" << total << std::endl;\n   }\n}",
            "// TODO: implement this method.\n\n   // you can ignore this\n\n}",
            "// get number of MPI ranks\n   int world_size;\n   MPI_Comm_size(MPI_COMM_WORLD, &world_size);\n\n   // get rank of this process\n   int world_rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\n\n   // check that there are at least 4 processes\n   assert(world_size >= 4);\n   // check that we have at least 1 point\n   assert(points.size() > 0);\n\n   // get number of points assigned to this process\n   int n = points.size() / world_size;\n   if (world_rank == world_size - 1) {\n      // this is the last rank, so add the remainder of the points\n      n += points.size() % world_size;\n   }\n   if (n == 0) {\n      // there are no points assigned to this rank, so don't do anything\n      return;\n   }\n\n   // each rank gets its own copy of points\n   std::vector<Point> points_for_rank(n);\n   // divide up the points across the ranks\n   std::copy(points.begin() + world_rank * n, points.begin() + (world_rank + 1) * n, points_for_rank.begin());\n\n   // count number of points in each quadrant\n   std::array<size_t, 4> bins_for_rank;\n   // the number of points in a quadrant is the number of points in the quadrant divided by four\n   // so we divide by 4 * the number of points in the quadrant\n   for (int i = 0; i < 4; ++i) {\n      bins_for_rank[i] = std::count_if(points_for_rank.begin(), points_for_rank.end(), [i](Point const& point) {\n         return (point.x >= 0 && point.y >= 0) && ((point.x > 0 && point.y > 0) || (i % 2 == 0));\n      }) / (4 * n);\n   }\n\n   // send the results to rank 0\n   MPI_Gather(bins_for_rank.data(), 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   if (world_rank == 0) {\n      // add up the results from all the ranks\n      for (int i = 1; i < world_size; ++i) {\n         for (int j = 0; j < 4; ++j) {\n            bins[j] += bins[j + 4 * i];\n         }\n      }\n   }\n}",
            "int rank;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    const int num_threads = 4;\n    omp_set_num_threads(num_threads);\n    // bins[0] = number of points in quadrant 0\n    // bins[1] = number of points in quadrant 1\n    // bins[2] = number of points in quadrant 2\n    // bins[3] = number of points in quadrant 3\n    // Assume `bins` has been zeroed before calling this function\n\n    /* TODO: Your code here */\n    // 1. calculate the number of points in each quadrant\n    // 2. store the result in bins\n}",
            "bins = {0, 0, 0, 0};\n\tsize_t num_points = points.size();\n\t// TODO: implement this function\n}",
            "// TODO implement this function\n   size_t size = points.size();\n   size_t rank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   std::vector<size_t> local_bins(4, 0);\n   int i;\n#pragma omp parallel for\n   for (i = 0; i < size; ++i) {\n      int rank;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      Point const& point = points[i];\n      if (point.x >= 0 && point.y >= 0)\n         local_bins[0]++;\n      else if (point.x < 0 && point.y >= 0)\n         local_bins[1]++;\n      else if (point.x < 0 && point.y < 0)\n         local_bins[2]++;\n      else\n         local_bins[3]++;\n   }\n#pragma omp barrier\n   MPI_Gather(&local_bins[0], 4, MPI_INT, &bins[0], 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   // distribute work to workers\n   int n = points.size();\n   std::vector<int> numPointsPerRank(size);\n   MPI_Allgather(&n, 1, MPI_INT, numPointsPerRank.data(), 1, MPI_INT, MPI_COMM_WORLD);\n\n   // compute starting index of each rank's work\n   std::vector<int> start(size);\n   start[0] = 0;\n   for (int i = 1; i < size; i++) {\n      start[i] = start[i-1] + numPointsPerRank[i-1];\n   }\n\n   // each rank performs work\n   int numPoints = start[size-1] + numPointsPerRank[size-1];\n   bins = std::array<size_t, 4>();\n\n#pragma omp parallel for\n   for (int i = 0; i < numPoints; i++) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0)\n         bins[0]++;\n      else if (p.x < 0 && p.y > 0)\n         bins[1]++;\n      else if (p.x < 0 && p.y < 0)\n         bins[2]++;\n      else\n         bins[3]++;\n   }\n\n   // combine results from each rank into bins on rank 0\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n\n   // TODO\n}",
            "//TODO: implement this function\n   return;\n}",
            "size_t n = points.size();\n  std::array<int, 4> count;\n\n  #pragma omp parallel for\n  for (size_t i = 0; i < n; i++) {\n    double x = points[i].x;\n    double y = points[i].y;\n\n    if (x > 0) {\n      if (y > 0) {\n        count[0]++;\n      } else {\n        count[1]++;\n      }\n    } else {\n      if (y > 0) {\n        count[2]++;\n      } else {\n        count[3]++;\n      }\n    }\n  }\n\n  MPI_Reduce(count.data(), bins.data(), count.size(), MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "bins.fill(0);\n   std::map<std::pair<int, int>, std::vector<Point>> buckets;\n\n   // bucket points by quadrant\n   #pragma omp parallel for\n   for(size_t i = 0; i < points.size(); ++i) {\n      int x_quadrant = points[i].x < 0? -1 : points[i].x > 0? 1 : 0;\n      int y_quadrant = points[i].y < 0? -1 : points[i].y > 0? 1 : 0;\n      buckets[std::make_pair(x_quadrant, y_quadrant)].push_back(points[i]);\n   }\n\n   // get counts\n   auto const& bucket_data = buckets.data();\n   #pragma omp parallel for\n   for(size_t i = 0; i < buckets.size(); ++i) {\n      bins[i] = bucket_data[i].second.size();\n   }\n}",
            "bins.fill(0);\n\n   // TODO\n}",
            "// TODO: Implement countQuadrants\n}",
            "// Count the number of points in each quadrant\n}",
            "// TODO: implement this function\n}",
            "//TODO\n}",
            "// TODO: Your code here\n}",
            "bins = {0, 0, 0, 0};\n   /* implement me */\n}",
            "// TODO: your code here\n   int numprocs = 1;\n   MPI_Comm_size(MPI_COMM_WORLD, &numprocs);\n   int myrank = 0;\n   MPI_Comm_rank(MPI_COMM_WORLD, &myrank);\n   int nthreads = 1;\n   #pragma omp parallel\n   {\n      nthreads = omp_get_num_threads();\n   }\n\n   if (numprocs > points.size()) {\n      numprocs = points.size();\n   }\n   std::vector<size_t> counts(numprocs, 0);\n   std::vector<size_t> start(numprocs, 0);\n   std::vector<size_t> end(numprocs, 0);\n\n   int min = 0;\n   int max = 0;\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      int x = points[i].x;\n      int y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         max++;\n      } else if (x < 0 && y >= 0) {\n         min++;\n      } else if (x < 0 && y < 0) {\n         min++;\n      } else if (x >= 0 && y < 0) {\n         max++;\n      }\n   }\n\n   MPI_Gather(&min, 1, MPI_INT, &counts[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n   if (myrank == 0) {\n      for (int i = 1; i < numprocs; i++) {\n         start[i] = counts[i-1];\n         counts[i] = counts[i-1] + counts[i];\n         end[i] = counts[i];\n      }\n   }\n   MPI_Scatter(counts.data(), 1, MPI_INT, &counts[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   #pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      int x = points[i].x;\n      int y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         int idx = start[myrank] + counts[myrank];\n         counts[myrank]++;\n      } else if (x < 0 && y >= 0) {\n         int idx = start[myrank] + counts[myrank];\n         counts[myrank]++;\n      } else if (x < 0 && y < 0) {\n         int idx = start[myrank] + counts[myrank];\n         counts[myrank]++;\n      } else if (x >= 0 && y < 0) {\n         int idx = start[myrank] + counts[myrank];\n         counts[myrank]++;\n      }\n   }\n\n   MPI_Gather(counts.data(), 1, MPI_INT, bins.data(), 1, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "int rank, nRanks;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nRanks);\n\n   // TODO\n\n   // for (int i = 0; i < 4; ++i) {\n   //    bins[i] = count;\n   // }\n}",
            "bins = {};\n   int rank, num_ranks;\n\n   // Get number of ranks and rank number\n   MPI_Comm_size(MPI_COMM_WORLD, &num_ranks);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   int num_points = points.size();\n\n   // Get the number of points that each rank will process\n   int count = num_points / num_ranks;\n\n   // Split the image points into chunks for each rank\n   std::vector<Point> image_points(count);\n   MPI_Scatter(points.data(), count, MPI_DOUBLE, image_points.data(), count, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n   // Count the number of points in each quadrant\n   omp_set_num_threads(num_ranks);\n   #pragma omp parallel\n   {\n       int thread_rank = omp_get_thread_num();\n       int num_threads = omp_get_num_threads();\n       std::vector<Point> thread_points(num_points/num_threads);\n       int start = thread_rank * (num_points/num_threads);\n       int end = (thread_rank + 1) * (num_points/num_threads);\n       std::copy(image_points.begin() + start, image_points.begin() + end, thread_points.begin());\n       int num_in_quad = std::count_if(thread_points.begin(), thread_points.end(),\n       [](Point const& p) {\n           return p.x > 0 && p.y > 0;\n       });\n       #pragma omp atomic\n       bins[0] += num_in_quad;\n   }\n\n   MPI_Gather(bins.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "// TODO: Fill in this function\n   size_t numPoints = points.size();\n   bins = std::array<size_t,4>();\n   #pragma omp parallel for\n   for (int i=0; i < numPoints; i++) {\n      int rank = omp_get_thread_num();\n      if (points[i].x > 0) {\n         if (points[i].y > 0) {\n            bins[rank] += 1;\n         }\n         else {\n            bins[rank+1] += 1;\n         }\n      }\n      else {\n         if (points[i].y > 0) {\n            bins[rank+2] += 1;\n         }\n         else {\n            bins[rank+3] += 1;\n         }\n      }\n   }\n}",
            "bins[0] = 0;\n\tbins[1] = 0;\n\tbins[2] = 0;\n\tbins[3] = 0;\n\n\t// parallel region\n\t#pragma omp parallel for\n\tfor (int i = 0; i < points.size(); i++) {\n\t\tif (points[i].x < 0 && points[i].y > 0) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[0] += 1;\n\t\t} else if (points[i].x > 0 && points[i].y > 0) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[1] += 1;\n\t\t} else if (points[i].x > 0 && points[i].y < 0) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[2] += 1;\n\t\t} else if (points[i].x < 0 && points[i].y < 0) {\n\t\t\t#pragma omp atomic\n\t\t\tbins[3] += 1;\n\t\t}\n\t}\n\n\t// reduce results to rank 0\n\tint rank;\n\tMPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\tif (rank == 0) {\n\t\t// TODO: You may want to use a reduction here, too.\n\t\tbins[0] += bins[1];\n\t\tbins[2] += bins[3];\n\t} else {\n\t\tbins[0] = 0;\n\t\tbins[1] = 0;\n\t\tbins[2] = 0;\n\t\tbins[3] = 0;\n\t}\n\n\tMPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\tMPI_Barrier(MPI_COMM_WORLD);\n\n\tif (rank == 0) {\n\t\tbins[0] /= 2;\n\t\tbins[1] /= 2;\n\t\tbins[2] /= 2;\n\t\tbins[3] /= 2;\n\t}\n}",
            "// TODO\n\n}",
            "// TODO: fill in code here\n}",
            "// TODO\n}",
            "#pragma omp parallel\n  {\n    #pragma omp single\n    {\n      MPI_Comm_size(MPI_COMM_WORLD, &bins[0]);\n      MPI_Comm_rank(MPI_COMM_WORLD, &bins[1]);\n    }\n  }\n  //...\n  #pragma omp parallel\n  {\n    #pragma omp for\n    for (int i = 0; i < bins[0]; i++) {\n      bins[2] = omp_get_num_threads();\n      bins[3] = omp_get_max_threads();\n    }\n  }\n}",
            "std::array<size_t, 4> local_bins;\n   for (auto &b : local_bins) b = 0;\n   #pragma omp parallel for\n   for (auto const& p : points) {\n      int rank = 0;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      int nprocs = 0;\n      MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n      if (rank < nprocs/4) {\n         if (p.x >= 0 && p.y >= 0) ++local_bins[0];\n         if (p.x <= 0 && p.y >= 0) ++local_bins[1];\n         if (p.x <= 0 && p.y <= 0) ++local_bins[2];\n         if (p.x >= 0 && p.y <= 0) ++local_bins[3];\n      }\n   }\n   MPI_Reduce(local_bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// your code here\n   int rank, size;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   int n = points.size();\n   int q1 = n / 2, q2 = n / 4;\n   int t1 = n / size;\n   int t2 = t1 + (n % size) / (size - 1);\n   int t3 = q1 + q2 - t1 - t2;\n   int t4 = q1 - (n % size) / (size - 1);\n   bins = {0, 0, 0, 0};\n\n   #pragma omp parallel for\n   for (int i = rank * t1; i < (rank + 1) * t1; i++) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n\n   #pragma omp parallel for\n   for (int i = rank * t2; i < (rank + 1) * t2; i++) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n\n   #pragma omp parallel for\n   for (int i = rank * t3; i < (rank + 1) * t3; i++) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n\n   #pragma omp parallel for\n   for (int i = rank * t4; i < (rank + 1) * t4; i++) {\n      Point p = points[i];\n      if (p.x > 0 && p.y > 0) {\n         bins[0]++;\n      } else if (p.x < 0 && p.y > 0) {\n         bins[1]++;\n      } else if (p.x < 0 && p.y < 0) {\n         bins[2]++;\n      } else if (p.x > 0 && p.y < 0) {\n         bins[3]++;\n      }\n   }\n\n   MPI_Reduce(bins.data(), bins.data() + 4, 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n   if (rank == 0) {\n      for (int i = 1; i < size; i++) {\n         bins[0] += bins[0 + i * 4];\n         bins[1] += bins[1 + i * 4];\n         bins[2] += bins[2 + i * 4];\n         bins[3] += bins[3 + i * 4];\n      }\n   }\n}",
            "// Insert your code here\n}",
            "// Your code here\n   int size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int num_threads = 2;\n   omp_set_num_threads(num_threads);\n   int count = points.size();\n   if (rank == 0) {\n      bins = std::array<size_t, 4> { 0, 0, 0, 0 };\n   }\n   int n = (count / size);\n   int r = count % size;\n   int start = n * rank + std::min(rank, r);\n   int end = start + n + (rank < r? 1 : 0);\n   for (int i = start; i < end; i++) {\n      std::pair<int, int> bin = getQuadrant(points[i]);\n      bins[bin.first] += 1;\n   }\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "// TODO: implement me\n}",
            "// TODO: implement me\n}",
            "// TODO: Implement this function\n}",
            "double const my_pi = 3.14159;\n#pragma omp parallel for\n    for (int i = 0; i < 1000000; ++i) {\n        Point point = points[i];\n        bins[0] += point.x >= 0 && point.y >= 0;\n        bins[1] += point.x < 0 && point.y >= 0;\n        bins[2] += point.x < 0 && point.y < 0;\n        bins[3] += point.x >= 0 && point.y < 0;\n    }\n}",
            "bins = {0, 0, 0, 0};\n\n   #pragma omp parallel for\n   for (size_t i = 0; i < points.size(); i++) {\n      double x = points[i].x;\n      double y = points[i].y;\n      int rank = 0;\n\n      #pragma omp critical(cartesianPoints)\n      {\n         double xmin = -10, ymin = -10, xmax = 10, ymax = 10;\n         double xwidth = xmax - xmin;\n         double ywidth = ymax - ymin;\n\n         if (x >= xmin && x <= xmax && y >= ymin && y <= ymax) {\n            if (x < xmin + xwidth / 2 && y < ymin + ywidth / 2) {\n               bins[0] += 1;\n            } else if (x >= xmin + xwidth / 2 && y < ymin + ywidth / 2) {\n               bins[1] += 1;\n            } else if (x < xmin + xwidth / 2 && y >= ymin + ywidth / 2) {\n               bins[2] += 1;\n            } else if (x >= xmin + xwidth / 2 && y >= ymin + ywidth / 2) {\n               bins[3] += 1;\n            }\n         }\n      }\n   }\n}",
            "// YOUR CODE HERE\n    bins = std::array<size_t, 4>();\n    // 1. Use MPI_Scatter to partition points among all the ranks\n    // 2. Each rank will count the number of points in each quadrant\n    // 3. Use MPI_Gather to collect counts from all ranks\n    // 4. Use MPI_Barrier to ensure all ranks have finished\n    // 5. Use MPI_Reduce to combine counts from all ranks\n}",
            "// TODO: Count the points in each quadrant in parallel\n  int rank, ntasks;\n  MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n  MPI_Comm_size(MPI_COMM_WORLD, &ntasks);\n\n  std::vector<Point> local_points;\n\n  if (rank == 0) {\n    local_points.assign(points.begin(), points.end());\n  }\n\n  // Send and receive points\n  MPI_Scatter(local_points.data(), local_points.size(), MPI_DOUBLE, &local_points, local_points.size(), MPI_DOUBLE, 0, MPI_COMM_WORLD);\n\n  std::vector<size_t> bins_local(4);\n\n  // Count in parallel\n  #pragma omp parallel num_threads(ntasks) shared(bins_local, local_points)\n  {\n    #pragma omp for\n    for (size_t i = 0; i < local_points.size(); ++i) {\n      if (local_points[i].x > 0 && local_points[i].y > 0) {\n        bins_local[0]++;\n      } else if (local_points[i].x < 0 && local_points[i].y > 0) {\n        bins_local[1]++;\n      } else if (local_points[i].x < 0 && local_points[i].y < 0) {\n        bins_local[2]++;\n      } else if (local_points[i].x > 0 && local_points[i].y < 0) {\n        bins_local[3]++;\n      }\n    }\n  }\n\n  // Send bins\n  MPI_Gather(bins_local.data(), bins_local.size(), MPI_UNSIGNED_LONG, bins.data(), bins.size(), MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n}",
            "//...\n}",
            "int nproc, rank;\n   MPI_Comm_size(MPI_COMM_WORLD, &nproc);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // TODO: your code here\n   size_t local_count = 0;\n   #pragma omp parallel for reduction(+:local_count)\n   for (int i = 0; i < points.size(); ++i) {\n      double x = points[i].x;\n      double y = points[i].y;\n      if (x >= 0 && y >= 0) {\n         local_count++;\n      }\n      else if (x < 0 && y >= 0) {\n         local_count++;\n      }\n      else if (x >= 0 && y < 0) {\n         local_count++;\n      }\n      else if (x < 0 && y < 0) {\n         local_count++;\n      }\n   }\n\n   MPI_Reduce(&local_count, &bins[0], 4, MPI_INT, MPI_SUM, 0, MPI_COMM_WORLD);\n\n   // TODO: your code here\n}",
            "MPI_Comm_size(MPI_COMM_WORLD, &bins[0]);\n   MPI_Comm_rank(MPI_COMM_WORLD, &bins[1]);\n   MPI_Comm comm;\n   MPI_Comm_split(MPI_COMM_WORLD, bins[1], bins[0], &comm);\n   MPI_Comm_size(comm, &bins[2]);\n   MPI_Comm_rank(comm, &bins[3]);\n\n   int rank = bins[3];\n   size_t const num_threads = omp_get_max_threads();\n   size_t const num_blocks = 256;\n   size_t const num_points = points.size();\n\n   // each rank has a copy of the points vector\n   std::vector<Point> rank_points(points);\n   // each rank has a copy of bins\n   std::array<size_t, 4> rank_bins = bins;\n\n   size_t num_pixels = 0;\n\n#pragma omp parallel num_threads(num_threads)\n   {\n      int const thread_num = omp_get_thread_num();\n      size_t start = thread_num * (num_points / num_threads);\n      size_t end = (thread_num + 1) * (num_points / num_threads);\n      if (thread_num == num_threads - 1) end = num_points;\n      size_t local_pixels = 0;\n      for (size_t i = start; i < end; i++) {\n         // count the pixels in this block\n         if (rank_points[i].x >= 0) {\n            // count in left quadrant\n            if (rank_points[i].y >= 0) {\n               // count in upper left quadrant\n               local_pixels++;\n            }\n            // count in right quadrant\n            if (rank_points[i].y <= 0) {\n               // count in upper right quadrant\n               local_pixels++;\n            }\n         }\n         // count in left quadrant\n         if (rank_points[i].x <= 0) {\n            // count in lower left quadrant\n            if (rank_points[i].y >= 0) {\n               // count in upper left quadrant\n               local_pixels++;\n            }\n            // count in right quadrant\n            if (rank_points[i].y <= 0) {\n               // count in lower right quadrant\n               local_pixels++;\n            }\n         }\n      }\n      // sum up the local pixels in this block and add to the global count\n      MPI_Reduce(&local_pixels, &num_pixels, 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n   }\n\n   // add the number of pixels counted from this rank to the total count\n   MPI_Reduce(&num_pixels, &bins[0], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, comm);\n\n   // print the counts for this rank\n   printf(\"Rank %d: x_rank=%ld, y_rank=%ld, x_size=%ld, y_size=%ld, num_pixels=%ld\\n\", rank, rank_bins[1], rank_bins[3], rank_bins[2], rank_bins[0], bins[0]);\n\n   MPI_Barrier(MPI_COMM_WORLD);\n}",
            "size_t numPoints = points.size();\n   size_t lower, upper;\n   if (numPoints > 0) {\n      lower = 0;\n      upper = numPoints;\n      size_t num_threads = 0;\n      size_t num_procs = 0;\n      MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n      MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n      num_threads = omp_get_max_threads();\n      // TODO: compute lower and upper in parallel\n      std::cout << \"num_procs: \" << num_procs << \" num_threads: \" << num_threads << std::endl;\n      int rank = 0;\n      MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n      std::cout << \"rank: \" << rank << std::endl;\n      size_t stride = numPoints / num_procs;\n      if (rank == 0) {\n         for (int i = 1; i < num_procs; ++i) {\n            size_t lower_temp = stride * i;\n            MPI_Send(&lower_temp, 1, MPI_UNSIGNED_LONG_LONG, i, 1, MPI_COMM_WORLD);\n         }\n      } else {\n         MPI_Status status;\n         MPI_Recv(&lower, 1, MPI_UNSIGNED_LONG_LONG, 0, 1, MPI_COMM_WORLD, &status);\n      }\n      if (rank == num_procs - 1) {\n         size_t upper_temp = numPoints;\n         MPI_Send(&upper_temp, 1, MPI_UNSIGNED_LONG_LONG, 0, 2, MPI_COMM_WORLD);\n      } else {\n         MPI_Status status;\n         MPI_Recv(&upper, 1, MPI_UNSIGNED_LONG_LONG, rank + 1, 2, MPI_COMM_WORLD, &status);\n      }\n      std::cout << \"lower: \" << lower << \" upper: \" << upper << std::endl;\n\n      // TODO: count in parallel\n      // OpenMP parallel for\n      #pragma omp parallel for schedule(dynamic, 1)\n      for (size_t i = lower; i < upper; i++) {\n         // TODO: Compute quadrant\n         // Get the point coordinates\n         double x = points[i].x;\n         double y = points[i].y;\n         // Get the quadrant number\n         int quadrant = 1;\n         // Count in parallel\n         #pragma omp critical\n         {\n            bins[quadrant]++;\n         }\n      }\n   }\n}",
            "std::array<std::vector<Point>, 4> bins_local;\n\n  // TODO\n  // Count the number of points in each quadrant. Hint: use MPI to distribute the points across ranks\n  // and use OpenMP to parallelize the work in each rank.\n\n  // TODO\n  // Collect all the counts to the master rank\n\n  // TODO\n  // The master rank should store the result in bins and return.\n\n  // TODO\n  // Wait until all ranks have finished before exiting.\n}",
            "int rank, size;\n    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n    // your code here\n    double start = omp_get_wtime();\n    #pragma omp parallel for schedule(dynamic, 1)\n    for (int i = 0; i < points.size(); i++){\n        double x = points[i].x;\n        double y = points[i].y;\n        int bin = 0;\n        if(x < 0){\n            bin += 1;\n        }\n        if(y < 0){\n            bin += 2;\n        }\n        bins[bin]++;\n    }\n    double end = omp_get_wtime();\n    double elapsed = end - start;\n    std::cout << elapsed << std::endl;\n}",
            "double xMin = std::numeric_limits<double>::max();\n    double xMax = std::numeric_limits<double>::min();\n    double yMin = std::numeric_limits<double>::max();\n    double yMax = std::numeric_limits<double>::min();\n    for (auto const& point : points) {\n        if (xMin > point.x) xMin = point.x;\n        if (xMax < point.x) xMax = point.x;\n        if (yMin > point.y) yMin = point.y;\n        if (yMax < point.y) yMax = point.y;\n    }\n\n    // your code here\n    // this is a sample solution for 2 points, 2D grid\n    double yMid = (yMin + yMax) / 2;\n    double xMid = (xMin + xMax) / 2;\n    std::vector<Point> leftPoints, rightPoints;\n    std::vector<Point> topPoints, bottomPoints;\n    for (auto const& point : points) {\n        if (point.y <= yMid) {\n            if (point.x <= xMid) leftPoints.push_back(point);\n            else rightPoints.push_back(point);\n        } else {\n            if (point.x <= xMid) topPoints.push_back(point);\n            else bottomPoints.push_back(point);\n        }\n    }\n    std::array<size_t, 4> binsLeft, binsRight, binsTop, binsBottom;\n#pragma omp parallel sections\n    {\n#pragma omp section\n        countQuadrants(leftPoints, binsLeft);\n#pragma omp section\n        countQuadrants(rightPoints, binsRight);\n#pragma omp section\n        countQuadrants(topPoints, binsTop);\n#pragma omp section\n        countQuadrants(bottomPoints, binsBottom);\n    }\n    for (int i = 0; i < 4; i++) {\n        bins[i] = binsLeft[i] + binsRight[i] + binsTop[i] + binsBottom[i];\n    }\n}",
            "// TODO: implement\n    bins = {};\n    //#pragma omp parallel for\n    //for (int i = 0; i < points.size(); i++) {\n        //if (points[i].x > 0 && points[i].y > 0) {\n            //bins[0]++;\n        //}\n        //if (points[i].x < 0 && points[i].y > 0) {\n            //bins[1]++;\n        //}\n        //if (points[i].x < 0 && points[i].y < 0) {\n            //bins[2]++;\n        //}\n        //if (points[i].x > 0 && points[i].y < 0) {\n            //bins[3]++;\n        //}\n    //}\n\n    int i;\n    for (i = 0; i < points.size(); i++) {\n        if (points[i].x > 0 && points[i].y > 0) {\n            bins[0]++;\n        }\n        else if (points[i].x < 0 && points[i].y > 0) {\n            bins[1]++;\n        }\n        else if (points[i].x < 0 && points[i].y < 0) {\n            bins[2]++;\n        }\n        else if (points[i].x > 0 && points[i].y < 0) {\n            bins[3]++;\n        }\n    }\n\n}",
            "bins = {};\n   // TODO: YOUR CODE HERE\n}",
            "size_t n = points.size();\n    bins = {0, 0, 0, 0};\n    \n    // Your code here\n    int numThreads = omp_get_max_threads();\n    int numProcesses = 0;\n    \n    MPI_Comm_size(MPI_COMM_WORLD, &numProcesses);\n    \n    std::vector<int> pointsPerProcess(numProcesses, 0);\n    std::vector<int> binsPerProcess(numProcesses*4, 0);\n    std::vector<int> pointsPerThread(numThreads, 0);\n    \n    std::vector<std::vector<Point>> pointsPerThreadPerProcess(numThreads);\n    \n    int p = omp_get_thread_num();\n    for (int i = 0; i < n; i++) {\n        pointsPerThreadPerProcess[p].push_back(points[i]);\n    }\n    int i = 0;\n    for (int j = 0; j < numThreads; j++) {\n        pointsPerThread[j] = pointsPerThreadPerProcess[j].size();\n    }\n    \n    MPI_Scatter(pointsPerThread.data(), 1, MPI_INT, &pointsPerProcess[0], 1, MPI_INT, 0, MPI_COMM_WORLD);\n    \n    int start = 0;\n    for (int j = 0; j < numProcesses; j++) {\n        start = (j == 0? 0 : (start + pointsPerProcess[j-1]));\n        for (int i = 0; i < 4; i++) {\n            binsPerProcess[j*4 + i] = 0;\n        }\n    }\n    \n    int chunkSize = n / numProcesses;\n    int remainder = n % numProcesses;\n    \n    #pragma omp parallel for schedule(dynamic)\n    for (int i = 0; i < numProcesses; i++) {\n        int procStart = (i * (chunkSize + 1) + remainder) - pointsPerProcess[i];\n        int procEnd = procStart + pointsPerProcess[i];\n        for (int p = procStart; p < procEnd; p++) {\n            int x = (int)points[p].x;\n            int y = (int)points[p].y;\n            int bin = 0;\n            if (x < 0) {\n                bin += 1;\n            } else if (x > 0) {\n                bin += 2;\n            }\n            if (y < 0) {\n                bin += 4;\n            } else if (y > 0) {\n                bin += 8;\n            }\n            binsPerProcess[i*4 + bin]++;\n        }\n    }\n    \n    MPI_Gather(binsPerProcess.data(), 4, MPI_INT, bins.data(), 4, MPI_INT, 0, MPI_COMM_WORLD);\n}",
            "#pragma omp parallel for\n    for (auto i = 0; i < points.size(); ++i) {\n        auto point = points[i];\n        if (point.x >= 0 && point.y >= 0) {\n            ++bins[0];\n        } else if (point.x < 0 && point.y >= 0) {\n            ++bins[1];\n        } else if (point.x < 0 && point.y < 0) {\n            ++bins[2];\n        } else {\n            ++bins[3];\n        }\n    }\n}",
            "}",
            "#pragma omp parallel for\n   for (auto i = 0; i < points.size(); i++) {\n      int xbin = 0;\n      int ybin = 0;\n      if (points[i].x >= 0) xbin = 1;\n      if (points[i].y >= 0) ybin = 1;\n      bins[xbin + 2 * ybin] += 1;\n   }\n}",
            "//...\n  // count quadrants\n}",
            "bins = std::array<size_t, 4>();\n#pragma omp parallel for\n   for (int i = 0; i < points.size(); i++) {\n      int x, y;\n      x = (int)(points[i].x / 5);\n      y = (int)(points[i].y / 5);\n      if ((x % 2) == (y % 2)) {\n         #pragma omp critical\n         bins[x % 2 + 2 * y % 2] += 1;\n      }\n   }\n}",
            "auto n = points.size();\n   auto width = std::sqrt(n);\n   auto rank = 0, size = 1;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n\n   bins.fill(0);\n\n   #pragma omp parallel for num_threads(size) reduction(+ : bins) schedule(dynamic)\n   for (auto i = 0; i < n; ++i) {\n      auto x = points[i].x, y = points[i].y;\n      auto bin_x = std::floor(x / width), bin_y = std::floor(y / width);\n      bins[bin_x + 2 * bin_y] += 1;\n   }\n\n   MPI_Reduce(bins.data(), bins.data(), 4, MPI_UNSIGNED_LONG_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n}",
            "int rank, num_procs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &num_procs);\n\n   if (rank == 0) {\n      bins = std::array<size_t, 4>{};\n   }\n\n   double num_points = points.size();\n   std::array<size_t, 4> local_bins;\n   local_bins = std::array<size_t, 4>{};\n\n   auto lower = 0;\n   auto upper = num_points;\n   double width = num_points / num_procs;\n\n   double min_x = -1000, max_x = 1000, min_y = -1000, max_y = 1000;\n\n   if (rank == 0) {\n      for (auto const& point: points) {\n         if (point.x < min_x) {\n            min_x = point.x;\n         }\n         if (point.x > max_x) {\n            max_x = point.x;\n         }\n         if (point.y < min_y) {\n            min_y = point.y;\n         }\n         if (point.y > max_y) {\n            max_y = point.y;\n         }\n      }\n\n      double height = max_y - min_y;\n      double width = max_x - min_x;\n\n      // calculate quadrant boundaries\n      double a_x_min = -width / 2;\n      double a_x_max = width / 2;\n      double a_y_min = -height / 2;\n      double a_y_max = height / 2;\n\n      double b_x_min = width / 2;\n      double b_x_max = width;\n      double b_y_min = -height / 2;\n      double b_y_max = height / 2;\n\n      double c_x_min = -width / 2;\n      double c_x_max = width / 2;\n      double c_y_min = height / 2;\n      double c_y_max = height;\n\n      double d_x_min = -width / 2;\n      double d_x_max = width / 2;\n      double d_y_min = -height;\n      double d_y_max = -height / 2;\n\n      std::array<std::array<double, 4>, 4> boundaries = {{\n         {a_x_min, a_x_max, a_y_min, a_y_max},\n         {b_x_min, b_x_max, b_y_min, b_y_max},\n         {c_x_min, c_x_max, c_y_min, c_y_max},\n         {d_x_min, d_x_max, d_y_min, d_y_max},\n      }};\n\n      for (size_t i = 1; i < num_procs; ++i) {\n         if (i * width < min_x) {\n            MPI_Send(boundaries[3].data(), boundaries[3].size(), MPI_DOUBLE, i, i, MPI_COMM_WORLD);\n         } else if (i * width > max_x) {\n            MPI_Send(boundaries[1].data(), boundaries[1].size(), MPI_DOUBLE, i, i, MPI_COMM_WORLD);\n         } else {\n            MPI_Send(boundaries[0].data(), boundaries[0].size(), MPI_DOUBLE, i, i, MPI_COMM_WORLD);\n         }\n      }\n   }\n\n   MPI_Status status;\n   MPI_Recv(boundaries[rank].data(), boundaries[rank].size(), MPI_DOUBLE, 0, rank, MPI_COMM_WORLD, &status);\n   if (rank == 0) {\n      printf(\"lower: %d\\nupper: %d\\nwidth: %d\\n\", lower, upper, width);\n      printf(\"min_x: %lf\\nmax_x: %lf\\nmin_y: %lf\\nmax_y: %lf\\n\", min_x, max_x, min_y, max_y);\n      printf(\"boundaries[0]: (%lf, %lf, %lf, %lf)\\n\", boundaries[0][0], boundaries[0][1], boundaries[0][2], boundaries[0][3]);\n      printf(\"boundaries[1]: (%lf, %lf, %lf, %lf)\\n\", boundaries[1][0], boundaries[1][1], boundaries[1][",
            "int rank, nprocs;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n\n   if (rank == 0) {\n      bins = std::array<size_t, 4>{};\n      //... code to initialize bins to 0\n   }\n\n   //... code to compute bins\n\n   //... code to combine bins on rank 0\n}",
            "/* Your code here */\n   int rank, size;\n   MPI_Comm_size(MPI_COMM_WORLD, &size);\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n   int proc_size = sqrt(size);\n   std::vector<Point> points_proc(0);\n   size_t start, end, bin;\n   int start_x, start_y, end_x, end_y;\n\n   if (rank == 0) {\n       for (int i = 0; i < proc_size; i++) {\n           for (int j = 0; j < proc_size; j++) {\n               start_x = 2 * i;\n               start_y = 2 * j;\n               end_x = 2 * (i+1);\n               end_y = 2 * (j+1);\n               start = 0;\n               end = points.size() - 1;\n               for (size_t k = 0; k < points.size(); k++) {\n                   if (points[k].x >= start_x && points[k].x < end_x && points[k].y >= start_y && points[k].y < end_y) {\n                       points_proc.push_back(points[k]);\n                       if (k == end) {\n                           break;\n                       }\n                       end++;\n                   } else {\n                       if (k == start) {\n                           start++;\n                       }\n                       if (k == end) {\n                           break;\n                       }\n                       end--;\n                   }\n               }\n               if (points_proc.size() > 0) {\n                   MPI_Send(&points_proc[0], points_proc.size(), MPI_DOUBLE, j+1, 0, MPI_COMM_WORLD);\n                   points_proc.clear();\n               }\n           }\n       }\n   } else {\n       MPI_Recv(&points_proc[0], points_proc.size(), MPI_DOUBLE, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n   }\n\n   if (rank == 0) {\n       for (int i = 0; i < proc_size; i++) {\n           MPI_Recv(&bins[0], bins.size(), MPI_UNSIGNED_LONG, i + 1, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n       }\n   } else {\n       for (int i = 0; i < points_proc.size(); i++) {\n           start_x = 2 * ((int)floor(points_proc[i].x/2));\n           start_y = 2 * ((int)floor(points_proc[i].y/2));\n           end_x = 2 * ((int)ceil(points_proc[i].x/2));\n           end_y = 2 * ((int)ceil(points_proc[i].y/2));\n           bin = 0;\n           if (start_x == end_x) {\n               if (start_y == end_y) {\n                   bin = 0;\n               } else if (start_y == end_y - 1) {\n                   bin = 1;\n               } else if (start_y == end_y - 2) {\n                   bin = 2;\n               } else {\n                   bin = 3;\n               }\n           } else if (start_x == end_x - 1) {\n               if (start_y == end_y) {\n                   bin = 0;\n               } else if (start_y == end_y - 1) {\n                   bin = 1;\n               } else if (start_y == end_y - 2) {\n                   bin = 2;\n               } else {\n                   bin = 3;\n               }\n           } else if (start_x == end_x - 2) {\n               if (start_y == end_y) {\n                   bin = 0;\n               } else if (start_y == end_y - 1) {\n                   bin = 1;\n               } else if (start_y == end_y - 2) {\n                   bin = 2;\n               } else {\n                   bin = 3;\n               }\n           } else {\n               if (start_y == end_y) {\n                   bin = 0;\n               } else if (start_y == end_y - 1) {\n                   bin = 1;\n               } else if (start_y == end_y - 2) {\n                   bin = 2;\n               } else {\n                   bin = 3;\n               }\n           }\n           MPI_Reduce(&bin, &bins[bin], 1, MPI_UNSIGNED_LONG, MPI_SUM, 0, MPI_COMM_WORLD);\n       }\n       MPI_",
            "bins.fill(0);\n\n  // TODO count points in parallel\n  // Hint:\n  // - Each rank has its own copy of image\n  // - All points are stored on rank 0\n  // - Use OpenMP\n  // - Use MPI collectives\n  // - The MPI ranks (0, 1) can count their points in parallel\n  // - The MPI ranks (2, 3) can count their points in parallel\n  // - MPI has already been initialized\n}",
            "// TODO: implement this function\n\n   // start timer\n   auto start = std::chrono::steady_clock::now();\n\n   // get number of processes\n   int numProcs;\n   MPI_Comm_size(MPI_COMM_WORLD, &numProcs);\n\n   // get number of threads\n   int numThreads = omp_get_max_threads();\n\n   // send number of threads\n   MPI_Bcast(&numThreads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n\n   // calculate number of points per thread\n   int pointsPerThread = points.size() / numThreads;\n\n   // divide up work by threads\n   std::vector<std::pair<size_t, size_t>> work;\n   for (int i = 0; i < numThreads; ++i) {\n      work.emplace_back(i * pointsPerThread, (i + 1) * pointsPerThread);\n   }\n\n   // get rank\n   int rank;\n   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n\n   // distribute work\n   std::vector<std::pair<size_t, size_t>> splitPoints;\n   splitPoints.resize(numProcs);\n   MPI_Scatter(work.data(), 2, MPI_UNSIGNED_LONG, splitPoints.data(), 2, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // get local points\n   std::vector<Point> localPoints;\n   localPoints.reserve(splitPoints[rank].second - splitPoints[rank].first);\n   for (size_t i = splitPoints[rank].first; i < splitPoints[rank].second; ++i) {\n      localPoints.emplace_back(points[i]);\n   }\n\n   // create bins\n   std::array<size_t, 4> localBins;\n   localBins.fill(0);\n\n   // calculate quadrant counts in parallel\n   #pragma omp parallel num_threads(numThreads)\n   {\n      // get thread id\n      int threadId = omp_get_thread_num();\n\n      // calculate quadrant counts\n      for (Point const& point : localPoints) {\n         if (point.x < 0 && point.y < 0) {\n            ++localBins[0];\n         } else if (point.x >= 0 && point.y < 0) {\n            ++localBins[1];\n         } else if (point.x < 0 && point.y >= 0) {\n            ++localBins[2];\n         } else {\n            ++localBins[3];\n         }\n      }\n   }\n\n   // gather local bins to rank 0\n   MPI_Gather(localBins.data(), 4, MPI_UNSIGNED_LONG, bins.data(), 4, MPI_UNSIGNED_LONG, 0, MPI_COMM_WORLD);\n\n   // end timer\n   auto end = std::chrono::steady_clock::now();\n\n   // print execution time\n   auto time = end - start;\n   if (rank == 0) {\n      std::cout << \"Execution time: \" << time.count() / 1e9 << \"s\\n\";\n   }\n}",
            "//...\n}",
            "// TODO: implement this function\n\n   // Your solution here\n}"
        ]
    }
]